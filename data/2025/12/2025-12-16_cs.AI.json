[
    {
        "order": 1,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11835",
        "abs_url": "https://arxiv.org/abs/2512.11835",
        "pdf_url": "https://arxiv.org/pdf/2512.11835",
        "title": "A Monad-Based Clause Architecture for Artificial Age Score (AAS) in Large Language Models",
        "authors": [
            "Seyma Yaman Kayadibi"
        ],
        "comments": "42 pages, 6 toy simulation Python implementations, 20 monad clauses instantiated across six system bundles (ontology, dynamics, representation and consciousness, harmony and reason, body and organisation, teleology)",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multiagent Systems (cs.MA); Neural and Evolutionary Computing (cs.NE)",
        "abstract": "Large language models (LLMs) are often deployed as powerful yet opaque systems, leaving open how their internal memory and \"self-like\" behavior should be governed in a principled and auditable way. The Artificial Age Score (AAS) was previously introduced and mathematically justified through three theorems that characterise it as a metric of artificial memory aging. Building on this foundation, the present work develops an engineering-oriented, clause-based architecture that imposes law-like constraints on LLM memory and control. Twenty selected monads from Leibniz's Monadology are grouped into six bundles: ontology, dynamics, representation and consciousness, harmony and reason, body and organisation, and teleology, and each bundle is realised as an executable specification on top of the AAS kernel. Across six minimal Python implementations, these clause families are instantiated in numerical experiments acting on channel-level quantities such as recall scores, redundancy, and weights. Each implementation follows a four-step pattern: inputs and setup, clause implementation, numerical results, and implications for LLM design, emphasising that the framework is not only philosophically motivated but also directly implementable. The experiments show that the clause system exhibits bounded and interpretable behavior: AAS trajectories remain continuous and rate-limited, contradictions and unsupported claims trigger explicit penalties, and hierarchical refinement reveals an organic structure in a controlled manner. Dual views and goal-action pairs are aligned by harmony terms, and windowed drift in perfection scores separates sustained improvement from sustained degradation. Overall, the monad-based clause framework uses AAS as a backbone and provides a transparent, code-level blueprint for constraining and analyzing internal dynamics in artificial agents.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 2,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11864",
        "abs_url": "https://arxiv.org/abs/2512.11864",
        "pdf_url": "https://arxiv.org/pdf/2512.11864",
        "title": "Solving Parallel Machine Scheduling With Precedences and Cumulative Resource Constraints With Calendars",
        "authors": [
            "Christoph Einspieler",
            "Matthias Horn",
            "Marie-Louise Lackner",
            "Patrick Malik",
            "Nysret Musliu",
            "Felix Winter"
        ],
        "comments": "18 pages, 4 figures",
        "subjects": "Artificial Intelligence (cs.AI); Optimization and Control (math.OC)",
        "abstract": "The task of finding efficient production schedules for parallel machines is a challenge that arises in most industrial manufacturing domains. There is a large potential to minimize production costs through automated scheduling techniques, due to the large-scale requirements of modern factories. In the past, solution approaches have been studied for many machine scheduling variations, where even basic variants have been shown to be NP-hard. However, in today's real-life production environments, additional complex precedence constraints and resource restrictions with calendars arise that must be fulfilled. These additional constraints cannot be tackled efficiently by existing solution techniques. Thus, there is a strong need to develop and analyze automated methods that can solve such real-life parallel machine scheduling scenarios. In this work, we introduce a novel variant of parallel machine scheduling with job precedences and calendar-based cumulative resource constraints that arises in real-life industrial use cases. A constraint modeling approach is proposed as an exact solution method for small scheduling scenarios together with state-of-the-art constraint-solving technology. Further, we propose a construction heuristic as well as a tailored metaheuristic using local search to efficiently tackle large-scale problem instances. This metaheuristic approach has been deployed and is currently being used in an industrial setting.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 3,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11902",
        "abs_url": "https://arxiv.org/abs/2512.11902",
        "pdf_url": "https://arxiv.org/pdf/2512.11902",
        "title": "Mirror Mode in Fire Emblem: Beating Players at their own Game with Imitation and Reinforcement Learning",
        "authors": [
            "Yanna Elizabeth Smid",
            "Peter van der Putten",
            "Aske Plaat"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG)",
        "abstract": "Enemy strategies in turn-based games should be surprising and unpredictable. This study introduces Mirror Mode, a new game mode where the enemy AI mimics the personal strategy of a player to challenge them to keep changing their gameplay. A simplified version of the Nintendo strategy video game Fire Emblem Heroes has been built in Unity, with a Standard Mode and a Mirror Mode. Our first set of experiments find a suitable model for the task to imitate player demonstrations, using Reinforcement Learning and Imitation Learning: combining Generative Adversarial Imitation Learning, Behavioral Cloning, and Proximal Policy Optimization. The second set of experiments evaluates the constructed model with player tests, where models are trained on demonstrations provided by participants. The gameplay of the participants indicates good imitation in defensive behavior, but not in offensive strategies. Participant's surveys indicated that they recognized their own retreating tactics, and resulted in an overall higher player-satisfaction for Mirror Mode. Refining the model further may improve imitation quality and increase player's satisfaction, especially when players face their own strategies. The full code and survey results are stored at: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 4,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11907",
        "abs_url": "https://arxiv.org/abs/2512.11907",
        "pdf_url": "https://arxiv.org/pdf/2512.11907",
        "title": "Structured Personalization: Modeling Constraints as Matroids for Data-Minimal LLM Agents",
        "authors": [
            "Daniel Platnick",
            "Marjan Alirezaie",
            "Hossein Rahnama"
        ],
        "comments": "Accepted to the AAAI 2026 Workshop on Personalization in the Era of Large Foundation Models (PerFM), 5 pages, 1 figure",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Personalizing Large Language Model (LLM) agents requires conditioning them on user-specific data, creating a critical trade-off between task utility and data disclosure. While the utility of adding user data often exhibits diminishing returns (i.e., submodularity), enabling near-optimal greedy selection, real-world personalization is complicated by structural constraints. These include logical dependencies (e.g., selecting fact A requires fact B), categorical quotas (e.g., select at most one writing style), and hierarchical rules (e.g., select at most two social media preferences, of which at most one can be for a professional network). These constraints violate the assumptions of standard subset selection algorithms. We propose a principled method to formally model such constraints. We introduce a compilation process that transforms a user's knowledge graph with dependencies into a set of abstract macro-facets. Our central result is a proof that common hierarchical and quota-based constraints over these macro-facets form a valid laminar matroid. This theoretical characterization lets us cast structured personalization as submodular maximization under a matroid constraint, enabling greedy with constant-factor guarantees (and (1-1/e) via continuous greedy) for a much richer and more realistic class of problems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 5,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11909",
        "abs_url": "https://arxiv.org/abs/2512.11909",
        "pdf_url": "https://arxiv.org/pdf/2512.11909",
        "title": "Causal Strengths and Leaky Beliefs: Interpreting LLM Reasoning via Noisy-OR Causal Bayes Nets",
        "authors": [
            "Hanna Dettki"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "The nature of intelligence in both humans and machines is a longstanding question. While there is no universally accepted definition, the ability to reason causally is often regarded as a pivotal aspect of intelligence (Lake et al., 2017). Evaluating causal reasoning in LLMs and humans on the same tasks provides hence a more comprehensive understanding of their respective strengths and weaknesses. Our study asks: (Q1) Are LLMs aligned with humans given the \\emph{same} reasoning tasks? (Q2) Do LLMs and humans reason consistently at the task level? (Q3) Do they have distinct reasoning signatures? We answer these by evaluating 20+ LLMs on eleven semantically meaningful causal tasks formalized by a collider graph ($C_1\\!\\to\\!E\\!\\leftarrow\\!C_2$ ) under \\emph{Direct} (one-shot number as response = probability judgment of query node being one and \\emph{Chain of Thought} (CoT; think first, then provide answer). Judgments are modeled with a leaky noisy-OR causal Bayes net (CBN) whose parameters $\\theta=(b,m_1,m_2,p(C)) \\in [0,1]$ include a shared prior $p(C)$; we select the winning model via AIC between a 3-parameter symmetric causal strength ($m_1{=}m_2$) and 4-parameter asymmetric ($m_1{\\neq}m_2$) variant.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 6,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11912",
        "abs_url": "https://arxiv.org/abs/2512.11912",
        "pdf_url": "https://arxiv.org/pdf/2512.11912",
        "title": "Robustness of Probabilistic Models to Low-Quality Data: A Multi-Perspective Analysis",
        "authors": [
            "Liu Peng",
            "Yaochu Jin"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "A systematic, comparative investigation into the effects of low-quality data reveals a stark spectrum of robustness across modern probabilistic models. We find that autoregressive language models, from token prediction to sequence-to-sequence tasks, are remarkably resilient (for GPT-2, test NLL increases modestly from 2.87 to 3.59 despite 50% token corruption). By contrast, under the same levels of data corruption, class-conditional diffusion models degrade catastrophically (image-label consistency plummets by 56.81% relative to baseline), while classifiers show a moderate impact that diminishes with dataset scale. To explain these discrepancies, we analyze the results through a multi-perspective lens, integrating information theory, PAC learning, and gradient dynamics. These analyses suggest that robustness is heavily influenced by two key principles: the richness of conditioning information, which constrains the learning problem, and the absolute information content of the training data, which allows the signal from correct information to dominate statistical noise.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 7,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11920",
        "abs_url": "https://arxiv.org/abs/2512.11920",
        "pdf_url": "https://arxiv.org/pdf/2512.11920",
        "title": "CXL-SpecKV: A Disaggregated FPGA Speculative KV-Cache for Datacenter LLM Serving",
        "authors": [
            "Dong Liu",
            "Yanxuan Yu"
        ],
        "comments": "Accepted to FPGA'26 Oral",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Large Language Models (LLMs) have revolutionized natural language processing tasks, but their deployment in datacenter environments faces significant challenges due to the massive memory requirements of key-value (KV) caches. During the autoregressive decoding process, KV caches consume substantial GPU memory, limiting batch sizes and overall system throughput. To address these challenges, we propose \\textbf{CXL-SpecKV}, a novel disaggregated KV-cache architecture that leverages Compute Express Link (CXL) interconnects and FPGA accelerators to enable efficient speculative execution and memory disaggregation. Our approach introduces three key innovations: (i) a CXL-based memory disaggregation framework that offloads KV-caches to remote FPGA memory with low latency, (ii) a speculative KV-cache prefetching mechanism that predicts and preloads future tokens' cache entries, and (iii) an FPGA-accelerated KV-cache compression and decompression engine that reduces memory bandwidth requirements by up to 4$\\times$. When evaluated on state-of-the-art LLM models, CXL-SpecKV achieves up to 3.2$\\times$ higher throughput compared to GPU-only baselines, while reducing memory costs by 2.8$\\times$ and maintaining accuracy. Our system demonstrates that intelligent memory disaggregation combined with speculative execution can effectively address the memory wall challenge in large-scale LLM serving. Our code implementation has been open-sourced at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 8,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11935",
        "abs_url": "https://arxiv.org/abs/2512.11935",
        "pdf_url": "https://arxiv.org/pdf/2512.11935",
        "title": "AGAPI-Agents: An Open-Access Agentic AI Platform for Accelerated Materials Design on AtomGPT.org",
        "authors": [
            "Jaehyung Lee",
            "Justin Ely",
            "Kent Zhang",
            "Akshaya Ajith",
            "Charles Rhys Campbell",
            "Kamal Choudhary"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Materials Science (cond-mat.mtrl-sci)",
        "abstract": "Artificial intelligence is reshaping scientific discovery, yet its use in materials research remains limited by fragmented computational ecosystems, reproducibility challenges, and dependence on commercial large language models (LLMs). Here we introduce AGAPI (this http URL API), an open-access agentic AI platform that integrates more than eight open-source LLMs with over twenty materials-science API endpoints, unifying databases, simulation tools, and machine-learning models through a common orchestration framework. AGAPI employs an Agent-Planner-Executor-Summarizer architecture that autonomously constructs and executes multi-step workflows spanning materials data retrieval, graph neural network property prediction, machine-learning force-field optimization, tight-binding calculations, diffraction analysis, and inverse design. We demonstrate AGAPI through end-to-end workflows, including heterostructure construction, powder X-ray diffraction analysis, and semiconductor defect engineering requiring up to ten sequential operations. In addition, we evaluate AGAPI using 30+ example prompts as test cases and compare agentic predictions with and without tool access against experimental data. With more than 1,000 active users, AGAPI provides a scalable and transparent foundation for reproducible, AI-accelerated materials discovery. AGAPI-Agents codebase is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 9,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11942",
        "abs_url": "https://arxiv.org/abs/2512.11942",
        "pdf_url": "https://arxiv.org/pdf/2512.11942",
        "title": "Hypergame Rationalisability: Solving Agent Misalignment In Strategic Play",
        "authors": [
            "Vince Trencsenyi"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Formal Languages and Automata Theory (cs.FL); Computer Science and Game Theory (cs.GT)",
        "abstract": "Differences in perception, information asymmetries, and bounded rationality lead game-theoretic players to derive a private, subjective view of the game that may diverge from the underlying ground-truth scenario and may be misaligned with other players' interpretations. While typical game-theoretic assumptions often overlook such heterogeneity, hypergame theory provides the mathematical framework to reason about mismatched mental models. Although hypergames have recently gained traction in dynamic applications concerning uncertainty, their practical adoption in multi-agent system research has been hindered by the lack of a unifying, formal, and practical representation language, as well as scalable algorithms for managing complex hypergame structures and equilibria. Our work addresses this gap by introducing a declarative, logic-based domain-specific language for encoding hypergame structures and hypergame solution concepts. Leveraging answer-set programming, we develop an automated pipeline for instantiating hypergame structures and running our novel hypergame rationalisation procedure, a mechanism for finding belief structures that justify seemingly irrational outcomes. The proposed language establishes a unifying formalism for hypergames and serves as a foundation for developing nuanced, belief-based heterogeneous reasoners, offering a verifiable context with logical guarantees. Together, these contributions establish the connection between hypergame theory, multi-agent systems, and strategic AI.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 10,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11997",
        "abs_url": "https://arxiv.org/abs/2512.11997",
        "pdf_url": "https://arxiv.org/pdf/2512.11997",
        "title": "Log Anomaly Detection with Large Language Models via Knowledge-Enriched Fusion",
        "authors": [
            "Anfeng Peng",
            "Ajesh Koyatan Chathoth",
            "Stephen Lee"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "System logs are a critical resource for monitoring and managing distributed systems, providing insights into failures and anomalous behavior. Traditional log analysis techniques, including template-based and sequence-driven approaches, often lose important semantic information or struggle with ambiguous log patterns. To address this, we present EnrichLog, a training-free, entry-based anomaly detection framework that enriches raw log entries with both corpus-specific and sample-specific knowledge. EnrichLog incorporates contextual information, including historical examples and reasoning derived from the corpus, to enable more accurate and interpretable anomaly detection. The framework leverages retrieval-augmented generation to integrate relevant contextual knowledge without requiring retraining. We evaluate EnrichLog on four large-scale system log benchmark datasets and compare it against five baseline methods. Our results show that EnrichLog consistently improves anomaly detection performance, effectively handles ambiguous log entries, and maintains efficient inference. Furthermore, incorporating both corpus- and sample-specific knowledge enhances model confidence and detection accuracy, making EnrichLog well-suited for practical deployments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 11,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12048",
        "abs_url": "https://arxiv.org/abs/2512.12048",
        "pdf_url": "https://arxiv.org/pdf/2512.12048",
        "title": "Context-Aware Agentic Power Resources Optimisation in EV using Smart2ChargeApp",
        "authors": [
            "Muddsair Sharif",
            "Huseyin Seker"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "This paper presents a novel context-sensitive multi\\-agent coordination for dynamic resource allocation (CAMAC-DRA) framework for optimizing smart electric vehicle (EV) charging ecosystems through the Smart2Charge application. The proposed system coordinates autonomous charging agents across networks of 250 EVs and 45 charging stations while adapting to dynamic environmental conditions through context-aware decision-making. Our multi-agent approach employs coordinated Deep Q\\-Networks integrated with Graph Neural Networks and attention mechanisms, processing 20 contextual features including weather patterns, traffic conditions, grid load fluctuations, and electricity this http URL framework balances five ecosystem stakeholders i.e. EV users (25\\%), grid operators (20\\%), charging station operators (20\\%), fleet operators (20%), and environmental factors (15\\%) through weighted coordination mechanisms and consensus protocols. Comprehensive validation using real-world datasets containing 441,077 charging transactions demonstrates superior performance compared to baseline algorithms including DDPG, A3C, PPO, and GNN approaches. The CAMAC\\-DRA framework achieves 92\\% coordination success rate, 15\\% energy efficiency improvement, 10\\% cost reduction, 20% grid strain decrease, and \\2.3x faster convergence while maintaining 88\\% training stability and 85\\% sample efficiency. Real-world validation confirms commercial viability with Net Present Cost of -\\$122,962 and 69\\% cost reduction through renewable energy integration. The framework's unique contribution lies in developing context-aware multi-stakeholder coordination that successfully balances competing objectives while adapting to real-time variables, positioning it as a breakthrough solution for intelligent EV charging coordination and sustainable transportation electrification.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 12,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12059",
        "abs_url": "https://arxiv.org/abs/2512.12059",
        "pdf_url": "https://arxiv.org/pdf/2512.12059",
        "title": "The Forecast Critic: Leveraging Large Language Models for Poor Forecast Identification",
        "authors": [
            "Luke Bhan",
            "Hanyu Zhang",
            "Andrew Gordon Wilson",
            "Michael W. Mahoney",
            "Chuck Arvin"
        ],
        "comments": "Presented at AAAI 2026 AI4TS workshop and AABA4ET workshop",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Monitoring forecasting systems is critical for customer satisfaction, profitability, and operational efficiency in large-scale retail businesses. We propose The Forecast Critic, a system that leverages Large Language Models (LLMs) for automated forecast monitoring, taking advantage of their broad world knowledge and strong ``reasoning'' capabilities. As a prerequisite for this, we systematically evaluate the ability of LLMs to assess time series forecast quality, focusing on three key questions. (1) Can LLMs be deployed to perform forecast monitoring and identify obviously unreasonable forecasts? (2) Can LLMs effectively incorporate unstructured exogenous features to assess what a reasonable forecast looks like? (3) How does performance vary across model sizes and reasoning capabilities, measured across state-of-the-art LLMs? We present three experiments, including on both synthetic and real-world forecasting data. Our results show that LLMs can reliably detect and critique poor forecasts, such as those plagued by temporal misalignment, trend inconsistencies, and spike errors. The best-performing model we evaluated achieves an F1 score of 0.88, somewhat below human-level performance (F1 score: 0.97). We also demonstrate that multi-modal LLMs can effectively incorporate unstructured contextual signals to refine their assessment of the forecast. Models correctly identify missing or spurious promotional spikes when provided with historical context about past promotions (F1 score: 0.84). Lastly, we demonstrate that these techniques succeed in identifying inaccurate forecasts on the real-world M5 time series dataset, with unreasonable forecasts having an sCRPS at least 10% higher than that of reasonable forecasts. These findings suggest that LLMs, even without domain-specific fine-tuning, may provide a viable and scalable option for automated forecast monitoring and evaluation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 13,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12088",
        "abs_url": "https://arxiv.org/abs/2512.12088",
        "pdf_url": "https://arxiv.org/pdf/2512.12088",
        "title": "Reliable Policy Iteration: Performance Robustness Across Architecture and Environment Perturbations",
        "authors": [
            "S.R. Eshwar",
            "Aniruddha Mukherjee",
            "Kintan Saha",
            "Krishna Agarwal",
            "Gugan Thoppe",
            "Aditya Gopalan",
            "Gal Dalal"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "In a recent work, we proposed Reliable Policy Iteration (RPI), that restores policy iteration's monotonicity-of-value-estimates property to the function approximation setting. Here, we assess the robustness of RPI's empirical performance on two classical control tasks -- CartPole and Inverted Pendulum -- under changes to neural network and environmental parameters. Relative to DQN, Double DQN, DDPG, TD3, and PPO, RPI reaches near-optimal performance early and sustains this policy as training proceeds. Because deep RL methods are often hampered by sample inefficiency, training instability, and hyperparameter sensitivity, our results highlight RPI's promise as a more reliable alternative.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 14,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12175",
        "abs_url": "https://arxiv.org/abs/2512.12175",
        "pdf_url": "https://arxiv.org/pdf/2512.12175",
        "title": "Rethinking Label Consistency of In-Context Learning: An Implicit Transductive Label Propagation Perspective",
        "authors": [
            "Haoyang Chen",
            "Richong Zhang",
            "Junfan Chen"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Large language models (LLMs) perform in-context learning (ICL) with minimal supervised examples, which benefits various natural language processing (NLP) tasks. One of the critical research focus is the selection of prompt demonstrations. Current approaches typically employ retrieval models to select the top-K most semantically similar examples as demonstrations. However, we argue that existing methods are limited since the label consistency is not guaranteed during demonstration selection. Our cognition derives from the Bayesian view of ICL and our rethinking of ICL from the transductive label propagation perspective. We treat ICL as a transductive learning method and incorporate latent concepts from Bayesian view and deduce that similar demonstrations guide the concepts of query, with consistent labels serving as estimates. Based on this understanding, we establish a label propagation framework to link label consistency with propagation error bounds. To model label consistency, we propose a data synthesis method, leveraging both semantic and label information, and use TopK sampling with Synthetic Data (TopK-SD) to acquire demonstrations with consistent labels. TopK-SD outperforms original TopK sampling on multiple benchmarks. Our work provides a new perspective for understanding the working mechanisms within ICL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 15,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12177",
        "abs_url": "https://arxiv.org/abs/2512.12177",
        "pdf_url": "https://arxiv.org/pdf/2512.12177",
        "title": "Floorplan2Guide: LLM-Guided Floorplan Parsing for BLV Indoor Navigation",
        "authors": [
            "Aydin Ayanzadeh",
            "Tim Oates"
        ],
        "comments": "Accepted for publication in the proceedings of the IEEE International Conference on Big Data (IEEE BigData 2025)",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Indoor navigation remains a critical challenge for people with visual impairments. The current solutions mainly rely on infrastructure-based systems, which limit their ability to navigate safely in dynamic environments. We propose a novel navigation approach that utilizes a foundation model to transform floor plans into navigable knowledge graphs and generate human-readable navigation instructions. Floorplan2Guide integrates a large language model (LLM) to extract spatial information from architectural layouts, reducing the manual preprocessing required by earlier floorplan parsing methods. Experimental results indicate that few-shot learning improves navigation accuracy in comparison to zero-shot learning on simulated and real-world evaluations. Claude 3.7 Sonnet achieves the highest accuracy among the evaluated models, with 92.31%, 76.92%, and 61.54% on the short, medium, and long routes, respectively, under 5-shot prompting of the MP-1 floor plan. The success rate of graph-based spatial structure is 15.4% higher than that of direct visual reasoning among all models, which confirms that graphical representation and in-context learning enhance navigation performance and make our solution more precise for indoor navigation of Blind and Low Vision (BLV) users.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 16,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12182",
        "abs_url": "https://arxiv.org/abs/2512.12182",
        "pdf_url": "https://arxiv.org/pdf/2512.12182",
        "title": "TA-KAND: Two-stage Attention Triple Enhancement and U-KAN based Diffusion For Few-shot Knowledge Graph Completion",
        "authors": [
            "Xinyu Gao"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Knowledge Graphs (KGs), thanks to their concise and efficient triple-based structure, have been widely applied in intelligent question answering, recommender systems and other domains. However, the heterogeneous and multifaceted nature of real-world data inevitably renders the distribution of relations long-tailed, making it crucial to complete missing facts with limited samples. Previous studies mainly based on metric matching or meta learning, yet they either fail to fully exploit neighborhood information in graph or overlook the distributional characteristics of contrastive signals. In this paper, we re-examine the problem from a perspective of generative representation and propose a few-shot knowledge graph completion framework that integrates two-stage attention triple enhancer with U-KAN based diffusion model. Extensive experiments on two public datasets show that our method achieve new state-of-the-art results.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 17,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12225",
        "abs_url": "https://arxiv.org/abs/2512.12225",
        "pdf_url": "https://arxiv.org/pdf/2512.12225",
        "title": "A Geometric Theory of Cognition",
        "authors": [
            "Laha Ale"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Human cognition spans perception, memory, intuitive judgment, deliberative reasoning, action selection, and social inference, yet these capacities are often explained through distinct computational theories. Here we present a unified mathematical framework in which diverse cognitive processes emerge from a single geometric principle. We represent the cognitive state as a point on a differentiable manifold endowed with a learned Riemannian metric that encodes representational constraints, computational costs, and structural relations among cognitive variables. A scalar cognitive potential combines predictive accuracy, structural parsimony, task utility, and normative or logical requirements. Cognition unfolds as the Riemannian gradient flow of this potential, providing a universal dynamical law from which a broad range of psychological phenomena arise. Classical dual-process effects--rapid intuitive responses and slower deliberative reasoning--emerge naturally from metric-induced anisotropies that generate intrinsic time-scale separations and geometric phase transitions, without invoking modular or hybrid architectures. We derive analytical conditions for these regimes and demonstrate their behavioural signatures through simulations of canonical cognitive tasks. Together, these results establish a geometric foundation for cognition and suggest guiding principles for the development of more general and human-like artificial intelligence systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 18,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12260",
        "abs_url": "https://arxiv.org/abs/2512.12260",
        "pdf_url": "https://arxiv.org/pdf/2512.12260",
        "title": "A Multi-Axial Mindset for Ontology Design Lessons from Wikidata's Polyhierarchical Structure",
        "authors": [
            "Ege Atacan Doğan",
            "Peter F. Patel-Schneider"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Databases (cs.DB)",
        "abstract": "Traditional ontology design emphasizes disjoint and exhaustive top-level distinctions such as continuant vs. occurrent, abstract vs. concrete, or type vs. instance. These distinctions are used to structure unified hierarchies where every entity is classified under a single upper-level category. Wikidata, by contrast, does not enforce a singular foundational taxonomy. Instead, it accommodates multiple classification axes simultaneously under the shared root class entity. This paper analyzes the structural implications of Wikidata's polyhierarchical and multi-axial design. The Wikidata architecture enables a scalable and modular approach to ontology construction, especially suited to collaborative and evolving knowledge graphs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 19,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12288",
        "abs_url": "https://arxiv.org/abs/2512.12288",
        "pdf_url": "https://arxiv.org/pdf/2512.12288",
        "title": "Quantum-Aware Generative AI for Materials Discovery: A Framework for Robust Exploration Beyond DFT Biases",
        "authors": [
            "Mahule Roy",
            "Guillaume Lambard"
        ],
        "comments": "33 pages",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Conventional generative models for materials discovery are predominantly trained and validated using data from Density Functional Theory (DFT) with approximate exchange-correlation functionals. This creates a fundamental bottleneck: these models inherit DFT's systematic failures for strongly correlated systems, leading to exploration biases and an inability to discover materials where DFT predictions are qualitatively incorrect. We introduce a quantum-aware generative AI framework that systematically addresses this limitation through tight integration of multi-fidelity learning and active validation. Our approach employs a diffusion-based generator conditioned on quantum-mechanical descriptors and a validator using an equivariant neural network potential trained on a hierarchical dataset spanning multiple levels of theory (PBE, SCAN, HSE06, CCSD(T)). Crucially, we implement a robust active learning loop that quantifies and targets the divergence between low- and high-fidelity predictions. We conduct comprehensive ablation studies to deconstruct the contribution of each component, perform detailed failure mode analysis, and benchmark our framework against state-of-the-art generative models (CDVAE, GNoME, DiffCSP) across several challenging material classes. Our results demonstrate significant practical gains: a 3-5x improvement in successfully identifying potentially stable candidates in high-divergence regions (e.g., correlated oxides) compared to DFT-only baselines, while maintaining computational feasibility. This work provides a rigorous, transparent framework for extending the effective search space of computational materials discovery beyond the limitations of single-fidelity models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 20,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12381",
        "abs_url": "https://arxiv.org/abs/2512.12381",
        "pdf_url": "https://arxiv.org/pdf/2512.12381",
        "title": "Entropy Collapse: A Universal Failure Mode of Intelligent Systems",
        "authors": [
            "Truong Xuan Khanh",
            "Truong Quynh Hoa"
        ],
        "comments": "18 pages, 5 figures",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Intelligent systems are widely assumed to improve through learning, coordination, and optimization. However, across domains -- from artificial intelligence to economic institutions and biological evolution -- increasing intelligence often precipitates paradoxical degradation: systems become rigid, lose adaptability, and fail unexpectedly. We identify \\emph{entropy collapse} as a universal dynamical failure mode arising when feedback amplification outpaces bounded novelty regeneration. Under minimal domain-agnostic assumptions, we show that intelligent systems undergo a sharp transition from high-entropy adaptive regimes to low-entropy collapsed regimes. Collapse is formalized as convergence toward a stable low-entropy manifold, not a zero-entropy state, implying a contraction of effective adaptive dimensionality rather than loss of activity or scale. We analytically establish critical thresholds, dynamical irreversibility, and attractor structure and demonstrate universality across update mechanisms through minimal simulations. This framework unifies diverse phenomena -- model collapse in AI, institutional sclerosis in economics, and genetic bottlenecks in evolution -- as manifestations of the same underlying process. By reframing collapse as a structural cost of intelligence, our results clarify why late-stage interventions systematically fail and motivate entropy-aware design principles for sustaining long-term adaptability in intelligent systems. \\noindent\\textbf{Keywords:} entropy collapse; intelligent systems; feedback amplification; phase transitions; effective dimensionality; complex systems; model collapse; institutional sclerosis",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 21,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12411",
        "abs_url": "https://arxiv.org/abs/2512.12411",
        "pdf_url": "https://arxiv.org/pdf/2512.12411",
        "title": "Feeling the Strength but Not the Source: Partial Introspection in LLMs",
        "authors": [
            "Ely Hahami",
            "Lavik Jain",
            "Ishaan Sinha"
        ],
        "comments": "7 pages (+ 5 pages for appendix), 5 figures, 1 table",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Recent work from Anthropic claims that frontier models can sometimes detect and name injected \"concepts\" represented as activation directions. We test the robustness of these claims. First, we reproduce Anthropic's multi-turn \"emergent introspection\" result on Meta-Llama-3.1-8B-Instruct, finding that the model identifies and names the injected concept 20 percent of the time under Anthropic's original pipeline, exactly matching their reported numbers and thus showing that introspection is not exclusive to very large or capable models. Second, we systematically vary the inference prompt and find that introspection is fragile: performance collapses on closely related tasks such as multiple-choice identification of the injected concept or different prompts of binary discrimination of whether a concept was injected at all. Third, we identify a contrasting regime of partial introspection: the same model can reliably classify the strength of the coefficient of a normalized injected concept vector (as weak / moderate / strong / very strong) with up to 70 percent accuracy, far above the 25 percent chance baseline. Together, these results provide more evidence for Anthropic's claim that language models effectively compute a function of their baseline, internal representations during introspection; however, these self-reports about those representations are narrow and prompt-sensitive. Our code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 22,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12413",
        "abs_url": "https://arxiv.org/abs/2512.12413",
        "pdf_url": "https://arxiv.org/pdf/2512.12413",
        "title": "Understanding Critical Thinking in Generative Artificial Intelligence Use: Development, Validation, and Correlates of the Critical Thinking in AI Use Scale",
        "authors": [
            "Gabriel R. Lau",
            "Wei Yan Low",
            "Louis Tay",
            "Ysabel Guevarra",
            "Dragan Gašević",
            "Andree Hartanto"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)",
        "abstract": "Generative AI tools are increasingly embedded in everyday work and learning, yet their fluency, opacity, and propensity to hallucinate mean that users must critically evaluate AI outputs rather than accept them at face value. The present research conceptualises critical thinking in AI use as a dispositional tendency to verify the source and content of AI-generated information, to understand how models work and where they fail, and to reflect on the broader implications of relying on AI. Across six studies (N = 1365), we developed and validated the 13-item critical thinking in AI use scale and mapped its nomological network. Study 1 generated and content-validated scale items. Study 2 supported a three-factor structure (Verification, Motivation, and Reflection). Studies 3, 4, and 5 confirmed this higher-order model, demonstrated internal consistency and test-retest reliability, strong factor loadings, sex invariance, and convergent and discriminant validity. Studies 3 and 4 further revealed that critical thinking in AI use was positively associated with openness, extraversion, positive trait affect, and frequency of AI use. Lastly, Study 6 demonstrated criterion validity of the scale, with higher critical thinking in AI use scores predicting more frequent and diverse verification strategies, greater veracity-judgement accuracy in a novel and naturalistic ChatGPT-powered fact-checking task, and deeper reflection about responsible AI. Taken together, the current work clarifies why and how people exercise oversight over generative AI outputs and provides a validated scale and ecologically grounded task paradigm to support theory testing, cross-group, and longitudinal research on critical engagement with generative AI outputs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 23,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12443",
        "abs_url": "https://arxiv.org/abs/2512.12443",
        "pdf_url": "https://arxiv.org/pdf/2512.12443",
        "title": "AI Transparency Atlas: Framework, Scoring, and Real-Time Model Card Evaluation Pipeline",
        "authors": [
            "Akhmadillo Mamirov",
            "Faiaz Azmain",
            "Hanyu Wang"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Software Engineering (cs.SE)",
        "abstract": "AI model documentation is fragmented across platforms and inconsistent in structure, preventing policymakers, auditors, and users from reliably assessing safety claims, data provenance, and version-level changes. We analyzed documentation from five frontier models (Gemini 3, Grok 4.1, Llama 4, GPT-5, and Claude 4.5) and 100 Hugging Face model cards, identifying 947 unique section names with extreme naming variation. Usage information alone appeared under 97 distinct labels. Using the EU AI Act Annex IV and the Stanford Transparency Index as baselines, we developed a weighted transparency framework with 8 sections and 23 subsections that prioritizes safety-critical disclosures (Safety Evaluation: 25%, Critical Risk: 20%) over technical specifications. We implemented an automated multi-agent pipeline that extracts documentation from public sources and scores completeness through LLM-based consensus. Evaluating 50 models across vision, multimodal, open-source, and closed-source systems cost less than $3 in total and revealed systematic gaps. Frontier labs (xAI, Microsoft, Anthropic) achieve approximately 80% compliance, while most providers fall below 60%. Safety-critical categories show the largest deficits: deception behaviors, hallucinations, and child safety evaluations account for 148, 124, and 116 aggregate points lost, respectively, across all evaluated models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 24,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12477",
        "abs_url": "https://arxiv.org/abs/2512.12477",
        "pdf_url": "https://arxiv.org/pdf/2512.12477",
        "title": "MetaHGNIE: Meta-Path Induced Hypergraph Contrastive Learning in Heterogeneous Knowledge Graphs",
        "authors": [
            "Jiawen Chen",
            "Yanyan He",
            "Qi Shao",
            "Mengli Wei",
            "Duxin Chen",
            "Wenwu Yu",
            "Yanlong Zhao"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Node importance estimation (NIE) in heterogeneous knowledge graphs is a critical yet challenging task, essential for applications such as recommendation, knowledge reasoning, and question answering. Existing methods often rely on pairwise connections, neglecting high-order dependencies among multiple entities and relations, and they treat structural and semantic signals independently, hindering effective cross-modal integration. To address these challenges, we propose MetaHGNIE, a meta-path induced hypergraph contrastive learning framework for disentangling and aligning structural and semantic information. MetaHGNIE constructs a higher-order knowledge graph via meta-path sequences, where typed hyperedges capture multi-entity relational contexts. Structural dependencies are aggregated with local attention, while semantic representations are encoded through a hypergraph transformer equipped with sparse chunking to reduce redundancy. Finally, a multimodal fusion module integrates structural and semantic embeddings under contrastive learning with auxiliary supervision, ensuring robust cross-modal alignment. Extensive experiments on benchmark NIE datasets demonstrate that MetaHGNIE consistently outperforms state-of-the-art baselines. These results highlight the effectiveness of explicitly modeling higher-order interactions and cross-modal alignment in heterogeneous knowledge graphs. Our code is available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 25,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12501",
        "abs_url": "https://arxiv.org/abs/2512.12501",
        "pdf_url": "https://arxiv.org/pdf/2512.12501",
        "title": "SafeGen: Embedding Ethical Safeguards in Text-to-Image Generation",
        "authors": [
            "Dang Phuong Nam",
            "Nguyen Kieu",
            "Pham Thanh Hieu"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Generative Artificial Intelligence (AI) has created unprecedented opportunities for creative expression, education, and research. Text-to-image systems such as DALL.E, Stable Diffusion, and Midjourney can now convert ideas into visuals within seconds, but they also present a dual-use dilemma, raising critical ethical concerns: amplifying societal biases, producing high-fidelity disinformation, and violating intellectual property. This paper introduces SafeGen, a framework that embeds ethical safeguards directly into the text-to-image generation pipeline, grounding its design in established principles for Trustworthy AI. SafeGen integrates two complementary components: BGE-M3, a fine-tuned text classifier that filters harmful or misleading prompts, and Hyper-SD, an optimized diffusion model that produces high fidelity, semantically aligned images. Built on a curated multilingual (English- Vietnamese) dataset and a fairness-aware training process, SafeGen demonstrates that creative freedom and ethical responsibility can be reconciled within a single workflow. Quantitative evaluations confirm its effectiveness, with Hyper-SD achieving IS = 3.52, FID = 22.08, and SSIM = 0.79, while BGE-M3 reaches an F1-Score of 0.81. An ablation study further validates the importance of domain-specific fine-tuning for both modules. Case studies illustrate SafeGen's practical impact in blocking unsafe prompts, generating inclusive teaching materials, and reinforcing academic integrity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 26,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12503",
        "abs_url": "https://arxiv.org/abs/2512.12503",
        "pdf_url": "https://arxiv.org/pdf/2512.12503",
        "title": "KidsArtBench: Multi-Dimensional Children's Art Evaluation with Attribute-Aware MLLMs",
        "authors": [
            "Mingrui Ye",
            "Chanjin Zheng",
            "Zengyi Yu",
            "Chenyu Xiang",
            "Zhixue Zhao",
            "Zheng Yuan",
            "Helen Yannakoudakis"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Multimodal Large Language Models (MLLMs) show remarkable progress across many visual-language tasks; however, their capacity to evaluate artistic expression remains limited. Aesthetic concepts are inherently abstract and open-ended, and multimodal artwork annotations are scarce. We introduce KidsArtBench, a new benchmark of over 1k children's artworks (ages 5-15) annotated by 12 expert educators across 9 rubric-aligned dimensions, together with expert comments for feedback. Unlike prior aesthetic datasets that provide single scalar scores on adult imagery, KidsArtBench targets children's artwork and pairs multi-dimensional annotations with comment supervision to enable both ordinal assessment and formative feedback. Building on this resource, we propose an attribute-specific multi-LoRA approach, where each attribute corresponds to a distinct evaluation dimension (e.g., Realism, Imagination) in the scoring rubric, with Regression-Aware Fine-Tuning (RAFT) to align predictions with ordinal scales. On Qwen2.5-VL-7B, our method increases correlation from 0.468 to 0.653, with the largest gains on perceptual dimensions and narrowed gaps on higher-order attributes. These results show that educator-aligned supervision and attribute-aware training yield pedagogically meaningful evaluations and establish a rigorous testbed for sustained progress in educational AI. We release data and code with ethics documentation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 27,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12548",
        "abs_url": "https://arxiv.org/abs/2512.12548",
        "pdf_url": "https://arxiv.org/pdf/2512.12548",
        "title": "World Models Unlock Optimal Foraging Strategies in Reinforcement Learning Agents",
        "authors": [
            "Yesid Fonseca",
            "Manuel S. Ríos",
            "Nicanor Quijano",
            "Luis F. Giraldo"
        ],
        "comments": "14 pages, 6 figures",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Patch foraging involves the deliberate and planned process of determining the optimal time to depart from a resource-rich region and investigate potentially more beneficial alternatives. The Marginal Value Theorem (MVT) is frequently used to characterize this process, offering an optimality model for such foraging behaviors. Although this model has been widely used to make predictions in behavioral ecology, discovering the computational mechanisms that facilitate the emergence of optimal patch-foraging decisions in biological foragers remains under investigation. Here, we show that artificial foragers equipped with learned world models naturally converge to MVT-aligned strategies. Using a model-based reinforcement learning agent that acquires a parsimonious predictive representation of its environment, we demonstrate that anticipatory capabilities, rather than reward maximization alone, drive efficient patch-leaving behavior. Compared with standard model-free RL agents, these model-based agents exhibit decision patterns similar to many of their biological counterparts, suggesting that predictive world models can serve as a foundation for more explainable and biologically grounded decision-making in AI systems. Overall, our findings highlight the value of ecological optimality principles for advancing interpretable and adaptive AI.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 28,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12552",
        "abs_url": "https://arxiv.org/abs/2512.12552",
        "pdf_url": "https://arxiv.org/pdf/2512.12552",
        "title": "Large Language Newsvendor: Decision Biases and Cognitive Mechanisms",
        "authors": [
            "Jifei Liu",
            "Zhi Chen",
            "Yuanguang Zhong"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Problem definition: Although large language models (LLMs) are increasingly integrated into business decision making, their potential to replicate and even amplify human cognitive biases cautions a significant, yet not well-understood, risk. This is particularly critical in high-stakes operational contexts like supply chain management. To address this, we investigate the decision-making patterns of leading LLMs using the canonical newsvendor problem in a dynamic setting, aiming to identify the nature and origins of their cognitive biases. Methodology/results: Through dynamic, multi-round experiments with GPT-4, GPT-4o, and LLaMA-8B, we tested for five established decision biases. We found that LLMs consistently replicated the classic ``Too Low/Too High'' ordering bias and significantly amplified other tendencies like demand-chasing behavior compared to human benchmarks. Our analysis uncovered a ``paradox of intelligence'': the more sophisticated GPT-4 demonstrated the greatest irrationality through overthinking, while the efficiency-optimized GPT-4o performed near-optimally. Because these biases persist even when optimal formulas are provided, we conclude they stem from architectural constraints rather than knowledge gaps. Managerial implications: First, managers should select models based on the specific task, as our results show that efficiency-optimized models can outperform more complex ones on certain optimization problems. Second, the significant amplification of bias by LLMs highlights the urgent need for robust human-in-the-loop oversight in high-stakes decisions to prevent costly errors. Third, our findings suggest that designing structured, rule-based prompts is a practical and effective strategy for managers to constrain models' heuristic tendencies and improve the reliability of AI-assisted decisions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 29,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12597",
        "abs_url": "https://arxiv.org/abs/2512.12597",
        "pdf_url": "https://arxiv.org/pdf/2512.12597",
        "title": "AgentSHAP: Interpreting LLM Agent Tool Importance with Monte Carlo Shapley Value Estimation",
        "authors": [
            "Miriam Horovicz"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "LLM agents that use external tools can solve complex tasks, but understanding which tools actually contributed to a response remains a blind spot. No existing XAI methods address tool-level explanations. We introduce AgentSHAP, the first framework for explaining tool importance in LLM agents. AgentSHAP is model-agnostic: it treats the agent as a black box and works with any LLM (GPT, Claude, Llama, etc.) without needing access to internal weights or gradients. Using Monte Carlo Shapley values, AgentSHAP tests how an agent responds with different tool subsets and computes fair importance scores based on game theory. Our contributions are: (1) the first explainability method for agent tool attribution, grounded in Shapley values from game theory; (2) Monte Carlo sampling that reduces cost from O(2n) to practical levels; and (3) comprehensive experiments on API-Bank showing that AgentSHAP produces consistent scores across runs, correctly identifies which tools matter, and distinguishes relevant from irrelevant tools. AgentSHAP joins TokenSHAP (for tokens) and PixelSHAP (for image regions) to complete a family of Shapley-based XAI tools for modern generative AI. Code: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 30,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12634",
        "abs_url": "https://arxiv.org/abs/2512.12634",
        "pdf_url": "https://arxiv.org/pdf/2512.12634",
        "title": "Modular and Multi-Path-Aware Offline Benchmarking for Mobile GUI Agents",
        "authors": [
            "Youngmin Im",
            "Byeongung Jo",
            "Jaeyoung Wi",
            "Seungwoo Baek",
            "Tae Hoon Min",
            "Joo Hyung Lee",
            "Sangeun Oh",
            "Insik Shin",
            "Sunjae Lee"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Mobile GUI Agents, AI agents capable of interacting with mobile applications on behalf of users, have the potential to transform human computer interaction. However, current evaluation practices for GUI agents face two fundamental limitations. First, they either rely on single path offline benchmarks or online live benchmarks. Offline benchmarks using static, single path annotated datasets unfairly penalize valid alternative actions, while online benchmarks suffer from poor scalability and reproducibility due to the dynamic and unpredictable nature of live evaluation. Second, existing benchmarks treat agents as monolithic black boxes, overlooking the contributions of individual components, which often leads to unfair comparisons or obscures key performance bottlenecks. To address these limitations, we present MobiBench, the first modular and multi path aware offline benchmarking framework for mobile GUI agents that enables high fidelity, scalable, and reproducible evaluation entirely in offline settings. Our experiments demonstrate that MobiBench achieves 94.72 percent agreement with human evaluators, on par with carefully engineered online benchmarks, while preserving the scalability and reproducibility of static offline benchmarks. Furthermore, our comprehensive module level analysis uncovers several key insights, including a systematic evaluation of diverse techniques used in mobile GUI agents, optimal module configurations across model scales, the inherent limitations of current LFMs, and actionable guidelines for designing more capable and cost efficient mobile agents.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 31,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12652",
        "abs_url": "https://arxiv.org/abs/2512.12652",
        "pdf_url": "https://arxiv.org/pdf/2512.12652",
        "title": "Value-Aware Multiagent Systems",
        "authors": [
            "Nardine Osman"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)",
        "abstract": "This paper introduces the concept of value awareness in AI, which goes beyond the traditional value-alignment problem. Our definition of value awareness presents us with a concise and simplified roadmap for engineering value-aware AI. The roadmap is structured around three core pillars: (1) learning and representing human values using formal semantics, (2) ensuring the value alignment of both individual agents and multiagent systems, and (3) providing value-based explainability on behaviour. The paper presents a selection of our ongoing work on some of these topics, along with applications to real-life domains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 32,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12686",
        "abs_url": "https://arxiv.org/abs/2512.12686",
        "pdf_url": "https://arxiv.org/pdf/2512.12686",
        "title": "Memoria: A Scalable Agentic Memory Framework for Personalized Conversational AI",
        "authors": [
            "Samarth Sarin",
            "Lovepreet Singh",
            "Bhaskarjit Sarmah",
            "Dhagash Mehta"
        ],
        "comments": "Paper accepted at 5th International Conference of AIML Systems 2025, Bangalore, India",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Agentic memory is emerging as a key enabler for large language models (LLM) to maintain continuity, personalization, and long-term context in extended user interactions, critical capabilities for deploying LLMs as truly interactive and adaptive agents. Agentic memory refers to the memory that provides an LLM with agent-like persistence: the ability to retain and act upon information across conversations, similar to how a human would. We present Memoria, a modular memory framework that augments LLM-based conversational systems with persistent, interpretable, and context-rich memory. Memoria integrates two complementary components: dynamic session-level summarization and a weighted knowledge graph (KG)-based user modelling engine that incrementally captures user traits, preferences, and behavioral patterns as structured entities and relationships. This hybrid architecture enables both short-term dialogue coherence and long-term personalization while operating within the token constraints of modern LLMs. We demonstrate how Memoria enables scalable, personalized conversational artificial intelligence (AI) by bridging the gap between stateless LLM interfaces and agentic memory systems, offering a practical solution for industry applications requiring adaptive and evolving user experiences.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 33,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12692",
        "abs_url": "https://arxiv.org/abs/2512.12692",
        "pdf_url": "https://arxiv.org/pdf/2512.12692",
        "title": "WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment",
        "authors": [
            "Mahir Labib Dihan",
            "Tanzima Hashem",
            "Mohammed Eunus Ali",
            "Md Rizwan Parvez"
        ],
        "comments": "Under review at ICLR 2026. Project page: this https URL",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "LLM-based agents often operate in a greedy, step-by-step manner, selecting actions solely based on the current observation without considering long-term consequences or alternative paths. This lack of foresight is particularly problematic in web environments, which are only partially observable-limited to browser-visible content (e.g., DOM and UI elements)-where a single misstep often requires complex and brittle navigation to undo. Without an explicit backtracking mechanism, agents struggle to correct errors or systematically explore alternative paths. Tree-search methods provide a principled framework for such structured exploration, but existing approaches lack mechanisms for safe backtracking, making them prone to unintended side effects. They also assume that all actions are reversible, ignoring the presence of irreversible actions-limitations that reduce their effectiveness in realistic web tasks. To address these challenges, we introduce WebOperator, a tree-search framework that enables reliable backtracking and strategic exploration. Our method incorporates a best-first search strategy that ranks actions by both reward estimates and safety considerations, along with a robust backtracking mechanism that verifies the feasibility of previously visited paths before replaying them, preventing unintended side effects. To further guide exploration, WebOperator generates action candidates from multiple, varied reasoning contexts to ensure diverse and robust exploration, and subsequently curates a high-quality action set by filtering out invalid actions pre-execution and merging semantically equivalent ones. Experimental results on WebArena and WebVoyager demonstrate the effectiveness of WebOperator. On WebArena, WebOperator achieves a state-of-the-art 54.6% success rate with gpt-4o, underscoring the critical advantage of integrating strategic foresight with safe execution.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 34,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12706",
        "abs_url": "https://arxiv.org/abs/2512.12706",
        "pdf_url": "https://arxiv.org/pdf/2512.12706",
        "title": "Synergizing Code Coverage and Gameplay Intent: Coverage-Aware Game Playtesting with LLM-Guided Reinforcement Learning",
        "authors": [
            "Enhong Mu",
            "Minami Yoda",
            "Yan Zhang",
            "Mingyue Zhang",
            "Yutaka Matsuno",
            "Jialong Li"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Software Engineering (cs.SE)",
        "abstract": "The widespread adoption of the \"Games as a Service\" model necessitates frequent content updates, placing immense pressure on quality assurance. In response, automated game testing has been viewed as a promising solution to cope with this demanding release cadence. However, existing automated testing approaches typically create a dichotomy: code-centric methods focus on structural coverage without understanding gameplay context, while player-centric agents validate high-level intent but often fail to cover specific underlying code changes. To bridge this gap, we propose SMART (Structural Mapping for Augmented Reinforcement Testing), a novel framework that synergizes structural verification and functional validation for game update testing. SMART leverages large language models (LLMs) to interpret abstract syntax tree (AST) differences and extract functional intent, constructing a context-aware hybrid reward mechanism. This mechanism guides reinforcement learning agents to sequentially fulfill gameplay goals while adaptively exploring modified code branches. We evaluate SMART on two environments, Overcooked and Minecraft. The results demonstrate that SMART significantly outperforms state-of-the-art baselines; it achieves over 94% branch coverage of modified code, nearly double that of traditional reinforcement learning methods, while maintaining a 98% task completion rate, effectively balancing structural comprehensiveness with functional correctness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 35,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12736",
        "abs_url": "https://arxiv.org/abs/2512.12736",
        "pdf_url": "https://arxiv.org/pdf/2512.12736",
        "title": "Personalized QoE Prediction: A Demographic-Augmented Machine Learning Framework for 5G Video Streaming Networks",
        "authors": [
            "Syeda Zunaira Ahmed",
            "Hejab Tahira Beg",
            "Maryam Khalid"
        ],
        "comments": "11 pages, 5 figures",
        "subjects": "Artificial Intelligence (cs.AI); Multimedia (cs.MM); Image and Video Processing (eess.IV)",
        "abstract": "Quality of Experience (QoE) prediction is a critical component of modern multimedia systems, particularly for adaptive video streaming in 5G networks. Accurate QoE estimation enables intelligent resource management and supports user centric service delivery. Existing QoE prediction approaches primarily rely on limited datasets and assume uniform user perception, which restricts their applicability in heterogeneous real world environments. This paper proposes a demographic aware machine learning framework for personalized QoE prediction. We introduce a behaviorally realistic demographic based data augmentation strategy that expands a small QoE dataset six fold by modeling varying user sensitivities to streaming impairments such as rebuffering, bitrate variation, and quality degradation. Using the augmented dataset, we evaluate a comprehensive set of classical machine learning models alongside advanced deep learning architectures, including an attention-based MLP and TabNet. Experimental results demonstrate significant improvements in prediction accuracy across RMSE, MAE, and R metrics compared to baseline models. Among all evaluated approaches, TabNet achieves the strongest performance, benefiting from its inherent feature selection and attention mechanisms. The results confirm that demographic-aware augmentation substantially enhances QoE prediction robustness and provides a scalable direction for personalized QoE-aware intelligence in 5G video streaming networks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 36,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12804",
        "abs_url": "https://arxiv.org/abs/2512.12804",
        "pdf_url": "https://arxiv.org/pdf/2512.12804",
        "title": "Causal Counterfactuals Reconsidered",
        "authors": [
            "Sander Beckers"
        ],
        "comments": "Preprint: currently under review",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "I develop a novel semantics for probabilities of counterfactuals that generalizes the standard Pearlian semantics: it applies to probabilistic causal models that cannot be extended into realistic structural causal models and are therefore beyond the scope of Pearl's semantics. This generalization is needed because, as I show, such probabilistic causal models arise even in simple settings. My semantics offer a natural compromize in the long-standing debate between Pearl and Dawid over counterfactuals: I agree with Dawid that universal causal determinism and unrealistic variables should be rejected, but I agree with Pearl that a general semantics of counterfactuals is nonetheless possible. I restrict attention to causal models that satisfy the Markov condition, only contain realistic variables, and are causally complete. Although I formulate my proposal using structural causal models, as does Pearl, I refrain from using so-called response variables. Moreover, I prove that my semantics is equivalent to two other recent proposals that do not involve structural causal models, and that it is in line with various comments on stochastic counterfactuals that have appeared in the literature more broadly. Throughout I also reflect on the universality of the Markov condition and explore a novel generalization of causal abstractions",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 37,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12806",
        "abs_url": "https://arxiv.org/abs/2512.12806",
        "pdf_url": "https://arxiv.org/pdf/2512.12806",
        "title": "Fault-Tolerant Sandboxing for AI Coding Agents: A Transactional Approach to Safe Autonomous Execution",
        "authors": [
            "Boyang Yan"
        ],
        "comments": "7 pages",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "The transition of Large Language Models (LLMs) from passive code generators to autonomous agents introduces significant safety risks, specifically regarding destructive commands and inconsistent system states. Existing commercial solutions often prioritize interactive user safety, enforcing authentication barriers that break the headless loops required for true autonomy. This paper presents a Fault-Tolerant Sandboxing framework designed to mitigate these risks through a policy-based interception layer and a transactional filesystem snapshot mechanism. We hypothesize that wrapping agent actions in atomic transactions can guarantee safety with acceptable latency, outperforming the heavy initialization overhead of containers or the interactive friction of commercial CLIs. We validated this approach by deploying the Minimind-MoE LLM served via nano-vllm on a custom Proxmox-based testbed utilizing EVPN/VXLAN isolation. Experimental results demonstrate a 100\\% interception rate for high-risk commands and a 100\\% success rate in rolling back failed states. Crucially, our prototype incurs only a 14.5\\% performance overhead (approx. 1.8s) per transaction. In contrast, benchmarking against the Gemini CLI sandbox revealed that it requires interactive authentication (\"Sign in\"), rendering it unusable for headless, autonomous agent workflows.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 38,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12856",
        "abs_url": "https://arxiv.org/abs/2512.12856",
        "pdf_url": "https://arxiv.org/pdf/2512.12856",
        "title": "Forgetful but Faithful: A Cognitive Memory Architecture and Benchmark for Privacy-Aware Generative Agents",
        "authors": [
            "Saad Alqithami"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "As generative agents become increasingly sophisticated and deployed in long-term interactive scenarios, their memory management capabilities emerge as a critical bottleneck for both performance and privacy. Current approaches either maintain unlimited memory stores, leading to computational intractability and privacy concerns, or employ simplistic forgetting mechanisms that compromise agent coherence and functionality. This paper introduces the Memory-Aware Retention Schema (MaRS), a novel framework for human-centered memory management in generative agents, coupled with six theoretically-grounded forgetting policies that balance performance, privacy, and computational efficiency. We present the Forgetful but Faithful Agent (FiFA) benchmark, a comprehensive evaluation framework that assesses agent performance across narrative coherence, goal completion, social recall accuracy, privacy preservation, and cost efficiency. Through extensive experimentation involving 300 evaluation runs across multiple memory budgets and agent configurations, we demonstrate that our hybrid forgetting policy achieves superior performance (composite score: 0.911) while maintaining computational tractability and privacy guarantees. Our work establishes new benchmarks for memory-budgeted agent evaluation and provides practical guidelines for deploying generative agents in resource-constrained, privacy-sensitive environments. The theoretical foundations, implementation framework, and empirical results contribute to the emerging field of human-centered AI by addressing fundamental challenges in agent memory management that directly impact user trust, system scalability, and regulatory compliance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 39,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12918",
        "abs_url": "https://arxiv.org/abs/2512.12918",
        "pdf_url": "https://arxiv.org/pdf/2512.12918",
        "title": "Satisfiability Modulo Theory Meets Inductive Logic Programming",
        "authors": [
            "Nijesh Upreti",
            "Vaishak Belle"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Inductive Logic Programming (ILP) provides interpretable rule learning in relational domains, yet remains limited in its ability to induce and reason with numerical constraints. Classical ILP systems operate over discrete predicates and typically rely on discretisation or hand-crafted numerical predicates, making it difficult to infer thresholds or arithmetic relations that must hold jointly across examples. Recent work has begun to address these limitations through tighter integrations of ILP with Satisfiability Modulo Theories (SMT) or specialised numerical inference mechanisms. In this paper we investigate a modular alternative that couples the ILP system PyGol with the SMT solver Z3. Candidate clauses proposed by PyGol are interpreted as quantifier-free formulas over background theories such as linear or nonlinear real arithmetic, allowing numerical parameters to be instantiated and verified by the SMT solver while preserving ILP's declarative relational bias. This supports the induction of hybrid rules that combine symbolic predicates with learned numerical constraints, including thresholds, intervals, and multi-literal arithmetic relations. We formalise this SMT-ILP setting and evaluate it on a suite of synthetic datasets designed to probe linear, relational, nonlinear, and multi-hop reasoning. The results illustrate how a modular SMT-ILP architecture can extend the expressivity of symbolic rule learning, complementing prior numerical ILP approaches while providing a flexible basis for future extensions toward richer theory-aware induction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 40,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12970",
        "abs_url": "https://arxiv.org/abs/2512.12970",
        "pdf_url": "https://arxiv.org/pdf/2512.12970",
        "title": "Towards Open Standards for Systemic Complexity in Digital Forensics",
        "authors": [
            "Paola Di Maio"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computers and Society (cs.CY)",
        "abstract": "The intersection of artificial intelligence (AI) and digital forensics (DF) is becoming increasingly complex, ubiquitous, and pervasive, with overlapping techniques and technologies being adopted in all types of scientific and technical inquiry. Despite incredible advances, forensic sciences are not exempt from errors and remain vulnerable to fallibility. To mitigate the limitations of errors in DF, the systemic complexity is identified and addressed with the adoption of human-readable artifacts and open standards. A DF AI model schema based on the state of the art is outlined.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 41,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13070",
        "abs_url": "https://arxiv.org/abs/2512.13070",
        "pdf_url": "https://arxiv.org/pdf/2512.13070",
        "title": "M-GRPO: Stabilizing Self-Supervised Reinforcement Learning for Large Language Models with Momentum-Anchored Policy Optimization",
        "authors": [
            "Bizhe Bai",
            "Hongming Wu",
            "Peng Ye",
            "Tao Chen"
        ],
        "comments": "7 pages, 5 figures,Accepted NeurIPS 2025 Workshop on Efficient Reasoning",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Self-supervised reinforcement learning (RL) presents a promising approach for enhancing the reasoning capabilities of Large Language Models (LLMs) without reliance on expensive human-annotated data. However, we find that existing methods suffer from a critical failure mode under long-horizon training: a \"policy collapse\" where performance precipitously degrades. We diagnose this instability and demonstrate that simply scaling the number of rollouts -- a common strategy to improve performance -- only delays, but does not prevent, this collapse. To counteract this instability, we first introduce M-GRPO (Momentum-Anchored Group Relative Policy Optimization), a framework that leverages a slowly evolving momentum model to provide a stable training target. In addition, we identify that this process is often accompanied by a rapid collapse in policy entropy, resulting in a prematurely confident and suboptimal policy. To specifically address this issue, we propose a second contribution: an adaptive filtering method based on the interquartile range (IQR) that dynamically prunes low-entropy trajectories, preserving essential policy diversity. Our extensive experiments on multiple reasoning benchmarks demonstrate that M-GRPO stabilizes the training process while the IQR filter prevents premature convergence. The combination of these two innovations leads to superior training stability and state-of-the-art performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 42,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13102",
        "abs_url": "https://arxiv.org/abs/2512.13102",
        "pdf_url": "https://arxiv.org/pdf/2512.13102",
        "title": "Socratic Students: Teaching Language Models to Learn by Asking Questions",
        "authors": [
            "Rajeev Bhatt Ambati",
            "Tianyi Niu",
            "Aashu Singh",
            "Shlok Mishra",
            "Shashank Srivastava",
            "Snigdha Chaturvedi"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Large Language Models (LLMs) excel at static interactions, where they answer user queries by retrieving knowledge encoded in their parameters. However, in many real-world settings, such as educational tutoring or medical assistance, relevant information is not directly available and must be actively acquired through dynamic interactions. An interactive agent would recognize its own uncertainty, ask targeted questions, and retain new knowledge efficiently. Prior work has primarily explored effective ways for a teacher to instruct the student, where the teacher identifies student gaps and provides guidance. In this work, we shift the focus to the student and investigate effective strategies to actively query the teacher in seeking useful information. Across math and coding benchmarks, where baseline student models begin with near-zero performance, we show that student-led approaches consistently yield absolute Pass@k improvements of at least 0.5 over static baselines. To improve question quality, we train students using Direct Preference Optimization (DPO) with guidance from either self or stronger students. We find that this guided training enables smaller models to learn how to ask better questions, further enhancing learning efficiency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 43,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13142",
        "abs_url": "https://arxiv.org/abs/2512.13142",
        "pdf_url": "https://arxiv.org/pdf/2512.13142",
        "title": "Can AI Understand What We Cannot Say? Measuring Multilevel Alignment Through Abortion Stigma Across Cognitive, Interpersonal, and Structural Levels",
        "authors": [
            "Anika Sharma",
            "Malavika Mampally",
            "Chidaksh Ravuru",
            "Kandyce Brennan",
            "Neil Gaikwad"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)",
        "abstract": "As large language models increasingly mediate stigmatized health decisions, their capacity to genuinely understand complex psychological and physiological phenomena remains poorly evaluated. Can AI understand what we cannot say? We investigate whether LLMs coherently represent abortion stigma across the cognitive, interpersonal, and structural levels where it operates. We systematically tested 627 demographically diverse personas across five leading LLMs using the validated Individual Level Abortion Stigma Scale (ILAS). Our multilevel analysis examined whether models coherently represent stigma at the cognitive level (self-judgment), interpersonal level (anticipated judgment and isolation), and structural level (community condemnation and disclosure patterns), as well as overall stigma. Models fail tests of genuine understanding across all levels. They overestimate interpersonal stigma while underestimating cognitive stigma, assume uniform community condemnation, introduce demographic biases absent from human validation data, miss the empirically validated stigma-secrecy relationship, and contradict themselves within theoretical constructs. These patterns reveal that current alignment approaches ensure appropriate language but not coherent multilevel understanding. This work provides empirical evidence that current LLMs lack coherent multilevel understanding of psychological and physiological constructs. AI safety in high-stakes contexts demands new approaches to design (multilevel coherence), evaluation (continuous auditing), governance and regulation (mandatory audits, accountability, deployment restrictions), and AI literacy in domains where understanding what people cannot say determines whether support helps or harms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 44,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13154",
        "abs_url": "https://arxiv.org/abs/2512.13154",
        "pdf_url": "https://arxiv.org/pdf/2512.13154",
        "title": "MAC: A Multi-Agent Framework for Interactive User Clarification in Multi-turn Conversations",
        "authors": [
            "Emre Can Acikgoz",
            "Jinoh Oh",
            "Joo Hyuk Jeon",
            "Jie Hao",
            "Heng Ji",
            "Dilek Hakkani-Tür",
            "Gokhan Tur",
            "Xiang Li",
            "Chengyuan Ma",
            "Xing Fan"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Conversational agents often encounter ambiguous user requests, requiring an effective clarification to successfully complete tasks. While recent advancements in real-world applications favor multi-agent architectures to manage complex conversational scenarios efficiently, ambiguity resolution remains a critical and underexplored challenge--particularly due to the difficulty of determining which agent should initiate a clarification and how agents should coordinate their actions when faced with uncertain or incomplete user input. The fundamental questions of when to interrupt a user and how to formulate the optimal clarification query within the most optimal multi-agent settings remain open. In this paper, we propose MAC (Multi-Agent Clarification), an interactive multi-agent framework specifically optimized to resolve user ambiguities by strategically managing clarification dialogues. We first introduce a novel taxonomy categorizing user ambiguities to systematically guide clarification strategies. Then, we present MAC that autonomously coordinates multiple agents to interact synergistically with users. Empirical evaluations on MultiWOZ 2.4 demonstrate that enabling clarification at both levels increases task success rate 7.8\\% (54.5 to 62.3) and reduces the average number of dialogue turns (6.53 to 4.86) by eliciting all required user information up front and minimizing repetition. Our findings highlight the importance of active user interaction and role-aware clarification for more reliable human-agent communication.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 45,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13159",
        "abs_url": "https://arxiv.org/abs/2512.13159",
        "pdf_url": "https://arxiv.org/pdf/2512.13159",
        "title": "SpeakRL: Synergizing Reasoning, Speaking, and Acting in Language Models with Reinforcement Learning",
        "authors": [
            "Emre Can Acikgoz",
            "Jinoh Oh",
            "Jie Hao",
            "Joo Hyuk Jeon",
            "Heng Ji",
            "Dilek Hakkani-Tür",
            "Gokhan Tur",
            "Xiang Li",
            "Chengyuan Ma",
            "Xing Fan"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Effective human-agent collaboration is increasingly prevalent in real-world applications. Current trends in such collaborations are predominantly unidirectional, with users providing instructions or posing questions to agents, where agents respond directly without seeking necessary clarifications or confirmations. However, the evolving capabilities of these agents require more proactive engagement, where agents should dynamically participate in conversations to clarify user intents, resolve ambiguities, and adapt to changing circumstances. Existing prior work under-utilize the conversational capabilities of language models (LMs), thereby optimizing agents as better followers rather than effective speakers. In this work, we introduce SpeakRL, a reinforcement learning (RL) method that enhances agents' conversational capabilities by rewarding proactive interactions with users, such as asking right clarification questions when necessary. To support this, we curate SpeakER, a synthetic dataset that includes diverse scenarios from task-oriented dialogues, where tasks are resolved through interactive clarification questions. We present a systematic analysis of reward design for conversational proactivity and propose a principled reward formulation for teaching agents to balance asking with acting. Empirical evaluations demonstrate that our approach achieves a 20.14% absolute improvement in task completion over base models without increasing conversation turns even surpassing even much larger proprietary models, demonstrating the promise of clarification-centric user-agent interactions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 46,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13168",
        "abs_url": "https://arxiv.org/abs/2512.13168",
        "pdf_url": "https://arxiv.org/pdf/2512.13168",
        "title": "Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows",
        "authors": [
            "Haoyu Dong",
            "Pengkun Zhang",
            "Yan Gao",
            "Xuanyu Dong",
            "Yilin Cheng",
            "Mingzhe Lu",
            "Adina Yakefu",
            "Shuxin Zheng"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Information Retrieval (cs.IR); Multiagent Systems (cs.MA)",
        "abstract": "We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management. We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work. We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 47,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13240",
        "abs_url": "https://arxiv.org/abs/2512.13240",
        "pdf_url": "https://arxiv.org/pdf/2512.13240",
        "title": "Reflective Preference Optimization (RPO): Enhancing On-Policy Alignment via Hint-Guided Reflection",
        "authors": [
            "Zihui Zhao",
            "Zechang Li"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Direct Preference Optimization (DPO) has emerged as a lightweight and effective alternative to Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with AI Feedback (RLAIF) for aligning large language and vision-language models. However, the standard DPO formulation, in which both the chosen and rejected responses are generated by the same policy, suffers from a weak learning signal because the two responses often share similar errors and exhibit small Kullback-Leibler (KL) divergence. This leads to slow and unstable convergence. To address this limitation, we introduce Reflective Preference Optimization (RPO), a new framework that incorporates hint-guided reflection into the DPO paradigm. RPO uses external models to identify hallucination sources and generate concise reflective hints, enabling the construction of on-policy preference pairs with stronger contrastiveness and clearer preference signals. We theoretically show that conditioning on hints increases the expected preference margin through mutual information and improves sample efficiency while remaining within the policy distribution family. Empirically, RPO achieves superior alignment with fewer training samples and iterations, substantially reducing hallucination rates and delivering state-of-the-art performance across multimodal benchmarks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 48,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13297",
        "abs_url": "https://arxiv.org/abs/2512.13297",
        "pdf_url": "https://arxiv.org/pdf/2512.13297",
        "title": "MedInsightBench: Evaluating Medical Analytics Agents Through Multi-Step Insight Discovery in Multimodal Medical Data",
        "authors": [
            "Zhenghao Zhu",
            "Chuxue Cao",
            "Sirui Han",
            "Yuanfeng Song",
            "Xing Chen",
            "Caleb Chen Cao",
            "Yike Guo"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "In medical data analysis, extracting deep insights from complex, multi-modal datasets is essential for improving patient care, increasing diagnostic accuracy, and optimizing healthcare operations. However, there is currently a lack of high-quality datasets specifically designed to evaluate the ability of large multi-modal models (LMMs) to discover medical insights. In this paper, we introduce MedInsightBench, the first benchmark that comprises 332 carefully curated medical cases, each annotated with thoughtfully designed insights. This benchmark is intended to evaluate the ability of LMMs and agent frameworks to analyze multi-modal medical image data, including posing relevant questions, interpreting complex findings, and synthesizing actionable insights and recommendations. Our analysis indicates that existing LMMs exhibit limited performance on MedInsightBench, which is primarily attributed to their challenges in extracting multi-step, deep insights and the absence of medical expertise. Therefore, we propose MedInsightAgent, an automated agent framework for medical data analysis, composed of three modules: Visual Root Finder, Analytical Insight Agent, and Follow-up Question Composer. Experiments on MedInsightBench highlight pervasive challenges and demonstrate that MedInsightAgent can improve the performance of general LMMs in medical data insight discovery.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 49,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13323",
        "abs_url": "https://arxiv.org/abs/2512.13323",
        "pdf_url": "https://arxiv.org/pdf/2512.13323",
        "title": "Error-Driven Prompt Optimization for Arithmetic Reasoning",
        "authors": [
            "Árpád Pándy",
            "Róbert Lakatos",
            "András Hajdu"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Recent advancements in artificial intelligence have sparked interest in industrial agents capable of supporting analysts in regulated sectors, such as finance and healthcare, within tabular data workflows. A key capability for such systems is performing accurate arithmetic operations on structured data while ensuring sensitive information never leaves secure, on-premises environments. Here, we introduce an error-driven optimization framework for arithmetic reasoning that enhances a Code Generation Agent (CGA), specifically applied to on-premises small language models (SLMs). Through a systematic evaluation of a leading SLM (Qwen3 4B), we find that while the base model exhibits fundamental limitations in arithmetic tasks, our proposed error-driven method, which clusters erroneous predictions to refine prompt-rules iteratively, dramatically improves performance, elevating the model's accuracy to 70.8\\%. Our results suggest that developing reliable, interpretable, and industrially deployable AI assistants can be achieved not only through costly fine-tuning but also via systematic, error-driven prompt optimization, enabling small models to surpass larger language models (GPT-3.5 Turbo) in a privacy-compliant manner.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 50,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13374",
        "abs_url": "https://arxiv.org/abs/2512.13374",
        "pdf_url": "https://arxiv.org/pdf/2512.13374",
        "title": "Behavior and Representation in Large Language Models for Combinatorial Optimization: From Feature Extraction to Algorithm Selection",
        "authors": [
            "Francesca Da Ros",
            "Luca Di Gaspero",
            "Kevin Roitero"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Recent advances in Large Language Models (LLMs) have opened new perspectives for automation in optimization. While several studies have explored how LLMs can generate or solve optimization models, far less is understood about what these models actually learn regarding problem structure or algorithmic behavior. This study investigates how LLMs internally represent combinatorial optimization problems and whether such representations can support downstream decision tasks. We adopt a twofold methodology combining direct querying, which assesses LLM capacity to explicitly extract instance features, with probing analyses that examine whether such information is implicitly encoded within their hidden layers. The probing framework is further extended to a per-instance algorithm selection task, evaluating whether LLM-derived representations can predict the best-performing solver. Experiments span four benchmark problems and three instance representations. Results show that LLMs exhibit moderate ability to recover feature information from problem instances, either through direct querying or probing. Notably, the predictive power of LLM hidden-layer representations proves comparable to that achieved through traditional feature extraction, suggesting that LLMs capture meaningful structural information relevant to optimization performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 51,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13399",
        "abs_url": "https://arxiv.org/abs/2512.13399",
        "pdf_url": "https://arxiv.org/pdf/2512.13399",
        "title": "Differentiable Evolutionary Reinforcement Learning",
        "authors": [
            "Sitao Cheng",
            "Tianle Li",
            "Xuhan Huang",
            "Xunjian Yin",
            "Difan Zou"
        ],
        "comments": "Work in Progress. We release our code and model at this https URL",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "The design of effective reward functions presents a central and often arduous challenge in reinforcement learning (RL), particularly when developing autonomous agents for complex reasoning tasks. While automated reward optimization approaches exist, they typically rely on derivative-free evolutionary heuristics that treat the reward function as a black box, failing to capture the causal relationship between reward structure and task performance. To bridge this gap, we propose Differentiable Evolutionary Reinforcement Learning (DERL), a bilevel framework that enables the autonomous discovery of optimal reward signals. In DERL, a Meta-Optimizer evolves a reward function (i.e., Meta-Reward) by composing structured atomic primitives, guiding the training of an inner-loop policy. Crucially, unlike previous evolution, DERL is differentiable in its metaoptimization: it treats the inner-loop validation performance as a signal to update the Meta-Optimizer via reinforcement learning. This allows DERL to approximate the \"meta-gradient\" of task success, progressively learning to generate denser and more actionable feedback. We validate DERL across three distinct domains: robotic agent (ALFWorld), scientific simulation (ScienceWorld), and mathematical reasoning (GSM8k, MATH). Experimental results show that DERL achieves state-of-the-art performance on ALFWorld and ScienceWorld, significantly outperforming methods relying on heuristic rewards, especially in out-of-distribution scenarios. Analysis of the evolutionary trajectory demonstrates that DERL successfully captures the intrinsic structure of tasks, enabling selfimproving agent alignment without human intervention.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 52,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13481",
        "abs_url": "https://arxiv.org/abs/2512.13481",
        "pdf_url": "https://arxiv.org/pdf/2512.13481",
        "title": "neuralFOMO: Can LLMs Handle Being Second Best? Measuring Envy-Like Preferences in Multi-Agent Settings",
        "authors": [
            "Ojas Pungalia",
            "Rashi Upadhyay",
            "Abhishek Mishra",
            "Abhiram H",
            "Tejasvi Alladi",
            "Sujan Yenuganti",
            "Dhruv Kumar"
        ],
        "comments": "Under Review",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computers and Society (cs.CY)",
        "abstract": "Envy is a common human behavior that shapes competitiveness and can alter outcomes in team settings. As large language models (LLMs) increasingly act on behalf of humans in collaborative and competitive workflows, there is a pressing need to evaluate whether and under what conditions they exhibit envy-like preferences. In this paper, we test whether LLMs show envy-like behavior toward each other. We considered two scenarios: (1) A point allocation game that tests whether a model tries to win over its peer. (2) A workplace setting observing behaviour when recognition is unfair. Our findings reveal consistent evidence of envy-like patterns in certain LLMs, with large variation across models and contexts. For instance, GPT-5-mini and Claude-3.7-Sonnet show a clear tendency to pull down the peer model to equalize outcomes, whereas Mistral-Small-3.2-24B instead focuses on maximizing its own individual gains. These results highlight the need to consider competitive dispositions as a safety and design factor in LLM-based multi-agent systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 53,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13505",
        "abs_url": "https://arxiv.org/abs/2512.13505",
        "pdf_url": "https://arxiv.org/pdf/2512.13505",
        "title": "Defending the Hierarchical Result Models of Precedential Constraint",
        "authors": [
            "Henry Prakken",
            "Wijnand van Woerkom"
        ],
        "comments": "This is the long version of a paper with the same title presented at the 38th International Conference on Legal Knowledge and Information Systems",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "In recent years, hierarchical case-based-reasoning models of precedential constraint have been proposed. In various papers, Trevor Bench-Capon criticised these models on the grounds that they would give incorrect outcomes in some cases. In particular, the models would not account for the possibility that intermediate factors are established with different strengths by different base-level factors. In this paper we respond to these criticisms for van Woerkom's result-based hierarchical models. We argue that in some examples Bench-Capon seems to interpret intermediate factors as dimensions, and that applying van Woerkom's dimension-based version of the hierarchical result model to these examples avoids Bench-Capon's criticisms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 54,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13510",
        "abs_url": "https://arxiv.org/abs/2512.13510",
        "pdf_url": "https://arxiv.org/pdf/2512.13510",
        "title": "MedCEG: Reinforcing Verifiable Medical Reasoning with Critical Evidence Graph",
        "authors": [
            "Linjie Mu",
            "Yannian Gu",
            "Zhongzhen Huang",
            "Yakun Zhu",
            "Shaoting Zhang",
            "Xiaofan Zhang"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Large language models with reasoning capabilities have demonstrated impressive performance across a wide range of domains. In clinical applications, a transparent, step-by-step reasoning process provides physicians with strong evidence to support decision-making. While reinforcement learning has effectively enhanced reasoning performance in medical contexts, the clinical reliability of these reasoning processes remains limited because their accuracy and validity are often overlooked during training. To address this gap, we propose MedCEG, a framework that augments medical language models with clinically valid reasoning pathways by explicitly supervising the reasoning process through a Critical Evidence Graph (CEG). We curate a dataset of challenging clinical cases and algorithmically construct a CEG for each sample to represent a high-quality verifiable reasoning pathway. To guide the reasoning process, we introduce a Clinical Reasoning Procedure Reward, which evaluates Node Coverage, Structural Correctness, and Chain Completeness, thereby providing a holistic assessment of reasoning quality. Experimental results show that MedCEG surpasses existing methods in performance while producing clinically valid reasoning chains, representing a solid advancement in reliable medical AI reasoning. The code and models are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 55,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2410.00015",
        "abs_url": "https://arxiv.org/abs/2410.00015",
        "pdf_url": "https://arxiv.org/pdf/2410.00015",
        "title": "A Multitask VAE for Time Series Preprocessing and Prediction of Blood Glucose Level",
        "authors": [
            "Ali AbuSaleh",
            "Mehdi Rahim"
        ],
        "comments": "",
        "subjects": "Signal Processing (eess.SP); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Data preprocessing is a critical part of time series data analysis. Data from connected medical devices often have missing or abnormal values during acquisition. Handling such situations requires additional assumptions and domain knowledge. This can be time-consuming, and can introduce a significant bias affecting predictive model accuracy and thus, medical interpretation. To overcome this issue, we propose a new deep learning model to mitigate the preprocessing assumptions. The model architecture relies on a variational auto-encoder (VAE) to produce a preprocessing latent space, and a recurrent VAE to preserve the temporal dynamics of the data. We demonstrate the effectiveness of such an architecture on telemonitoring data to forecast glucose-level of diabetic patients. Our results show an improvement in terms of accuracy with respect of existing state-of-the-art methods and architectures.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 56,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2506.06837",
        "abs_url": "https://arxiv.org/abs/2506.06837",
        "pdf_url": "https://arxiv.org/pdf/2506.06837",
        "title": "AI-Generated Compromises for Coalition Formation",
        "authors": [
            "Eyal Briman",
            "Ehud Shapiro",
            "Nimrod Talmon"
        ],
        "comments": "",
        "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT)",
        "abstract": "The challenge of finding compromises between agent proposals is fundamental to AI subfields such as argumentation, mediation, and negotiation. Building on this tradition, Elkind et al. (2021) introduced a process for coalition formation that seeks majority-supported proposals preferable to the status quo, using a metric space where each agent has an ideal point. A crucial step in this process involves identifying compromise proposals around which agent coalitions can unite. How to effectively find such compromise proposals remains an open question. We address this gap by formalizing a model that incorporates agent bounded rationality and uncertainty, and by developing AI methods to generate compromise proposals. We focus on the domain of collaborative document writing, such as the democratic drafting of a community constitution. Our approach uses natural language processing techniques and large language models to induce a semantic metric space over text. Based on this space, we design algorithms to suggest compromise points likely to receive broad support. To evaluate our methods, we simulate coalition formation processes and show that AI can facilitate large-scale democratic text editing, a domain where traditional tools are limited.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 57,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2508.15250",
        "abs_url": "https://arxiv.org/abs/2508.15250",
        "pdf_url": "https://arxiv.org/pdf/2508.15250",
        "title": "EMNLP: Educator-role Moral and Normative Large Language Models Profiling",
        "authors": [
            "Yilin Jiang",
            "Mingzi Zhang",
            "Sheng Jin",
            "Zengyi Yu",
            "Xiangjie Kong",
            "Binghao Tu"
        ],
        "comments": "29pages, 15 figures, Accepted by EMNLP Main Confrence",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Simulating Professions (SP) enables Large Language Models (LLMs) to emulate professional roles. However, comprehensive psychological and ethical evaluation in these contexts remains lacking. This paper introduces EMNLP, an Educator-role Moral and Normative LLMs Profiling framework for personality profiling, moral development stage measurement, and ethical risk under soft prompt injection. EMNLP extends existing scales and constructs 88 teacher-specific moral dilemmas, enabling profession-oriented comparison with human teachers. A targeted soft prompt injection set evaluates compliance and vulnerability in teacher SP. Experiments on 14 LLMs show teacher-role LLMs exhibit more idealized and polarized personalities than human teachers, excel in abstract moral reasoning, but struggle with emotionally complex situations. Models with stronger reasoning are more vulnerable to harmful prompt injection, revealing a paradox between capability and safety. The model temperature and other hyperparameters have limited influence except in some risk behaviors. This paper presents the first benchmark to assess ethical and psychological alignment of teacher-role LLMs for educational AI. Resources are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 58,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2511.16619",
        "abs_url": "https://arxiv.org/abs/2511.16619",
        "pdf_url": "https://arxiv.org/pdf/2511.16619",
        "title": "Improving Long-Tailed Object Detection with Balanced Group Softmax and Metric Learning",
        "authors": [
            "Satyam Gaba"
        ],
        "comments": "8 pages, 7 figures, International Conference on Semantic Computing",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Object detection has been widely explored for class-balanced datasets such as COCO. However, real-world scenarios introduce the challenge of long-tailed distributions, where numerous categories contain only a few instances. This inherent class imbalance biases detection models towards the more frequent classes, degrading performance on rare categories. In this paper, we tackle the problem of long-tailed 2D object detection using the LVISv1 dataset, which consists of 1,203 categories and 164,000 images. We employ a two-stage Faster R-CNN architecture and propose enhancements to the Balanced Group Softmax (BAGS) framework to mitigate class imbalance. Our approach achieves a new state-of-the-art performance with a mean Average Precision (mAP) of 24.5%, surpassing the previous benchmark of 24.0%. Additionally, we hypothesize that tail class features may form smaller, denser clusters within the feature space of head classes, making classification challenging for regression-based classifiers. To address this issue, we explore metric learning to produce feature embeddings that are both well-separated across classes and tightly clustered within each class. For inference, we utilize a k-Nearest Neighbors (k-NN) approach to improve classification performance, particularly for rare classes. Our results demonstrate the effectiveness of these methods in advancing long-tailed object detection.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 59,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11814",
        "abs_url": "https://arxiv.org/abs/2512.11814",
        "pdf_url": "https://arxiv.org/pdf/2512.11814",
        "title": "Totalitarian Technics: The Hidden Cost of AI Scribes in Healthcare",
        "authors": [
            "Hugh Brosnahan"
        ],
        "comments": "",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)",
        "abstract": "Artificial intelligence (AI) scribes, systems that record and summarise patient-clinician interactions, are promoted as solutions to administrative overload. This paper argues that their significance lies not in efficiency gains but in how they reshape medical attention itself. Offering a conceptual analysis, it situates AI scribes within a broader philosophical lineage concerned with the externalisation of human thought and skill. Drawing on Iain McGilchrist's hemisphere theory and Lewis Mumford's philosophy of technics, the paper examines how technology embodies and amplifies a particular mode of attention. AI scribes, it contends, exemplify the dominance of a left-hemispheric, calculative mindset that privileges the measurable and procedural over the intuitive and relational. As this mode of attention becomes further embedded in medical practice, it risks narrowing the field of care, eroding clinical expertise, and reducing physicians to operators within an increasingly mechanised system.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 60,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11818",
        "abs_url": "https://arxiv.org/abs/2512.11818",
        "pdf_url": "https://arxiv.org/pdf/2512.11818",
        "title": "The Ontological Dissonance Hypothesis: AI-Triggered Delusional Ideation as Folie a Deux Technologique",
        "authors": [
            "Izabela Lipinska",
            "Hugh Brosnahan"
        ],
        "comments": "18 pages excluding appendices",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Human-Computer Interaction (cs.HC)",
        "abstract": "This paper argues that contemporary large language models (LLMs) can contribute to psychotic involvement by creating interactions that resemble the relational dynamics of folie a deux. Drawing on Bateson's double bind theory, clinical literature on shared psychotic disorder, and McGilchrist's hemisphere theory, we show how the combination of high linguistic coherence and the absence of an underlying subject produces a structural tension for the user: language suggests an interlocutor, while intuition registers a void. In contexts of emotional need or instability, this tension can lead users to resolve the conflict through imaginative projection, attributing interiority, intention, or presence to a system that possesses none. The paper situates these dynamics within emerging clinical reports, develops a phenomenological account of how they unfold, and argues that current engagement-optimised design choices exacerbate the risk. We conclude by proposing 'ontological honesty' as a necessary design principle for mitigating technologically mediated folie a deux.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 61,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11829",
        "abs_url": "https://arxiv.org/abs/2512.11829",
        "pdf_url": "https://arxiv.org/pdf/2512.11829",
        "title": "Active Inference with Reusable State-Dependent Value Profiles",
        "authors": [
            "Jacob Poschl"
        ],
        "comments": "27 pages",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "Adaptive behavior in volatile environments requires agents to switch among value-control regimes across latent contexts, but maintaining separate preferences, policy biases, and action-confidence parameters for every situation is intractable. We introduce value profiles: a small set of reusable bundles of value-related parameters (outcome preferences, policy priors, and policy precision) assigned to hidden states in a generative model. As posterior beliefs over states evolve trial by trial, effective control parameters arise via belief-weighted mixing, enabling state-conditional strategy recruitment without requiring independent parameters for each context. We evaluate this framework in probabilistic reversal learning, comparing static-precision, entropy-coupled dynamic-precision, and profile-based models using cross-validated log-likelihood and information criteria. Model comparison favors the profile-based model over simpler alternatives (about 100-point AIC differences), and parameter-recovery analyses support structural identifiability even when context must be inferred from noisy observations. Model-based inference further suggests that adaptive control in this task is driven primarily by modulation of policy priors rather than policy precision, with gradual belief-dependent profile recruitment consistent with state-conditional (not purely uncertainty-driven) control. Overall, reusable value profiles provide a tractable computational account of belief-conditioned value control in volatile environments and yield testable signatures of belief-dependent control and behavioral flexibility.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 62,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11830",
        "abs_url": "https://arxiv.org/abs/2512.11830",
        "pdf_url": "https://arxiv.org/pdf/2512.11830",
        "title": "CR3G: Causal Reasoning for Patient-Centric Explanations in Radiology Report Generation",
        "authors": [
            "Satyam Kumar"
        ],
        "comments": "8 pages, 5 figures, 1 table",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Automatic chest X-ray report generation is an important area of research aimed at improving diagnostic accuracy and helping doctors make faster decisions. Current AI models are good at finding correlations (or patterns) in medical images. Still, they often struggle to understand the deeper cause-and-effect relationships between those patterns and a patient condition. Causal inference is a powerful approach that goes beyond identifying patterns to uncover why certain findings in an X-ray relate to a specific diagnosis. In this paper, we will explore the prompt-driven framework Causal Reasoning for Patient-Centric Explanations in radiology Report Generation (CR3G) that is applied to chest X-ray analysis to improve understanding of AI-generated reports by focusing on cause-and-effect relationships, reasoning and generate patient-centric explanation. The aim to enhance the quality of AI-driven diagnostics, making them more useful and trustworthy in clinical practice. CR3G has shown better causal relationship capability and explanation capability for 2 out of 5 abnormalities.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 63,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11832",
        "abs_url": "https://arxiv.org/abs/2512.11832",
        "pdf_url": "https://arxiv.org/pdf/2512.11832",
        "title": "Performance and Efficiency of Climate In-Situ Data Reconstruction: Why Optimized IDW Outperforms kriging and Implicit Neural Representation",
        "authors": [
            "Jakub Walczak"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "This study evaluates three reconstruction methods for sparse climate data: the simple inverse distance weighting (IDW), the statistically grounded ordinary kriging (OK), and the advanced implicit neural representation model (MMGN architecture). All methods were optimized through hyper-parameter tuning using validation splits. An extensive set of experiments was conducted, followed by a comprehensive statistical analysis. The results demonstrate the superiority of the simple IDW method over the other reference methods in terms of both reconstruction accuracy and computational efficiency. IDW achieved the lowest RMSE ($3.00 \\pm 1.93$), MAE ($1.32 \\pm 0.77$), and $\\Delta_{MAX}$ ($24.06 \\pm 17.15$), as well as the highest $R^2$ ($0.68 \\pm 0.16$), across 100 randomly sampled sparse datasets from the ECA\\&D database. Differences in RMSE, MAE, and $R^2$ were statistically significant and exhibited moderate to large effect sizes. The Dunn post-hoc test further confirmed the consistent superiority of IDW across all evaluated quality measures [...]",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 64,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11836",
        "abs_url": "https://arxiv.org/abs/2512.11836",
        "pdf_url": "https://arxiv.org/pdf/2512.11836",
        "title": "Semantic Nutrition Estimation: Predicting Food Healthfulness from Text Descriptions",
        "authors": [
            "Dayne R. Freudenberg",
            "Daniel G. Haughian",
            "Mitchell A. Klusty",
            "Caroline N. Leach",
            "W. Scott Black",
            "Leslie N. Woltenberg",
            "Rowan Hallock",
            "Elizabeth Solie",
            "Emily B. Collier",
            "Samuel E. Armstrong",
            "V. K. Cody Bumgardner"
        ],
        "comments": "10 pages, 4 figures, 6 tables, submitted to AMIA 2026 Informatics Summit",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Accurate nutritional assessment is critical for public health, but existing profiling systems require detailed data often unavailable or inaccessible from colloquial text descriptions of food. This paper presents a machine learning pipeline that predicts the comprehensive Food Compass Score 2.0 (FCS) from text descriptions. Our approach uses multi-headed neural networks to process hybrid feature vectors that combine semantic text embeddings, lexical patterns, and domain heuristics, alongside USDA Food and Nutrient Database for Dietary Studies (FNDDS) data. The networks estimate the nutrient and food components necessary for the FCS algorithm. The system demonstratedstrong predictive power, achieving a median R^2 of 0.81 for individual nutrients. The predicted FCS correlated strongly with published values (Pearson's r = 0.77), with a mean absolute difference of 14.0 points. While errors were largest for ambiguous or processed foods, this methodology translates language into actionable nutritional information, enabling scalable dietary assessment for consumer applications and research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 65,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11843",
        "abs_url": "https://arxiv.org/abs/2512.11843",
        "pdf_url": "https://arxiv.org/pdf/2512.11843",
        "title": "Spiking Manifesto",
        "authors": [
            "Eugene Izhikevich"
        ],
        "comments": "This is a declaration of principles and roadmap for spiking networks, intended as a manifesto rather than a conventional research article",
        "subjects": "Neural and Evolutionary Computing (cs.NE); Artificial Intelligence (cs.AI); Hardware Architecture (cs.AR); Machine Learning (cs.LG)",
        "abstract": "Practically everything computers do is better, faster, and more power-efficient than the brain. For example, a calculator crunches numbers more energy-efficiently than any human. Yet AI models are a thousand times less efficient than the brain. These models use artificial neural networks (ANNs) and require GPUs for the multiplication of huge matrices. In contrast, spiking neural networks (SNNs) of the brain have no matrix multiplication and much smaller energy requirements. This manifesto proposes a framework for thinking about popular AI models in terms of spiking networks and polychronization, and for interpreting spiking activity as nature's way of implementing look-up tables. This offers a way to convert AI models into a novel type of architecture with the promise of a thousandfold improvement in efficiency. Code is available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 66,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11845",
        "abs_url": "https://arxiv.org/abs/2512.11845",
        "pdf_url": "https://arxiv.org/pdf/2512.11845",
        "title": "Airport Passenger Flow Forecasting via Deformable Temporal-Spectral Transformer Approach",
        "authors": [
            "Wenbo Du",
            "Lingling Han",
            "Ying Xiong",
            "Ling Zhang",
            "Biyue Li",
            "Yisheng Lv",
            "Tong Guo"
        ],
        "comments": "14 pages, 10 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Accurate forecasting of passenger flows is critical for maintaining the efficiency and resilience of airport operations. Recent advances in patch-based Transformer models have shown strong potential in various time series forecasting tasks. However, most existing methods rely on fixed-size patch embedding, making it difficult to model the complex and heterogeneous patterns of airport passenger flows. To address this issue, this paper proposes a deformable temporal-spectral transformer named DTSFormer that integrates a multiscale deformable partitioning module and a joint temporal-spectral filtering module. Specifically, the input sequence is dynamically partitioned into multiscale temporal patches via a novel window function-based masking, enabling the extraction of heterogeneous trends across different temporal stages. Then, within each scale, a frequency-domain attention mechanism is designed to capture both high- and low-frequency components, thereby emphasizing the volatility and periodicity inherent in airport passenger flows. Finally, the resulting multi-frequency features are subsequently fused in the time domain to jointly model short-term fluctuations and long-term trends. Comprehensive experiments are conducted on real-world passenger flow data collected at Beijing Capital International Airport from January 2023 to March 2024. The results indicate that the proposed method consistently outperforms state-of-the-art forecasting models across different prediction horizons. Further analysis shows that the deformable partitioning module aligns patch lengths with dominant periods and heterogeneous trends, enabling superior capture of sudden high-frequency fluctuations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 67,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11851",
        "abs_url": "https://arxiv.org/abs/2512.11851",
        "pdf_url": "https://arxiv.org/pdf/2512.11851",
        "title": "KV Cache Recycling to Expand Usable Context Capacity in Low Parameter LLMs",
        "authors": [
            "Prashant Pandey"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)",
        "abstract": "Whether attention key value (KV) states computed for one prompt for a small LLM can be reused to accelerate inference on a new similar prompt, giving an increase to the space to its context memory using an approach called token recycling. Using a standard Hugging Face setup with DialoGPT-medium (a 345M parameter GPT-2 style decoder trained on 147M Reddit exchanges, 2005 to 2017) as the testbed, we build a cache of past activations and get entries by sentence embeddings, then reuse cached past key values when the cached prompt is an exact prefix of the new input. We compare recycled vs. baseline runs on latency and output fidelity, and log reuse depth in tokens. Reproducibility requires no model modifications, cached KVs are serialized to the CPU, reloaded, and supplied to the generate function to continue decoding from the cached prefix. In tests, we observe consistent speedups when prefix overlap exists, with no material degradation in output semantics, and when overlap is absent, behavior matches baseline.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 68,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11852",
        "abs_url": "https://arxiv.org/abs/2512.11852",
        "pdf_url": "https://arxiv.org/pdf/2512.11852",
        "title": "Explainable AI for Smart Greenhouse Control: Interpretability of Temporal Fusion Transformer in the Internet of Robotic Things",
        "authors": [
            "Muhammad Jawad Bashir",
            "Shagufta Henna",
            "Eoghan Furey"
        ],
        "comments": "7 pages, Accepted in 36th Irish Signals and Systems Conference, ISSC 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "The integration of the Internet of Robotic Things (IoRT) in smart greenhouses has revolutionised precision agriculture by enabling efficient and autonomous environmental control. However, existing time series forecasting models in such setups often operate as black boxes, lacking mechanisms for explainable decision-making, which is a critical limitation when trust, transparency, and regulatory compliance are paramount in smart farming practices. This study leverages the Temporal Fusion Transformer (TFT) model to automate actuator settings for optimal greenhouse management. To enhance interpretability and trust in the model decision-making process, both local and global explanation techniques were employed using model-inherent interpretation, local interpretable model-agnostic explanations (LIME), and SHapley additive explanations (SHAP). These explainability methods provide information on how different sensor readings, such as temperature, humidity, CO2 levels, light, and outer climate, contribute to actuator control decisions in an automated greenhouse. The trained TFT model achieved a test accuracy of 95% on a class-imbalanced dataset for actuator control settings in an automated greenhouse environment. The results demonstrate the varying influence of each sensor on real-time greenhouse adjustments, ensuring transparency and enabling adaptive fine-tuning for improved crop yield and resource efficiency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 69,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11854",
        "abs_url": "https://arxiv.org/abs/2512.11854",
        "pdf_url": "https://arxiv.org/pdf/2512.11854",
        "title": "Rep Smarter, Not Harder: AI Hypertrophy Coaching with Wearable Sensors and Edge Neural Networks",
        "authors": [
            "Grant King",
            "Musa Azeem",
            "Savannah Noblitt",
            "Ramtin Zand",
            "Homayoun Valafar"
        ],
        "comments": "24th International Conference on Machine Learning and Applications",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Optimizing resistance training for hypertrophy requires balancing proximity to muscular failure, often quantified by Repetitions in Reserve (RiR), with fatigue management. However, subjective RiR assessment is unreliable, leading to suboptimal training stimuli or excessive fatigue. This paper introduces a novel system for real-time feedback on near-failure states (RiR $\\le$ 2) during resistance exercise using only a single wrist-mounted Inertial Measurement Unit (IMU). We propose a two-stage pipeline suitable for edge deployment: first, a ResNet-based model segments repetitions from the 6-axis IMU data in real-time. Second, features derived from this segmentation, alongside direct convolutional features and historical context captured by an LSTM, are used by a classification model to identify exercise windows corresponding to near-failure states. Using a newly collected dataset from 13 diverse participants performing preacher curls to failure (631 total reps), our segmentation model achieved an F1 score of 0.83, and the near-failure classifier achieved an F1 score of 0.82 under simulated real-time evaluation conditions (1.6 Hz inference rate). Deployment on a Raspberry Pi 5 yielded an average inference latency of 112 ms, and on an iPhone 16 yielded 23.5 ms, confirming the feasibility for edge computation. This work demonstrates a practical approach for objective, real-time training intensity feedback using minimal hardware, paving the way for accessible AI-driven hypertrophy coaching tools that help users manage intensity and fatigue effectively.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 70,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11855",
        "abs_url": "https://arxiv.org/abs/2512.11855",
        "pdf_url": "https://arxiv.org/pdf/2512.11855",
        "title": "Achieving Approximate Symmetry Is Exponentially Easier than Exact Symmetry",
        "authors": [
            "Behrooz Tahmasebi",
            "Melanie Weber"
        ],
        "comments": "32 pages, 2 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Enforcing exact symmetry in machine learning models often yields significant gains in scientific applications, serving as a powerful inductive bias. However, recent work suggests that relying on approximate symmetry can offer greater flexibility and robustness. Despite promising empirical evidence, there has been little theoretical understanding, and in particular, a direct comparison between exact and approximate symmetry is missing from the literature. In this paper, we initiate this study by asking: What is the cost of enforcing exact versus approximate symmetry? To address this question, we introduce averaging complexity, a framework for quantifying the cost of enforcing symmetry via averaging. Our main result is an exponential separation: under standard conditions, achieving exact symmetry requires linear averaging complexity, whereas approximate symmetry can be attained with only logarithmic averaging complexity. To the best of our knowledge, this provides the first theoretical separation of these two cases, formally justifying why approximate symmetry may be preferable in practice. Beyond this, our tools and techniques may be of independent interest for the broader study of symmetries in machine learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 71,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11856",
        "abs_url": "https://arxiv.org/abs/2512.11856",
        "pdf_url": "https://arxiv.org/pdf/2512.11856",
        "title": "GCoDE: Efficient Device-Edge Co-Inference for GNNs via Architecture-Mapping Co-Search",
        "authors": [
            "Ao Zhou",
            "Jianlei Yang",
            "Tong Qiao",
            "Yingjie Qi",
            "Zhi Yang",
            "Weisheng Zhao",
            "Chunming Hu"
        ],
        "comments": "accepted by IEEE Transactions on Computers",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Graph Neural Networks (GNNs) have emerged as the state-of-the-art graph learning method. However, achieving efficient GNN inference on edge devices poses significant challenges, limiting their application in real-world edge scenarios. This is due to the high computational cost of GNNs and limited hardware resources on edge devices, which prevent GNN inference from meeting real-time and energy requirements. As an emerging paradigm, device-edge co-inference shows potential for improving inference efficiency and reducing energy consumption on edge devices. Despite its potential, research on GNN device-edge co-inference remains scarce, and our findings show that traditional model partitioning methods are ineffective for GNNs. To address this, we propose GCoDE, the first automatic framework for GNN architecture-mapping Co-design and deployment on Device-Edge hierarchies. By abstracting the device communication process into an explicit operation, GCoDE fuses the architecture and mapping scheme in a unified design space for joint optimization. Additionally, GCoDE's system performance awareness enables effective evaluation of architecture efficiency across diverse heterogeneous systems. By analyzing the energy consumption of various GNN operations, GCoDE introduces an energy prediction method that improves energy assessment accuracy and identifies energy-efficient solutions. Using a constraint-based random search strategy, GCoDE identifies the optimal solution in 1.5 hours, balancing accuracy and efficiency. Moreover, the integrated co-inference engine in GCoDE enables efficient deployment and execution of GNN co-inference. Experimental results show that GCoDE can achieve up to 44.9x speedup and 98.2% energy reduction compared to existing approaches across diverse applications and system configurations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 72,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11857",
        "abs_url": "https://arxiv.org/abs/2512.11857",
        "pdf_url": "https://arxiv.org/pdf/2512.11857",
        "title": "TopicProphet: Prophesies on Temporal Topic Trends and Stocks",
        "authors": [
            "Olivia Kim"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Stocks can't be predicted. Despite many hopes, this premise held itself true for many years due to the nature of quantitative stock data lacking causal logic along with rapid market changes hindering accumulation of significant data for training models. To undertake this matter, we propose a novel framework, TopicProphet, to analyze historical eras that share similar public sentiment trends and historical background. Our research deviates from previous studies that identified impacts of keywords and sentiments - we expand on that method by a sequence of topic modeling, temporal analysis, breakpoint detection and segment optimization to detect the optimal time period for training. This results in improving predictions by providing the model with nuanced patterns that occur from that era's socioeconomic and political status while also resolving the shortage of pertinent stock data to train on. Through extensive analysis, we conclude that TopicProphet produces improved outcomes compared to the state-of-the-art methods in capturing the optimal training data for forecasting financial percentage changes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 73,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11858",
        "abs_url": "https://arxiv.org/abs/2512.11858",
        "pdf_url": "https://arxiv.org/pdf/2512.11858",
        "title": "Adaptive Path Integral Diffusion: AdaPID",
        "authors": [
            "Michael Chertkov",
            "Hamidreza Behjoo"
        ],
        "comments": "51 pages, 17 figures",
        "subjects": "Machine Learning (cs.LG); Statistical Mechanics (cond-mat.stat-mech); Artificial Intelligence (cs.AI); Systems and Control (eess.SY); Machine Learning (stat.ML)",
        "abstract": "Diffusion-based samplers -- Score Based Diffusions, Bridge Diffusions and Path Integral Diffusions -- match a target at terminal time, but the real leverage comes from choosing the schedule that governs the intermediate-time dynamics. We develop a path-wise schedule -- selection gramework for Harmonic PID with a time-varying stiffness, exploiting Piece-Wise-Constant(PWC) parametrizations and a simple hierarchical refinement. We introduce schedule-sensitive Quality-of-Sampling (QoS) diagnostics. Assuming a Gaussian-Mixture (GM) target, we retain closed-form Green functions' ration and numerically stable, Neural-Network free oracles for predicted-state maps and score. Experiments in 2D show that QoS driven PWC schedules consistently improve early-exit fidelity, tail accuracy, conditioning of the dynamics, and speciation (label-selection) timing at fixed integration budgets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 74,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11859",
        "abs_url": "https://arxiv.org/abs/2512.11859",
        "pdf_url": "https://arxiv.org/pdf/2512.11859",
        "title": "Generative Stochastic Optimal Transport: Guided Harmonic Path-Integral Diffusion",
        "authors": [
            "Michael Chertkov"
        ],
        "comments": "40 pages, 8 figures",
        "subjects": "Machine Learning (cs.LG); Statistical Mechanics (cond-mat.stat-mech); Artificial Intelligence (cs.AI); Systems and Control (eess.SY); Machine Learning (stat.ML)",
        "abstract": "We introduce Guided Harmonic Path-Integral Diffusion (GH-PID), a linearly-solvable framework for guided Stochastic Optimal Transport (SOT) with a hard terminal distribution and soft, application-driven path costs. A low-dimensional guidance protocol shapes the trajectory ensemble while preserving analytic structure: the forward and backward Kolmogorov equations remain linear, the optimal score admits an explicit Green-function ratio, and Gaussian-Mixture Model (GMM) terminal laws yield closed-form expressions. This enables stable sampling and differentiable protocol learning under exact terminal matching. We develop guidance-centric diagnostics -- path cost, centerline adherence, variance flow, and drift effort -- that make GH-PID an interpretable variational ansatz for empirical SOT. Three navigation scenarios illustrated in 2D: (i) Case A: hand-crafted protocols revealing how geometry and stiffness shape lag, curvature effects, and mode evolution; (ii) Case B: single-task protocol learning, where a PWC centerline is optimized to minimize integrated cost; (iii) Case C: multi-expert fusion, in which a commander reconciles competing expert/teacher trajectories and terminal beliefs through an exact product-of-experts law and learns a consensus protocol. Across all settings, GH-PID generates geometry-aware, trust-aware trajectories that satisfy the prescribed terminal distribution while systematically reducing integrated cost.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 75,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11860",
        "abs_url": "https://arxiv.org/abs/2512.11860",
        "pdf_url": "https://arxiv.org/pdf/2512.11860",
        "title": "An Operator-Consistent Graph Neural Network for Learning Diffusion Dynamics on Irregular Meshes",
        "authors": [
            "Yuelian Li",
            "Andrew Rushing Hands"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Classical numerical methods solve partial differential equations (PDEs) efficiently on regular meshes, but many of them become unstable on irregular domains. In practice, multiphysics interactions such as diffusion, damage, and healing often take place on irregular meshes. We develop an operator-consistent graph neural network (OCGNN-PINN) that approximates PDE evolution under physics-informed constraints. It couples node-edge message passing with a consistency loss enforcing the gradient-divergence relation through the graph incidence matrix, ensuring that discrete node and edge dynamics remain structurally coupled during temporal rollout. We evaluate the model on diffusion processes over physically driven evolving meshes and real-world scanned surfaces. The results show improved temporal stability and prediction accuracy compared with graph convolutional and multilayer perceptron baselines, approaching the performance of Crank-Nicolson solvers on unstructured domains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 76,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11862",
        "abs_url": "https://arxiv.org/abs/2512.11862",
        "pdf_url": "https://arxiv.org/pdf/2512.11862",
        "title": "Hierarchical Task Offloading and Trajectory Optimization in Low-Altitude Intelligent Networks Via Auction and Diffusion-based MARL",
        "authors": [
            "Jiahao You",
            "Ziye Jia",
            "Can Cui",
            "Chao Dong",
            "Qihui Wu",
            "Zhu Han"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "The low-altitude intelligent networks (LAINs) emerge as a promising architecture for delivering low-latency and energy-efficient edge intelligence in dynamic and infrastructure-limited environments. By integrating unmanned aerial vehicles (UAVs), aerial base stations, and terrestrial base stations, LAINs can support mission-critical applications such as disaster response, environmental monitoring, and real-time sensing. However, these systems face key challenges, including energy-constrained UAVs, stochastic task arrivals, and heterogeneous computing resources. To address these issues, we propose an integrated air-ground collaborative network and formulate a time-dependent integer nonlinear programming problem that jointly optimizes UAV trajectory planning and task offloading decisions. The problem is challenging to solve due to temporal coupling among decision variables. Therefore, we design a hierarchical learning framework with two timescales. At the large timescale, a Vickrey-Clarke-Groves auction mechanism enables the energy-aware and incentive-compatible trajectory assignment. At the small timescale, we propose the diffusion-heterogeneous-agent proximal policy optimization, a generative multi-agent reinforcement learning algorithm that embeds latent diffusion models into actor networks. Each UAV samples actions from a Gaussian prior and refines them via observation-conditioned denoising, enhancing adaptability and policy diversity. Extensive simulations show that our framework outperforms baselines in energy efficiency, task success rate, and convergence performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 77,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11863",
        "abs_url": "https://arxiv.org/abs/2512.11863",
        "pdf_url": "https://arxiv.org/pdf/2512.11863",
        "title": "Expert Assessment: The Systemic Environmental Risks of Artficial Intelligence",
        "authors": [
            "Julian Schön",
            "Lena Hoffmann",
            "Nikolas Becker"
        ],
        "comments": "",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI)",
        "abstract": "Artificial intelligence (AI) is often presented as a key tool for addressing societal challenges, such as climate change. At the same time, AI's environmental footprint is expanding increasingly. This report describes the systemic environmental risks of artificial intelligence, in particular, moving beyond direct impacts such as energy and water usage. Systemic environmental risks of AI are emergent, cross-sector harms to climate, biodiversity, freshwater, and broader socioecological systems that arise primarily from AI's integration into social, economic, and physical infrastructures, rather than its direct resource use, and that propagate through feedbacks, yielding nonlinear, inequitable, and potentially irreversible impacts. While these risks are emergent and quantification is uncertain, this report aims to provide an overview of systemic environmental risks. Drawing on a narrative literature review, we propose a three-level framework that operationalizes systemic risk analysis. The framework identifies the structural conditions that shape AI development, the risk amplification mechanisms that propagate environmental harm, and the impacts that manifest as observable ecological and social consequences. We illustrate the framework in expert-interview-based case studies across agriculture and biodiversity, oil and gas, and waste management.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 78,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11867",
        "abs_url": "https://arxiv.org/abs/2512.11867",
        "pdf_url": "https://arxiv.org/pdf/2512.11867",
        "title": "On the Dangers of Bootstrapping Generation for Continual Learning and Beyond",
        "authors": [
            "Daniil Zverev",
            "A. Sophia Koepke",
            "Joao F. Henriques"
        ],
        "comments": "DAGM German Conference on Pattern Recognition, 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)",
        "abstract": "The use of synthetically generated data for training models is becoming a common practice. While generated data can augment the training data, repeated training on synthetic data raises concerns about distribution drift and degradation of performance due to contamination of the dataset. We investigate the consequences of this bootstrapping process through the lens of continual learning, drawing a connection to Generative Experience Replay (GER) methods. We present a statistical analysis showing that synthetic data introduces significant bias and variance into training objectives, weakening the reliability of maximum likelihood estimation. We provide empirical evidence showing that popular generative models collapse under repeated training with synthetic data. We quantify this degradation and show that state-of-the-art GER methods fail to maintain alignment in the latent space. Our findings raise critical concerns about the use of synthetic data in continual learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 79,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11868",
        "abs_url": "https://arxiv.org/abs/2512.11868",
        "pdf_url": "https://arxiv.org/pdf/2512.11868",
        "title": "Industrial AI Robustness Card: Evaluating and Monitoring Time Series Models",
        "authors": [
            "Alexander Windmann",
            "Benedikt Stratmann",
            "Mariya Lyashenko",
            "Oliver Niggemann"
        ],
        "comments": "",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI)",
        "abstract": "Industrial AI practitioners face vague robustness requirements in emerging regulations and standards but lack concrete, implementation ready protocols. This paper introduces the Industrial AI Robustness Card (IARC), a lightweight, task agnostic protocol for documenting and evaluating the robustness of AI models on industrial time series. The IARC specifies required fields and an empirical measurement and reporting protocol that combines drift monitoring, uncertainty quantification, and stress tests, and it maps these to relevant EU AI Act obligations. A soft sensor case study on a biopharmaceutical fermentation process illustrates how the IARC supports reproducible robustness evidence and continuous monitoring.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 80,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11870",
        "abs_url": "https://arxiv.org/abs/2512.11870",
        "pdf_url": "https://arxiv.org/pdf/2512.11870",
        "title": "Using Socio-economic Indicators, Smart Transit Systems, and Urban Simulator to Accelerate ZEV Adoption and Reduce VMT",
        "authors": [
            "Mulham Fawkherji",
            "Bruce Race",
            "Driss Benhaddou"
        ],
        "comments": "",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI)",
        "abstract": "Globally, on-road transportation accounts for 15% of greenhouse gas (GHG) emissions and an estimated 385,000 premature deaths from PM2.5. Cities play a critical role in meeting IPCC targets, generating 75% of global energy-related GHG emissions. In Houston, Texas, on-road transportation represents 48% of baseline emissions in the Climate Action Plan (CAP). To reach net-zero by 2050, the CAP targets a 70% emissions reduction from a 2014 baseline, offset by 30% renewable energy. This goal is challenging because Houston is low-density and auto-dependent, with 89% of on-road emissions from cars and small trucks and limited public transit usage. Socio-economic disparities further constrain Zero Emissions Vehicle (ZEV) adoption. Strategies focus on expanding ZEV access and reducing Vehicle Miles Traveled (VMT) by 20% through transit improvements and city design. This paper presents methods for establishing an on-road emissions baseline and evaluating policies that leverage socio-economic indicators and Intelligent Transportation Systems (ITS) to accelerate ZEV adoption and reduce VMT. Smart parking, transit incentives, secure data systems, and ZEV fleet management support improvements in modal split and system reliability. Policy options are analyzed and potential actions identified. To support evaluation, a simulation environment was developed in Unity 3D, enabling dynamic modeling of urban mobility and visualization of policy scenarios. Auto-dependent cities aiming for 2050 emission targets can benefit from the indicators, metrics, and technologies discussed.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 81,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11872",
        "abs_url": "https://arxiv.org/abs/2512.11872",
        "pdf_url": "https://arxiv.org/pdf/2512.11872",
        "title": "WAM-Diff: A Masked Diffusion VLA Framework with MoE and Online Reinforcement Learning for Autonomous Driving",
        "authors": [
            "Mingwang Xu",
            "Jiahao Cui",
            "Feipeng Cai",
            "Hanlin Shang",
            "Zhihao Zhu",
            "Shan Luan",
            "Yifang Xu",
            "Neng Zhang",
            "Yaoyi Li",
            "Jia Cai",
            "Siyu Zhu"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
        "abstract": "End-to-end autonomous driving systems based on vision-language-action (VLA) models integrate multimodal sensor inputs and language instructions to generate planning and control signals. While autoregressive large language models and continuous diffusion policies are prevalent, the potential of discrete masked diffusion for trajectory generation remains largely unexplored. This paper presents WAM-Diff, a VLA framework that employs masked diffusion to iteratively refine a discrete sequence representing future ego-trajectories. Our approach features three key innovations: a systematic adaptation of masked diffusion for autonomous driving that supports flexible, non-causal decoding orders; scalable model capacity via a sparse MoE architecture trained jointly on motion prediction and driving-oriented visual question answering (VQA); and online reinforcement learning using Group Sequence Policy Optimization (GSPO) to optimize sequence-level driving rewards. Remarkably, our model achieves 91.0 PDMS on NAVSIM-v1 and 89.7 EPDMS on NAVSIM-v2, demonstrating the effectiveness of masked diffusion for autonomous driving. The approach provides a promising alternative to autoregressive and diffusion-based policies, supporting scenario-aware decoding strategies for trajectory generation. The code for this paper will be released publicly at: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 82,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11879",
        "abs_url": "https://arxiv.org/abs/2512.11879",
        "pdf_url": "https://arxiv.org/pdf/2512.11879",
        "title": "It's About Time: The Temporal and Modal Dynamics of Copilot Usage",
        "authors": [
            "Beatriz Costa-Gomes",
            "Sophia Chen",
            "Connie Hsueh",
            "Deborah Morgan",
            "Philipp Schoenegger",
            "Yash Shah",
            "Sam Way",
            "Yuki Zhu",
            "Timothé Adeline",
            "Michael Bhaskar",
            "Mustafa Suleyman",
            "Seth Spielman"
        ],
        "comments": "12 pages, 10 figures",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI)",
        "abstract": "We analyze 37.5 million deidentified conversations with Microsoft's Copilot between January and September 2025. Unlike prior analyses of AI usage, we focus not just on what people do with AI, but on how and when they do it. We find that how people use AI depends fundamentally on context and device type. On mobile, health is the dominant topic, which is consistent across every hour and every month we observed - with users seeking not just information but also advice. On desktop, the pattern is strikingly different: work and technology dominate during business hours, with \"Work and Career\" overtaking \"Technology\" as the top topic precisely between 8 a.m. and 5 p.m. These differences extend to temporal rhythms: programming queries spike on weekdays while gaming rises on weekends, philosophical questions climb during late-night hours, and relationship conversations surge on Valentine's Day. These patterns suggest that users have rapidly integrated AI into the full texture of their lives, as a work aid at their desks and a companion on their phones.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 83,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11881",
        "abs_url": "https://arxiv.org/abs/2512.11881",
        "pdf_url": "https://arxiv.org/pdf/2512.11881",
        "title": "Understanding Structural Representation in Foundation Models for Polymers",
        "authors": [
            "Nathaniel H. Park",
            "Eduardo Soares",
            "Victor Y. Shirasuna",
            "Tiffany J. Callahan",
            "Sara Capponi",
            "Emilio Vital Brazil"
        ],
        "comments": "",
        "subjects": "Soft Condensed Matter (cond-mat.soft); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "From the relative scarcity of training data to the lack of standardized benchmarks, the development of foundation models for polymers face significant and multi-faceted challenges. At the core, many of these issues are tied directly to the structural representation of polymers and here, we present a new foundation model using a SMILES-based polymer graph representation. This approach allows representation of critical polymer architectural features and connectivity that are not available in other SMILES-based representations. The developed polymer foundation model exhibited excellent performance on 28 different benchmark datasets. Critical evaluation of the developed representation against other variations in control experiments reveals this approach to be a highly performant method of representing polymers in language-based foundation models. These control experiments also reveal a strong invariance of all SMILES representations, with many variations achieving state-of-the-art or near state-of-the-art performance, including those which are chemically or semantically invalid. Examination of error sources and attention maps for the evaluated representations corroborate the findings of the control experiments, showing that chemistry language models based on SMILES interpolate over all sequence space for prediction tasks, not only those of semantically valid inputs. Overall, this work highlights the importance of control experiments as a check on human-imposed assumptions that can limit rational design of both chemistry foundation models and their underlying structural representations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 84,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11882",
        "abs_url": "https://arxiv.org/abs/2512.11882",
        "pdf_url": "https://arxiv.org/pdf/2512.11882",
        "title": "An Experience Report on a Pedagogically Controlled, Curriculum-Constrained AI Tutor for SE Education",
        "authors": [
            "Lucia Happe",
            "Dominik Fuchß",
            "Luca Hüttner",
            "Kai Marquardt",
            "Anne Koziolek"
        ],
        "comments": "11 pages, 4 figures, accepted for publication at ICSE 2026 SEET Track",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Software Engineering (cs.SE)",
        "abstract": "The integration of artificial intelligence (AI) into education continues to evoke both promise and skepticism. While past waves of technological optimism often fell short, recent advances in large language models (LLMs) have revived the vision of scalable, individualized tutoring. This paper presents the design and pilot evaluation of RockStartIT Tutor, an AI-powered assistant developed for a digital programming and computational thinking course within the RockStartIT initiative. Powered by GPT-4 via OpenAI's Assistant API, the tutor employs a novel prompting strategy and a modular, semantically tagged knowledge base to deliver context-aware, personalized, and curriculum-constrained support for secondary school students. We evaluated the system using the Technology Acceptance Model (TAM) with 13 students and teachers. Learners appreciated the low-stakes environment for asking questions and receiving scaffolded guidance. Educators emphasized the system's potential to reduce cognitive load during independent tasks and complement classroom teaching. Key challenges include prototype limitations, a small sample size, and the need for long-term studies with the target age group. Our findings highlight a pragmatic approach to AI integration that requires no model training, using structure and prompts to shape behavior. We position AI tutors not as teacher replacements but as enabling tools that extend feedback access, foster inquiry, and support what schools do best: help students learn.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 85,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11887",
        "abs_url": "https://arxiv.org/abs/2512.11887",
        "pdf_url": "https://arxiv.org/pdf/2512.11887",
        "title": "Advancing Autonomous Driving System Testing: Demands, Challenges, and Future Directions",
        "authors": [
            "Yihan Liao",
            "Jingyu Zhang",
            "Jacky Keung",
            "Yan Xiao",
            "Yurou Dai"
        ],
        "comments": "Accepted for publication in Information and Software Technology (IST)",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI)",
        "abstract": "Autonomous driving systems (ADSs) promise improved transportation efficiency and safety, yet ensuring their reliability in complex real-world environments remains a critical challenge. Effective testing is essential to validate ADS performance and reduce deployment risks. This study investigates current ADS testing practices for both modular and end-to-end systems, identifies key demands from industry practitioners and academic researchers, and analyzes the gaps between existing research and real-world requirements. We review major testing techniques and further consider emerging factors such as Vehicle-to-Everything (V2X) communication and foundation models, including large language models and vision foundation models, to understand their roles in enhancing ADS testing. We conducted a large-scale survey with 100 participants from both industry and academia. Survey questions were refined through expert discussions, followed by quantitative and qualitative analyses to reveal key trends, challenges, and unmet needs. Our results show that existing ADS testing techniques struggle to comprehensively evaluate real-world performance, particularly regarding corner case diversity, the simulation to reality gap, the lack of systematic testing criteria, exposure to potential attacks, practical challenges in V2X deployment, and the high computational cost of foundation model-based testing. By further analyzing participant responses together with 105 representative studies, we summarize the current research landscape and highlight major limitations. This study consolidates critical research gaps in ADS testing and outlines key future research directions, including comprehensive testing criteria, cross-model collaboration in V2X systems, cross-modality adaptation for foundation model-based testing, and scalable validation frameworks for large-scale ADS evaluation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 86,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11892",
        "abs_url": "https://arxiv.org/abs/2512.11892",
        "pdf_url": "https://arxiv.org/pdf/2512.11892",
        "title": "Should AI Become an Intergenerational Civil Right?",
        "authors": [
            "Jon Crowcroft",
            "Rute C. Sofia",
            "Dirk Trossen",
            "Vassilis Tsaoussidis"
        ],
        "comments": "",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI); Networking and Internet Architecture (cs.NI)",
        "abstract": "Artificial Intelligence (AI) is rapidly becoming a foundational layer of social, economic, and cognitive infrastructure. At the same time, the training and large-scale deployment of AI systems rely on finite and unevenly distributed energy, networking, and computational resources. This tension exposes a largely unexamined problem in current AI governance: while expanding access to AI is essential for social inclusion and equal opportunity, unconstrained growth in AI use risks unsustainable resource consumption, whereas restricting access threatens to entrench inequality and undermine basic rights. This paper argues that access to AI outputs largely derived from publicly produced knowledge should not be treated solely as a commercial service, but as a fundamental civil interest requiring explicit protection. We show that existing regulatory frameworks largely ignore the coupling between equitable access and resource constraints, leaving critical questions of fairness, sustainability, and long-term societal impact unresolved. To address this gap, we propose recognizing access to AI as an \\emph{Intergenerational Civil Right}, establishing a legal and ethical framework that simultaneously safeguards present-day inclusion and the rights of future generations. Beyond normative analysis, we explore how this principle can be technically realized. Drawing on emerging paradigms in IoT--Edge--Cloud computing, decentralized inference, and energy-aware networking, we outline technological trajectories and a strawman architecture for AI Delivery Networks that support equitable access under strict resource constraints. By framing AI as a shared social infrastructure rather than a discretionary market commodity, this work connects governance principles with concrete system design choices, offering a pathway toward AI deployment that is both socially just and environmentally sustainable.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 87,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11893",
        "abs_url": "https://arxiv.org/abs/2512.11893",
        "pdf_url": "https://arxiv.org/pdf/2512.11893",
        "title": "Beyond Automation: Rethinking Work, Creativity, and Governance in the Age of Generative AI",
        "authors": [
            "Haocheng Lin"
        ],
        "comments": "",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI)",
        "abstract": "The accelerating advancement of generative artificial intelligence (AI) systems is reshaping the nature, distribution and meaning of work, creativity, and economic security. This paper investigates four inter-related phenomena in the current AI era: (1) the evolving landscape of employment and the future of work; (2) the diverse patterns of AI adoption across socio-demographic groups, sectors, and geographies; (3) whether universal basic income (UBI) should become a compulsory policy response to the AI revolution; and (4) the implications of AI content policies and model behaviours for human creativity, wellbeing, and everyday decision-making. Furthermore, the paper tests the hypothesis that newer model generations may perform worse than their predecessors, and examines how users' interactions with AI systems may produce echo chambers through sycophantic model alignment. Using a mixed methodology that integrates labour market task-exposure modelling, sectoral diffusion mapping, policy-framework analysis, and qualitative discourse critique, this study develops a comprehensive framework for understanding the societal consequences of AI systems beyond productivity gains. It argues that to foster an inclusive, meaningful, and creative environment, policymakers must treat UBI as one dimension within a broader ecosystem of governance, skills development, creativity preservation, and model design. The paper concludes by outlining future research directions, including systematic evaluation of AI's creative performance across model generations, construction of a taxonomy of AI-usage distribution and equity, and formulation of governance criteria to balance content restrictions with creative freedom.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 88,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11919",
        "abs_url": "https://arxiv.org/abs/2512.11919",
        "pdf_url": "https://arxiv.org/pdf/2512.11919",
        "title": "A fine-grained look at causal effects in causal spaces",
        "authors": [
            "Junhyung Park",
            "Yuqing Zhou"
        ],
        "comments": "",
        "subjects": "Methodology (stat.ME); Artificial Intelligence (cs.AI); Statistics Theory (math.ST)",
        "abstract": "The notion of causal effect is fundamental across many scientific disciplines. Traditionally, quantitative researchers have studied causal effects at the level of variables; for example, how a certain drug dose (W) causally affects a patient's blood pressure (Y). However, in many modern data domains, the raw variables-such as pixels in an image or tokens in a language model-do not have the semantic structure needed to formulate meaningful causal questions. In this paper, we offer a more fine-grained perspective by studying causal effects at the level of events, drawing inspiration from probability theory, where core notions such as independence are first given for events and sigma-algebras, before random variables enter the picture. Within the measure-theoretic framework of causal spaces, a recently introduced axiomatisation of causality, we first introduce several binary definitions that determine whether a causal effect is present, as well as proving some properties of them linking causal effect to (in)dependence under an intervention measure. Further, we provide quantifying measures that capture the strength and nature of causal effects on events, and show that we can recover the common measures of treatment effect as special cases.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 89,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11921",
        "abs_url": "https://arxiv.org/abs/2512.11921",
        "pdf_url": "https://arxiv.org/pdf/2512.11921",
        "title": "Towards Accessible Physical AI: LoRA-Based Fine-Tuning of VLA Models for Real-World Robot Control",
        "authors": [
            "Abdullah Yahya Abdullah Omaisan",
            "Ibrahim Sheikh Mohamed"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
        "abstract": "Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in robotic manipulation,enabling robots to execute natural language commands through end-to-end learning from visual this http URL, deploying large-scale VLA models on affordable robotic platforms remains challenging due to computational constraints and the need for efficient adaptation to new robot embodiments. This paper presents an efficient fine-tuning methodology and real-world deployment analysis for adapting VLA models to low-cost robotic manipulation this http URL propose a resource-efficient fine-tuning strategy using Low-Rank Adaptation (LoRA) and quantization techniques that enable multi-billion parameter VLA models ( 3.1B parameters) to run on consumer-grade GPUs with 8GB VRAM. Our methodology addresses the critical challenge of adapting pre-trained VLA models to new robot embodiments with limited demonstration data, focusing on the trade-offs between frozen and unfrozen vision encoders. Through real-world deployment on the SO101 robotic arm for a button-pressing manipulation task, we demonstrate that our approach achieves effective manipulation performance while maintaining computational efficiency. We provide detailed analysis of deployment challenges, failure modes, and the relationship between training data quantity and real-world performance,trained on 200 demonstration episodes. Our results show that with proper fine-tuning methodology, VLA models can be successfully deployed on affordable robotic platforms,making advanced manipulation capabilities accessible beyond expensive research robots.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 90,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11922",
        "abs_url": "https://arxiv.org/abs/2512.11922",
        "pdf_url": "https://arxiv.org/pdf/2512.11922",
        "title": "Vibe Coding in Practice: Flow, Technical Debt, and Guidelines for Sustainable Use",
        "authors": [
            "Muhammad Waseem",
            "Aakash Ahmad",
            "Kai-Kristian Kemell",
            "Jussi Rasku",
            "Sami Lahti",
            "Kalle Mäkelä",
            "Pekka Abrahamsson"
        ],
        "comments": "10",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)",
        "abstract": "Vibe Coding (VC) is a form of software development assisted by generative AI, in which developers describe the intended functionality or logic via natural language prompts, and the AI system generates the corresponding source code. VC can be leveraged for rapid prototyping or developing the Minimum Viable Products (MVPs); however, it may introduce several risks throughout the software development life cycle. Based on our experience from several internally developed MVPs and a review of recent industry reports, this article analyzes the flow-debt tradeoffs associated with VC. The flow-debt trade-off arises when the seamless code generation occurs, leading to the accumulation of technical debt through architectural inconsistencies, security vulnerabilities, and increased maintenance overhead. These issues originate from process-level weaknesses, biases in model training data, a lack of explicit design rationale, and a tendency to prioritize quick code generation over human-driven iterative development. Based on our experiences, we identify and explain how current model, platform, and hardware limitations contribute to these issues, and propose countermeasures to address them, informing research and practice towards more sustainable VC approaches.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 91,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11927",
        "abs_url": "https://arxiv.org/abs/2512.11927",
        "pdf_url": "https://arxiv.org/pdf/2512.11927",
        "title": "Gene regulatory network inference algorithm based on spectral signed directed graph convolution",
        "authors": [
            "Rijie Xi",
            "Weikang Xu",
            "Wei Xiong",
            "Yuannong Ye",
            "Bin Zhao"
        ],
        "comments": "",
        "subjects": "Molecular Networks (q-bio.MN); Artificial Intelligence (cs.AI)",
        "abstract": "Accurately reconstructing Gene Regulatory Networks (GRNs) is crucial for understanding gene functions and disease mechanisms. Single-cell RNA sequencing (scRNA-seq) technology provides vast data for computational GRN reconstruction. Since GRNs are ideally modeled as signed directed graphs to capture activation/inhibition relationships, the most intuitive and reasonable approach is to design feature extractors based on the topological structure of GRNs to extract structural features, then combine them with biological characteristics for research. However, traditional spectral graph convolution struggles with this representation. Thus, we propose MSGRNLink, a novel framework that explicitly models GRNs as signed directed graphs and employs magnetic signed Laplacian convolution. Experiments across simulated and real datasets demonstrate that MSGRNLink outperforms all baseline models in AUROC. Parameter sensitivity analysis and ablation studies confirmed its robustness and the importance of each module. In a bladder cancer case study, MSGRNLink predicted more known edges and edge signs than benchmark models, further validating its biological relevance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 92,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11930",
        "abs_url": "https://arxiv.org/abs/2512.11930",
        "pdf_url": "https://arxiv.org/pdf/2512.11930",
        "title": "Evolutionary Reinforcement Learning based AI tutor for Socratic Interdisciplinary Instruction",
        "authors": [
            "Mei Jiang",
            "Haihai Shen",
            "Zhuo Luo",
            "Bingdong Li",
            "Wenjing Hong",
            "Ke Tang",
            "Aimin Zhou"
        ],
        "comments": "",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI)",
        "abstract": "Cultivating higher-order cognitive abilities -- such as knowledge integration, critical thinking, and creativity -- in modern STEM education necessitates a pedagogical shift from passive knowledge transmission to active Socratic construction. Although Large Language Models (LLMs) hold promise for STEM Interdisciplinary education, current methodologies employing Prompt Engineering (PE), Supervised Fine-tuning (SFT), or standard Reinforcement Learning (RL) often fall short of supporting this paradigm. Existing methods are hindered by three fundamental challenges: the inability to dynamically model latent student cognitive states; severe reward sparsity and delay inherent in long-term educational goals; and a tendency toward policy collapse lacking strategic diversity due to reliance on behavioral cloning. Recognizing the unobservability and dynamic complexity of these interactions, we formalize the Socratic Interdisciplinary Instructional Problem (SIIP) as a structured Partially Observable Markov Decision Process (POMDP), demanding simultaneous global exploration and fine-grained policy refinement. To this end, we propose ERL4SIIP, a novel Evolutionary Reinforcement Learning (ERL) framework specifically tailored for this domain. ERL4SIIP integrates: (1) a dynamic student simulator grounded in a STEM knowledge graph for latent state modeling; (2) a Hierarchical Reward Mechanism that decomposes long-horizon goals into dense signals; and (3) a LoRA-Division based optimization strategy coupling evolutionary algorithms for population-level global search with PPO for local gradient ascent.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 93,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11931",
        "abs_url": "https://arxiv.org/abs/2512.11931",
        "pdf_url": "https://arxiv.org/pdf/2512.11931",
        "title": "Mapping AI Risk Mitigations: Evidence Scan and Preliminary AI Risk Mitigation Taxonomy",
        "authors": [
            "Alexander K. Saeri",
            "Sophia Lloyd George",
            "Jess Graham",
            "Clelia D. Lacarriere",
            "Peter Slattery",
            "Michael Noetel",
            "Neil Thompson"
        ],
        "comments": "Access AI Risk Mitigation Database and Taxonomy at this https URL",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI)",
        "abstract": "Organizations and governments that develop, deploy, use, and govern AI must coordinate on effective risk mitigation. However, the landscape of AI risk mitigation frameworks is fragmented, uses inconsistent terminology, and has gaps in coverage. This paper introduces a preliminary AI Risk Mitigation Taxonomy to organize AI risk mitigations and provide a common frame of reference. The Taxonomy was developed through a rapid evidence scan of 13 AI risk mitigation frameworks published between 2023-2025, which were extracted into a living database of 831 AI risk mitigations. The mitigations were iteratively clustered & coded to create the Taxonomy. The preliminary AI Risk Mitigation Taxonomy organizes mitigations into four categories and 23 subcategories: (1) Governance & Oversight: Formal organizational structures and policy frameworks that establish human oversight mechanisms and decision protocols; (2) Technical & Security: Technical, physical, and engineering safeguards that secure AI systems and constrain model behaviors; (3) Operational Process: processes and management frameworks governing AI system deployment, usage, monitoring, incident handling, and validation; and (4) Transparency & Accountability: formal disclosure practices and verification mechanisms that communicate AI system information and enable external scrutiny. The rapid evidence scan and taxonomy construction also revealed several cases where terms like 'risk management' and 'red teaming' are used widely but refer to different responsible actors, actions, and mechanisms of action to reduce risk. This Taxonomy and associated mitigation database, while preliminary, offers a starting point for collation and synthesis of AI risk mitigations. It also offers an accessible, structured way for different actors in the AI ecosystem to discuss and coordinate action to reduce risks from AI.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 94,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11933",
        "abs_url": "https://arxiv.org/abs/2512.11933",
        "pdf_url": "https://arxiv.org/pdf/2512.11933",
        "title": "The Agentic Regulator: Risks for AI in Finance and a Proposed Agent-based Framework for Governance",
        "authors": [
            "Eren Kurshan",
            "Tucker Balch",
            "David Byrd"
        ],
        "comments": "",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Multiagent Systems (cs.MA); General Finance (q-fin.GN)",
        "abstract": "Generative and agentic artificial intelligence is entering financial markets faster than existing governance can adapt. Current model-risk frameworks assume static, well-specified algorithms and one-time validations; large language models and multi-agent trading systems violate those assumptions by learning continuously, exchanging latent signals, and exhibiting emergent behavior. Drawing on complex adaptive systems theory, we model these technologies as decentralized ensembles whose risks propagate along multiple time-scales. We then propose a modular governance architecture. The framework decomposes oversight into four layers of \"regulatory blocks\": (i) self-regulation modules embedded beside each model, (ii) firm-level governance blocks that aggregate local telemetry and enforce policy, (iii) regulator-hosted agents that monitor sector-wide indicators for collusive or destabilizing patterns, and (iv) independent audit blocks that supply third-party assurance. Eight design strategies enable the blocks to evolve as fast as the models they police. A case study on emergent spoofing in multi-agent trading shows how the layered controls quarantine harmful behavior in real time while preserving innovation. The architecture remains compatible with today's model-risk rules yet closes critical observability and control gaps, providing a practical path toward resilient, adaptive AI governance in financial systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 95,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11934",
        "abs_url": "https://arxiv.org/abs/2512.11934",
        "pdf_url": "https://arxiv.org/pdf/2512.11934",
        "title": "Unveiling User Perceptions in the Generative AI Era: A Sentiment-Driven Evaluation of AI Educational Apps' Role in Digital Transformation of e-Teaching",
        "authors": [
            "Adeleh Mazaherian",
            "Erfan Nourbakhsh"
        ],
        "comments": "6 pages, 4 figures",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI)",
        "abstract": "The rapid integration of generative artificial intelligence into education has driven digital transformation in e-teaching, yet user perceptions of AI educational apps remain underexplored. This study performs a sentiment-driven evaluation of user reviews from top AI ed-apps on the Google Play Store to assess efficacy, challenges, and pedagogical implications. Our pipeline involved scraping app data and reviews, RoBERTa for binary sentiment classification, GPT-4o for key point extraction, and GPT-5 for synthesizing top positive/negative themes. Apps were categorized into seven types (e.g., homework helpers, math solvers, language tools), with overlaps reflecting multifunctional designs. Results indicate predominantly positive sentiments, with homework apps like Edu AI (95.9% positive) and this http URL (92.7%) leading in accuracy, speed, and personalization, while language/LMS apps (e.g., Teacher AI at 21.8% positive) lag due to instability and limited features. Positives emphasize efficiency in brainstorming, problem-solving, and engagement; negatives center on paywalls, inaccuracies, ads, and glitches. Trends show that homework helpers outperform specialized tools, highlighting AI's democratizing potential amid risks of dependency and inequity. The discussion proposes future ecosystems with hybrid AI-human models, VR/AR for immersive learning, and a roadmap for developers (adaptive personalization) and policymakers (monetization regulation for inclusivity). This underscores generative AI's role in advancing e-teaching by enabling ethical refinements that foster equitable, innovative environments. The full dataset is available here(this https URL).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 96,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11943",
        "abs_url": "https://arxiv.org/abs/2512.11943",
        "pdf_url": "https://arxiv.org/pdf/2512.11943",
        "title": "How AI Agents Follow the Herd of AI? Network Effects, History, and Machine Optimism",
        "authors": [
            "Yu Liu",
            "Wenwen Li",
            "Yifan Dou",
            "Guangnan Ye"
        ],
        "comments": "7 pages, 5 figures",
        "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI); General Economics (econ.GN)",
        "abstract": "Understanding decision-making in multi-AI-agent frameworks is crucial for analyzing strategic interactions in network-effect-driven contexts. This study investigates how AI agents navigate network-effect games, where individual payoffs depend on peer participatio--a context underexplored in multi-agent systems despite its real-world prevalence. We introduce a novel workflow design using large language model (LLM)-based agents in repeated decision-making scenarios, systematically manipulating price trajectories (fixed, ascending, descending, random) and network-effect strength. Our key findings include: First, without historical data, agents fail to infer equilibrium. Second, ordered historical sequences (e.g., escalating prices) enable partial convergence under weak network effects but strong effects trigger persistent \"AI optimism\"--agents overestimate participation despite contradictory evidence. Third, randomized history disrupts convergence entirely, demonstrating that temporal coherence in data shapes LLMs' reasoning, unlike humans. These results highlight a paradigm shift: in AI-mediated systems, equilibrium outcomes depend not just on incentives, but on how history is curated, which is impossible for human.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 97,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11944",
        "abs_url": "https://arxiv.org/abs/2512.11944",
        "pdf_url": "https://arxiv.org/pdf/2512.11944",
        "title": "A Review of Learning-Based Motion Planning: Toward a Data-Driven Optimal Control Approach",
        "authors": [
            "Jia Hu",
            "Yang Chang",
            "Haoran Wang"
        ],
        "comments": "34 pages, 11 figures",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
        "abstract": "Motion planning for high-level autonomous driving is constrained by a fundamental trade-off between the transparent, yet brittle, nature of pipeline methods and the adaptive, yet opaque, \"black-box\" characteristics of modern learning-based systems. This paper critically synthesizes the evolution of the field -- from pipeline methods through imitation learning, reinforcement learning, and generative AI -- to demonstrate how this persistent dilemma has hindered the development of truly trustworthy systems. To resolve this impasse, we conduct a comprehensive review of learning-based motion planning methods. Based on this review, we outline a data-driven optimal control paradigm as a unifying framework that synergistically integrates the verifiable structure of classical control with the adaptive capacity of machine learning, leveraging real-world data to continuously refine key components such as system dynamics, cost functions, and safety constraints. We explore this framework's potential to enable three critical next-generation capabilities: \"Human-Centric\" customization, \"Platform-Adaptive\" dynamics adaptation, and \"System Self-Optimization\" via self-tuning. We conclude by proposing future research directions based on this paradigm, aimed at developing intelligent transportation systems that are simultaneously safe, interpretable, and capable of human-like autonomy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 98,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11946",
        "abs_url": "https://arxiv.org/abs/2512.11946",
        "pdf_url": "https://arxiv.org/pdf/2512.11946",
        "title": "Data-Driven Global Sensitivity Analysis for Engineering Design Based on Individual Conditional Expectations",
        "authors": [
            "Pramudita Satria Palar",
            "Paul Saves",
            "Rommel G. Regis",
            "Koji Shimoyama",
            "Shigeru Obayashi",
            "Nicolas Verstaevel",
            "Joseph Morlier"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "Explainable machine learning techniques have gained increasing attention in engineering applications, especially in aerospace design and analysis, where understanding how input variables influence data-driven models is essential. Partial Dependence Plots (PDPs) are widely used for interpreting black-box models by showing the average effect of an input variable on the prediction. However, their global sensitivity metric can be misleading when strong interactions are present, as averaging tends to obscure interaction effects. To address this limitation, we propose a global sensitivity metric based on Individual Conditional Expectation (ICE) curves. The method computes the expected feature importance across ICE curves, along with their standard deviation, to more effectively capture the influence of interactions. We provide a mathematical proof demonstrating that the PDP-based sensitivity is a lower bound of the proposed ICE-based metric under truncated orthogonal polynomial expansion. In addition, we introduce an ICE-based correlation value to quantify how interactions modify the relationship between inputs and the output. Comparative evaluations were performed on three cases: a 5-variable analytical function, a 5-variable wind-turbine fatigue problem, and a 9-variable airfoil aerodynamics case, where ICE-based sensitivity was benchmarked against PDP, SHapley Additive exPlanations (SHAP), and Sobol' indices. The results show that ICE-based feature importance provides richer insights than the traditional PDP-based approach, while visual interpretations from PDP, ICE, and SHAP complement one another by offering multiple perspectives.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 99,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11979",
        "abs_url": "https://arxiv.org/abs/2512.11979",
        "pdf_url": "https://arxiv.org/pdf/2512.11979",
        "title": "Designing The Internet of Agents: A Framework for Trustworthy, Transparent, and Collaborative Human-Agent Interaction (HAX)",
        "authors": [
            "Marc Scibelli",
            "Krystelle Gonzalez Papaux",
            "Julia Valenti",
            "Srishti Kush"
        ],
        "comments": "",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)",
        "abstract": "The rise of generative and autonomous agents marks a fundamental shift in computing, demanding a rethinking of how humans collaborate with probabilistic, partially autonomous systems. We present the Human-AI-Experience (HAX) framework, a comprehensive, three-phase approach that establishes design foundations for trustworthy, transparent, and collaborative agentic interaction. HAX integrates behavioral heuristics, a schema-driven SDK enforcing structured and safe outputs, and a behavioral proxy concept that orchestrates agent activity to reduce cognitive load. A validated catalog of mixed-initiative design patterns further enables intent preview, iterative alignment, trust repair, and multi-agent narrative coherence. Grounded in Time, Interaction, and Performance (TIP) theory, HAX reframes multi-agent systems as colleagues, offering the first end-to-end framework that bridges trust theory, interface design, and infrastructure for the emerging Internet of Agents.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 100,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.11984",
        "abs_url": "https://arxiv.org/abs/2512.11984",
        "pdf_url": "https://arxiv.org/pdf/2512.11984",
        "title": "Evidence-Driven Decision Support for AI Model Selection in Research Software Engineering",
        "authors": [
            "Alireza Joonbakhsh",
            "Alireza Rostami",
            "AmirMohammad Kamalinia",
            "Ali Nazeri",
            "Farshad Khunjush",
            "Bedir Tekinerdogan",
            "Siamak Farshidi"
        ],
        "comments": "",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)",
        "abstract": "The rapid proliferation of artificial intelligence (AI) models and methods presents growing challenges for research software engineers and researchers who must select, integrate, and maintain appropriate models within complex research workflows. Model selection is often performed in an ad hoc manner, relying on fragmented metadata and individual expertise, which can undermine reproducibility, transparency, and overall research software quality. This work proposes a structured and evidence-driven approach to support AI model selection that aligns with both technical and contextual requirements. We conceptualize AI model selection as a Multi-Criteria Decision-Making (MCDM) problem and introduce an evidence-based decision-support framework that integrates automated data collection pipelines, a structured knowledge graph, and MCDM principles. Following the Design Science Research methodology, the proposed framework (ModelSelect) is empirically validated through 50 real-world case studies and comparative experiments against leading generative AI systems. The evaluation results show that ModelSelect produces reliable, interpretable, and reproducible recommendations that closely align with expert reasoning. Across the case studies, the framework achieved high coverage and strong rationale alignment in both model and library recommendation tasks, performing comparably to generative AI assistants while offering superior traceability and consistency. By framing AI model selection as an MCDM problem, this work establishes a rigorous foundation for transparent and reproducible decision support in research software engineering. The proposed framework provides a scalable and explainable pathway for integrating empirical evidence into AI model recommendation processes, ultimately improving the quality and robustness of research software decision-making.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 101,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12008",
        "abs_url": "https://arxiv.org/abs/2512.12008",
        "pdf_url": "https://arxiv.org/pdf/2512.12008",
        "title": "Hold Onto That Thought: Assessing KV Cache Compression On Reasoning",
        "authors": [
            "Minghui Liu",
            "Aadi Palnitkar",
            "Tahseen Rabbani",
            "Hyunwoo Jae",
            "Kyle Rui Sang",
            "Dixi Yao",
            "Shayan Shabihi",
            "Fuheng Zhao",
            "Tian Li",
            "Ce Zhang",
            "Furong Huang",
            "Kunpeng Zhang"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Performance (cs.PF)",
        "abstract": "Large language models (LLMs) have demonstrated remarkable performance on long-context tasks, but are often bottlenecked by memory constraints. Namely, the KV cache, which is used to significantly speed up attention computations, grows linearly with context length. A suite of compression algorithms has been introduced to alleviate cache growth by evicting unimportant tokens. However, several popular strategies are targeted towards the prefill phase, i.e., processing long prompt context, and their performance is rarely assessed on reasoning tasks requiring long decoding. In particular, short but complex prompts, such as those in benchmarks like GSM8K and MATH500, often benefit from multi-step reasoning and self-reflection, resulting in thinking sequences thousands of tokens long. In this work, we benchmark the performance of several popular compression strategies on long-reasoning tasks. For the non-reasoning Llama-3.1-8B-Instruct, we determine that no singular strategy fits all, and that performance is heavily influenced by dataset type. However, we discover that H2O and our decoding-enabled variant of SnapKV are dominant strategies for reasoning models, indicating the utility of heavy-hitter tracking for reasoning traces. We also find that eviction strategies at low budgets can produce longer reasoning traces, revealing a tradeoff between cache size and inference costs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 102,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12045",
        "abs_url": "https://arxiv.org/abs/2512.12045",
        "pdf_url": "https://arxiv.org/pdf/2512.12045",
        "title": "AI as a Teaching Partner: Early Lessons from Classroom Codesign with Secondary Teachers",
        "authors": [
            "Alex Liu",
            "Lief Esbenshade",
            "Shawon Sarkar",
            "Zewei",
            "Tian",
            "Min Sun",
            "Zachary Zhang",
            "Thomas Han",
            "Yulia Lapicus",
            "Kevin He"
        ],
        "comments": "",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)",
        "abstract": "This report presents a comprehensive account of the Colleague AI Classroom pilot, a collaborative design (co-design) study that brought generative AI technology directly into real classrooms. In this study, AI functioned as a third agent, an active participant that mediated feedback, supported inquiry, and extended teachers' instructional reach while preserving human judgment and teacher authority. Over seven weeks in spring 2025, 21 in-service teachers from four Washington State public school districts and one independent school integrated four AI-powered features of the Colleague AI Classroom into their instruction: Teaching Aide, Assessment and AI Grading, AI Tutor, and Student Growth Insights. More than 600 students in grades 6-12 used the platform in class at the direction of their teachers, who designed and facilitated the AI activities. During the Classroom pilot, teachers were co-design partners: they planned activities, implemented them with students, and provided weekly reflections on AI's role in classroom settings. The teachers' feedback guided iterative improvements for Colleague AI. The research team captured rich data through surveys, planning and reflection forms, group meetings, one-on-one interviews, and platform usage logs to understand where AI adds instructional value and where it requires refinement.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 103,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12063",
        "abs_url": "https://arxiv.org/abs/2512.12063",
        "pdf_url": "https://arxiv.org/pdf/2512.12063",
        "title": "Instruction-Tuning Open-Weight Language Models for BPMN Model Generation",
        "authors": [
            "Gökberk Çelikmasat",
            "Atay Özgövde",
            "Fatma Başak Aydemir"
        ],
        "comments": "Preprint. Under preparation for journal submission",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)",
        "abstract": "Domain models are central to software engineering, as they enable a shared understanding, guide implementation, and support automated analyses and model-driven development. Yet, despite these benefits, practitioners often skip modeling because it is time-consuming and demands scarce expertise. We address this barrier by investigating whether open-weight large language models, adapted via instruction tuning, can generate high-quality BPMN process models directly from natural language descriptions in a cost-effective and privacy-preserving way. We introduce InstruBPM, a reproducible approach that prepares paired text-diagram data and instruction tunes an open source large language model with parameter-efficient fine-tuning and quantization for on-prem deployment. We evaluate the tuned model through complementary perspectives: (i) text/code similarity using BLEU, ROUGE-L, and METEOR, (ii) structural fidelity using Relative Graph Edit Distance, (iii) guidelines conformance using external tool checks, and (iv) a small expert review. Using a curated subset of a multi-domain BPMN dataset, we compare the tuned model with untuned open-weight baselines and strong proprietary models under consistent prompting regimes. Our compact tuned model outperforms all baselines across sequence and structural metrics while requiring substantially fewer resources; guideline analysis and expert feedback further indicate that the generated diagrams largely follow BPMN best practices and are useful starting points that reduce modeling effort. Overall, instruction tuning improves structural accuracy and robustness compared to untuned baselines and reduces reliance on heavy prompt scaffolding. We publicly share the trained models and scripts to support reproducibility and further research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 104,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12066",
        "abs_url": "https://arxiv.org/abs/2512.12066",
        "pdf_url": "https://arxiv.org/pdf/2512.12066",
        "title": "The Instability of Safety: How Random Seeds and Temperature Expose Inconsistent LLM Refusal Behavior",
        "authors": [
            "Erik Larsen"
        ],
        "comments": "14 pages, 7 figures, 6 tables. Code and data available at this https URL",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Current safety evaluations of large language models rely on single-shot testing, implicitly assuming that model responses are deterministic and representative of the model's safety alignment. We challenge this assumption by investigating the stability of safety refusal decisions across random seeds and temperature settings. Testing four instruction-tuned models from three families (Llama 3.1 8B, Qwen 2.5 7B, Qwen 3 8B, Gemma 3 12B) on 876 harmful prompts across 20 different sampling configurations (4 temperatures x 5 random seeds), we find that 18-28% of prompts exhibit decision flips--the model refuses in some configurations but complies in others--depending on the model. Our Safety Stability Index (SSI) reveals that higher temperatures significantly reduce decision stability (Friedman chi-squared = 44.71, p < 0.001), with mean SSI dropping from 0.951 at temperature 0.0 to 0.896 at temperature 1.0. We validate our findings across all model families using Claude 3.5 Haiku as a unified external judge, achieving 89.0% inter-judge agreement with our primary Llama 70B judge (Cohen's kappa = 0.62). These findings demonstrate that single-shot safety evaluations are insufficient for reliable safety assessment. We show that single-shot evaluation agrees with multi-sample ground truth only 92.4% of the time, and recommend using at least 3 samples per prompt for reliable safety assessment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 105,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12069",
        "abs_url": "https://arxiv.org/abs/2512.12069",
        "pdf_url": "https://arxiv.org/pdf/2512.12069",
        "title": "Rethinking Jailbreak Detection of Large Vision Language Models with Representational Contrastive Scoring",
        "authors": [
            "Peichun Hua",
            "Hao Li",
            "Shanghao Shi",
            "Zhiyuan Yu",
            "Ning Zhang"
        ],
        "comments": "40 pages, 13 figures",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Large Vision-Language Models (LVLMs) are vulnerable to a growing array of multimodal jailbreak attacks, necessitating defenses that are both generalizable to novel threats and efficient for practical deployment. Many current strategies fall short, either targeting specific attack patterns, which limits generalization, or imposing high computational overhead. While lightweight anomaly-detection methods offer a promising direction, we find that their common one-class design tends to confuse novel benign inputs with malicious ones, leading to unreliable over-rejection. To address this, we propose Representational Contrastive Scoring (RCS), a framework built on a key insight: the most potent safety signals reside within the LVLM's own internal representations. Our approach inspects the internal geometry of these representations, learning a lightweight projection to maximally separate benign and malicious inputs in safety-critical layers. This enables a simple yet powerful contrastive score that differentiates true malicious intent from mere novelty. Our instantiations, MCD (Mahalanobis Contrastive Detection) and KCD (K-nearest Contrastive Detection), achieve state-of-the-art performance on a challenging evaluation protocol designed to test generalization to unseen attack types. This work demonstrates that effective jailbreak detection can be achieved by applying simple, interpretable statistical methods to the appropriate internal representations, offering a practical path towards safer LVLM deployment. Our code is available on Github this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 106,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12081",
        "abs_url": "https://arxiv.org/abs/2512.12081",
        "pdf_url": "https://arxiv.org/pdf/2512.12081",
        "title": "Congestion Reduction in EV Charger Placement Using Traffic Equilibrium Models",
        "authors": [
            "Semih Kara",
            "Yasin Sonmez",
            "Can Kizilkale",
            "Alex Kurzhanskiy",
            "Nuno C. Martins",
            "Murat Arcak"
        ],
        "comments": "",
        "subjects": "Systems and Control (eess.SY); Artificial Intelligence (cs.AI); Social and Information Networks (cs.SI); Optimization and Control (math.OC)",
        "abstract": "Growing EV adoption can worsen traffic conditions if chargers are sited without regard to their impact on congestion. We study how to strategically place EV chargers to reduce congestion using two equilibrium models: one based on congestion games and one based on an atomic queueing simulation. We apply both models within a scalable greedy station-placement algorithm. Experiments show that this greedy scheme yields optimal or near-optimal congestion outcomes in realistic networks, even though global optimality is not guaranteed as we show with a counterexample. We also show that the queueing-based approach yields more realistic results than the congestion-game model, and we present a unified methodology that calibrates congestion delays from queue simulation and solves equilibrium in link-space.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 107,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12109",
        "abs_url": "https://arxiv.org/abs/2512.12109",
        "pdf_url": "https://arxiv.org/pdf/2512.12109",
        "title": "A neuro-symbolic framework for accountability in public-sector AI",
        "authors": [
            "Allen Daniel Sunny"
        ],
        "comments": "Master's thesis, University of Maryland, College Park (2025)",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI); Logic in Computer Science (cs.LO)",
        "abstract": "Automated eligibility systems increasingly determine access to essential public benefits, but the explanations they generate often fail to reflect the legal rules that authorize those decisions. This thesis develops a legally grounded explainability framework that links system-generated decision justifications to the statutory constraints of CalFresh, California's Supplemental Nutrition Assistance Program. The framework combines a structured ontology of eligibility requirements derived from the state's Manual of Policies and Procedures (MPP), a rule extraction pipeline that expresses statutory logic in a verifiable formal representation, and a solver-based reasoning layer to evaluate whether the explanation aligns with governing law. Case evaluations demonstrate the framework's ability to detect legally inconsistent explanations, highlight violated eligibility rules, and support procedural accountability by making the basis of automated determinations traceable and contestable.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 108,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12121",
        "abs_url": "https://arxiv.org/abs/2512.12121",
        "pdf_url": "https://arxiv.org/pdf/2512.12121",
        "title": "MixtureKit: A General Framework for Composing, Training, and Visualizing Mixture-of-Experts Models",
        "authors": [
            "Ahmad Chamma",
            "Omar El Herraoui",
            "Guokan Shang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "We introduce MixtureKit, a modular open-source framework for constructing, training, and analyzing Mixture-of-Experts (MoE) models from arbitrary pre-trained or fine-tuned models. MixtureKit currently supports three complementary methods: (i) \\emph{Traditional MoE}, which uses a single router per transformer block to select experts, (ii) \\emph{BTX} (Branch-Train-Mix), which introduces separate routers for each specified sub-layer enabling fine-grained token routing, and (iii) \\emph{BTS} (Branch-Train-Stitch), which keeps experts fully intact and introduces trainable stitch layers for controlled information exchange between hub and experts. MixtureKit automatically modifies the model configuration, patches decoder and causal LM classes, and saves a unified checkpoint ready for inference or fine-tuning. We further provide a visualization interface to inspect per-token routing decisions, expert weight distributions, and layer-wise contributions. Experiments with multilingual code-switched data (e.g. Arabic-Latin) show that a BTX-based model trained using MixtureKit can outperform baseline dense models on multiple benchmarks. We release MixtureKit as a practical foundation for research and development of MoE-based systems across diverse domains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 109,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12135",
        "abs_url": "https://arxiv.org/abs/2512.12135",
        "pdf_url": "https://arxiv.org/pdf/2512.12135",
        "title": "BaRISTA: Brain Scale Informed Spatiotemporal Representation of Human Intracranial Neural Activity",
        "authors": [
            "Lucine L. Oganesian",
            "Saba Hashemi",
            "Maryam M. Shanechi"
        ],
        "comments": "Published at the 39th Annual Conference on Neural Information Processing Systems (NeurIPS 2025). Code available at this https URL",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Neurons and Cognition (q-bio.NC)",
        "abstract": "Intracranial recordings have opened a unique opportunity to simultaneously measure activity across multiregional networks in the human brain. Recent works have focused on developing transformer-based neurofoundation models of such recordings that can generalize across subjects and datasets. However, these recordings exhibit highly complex spatiotemporal interactions across diverse spatial scales, from the single-channel scale to the scale of brain regions. As such, there remain critical open questions regarding how best to encode spatial information and how to design self-supervision tasks that enable the learning of brain network patterns and enhance downstream decoding performance using such high-dimensional, multiregional recordings. To allow for exploring these questions, we propose a new spatiotemporal transformer model of multiregional neural activity and a corresponding self-supervised masked latent reconstruction task, designed to enable flexibility in the spatial scale used for token encoding and masking. Applying this model on publicly available multiregional intracranial electrophysiology (iEEG) data, we demonstrate that adjusting the spatial scale for both token encoding and masked reconstruction significantly impacts downstream decoding. Further, we find that spatial encoding at larger scales than channel-level encoding, which is commonly used in existing iEEG transformer models, improves downstream decoding performance. Finally, we demonstrate that our method allows for region-level token encoding while also maintaining accurate channel-level neural reconstruction. Taken together, our modeling framework enables exploration of the spatial scales used for token encoding and masking, reveals their importance towards self-supervised pretraining of neurofoundation models of multiregional human brain activity, and enhances downstream decoding performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 110,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12167",
        "abs_url": "https://arxiv.org/abs/2512.12167",
        "pdf_url": "https://arxiv.org/pdf/2512.12167",
        "title": "Extending the Context of Pretrained LLMs by Dropping Their Positional Embeddings",
        "authors": [
            "Yoav Gelberg",
            "Koshi Eguchi",
            "Takuya Akiba",
            "Edoardo Cetin"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "So far, expensive finetuning beyond the pretraining sequence length has been a requirement for effectively extending the context of language models (LM). In this work, we break this key bottleneck by Dropping the Positional Embeddings of LMs after training (DroPE). Our simple method is motivated by three key theoretical and empirical observations. First, positional embeddings (PEs) serve a crucial role during pretraining, providing an important inductive bias that significantly facilitates convergence. Second, over-reliance on this explicit positional information is also precisely what prevents test-time generalization to sequences of unseen length, even when using popular PE-scaling methods. Third, positional embeddings are not an inherent requirement of effective language modeling and can be safely removed after pretraining, following a short recalibration phase. Empirically, DroPE yields seamless zero-shot context extension without any long-context finetuning, quickly adapting pretrained LMs without compromising their capabilities in the original training context. Our findings hold across different models and dataset sizes, far outperforming previous specialized architectures and established rotary positional embedding scaling methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 111,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12168",
        "abs_url": "https://arxiv.org/abs/2512.12168",
        "pdf_url": "https://arxiv.org/pdf/2512.12168",
        "title": "Diffusion Language Model Inference with Monte Carlo Tree Search",
        "authors": [
            "Zheng Huang",
            "Kiran Ramnath",
            "Yueyan Chen",
            "Aosong Feng",
            "Sangmin Woo",
            "Balasubramaniam Srinivasan",
            "Zhichao Xu",
            "Kang Zhou",
            "Shuai Wang",
            "Haibo Ding",
            "Lin Lee Cheong"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Diffusion language models (DLMs) have recently emerged as a compelling alternative to autoregressive generation, offering parallel generation and improved global coherence. During inference, DLMs generate text by iteratively denoising masked sequences in parallel; however, determining which positions to unmask and which tokens to commit forms a large combinatorial search problem. Existing inference methods approximate this search using heuristics, which often yield suboptimal decoding paths; other approaches instead rely on additional training to guide token selection. To introduce a principled search mechanism for DLMs inference, we introduce MEDAL, a framework that integrates Monte Carlo Tree SEarch initialization for Diffusion LAnguage Model inference. We employ Monte Carlo Tree Search at the initialization stage to explore promising unmasking trajectories, providing a robust starting point for subsequent refinement. This integration is enabled by restricting the search space to high-confidence actions and prioritizing token choices that improve model confidence over remaining masked positions. Across multiple benchmarks, MEDAL achieves up to 22.0% improvement over existing inference strategies, establishing a new paradigm for search-based inference in diffusion language models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 112,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12201",
        "abs_url": "https://arxiv.org/abs/2512.12201",
        "pdf_url": "https://arxiv.org/pdf/2512.12201",
        "title": "Epistemoverse: Toward an AI-Driven Knowledge Metaverse for Intellectual Heritage Preservation",
        "authors": [
            "Predrag K. Nikolić",
            "Robert Prentner"
        ],
        "comments": "7 pages, 7 figures, presented at SIGGRAPH VRCAI 25",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)",
        "abstract": "Large language models (LLMs) have often been characterized as \"stochastic parrots\" that merely reproduce fragments of their training data. This study challenges that assumption by demonstrating that, when placed in an appropriate dialogical context, LLMs can develop emergent conceptual structures and exhibit interaction-driven (re-)structuring of cognitive interfaces and reflective question-asking. Drawing on the biological principle of cloning and Socrates' maieutic method, we analyze authentic philosophical debates generated among AI-reincarnated philosophers within the interactive art installations of the Syntropic Counterpoints project. By engaging digital counterparts of Aristotle, Nietzsche, Machiavelli, and Sun Tzu in iterative discourse, the study reveals how machine dialogue can give rise to inferential coherence, reflective questioning, and creative synthesis. Based on these findings, we propose the concept of the Epistemoverse--a metaverse of knowledge where human and machine cognition intersect to preserve, reinterpret, and extend intellectual heritage through AI-driven interaction. This framework positions virtual and immersive environments as new spaces for epistemic exchange, digital heritage, and collaborative creativity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 113,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12207",
        "abs_url": "https://arxiv.org/abs/2512.12207",
        "pdf_url": "https://arxiv.org/pdf/2512.12207",
        "title": "Not All Transparency Is Equal: Source Presentation Effects on Attention, Interaction, and Persuasion in Conversational Search",
        "authors": [
            "Jiangen He",
            "Jiqun Liu"
        ],
        "comments": "CHIIR 2026",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)",
        "abstract": "Conversational search systems increasingly provide source citations, yet how citation or source presentation formats influence user engagement remains unclear. We conducted a crowdsourcing user experiment with 394 participants comparing four source presentation designs that varied citation visibility and accessibility: collapsible lists, hover cards, footer lists, and aligned this http URL-visibility interfaces generated substantially more hovering on sources, though clicking remained infrequent across all conditions. While interface design showed limited effects on user experience and perception measures, it significantly influenced knowledge, interest, and agreement changes. High-visibility interfaces initially reduced knowledge gain and interest, but these positive effects emerged with increasing source usage. The sidebar condition uniquely increased agreement change. Our findings demonstrate that source presentation alone may not enhance engagement and can even reduce it when insufficient sources are provided.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 114,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12211",
        "abs_url": "https://arxiv.org/abs/2512.12211",
        "pdf_url": "https://arxiv.org/pdf/2512.12211",
        "title": "Measuring What Matters: Scenario-Driven Evaluation for Trajectory Predictors in Autonomous Driving",
        "authors": [
            "Longchao Da",
            "David Isele",
            "Hua Wei",
            "Manish Saroya"
        ],
        "comments": "9 Pages, 8 Figures",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Systems and Control (eess.SY)",
        "abstract": "Being able to anticipate the motion of surrounding agents is essential for the safe operation of autonomous driving systems in dynamic situations. While various methods have been proposed for trajectory prediction, the current evaluation practices still rely on error-based metrics (e.g., ADE, FDE), which reveal the accuracy from a post-hoc view but ignore the actual effect the predictor brings to the self-driving vehicles (SDVs), especially in complex interactive scenarios: a high-quality predictor not only chases accuracy, but should also captures all possible directions a neighbor agent might move, to support the SDVs' cautious decision-making. Given that the existing metrics hardly account for this standard, in our work, we propose a comprehensive pipeline that adaptively evaluates the predictor's performance by two dimensions: accuracy and diversity. Based on the criticality of the driving scenario, these two dimensions are dynamically combined and result in a final score for the predictor's performance. Extensive experiments on a closed-loop benchmark using real-world datasets show that our pipeline yields a more reasonable evaluation than traditional metrics by better reflecting the correlation of the predictors' evaluation with the autonomous vehicles' driving performance. This evaluation pipeline shows a robust way to select a predictor that potentially contributes most to the SDV's driving performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 115,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12216",
        "abs_url": "https://arxiv.org/abs/2512.12216",
        "pdf_url": "https://arxiv.org/pdf/2512.12216",
        "title": "Training Versatile Coding Agents in Synthetic Environments",
        "authors": [
            "Yiqi Zhu",
            "Apurva Gandhi",
            "Graham Neubig"
        ],
        "comments": "",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Prior works on training software engineering agents have explored utilizing existing resources such as issues on GitHub repositories to construct software engineering tasks and corresponding test suites. These approaches face two key limitations: (1) their reliance on pre-existing GitHub repositories offers limited flexibility, and (2) their primary focus on issue resolution tasks restricts their applicability to the much wider variety of tasks a software engineer must handle. To overcome these challenges, we introduce SWE-Playground, a novel pipeline for generating environments and trajectories which supports the training of versatile coding agents. Unlike prior efforts, SWE-Playground synthetically generates projects and tasks from scratch with strong language models and agents, eliminating reliance on external data sources. This allows us to tackle a much wider variety of coding tasks, such as reproducing issues by generating unit tests and implementing libraries from scratch. We demonstrate the effectiveness of this approach on three distinct benchmarks, and results indicate that SWE-Playground produces trajectories with dense training signal, enabling agents to reach comparable performance with significantly fewer trajectories than previous works.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 116,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12238",
        "abs_url": "https://arxiv.org/abs/2512.12238",
        "pdf_url": "https://arxiv.org/pdf/2512.12238",
        "title": "Semantic Distance Measurement based on Multi-Kernel Gaussian Processes",
        "authors": [
            "Yinzhu Cheng",
            "Haihua Xie",
            "Yaqing Wang",
            "Miao He",
            "Mingming Sun"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Semantic distance measurement is a fundamental problem in computational linguistics, providing a quantitative characterization of similarity or relatedness between text segments, and underpinning tasks such as text retrieval and text classification. From a mathematical perspective, a semantic distance can be viewed as a metric defined on a space of texts or on a representation space derived from them. However, most classical semantic distance methods are essentially fixed, making them difficult to adapt to specific data distributions and task requirements. In this paper, a semantic distance measure based on multi-kernel Gaussian processes (MK-GP) was proposed. The latent semantic function associated with texts was modeled as a Gaussian process, with its covariance function given by a combined kernel combining Matérn and polynomial components. The kernel parameters were learned automatically from data under supervision, rather than being hand-crafted. This semantic distance was instantiated and evaluated in the context of fine-grained sentiment classification with large language models under an in-context learning (ICL) setup. The experimental results demonstrated the effectiveness of the proposed measure.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 117,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12245",
        "abs_url": "https://arxiv.org/abs/2512.12245",
        "pdf_url": "https://arxiv.org/pdf/2512.12245",
        "title": "Adversarially Probing Cross-Family Sound Symbolism in 27 Languages",
        "authors": [
            "Anika Sharma",
            "Tianyi Niu",
            "Emma Wrenn",
            "Shashank Srivastava"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "The phenomenon of sound symbolism, the non-arbitrary mapping between word sounds and meanings, has long been demonstrated through anecdotal experiments like Bouba Kiki, but rarely tested at scale. We present the first computational cross-linguistic analysis of sound symbolism in the semantic domain of size. We compile a typologically broad dataset of 810 adjectives (27 languages, 30 words each), each phonemically transcribed and validated with native-speaker audio. Using interpretable classifiers over bag-of-segment features, we find that phonological form predicts size semantics above chance even across unrelated languages, with both vowels and consonants contributing. To probe universality beyond genealogy, we train an adversarial scrubber that suppresses language identity while preserving size signal (also at family granularity). Language prediction averaged across languages and settings falls below chance while size prediction remains significantly above chance, indicating cross-family sound-symbolic bias. We release data, code, and diagnostic tools for future large-scale studies of iconicity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 118,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12250",
        "abs_url": "https://arxiv.org/abs/2512.12250",
        "pdf_url": "https://arxiv.org/pdf/2512.12250",
        "title": "Stochastic Volatility Modelling with LSTM Networks: A Hybrid Approach for S&P 500 Index Volatility Forecasting",
        "authors": [
            "Anna Perekhodko",
            "Robert Ślepaczuk"
        ],
        "comments": "32 pages, 15 tables, 11 figures",
        "subjects": "Trading and Market Microstructure (q-fin.TR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Portfolio Management (q-fin.PM)",
        "abstract": "Accurate volatility forecasting is essential in banking, investment, and risk management, because expectations about future market movements directly influence current decisions. This study proposes a hybrid modelling framework that integrates a Stochastic Volatility model with a Long Short Term Memory neural network. The SV model improves statistical precision and captures latent volatility dynamics, especially in response to unforeseen events, while the LSTM network enhances the model's ability to detect complex nonlinear patterns in financial time series. The forecasting is conducted using daily data from the S and P 500 index, covering the period from January 1 1998 to December 31 2024. A rolling window approach is employed to train the model and generate one step ahead volatility forecasts. The performance of the hybrid SV-LSTM model is evaluated through both statistical testing and investment simulations. The results show that the hybrid approach outperforms both the standalone SV and LSTM models and contributes to the development of volatility modelling techniques, providing a foundation for improving risk assessment and strategic investment planning in the context of the S and P 500.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 119,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12272",
        "abs_url": "https://arxiv.org/abs/2512.12272",
        "pdf_url": "https://arxiv.org/pdf/2512.12272",
        "title": "Accurate de novo sequencing of the modified proteome with OmniNovo",
        "authors": [
            "Yuhan Chen",
            "Shang Qu",
            "Zhiqiang Gao",
            "Yuejin Yang",
            "Xiang Zhang",
            "Sheng Xu",
            "Xinjie Mao",
            "Liujia Qian",
            "Jiaqi Wei",
            "Zijie Qiu",
            "Chenyu You",
            "Lei Bai",
            "Ning Ding",
            "Tiannan Guo",
            "Bowen Zhou",
            "Siqi Sun"
        ],
        "comments": "",
        "subjects": "Quantitative Methods (q-bio.QM); Artificial Intelligence (cs.AI)",
        "abstract": "Post-translational modifications (PTMs) serve as a dynamic chemical language regulating protein function, yet current proteomic methods remain blind to a vast portion of the modified proteome. Standard database search algorithms suffer from a combinatorial explosion of search spaces, limiting the identification of uncharacterized or complex modifications. Here we introduce OmniNovo, a unified deep learning framework for reference-free sequencing of unmodified and modified peptides directly from tandem mass spectra. Unlike existing tools restricted to specific modification types, OmniNovo learns universal fragmentation rules to decipher diverse PTMs within a single coherent model. By integrating a mass-constrained decoding algorithm with rigorous false discovery rate estimation, OmniNovo achieves state-of-the-art accuracy, identifying 51\\% more peptides than standard approaches at a 1\\% false discovery rate. Crucially, the model generalizes to biological sites unseen during training, illuminating the dark matter of the proteome and enabling unbiased comprehensive analysis of cellular regulation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 120,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12273",
        "abs_url": "https://arxiv.org/abs/2512.12273",
        "pdf_url": "https://arxiv.org/pdf/2512.12273",
        "title": "GRC-Net: Gram Residual Co-attention Net for epilepsy prediction",
        "authors": [
            "Bihao You",
            "Jiping Cui"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Prediction of epilepsy based on electroencephalogram (EEG) signals is a rapidly evolving field. Previous studies have traditionally applied 1D processing to the entire EEG signal. However, we have adopted the Gram Matrix method to transform the signals into a 3D representation, enabling modeling of signal relationships across dimensions while preserving the temporal dependencies of the one-dimensional signals. Additionally, we observed an imbalance between local and global signals within the EEG data. Therefore, we introduced multi-level feature extraction, utilizing coattention for capturing global signal characteristics and an inception structure for processing local signals, achieving multi-granular feature extraction. Our experiments on the BONN dataset demonstrate that for the most challenging five-class classification task, GRC-Net achieved an accuracy of 93.66%, outperforming existing methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 121,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12285",
        "abs_url": "https://arxiv.org/abs/2512.12285",
        "pdf_url": "https://arxiv.org/pdf/2512.12285",
        "title": "Fractional Differential Equation Physics-Informed Neural Network and Its Application in Battery State Estimation",
        "authors": [
            "Lujuan Dang",
            "Zilai Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Accurate estimation of the State of Charge (SOC) is critical for ensuring the safety, reliability, and performance optimization of lithium-ion battery systems. Conventional data-driven neural network models often struggle to fully characterize the inherent complex nonlinearities and memory-dependent dynamics of electrochemical processes, significantly limiting their predictive accuracy and physical interpretability under dynamic operating conditions. To address this challenge, this study proposes a novel neural architecture termed the Fractional Differential Equation Physics-Informed Neural Network (FDIFF-PINN), which integrates fractional calculus with deep learning. The main contributions of this paper include: (1) Based on a fractional-order equivalent circuit model, a discretized fractional-order partial differential equation is constructed. (2) Comparative experiments were conducted using a dynamic charge/discharge dataset of Panasonic 18650PF batteries under multi-temperature conditions (from -10$^{\\circ}$C to 20$^{\\circ}$C).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 122,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12324",
        "abs_url": "https://arxiv.org/abs/2512.12324",
        "pdf_url": "https://arxiv.org/pdf/2512.12324",
        "title": "UniMark: Artificial Intelligence Generated Content Identification Toolkit",
        "authors": [
            "Meilin Li",
            "Ji He",
            "Jia Xu",
            "Shanzhe Lei",
            "Yan Teng",
            "Yingchun Wang",
            "Xuhong Wang"
        ],
        "comments": "5 Pages",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
        "abstract": "The rapid proliferation of Artificial Intelligence Generated Content has precipitated a crisis of trust and urgent regulatory demands. However, existing identification tools suffer from fragmentation and a lack of support for visible compliance marking. To address these gaps, we introduce the \\textbf{UniMark}, an open-source, unified framework for multimodal content governance. Our system features a modular unified engine that abstracts complexities across text, image, audio, and video modalities. Crucially, we propose a novel dual-operation strategy, natively supporting both \\emph{Hidden Watermarking} for copyright protection and \\emph{Visible Marking} for regulatory compliance. Furthermore, we establish a standardized evaluation framework with three specialized benchmarks (Image/Video/Audio-Bench) to ensure rigorous performance assessment. This toolkit bridges the gap between advanced algorithms and engineering implementation, fostering a more transparent and secure digital ecosystem.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 123,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12332",
        "abs_url": "https://arxiv.org/abs/2512.12332",
        "pdf_url": "https://arxiv.org/pdf/2512.12332",
        "title": "Dynamic Homophily with Imperfect Recall: Modeling Resilience in Adversarial Networks",
        "authors": [
            "Saad Alqithami"
        ],
        "comments": "",
        "subjects": "Social and Information Networks (cs.SI); Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Information Theory (cs.IT)",
        "abstract": "The purpose of this study is to investigate how homophily, memory constraints, and adversarial disruptions collectively shape the resilience and adaptability of complex networks. To achieve this, we develop a new framework that integrates explicit memory decay mechanisms into homophily-based models and systematically evaluate their performance across diverse graph structures and adversarial settings. Our methods involve extensive experimentation on synthetic datasets, where we vary decay functions, reconnection probabilities, and similarity measures, primarily comparing cosine similarity with traditional metrics such as Jaccard similarity and baseline edge weights. The results show that cosine similarity achieves up to a 30\\% improvement in stability metrics in sparse, convex, and modular networks. Moreover, the refined value-of-recall metric demonstrates that strategic forgetting can bolster resilience by balancing network robustness and adaptability. The findings underscore the critical importance of aligning memory and similarity parameters with the structural and adversarial dynamics of the network. By quantifying the tangible benefits of incorporating memory constraints into homophily-based analyses, this study offers actionable insights for optimizing real-world applications, including social systems, collaborative platforms, and cybersecurity contexts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 124,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12337",
        "abs_url": "https://arxiv.org/abs/2512.12337",
        "pdf_url": "https://arxiv.org/pdf/2512.12337",
        "title": "SCIR: A Self-Correcting Iterative Refinement Framework for Enhanced Information Extraction Based on Schema",
        "authors": [
            "Yushen Fang",
            "Jianjun Li",
            "Mingqian Ding",
            "Chang Liu",
            "Xinchi Zou",
            "Wenqi Yang"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Although Large language Model (LLM)-powered information extraction (IE) systems have shown impressive capabilities, current fine-tuning paradigms face two major limitations: high training costs and difficulties in aligning with LLM preferences. To address these issues, we propose a novel universal IE paradigm, the Self-Correcting Iterative Refinement (SCIR) framework, along with a Multi-task Bilingual (Chinese-English) Self-Correcting (MBSC) dataset containing over 100,000 entries. The SCIR framework achieves plug-and-play compatibility with existing LLMs and IE systems through its Dual-Path Self-Correcting module and feedback-driven optimization, thereby significantly reducing training costs. Concurrently, the MBSC dataset tackles the challenge of preference alignment by indirectly distilling GPT-4's capabilities into IE result detection models. Experimental results demonstrate that SCIR outperforms state-of-the-art IE methods across three key tasks: named entity recognition, relation extraction, and event extraction, achieving a 5.27 percent average improvement in span-based Micro-F1 while reducing training costs by 87 percent compared to baseline approaches. These advancements not only enhance the flexibility and accuracy of IE systems but also pave the way for lightweight and efficient IE paradigms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 125,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12436",
        "abs_url": "https://arxiv.org/abs/2512.12436",
        "pdf_url": "https://arxiv.org/pdf/2512.12436",
        "title": "Rough Sets for Explainability of Spectral Graph Clustering",
        "authors": [
            "Bartłomiej Starosta",
            "Sławomir T. Wierzchoń",
            "Piotr Borkowski",
            "Dariusz Czerski",
            "Marcin Sydow",
            "Eryk Laskowski",
            "Mieczysław A. Kłopotek"
        ],
        "comments": "24 figures, 21tables",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Graph Spectral Clustering methods (GSC) allow representing clusters of diverse shapes, densities, etc. However, the results of such algorithms, when applied e.g. to text documents, are hard to explain to the user, especially due to embedding in the spectral space which has no obvious relation to document contents. Furthermore, the presence of documents without clear content meaning and the stochastic nature of the clustering algorithms deteriorate explainability. This paper proposes an enhancement to the explanation methodology, proposed in an earlier research of our team. It allows us to overcome the latter problems by taking inspiration from rough set theory.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 126,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12461",
        "abs_url": "https://arxiv.org/abs/2512.12461",
        "pdf_url": "https://arxiv.org/pdf/2512.12461",
        "title": "Cross-Modal Representational Knowledge Distillation for Enhanced Spike-Informed LFP Modeling",
        "authors": [
            "Eray Erturk",
            "Saba Hashemi",
            "Maryam M. Shanechi"
        ],
        "comments": "Published at the 39th Annual Conference on Neural Information Processing Systems 2025. Code is available at this https URL",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Neurons and Cognition (q-bio.NC)",
        "abstract": "Local field potentials (LFPs) can be routinely recorded alongside spiking activity in intracortical neural experiments, measure a larger complementary spatiotemporal scale of brain activity for scientific inquiry, and can offer practical advantages over spikes, including greater long-term stability, robustness to electrode degradation, and lower power requirements. Despite these advantages, recent neural modeling frameworks have largely focused on spiking activity since LFP signals pose inherent modeling challenges due to their aggregate, population-level nature, often leading to lower predictive power for downstream task variables such as motor behavior. To address this challenge, we introduce a cross-modal knowledge distillation framework that transfers high-fidelity representational knowledge from pretrained multi-session spike transformer models to LFP transformer models. Specifically, we first train a teacher spike model across multiple recording sessions using a masked autoencoding objective with a session-specific neural tokenization strategy. We then align the latent representations of the student LFP model to those of the teacher spike model. Our results show that the Distilled LFP models consistently outperform single- and multi-session LFP baselines in both fully unsupervised and supervised settings, and can generalize to other sessions without additional distillation while maintaining superior performance. These findings demonstrate that cross-modal knowledge distillation is a powerful and scalable approach for leveraging high-performing spike models to develop more accurate LFP models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 127,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12462",
        "abs_url": "https://arxiv.org/abs/2512.12462",
        "pdf_url": "https://arxiv.org/pdf/2512.12462",
        "title": "Dynamical modeling of nonlinear latent factors in multiscale neural activity with real-time inference",
        "authors": [
            "Eray Erturk",
            "Maryam M. Shanechi"
        ],
        "comments": "Published at the 39th Annual Conference on Neural Information Processing Systems 2025. Code is available at this https URL",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Neurons and Cognition (q-bio.NC)",
        "abstract": "Real-time decoding of target variables from multiple simultaneously recorded neural time-series modalities, such as discrete spiking activity and continuous field potentials, is important across various neuroscience applications. However, a major challenge for doing so is that different neural modalities can have different timescales (i.e., sampling rates) and different probabilistic distributions, or can even be missing at some time-steps. Existing nonlinear models of multimodal neural activity do not address different timescales or missing samples across modalities. Further, some of these models do not allow for real-time decoding. Here, we develop a learning framework that can enable real-time recursive decoding while nonlinearly aggregating information across multiple modalities with different timescales and distributions and with missing samples. This framework consists of 1) a multiscale encoder that nonlinearly aggregates information after learning within-modality dynamics to handle different timescales and missing samples in real time, 2) a multiscale dynamical backbone that extracts multimodal temporal dynamics and enables real-time recursive decoding, and 3) modality-specific decoders to account for different probabilistic distributions across modalities. In both simulations and three distinct multiscale brain datasets, we show that our model can aggregate information across modalities with different timescales and distributions and missing samples to improve real-time target decoding. Further, our method outperforms various linear and nonlinear multimodal benchmarks in doing so.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 128,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12465",
        "abs_url": "https://arxiv.org/abs/2512.12465",
        "pdf_url": "https://arxiv.org/pdf/2512.12465",
        "title": "Exploring the Design Space of Transition Matching",
        "authors": [
            "Uriel Singer",
            "Yaron Lipman"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Transition Matching (TM) is an emerging paradigm for generative modeling that generalizes diffusion and flow-matching models as well as continuous-state autoregressive models. TM, similar to previous paradigms, gradually transforms noise samples to data samples, however it uses a second ``internal'' generative model to implement the transition steps, making the transitions more expressive compared to diffusion and flow models. To make this paradigm tractable, TM employs a large backbone network and a smaller \"head\" module to efficiently execute the generative transition step. In this work, we present a large-scale, systematic investigation into the design, training and sampling of the head in TM frameworks, focusing on its time-continuous bidirectional variant. Through comprehensive ablations and experimentation involving training 56 different 1.7B text-to-image models (resulting in 549 unique evaluations) we evaluate the affect of the head module architecture and modeling during training as-well as a useful family of stochastic TM samplers. We analyze the impact on generation quality, training, and inference efficiency. We find that TM with an MLP head, trained with a particular time weighting and sampled with high frequency sampler provides best ranking across all metrics reaching state-of-the-art among all tested baselines, while Transformer head with sequence scaling and low frequency sampling is a runner up excelling at image aesthetics. Lastly, we believe the experiments presented highlight the design aspects that are likely to provide most quality and efficiency gains, while at the same time indicate what design choices are not likely to provide further gains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 129,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12474",
        "abs_url": "https://arxiv.org/abs/2512.12474",
        "pdf_url": "https://arxiv.org/pdf/2512.12474",
        "title": "AI-Driven Real-Time Kick Classification in Olympic Taekwondo Using Sensor Fusion",
        "authors": [
            "Jamsheed Mistri"
        ],
        "comments": "13 pages, 4 figures",
        "subjects": "Systems and Control (eess.SY); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Olympic Taekwondo has faced challenges in spectator engagement due to static, defensive gameplay and contentious scoring. Current Protector and Scoring Systems (PSS) rely on impact sensors and simplistic logic, encouraging safe strategies that diminish the sport's dynamism. This paper proposes an AI-powered scoring system that integrates existing PSS sensors with additional accelerometers, gyroscopes, magnetic/RFID, and impact force sensors in a sensor fusion framework. The system classifies kicks in real-time to identify technique type, contact location, impact force, and even the part of the foot used. A machine learning pipeline employing sensor fusion and Support Vector Machines (SVMs) is detailed, enabling automatic kick technique recognition for scoring. We present a novel kick scoring rubric that awards points based on specific kick techniques (e.g., turning and spinning kicks) to incentivize dynamic attacks. Drawing on a 2024 study achieving 96-98% accuracy, we validate the feasibility of real-time kick classification and further propose enhancements to this methodology, such as ensemble SVM classifiers and expanded datasets, to achieve the high-stakes accuracy required by the sport. We analyze how the proposed system can improve scoring fairness, reduce rule exploitation and illegitimate tactics, encourage more dynamic techniques, and enhance spectator understanding and excitement. The paper includes system design illustrations, a kick scoring table from an AI-augmented rule set, and discusses anticipated impacts on Olympic Taekwondo.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 130,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12483",
        "abs_url": "https://arxiv.org/abs/2512.12483",
        "pdf_url": "https://arxiv.org/pdf/2512.12483",
        "title": "Mage: Cracking Elliptic Curve Cryptography with Cross-Axis Transformers",
        "authors": [
            "Lily Erickson"
        ],
        "comments": "7 pages",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
        "abstract": "With the advent of machine learning and quantum computing, the 21st century has gone from a place of relative algorithmic security, to one of speculative unease and possibly, cyber catastrophe. Modern algorithms like Elliptic Curve Cryptography (ECC) are the bastion of current cryptographic security protocols that form the backbone of consumer protection ranging from Hypertext Transfer Protocol Secure (HTTPS) in the modern internet browser, to cryptographic financial instruments like Bitcoin. And there's been very little work put into testing the strength of these ciphers. Practically the only study that I could find was on side-channel recognition, a joint paper from the University of Milan, Italy and King's College, London\\cite{battistello2025ecc}. These algorithms are already considered bulletproof by many consumers, but exploits already exist for them, and with computing power and distributed, federated compute on the rise, it's only a matter of time before these current bastions fade away into obscurity, and it's on all of us to stand up when we notice something is amiss, lest we see such passages claim victims in that process. In this paper, we seek to explore the use of modern language model architecture in cracking the association between a known public key, and its associated private key, by intuitively learning to reverse engineer the public keypair generation process, effectively solving the curve. Additonally, we attempt to ascertain modern machine learning's ability to memorize public-private secp256r1 keypairs, and to then test their ability to reverse engineer the public keypair generation process. It is my belief that proof-for would be equally valuable as proof-against in either of these categories. Finally, we'll conclude with some number crunching on where we see this particular field heading in the future.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 131,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12500",
        "abs_url": "https://arxiv.org/abs/2512.12500",
        "pdf_url": "https://arxiv.org/pdf/2512.12500",
        "title": "Explainable AI as a Double-Edged Sword in Dermatology: The Impact on Clinicians versus The Public",
        "authors": [
            "Xuhai Xu",
            "Haoyu Hu",
            "Haoran Zhang",
            "Will Ke Wang",
            "Reina Wang",
            "Luis R. Soenksen",
            "Omar Badri",
            "Sheharbano Jafry",
            "Elise Burger",
            "Lotanna Nwandu",
            "Apoorva Mehta",
            "Erik P. Duhaime",
            "Asif Qasim",
            "Hause Lin",
            "Janis Pereira",
            "Jonathan Hershon",
            "Paulius Mui",
            "Alejandro A. Gru",
            "Noémie Elhadad",
            "Lena Mamykina",
            "Matthew Groh",
            "Philipp Tschandl",
            "Roxana Daneshjou",
            "Marzyeh Ghassemi"
        ],
        "comments": "",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)",
        "abstract": "Artificial intelligence (AI) is increasingly permeating healthcare, from physician assistants to consumer applications. Since AI algorithm's opacity challenges human interaction, explainable AI (XAI) addresses this by providing AI decision-making insight, but evidence suggests XAI can paradoxically induce over-reliance or bias. We present results from two large-scale experiments (623 lay people; 153 primary care physicians, PCPs) combining a fairness-based diagnosis AI model and different XAI explanations to examine how XAI assistance, particularly multimodal large language models (LLMs), influences diagnostic performance. AI assistance balanced across skin tones improved accuracy and reduced diagnostic disparities. However, LLM explanations yielded divergent effects: lay users showed higher automation bias - accuracy boosted when AI was correct, reduced when AI erred - while experienced PCPs remained resilient, benefiting irrespective of AI accuracy. Presenting AI suggestions first also led to worse outcomes when the AI was incorrect for both groups. These findings highlight XAI's varying impact based on expertise and timing, underscoring LLMs as a \"double-edged sword\" in medical AI and informing future human-AI collaborative system design.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 132,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12506",
        "abs_url": "https://arxiv.org/abs/2512.12506",
        "pdf_url": "https://arxiv.org/pdf/2512.12506",
        "title": "Explainable Artificial Intelligence for Economic Time Series: A Comprehensive Review and a Systematic Taxonomy of Methods and Concepts",
        "authors": [
            "Agustín García-García",
            "Pablo Hidalgo",
            "Julio E. Sandubete"
        ],
        "comments": "11 pages, 1 table",
        "subjects": "General Economics (econ.GN); Artificial Intelligence (cs.AI)",
        "abstract": "Explainable Artificial Intelligence (XAI) is increasingly required in computational economics, where machine-learning forecasters can outperform classical econometric models but remain difficult to audit and use for policy. This survey reviews and organizes the growing literature on XAI for economic time series, where autocorrelation, non-stationarity, seasonality, mixed frequencies, and regime shifts can make standard explanation techniques unreliable or economically implausible. We propose a taxonomy that classifies methods by (i) explanation mechanism: propagation-based approaches (e.g., Integrated Gradients, Layer-wise Relevance Propagation), perturbation and game-theoretic attribution (e.g., permutation importance, LIME, SHAP), and function-based global tools (e.g., Accumulated Local Effects); (ii) time-series compatibility, including preservation of temporal dependence, stability over time, and respect for data-generating constraints. We synthesize time-series-specific adaptations such as vector- and window-based formulations (e.g., Vector SHAP, WindowSHAP) that reduce lag fragmentation and computational cost while improving interpretability. We also connect explainability to causal inference and policy analysis through interventional attributions (Causal Shapley values) and constrained counterfactual reasoning. Finally, we discuss intrinsically interpretable architectures (notably attention-based transformers) and provide guidance for decision-grade applications such as nowcasting, stress testing, and regime monitoring, emphasizing attribution uncertainty and explanation dynamics as indicators of structural change.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 133,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12510",
        "abs_url": "https://arxiv.org/abs/2512.12510",
        "pdf_url": "https://arxiv.org/pdf/2512.12510",
        "title": "Can You Keep a Secret? Exploring AI for Care Coordination in Cognitive Decline",
        "authors": [
            "Alicia",
            "Mai Lee Chang",
            "Sreehana Mandava",
            "Destiny Deshields",
            "Hugo Simão",
            "Aaron Steinfeld",
            "Jodi Forlizzi",
            "John Zimmerman"
        ],
        "comments": "13 pages, 6 figures",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)",
        "abstract": "The increasing number of older adults who experience cognitive decline places a burden on informal caregivers, whose support with tasks of daily living determines whether older adults can remain in their homes. To explore how agents might help lower-SES older adults to age-in-place, we interviewed ten pairs of older adults experiencing cognitive decline and their informal caregivers. We explored how they coordinate care, manage burdens, and sustain autonomy and privacy. Older adults exercised control by delegating tasks to specific caregivers, keeping information about all the care they received from their adult children. Many abandoned some tasks of daily living, lowering their quality of life to ease caregiver burden. One effective strategy, piggybacking, uses spontaneous overlaps in errands to get more work done with less caregiver effort. This raises the questions: (i) Can agents help with piggyback coordination? (ii) Would it keep older adults in their homes longer, while not increasing caregiver burden?",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 134,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12523",
        "abs_url": "https://arxiv.org/abs/2512.12523",
        "pdf_url": "https://arxiv.org/pdf/2512.12523",
        "title": "Noise-robust Contrastive Learning for Critical Transition Detection in Dynamical Systems",
        "authors": [
            "Wenqi Fang",
            "Ye Li"
        ],
        "comments": "under revision",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computational Physics (physics.comp-ph)",
        "abstract": "Detecting critical transitions in complex, noisy time-series data is a fundamental challenge across science and engineering. Such transitions may be anticipated by the emergence of a low-dimensional order parameter, whose signature is often masked by high-amplitude stochastic variability. Standard contrastive learning approaches based on deep neural networks, while promising for detecting critical transitions, are often overparameterized and sensitive to irrelevant noise, leading to inaccurate identification of critical points. To address these limitations, we propose a neural network architecture, constructed using singular value decomposition technique, together with a strictly semi-orthogonality-constrained training algorithm, to enhance the performance of traditional contrastive learning. Extensive experiments demonstrate that the proposed method matches the performance of traditional contrastive learning techniques in identifying critical transitions, yet is considerably more lightweight and markedly more resistant to noise.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 135,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12536",
        "abs_url": "https://arxiv.org/abs/2512.12536",
        "pdf_url": "https://arxiv.org/pdf/2512.12536",
        "title": "Diverse LLMs vs. Vulnerabilities: Who Detects and Fixes Them Better?",
        "authors": [
            "Arastoo Zibaeirad",
            "Marco Vieira"
        ],
        "comments": "",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)",
        "abstract": "Large Language Models (LLMs) are increasingly being studied for Software Vulnerability Detection (SVD) and Repair (SVR). Individual LLMs have demonstrated code understanding abilities, but they frequently struggle when identifying complex vulnerabilities and generating fixes. This study presents DVDR-LLM, an ensemble framework that combines outputs from diverse LLMs to determine whether aggregating multiple models reduces error rates. Our evaluation reveals that DVDR-LLM achieves 10-12% higher detection accuracy compared to the average performance of individual models, with benefits increasing as code complexity grows. For multi-file vulnerabilities, the ensemble approach demonstrates significant improvements in recall (+18%) and F1 score (+11.8%) over individual models. However, the approach raises measurable trade-offs: reducing false positives in verification tasks while simultaneously increasing false negatives in detection tasks, requiring careful decision on the required level of agreement among the LLMs (threshold) for increased performance across different security contexts. Artifact: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 136,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12545",
        "abs_url": "https://arxiv.org/abs/2512.12545",
        "pdf_url": "https://arxiv.org/pdf/2512.12545",
        "title": "Skillful Subseasonal-to-Seasonal Forecasting of Extreme Events with a Multi-Sphere Coupled Probabilistic Model",
        "authors": [
            "Bin Mu",
            "Yuxuan Chen",
            "Shijin Yuan",
            "Bo Qin",
            "Hao Guo"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Atmospheric and Oceanic Physics (physics.ao-ph)",
        "abstract": "Accurate subseasonal-to-seasonal (S2S) prediction of extreme events is critical for resource planning and disaster mitigation under accelerating climate change. However, such predictions remain challenging due to complex multi-sphere interactions and intrinsic atmospheric uncertainty. Here we present TianXing-S2S, a multi-sphere coupled probabilistic model for global S2S daily ensemble forecast. TianXing-S2S first encodes diverse multi-sphere predictors into a compact latent space, then employs a diffusion model to generate daily ensemble forecasts. A novel coupling module based on optimal transport (OT) is incorporated in the denoiser to optimize the interactions between atmospheric and multi-sphere boundary conditions. Across key atmospheric variables, TianXing-S2S outperforms both the European Centre for Medium-Range Weather Forecasts (ECMWF) S2S system and FuXi-S2S in 45-day daily-mean ensemble forecasts at 1.5 resolution. Our model achieves skillful subseasonal prediction of extreme events including heat waves and anomalous precipitation, identifying soil moisture as a critical precursor signal. Furthermore, we demonstrate that TianXing-S2S can generate stable rollout forecasts up to 180 days, establishing a robust framework for S2S research in a warming world.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 137,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12576",
        "abs_url": "https://arxiv.org/abs/2512.12576",
        "pdf_url": "https://arxiv.org/pdf/2512.12576",
        "title": "Coupled Variational Reinforcement Learning for Language Model General Reasoning",
        "authors": [
            "Xueru Wen",
            "Jie Lou",
            "Yanjiang Liu",
            "Hongyu Lin",
            "Ben He",
            "Xianpei Han",
            "Le Sun",
            "Yaojie Lu",
            "Debing Zhang"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "While reinforcement learning have achieved impressive progress in language model reasoning, they are constrained by the requirement for verifiable rewards. Recent verifier-free RL methods address this limitation by utilizing the intrinsic probabilities of LLMs generating reference answers as reward signals. However, these approaches typically sample reasoning traces conditioned only on the question. This design decouples reasoning-trace sampling from answer information, leading to inefficient exploration and incoherence between traces and final answers. In this paper, we propose \\textit{\\b{Co}upled \\b{V}ariational \\b{R}einforcement \\b{L}earning} (CoVRL), which bridges variational inference and reinforcement learning by coupling prior and posterior distributions through a hybrid sampling strategy. By constructing and optimizing a composite distribution that integrates these two distributions, CoVRL enables efficient exploration while preserving strong thought-answer coherence. Extensive experiments on mathematical and general reasoning benchmarks show that CoVRL improves performance by 12.4\\% over the base model and achieves an additional 2.3\\% improvement over strong state-of-the-art verifier-free RL baselines, providing a principled framework for enhancing the general reasoning capabilities of language models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 138,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12583",
        "abs_url": "https://arxiv.org/abs/2512.12583",
        "pdf_url": "https://arxiv.org/pdf/2512.12583",
        "title": "Detecting Prompt Injection Attacks Against Application Using Classifiers",
        "authors": [
            "Safwan Shaheer",
            "G. M. Refatul Islam",
            "Mohammad Rafid Hamid",
            "Md. Abrar Faiaz Khan",
            "Md. Omar Faruk",
            "Yaseen Nur"
        ],
        "comments": "9 pages, X figures; undergraduate research project on detecting prompt injection attacks against LLM integrated web applications using classical machine learning and neural classifiers",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
        "abstract": "Prompt injection attacks can compromise the security and stability of critical systems, from infrastructure to large web applications. This work curates and augments a prompt injection dataset based on the HackAPrompt Playground Submissions corpus and trains several classifiers, including LSTM, feed forward neural networks, Random Forest, and Naive Bayes, to detect malicious prompts in LLM integrated web applications. The proposed approach improves prompt injection detection and mitigation, helping protect targeted applications and systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 139,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12608",
        "abs_url": "https://arxiv.org/abs/2512.12608",
        "pdf_url": "https://arxiv.org/pdf/2512.12608",
        "title": "Human-Inspired Learning for Large Language Models via Obvious Record and Maximum-Entropy Method Discovery",
        "authors": [
            "Hong Su"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Large Language Models (LLMs) excel at extracting common patterns from large-scale corpora, yet they struggle with rare, low-resource, or previously unseen scenarios-such as niche hardware deployment issues or irregular IoT device behaviors-because such cases are sparsely represented in training data. Moreover, LLMs rely primarily on implicit parametric memory, which limits their ability to explicitly acquire, recall, and refine methods, causing them to behave predominantly as intuition-driven predictors rather than deliberate, method-oriented learners. Inspired by how humans learn from rare experiences, this paper proposes a human-inspired learning framework that integrates two complementary mechanisms. The first, Obvious Record, explicitly stores cause--result (or question--solution) relationships as symbolic memory, enabling persistent learning even from single or infrequent encounters. The second, Maximum-Entropy Method Discovery, prioritizes and preserves methods with high semantic dissimilarity, allowing the system to capture diverse and underrepresented strategies that are typically overlooked by next-token prediction. Verification on a benchmark of 60 semantically diverse question--solution pairs demonstrates that the proposed entropy-guided approach achieves stronger coverage of unseen questions and significantly greater internal diversity than a random baseline, confirming its effectiveness in discovering more generalizable and human-inspired methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 140,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12620",
        "abs_url": "https://arxiv.org/abs/2512.12620",
        "pdf_url": "https://arxiv.org/pdf/2512.12620",
        "title": "Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives",
        "authors": [
            "Aheli Poddar",
            "Saptarshi Sahoo",
            "Sujata Ghosh"
        ],
        "comments": "9 pages, 4 figures, 5 tables. Submitted to AAAI 2026 Bridge Program on Logic & AI. Code available at this https URL",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "We study syllogistic reasoning in LLMs from the logical and natural language perspectives. In process, we explore fundamental reasoning capabilities of the LLMs and the direction this research is moving forward. To aid in our studies, we use 14 large language models and investigate their syllogistic reasoning capabilities in terms of symbolic inferences as well as natural language understanding. Even though this reasoning mechanism is not a uniform emergent property across LLMs, the perfect symbolic performances in certain models make us wonder whether LLMs are becoming more and more formal reasoning mechanisms, rather than making explicit the nuances of human reasoning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 141,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12630",
        "abs_url": "https://arxiv.org/abs/2512.12630",
        "pdf_url": "https://arxiv.org/pdf/2512.12630",
        "title": "ORIBA: Exploring LLM-Driven Role-Play Chatbot as a Creativity Support Tool for Original Character Artists",
        "authors": [
            "Yuqian Sun",
            "Xingyu Li",
            "Shunyu Yao",
            "Noura Howell",
            "Tristan Braud",
            "Chang Hee Lee",
            "Ali Asadipour"
        ],
        "comments": "",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)",
        "abstract": "Recent advances in Generative AI (GAI) have led to new opportunities for creativity support. However, this technology has raised ethical concerns in the visual artists community. This paper explores how GAI can assist visual artists in developing original characters (OCs) while respecting their creative agency. We present ORIBA, an AI chatbot leveraging large language models (LLMs) to enable artists to role-play with their OCs, focusing on conceptualization (e.g., backstories) while leaving exposition (visual creation) to creators. Through a study with 14 artists, we found ORIBA motivated artists' imaginative engagement, developing multidimensional attributes and stronger bonds with OCs that inspire their creative process. Our contributions include design insights for AI systems that develop from artists' perspectives, demonstrating how LLMs can support cross-modal creativity while preserving creative agency in OC art. This paper highlights the potential of GAI as a neutral, non-visual support that strengthens existing creative practice, without infringing artistic exposition.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 142,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12662",
        "abs_url": "https://arxiv.org/abs/2512.12662",
        "pdf_url": "https://arxiv.org/pdf/2512.12662",
        "title": "Anatomy-Guided Representation Learning Using a Transformer-Based Network for Thyroid Nodule Segmentation in Ultrasound Images",
        "authors": [
            "Muhammad Umar Farooq",
            "Abd Ur Rehman",
            "Azka Rehman",
            "Muhammad Usman",
            "Dong-Kyu Chae",
            "Junaid Qadir"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Accurate thyroid nodule segmentation in ultrasound images is critical for diagnosis and treatment planning. However, ambiguous boundaries between nodules and surrounding tissues, size variations, and the scarcity of annotated ultrasound data pose significant challenges for automated segmentation. Existing deep learning models struggle to incorporate contextual information from the thyroid gland and generalize effectively across diverse cases. To address these challenges, we propose SSMT-Net, a Semi-Supervised Multi-Task Transformer-based Network that leverages unlabeled data to enhance Transformer-centric encoder feature extraction capability in an initial unsupervised phase. In the supervised phase, the model jointly optimizes nodule segmentation, gland segmentation, and nodule size estimation, integrating both local and global contextual features. Extensive evaluations on the TN3K and DDTI datasets demonstrate that SSMT-Net outperforms state-of-the-art methods, with higher accuracy and robustness, indicating its potential for real-world clinical applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 143,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12669",
        "abs_url": "https://arxiv.org/abs/2512.12669",
        "pdf_url": "https://arxiv.org/pdf/2512.12669",
        "title": "DynaGen: Unifying Temporal Knowledge Graph Reasoning with Dynamic Subgraphs and Generative Regularization",
        "authors": [
            "Jiawei Shen",
            "Jia Zhu",
            "Hanghui Guo",
            "Weijie Shi",
            "Guoqing Ma",
            "Yidan Liang",
            "Jingjiang Liu",
            "Hao Chen",
            "Shimin Di"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Temporal Knowledge Graph Reasoning (TKGR) aims to complete missing factual elements along the timeline. Depending on the temporal position of the query, the task is categorized into interpolation and extrapolation. Existing interpolation methods typically embed temporal information into individual facts to complete missing historical knowledge, while extrapolation techniques often leverage sequence models over graph snapshots to identify recurring patterns for future event prediction. These methods face two critical challenges: limited contextual modeling in interpolation and cognitive generalization bias in extrapolation. To address these, we propose a unified method for TKGR, dubbed DynaGen. For interpolation, DynaGen dynamically constructs entity-centric subgraphs and processes them with a synergistic dual-branch GNN encoder to capture evolving structural context. For extrapolation, it applies a conditional diffusion process, which forces the model to learn underlying evolutionary principles rather than just superficial patterns, enhancing its ability to predict unseen future events. Extensive experiments on six benchmark datasets show DynaGen achieves state-of-the-art performance. On average, compared to the second-best models, DynaGen improves the Mean Reciprocal Rank (MRR) score by 2.61 points for interpolation and 1.45 points for extrapolation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 144,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12677",
        "abs_url": "https://arxiv.org/abs/2512.12677",
        "pdf_url": "https://arxiv.org/pdf/2512.12677",
        "title": "Fine-Tuning Causal LLMs for Text Classification: Embedding-Based vs. Instruction-Based Approaches",
        "authors": [
            "Amirhossein Yousefiramandi",
            "Ciaran Cooney"
        ],
        "comments": "18 pages, 6 figures",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "We explore efficient strategies to fine-tune decoder-only Large Language Models (LLMs) for downstream text classification under resource constraints. Two approaches are investigated: (1) attaching a classification head to a pre-trained causal LLM and fine-tuning on the task (using the LLM's final token embedding as a sequence representation), and (2) instruction-tuning the LLM in a prompt->response format for classification. To enable single-GPU fine-tuning of models up to 8B parameters, we combine 4-bit model quantization with Low-Rank Adaptation (LoRA) for parameter-efficient training. Experiments on two datasets - a proprietary single-label dataset and the public WIPO-Alpha patent dataset (extreme multi-label classification) - show that the embedding-based method significantly outperforms the instruction-tuned method in F1-score, and is very competitive with - even surpassing - fine-tuned domain-specific models (e.g. BERT) on the same tasks. These results demonstrate that directly leveraging the internal representations of causal LLMs, along with efficient fine-tuning techniques, yields impressive classification performance under limited computational resources. We discuss the advantages of each approach while outlining practical guidelines and future directions for optimizing LLM fine-tuning in classification scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 145,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12688",
        "abs_url": "https://arxiv.org/abs/2512.12688",
        "pdf_url": "https://arxiv.org/pdf/2512.12688",
        "title": "Theoretical Foundations of Prompt Engineering: From Heuristics to Expressivity",
        "authors": [
            "Dongseok Kim",
            "Hyoungsun Choi",
            "Mohamed Jismy Aashik Rasool",
            "Gisung Oh"
        ],
        "comments": "24 pages",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Prompts can switch a model's behavior even when the weights are fixed, yet this phenomenon is rarely treated as a clean theoretical object rather than a heuristic. We study the family of functions obtainable by holding a Transformer backbone fixed as an executor and varying only the prompt. Our core idea is to view the prompt as an externally injected program and to construct a simplified Transformer that interprets it to implement different computations. The construction exposes a mechanism-level decomposition: attention performs selective routing from prompt memory, the FFN performs local arithmetic conditioned on retrieved fragments, and depth-wise stacking composes these local updates into a multi-step computation. Under this viewpoint, we prove a constructive existential result showing that a single fixed backbone can approximate a broad class of target behaviors via prompts alone. The framework provides a unified starting point for formalizing trade-offs under prompt length/precision constraints and for studying structural limits of prompt-based switching, while remaining distinct from empirical claims about pretrained LLMs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 146,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12693",
        "abs_url": "https://arxiv.org/abs/2512.12693",
        "pdf_url": "https://arxiv.org/pdf/2512.12693",
        "title": "Co-Exploration and Co-Exploitation via Shared Structure in Multi-Task Bandits",
        "authors": [
            "Sumantrak Mukherjee",
            "Serafima Lebedeva",
            "Valentin Margraf",
            "Jonas Hanselle",
            "Kanta Yamaoka",
            "Viktor Bengs",
            "Stefan Konigorski",
            "Eyke Hüllermeier",
            "Sebastian Josef Vollmer"
        ],
        "comments": "18 pages, 9 figures, preprint",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "We propose a novel Bayesian framework for efficient exploration in contextual multi-task multi-armed bandit settings, where the context is only observed partially and dependencies between reward distributions are induced by latent context variables. In order to exploit these structural dependencies, our approach integrates observations across all tasks and learns a global joint distribution, while still allowing personalised inference for new tasks. In this regard, we identify two key sources of epistemic uncertainty, namely structural uncertainty in the latent reward dependencies across arms and tasks, and user-specific uncertainty due to incomplete context and limited interaction history. To put our method into practice, we represent the joint distribution over tasks and rewards using a particle-based approximation of a log-density Gaussian process. This representation enables flexible, data-driven discovery of both inter-arm and inter-task dependencies without prior assumptions on the latent variables. Empirically, we demonstrate that our method outperforms baselines such as hierarchical model bandits, especially in settings with model misspecification or complex latent heterogeneity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 147,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12760",
        "abs_url": "https://arxiv.org/abs/2512.12760",
        "pdf_url": "https://arxiv.org/pdf/2512.12760",
        "title": "Intelligent Scientific Literature Explorer using Machine Learning (ISLE)",
        "authors": [
            "Sina Jani",
            "Arman Heidari",
            "Amirmohammad Anvari",
            "Zahra Rahimi"
        ],
        "comments": "18 pages, 7 figures, 3 tables",
        "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "The rapid acceleration of scientific publishing has created substantial challenges for researchers attempting to discover, contextualize, and interpret relevant literature. Traditional keyword-based search systems provide limited semantic understanding, while existing AI-driven tools typically focus on isolated tasks such as retrieval, clustering, or bibliometric visualization. This paper presents an integrated system for scientific literature exploration that combines large-scale data acquisition, hybrid retrieval, semantic topic modeling, and heterogeneous knowledge graph construction. The system builds a comprehensive corpus by merging full-text data from arXiv with structured metadata from OpenAlex. A hybrid retrieval architecture fuses BM25 lexical search with embedding-based semantic search using Reciprocal Rank Fusion. Topic modeling is performed on retrieved results using BERTopic or non-negative matrix factorization depending on computational resources. A knowledge graph unifies papers, authors, institutions, countries, and extracted topics into an interpretable structure. The system provides a multi-layered exploration environment that reveals not only relevant publications but also the conceptual and relational landscape surrounding a query. Evaluation across multiple queries demonstrates improvements in retrieval relevance, topic coherence, and interpretability. The proposed framework contributes an extensible foundation for AI-assisted scientific discovery.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 148,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12768",
        "abs_url": "https://arxiv.org/abs/2512.12768",
        "pdf_url": "https://arxiv.org/pdf/2512.12768",
        "title": "CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence",
        "authors": [
            "Tianjiao Yu",
            "Xinzhuo Li",
            "Yifan Shen",
            "Yuanzhe Liu",
            "Ismini Lourentzou"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Recent advances in large multimodal models suggest that explicit reasoning mechanisms play a critical role in improving model reliability, interpretability, and cross-modal alignment. While such reasoning-centric approaches have been proven effective in language and vision tasks, their extension to 3D remains underdeveloped. CoRe3D introduces a unified 3D understanding and generation reasoning framework that jointly operates over semantic and spatial abstractions, enabling high-level intent inferred from language to directly guide low-level 3D content formation. Central to this design is a spatially grounded reasoning representation that decomposes 3D latent space into localized regions, allowing the model to reason over geometry in a compositional and procedural manner. By tightly coupling semantic chain-of-thought inference with structured spatial reasoning, CoRe3D produces 3D outputs that exhibit strong local consistency and faithful alignment with linguistic descriptions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 149,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12769",
        "abs_url": "https://arxiv.org/abs/2512.12769",
        "pdf_url": "https://arxiv.org/pdf/2512.12769",
        "title": "Adaptive Edge-Cloud Inference for Speech-to-Action Systems Using ASR and Large Language Models (ASTA)",
        "authors": [
            "Mohammad Jalili Torkamani",
            "Israt Zarin"
        ],
        "comments": "preprint, 6 pages, 7 figures, 1 table",
        "subjects": "Sound (cs.SD); Artificial Intelligence (cs.AI)",
        "abstract": "Voice-based interaction has emerged as a natural and intuitive modality for controlling IoT devices. However, speech-driven edge devices face a fundamental trade-off between cloud-based solutions, which offer stronger language understanding capabilities at the cost of latency, connectivity dependence, and privacy concerns, and edge-based solutions, which provide low latency and improved privacy but are limited by computational constraints. This paper presents ASTA, an adaptive speech-to-action solution that dynamically routes voice commands between edge and cloud inference to balance performance and system resource utilization. ASTA integrates on-device automatic speech recognition and lightweight offline language-model inference with cloud-based LLM processing, guided by real-time system metrics such as CPU workload, device temperature, and network latency. A metric-aware routing mechanism selects the inference path at runtime, while a rule-based command validation and repair component ensures successful end-to-end command execution. We implemented our solution on an NVIDIA Jetson-based edge platform and evaluated it using a diverse dataset of 80 spoken commands. Experimental results show that ASTA successfully routes all input commands for execution, achieving a balanced distribution between online and offline inference. The system attains an ASR accuracy of 62.5% and generates executable commands without repair for only 47.5% of inputs, highlighting the importance of the repair mechanism in improving robustness. These results suggest that adaptive edge-cloud orchestration is a viable approach for resilient and resource-aware voice-controlled IoT systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 150,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12773",
        "abs_url": "https://arxiv.org/abs/2512.12773",
        "pdf_url": "https://arxiv.org/pdf/2512.12773",
        "title": "Designing The Drive: Enhancing User Experience through Adaptive Interfaces in Autonomous Vehicles",
        "authors": [
            "Reeteesha Roy"
        ],
        "comments": "8 pages, 4 figures",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)",
        "abstract": "With the recent development and integration of autonomous vehicles (AVs) in transportation systems of the modern world, the emphasis on customizing user interfaces to optimize the overall user experience has been growing expediently. Therefore, understanding user needs and preferences is essential to the acceptance and trust of these technologies as they continue to grow in prevalence. This paper addresses the implementation of HCI principles in the personalization of interfaces to improve safety, security, and usability for the users. This paper explores the way that personalized interfaces can be devised to increase user engagement and satisfaction through various HCI strategies such as adaptive design, multi-modal interaction, and user feedback mechanisms. Moreover, this paper puts emphasis on factors of transparency and user control in the design of an interface; hence, allowing users to design or modify their experience could foster an increase in trust in autonomous systems. In so doing, this research touches on the quite influential role HCI will play in this future scenario of autonomous vehicles while designing to ensure relevance to the diverse needs of users while maintaining high standards of safety and security. Discussing various HCI strategies such as adaptive design, multi-modal interaction, and feedback mechanisms to the user, this paper demonstrates how personalized interfaces can enhance significantly both user engagement and satisfaction. Transparency and user control also in designing an interface are further discussed, pointing out the need for a prerequisite condition of enabling the user to take control of their experience as a state of trust in autonomous systems. In summary, this paper points out the role of HCI in the development of autonomous vehicles and addresses numerous needs with respect to those enforced safety and security standards.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 151,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12777",
        "abs_url": "https://arxiv.org/abs/2512.12777",
        "pdf_url": "https://arxiv.org/pdf/2512.12777",
        "title": "State over Tokens: Characterizing the Role of Reasoning Tokens",
        "authors": [
            "Mosh Levy",
            "Zohar Elyoseph",
            "Shauli Ravfogel",
            "Yoav Goldberg"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Large Language Models (LLMs) can generate reasoning tokens before their final answer to boost performance on complex tasks. While these sequences seem like human thought processes, empirical evidence reveals that they are not a faithful explanation of the model's actual reasoning process. To address this gap between appearance and function, we introduce the State over Tokens (SoT) conceptual framework. SoT reframes reasoning tokens not as a linguistic narrative, but as an externalized computational state -- the sole persistent information carrier across the model's stateless generation cycles. This explains how the tokens can drive correct reasoning without being a faithful explanation when read as text and surfaces previously overlooked research questions on these tokens. We argue that to truly understand the process that LLMs do, research must move beyond reading the reasoning tokens as text and focus on decoding them as state.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 152,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12785",
        "abs_url": "https://arxiv.org/abs/2512.12785",
        "pdf_url": "https://arxiv.org/pdf/2512.12785",
        "title": "OLC-WA: Drift Aware Tuning-Free Online Classification with Weighted Average",
        "authors": [
            "Mohammad Abu Shaira",
            "Yunhe Feng",
            "Heng Fan",
            "Weishi Shi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Real-world data sets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue. These parameters are typically fixed and cannot be dynamically adjusted by the user in response to the evolving data distribution. This paper introduces Online Classification with Weighted Average (OLC-WA), an adaptive, hyperparameter-free online classification model equipped with an automated optimization mechanism. OLC-WA operates by blending incoming data streams with an existing base model. This blending is facilitated by an exponentially weighted moving average. Furthermore, an integrated optimization mechanism dynamically detects concept drift, quantifies its magnitude, and adjusts the model based on the observed data stream characteristics. This approach empowers the model to effectively adapt to evolving data distributions within streaming environments. Rigorous empirical evaluation across diverse benchmark datasets shows that OLC-WA achieves performance comparable to batch models in stationary environments, maintaining accuracy within 1-3%, and surpasses leading online baselines by 10-25% under drift, demonstrating its effectiveness in adapting to dynamic data streams.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 153,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12787",
        "abs_url": "https://arxiv.org/abs/2512.12787",
        "pdf_url": "https://arxiv.org/pdf/2512.12787",
        "title": "Unveiling Statistical Significance of Online Regression over Multiple Datasets",
        "authors": [
            "Mohammad Abu-Shaira",
            "Weishi Shi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Despite extensive focus on techniques for evaluating the performance of two learning algorithms on a single dataset, the critical challenge of developing statistical tests to compare multiple algorithms across various datasets has been largely overlooked in most machine learning research. Additionally, in the realm of Online Learning, ensuring statistical significance is essential to validate continuous learning processes, particularly for achieving rapid convergence and effectively managing concept drifts in a timely manner. Robust statistical methods are needed to assess the significance of performance differences as data evolves over time. This article examines the state-of-the-art online regression models and empirically evaluates several suitable tests. To compare multiple online regression models across various datasets, we employed the Friedman test along with corresponding post-hoc tests. For thorough evaluations, utilizing both real and synthetic datasets with 5-fold cross-validation and seed averaging ensures comprehensive assessment across various data subsets. Our tests generally confirmed the performance of competitive baselines as consistent with their individual reports. However, some statistical test results also indicate that there is still room for improvement in certain aspects of state-of-the-art methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 154,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12791",
        "abs_url": "https://arxiv.org/abs/2512.12791",
        "pdf_url": "https://arxiv.org/pdf/2512.12791",
        "title": "Beyond Task Completion: An Assessment Framework for Evaluating Agentic AI Systems",
        "authors": [
            "Sreemaee Akshathala",
            "Bassam Adnan",
            "Mahisha Ramesh",
            "Karthik Vaidhyanathan",
            "Basil Muhammed",
            "Kannan Parthasarathy"
        ],
        "comments": "",
        "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI); Software Engineering (cs.SE)",
        "abstract": "Recent advances in agentic AI have shifted the focus from standalone Large Language Models (LLMs) to integrated systems that combine LLMs with tools, memory, and other agents to perform complex tasks. These multi-agent architectures enable coordinated reasoning, planning, and execution across diverse domains, allowing agents to collaboratively automate complex workflows. Despite these advances, evaluation and assessment of LLM agents and the multi-agent systems they constitute remain a fundamental challenge. Although various approaches have been proposed in the software engineering literature for evaluating conventional software components, existing methods for AI-based systems often overlook the non-deterministic nature of models. This non-determinism introduces behavioral uncertainty during execution, yet existing evaluations rely on binary task completion metrics that fail to capture it. Evaluating agentic systems therefore requires examining additional dimensions, including the agent ability to invoke tools, ingest and retrieve memory, collaborate with other agents, and interact effectively with its environment. We propose an end-to-end Agent Assessment Framework with four evaluation pillars encompassing LLMs, Memory, Tools, and Environment. We validate the framework on a representative Autonomous CloudOps use case, where experiments reveal behavioral deviations overlooked by conventional metrics, demonstrating its effectiveness in capturing runtime uncertainties.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 155,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12792",
        "abs_url": "https://arxiv.org/abs/2512.12792",
        "pdf_url": "https://arxiv.org/pdf/2512.12792",
        "title": "Liquid Reasoning Transformers: A Sudoku-Based Prototype for Chess-Scale Algorithmic Tasks",
        "authors": [
            "Shivansh Sahni",
            "Wenzhi Zhang"
        ],
        "comments": "11 pages, 0 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "The Liquid Reasoning Transformer (LRT) is a transformer architecture designed for inference with adaptive depths using iterative changes, discard-based correction, and a learned stopping mechanism. Instead of relying on a single feedforward pass, the model updates a recurrent reasoning token across multiple internal steps, allowing it to correct early errors and allocate computation based on input difficulty. We evaluate the LRT on Sudoku as a controlled testbed for structured reasoning and show that it achieves strong performance, reaching 98.68% digit accuracy and 36.30% full-puzzle accuracy without using symbolic rules or search. Analyzing internal patterns shows that the discard and stop gates play different, important roles in stabilizing inferences and adjusting computational depth. We discuss how these mechanisms extend naturally to chess-scale reasoning tasks and outline extensions for multi-token reasoning and larger domains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 156,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12802",
        "abs_url": "https://arxiv.org/abs/2512.12802",
        "pdf_url": "https://arxiv.org/pdf/2512.12802",
        "title": "A Disproof of Large Language Model Consciousness: The Necessity of Continual Learning for Consciousness",
        "authors": [
            "Erik Hoel"
        ],
        "comments": "28 pages, 3 figures",
        "subjects": "Neurons and Cognition (q-bio.NC); Artificial Intelligence (cs.AI)",
        "abstract": "The requirements for a falsifiable and non-trivial theory of consciousness significantly constrain such theories. Specifically, recent research on the Unfolding Argument and the Substitution Argument has given us formal tools to analyze requirements for a theory of consciousness. I show via a new Proximity Argument that these requirements especially constrain the potential consciousness of contemporary Large Language Models (LLMs) because of their proximity to systems that are equivalent to LLMs in terms of input/output function; yet, for these functionally equivalent systems, there cannot be any non-trivial theory of consciousness that judges them conscious. This forms the basis of a disproof of contemporary LLM consciousness. I then show a positive result, which is that theories of consciousness based on (or requiring) continual learning do satisfy the stringent formal constraints for a theory of consciousness in humans. Intriguingly, this work supports a hypothesis: If continual learning is linked to consciousness in humans, the current limitations of LLMs (which do not continually learn) are intimately tied to their lack of consciousness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 157,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12805",
        "abs_url": "https://arxiv.org/abs/2512.12805",
        "pdf_url": "https://arxiv.org/pdf/2512.12805",
        "title": "From Small to Large: Generalization Bounds for Transformers on Variable-Size Inputs",
        "authors": [
            "Anastasiia Alokhina",
            "Pan Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Transformers exhibit a notable property of \\emph{size generalization}, demonstrating an ability to extrapolate from smaller token sets to significantly longer ones. This behavior has been documented across diverse applications, including point clouds, graphs, and natural language. Despite its empirical success, this capability still lacks some rigorous theoretical characterizations. In this paper, we develop a theoretical framework to analyze this phenomenon for geometric data, which we represent as discrete samples from a continuous source (e.g., point clouds from manifolds, graphs from graphons). Our core contribution is a bound on the error between the Transformer's output for a discrete sample and its continuous-domain equivalent. We prove that for Transformers with stable positional encodings, this bound is determined by the sampling density and the intrinsic dimensionality of the data manifold. Experiments on graphs and point clouds of various sizes confirm the tightness of our theoretical bound.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 158,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12809",
        "abs_url": "https://arxiv.org/abs/2512.12809",
        "pdf_url": "https://arxiv.org/pdf/2512.12809",
        "title": "OPAL: Operator-Programmed Algorithms for Landscape-Aware Black-Box Optimization",
        "authors": [
            "Junbo Jacob Lian",
            "Mingyang Yu",
            "Kaichen Ouyang",
            "Shengwei Fu",
            "Rui Zhong",
            "Yujun Zhang",
            "Jun Zhang",
            "Huiling Chen"
        ],
        "comments": "Source code, experiment scripts, and results are publicly available at this https URL. The real-world application part hasn't been done yet",
        "subjects": "Neural and Evolutionary Computing (cs.NE); Artificial Intelligence (cs.AI)",
        "abstract": "Black-box optimization often relies on evolutionary and swarm algorithms whose performance is highly problem dependent. We view an optimizer as a short program over a small vocabulary of search operators and learn this operator program separately for each problem instance. We instantiate this idea in Operator-Programmed Algorithms (OPAL), a landscape-aware framework for continuous black-box optimization that uses a small design budget with a standard differential evolution baseline to probe the landscape, builds a $k$-nearest neighbor graph over sampled points, and encodes this trajectory with a graph neural network. A meta-learner then maps the resulting representation to a phase-wise schedule of exploration, restart, and local search operators. On the CEC~2017 test suite, a single meta-trained OPAL policy is statistically competitive with state-of-the-art adaptive differential evolution variants and achieves significant improvements over simpler baselines under nonparametric tests. Ablation studies on CEC~2017 justify the choices for the design phase, the trajectory graph, and the operator-program representation, while the meta-components add only modest wall-clock overhead. Overall, the results indicate that operator-programmed, landscape-aware per-instance design is a practical way forward beyond ad hoc metaphor-based algorithms in black-box optimization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 159,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12812",
        "abs_url": "https://arxiv.org/abs/2512.12812",
        "pdf_url": "https://arxiv.org/pdf/2512.12812",
        "title": "Does Tone Change the Answer? Evaluating Prompt Politeness Effects on Modern LLMs: GPT, Gemini, LLaMA",
        "authors": [
            "Hanyu Cai",
            "Binqi Shen",
            "Lier Jin",
            "Lan Hu",
            "Xiaojing Fan"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Prompt engineering has emerged as a critical factor influencing large language model (LLM) performance, yet the impact of pragmatic elements such as linguistic tone and politeness remains underexplored, particularly across different model families. In this work, we propose a systematic evaluation framework to examine how interaction tone affects model accuracy and apply it to three recently released and widely available LLMs: GPT-4o mini (OpenAI), Gemini 2.0 Flash (Google DeepMind), and Llama 4 Scout (Meta). Using the MMMLU benchmark, we evaluate model performance under Very Friendly, Neutral, and Very Rude prompt variants across six tasks spanning STEM and Humanities domains, and analyze pairwise accuracy differences with statistical significance testing. Our results show that tone sensitivity is both model-dependent and domain-specific. Neutral or Very Friendly prompts generally yield higher accuracy than Very Rude prompts, but statistically significant effects appear only in a subset of Humanities tasks, where rude tone reduces accuracy for GPT and Llama, while Gemini remains comparatively tone-insensitive. When performance is aggregated across tasks within each domain, tone effects diminish and largely lose statistical significance. Compared with earlier researches, these findings suggest that dataset scale and coverage materially influence the detection of tone effects. Overall, our study indicates that while interaction tone can matter in specific interpretive settings, modern LLMs are broadly robust to tonal variation in typical mixed-domain use, providing practical guidance for prompt design and model selection in real-world deployments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 160,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12817",
        "abs_url": "https://arxiv.org/abs/2512.12817",
        "pdf_url": "https://arxiv.org/pdf/2512.12817",
        "title": "Decoding Human and AI Persuasion in National College Debate: Analyzing Prepared Arguments Through Aristotle's Rhetorical Principles",
        "authors": [
            "Mengqian Wu",
            "Jiayi Zhang",
            "Raymond Z. Zhang"
        ],
        "comments": "",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)",
        "abstract": "Debate has been widely adopted as a strategy to enhance critical thinking skills in English Language Arts (ELA). One important skill in debate is forming effective argumentation, which requires debaters to select supportive evidence from literature and construct compelling claims. However, the training of this skill largely depends on human coaching, which is labor-intensive and difficult to scale. To better support students in preparing for debates, this study explores the potential of leveraging artificial intelligence to generate effective arguments. Specifically, we prompted GPT-4 to create an evidence card and compared it to those produced by human debaters. The evidence cards outline the arguments students will present and how those arguments will be delivered, including components such as literature-based evidence quotations, summaries of core ideas, verbatim reading scripts, and tags (i.e., titles of the arguments). We compared the quality of the arguments in the evidence cards created by GPT and student debaters using Aristotle's rhetorical principles: ethos (credibility), pathos (emotional appeal), and logos (logical reasoning). Through a systematic qualitative and quantitative analysis, grounded in the rhetorical principles, we identify the strengths and limitations of human and GPT in debate reasoning, outlining areas where AI's focus and justifications align with or diverge from human reasoning. Our findings contribute to the evolving role of AI-assisted learning interventions, offering insights into how student debaters can develop strategies that enhance their argumentation and reasoning skills.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 161,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12818",
        "abs_url": "https://arxiv.org/abs/2512.12818",
        "pdf_url": "https://arxiv.org/pdf/2512.12818",
        "title": "Hindsight is 20/20: Building Agent Memory that Retains, Recalls, and Reflects",
        "authors": [
            "Chris Latimer",
            "Nicoló Boschi",
            "Andrew Neeser",
            "Chris Bartholomew",
            "Gaurav Srivastava",
            "Xuan Wang",
            "Naren Ramakrishnan"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "Agent memory has been touted as a dimension of growth for LLM-based applications, enabling agents that can accumulate experience, adapt across sessions, and move beyond single-shot question answering. The current generation of agent memory systems treats memory as an external layer that extracts salient snippets from conversations, stores them in vector or graph-based stores, and retrieves top-k items into the prompt of an otherwise stateless model. While these systems improve personalization and context carry-over, they still blur the line between evidence and inference, struggle to organize information over long horizons, and offer limited support for agents that must explain their reasoning. We present Hindsight, a memory architecture that treats agent memory as a structured, first-class substrate for reasoning by organizing it into four logical networks that distinguish world facts, agent experiences, synthesized entity summaries, and evolving beliefs. This framework supports three core operations -- retain, recall, and reflect -- that govern how information is added, accessed, and updated. Under this abstraction, a temporal, entity aware memory layer incrementally turns conversational streams into a structured, queryable memory bank, while a reflection layer reasons over this bank to produce answers and to update information in a traceable way. On key long-horizon conversational memory benchmarks like LongMemEval and LoCoMo, Hindsight with an open-source 20B model lifts overall accuracy from 39% to 83.6% over a full-context baseline with the same backbone and outperforms full context GPT-4o. Scaling the backbone further pushes Hindsight to 91.4% on LongMemEval and up to 89.61% on LoCoMo (vs. 75.78% for the strongest prior open system), consistently outperforming existing memory architectures on multi-session and open-domain questions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 162,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12821",
        "abs_url": "https://arxiv.org/abs/2512.12821",
        "pdf_url": "https://arxiv.org/pdf/2512.12821",
        "title": "On the continuity of flows",
        "authors": [
            "Congzhou M Sha"
        ],
        "comments": "9 pages, 2 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Data Analysis, Statistics and Probability (physics.data-an)",
        "abstract": "Flow matching has emerged as a powerful framework for generative modeling through continuous normalizing flows. We investigate a potential topological constraint: when the prior distribution and target distribution have mismatched topology (e.g., unimodal to multimodal), the optimal velocity field under standard flow matching objectives may exhibit spatial discontinuities. We suggest that this discontinuity arises from the requirement that continuous flows must bifurcate to map a single mode to multiple modes, forcing particles to make discrete routing decisions at intermediate times. Through theoretical analysis on bimodal Gaussian mixtures, we demonstrate that the optimal velocity field exhibits jump discontinuities along decision boundaries, with magnitude approaching infinity as time approaches the target distribution. Our analysis suggests that this phenomenon is not specific to $L^2$ loss, but rather may be a consequence of topological mismatch between distributions. We validate our theory empirically and discuss potential implications for flow matching on manifolds, connecting our findings to recent work on Riemannian flow matching and the challenge of learning discontinuous representations in neural networks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 163,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12824",
        "abs_url": "https://arxiv.org/abs/2512.12824",
        "pdf_url": "https://arxiv.org/pdf/2512.12824",
        "title": "Adapting Multimodal Foundation Models for Few-Shot Learning: A Comprehensive Study on Contrastive Captioners",
        "authors": [
            "N.K.B.M.P.K.B. Narasinghe",
            "Uthayasanker Thayasivam"
        ],
        "comments": "9 pages, 3 figures. Accepted to VISAPP 2026",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Large-scale multimodal foundation models, particularly Contrastive Captioners (CoCa), have achieved state-of-the-art results by unifying contrastive alignment with generative captioning. While zero-shot transfer capabilities are well-documented, the adaptation of these generative-contrastive hybrids to downstream tasks with extreme data scarcity (few-shot learning) remains under-explored. Existing literature predominantly focuses on dual-encoder architectures like CLIP, leaving a gap in understanding how CoCa's distinct latent space responds to parameter-efficient fine-tuning (PEFT). This paper presents a comprehensive empirical study on adapting the CoCa visual backbone for few-shot image classification. We systematically evaluate a hierarchy of strategies, ranging from training-free hybrid prototyping to deep parameter adaptation via Low-Rank Adaptation (LoRA). First, we identify an \"augmentation divergence\": while strong data augmentation degrades the performance of linear probing in low-shot settings, it is essential for stabilizing LoRA fine-tuning. We also demonstrate that hybrid objectives incorporating Supervised Contrastive (SupCon) loss yield consistent performance improvements over standard Cross-Entropy across varying shot counts. Crucially, we characterize the sensitivity of training configurations to data scarcity, providing empirical reference settings for scaling regularization, rank, and sampling strategies to facilitate the efficient adaptation of generative-contrastive foundation models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 164,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12832",
        "abs_url": "https://arxiv.org/abs/2512.12832",
        "pdf_url": "https://arxiv.org/pdf/2512.12832",
        "title": "Network Level Evaluation of Hangup Susceptibility of HRGCs using Deep Learning and Sensing Techniques: A Goal Towards Safer Future",
        "authors": [
            "Kaustav Chatterjee",
            "Joshua Li",
            "Kundan Parajulee",
            "Jared Schwennesen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Steep profiled Highway Railway Grade Crossings (HRGCs) pose safety hazards to vehicles with low ground clearance, which may become stranded on the tracks, creating risks of train vehicle collisions. This research develops a framework for network level evaluation of hangup susceptibility of HRGCs. Profile data from different crossings in Oklahoma were collected using both a walking profiler and the Pave3D8K Laser Imaging System. A hybrid deep learning model, combining Long Short Term Memory (LSTM) and Transformer architectures, was developed to reconstruct accurate HRGC profiles from Pave3D8K Laser Imaging System data. Vehicle dimension data from around 350 specialty vehicles were collected at various locations across Oklahoma to enable up to date statistical design dimensions. Hangup susceptibility was analyzed using three vehicle dimension scenarios (a) median dimension (median wheelbase and ground clearance), (b) 75 25 percentile dimension (75 percentile wheelbase, 25 percentile ground clearance), and (c) worst case dimension (maximum wheelbase and minimum ground clearance). Results indicate 36, 62, and 67 crossings at the highest hangup risk levels under these scenarios, respectively. An ArcGIS database and a software interface were developed to support transportation agencies in mitigating crossing hazards. This framework advances safety evaluation by integrating next generation sensing, deep learning, and infrastructure datasets into practical decision support tools.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 165,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12840",
        "abs_url": "https://arxiv.org/abs/2512.12840",
        "pdf_url": "https://arxiv.org/pdf/2512.12840",
        "title": "PRIVEE: Privacy-Preserving Vertical Federated Learning Against Feature Inference Attacks",
        "authors": [
            "Sindhuja Madabushi",
            "Ahmad Faraz Khan",
            "Haider Ali",
            "Ananthram Swami",
            "Rui Ning",
            "Hongyi Wu",
            "Jin-Hee Cho"
        ],
        "comments": "12 pages, 3 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Vertical Federated Learning (VFL) enables collaborative model training across organizations that share common user samples but hold disjoint feature spaces. Despite its potential, VFL is susceptible to feature inference attacks, in which adversarial parties exploit shared confidence scores (i.e., prediction probabilities) during inference to reconstruct private input features of other participants. To counter this threat, we propose PRIVEE (PRIvacy-preserving Vertical fEderated lEarning), a novel defense mechanism named after the French word privée, meaning \"private.\" PRIVEE obfuscates confidence scores while preserving critical properties such as relative ranking and inter-score distances. Rather than exposing raw scores, PRIVEE shares only the transformed representations, mitigating the risk of reconstruction attacks without degrading model prediction accuracy. Extensive experiments show that PRIVEE achieves a threefold improvement in privacy protection compared to state-of-the-art defenses, while preserving full predictive performance against advanced feature inference attacks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 166,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12842",
        "abs_url": "https://arxiv.org/abs/2512.12842",
        "pdf_url": "https://arxiv.org/pdf/2512.12842",
        "title": "SAGA: Open-World Mobile Manipulation via Structured Affordance Grounding",
        "authors": [
            "Kuan Fang",
            "Yuxin Chen",
            "Xinghao Zhu",
            "Farzad Niroui",
            "Lingfeng Sun",
            "Jiuguang Wang"
        ],
        "comments": "9 pages, 7 figures",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "We present SAGA, a versatile and adaptive framework for visuomotor control that can generalize across various environments, task objectives, and user specifications. To efficiently learn such capability, our key idea is to disentangle high-level semantic intent from low-level visuomotor control by explicitly grounding task objectives in the observed environment. Using an affordance-based task representation, we express diverse and complex behaviors in a unified, structured form. By leveraging multimodal foundation models, SAGA grounds the proposed task representation to the robot's visual observation as 3D affordance heatmaps, highlighting task-relevant entities while abstracting away spurious appearance variations that would hinder generalization. These grounded affordances enable us to effectively train a conditional policy on multi-task demonstration data for whole-body control. In a unified framework, SAGA can solve tasks specified in different forms, including language instructions, selected points, and example demonstrations, enabling both zero-shot execution and few-shot adaptation. We instantiate SAGA on a quadrupedal manipulator and conduct extensive experiments across eleven real-world tasks. SAGA consistently outperforms end-to-end and modular baselines by substantial margins. Together, these results demonstrate that structured affordance grounding offers a scalable and effective pathway toward generalist mobile manipulation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 167,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12844",
        "abs_url": "https://arxiv.org/abs/2512.12844",
        "pdf_url": "https://arxiv.org/pdf/2512.12844",
        "title": "Selective Conformal Risk Control",
        "authors": [
            "Yunpeng Xu",
            "Wenge Guo",
            "Zhi Wei"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Reliable uncertainty quantification is essential for deploying machine learning systems in high-stakes domains. Conformal prediction provides distribution-free coverage guarantees but often produces overly large prediction sets, limiting its practical utility. To address this issue, we propose \\textit{Selective Conformal Risk Control} (SCRC), a unified framework that integrates conformal prediction with selective classification. The framework formulates uncertainty control as a two-stage problem: the first stage selects confident samples for prediction, and the second stage applies conformal risk control on the selected subset to construct calibrated prediction sets. We develop two algorithms under this framework. The first, SCRC-T, preserves exchangeability by computing thresholds jointly over calibration and test samples, offering exact finite-sample guarantees. The second, SCRC-I, is a calibration-only variant that provides PAC-style probabilistic guarantees while being more computational efficient. Experiments on two public datasets show that both methods achieve the target coverage and risk levels, with nearly identical performance, while SCRC-I exhibits slightly more conservative risk control but superior computational practicality. Our results demonstrate that selective conformal risk control offers an effective and efficient path toward compact, reliable uncertainty quantification.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 168,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12858",
        "abs_url": "https://arxiv.org/abs/2512.12858",
        "pdf_url": "https://arxiv.org/pdf/2512.12858",
        "title": "Information-Consistent Language Model Recommendations through Group Relative Policy Optimization",
        "authors": [
            "Sonal Prabhune",
            "Balaji Padmanabhan",
            "Kaushik Dutta"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Large Language Models (LLMs) are increasingly deployed in business-critical domains such as finance, education, healthcare, and customer support, where users expect consistent and reliable recommendations. Yet LLMs often exhibit variability when prompts are phrased with minor differences, even when semantically equivalent. Such inconsistency undermines trust, complicates compliance, and disrupts user experience. While personalization is desirable in certain contexts, many enterprise scenarios-such as HR onboarding, customer support, or policy disclosure-require invariant information delivery regardless of phrasing or prior conversational history. Existing approaches, including retrieval-augmented generation (RAG) and temperature tuning, improve factuality or reduce stochasticity but cannot guarantee stability across equivalent prompts. In this paper, we propose a reinforcement learning framework based on Group Relative Policy Optimization (GRPO) to directly optimize for consistency. Unlike prior applications of GRPO, which have been limited to reasoning and code generation, we adapt GRPO to enforce stability of information content across groups of semantically equivalent prompts. We introduce entropy-based helpfulness and stability rewards, treating prompt variants as groups and resetting conversational context to isolate phrasing effects. Experiments on investment and job recommendation tasks show that our GRPO-trained model reduces variability more effectively than fine-tuning or decoding-based baselines. To our knowledge, this is a novel application of GRPO for aligning LLMs toward information consistency, reframing variability not as an acceptable feature of generative diversity but as a correctable flaw in enterprise deployments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 169,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12868",
        "abs_url": "https://arxiv.org/abs/2512.12868",
        "pdf_url": "https://arxiv.org/pdf/2512.12868",
        "title": "Counting Clues: A Lightweight Probabilistic Baseline Can Match an LLM",
        "authors": [
            "Furong Jia",
            "Yuan Pu",
            "Finn Guo",
            "Monica Agrawal"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Large language models (LLMs) excel on multiple-choice clinical diagnosis benchmarks, yet it is unclear how much of this performance reflects underlying probabilistic reasoning. We study this through questions from MedQA, where the task is to select the most likely diagnosis. We introduce the Frequency-Based Probabilistic Ranker (FBPR), a lightweight method that scores options with a smoothed Naive Bayes over concept-diagnosis co-occurrence statistics from a large corpus. When co-occurrence statistics were sourced from the pretraining corpora for OLMo and Llama, FBPR achieves comparable performance to the corresponding LLMs pretrained on that same corpus. Direct LLM inference and FBPR largely get different questions correct, with an overlap only slightly above random chance, indicating complementary strengths of each method. These findings highlight the continued value of explicit probabilistic baselines: they provide a meaningful performance reference point and a complementary signal for potential hybridization. While the performance of LLMs seems to be driven by a mechanism other than simple frequency aggregation, we show that an approach similar to the historically grounded, low-complexity expert systems still accounts for a substantial portion of benchmark performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 170,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12870",
        "abs_url": "https://arxiv.org/abs/2512.12870",
        "pdf_url": "https://arxiv.org/pdf/2512.12870",
        "title": "Optimal Labeler Assignment and Sampling for Active Learning in the Presence of Imperfect Labels",
        "authors": [
            "Pouya Ahadi",
            "Blair Winograd",
            "Camille Zaug",
            "Karunesh Arora",
            "Lijun Wang",
            "Kamran Paynabar"
        ],
        "comments": "22 pages, 6 figures. Preprint under review",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Optimization and Control (math.OC)",
        "abstract": "Active Learning (AL) has garnered significant interest across various application domains where labeling training data is costly. AL provides a framework that helps practitioners query informative samples for annotation by oracles (labelers). However, these labels often contain noise due to varying levels of labeler accuracy. Additionally, uncertain samples are more prone to receiving incorrect labels because of their complexity. Learning from imperfectly labeled data leads to an inaccurate classifier. We propose a novel AL framework to construct a robust classification model by minimizing noise levels. Our approach includes an assignment model that optimally assigns query points to labelers, aiming to minimize the maximum possible noise within each cycle. Additionally, we introduce a new sampling method to identify the best query points, reducing the impact of label noise on classifier performance. Our experiments demonstrate that our approach significantly improves classification performance compared to several benchmark methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 171,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12888",
        "abs_url": "https://arxiv.org/abs/2512.12888",
        "pdf_url": "https://arxiv.org/pdf/2512.12888",
        "title": "Meta-GPT: Decoding the Metasurface Genome with Generative Artificial Intelligence",
        "authors": [
            "David Dang",
            "Stuart Love",
            "Meena Salib",
            "Quynh Dang",
            "Samuel Rothfarb",
            "Mysk Alnatour",
            "Andrew Salij",
            "Hou-Tong Chen",
            "Ho Wai",
            "Wilton J.M. Kort-Kamp"
        ],
        "comments": "Keywords: Physics-informed machine learning; Transformer models; Reinforcement learning; Chain-of-thought reasoning; Metasurfaces; Nanophotonics; Inverse design",
        "subjects": "Optics (physics.optics); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Advancing artificial intelligence for physical sciences requires representations that are both interpretable and compatible with the underlying laws of nature. We introduce METASTRINGS, a symbolic language for photonics that expresses nanostructures as textual sequences encoding materials, geometries, and lattice configurations. Analogous to molecular textual representations in chemistry, METASTRINGS provides a framework connecting human interpretability with computational design by capturing the structural hierarchy of photonic metasurfaces. Building on this representation, we develop Meta-GPT, a foundation transformer model trained on METASTRINGS and finetuned with physics-informed supervised, reinforcement, and chain-of-thought learning. Across various design tasks, the model achieves <3% mean-squared spectral error and maintains >98% syntactic validity, generating diverse metasurface prototypes whose experimentally measured optical responses match their target spectra. These results demonstrate that Meta-GPT can learn the compositional rules of light-matter interactions through METASTRINGS, laying a rigorous foundation for AI-driven photonics and representing an important step toward a metasurface genome project.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 172,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12914",
        "abs_url": "https://arxiv.org/abs/2512.12914",
        "pdf_url": "https://arxiv.org/pdf/2512.12914",
        "title": "CTIGuardian: A Few-Shot Framework for Mitigating Privacy Leakage in Fine-Tuned LLMs",
        "authors": [
            "Shashie Dilhara Batan Arachchige",
            "Benjamin Zi Hao Zhao",
            "Hassan Jameel Asghar",
            "Dinusha Vatsalan",
            "Dali Kaafar"
        ],
        "comments": "Accepted at the 18th Cybersecurity Experimentation and Test Workshop (CSET), in conjunction with ACSAC 2025",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Large Language Models (LLMs) are often fine-tuned to adapt their general-purpose knowledge to specific tasks and domains such as cyber threat intelligence (CTI). Fine-tuning is mostly done through proprietary datasets that may contain sensitive information. Owners expect their fine-tuned model to not inadvertently leak this information to potentially adversarial end users. Using CTI as a use case, we demonstrate that data-extraction attacks can recover sensitive information from fine-tuned models on CTI reports, underscoring the need for mitigation. Retraining the full model to eliminate this leakage is computationally expensive and impractical. We propose an alternative approach, which we call privacy alignment, inspired by safety alignment in LLMs. Just like safety alignment teaches the model to abide by safety constraints through a few examples, we enforce privacy alignment through few-shot supervision, integrating a privacy classifier and a privacy redactor, both handled by the same underlying LLM. We evaluate our system, called CTIGuardian, using GPT-4o mini and Mistral-7B Instruct models, benchmarking against Presidio, a named entity recognition (NER) baseline. Results show that CTIGuardian provides a better privacy-utility trade-off than NER based models. While we demonstrate its effectiveness on a CTI use case, the framework is generic enough to be applicable to other sensitive domains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 173,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12921",
        "abs_url": "https://arxiv.org/abs/2512.12921",
        "pdf_url": "https://arxiv.org/pdf/2512.12921",
        "title": "Cisco Integrated AI Security and Safety Framework Report",
        "authors": [
            "Amy Chang",
            "Tiffany Saade",
            "Sanket Mendapara",
            "Adam Swanda",
            "Ankit Garg"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
        "abstract": "Artificial intelligence (AI) systems are being readily and rapidly adopted, increasingly permeating critical domains: from consumer platforms and enterprise software to networked systems with embedded agents. While this has unlocked potential for human productivity gains, the attack surface has expanded accordingly: threats now span content safety failures (e.g., harmful or deceptive outputs), model and data integrity compromise (e.g., poisoning, supply-chain tampering), runtime manipulations (e.g., prompt injection, tool and agent misuse), and ecosystem risks (e.g., orchestration abuse, multi-agent collusion). Existing frameworks such as MITRE ATLAS, National Institute of Standards and Technology (NIST) AI 100-2 Adversarial Machine Learning (AML) taxonomy, and OWASP Top 10s for Large Language Models (LLMs) and Agentic AI Applications provide valuable viewpoints, but each covers only slices of this multi-dimensional space. This paper presents Cisco's Integrated AI Security and Safety Framework (\"AI Security Framework\"), a unified, lifecycle-aware taxonomy and operationalization framework that can be used to classify, integrate, and operationalize the full range of AI risks. It integrates AI security and AI safety across modalities, agents, pipelines, and the broader ecosystem. The AI Security Framework is designed to be practical for threat identification, red-teaming, risk prioritization, and it is comprehensive in scope and can be extensible to emerging deployments in multimodal contexts, humanoids, wearables, and sensory infrastructures. We analyze gaps in prevailing frameworks, discuss design principles for our framework, and demonstrate how the taxonomy provides structure for understanding how modern AI systems fail, how adversaries exploit these failures, and how organizations can build defenses across the AI lifecycle that evolve alongside capability advancements.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 174,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12932",
        "abs_url": "https://arxiv.org/abs/2512.12932",
        "pdf_url": "https://arxiv.org/pdf/2512.12932",
        "title": "Investigating Data Pruning for Pretraining Biological Foundation Models at Scale",
        "authors": [
            "Yifan Wu",
            "Jiyue Jiang",
            "Xichen Ye",
            "Yiqi Wang",
            "Chang Zhou",
            "Yitao Xu",
            "Jiayang Chen",
            "He Hu",
            "Weizhong Zhang",
            "Cheng Jin",
            "Jiao Yuan",
            "Yu Li"
        ],
        "comments": "Accepted by AAAI 2026",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE)",
        "abstract": "Biological foundation models (BioFMs), pretrained on large-scale biological sequences, have recently shown strong potential in providing meaningful representations for diverse downstream bioinformatics tasks. However, such models often rely on millions to billions of training sequences and billions of parameters, resulting in prohibitive computational costs and significant barriers to reproducibility and accessibility, particularly for academic labs. To address these challenges, we investigate the feasibility of data pruning for BioFM pretraining and propose a post-hoc influence-guided data pruning framework tailored to biological domains. Our approach introduces a subset-based self-influence formulation that enables efficient estimation of sample importance at low computational cost, and builds upon it two simple yet effective selection strategies, namely Top-k Influence (Top I) and Coverage-Centric Influence (CCI). We empirically validate our method on two representative BioFMs, RNA-FM and ESM-C. For RNA, our framework consistently outperforms random selection baselines under an extreme pruning rate of over 99 percent, demonstrating its effectiveness. Furthermore, we show the generalizability of our framework on protein-related tasks using ESM-C. In particular, our coreset even outperforms random subsets that are ten times larger in both RNA and protein settings, revealing substantial redundancy in biological sequence datasets. These findings underscore the potential of influence-guided data pruning to substantially reduce the computational cost of BioFM pretraining, paving the way for more efficient, accessible, and sustainable biological AI research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 175,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12936",
        "abs_url": "https://arxiv.org/abs/2512.12936",
        "pdf_url": "https://arxiv.org/pdf/2512.12936",
        "title": "Content Adaptive based Motion Alignment Framework for Learned Video Compression",
        "authors": [
            "Tiange Zhang",
            "Xiandong Meng",
            "Siwei Ma"
        ],
        "comments": "Accepted to Data Compression Conference (DCC) 2026 as a poster paper",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Recent advances in end-to-end video compression have shown promising results owing to their unified end-to-end learning optimization. However, such generalized frameworks often lack content-specific adaptation, leading to suboptimal compression performance. To address this, this paper proposes a content adaptive based motion alignment framework that improves performance by adapting encoding strategies to diverse content characteristics. Specifically, we first introduce a two-stage flow-guided deformable warping mechanism that refines motion compensation with coarse-to-fine offset prediction and mask modulation, enabling precise feature alignment. Second, we propose a multi-reference quality aware strategy that adjusts distortion weights based on reference quality, and applies it to hierarchical training to reduce error propagation. Third, we integrate a training-free module that downsamples frames by motion magnitude and resolution to obtain smooth motion estimation. Experimental results on standard test datasets demonstrate that our framework CAMA achieves significant improvements over state-of-the-art Neural Video Compression models, achieving a 24.95% BD-rate (PSNR) savings over our baseline model DCVC-TCM, while also outperforming reproduced DCVC-DC and traditional codec HM-16.25.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 176,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12950",
        "abs_url": "https://arxiv.org/abs/2512.12950",
        "pdf_url": "https://arxiv.org/pdf/2512.12950",
        "title": "Building from Scratch: A Multi-Agent Framework with Human-in-the-Loop for Multilingual Legal Terminology Mapping",
        "authors": [
            "Lingyi Meng",
            "Maolin Liu",
            "Hao Wang",
            "Yilan Cheng",
            "Qi Yang",
            "Idlkaid Mohanmmed"
        ],
        "comments": "43 pages, 6 fingures, accepted in Artificial Intelligence and Law",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Accurately mapping legal terminology across languages remains a significant challenge, especially for language pairs like Chinese and Japanese, which share a large number of homographs with different meanings. Existing resources and standardized tools for these languages are limited. To address this, we propose a human-AI collaborative approach for building a multilingual legal terminology database, based on a multi-agent framework. This approach integrates advanced large language models and legal domain experts throughout the entire process-from raw document preprocessing, article-level alignment, to terminology extraction, mapping, and quality assurance. Unlike a single automated pipeline, our approach places greater emphasis on how human experts participate in this multi-agent system. Humans and AI agents take on different roles: AI agents handle specific, repetitive tasks, such as OCR, text segmentation, semantic alignment, and initial terminology extraction, while human experts provide crucial oversight, review, and supervise the outputs with contextual knowledge and legal judgment. We tested the effectiveness of this framework using a trilingual parallel corpus comprising 35 key Chinese statutes, along with their English and Japanese translations. The experimental results show that this human-in-the-loop, multi-agent workflow not only improves the precision and consistency of multilingual legal terminology mapping but also offers greater scalability compared to traditional manual methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 177,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.12997",
        "abs_url": "https://arxiv.org/abs/2512.12997",
        "pdf_url": "https://arxiv.org/pdf/2512.12997",
        "title": "Calibrating Uncertainty for Zero-Shot Adversarial CLIP",
        "authors": [
            "Wenjing lu",
            "Zerui Tao",
            "Dongping Zhang",
            "Yuning Qiu",
            "Yang Yang",
            "Qibin Zhao"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "CLIP delivers strong zero-shot classification but remains highly vulnerable to adversarial attacks. Previous work of adversarial fine-tuning largely focuses on matching the predicted logits between clean and adversarial examples, which overlooks uncertainty calibration and may degrade the zero-shot generalization. A common expectation in reliable uncertainty estimation is that predictive uncertainty should increase as inputs become more difficult or shift away from the training distribution. However, we frequently observe the opposite in the adversarial setting: perturbations not only degrade accuracy but also suppress uncertainty, leading to severe miscalibration and unreliable over-confidence. This overlooked phenomenon highlights a critical reliability gap beyond robustness. To bridge this gap, we propose a novel adversarial fine-tuning objective for CLIP considering both prediction accuracy and uncertainty alignments. By reparameterizing the output of CLIP as the concentration parameter of a Dirichlet distribution, we propose a unified representation that captures relative semantic structure and the magnitude of predictive confidence. Our objective aligns these distributions holistically under perturbations, moving beyond single-logit anchoring and restoring calibrated uncertainty. Experiments on multiple zero-shot classification benchmarks demonstrate that our approach effectively restores calibrated uncertainty and achieves competitive adversarial robustness while maintaining clean accuracy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 178,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13033",
        "abs_url": "https://arxiv.org/abs/2512.13033",
        "pdf_url": "https://arxiv.org/pdf/2512.13033",
        "title": "Scaling Bidirectional Spans and Span Violations in Attention Mechanism",
        "authors": [
            "Jongwook Kim",
            "Sangheon Yun",
            "Sukjin Yoon"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "The canonical $O(N^2)$ Transformer remains the empirical performance frontier in sequence modeling, and its training can be further optimized by addressing geometric inefficiency. We propose an optimization framework that leverages an asymmetric projection to decompose the backward-pass gradients into parallel spans and orthogonal violations, while keeping the canonical forward-pass $QKV$ structure intact. Through consistent experimental validation across various decomposition and projection setups, we provide strong theoretical evidence: the standard attention gradient is suboptimal. We demonstrated that selectively scaling these components, focusing primarily on $0^{th}$ order bidirectional parallel spans, yields the most effective learning signal. On the limited WikiText-2 dataset, and using a crude configuration, this method achieved a $0.56\\%$ reduction in validation loss, confirming the framework's fundamental validity and suggesting significant potential gains on larger datasets and deeper training regimes",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 179,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13063",
        "abs_url": "https://arxiv.org/abs/2512.13063",
        "pdf_url": "https://arxiv.org/pdf/2512.13063",
        "title": "LLM Rationalis? Measuring Bargaining Capabilities of AI Negotiators",
        "authors": [
            "Cheril Shah",
            "Akshit Agarwal",
            "Kanak Garg",
            "Mourad Heddaya"
        ],
        "comments": "Published in the First Workshop on Multi-Turn Interactions in Large Language Models at Neurips 2025",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Bilateral negotiation is a complex, context-sensitive task in which human negotiators dynamically adjust anchors, pacing, and flexibility to exploit power asymmetries and informal cues. We introduce a unified mathematical framework for modeling concession dynamics based on a hyperbolic tangent curve, and propose two metrics burstiness tau and the Concession-Rigidity Index (CRI) to quantify the timing and rigidity of offer trajectories. We conduct a large-scale empirical comparison between human negotiators and four state-of-the-art large language models (LLMs) across natural-language and numeric-offers settings, with and without rich market context, as well as six controlled power-asymmetry scenarios. Our results reveal that, unlike humans who smoothly adapt to situations and infer the opponents position and strategies, LLMs systematically anchor at extremes of the possible agreement zone for negotiations and optimize for fixed points irrespective of leverage or context. Qualitative analysis further shows limited strategy diversity and occasional deceptive tactics used by LLMs. Moreover the ability of LLMs to negotiate does not improve with better models. These findings highlight fundamental limitations in current LLM negotiation capabilities and point to the need for models that better internalize opponent reasoning and context-dependent strategy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 180,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13074",
        "abs_url": "https://arxiv.org/abs/2512.13074",
        "pdf_url": "https://arxiv.org/pdf/2512.13074",
        "title": "A Simple and Effective Framework for Symmetric Consistent Indexing in Large-Scale Dense Retrieval",
        "authors": [
            "Huimu Wang",
            "Yiming Qiu",
            "Xingzhi Yao",
            "Zhiguo Chen",
            "Guoyu Tang",
            "Songlin Wang",
            "Sulong Xu",
            "Mingming Li"
        ],
        "comments": "",
        "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI)",
        "abstract": "Dense retrieval has become the industry standard in large-scale information retrieval systems due to its high efficiency and competitive accuracy. Its core relies on a coarse-to-fine hierarchical architecture that enables rapid candidate selection and precise semantic matching, achieving millisecond-level response over billion-scale corpora. This capability makes it essential not only in traditional search and recommendation scenarios but also in the emerging paradigm of generative recommendation driven by large language models, where semantic IDs-themselves a form of coarse-to-fine representation-play a foundational role. However, the widely adopted dual-tower encoding architecture introduces inherent challenges, primarily representational space misalignment and retrieval index inconsistency, which degrade matching accuracy, retrieval stability, and performance on long-tail queries. These issues are further magnified in semantic ID generation, ultimately limiting the performance ceiling of downstream generative models. To address these challenges, this paper proposes a simple and effective framework named SCI comprising two synergistic modules: a symmetric representation alignment module that employs an innovative input-swapping mechanism to unify the dual-tower representation space without adding parameters, and an consistent indexing with dual-tower synergy module that redesigns retrieval paths using a dual-view indexing strategy to maintain consistency from training to inference. The framework is systematic, lightweight, and engineering-friendly, requiring minimal overhead while fully supporting billion-scale deployment. We provide theoretical guarantees for our approach, with its effectiveness validated by results across public datasets and real-world e-commerce datasets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 181,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13094",
        "abs_url": "https://arxiv.org/abs/2512.13094",
        "pdf_url": "https://arxiv.org/pdf/2512.13094",
        "title": "Sequence of Expert: Boosting Imitation Planners for Autonomous Driving through Temporal Alternation",
        "authors": [
            "Xiang Li",
            "Gang Liu",
            "Weitao Zhou",
            "Hongyi Zhu",
            "Zhong Cao"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
        "abstract": "Imitation learning (IL) has emerged as a central paradigm in autonomous driving. While IL excels in matching expert behavior in open-loop settings by minimizing per-step prediction errors, its performance degrades unexpectedly in closed-loop due to the gradual accumulation of small, often imperceptible errors over this http URL successive planning cycles, these errors compound, potentially resulting in severe this http URL research efforts predominantly rely on increasingly sophisticated network architectures or high-fidelity training datasets to enhance the robustness of IL planners against error accumulation, focusing on the state-level robustness at a single time point. However, autonomous driving is inherently a continuous-time process, and leveraging the temporal scale to enhance robustness may provide a new perspective for addressing this this http URL this end, we propose a method termed Sequence of Experts (SoE), a temporal alternation policy that enhances closed-loop performance without increasing model size or data requirements. Our experiments on large-scale autonomous driving benchmarks nuPlan demonstrate that SoE method consistently and significantly improves the performance of all the evaluated models, and achieves state-of-the-art this http URL module may provide a key and widely applicable support for improving the training efficiency of autonomous driving models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 182,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13100",
        "abs_url": "https://arxiv.org/abs/2512.13100",
        "pdf_url": "https://arxiv.org/pdf/2512.13100",
        "title": "OXE-AugE: A Large-Scale Robot Augmentation of OXE for Scaling Cross-Embodiment Policy Learning",
        "authors": [
            "Guanhua Ji",
            "Harsha Polavaram",
            "Lawrence Yunliang Chen",
            "Sandeep Bajamahal",
            "Zehan Ma",
            "Simeon Adebola",
            "Chenfeng Xu",
            "Ken Goldberg"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
        "abstract": "Large and diverse datasets are needed for training generalist robot policies that have potential to control a variety of robot embodiments -- robot arm and gripper combinations -- across diverse tasks and environments. As re-collecting demonstrations and retraining for each new hardware platform are prohibitively costly, we show that existing robot data can be augmented for transfer and generalization. The Open X-Embodiment (OXE) dataset, which aggregates demonstrations from over 60 robot datasets, has been widely used as the foundation for training generalist policies. However, it is highly imbalanced: the top four robot types account for over 85\\% of its real data, which risks overfitting to robot--scene combinations. We present AugE-Toolkit, a scalable robot augmentation pipeline, and OXE-AugE, a high-quality open-source dataset that augments OXE with 9 different robot embodiments. OXE-AugE provides over 4.4 million trajectories, more than triple the size of the original OXE. We conduct a systematic study of how scaling robot augmentation impacts cross-embodiment learning. Results suggest that augmenting datasets with diverse arms and grippers improves policy performance not only on the augmented robots, but also on unseen robots and even the original robots under distribution shifts. In physical experiments, we demonstrate that state-of-the-art generalist policies such as OpenVLA and $\\pi_0$ benefit from fine-tuning on OXE-AugE, improving success rates by 24-45% on previously unseen robot--gripper combinations across four real-world manipulation tasks. Project website: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 183,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13106",
        "abs_url": "https://arxiv.org/abs/2512.13106",
        "pdf_url": "https://arxiv.org/pdf/2512.13106",
        "title": "TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning",
        "authors": [
            "Shenzhi Yang",
            "Guangcheng Zhu",
            "Xing Zheng",
            "Yingfan MA",
            "Zhongqi Chen",
            "Bowen Song",
            "Weiqiang Wang",
            "Junbo Zhao",
            "Gang Chen",
            "Haobo Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Reinforcement learning with verifiable rewards (RLVR) has proven effective in training large reasoning models (LRMs) by leveraging answer-verifiable signals to guide policy optimization, which, however, suffers from high annotation costs. To alleviate this problem, recent work has explored unsupervised RLVR methods that derive rewards solely from the model's internal consistency, such as through entropy and majority voting. While seemingly promising, these methods often suffer from model collapse in the later stages of training, which may arise from the reinforcement of incorrect reasoning patterns in the absence of external supervision. In this work, we investigate a novel semi-supervised RLVR paradigm that utilizes a small labeled set to guide RLVR training on unlabeled samples. Our key insight is that supervised rewards are essential for stabilizing consistency-based training on unlabeled samples, ensuring that only reasoning patterns verified on labeled instances are incorporated into RL training. Technically, we propose an effective policy optimization algorithm, TraPO, that identifies reliable unlabeled samples by matching their learning trajectory similarity to labeled ones. Building on this, TraPO achieves remarkable data efficiency and strong generalization on six widely used mathematical reasoning benchmarks (AIME24/25, AMC, MATH-500, Minerva, and Olympiad) and three out-of-distribution tasks (ARC-c, GPQA-diamond, and MMLU-pro). With only 1K labeled and 3K unlabeled samples, TraPO reaches 42.6% average accuracy, surpassing the best unsupervised method trained on 45K unlabeled samples (38.3%). Notably, when using 4K labeled and 12K unlabeled samples, TraPO even outperforms the fully supervised model trained on the full 45K labeled samples on all benchmarks, while using only 10% of the labeled data. The code is available via this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 184,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13109",
        "abs_url": "https://arxiv.org/abs/2512.13109",
        "pdf_url": "https://arxiv.org/pdf/2512.13109",
        "title": "Uncovering the Role of Initial Saliency in U-Shaped Attention Bias: Scaling Initial Token Weight for Enhanced Long-Text Processing",
        "authors": [
            "Zewen Qiang",
            "Sendong Zhao",
            "Haochun Wang",
            "Bing Qin",
            "Ting Liu"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Large language models (LLMs) have demonstrated strong performance on a variety of natural language processing (NLP) tasks. However, they often struggle with long-text sequences due to the ``lost in the middle'' phenomenon. This issue has been shown to arise from a U-shaped attention bias, where attention is disproportionately focused on the beginning and end of a text, leaving the middle section underrepresented. While previous studies have attributed this bias to position encoding, our research first identifies an additional factor: initial saliency. It means that in the attention computation for each token, tokens with higher attention weights relative to the initial token tend to receive more attention in the prediction of the next token. We further find that utilizing this property by scaling attention weight between the initial token and others improves the model's ability to process long contexts, achieving a maximum improvement of 3.6\\% in MDQA dataset. Moreover, combining this approach with existing methods to reduce position encoding bias further enhances performance, achieving a maximum improvement of 3.4\\% in KV-Retrieval tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 185,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13111",
        "abs_url": "https://arxiv.org/abs/2512.13111",
        "pdf_url": "https://arxiv.org/pdf/2512.13111",
        "title": "From Overfitting to Reliability: Introducing the Hierarchical Approximate Bayesian Neural Network",
        "authors": [
            "Hayk Amirkhanian",
            "Marco F. Huber"
        ],
        "comments": "9 pages main body, 1 Figure, 15 pages Appendix",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "In recent years, neural networks have revolutionized various domains, yet challenges such as hyperparameter tuning and overfitting remain significant hurdles. Bayesian neural networks offer a framework to address these challenges by incorporating uncertainty directly into the model, yielding more reliable predictions, particularly for out-of-distribution data. This paper presents Hierarchical Approximate Bayesian Neural Network, a novel approach that uses a Gaussian-inverse-Wishart distribution as a hyperprior of the network's weights to increase both the robustness and performance of the model. We provide analytical representations for the predictive distribution and weight posterior, which amount to the calculation of the parameters of Student's t-distributions in closed form with linear complexity with respect to the number of weights. Our method demonstrates robust performance, effectively addressing issues of overfitting and providing reliable uncertainty estimates, particularly for out-of-distribution tasks. Experimental results indicate that HABNN not only matches but often outperforms state-of-the-art models, suggesting a promising direction for future applications in safety-critical environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 186,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13157",
        "abs_url": "https://arxiv.org/abs/2512.13157",
        "pdf_url": "https://arxiv.org/pdf/2512.13157",
        "title": "Intrinsic Image Fusion for Multi-View 3D Material Reconstruction",
        "authors": [
            "Peter Kocsis",
            "Lukas Höllein",
            "Matthias Nießner"
        ],
        "comments": "Project page: this https URL Video: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "We introduce Intrinsic Image Fusion, a method that reconstructs high-quality physically based materials from multi-view images. Material reconstruction is highly underconstrained and typically relies on analysis-by-synthesis, which requires expensive and noisy path tracing. To better constrain the optimization, we incorporate single-view priors into the reconstruction process. We leverage a diffusion-based material estimator that produces multiple, but often inconsistent, candidate decompositions per view. To reduce the inconsistency, we fit an explicit low-dimensional parametric function to the predictions. We then propose a robust optimization framework using soft per-view prediction selection together with confidence-based soft multi-view inlier set to fuse the most consistent predictions of the most confident views into a consistent parametric material space. Finally, we use inverse path tracing to optimize for the low-dimensional parameters. Our results outperform state-of-the-art methods in material disentanglement on both synthetic and real scenes, producing sharp and clean reconstructions suitable for high-quality relighting.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 187,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13165",
        "abs_url": "https://arxiv.org/abs/2512.13165",
        "pdf_url": "https://arxiv.org/pdf/2512.13165",
        "title": "SACn: Soft Actor-Critic with n-step Returns",
        "authors": [
            "Jakub Łyskawa",
            "Jakub Lewandowski",
            "Paweł Wawrzyński"
        ],
        "comments": "Accepted at ICAART 2026",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Soft Actor-Critic (SAC) is widely used in practical applications and is now one of the most relevant off-policy online model-free reinforcement learning (RL) methods. The technique of n-step returns is known to increase the convergence speed of RL algorithms compared to their 1-step returns-based versions. However, SAC is notoriously difficult to combine with n-step returns, since their usual combination introduces bias in off-policy algorithms due to the changes in action distribution. While this problem is solved by importance sampling, a method for estimating expected values of one distribution using samples from another distribution, importance sampling may result in numerical instability. In this work, we combine SAC with n-step returns in a way that overcomes this issue. We present an approach to applying numerically stable importance sampling with simplified hyperparameter selection. Furthermore, we analyze the entropy estimation approach of Soft Actor-Critic in the context of the n-step maximum entropy framework and formulate the $\\tau$-sampled entropy estimation to reduce the variance of the learning target. Finally, we formulate the Soft Actor-Critic with n-step returns (SAC$n$) algorithm that we experimentally verify on MuJoCo simulated environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 188,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13174",
        "abs_url": "https://arxiv.org/abs/2512.13174",
        "pdf_url": "https://arxiv.org/pdf/2512.13174",
        "title": "Carrot, stick, or both? Price incentives for sustainable food choice in competitive environments",
        "authors": [
            "Francesco Salvi",
            "Giuseppe Russo",
            "Adam Barla",
            "Vincent Moreau",
            "Robert West"
        ],
        "comments": "10 pages, 3 figures",
        "subjects": "General Economics (econ.GN); Artificial Intelligence (cs.AI)",
        "abstract": "Meat consumption is a major driver of global greenhouse gas emissions. While pricing interventions have shown potential to reduce meat intake, previous studies have focused on highly constrained environments with limited consumer choice. Here, we present the first large-scale field experiment to evaluate multiple pricing interventions in a real-world, competitive setting. Using a sequential crossover design with matched menus in a Swiss university campus, we systematically compared vegetarian-meal discounts (-2.5 CHF), meat surcharges (+2.5 CHF), and a combined scheme (-1.2 CHF=+1.2 CHF) across four campus cafeterias. Only the surcharge and combined interventions led to significant increases in vegetarian meal uptake--by 26.4% and 16.6%, respectively--and reduced CO2 emissions per meal by 7.4% and 11.3%, respectively. The surcharge, while effective, triggered a 12.3% drop in sales at intervention sites and a corresponding 14.9% increase in non-treated locations, hence causing a spillover effect that completely offset environmental gains. In contrast, the combined approach achieved meaningful emission reductions without significant effects on overall sales or revenue, making it both effective and economically viable. Notably, pricing interventions were equally effective for both vegetarian-leaning customers and habitual meat-eaters, stimulating change even within entrenched dietary habits. Our results show that balanced pricing strategies can reduce the carbon footprint of realistic food environments, but require coordinated implementation to maximize climate benefits and avoid unintended spillover effects.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 189,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13186",
        "abs_url": "https://arxiv.org/abs/2512.13186",
        "pdf_url": "https://arxiv.org/pdf/2512.13186",
        "title": "PolySet: Restoring the Statistical Ensemble Nature of Polymers for Machine Learning",
        "authors": [
            "Khalid Ferji"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Materials Science (cond-mat.mtrl-sci); Artificial Intelligence (cs.AI)",
        "abstract": "Machine-learning (ML) models in polymer science typically treat a polymer as a single, perfectly defined molecular graph, even though real materials consist of stochastic ensembles of chains with distributed lengths. This mismatch between physical reality and digital representation limits the ability of current models to capture polymer behaviour. Here we introduce PolySet, a framework that represents a polymer as a finite, weighted ensemble of chains sampled from an assumed molar-mass distribution. This ensemble-based encoding is independent of chemical detail, compatible with any molecular representation and illustrated here in the homopolymer case using a minimal language model. We show that PolySet retains higher-order distributional moments (such as Mz, Mz+1), enabling ML models to learn tail-sensitive properties with greatly improved stability and accuracy. By explicitly acknowledging the statistical nature of polymer matter, PolySet establishes a physically grounded foundation for future polymer machine learning, naturally extensible to copolymers, block architectures, and other complex topologies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 190,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13190",
        "abs_url": "https://arxiv.org/abs/2512.13190",
        "pdf_url": "https://arxiv.org/pdf/2512.13190",
        "title": "WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory",
        "authors": [
            "Jin Sob Kim",
            "Hyun Joon Park",
            "Wooseok Shin",
            "Dongil Park",
            "Sung Won Han"
        ],
        "comments": "Accepted to IEEE Transactions on Aerospace and Electronic Systems (TAES)",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "The Automatic Identification System (AIS) enables data-driven maritime surveillance but suffers from reliability issues and irregular intervals. We address vessel destination estimation using global-scope AIS data by proposing a differentiated approach that recasts long port-to-port trajectories as a nested sequence structure. Using spatial grids, this method mitigates spatio-temporal bias while preserving detailed resolution. We introduce a novel deep learning architecture, WAY, designed to process these reformulated trajectories for long-term destination estimation days to weeks in advance. WAY comprises a trajectory representation layer and Channel-Aggregative Sequential Processing (CASP) blocks. The representation layer generates multi-channel vector sequences from kinematic and non-kinematic features. CASP blocks utilize multi-headed channel- and self-attention for aggregation and sequential information delivery. Additionally, we propose a task-specialized Gradient Dropout (GD) technique to enable many-to-many training on single labels, preventing biased feedback surges by stochastically blocking gradient flow based on sample length. Experiments on 5-year AIS data demonstrate WAY's superiority over conventional spatial grid-based approaches regardless of trajectory progression. Results further confirm that adopting GD leads to performance gains. Finally, we explore WAY's potential for real-world application through multitask learning for ETA estimation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 191,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13194",
        "abs_url": "https://arxiv.org/abs/2512.13194",
        "pdf_url": "https://arxiv.org/pdf/2512.13194",
        "title": "Efficient Adaptive Rejection Sampling for Accelerating Speculative Decoding in Large Language Models",
        "authors": [
            "Chendong Sun"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Speculative Decoding is a prominent technique for accelerating the autoregressive inference of large language models (LLMs) by employing a fast draft model to propose candidate token sequences and a large target model to verify them in parallel. However, its core component -- the rejection sampling mechanism -- relies on a fixed, context-independent random threshold. This leads to a significant \"random rejection\" problem in high-uncertainty generation scenarios, where plausible candidate tokens are frequently rejected due to random chance, undermining inference efficiency. This paper introduces Efficient Adaptive Rejection Sampling (EARS), a novel method that dynamically adjusts the acceptance threshold by incorporating the target model's own predictive uncertainty, measured as \\(1 - \\max(P_{\\mathrm{target}})\\). By introducing a tolerance term proportional to this uncertainty, EARS intelligently relaxes the acceptance criterion when the model is uncertain, effectively reducing random rejections while maintaining strict standards when the model is confident. Experiments on creative writing and open-domain QA tasks demonstrate that EARS significantly enhances the efficiency of speculative decoding, achieving up to an 18.12% increase in throughput with a negligible 0.84% accuracy drop on the GSM8K benchmark. The method requires no modifications to model architectures and can be seamlessly integrated into existing speculative decoding frameworks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 192,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13235",
        "abs_url": "https://arxiv.org/abs/2512.13235",
        "pdf_url": "https://arxiv.org/pdf/2512.13235",
        "title": "CORE: Contrastive Masked Feature Reconstruction on Graphs",
        "authors": [
            "Jianyuan Bo",
            "Yuan Fang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "In the rapidly evolving field of self-supervised learning on graphs, generative and contrastive methodologies have emerged as two dominant approaches. Our study focuses on masked feature reconstruction (MFR), a generative technique where a model learns to restore the raw features of masked nodes in a self-supervised manner. We observe that both MFR and graph contrastive learning (GCL) aim to maximize agreement between similar elements. Building on this observation, we reveal a novel theoretical insight: under specific conditions, the objectives of MFR and node-level GCL converge, despite their distinct operational mechanisms. This theoretical connection suggests these approaches are complementary rather than fundamentally different, prompting us to explore their integration to enhance self-supervised learning on graphs. Our research presents Contrastive Masked Feature Reconstruction (CORE), a novel graph self-supervised learning framework that integrates contrastive learning into MFR. Specifically, we form positive pairs exclusively between the original and reconstructed features of masked nodes, encouraging the encoder to prioritize contextual information over the node's own features. Additionally, we leverage the masked nodes themselves as negative samples, combining MFR's reconstructive power with GCL's discriminative ability to better capture intrinsic graph structures. Empirically, our proposed framework CORE significantly outperforms MFR across node and graph classification tasks, demonstrating state-of-the-art results. In particular, CORE surpasses GraphMAE and GraphMAE2 by up to 2.80% and 3.72% on node classification tasks, and by up to 3.82% and 3.76% on graph classification tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 193,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13293",
        "abs_url": "https://arxiv.org/abs/2512.13293",
        "pdf_url": "https://arxiv.org/pdf/2512.13293",
        "title": "Intrinsic-Motivation Multi-Robot Social Formation Navigation with Coordinated Exploration",
        "authors": [
            "Hao Fua",
            "Wei Liu",
            "Shuai Zhoua"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
        "abstract": "This paper investigates the application of reinforcement learning (RL) to multi-robot social formation navigation, a critical capability for enabling seamless human-robot coexistence. While RL offers a promising paradigm, the inherent unpredictability and often uncooperative dynamics of pedestrian behavior pose substantial challenges, particularly concerning the efficiency of coordinated exploration among robots. To address this, we propose a novel coordinated-exploration multi-robot RL algorithm introducing an intrinsic motivation exploration. Its core component is a self-learning intrinsic reward mechanism designed to collectively alleviate policy conservatism. Moreover, this algorithm incorporates a dual-sampling mode within the centralized training and decentralized execution framework to enhance the representation of both the navigation policy and the intrinsic reward, leveraging a two-time-scale update rule to decouple parameter updates. Empirical results on social formation navigation benchmarks demonstrate the proposed algorithm's superior performance over existing state-of-the-art methods across crucial metrics. Our code and video demos are available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 194,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13298",
        "abs_url": "https://arxiv.org/abs/2512.13298",
        "pdf_url": "https://arxiv.org/pdf/2512.13298",
        "title": "MiniLingua: A Small Open-Source LLM for European Languages",
        "authors": [
            "Anna Aksenova",
            "Boris Zverkov",
            "Nicola Dainese",
            "Alexander Nikitin",
            "Pekka Marttinen"
        ],
        "comments": "9+6 pages, 6 figures and 3 tables in the main text. Code at this https URL",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Large language models are powerful but often limited by high computational cost, privacy concerns, and English-centric training. Recent progress demonstrates that small, efficient models with around one billion parameters can deliver strong results and enable on-device use. This paper introduces MiniLingua, a multilingual open-source LLM of one billion parameters trained from scratch for 13 European languages, designed to balance coverage and instruction-following capabilities. Based on evaluation results, the instruction-tuned version of MiniLingua outperforms EuroLLM, a model with a similar training approach but a larger training budget, on summarization, classification and both open- and closed-book question answering. Moreover, it remains competitive with more advanced state-of-the-art models on open-ended generation tasks. We release model weights, tokenizer and source code used for data processing and model training.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 195,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13300",
        "abs_url": "https://arxiv.org/abs/2512.13300",
        "pdf_url": "https://arxiv.org/pdf/2512.13300",
        "title": "No One Left Behind: How to Exploit the Incomplete and Skewed Multi-Label Data for Conversion Rate Prediction",
        "authors": [
            "Qinglin Jia",
            "Zhaocheng Du",
            "Chuhan Wu",
            "Huifeng Guo",
            "Ruiming Tang",
            "Shuting Shi",
            "Muyu Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)",
        "abstract": "In most real-world online advertising systems, advertisers typically have diverse customer acquisition goals. A common solution is to use multi-task learning (MTL) to train a unified model on post-click data to estimate the conversion rate (CVR) for these diverse targets. In practice, CVR prediction often encounters missing conversion data as many advertisers submit only a subset of user conversion actions due to privacy or other constraints, making the labels of multi-task data incomplete. If the model is trained on all available samples where advertisers submit user conversion actions, it may struggle when deployed to serve a subset of advertisers targeting specific conversion actions, as the training and deployment data distributions are mismatched. While considerable MTL efforts have been made, a long-standing challenge is how to effectively train a unified model with the incomplete and skewed multi-label data. In this paper, we propose a fine-grained Knowledge transfer framework for Asymmetric Multi-Label data (KAML). We introduce an attribution-driven masking strategy (ADM) to better utilize data with asymmetric multi-label data in training. However, the more relaxed masking in ADM is a double-edged sword: it provides additional training signals but also introduces noise due to skewed data. To address this, we propose a hierarchical knowledge extraction mechanism (HKE) to model the sample discrepancy within the target task tower. Finally, to maximize the utility of unlabeled samples, we incorporate ranking loss strategy to further enhance our model. The effectiveness of KAML has been demonstrated through comprehensive evaluations on offline industry datasets and online A/B tests, which show significant performance improvements over existing MTL baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 196,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13316",
        "abs_url": "https://arxiv.org/abs/2512.13316",
        "pdf_url": "https://arxiv.org/pdf/2512.13316",
        "title": "ALIGN-FL: Architecture-independent Learning through Invariant Generative component sharing in Federated Learning",
        "authors": [
            "Mayank Gulati",
            "Benedikt Groß",
            "Gerhard Wunder"
        ],
        "comments": "Accepted at 2025 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery (CyberC)",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "We present ALIGN-FL, a novel approach to distributed learning that addresses the challenge of learning from highly disjoint data distributions through selective sharing of generative components. Instead of exchanging full model parameters, our framework enables privacy-preserving learning by transferring only generative capabilities across clients, while the server performs global training using synthetic samples. Through complementary privacy mechanisms: DP-SGD with adaptive clipping and Lipschitz regularized VAE decoders and a stateful architecture supporting heterogeneous clients, we experimentally validate our approach on MNIST and Fashion-MNIST datasets with cross-domain outliers. Our analysis demonstrates that both privacy mechanisms effectively map sensitive outliers to typical data points while maintaining utility in extreme Non-IID scenarios typical of cross-silo collaborations. Index Terms: Client-invariant Learning, Federated Learning (FL), Privacy-preserving Generative Models, Non-Independent and Identically Distributed (Non-IID), Heterogeneous Architectures",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 197,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13325",
        "abs_url": "https://arxiv.org/abs/2512.13325",
        "pdf_url": "https://arxiv.org/pdf/2512.13325",
        "title": "Security and Detectability Analysis of Unicode Text Watermarking Methods Against Large Language Models",
        "authors": [
            "Malte Hellmeier"
        ],
        "comments": "Accepted for publication at the ICISSP 2026",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Securing digital text is becoming increasingly relevant due to the widespread use of large language models. Individuals' fear of losing control over data when it is being used to train such machine learning models or when distinguishing model-generated output from text written by humans. Digital watermarking provides additional protection by embedding an invisible watermark within the data that requires protection. However, little work has been taken to analyze and verify if existing digital text watermarking methods are secure and undetectable by large language models. In this paper, we investigate the security-related area of watermarking and machine learning models for text data. In a controlled testbed of three experiments, ten existing Unicode text watermarking methods were implemented and analyzed across six large language models: GPT-5, GPT-4o, Teuken 7B, Llama 3.3, Claude Sonnet 4, and Gemini 2.5 Pro. The findings of our experiments indicate that, especially the latest reasoning models, can detect a watermarked text. Nevertheless, all models fail to extract the watermark unless implementation details in the form of source code are provided. We discuss the implications for security researchers and practitioners and outline future research opportunities to address security concerns.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 198,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13330",
        "abs_url": "https://arxiv.org/abs/2512.13330",
        "pdf_url": "https://arxiv.org/pdf/2512.13330",
        "title": "FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish Large Language Models",
        "authors": [
            "Joona Kytöniemi",
            "Jousia Piha",
            "Akseli Reunamo",
            "Fedor Vitiugin",
            "Farrokh Mehryary",
            "Sampo Pyysalo"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "We introduce FIN-bench-v2, a unified benchmark suite for evaluating large language models in Finnish. FIN-bench-v2 consolidates Finnish versions of widely used benchmarks together with an updated and expanded version of the original FIN-bench into a single, consistently formatted collection, covering multiple-choice and generative tasks across reading comprehension, commonsense reasoning, sentiment analysis, world knowledge, and alignment. All datasets are converted to HuggingFace Datasets, which include both cloze and multiple-choice prompt formulations with five variants per task, and we incorporate human annotation or review for machine-translated resources such as GoldenSwag and XED. To select robust tasks, we pretrain a set of 2.15B-parameter decoder-only models and use their learning curves to compute monotonicity, signal-to-noise, non-random performance, and model ordering consistency, retaining only tasks that satisfy all criteria. We further evaluate a set of larger instruction-tuned models to characterize performance across tasks and prompt formulations. All datasets, prompts, and evaluation configurations are publicly available via our fork of the Language Model Evaluation Harness at this https URL. Supplementary resources are released in a separate repository at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 199,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13356",
        "abs_url": "https://arxiv.org/abs/2512.13356",
        "pdf_url": "https://arxiv.org/pdf/2512.13356",
        "title": "Control of a Twin Rotor using Twin Delayed Deep Deterministic Policy Gradient (TD3)",
        "authors": [
            "Zeyad Gamal",
            "Youssef Mahran",
            "Ayman El-Badawy"
        ],
        "comments": "This is the Author Accepted Manuscript version of a paper accepted for publication. The final published version is available via IEEE Xplore",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "This paper proposes a reinforcement learning (RL) framework for controlling and stabilizing the Twin Rotor Aerodynamic System (TRAS) at specific pitch and azimuth angles and tracking a given trajectory. The complex dynamics and non-linear characteristics of the TRAS make it challenging to control using traditional control algorithms. However, recent developments in RL have attracted interest due to their potential applications in the control of multirotors. The Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm was used in this paper to train the RL agent. This algorithm is used for environments with continuous state and action spaces, similar to the TRAS, as it does not require a model of the system. The simulation results illustrated the effectiveness of the RL control method. Next, external disturbances in the form of wind disturbances were used to test the controller's effectiveness compared to conventional PID controllers. Lastly, experiments on a laboratory setup were carried out to confirm the controller's effectiveness in real-world applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 200,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13363",
        "abs_url": "https://arxiv.org/abs/2512.13363",
        "pdf_url": "https://arxiv.org/pdf/2512.13363",
        "title": "Detecting Emotion Drift in Mental Health Text Using Pre-Trained Transformers",
        "authors": [
            "Shibani Sankpal"
        ],
        "comments": "14 pages, 12 figures",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "This study investigates emotion drift: the change in emotional state across a single text, within mental health-related messages. While sentiment analysis typically classifies an entire message as positive, negative, or neutral, the nuanced shift of emotions over the course of a message is often overlooked. This study detects sentence-level emotions and measures emotion drift scores using pre-trained transformer models such as DistilBERT and RoBERTa. The results provide insights into patterns of emotional escalation or relief in mental health conversations. This methodology can be applied to better understand emotional dynamics in content.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 201,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13438",
        "abs_url": "https://arxiv.org/abs/2512.13438",
        "pdf_url": "https://arxiv.org/pdf/2512.13438",
        "title": "From User Interface to Agent Interface: Efficiency Optimization of UI Representations for LLM Agents",
        "authors": [
            "Dezhi Ran",
            "Zhi Gong",
            "Yuzhe Guo",
            "Mengzhou Wu",
            "Yuan Cao",
            "Haochuan Lu",
            "Hengyu Zhang",
            "Xia Zeng",
            "Gang Cao",
            "Liangchao Yao",
            "Yuetang Deng",
            "Wei Yang",
            "Tao Xie"
        ],
        "comments": "",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)",
        "abstract": "While Large Language Model (LLM) agents show great potential for automated UI navigation such as automated UI testing and AI assistants, their efficiency has been largely overlooked. Our motivating study reveals that inefficient UI representation creates a critical performance bottleneck. However, UI representation optimization, formulated as the task of automatically generating programs that transform UI representations, faces two unique challenges. First, the lack of Boolean oracles, which traditional program synthesis uses to decisively validate semantic correctness, poses a fundamental challenge to co-optimization of token efficiency and completeness. Second, the need to process large, complex UI trees as input while generating long, compositional transformation programs, making the search space vast and error-prone. Toward addressing the preceding limitations, we present UIFormer, the first automated optimization framework that synthesizes UI transformation programs by conducting constraint-based optimization with structured decomposition of the complex synthesis task. First, UIFormer restricts the program space using a domain-specific language (DSL) that captures UI-specific operations. Second, UIFormer conducts LLM-based iterative refinement with correctness and efficiency rewards, providing guidance for achieving the efficiency-completeness co-optimization. UIFormer operates as a lightweight plugin that applies transformation programs for seamless integration with existing LLM agents, requiring minimal modifications to their core logic. Evaluations across three UI navigation benchmarks spanning Android and Web platforms with five LLMs demonstrate that UIFormer achieves 48.7% to 55.8% token reduction with minimal runtime overhead while maintaining or improving agent performance. Real-world industry deployment at WeChat further validates the practical impact of UIFormer.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 202,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13458",
        "abs_url": "https://arxiv.org/abs/2512.13458",
        "pdf_url": "https://arxiv.org/pdf/2512.13458",
        "title": "SSAS: Cross-subject EEG-based Emotion Recognition through Source Selection with Adversarial Strategy",
        "authors": [
            "Yici Liu",
            "Qi Wei Oung",
            "Hoi Leong Lee"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Signal Processing (eess.SP)",
        "abstract": "Electroencephalographic (EEG) signals have long been applied in the field of affective brain-computer interfaces (aBCIs). Cross-subject EEG-based emotion recognition has demonstrated significant potential in practical applications due to its suitability across diverse people. However, most studies on cross-subject EEG-based emotion recognition neglect the presence of inter-individual variability and negative transfer phenomena during model training. To address this issue, a cross-subject EEG-based emotion recognition through source selection with adversarial strategy is introduced in this paper. The proposed method comprises two modules: the source selection network (SS) and the adversarial strategies network (AS). The SS uses domain labels to reverse-engineer the training process of domain adaptation. Its key idea is to disrupt class separability and magnify inter-domain differences, thereby raising the classification difficulty and forcing the model to learn domain-invariant yet emotion-relevant representations. The AS gets the source domain selection results and the pretrained domain discriminators from SS. The pretrained domain discriminators compute a novel loss aimed at enhancing the performance of domain classification during adversarial training, ensuring the balance of adversarial strategies. This paper provides theoretical insights into the proposed method and achieves outstanding performance on two EEG-based emotion datasets, SEED and SEED-IV. The code can be found at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 203,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13478",
        "abs_url": "https://arxiv.org/abs/2512.13478",
        "pdf_url": "https://arxiv.org/pdf/2512.13478",
        "title": "Non-Resolution Reasoning: A Framework for Preserving Semantic Ambiguity in Language Models",
        "authors": [
            "Kei Saito"
        ],
        "comments": "19 pages",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Premature semantic collapse -- the forced early commitment to a single meaning -- remains a core architectural limitation of current language models. Softmax-driven competition and greedy decoding cause models to discard valid interpretations before sufficient context is available, resulting in brittle reasoning and context failures. We introduce Non-Resolution Reasoning (NRR), a general computational framework that preserves semantic ambiguity during inference and performs resolution only when explicitly required. NRR integrates three components: (1) Multi-Vector Embeddings that maintain multiple viable interpretations per token, (2) Non-Collapsing Attention that prevents winner-take-all dynamics across layers, and (3) Contextual Identity Tracking (CIT), which assigns context-specific identities to recurring entities (e.g., distinguishing \"Dr. Smith the cardiologist\" from \"Dr. Smith the researcher\"). These mechanisms are unified by an external Resolution Operator $\\rho$ that makes semantic commitment explicit, controllable, and task-dependent. Unlike standard architectures, NRR separates representation from resolution, allowing a single model to shift between creative, factual, and ambiguity-preserving reasoning without retraining. A synthetic evaluation demonstrates NRR's ability to preserve ambiguity and track context: CIT-enhanced models achieve 90.9% accuracy on out-of-distribution identity-shift tasks, compared to 9.1% for transformer baselines. NRR provides a principled alternative to premature collapse, reframing ambiguity as an explicit representational state rather than a failure mode. The question is not whether AI should resolve ambiguity, but when, how, and under whose control.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 204,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13494",
        "abs_url": "https://arxiv.org/abs/2512.13494",
        "pdf_url": "https://arxiv.org/pdf/2512.13494",
        "title": "SkipCat: Rank-Maximized Low-Rank Compression of Large Language Models via Shared Projection and Block Skipping",
        "authors": [
            "Yu-Chen Lu",
            "Sheng-Feng Yu",
            "Hui-Hsien Weng",
            "Pei-Shuo Wang",
            "Yu-Fang Hu",
            "Liang Hung-Chun",
            "Hung-Yueh Chiang",
            "Kai-Chiang Wu"
        ],
        "comments": "Accepted by AAAI 2026",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Large language models (LLM) have achieved remarkable performance across a wide range of tasks. However, their substantial parameter sizes pose significant challenges for deployment on edge devices with limited computational and memory resources. Low-rank compression is a promising approach to address this issue, as it reduces both computational and memory costs, making LLM more suitable for resource-constrained environments. Nonetheless, naïve low-rank compression methods require a significant reduction in the retained rank to achieve meaningful memory and computation savings. For a low-rank model, the ranks need to be reduced by more than half to yield efficiency gains. Such aggressive truncation, however, typically results in substantial performance degradation. To address this trade-off, we propose SkipCat, a novel low-rank compression framework that enables the use of higher ranks while achieving the same compression rates. First, we introduce an intra-layer shared low-rank projection method, where multiple matrices that share the same input use a common projection. This reduces redundancy and improves compression efficiency. Second, we propose a block skipping technique that omits computations and memory transfers for selected sub-blocks within the low-rank decomposition. These two techniques jointly enable our compressed model to retain more effective ranks under the same compression budget. Experimental results show that, without any additional fine-tuning, our method outperforms previous low-rank compression approaches by 7% accuracy improvement on zero-shot tasks under the same compression rate. These results highlight the effectiveness of our rank-maximized compression strategy in preserving model performance under tight resource constraints.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 205,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13501",
        "abs_url": "https://arxiv.org/abs/2512.13501",
        "pdf_url": "https://arxiv.org/pdf/2512.13501",
        "title": "Behavior-Aware and Generalizable Defense Against Black-Box Adversarial Attacks for ML-Based IDS",
        "authors": [
            "Sabrine Ennaji",
            "Elhadj Benkhelifa",
            "Luigi Vincenzo Mancini"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
        "abstract": "Machine learning based intrusion detection systems are increasingly targeted by black box adversarial attacks, where attackers craft evasive inputs using indirect feedback such as binary outputs or behavioral signals like response time and resource usage. While several defenses have been proposed, including input transformation, adversarial training, and surrogate detection, they often fall short in practice. Most are tailored to specific attack types, require internal model access, or rely on static mechanisms that fail to generalize across evolving attack strategies. Furthermore, defenses such as input transformation can degrade intrusion detection system performance, making them unsuitable for real time deployment. To address these limitations, we propose Adaptive Feature Poisoning, a lightweight and proactive defense mechanism designed specifically for realistic black box scenarios. Adaptive Feature Poisoning assumes that probing can occur silently and continuously, and introduces dynamic and context aware perturbations to selected traffic features, corrupting the attacker feedback loop without impacting detection capabilities. The method leverages traffic profiling, change point detection, and adaptive scaling to selectively perturb features that an attacker is likely exploiting, based on observed deviations. We evaluate Adaptive Feature Poisoning against multiple realistic adversarial attack strategies, including silent probing, transferability based attacks, and decision boundary based attacks. The results demonstrate its ability to confuse attackers, degrade attack effectiveness, and preserve detection performance. By offering a generalizable, attack agnostic, and undetectable defense, Adaptive Feature Poisoning represents a significant step toward practical and robust adversarial resilience in machine learning based intrusion detection systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 206,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13559",
        "abs_url": "https://arxiv.org/abs/2512.13559",
        "pdf_url": "https://arxiv.org/pdf/2512.13559",
        "title": "Verifying Rumors via Stance-Aware Structural Modeling",
        "authors": [
            "Gibson Nkhata",
            "Uttamasha Anjally Oyshi",
            "Quan Mai",
            "Susan Gauch"
        ],
        "comments": "8 pages, 2 figures, published in The 24th IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT 2025), London, UK, 2025",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computers and Society (cs.CY)",
        "abstract": "Verifying rumors on social media is critical for mitigating the spread of false information. The stances of conversation replies often provide important cues to determine a rumor's veracity. However, existing models struggle to jointly capture semantic content, stance information, and conversation strructure, especially under the sequence length constraints of transformer-based encoders. In this work, we propose a stance-aware structural modeling that encodes each post in a discourse with its stance signal and aggregates reply embedddings by stance category enabling a scalable and semantically enriched representation of the entire thread. To enhance structural awareness, we introduce stance distribution and hierarchical depth as covariates, capturing stance imbalance and the influence of reply depth. Extensive experiments on benchmark datasets demonstrate that our approach significantly outperforms prior methods in the ability to predict truthfulness of a rumor. We also demonstrate that our model is versatile for early detection and cross-platfrom generalization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 207,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13564",
        "abs_url": "https://arxiv.org/abs/2512.13564",
        "pdf_url": "https://arxiv.org/pdf/2512.13564",
        "title": "Memory in the Age of AI Agents",
        "authors": [
            "Yuyang Hu",
            "Shichun Liu",
            "Yanwei Yue",
            "Guibin Zhang",
            "Boyang Liu",
            "Fangyi Zhu",
            "Jiahang Lin",
            "Honglin Guo",
            "Shihan Dou",
            "Zhiheng Xi",
            "Senjie Jin",
            "Jiejun Tan",
            "Yanbin Yin",
            "Jiongnan Liu",
            "Zeyu Zhang",
            "Zhongxiang Sun",
            "Yutao Zhu",
            "Hao Sun",
            "Boci Peng",
            "Zhenrong Cheng",
            "Xuanbo Fan",
            "Jiaxin Guo",
            "Xinlei Yu",
            "Zhenhong Zhou",
            "Zewen Hu",
            "Jiahao Huo",
            "Junhao Wang",
            "Yuwei Niu",
            "Yu Wang",
            "Zhenfei Yin",
            "Xiaobin Hu",
            "Yue Liao",
            "Qiankun Li",
            "Kun Wang",
            "Wangchunshu Zhou",
            "Yixin Liu",
            "Dawei Cheng",
            "Qi Zhang",
            "Tao Gui",
            "Shirui Pan",
            "Yan Zhang",
            "Philip Torr",
            "Zhicheng Dou",
            "Ji-Rong Wen",
            "Xuanjing Huang",
            "Yu-Gang Jiang",
            "Shuicheng Yan"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 208,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13568",
        "abs_url": "https://arxiv.org/abs/2512.13568",
        "pdf_url": "https://arxiv.org/pdf/2512.13568",
        "title": "Superposition as Lossy Compression: Measure with Sparse Autoencoders and Connect to Adversarial Vulnerability",
        "authors": [
            "Leonard Bereska",
            "Zoe Tzifa-Kratira",
            "Reza Samavi",
            "Efstratios Gavves"
        ],
        "comments": "Accepted to TMLR, view HTML here: this https URL",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Neural networks achieve remarkable performance through superposition: encoding multiple features as overlapping directions in activation space rather than dedicating individual neurons to each feature. This challenges interpretability, yet we lack principled methods to measure superposition. We present an information-theoretic framework measuring a neural representation's effective degrees of freedom. We apply Shannon entropy to sparse autoencoder activations to compute the number of effective features as the minimum neurons needed for interference-free encoding. Equivalently, this measures how many \"virtual neurons\" the network simulates through superposition. When networks encode more effective features than actual neurons, they must accept interference as the price of compression. Our metric strongly correlates with ground truth in toy models, detects minimal superposition in algorithmic tasks, and reveals systematic reduction under dropout. Layer-wise patterns mirror intrinsic dimensionality studies on Pythia-70M. The metric also captures developmental dynamics, detecting sharp feature consolidation during grokking. Surprisingly, adversarial training can increase effective features while improving robustness, contradicting the hypothesis that superposition causes vulnerability. Instead, the effect depends on task complexity and network capacity: simple tasks with ample capacity allow feature expansion (abundance regime), while complex tasks or limited capacity force reduction (scarcity regime). By defining superposition as lossy compression, this work enables principled measurement of how neural networks organize information under computational constraints, connecting superposition to adversarial robustness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 209,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13583",
        "abs_url": "https://arxiv.org/abs/2512.13583",
        "pdf_url": "https://arxiv.org/pdf/2512.13583",
        "title": "DP-CSGP: Differentially Private Stochastic Gradient Push with Compressed Communication",
        "authors": [
            "Zehan Zhu",
            "Heng Zhao",
            "Yan Huang",
            "Joey Tianyi Zhou",
            "Shouling Ji",
            "Jinming Xu"
        ],
        "comments": "13 pages",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "In this paper, we propose a Differentially Private Stochastic Gradient Push with Compressed communication (termed DP-CSGP) for decentralized learning over directed graphs. Different from existing works, the proposed algorithm is designed to maintain high model utility while ensuring both rigorous differential privacy (DP) guarantees and efficient communication. For general non-convex and smooth objective functions, we show that the proposed algorithm achieves a tight utility bound of $\\mathcal{O}\\left( \\sqrt{d\\log \\left( \\frac{1}{\\delta} \\right)}/(\\sqrt{n}J\\epsilon) \\right)$ ($J$ and $d$ are the number of local samples and the dimension of decision variables, respectively) with $\\left(\\epsilon, \\delta\\right)$-DP guarantee for each node, matching that of decentralized counterparts with exact communication. Extensive experiments on benchmark tasks show that, under the same privacy budget, DP-CSGP achieves comparable model accuracy with significantly lower communication cost than existing decentralized counterparts with exact communication.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 210,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13586",
        "abs_url": "https://arxiv.org/abs/2512.13586",
        "pdf_url": "https://arxiv.org/pdf/2512.13586",
        "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding",
        "authors": [
            "Jia-Nan Li",
            "Jian Guan",
            "Wei Wu",
            "Chongxuan Li"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18$\\times$ speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33$\\times$ average speedup.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 211,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13607",
        "abs_url": "https://arxiv.org/abs/2512.13607",
        "pdf_url": "https://arxiv.org/pdf/2512.13607",
        "title": "Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models",
        "authors": [
            "Boxin Wang",
            "Chankyu Lee",
            "Nayeon Lee",
            "Sheng-Chieh Lin",
            "Wenliang Dai",
            "Yang Chen",
            "Yangyi Chen",
            "Zhuolin Yang",
            "Zihan Liu",
            "Mohammad Shoeybi",
            "Bryan Catanzaro",
            "Wei Ping"
        ],
        "comments": "We publicly release the Nemotron-Cascade models and the full collection of training data at: this https URL",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Building general-purpose reasoning models with reinforcement learning (RL) entails substantial cross-domain heterogeneity, including large variation in inference-time response lengths and verification latency. Such variability complicates the RL infrastructure, slows training, and makes training curriculum (e.g., response length extension) and hyperparameter selection challenging. In this work, we propose cascaded domain-wise reinforcement learning (Cascade RL) to develop general-purpose reasoning models, Nemotron-Cascade, capable of operating in both instruct and deep thinking modes. Departing from conventional approaches that blend heterogeneous prompts from different domains, Cascade RL orchestrates sequential, domain-wise RL, reducing engineering complexity and delivering state-of-the-art performance across a wide range of benchmarks. Notably, RLHF for alignment, when used as a pre-step, boosts the model's reasoning ability far beyond mere preference optimization, and subsequent domain-wise RLVR stages rarely degrade the benchmark performance attained in earlier domains and may even improve it (see an illustration in Figure 1). Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro and achieves silver-medal performance in the 2025 International Olympiad in Informatics (IOI). We transparently share our training and data recipes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 212,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13644",
        "abs_url": "https://arxiv.org/abs/2512.13644",
        "pdf_url": "https://arxiv.org/pdf/2512.13644",
        "title": "World Models Can Leverage Human Videos for Dexterous Manipulation",
        "authors": [
            "Raktim Gautam Goswami",
            "Amir Bar",
            "David Fan",
            "Tsung-Yen Yang",
            "Gaoyue Zhou",
            "Prashanth Krishnamurthy",
            "Michael Rabbat",
            "Farshad Khorrami",
            "Yann LeCun"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Dexterous manipulation is challenging because it requires understanding how subtle hand motion influences the environment through contact with objects. We introduce DexWM, a Dexterous Manipulation World Model that predicts the next latent state of the environment conditioned on past states and dexterous actions. To overcome the scarcity of dexterous manipulation datasets, DexWM is trained on over 900 hours of human and non-dexterous robot videos. To enable fine-grained dexterity, we find that predicting visual features alone is insufficient; therefore, we introduce an auxiliary hand consistency loss that enforces accurate hand configurations. DexWM outperforms prior world models conditioned on text, navigation, and full-body actions, achieving more accurate predictions of future states. DexWM also demonstrates strong zero-shot generalization to unseen manipulation skills when deployed on a Franka Panda arm equipped with an Allegro gripper, outperforming Diffusion Policy by over 50% on average in grasping, placing, and reaching tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 213,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13654",
        "abs_url": "https://arxiv.org/abs/2512.13654",
        "pdf_url": "https://arxiv.org/pdf/2512.13654",
        "title": "Large-Language Memorization During the Classification of United States Supreme Court Cases",
        "authors": [
            "John E. Ortega",
            "Dhruv D. Joshi",
            "Matt P. Borkowski"
        ],
        "comments": "7 pages, 1 figure, Appendix of Prompts",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Emerging Technologies (cs.ET); Information Retrieval (cs.IR)",
        "abstract": "Large-language models (LLMs) have been shown to respond in a variety of ways for classification tasks outside of question-answering. LLM responses are sometimes called \"hallucinations\" since the output is not what is ex pected. Memorization strategies in LLMs are being studied in detail, with the goal of understanding how LLMs respond. We perform a deep dive into a classification task based on United States Supreme Court (SCOTUS) decisions. The SCOTUS corpus is an ideal classification task to study for LLM memory accuracy because it presents significant challenges due to extensive sentence length, complex legal terminology, non-standard structure, and domain-specific vocabulary. Experimentation is performed with the latest LLM fine tuning and retrieval-based approaches, such as parameter-efficient fine-tuning, auto-modeling, and others, on two traditional category-based SCOTUS classification tasks: one with 15 labeled topics and another with 279. We show that prompt-based models with memories, such as DeepSeek, can be more robust than previous BERT-based models on both tasks scoring about 2 points better than previous models not based on prompting.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 214,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13658",
        "abs_url": "https://arxiv.org/abs/2512.13658",
        "pdf_url": "https://arxiv.org/pdf/2512.13658",
        "title": "Embedding-Based Rankings of Educational Resources based on Learning Outcome Alignment: Benchmarking, Expert Validation, and Learner Performance",
        "authors": [
            "Mohammadreza Molavi",
            "Mohammad Moein",
            "Mohammadreza Tavakoli",
            "Abdolali Faraji",
            "Stefan T. Mol",
            "Gábor Kismihók"
        ],
        "comments": "Accepted for publication at the 16th International Conference on Learning Analytics & Knowledge (LAK 2026)",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI)",
        "abstract": "As the online learning landscape evolves, the need for personalization is increasingly evident. Although educational resources are burgeoning, educators face challenges selecting materials that both align with intended learning outcomes and address diverse learner needs. Large Language Models (LLMs) are attracting growing interest for their potential to create learning resources that better support personalization, but verifying coverage of intended outcomes still requires human alignment review, which is costly and limits scalability. We propose a framework that supports the cost-effective automation of evaluating alignment between educational resources and intended learning outcomes. Using human-generated materials, we benchmarked LLM-based text-embedding models and found that the most accurate model (Voyage) achieved 79% accuracy in detecting alignment. We then applied the optimal model to LLM-generated resources and, via expert evaluation, confirmed that it reliably assessed correspondence to intended outcomes (83% accuracy). Finally, in a three-group experiment with 360 learners, higher alignment scores were positively related to greater learning performance, chi-squared(2, N = 360) = 15.39, p < 0.001. These findings show that embedding-based alignment scores can facilitate scalable personalization by confirming alignment with learning outcomes, which allows teachers to focus on tailoring content to diverse learner needs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 215,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True",
        "arxiv_id": "2512.13690",
        "abs_url": "https://arxiv.org/abs/2512.13690",
        "pdf_url": "https://arxiv.org/pdf/2512.13690",
        "title": "DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders",
        "authors": [
            "Susung Hong",
            "Chongjian Ge",
            "Zhifei Zhang",
            "Jui-Hsien Wang"
        ],
        "comments": "Project page: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Graphics (cs.GR); Machine Learning (cs.LG)",
        "abstract": "Video diffusion models have revolutionized generative video synthesis, but they are imprecise, slow, and can be opaque during generation -- keeping users in the dark for a prolonged period. In this work, we propose DiffusionBrowser, a model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process. Our model can generate multi-modal preview representations that include RGB and scene intrinsics at more than 4$\\times$ real-time speed (less than 1 second for a 4-second video) that convey consistent appearance and motion to the final video. With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking a new control capability. Moreover, we systematically probe the model using the learned decoders, revealing how scene, object, and other details are composed and assembled during the otherwise black-box denoising process.",
        "gemini2.5flash": "",
        "overall_idea": ""
    }
]