[
    {
        "order": 1,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11568",
        "abs_url": "https://arxiv.org/abs/2601.11568",
        "pdf_url": "https://arxiv.org/pdf/2601.11568",
        "title": "AdaFRUGAL: Adaptive Memory-Efficient Training with Dynamic Control",
        "authors": [
            "Quang-Hung Bui",
            "Anh Son Ta"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Training Large Language Models (LLMs) is highly memory-intensive due to optimizer state overhead. The FRUGAL framework mitigates this with gradient splitting, but its static hyperparameters -- the subspace ratio ($\\rho$) and update frequency ($T$) -- require costly manual tuning, limiting adaptability. We present AdaFRUGAL, which automates this process by introducing two dynamic controls: (i) a linear decay for $\\rho$ to progressively reduce memory, and (ii) a loss-aware schedule for $T$ to lower computational overhead. Experiments across large-scale pre-training (English C4, Vietnamese VietVault) and fine-tuning (GLUE) demonstrate that AdaFRUGAL achieves a compelling trade-off. It maintains competitive performance against AdamW and static FRUGAL while significantly reducing both GPU memory and training time, offering a more practical, autonomous solution for resource-constrained LLM training.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 2,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11574",
        "abs_url": "https://arxiv.org/abs/2601.11574",
        "pdf_url": "https://arxiv.org/pdf/2601.11574",
        "title": "GRADE: Replacing Policy Gradients with Backpropagation for LLM Alignment",
        "authors": [
            "Lukas Abrie Nel"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Reinforcement learning from human feedback (RLHF) has become the dominant paradigm for aligning large language models with human preferences. However, policy gradient methods such as PPO suffer from high variance gradient estimates, requiring careful hyperparameter tuning and extensive computational resources. We introduce GRADE (Gumbel-softmax Relaxation for Alignment via Differentiable Estimation), a method that replaces high-variance policy gradient estimation with direct backpropagation through a differentiable relaxation of the discrete token sampling process. Using the Gumbel-Softmax reparameterization with straight-through estimation (GRADE-STE), we enable end-to-end gradient flow from reward signals through generated tokens to model parameters. On sentiment-controlled text generation using the IMDB dataset, GRADE-STE achieves a test reward of 0.763 +- 0.344 compared to PPO's 0.510 +- 0.313 and REINFORCE's 0.617 +- 0.378, representing a 50% relative improvement over PPO. Critically, GRADE-STE exhibits gradient variance over 14 times lower than REINFORCE and maintains stable training dynamics throughout optimization. Our rigorous evaluation with proper train/validation/test splits demonstrates that these improvements generalize to held-out data, with GRADE-STE showing the best generalization characteristics among all methods tested. GRADE offers a simpler, more stable, and more effective alternative to reinforcement learning for LLM alignment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 3,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11606",
        "abs_url": "https://arxiv.org/abs/2601.11606",
        "pdf_url": "https://arxiv.org/pdf/2601.11606",
        "title": "A Multimodal Data Processing Pipeline for MIMIC-IV Dataset",
        "authors": [
            "Farzana Islam Adiba",
            "Varsha Danduri",
            "Fahmida Liza Piya",
            "Ali Abbasi",
            "Mehak Gupta",
            "Rahmatollah Beheshti"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The MIMIC-IV dataset is a large, publicly available electronic health record (EHR) resource widely used for clinical machine learning research. It comprises multiple modalities, including structured data, clinical notes, waveforms, and imaging data. Working with these disjointed modalities requires an extensive manual effort to preprocess and align them for downstream analysis. While several pipelines for MIMIC-IV data extraction are available, they target a small subset of modalities or do not fully support arbitrary downstream applications. In this work, we greatly expand our prior popular unimodal pipeline and present a comprehensive and customizable multimodal pipeline that can significantly reduce multimodal processing time and enhance the reproducibility of MIMIC-based studies. Our pipeline systematically integrates the listed modalities, enabling automated cohort selection, temporal alignment across modalities, and standardized multimodal output formats suitable for arbitrary static and time-series downstream applications. We release the code, a simple UI, and a Python package for selective integration (with embedding) at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 4,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11609",
        "abs_url": "https://arxiv.org/abs/2601.11609",
        "pdf_url": "https://arxiv.org/pdf/2601.11609",
        "title": "Auxiliary-predicted Compress Memory Model(ApCM Model): A Neural Memory Storage Model Based on Invertible Compression and Learnable Prediction",
        "authors": [
            "Weinuo Ou"
        ],
        "comments": "9 pages, 7 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Current large language models (LLMs) generally lack an effective runtime memory mechanism,making it difficult to adapt to dynamic and personalized interaction requirements. To address this issue, this paper proposes a novel neural memory storage architecture--the Auxiliary Prediction Compression Memory Model (ApCM Model).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 5,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11611",
        "abs_url": "https://arxiv.org/abs/2601.11611",
        "pdf_url": "https://arxiv.org/pdf/2601.11611",
        "title": "Integrating Temporal Context into Streaming Data for Human Activity Recognition in Smart Home",
        "authors": [
            "Marina Vicini",
            "Martin Rudorfer",
            "Zhuangzhuang Dai",
            "Luis J. Manso"
        ],
        "comments": "Accepted to International Conference on Ubiquitous Computing and Ambient Intelligence (UCAmI) 2024",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "With the global population ageing, it is crucial to enable individuals to live independently and safely in their homes. Using ubiquitous sensors such as Passive InfraRed sensors (PIR) and door sensors is drawing increasing interest for monitoring daily activities and facilitating preventative healthcare interventions for the elderly. Human Activity Recognition (HAR) from passive sensors mostly relies on traditional machine learning and includes data segmentation, feature extraction, and classification. While techniques like Sensor Weighting Mutual Information (SWMI) capture spatial context in a feature vector, effectively leveraging temporal information remains a challenge. We tackle this by clustering activities into morning, afternoon, and night, and encoding them into the feature weighting method calculating distinct mutual information matrices. We further propose to extend the feature vector by incorporating time of day and day of week as cyclical temporal features, as well as adding a feature to track the user's location. The experiments show improved accuracy and F1-score over existing state-of-the-art methods in three out of four real-world datasets, with highest gains in a low-data regime. These results highlight the potential of our approach for developing effective smart home solutions to support ageing in place.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 6,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11615",
        "abs_url": "https://arxiv.org/abs/2601.11615",
        "pdf_url": "https://arxiv.org/pdf/2601.11615",
        "title": "A Review on Machine Learning Approaches for the Prediction of Glucose Levels and Hypogylcemia",
        "authors": [
            "Beyza Cinar",
            "Louisa van den Boom",
            "Maria Maleshkova"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Type 1 Diabetes (T1D) is an autoimmune disease leading to insulin insufficiency. Thus, patients require lifelong insulin therapy, which has a side effect of hypoglycemia. Hypoglycemia is a critical state of decreased blood glucose levels (BGL) below 70 mg/dL and is associated with increased risk of mortality. Machine learning (ML) models can improve diabetes management by predicting hypoglycemia and providing optimal prevention methods. ML models are classified into regression and classification based, that forecast glucose levels and identify events based on defined labels, respectively. This review investigates state-of-the-art models trained on data of continuous glucose monitoring (CGM) devices from patients with T1D. We compare the models' performance across short-term (15 to 120 min) and long term (3 to more than 24 hours) prediction horizons (PHs). Particularly, we explore: 1) How much in advance can glucose values or a hypoglycemic event be accurately predicted? 2) Which models have the best performance? 3) Which factors impact the performance? and 4) Does personalization increase performance? The results show that 1) a PH of up to 1 hour provides the best results. 2) Conventional ML methods yield the best results for classification and DL for regression. A single model cannot adequately classify across multiple PHs. 3) The model performance is influenced by multivariate datasets and the input sequence length (ISL). 4) Personal data enhances performance but due to limited data quality population-based models are preferred.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 7,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11618",
        "abs_url": "https://arxiv.org/abs/2601.11618",
        "pdf_url": "https://arxiv.org/pdf/2601.11618",
        "title": "Geometric Attention: A Regime-Explicit Operator Semantics for Transformer Attention",
        "authors": [
            "Luis Rosario Freytes"
        ],
        "comments": "57 pages",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Geometric Attention (GA) specifies an attention layer by four independent inputs: a finite carrier (what indices are addressable), an evidence-kernel rule (how masked proto-scores and a link induce nonnegative weights), a probe family (which observables are treated as admissible), and an anchor/update rule (which representative kernel is selected and how it is applied). Probe families induce an operational equivalence relation on kernels and therefore a gauge; anchors select representatives relative to that probe. Under a scalar relational-work representation and a multiplicative compositionality law for evidence, the admissible link family is exponential, yielding Gibbs weights; with row anchoring this includes the softmax kernel family as a subregime. After quotienting unary row/column score fields, the remaining interaction component admits a canonical rank-r normal form (Eckart-Young/SVD); dot-product score charts implement the corresponding low-rank interaction regime. Fixing the carrier and extensionalizing the update yields the standard fixed-token Transformer attention operator; allowing carrier updates yields adaptive-carrier and staged-depth regimes. The operator language also supports multihead/mixed kernels, plan-based anchors (e.g., entropic OT/Sinkhorn), and unary operators (e.g., FFN-style fields) as explicit regime choices. This separates invariant structure from modeling choice, enabling principled comparison and extension of attention mechanisms, and attention-based architectures.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 8,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11638",
        "abs_url": "https://arxiv.org/abs/2601.11638",
        "pdf_url": "https://arxiv.org/pdf/2601.11638",
        "title": "Verifying Physics-Informed Neural Network Fidelity using Classical Fisher Information from Differentiable Dynamical System",
        "authors": [
            "Josafat Ribeiro Leal Filho",
            "Antônio Augusto Fröhlich"
        ],
        "comments": "This paper has been submitted and is currently under review at IEEE Transactions on Neural Networks and Learning Systems (TNNLS)",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Physics-Informed Neural Networks (PINNs) have emerged as a powerful tool for solving differential equations and modeling physical systems by embedding physical laws into the learning process. However, rigorously quantifying how well a PINN captures the complete dynamical behavior of the system, beyond simple trajectory prediction, remains a challenge. This paper proposes a novel experimental framework to address this by employing Fisher information for differentiable dynamical systems, denoted $g_F^C$. This Fisher information, distinct from its statistical counterpart, measures inherent uncertainties in deterministic systems, such as sensitivity to initial conditions, and is related to the phase space curvature and the net stretching action of the state space evolution. We hypothesize that if a PINN accurately learns the underlying dynamics of a physical system, then the Fisher information landscape derived from the PINN's learned equations of motion will closely match that of the original analytical model. This match would signify that the PINN has achieved comprehensive fidelity capturing not only the state evolution but also crucial geometric and stability properties. We outline an experimental methodology using the dynamical model of a car to compute and compare $g_F^C$ for both the analytical model and a trained PINN. The comparison, based on the Jacobians of the respective system dynamics, provides a quantitative measure of the PINN's fidelity in representing the system's intricate dynamical characteristics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 9,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11639",
        "abs_url": "https://arxiv.org/abs/2601.11639",
        "pdf_url": "https://arxiv.org/pdf/2601.11639",
        "title": "Global Optimization By Gradient from Hierarchical Score-Matching Spaces",
        "authors": [
            "Ming Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Gradient descent is the most commonly used optimization method, but limited to local optimality, and confined to the field of continuous differentiable problems with simple convex constraints. This work solve these limitations and restrictions by unifying all optimization problems with various complex constraints as a general hierarchical optimization objective without constraints, which is optimized by gradient obtained through score matching. By this way, global optimization by deterministic method using strict gradient is achieved for the first time, and verified through simple-constructed and complex-practical experiments. Even more importantly, it reveals the profound connection between global optimization and diffusion based generative modeling.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 10,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11661",
        "abs_url": "https://arxiv.org/abs/2601.11661",
        "pdf_url": "https://arxiv.org/pdf/2601.11661",
        "title": "Machine learning model for predicting surface wettability in laser-textured metal alloys",
        "authors": [
            "Mohammad Mohammadzadeh Sanandaji",
            "Danial Ebrahimzadeh",
            "Mohammad Ikram Haider",
            "Yaser Mike Banad",
            "Aleksandar Poleksic",
            "Hongtao Ding"
        ],
        "comments": "This manuscript has 9 figures and contains 16 pages two column. submitted to journal of laser applications. Under review",
        "subjects": "Machine Learning (cs.LG); Materials Science (cond-mat.mtrl-sci)",
        "abstract": "Surface wettability, governed by both topography and chemistry, plays a critical role in applications such as heat transfer, lubrication, microfluidics, and surface coatings. In this study, we present a machine learning (ML) framework capable of accurately predicting the wettability of laser-textured metal alloys using experimentally derived morphological and chemical features. Superhydrophilic and superhydrophobic surfaces were fabricated on AA6061 and AISI 4130 alloys via nanosecond laser texturing followed by chemical immersion treatments. Surface morphology was quantified using the Laws texture energy method and profilometry, while surface chemistry was characterized through X-ray photoelectron spectroscopy (XPS), extracting features such as functional group polarity, molecular volume, and peak area fraction. These features were used to train an ensemble neural network model incorporating residual connections, batch normalization, and dropout regularization. The model achieved high predictive accuracy (R2 = 0.942, RMSE = 13.896), outperforming previous approaches. Feature importance analysis revealed that surface chemistry had the strongest influence on contact angle prediction, with topographical features also contributing significantly. This work demonstrates the potential of artificial intelligence to model and predict wetting behavior by capturing the complex interplay of surface characteristics, offering a data-driven pathway for designing tailored functional surfaces.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 11,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11667",
        "abs_url": "https://arxiv.org/abs/2601.11667",
        "pdf_url": "https://arxiv.org/pdf/2601.11667",
        "title": "Distill-then-Replace: Efficient Task-Specific Hybrid Attention Model Construction",
        "authors": [
            "Xiaojie Xia",
            "Huigang Zhang",
            "Chaoliang Zhong",
            "Jun Sun",
            "Yusuke Oishi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Transformer architectures deliver state-of-the-art accuracy via dense full-attention, but their quadratic time and memory complexity with respect to sequence length limits practical deployment. Linear attention mechanisms offer linear or near-linear scaling yet often incur performance degradation. Hybrid models that integrate full and linear attention layers promise a balance between efficiency and expressiveness, but face two major challenges: training such hybrid models from scratch is computationally expensive, and manually designing the optimal placement of attention types is highly nontrivial. We address both issues by first transferring weights from the pretrained full-attention modules to its linear attention counterparts through blockwise local distillation, and second, introducing a greedy layer replacement strategy that iteratively substitutes full attention blocks with linear ones while monitoring validation performance on the target task. This yields a task-specific hybrid model in a single efficient pass, without costly re-training or neural architecture search, and can be applied to any pretrained full-attention backbone for diverse downstream tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 12,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11670",
        "abs_url": "https://arxiv.org/abs/2601.11670",
        "pdf_url": "https://arxiv.org/pdf/2601.11670",
        "title": "A Confidence-Variance Theory for Pseudo-Label Selection in Semi-Supervised Learning",
        "authors": [
            "Jinshi Liu",
            "Pan Liu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Most pseudo-label selection strategies in semi-supervised learning rely on fixed confidence thresholds, implicitly assuming that prediction confidence reliably indicates correctness. In practice, deep networks are often overconfident: high-confidence predictions can still be wrong, while informative low-confidence samples near decision boundaries are discarded. This paper introduces a Confidence-Variance (CoVar) theory framework that provides a principled joint reliability criterion for pseudo-label selection. Starting from the entropy minimization principle, we derive a reliability measure that combines maximum confidence (MC) with residual-class variance (RCV), which characterizes how probability mass is distributed over non-maximum classes. The derivation shows that reliable pseudo-labels should have both high MC and low RCV, and that the influence of RCV increases as confidence grows, thereby correcting overconfident but unstable predictions. From this perspective, we cast pseudo-label selection as a spectral relaxation problem that maximizes separability in a confidence-variance feature space, and design a threshold-free selection mechanism to distinguish high- from low-reliability predictions. We integrate CoVar as a plug-in module into representative semi-supervised semantic segmentation and image classification methods. Across PASCAL VOC 2012, Cityscapes, CIFAR-10, and Mini-ImageNet with varying label ratios and backbones, it consistently improves over strong baselines, indicating that combining confidence with residual-class variance provides a more reliable basis for pseudo-label selection than fixed confidence thresholds. (Code: this https URL)",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 13,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11719",
        "abs_url": "https://arxiv.org/abs/2601.11719",
        "pdf_url": "https://arxiv.org/pdf/2601.11719",
        "title": "jBOT: Semantic Jet Representation Clustering Emerges from Self-Distillation",
        "authors": [
            "Ho Fung Tsoi",
            "Dylan Rankin"
        ],
        "comments": "Under review",
        "subjects": "Machine Learning (cs.LG); High Energy Physics - Experiment (hep-ex)",
        "abstract": "Self-supervised learning is a powerful pre-training method for learning feature representations without labels, which often capture generic underlying semantics from the data and can later be fine-tuned for downstream tasks. In this work, we introduce jBOT, a pre-training method based on self-distillation for jet data from the CERN Large Hadron Collider, which combines local particle-level distillation with global jet-level distillation to learn jet representations that support downstream tasks such as anomaly detection and classification. We observe that pre-training on unlabeled jets leads to emergent semantic class clustering in the representation space. The clustering in the frozen embedding, when pre-trained on background jets only, enables anomaly detection via simple distance-based metrics, and the learned embedding can be fine-tuned for classification with improved performance compared to supervised models trained from scratch.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 14,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11789",
        "abs_url": "https://arxiv.org/abs/2601.11789",
        "pdf_url": "https://arxiv.org/pdf/2601.11789",
        "title": "Suspicious Alignment of SGD: A Fine-Grained Step Size Condition Analysis",
        "authors": [
            "Shenyang Deng",
            "Boyao Liao",
            "Zhuoli Ouyang",
            "Tianyu Pang",
            "Minhak Song",
            "Yaoqing Yang"
        ],
        "comments": "The 37th International Conference on Algorithmic Learning Theory",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "This paper explores the suspicious alignment phenomenon in stochastic gradient descent (SGD) under ill-conditioned optimization, where the Hessian spectrum splits into dominant and bulk subspaces. This phenomenon describes the behavior of gradient alignment in SGD updates. Specifically, during the initial phase of SGD updates, the alignment between the gradient and the dominant subspace tends to decrease. Subsequently, it enters a rising phase and eventually stabilizes in a high-alignment phase. The alignment is considered ``suspicious'' because, paradoxically, the projected gradient update along this highly-aligned dominant subspace proves ineffective at reducing the loss. The focus of this work is to give a fine-grained analysis in a high-dimensional quadratic setup about how step size selection produces this phenomenon. Our main contribution can be summarized as follows: We propose a step-size condition revealing that in low-alignment regimes, an adaptive critical step size $\\eta_t^*$ separates alignment-decreasing ($\\eta_t < \\eta_t^*$) from alignment-increasing ($\\eta_t > \\eta_t^*$) regimes, whereas in high-alignment regimes, the alignment is self-correcting and decreases regardless of the step size. We further show that under sufficient ill-conditioning, a step size interval exists where projecting the SGD updates to the bulk space decreases the loss while projecting them to the dominant space increases the loss, which explains a recent empirical observation that projecting gradient updates to the dominant subspace is ineffective. Finally, based on this adaptive step-size theory, we prove that for a constant step size and large initialization, SGD exhibits this distinct two-phase behavior: an initial alignment-decreasing phase, followed by stabilization at high alignment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 15,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11821",
        "abs_url": "https://arxiv.org/abs/2601.11821",
        "pdf_url": "https://arxiv.org/pdf/2601.11821",
        "title": "Shapelets-Enriched Selective Forecasting using Time Series Foundation Models",
        "authors": [
            "Shivani Tomar",
            "Seshu Tirupathi",
            "Elizabeth Daly",
            "Ivana Dusparic"
        ],
        "comments": "Accepted by the AAAI-26 Workshop on Artificial Intelligence for Time Series Analysis (AI4TS)",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Time series foundation models have recently gained a lot of attention due to their ability to model complex time series data encompassing different domains including traffic, energy, and weather. Although they exhibit strong average zero-shot performance on forecasting tasks, their predictions on certain critical regions of the data are not always reliable, limiting their usability in real-world applications, especially when data exhibits unique trends. In this paper, we propose a selective forecasting framework to identify these critical segments of time series using shapelets. We learn shapelets using shift-invariant dictionary learning on the validation split of the target domain dataset. Utilizing distance-based similarity to these shapelets, we facilitate the user to selectively discard unreliable predictions and be informed of the model's realistic capabilities. Empirical results on diverse benchmark time series datasets demonstrate that our approach leveraging both zero-shot and full-shot fine-tuned models reduces the overall error by an average of 22.17% for zero-shot and 22.62% for full-shot fine-tuned model. Furthermore, our approach using zero-shot and full-shot fine-tuned models, also outperforms its random selection counterparts by up to 21.41% and 21.43% on one of the datasets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 16,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11864",
        "abs_url": "https://arxiv.org/abs/2601.11864",
        "pdf_url": "https://arxiv.org/pdf/2601.11864",
        "title": "AGGC: Adaptive Group Gradient Clipping for Stabilizing Large Language Model Training",
        "authors": [
            "Zhiyuan Li",
            "Yuan Wu",
            "Yi Chang"
        ],
        "comments": "13 pages",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
        "abstract": "To stabilize the training of Large Language Models (LLMs), gradient clipping is a nearly ubiquitous heuristic used to alleviate exploding gradients. However, traditional global norm clipping erroneously presupposes gradient homogeneity across different functional modules, leading to an adverse \"spill-over\" effect where volatile parameters force unnecessary scaling on stable ones. To overcome this, we propose Adaptive Group-wise Gradient Clipping (AGGC). AGGC partitions parameters into groups based on functional types and regulates each according to its historical behavior using an Exponential Moving Average (EMA). Specifically, it constructs an adaptive interval to simultaneously mitigate gradient explosion and vanishing, while employing a time-dependent scheduling mechanism to balance exploration and convergence. Experiments on LLaMA 2-7B, Mistral-7B, and Gemma-7B models show that AGGC consistently outperforms LoRA and frequently surpasses Full Fine-Tuning. On the GSM8K benchmark, Mistral-7B fine-tuned with AGGC achieves an accuracy of 72.93%, exceeding LoRA's 69.5%. AGGC also effectively stabilizes Reinforcement Learning with Verifiable Rewards (RLVR), enhancing the logic deduction of Qwen 2.5 and Llama 3.2 models. Experimental results demonstrate that AGGC effectively addresses the limitations of traditional gradient clipping methods, particularly in overcoming gradient heterogeneity, by utilizing a modular, adaptive clipping strategy to stabilize the training process. Due to its lightweight design, AGGC can be seamlessly integrated into existing post-training pipelines with negligible overhead.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 17,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11880",
        "abs_url": "https://arxiv.org/abs/2601.11880",
        "pdf_url": "https://arxiv.org/pdf/2601.11880",
        "title": "TF-CoDiT: Conditional Time Series Synthesis with Diffusion Transformers for Treasury Futures",
        "authors": [
            "Yingxiao Zhang",
            "Jiaxin Duan",
            "Junfu Zhang",
            "Ke Feng"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Diffusion Transformers (DiT) have achieved milestones in synthesizing financial time-series data, such as stock prices and order flows. However, their performance in synthesizing treasury futures data is still underexplored. This work emphasizes the characteristics of treasury futures data, including its low volume, market dependencies, and the grouped correlations among multivariables. To overcome these challenges, we propose TF-CoDiT, the first DiT framework for language-controlled treasury futures synthesis. To facilitate low-data learning, TF-CoDiT adapts the standard DiT by transforming multi-channel 1-D time series into Discrete Wavelet Transform (DWT) coefficient matrices. A U-shape VAE is proposed to encode cross-channel dependencies hierarchically into a latent variable and bridge the latent and DWT spaces through decoding, thereby enabling latent diffusion generation. To derive prompts that cover essential conditions, we introduce the Financial Market Attribute Protocol (FinMAP) - a multi-level description system that standardizes daily$/$periodical market dynamics by recognizing 17$/$23 economic indicators from 7/8 perspectives. In our experiments, we gather four types of treasury futures data covering the period from 2015 to 2025, and define data synthesis tasks with durations ranging from one week to four months. Extensive evaluations demonstrate that TF-CoDiT can produce highly authentic data with errors at most 0.433 (MSE) and 0.453 (MAE) to the ground-truth. Further studies evidence the robustness of TF-CoDiT across contracts and temporal horizons.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 18,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11883",
        "abs_url": "https://arxiv.org/abs/2601.11883",
        "pdf_url": "https://arxiv.org/pdf/2601.11883",
        "title": "Approximation Algorithm for Constrained $k$-Center Clustering: A Local Search Approach",
        "authors": [
            "Chaoqi Jia",
            "Longkun Guo",
            "Kewen Liao",
            "Zhigang Lu",
            "Chao Chen",
            "Jason Xue"
        ],
        "comments": "AAAI-26",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Clustering is a long-standing research problem and a fundamental tool in AI and data analysis. The traditional k-center problem, a fundamental theoretical challenge in clustering, has a best possible approximation ratio of 2, and any improvement to a ratio of 2 - {\\epsilon} would imply P = NP. In this work, we study the constrained k-center clustering problem, where instance-level cannot-link (CL) and must-link (ML) constraints are incorporated as background knowledge. Although general CL constraints significantly increase the hardness of approximation, previous work has shown that disjoint CL sets permit constant-factor approximations. However, whether local search can achieve such a guarantee in this setting remains an open question. To this end, we propose a novel local search framework based on a transformation to a dominating matching set problem, achieving the best possible approximation ratio of 2. The experimental results on both real-world and synthetic datasets demonstrate that our algorithm outperforms baselines in solution quality.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 19,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11890",
        "abs_url": "https://arxiv.org/abs/2601.11890",
        "pdf_url": "https://arxiv.org/pdf/2601.11890",
        "title": "From Relative Entropy to Minimax: A Unified Framework for Coverage in MDPs",
        "authors": [
            "Xihe Gu",
            "Urbashi Mitra",
            "Tara Javidi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Targeted and deliberate exploration of state--action pairs is essential in reward-free Markov Decision Problems (MDPs). More precisely, different state-action pairs exhibit different degree of importance or difficulty which must be actively and explicitly built into a controlled exploration strategy. To this end, we propose a weighted and parameterized family of concave coverage objectives, denoted by $U_\\rho$, defined directly over state--action occupancy measures. This family unifies several widely studied objectives within a single framework, including divergence-based marginal matching, weighted average coverage, and worst-case (minimax) coverage. While the concavity of $U_\\rho$ captures the diminishing return associated with over-exploration, the simple closed form of the gradient of $U_\\rho$ enables an explicit control to prioritize under-explored state--action pairs. Leveraging this structure, we develop a gradient-based algorithm that actively steers the induced occupancy toward a desired coverage pattern. Moreover, we show that as $\\rho$ increases, the resulting exploration strategy increasingly emphasizes the least-explored state--action pairs, recovering worst-case coverage behavior in the limit.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 20,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11897",
        "abs_url": "https://arxiv.org/abs/2601.11897",
        "pdf_url": "https://arxiv.org/pdf/2601.11897",
        "title": "Task-tailored Pre-processing: Fair Downstream Supervised Learning",
        "authors": [
            "Jinwon Sohn",
            "Guang Lin",
            "Qifan Song"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Methodology (stat.ME); Machine Learning (stat.ML)",
        "abstract": "Fairness-aware machine learning has recently attracted various communities to mitigate discrimination against certain societal groups in data-driven tasks. For fair supervised learning, particularly in pre-processing, there have been two main categories: data fairness and task-tailored fairness. The former directly finds an intermediate distribution among the groups, independent of the type of the downstream model, so a learned downstream classification/regression model returns similar predictive scores to individuals inputting the same covariates irrespective of their sensitive attributes. The latter explicitly takes the supervised learning task into account when constructing the pre-processing map. In this work, we study algorithmic fairness for supervised learning and argue that the data fairness approaches impose overly strong regularization from the perspective of the HGR correlation. This motivates us to devise a novel pre-processing approach tailored to supervised learning. We account for the trade-off between fairness and utility in obtaining the pre-processing map. Then we study the behavior of arbitrary downstream supervised models learned on the transformed data to find sufficient conditions to guarantee their fairness improvement and utility preservation. To our knowledge, no prior work in the branch of task-tailored methods has theoretically investigated downstream guarantees when using pre-processed data. We further evaluate our framework through comparison studies based on tabular and image data sets, showing the superiority of our framework which preserves consistent trade-offs among multiple downstream models compared to recent competing models. Particularly for computer vision data, we see our method alters only necessary semantic features related to the central machine learning task to achieve fairness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 21,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11924",
        "abs_url": "https://arxiv.org/abs/2601.11924",
        "pdf_url": "https://arxiv.org/pdf/2601.11924",
        "title": "Communication-Corruption Coupling and Verification in Cooperative Multi-Objective Bandits",
        "authors": [
            "Ming Shi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We study cooperative stochastic multi-armed bandits with vector-valued rewards under adversarial corruption and limited verification. In each of $T$ rounds, each of $N$ agents selects an arm, the environment generates a clean reward vector, and an adversary perturbs the observed feedback subject to a global corruption budget $\\Gamma$. Performance is measured by team regret under a coordinate-wise nondecreasing, $L$-Lipschitz scalarization $\\phi$, covering linear, Chebyshev, and smooth monotone utilities. Our main contribution is a communication-corruption coupling: we show that a fixed environment-side budget $\\Gamma$ can translate into an effective corruption level ranging from $\\Gamma$ to $N\\Gamma$, depending on whether agents share raw samples, sufficient statistics, or only arm recommendations. We formalize this via a protocol-induced multiplicity functional and prove regret bounds parameterized by the resulting effective corruption. As corollaries, raw-sample sharing can suffer an $N$-fold larger additive corruption penalty, whereas summary sharing and recommendation-only sharing preserve an unamplified $O(\\Gamma)$ term and achieve centralized-rate team regret. We further establish information-theoretic limits, including an unavoidable additive $\\Omega(\\Gamma)$ penalty and a high-corruption regime $\\Gamma=\\Theta(NT)$ where sublinear regret is impossible without clean information. Finally, we characterize how a global budget $\\nu$ of verified observations restores learnability. That is, verification is necessary in the high-corruption regime, and sufficient once it crosses the identification threshold, with certified sharing enabling the team's regret to become independent of $\\Gamma$.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 22,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11942",
        "abs_url": "https://arxiv.org/abs/2601.11942",
        "pdf_url": "https://arxiv.org/pdf/2601.11942",
        "title": "Trainability-Oriented Hybrid Quantum Regression via Geometric Preconditioning and Curriculum Optimization",
        "authors": [
            "Qingyu Meng",
            "Yangshuai Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Quantum Physics (quant-ph)",
        "abstract": "Quantum neural networks (QNNs) have attracted growing interest for scientific machine learning, yet in regression settings they often suffer from limited trainability under noisy gradients and ill-conditioned optimization. We propose a hybrid quantum-classical regression framework designed to mitigate these bottlenecks. Our model prepends a lightweight classical embedding that acts as a learnable geometric preconditioner, reshaping the input representation to better condition a downstream variational quantum circuit. Building on this architecture, we introduce a curriculum optimization protocol that progressively increases circuit depth and transitions from SPSA-based stochastic exploration to Adam-based gradient fine-tuning. We evaluate the approach on PDE-informed regression benchmarks and standard regression datasets under a fixed training budget in a simulator setting. Empirically, the proposed framework consistently improves over pure QNN baselines and yields more stable convergence in data-limited regimes. We further observe reduced structured errors that are visually correlated with oscillatory components on several scientific benchmarks, suggesting that geometric preconditioning combined with curriculum training is a practical approach for stabilizing quantum regression.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 23,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11953",
        "abs_url": "https://arxiv.org/abs/2601.11953",
        "pdf_url": "https://arxiv.org/pdf/2601.11953",
        "title": "Controlling Underestimation Bias in Constrained Reinforcement Learning for Safe Exploration",
        "authors": [
            "Shiqing Gao",
            "Jiaxin Ding",
            "Luoyi Fu",
            "Xinbing Wang"
        ],
        "comments": "Published in the 42nd International Conference on Machine Learning (ICML 2025, Oral)",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Constrained Reinforcement Learning (CRL) aims to maximize cumulative rewards while satisfying constraints. However, existing CRL algorithms often encounter significant constraint violations during training, limiting their applicability in safety-critical scenarios. In this paper, we identify the underestimation of the cost value function as a key factor contributing to these violations. To address this issue, we propose the Memory-driven Intrinsic Cost Estimation (MICE) method, which introduces intrinsic costs to mitigate underestimation and control bias to promote safer exploration. Inspired by flashbulb memory, where humans vividly recall dangerous experiences to avoid risks, MICE constructs a memory module that stores previously explored unsafe states to identify high-cost regions. The intrinsic cost is formulated as the pseudo-count of the current state visiting these risk regions. Furthermore, we propose an extrinsic-intrinsic cost value function that incorporates intrinsic costs and adopts a bias correction strategy. Using this function, we formulate an optimization objective within the trust region, along with corresponding optimization methods. Theoretically, we provide convergence guarantees for the proposed cost value function and establish the worst-case constraint violation for the MICE update. Extensive experiments demonstrate that MICE significantly reduces constraint violations while preserving policy performance comparable to baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 24,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11954",
        "abs_url": "https://arxiv.org/abs/2601.11954",
        "pdf_url": "https://arxiv.org/pdf/2601.11954",
        "title": "Data-centric Prompt Tuning for Dynamic Graphs",
        "authors": [
            "Yufei Peng",
            "Cheng Yang",
            "Zhengjie Fan",
            "Chuan Shi"
        ],
        "comments": "CIKM 2025",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Dynamic graphs have attracted increasing attention due to their ability to model complex and evolving relationships in real-world scenarios. Traditional approaches typically pre-train models using dynamic link prediction and directly apply the resulting node temporal embeddings to specific downstream tasks. However, the significant differences among downstream tasks often lead to performance degradation, especially under few-shot settings. Prompt tuning has emerged as an effective solution to this problem. Existing prompting methods are often strongly coupled with specific model architectures or pretraining tasks, which makes it difficult to adapt to recent or future model designs. Moreover, their exclusive focus on modifying node or temporal features while neglecting spatial structural information leads to limited expressiveness and degraded performance. To address these limitations, we propose DDGPrompt, a data-centric prompting framework designed to effectively refine pre-trained node embeddings at the input data level, enabling better adaptability to diverse downstream tasks. We first define a unified node expression feature matrix that aggregates all relevant temporal and structural information of each node, ensuring compatibility with a wide range of dynamic graph models. Then, we introduce three prompt matrices (temporal bias, edge weight, and feature mask) to adjust the feature matrix completely, achieving task-specific adaptation of node embeddings. We evaluate DDGPrompt under a strict few-shot setting on four public dynamic graph datasets. Experimental results demonstrate that our method significantly outperforms traditional methods and prompting approaches in scenarios with limited labels and cold-start conditions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 25,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11977",
        "abs_url": "https://arxiv.org/abs/2601.11977",
        "pdf_url": "https://arxiv.org/pdf/2601.11977",
        "title": "One-Shot Price Forecasting with Covariate-Guided Experts under Privacy Constraints",
        "authors": [
            "Ren He",
            "Yinliang Xu",
            "Jinfeng Wang",
            "Jeremy Watson",
            "Jian Song"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Forecasting in power systems often involves multivariate time series with complex dependencies and strict privacy constraints across regions. Traditional forecasting methods require significant expert knowledge and struggle to generalize across diverse deployment scenarios. Recent advancements in pre-trained time series models offer new opportunities, but their zero-shot performance on domain-specific tasks remains limited. To address these challenges, we propose a novel MoE Encoder module that augments pretrained forecasting models by injecting a sparse mixture-of-experts layer between tokenization and encoding. This design enables two key capabilities: (1) trans forming multivariate forecasting into an expert-guided univariate task, allowing the model to effectively capture inter-variable relations, and (2) supporting localized training and lightweight parameter sharing in federated settings where raw data cannot be exchanged. Extensive experiments on public multivariate datasets demonstrate that MoE-Encoder significantly improves forecasting accuracy compared to strong baselines. We further simulate federated environments and show that transferring only MoE-Encoder parameters allows efficient adaptation to new regions, with minimal performance degradation. Our findings suggest that MoE-Encoder provides a scalable and privacy-aware extension to foundation time series models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 26,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12008",
        "abs_url": "https://arxiv.org/abs/2601.12008",
        "pdf_url": "https://arxiv.org/pdf/2601.12008",
        "title": "Extreme Value Policy Optimization for Safe Reinforcement Learning",
        "authors": [
            "Shiqing Gao",
            "Yihang Zhou",
            "Shuai Shao",
            "Haoyu Luo",
            "Yiheng Bing",
            "Jiaxin Ding",
            "Luoyi Fu",
            "Xinbing Wang"
        ],
        "comments": "Published in the 42nd International Conference on Machine Learning (ICML 2025)",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Ensuring safety is a critical challenge in applying Reinforcement Learning (RL) to real-world scenarios. Constrained Reinforcement Learning (CRL) addresses this by maximizing returns under predefined constraints, typically formulated as the expected cumulative cost. However, expectation-based constraints overlook rare but high-impact extreme value events in the tail distribution, such as black swan incidents, which can lead to severe constraint violations. To address this issue, we propose the Extreme Value policy Optimization (EVO) algorithm, leveraging Extreme Value Theory (EVT) to model and exploit extreme reward and cost samples, reducing constraint violations. EVO introduces an extreme quantile optimization objective to explicitly capture extreme samples in the cost tail distribution. Additionally, we propose an extreme prioritization mechanism during replay, amplifying the learning signal from rare but high-impact extreme samples. Theoretically, we establish upper bounds on expected constraint violations during policy updates, guaranteeing strict constraint satisfaction at a zero-violation quantile level. Further, we demonstrate that EVO achieves a lower probability of constraint violations than expectation-based methods and exhibits lower variance than quantile regression methods. Extensive experiments show that EVO significantly reduces constraint violations during training while maintaining competitive policy performance compared to baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 27,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12011",
        "abs_url": "https://arxiv.org/abs/2601.12011",
        "pdf_url": "https://arxiv.org/pdf/2601.12011",
        "title": "Why Loss Re-weighting Works If You Stop Early: Training Dynamics of Unconstrained Features",
        "authors": [
            "Yize Zhao",
            "Christos Thrampoulidis"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The application of loss reweighting in modern deep learning presents a nuanced picture. While it fails to alter the terminal learning phase in overparameterized deep neural networks (DNNs) trained on high-dimensional datasets, empirical evidence consistently shows it offers significant benefits early in training. To transparently demonstrate and analyze this phenomenon, we introduce a small-scale model (SSM). This model is specifically designed to abstract the inherent complexities of both the DNN architecture and the input data, while maintaining key information about the structure of imbalance within its spectral components. On the one hand, the SSM reveals how vanilla empirical risk minimization preferentially learns to distinguish majority classes over minorities early in training, consequently delaying minority learning. In stark contrast, reweighting restores balanced learning dynamics, enabling the simultaneous learning of features associated with both majorities and minorities.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 28,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12083",
        "abs_url": "https://arxiv.org/abs/2601.12083",
        "pdf_url": "https://arxiv.org/pdf/2601.12083",
        "title": "Learning to Factorize and Adapt: A Versatile Approach Toward Universal Spatio-Temporal Foundation Models",
        "authors": [
            "Siru Zhong",
            "Junjie Qiu",
            "Yangyu Wu",
            "Yiqiu Liu",
            "Yuanpeng He",
            "Zhongwen Rao",
            "Bin Yang",
            "Chenjuan Guo",
            "Hao Xu",
            "Yuxuan Liang"
        ],
        "comments": "This is an extended version of the paper presented at NeurIPS 2025. Code available at this https URL",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Spatio-Temporal (ST) Foundation Models (STFMs) promise cross-dataset generalization, yet joint ST pretraining is computationally expensive and grapples with the heterogeneity of domain-specific spatial patterns. Substantially extending our preliminary conference version, we present FactoST-v2, an enhanced factorized framework redesigned for full weight transfer and arbitrary-length generalization. FactoST-v2 decouples universal temporal learning from domain-specific spatial adaptation. The first stage pretrains a minimalist encoder-only backbone using randomized sequence masking to capture invariant temporal dynamics, enabling probabilistic quantile prediction across variable horizons. The second stage employs a streamlined adapter to rapidly inject spatial awareness via meta adaptive learning and prompting. Comprehensive evaluations across diverse domains demonstrate that FactoST-v2 achieves state-of-the-art accuracy with linear efficiency - significantly outperforming existing foundation models in zero-shot and few-shot scenarios while rivaling domain-specific expert baselines. This factorized paradigm offers a practical, scalable path toward truly universal STFMs. Code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 29,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12091",
        "abs_url": "https://arxiv.org/abs/2601.12091",
        "pdf_url": "https://arxiv.org/pdf/2601.12091",
        "title": "Mitigating Cultural Bias in LLMs via Multi-Agent Cultural Debate",
        "authors": [
            "Qian Tan",
            "Lei Jiang",
            "Yuting Zeng",
            "Shuoyang Ding",
            "Xiaohua Xu"
        ],
        "comments": "13 pages",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Large language models (LLMs) exhibit systematic Western-centric bias, yet whether prompting in non-Western languages (e.g., Chinese) can mitigate this remains understudied. Answering this question requires rigorous evaluation and effective mitigation, but existing approaches fall short on both fronts: evaluation methods force outputs into predefined cultural categories without a neutral option, while mitigation relies on expensive multi-cultural corpora or agent frameworks that use functional roles (e.g., Planner--Critique) lacking explicit cultural representation. To address these gaps, we introduce CEBiasBench, a Chinese--English bilingual benchmark, and Multi-Agent Vote (MAV), which enables explicit ``no bias'' judgments. Using this framework, we find that Chinese prompting merely shifts bias toward East Asian perspectives rather than eliminating it. To mitigate such persistent bias, we propose Multi-Agent Cultural Debate (MACD), a training-free framework that assigns agents distinct cultural personas and orchestrates deliberation via a \"Seeking Common Ground while Reserving Differences\" strategy. Experiments demonstrate that MACD achieves 57.6% average No Bias Rate evaluated by LLM-as-judge and 86.0% evaluated by MAV (vs. 47.6% and 69.0% baseline using GPT-4o as backbone) on CEBiasBench and generalizes to the Arabic CAMeL benchmark, confirming that explicit cultural representation in agent frameworks is essential for cross-cultural fairness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 30,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12093",
        "abs_url": "https://arxiv.org/abs/2601.12093",
        "pdf_url": "https://arxiv.org/pdf/2601.12093",
        "title": "PTL-PINNs: Perturbation-Guided Transfer Learning with Physics- Informed Neural Networks for Nonlinear Systems",
        "authors": [
            "Duarte Alexandrino",
            "Ben Moseley",
            "Pavlos Protopapas"
        ],
        "comments": "51 pages, 14 figures, 7 tables",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Accurately and efficiently solving nonlinear differential equations is crucial for modeling dynamic behavior across science and engineering. Physics-Informed Neural Networks (PINNs) have emerged as a powerful solution that embeds physical laws in training by enforcing equation residuals. However, these struggle to model nonlinear dynamics, suffering from limited generalization across problems and long training times. To address these limitations, we propose a perturbation-guided transfer learning framework for PINNs (PTL-PINN), which integrates perturbation theory with transfer learning to efficiently solve nonlinear equations. Unlike gradient-based transfer learning, PTL-PINNs solve an approximate linear perturbative system using closed-form expressions, enabling rapid generalization with the time complexity of matrix-vector multiplication. We show that PTL-PINNs achieve accuracy comparable to various Runge-Kutta methods, with computational speeds up to one order of magnitude faster. To benchmark performance, we solve a broad set of problems, including nonlinear oscillators across various damping regimes, the equilibrium-centered Lotka-Volterra system, the KPP-Fisher and the Wave equation. Since perturbation theory sets the accuracy bound of PTL-PINNs, we systematically evaluate its practical applicability. This work connects long-standing perturbation methods with PINNs, demonstrating how perturbation theory can guide foundational models to solve nonlinear systems with speeds comparable to those of classical solvers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 31,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12095",
        "abs_url": "https://arxiv.org/abs/2601.12095",
        "pdf_url": "https://arxiv.org/pdf/2601.12095",
        "title": "Neural Isomorphic Fields: A Transformer-based Algebraic Numerical Embedding",
        "authors": [
            "Hamidreza Sadeghi",
            "Saeedeh Momtazi",
            "Reza Safabakhsh"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Neural network models often face challenges when processing very small or very large numbers due to issues such as overflow, underflow, and unstable output variations. To mitigate these problems, we propose using embedding vectors for numbers instead of directly using their raw values. These embeddings aim to retain essential algebraic properties while preventing numerical instabilities. In this paper, we introduce, for the first time, a fixed-length number embedding vector that preserves algebraic operations, including addition, multiplication, and comparison, within the field of rational numbers. We propose a novel Neural Isomorphic Field, a neural abstraction of algebraic structures such as groups and fields. The elements of this neural field are embedding vectors that maintain algebraic structure during computations. Our experiments demonstrate that addition performs exceptionally well, achieving over 95 percent accuracy on key algebraic tests such as identity, closure, and associativity. In contrast, multiplication exhibits challenges, with accuracy ranging from 53 percent to 73 percent across various algebraic properties. These findings highlight the model's strengths in preserving algebraic properties under addition while identifying avenues for further improvement in handling multiplication.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 32,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12131",
        "abs_url": "https://arxiv.org/abs/2601.12131",
        "pdf_url": "https://arxiv.org/pdf/2601.12131",
        "title": "SolarGPT-QA: A Domain-Adaptive Large Language Model for Educational Question Answering in Space Weather and Heliophysics",
        "authors": [
            "Santosh Chapagain",
            "MohammadReza EskandariNasab",
            "Onur Vural",
            "Shah Muhammad Hamdi",
            "Soukaina Filali Boubrahimi"
        ],
        "comments": "This is preliminary work towards a broader SolarGPT framework",
        "subjects": "Machine Learning (cs.LG); Human-Computer Interaction (cs.HC)",
        "abstract": "Solar activity, including solar flares, coronal mass ejections (CMEs), and geomagnetic storms, can significantly impact satellites, aviation, power grids, data centers, and space missions. Extreme solar events can cause substantial economic damage if not predicted in advance, highlighting the importance of accurate forecasting and effective education in space science. Although large language models (LLMs) perform well on general tasks, they often lack domain-specific knowledge and pedagogical capability to clearly explain complex space science concepts. We introduce SolarGPT-QA, a question answering system based on a domain-adapted large language model built on the LLaMA-3 base model. The model is trained using scientific literature and large-scale question-answer data generated with GPT-4 and refined using Grok-3 in a student-friendly storytelling style. Human pairwise evaluations show that SolarGPT-QA outperforms general-purpose models in zero-shot settings and achieves competitive performance compared to instruction-tuned models for educational explanations in space weather and heliophysics. A small pilot student comprehension study further suggests improved clarity and accessibility of the generated explanations. Ablation experiments indicate that combining domain-adaptive pretraining with pedagogical fine-tuning is important for balancing scientific accuracy and educational effectiveness. This work represents an initial step toward a broader SolarGPT framework for space science education and forecasting.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 33,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12145",
        "abs_url": "https://arxiv.org/abs/2601.12145",
        "pdf_url": "https://arxiv.org/pdf/2601.12145",
        "title": "Threshold Differential Attention for Sink-Free, Ultra-Sparse, and Non-Dispersive Language Modeling",
        "authors": [
            "Xingyue Huang",
            "Xueying Ding",
            "Mingxuan Ju",
            "Yozen Liu",
            "Neil Shah",
            "Tong Zhao"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Softmax attention struggles with long contexts due to structural limitations: the strict sum-to-one constraint forces attention sinks on irrelevant tokens, and probability mass disperses as sequence lengths increase. We tackle these problems with Threshold Differential Attention (TDA), a sink-free attention mechanism that achieves ultra-sparsity and improved robustness at longer sequence lengths without the computational overhead of projection methods or the performance degradation caused by noise accumulation of standard rectified attention. TDA applies row-wise extreme-value thresholding with a length-dependent gate, retaining only exceedances. Inspired by the differential transformer, TDA also subtracts an inhibitory view to enhance expressivity. Theoretically, we prove that TDA controls the expected number of spurious survivors per row to $O(1)$ and that consensus spurious matches across independent views vanish as context grows. Empirically, TDA produces $>99\\%$ exact zeros and eliminates attention sinks while maintaining competitive performance on standard and long-context benchmarks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 34,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12178",
        "abs_url": "https://arxiv.org/abs/2601.12178",
        "pdf_url": "https://arxiv.org/pdf/2601.12178",
        "title": "Federated Learning for the Design of Parametric Insurance Indices under Heterogeneous Renewable Production Losses",
        "authors": [
            "Fallou Niakh"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "We propose a federated learning framework for the calibration of parametric insurance indices under heterogeneous renewable energy production losses. Producers locally model their losses using Tweedie generalized linear models and private data, while a common index is learned through federated optimization without sharing raw observations. The approach accommodates heterogeneity in variance and link functions and directly minimizes a global deviance objective in a distributed setting. We implement and compare FedAvg, FedProx and FedOpt, and benchmark them against an existing approximation-based aggregation method. An empirical application to solar power production in Germany shows that federated learning recovers comparable index coefficients under moderate heterogeneity, while providing a more general and scalable framework.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 35,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12213",
        "abs_url": "https://arxiv.org/abs/2601.12213",
        "pdf_url": "https://arxiv.org/pdf/2601.12213",
        "title": "One-Sided Matrix Completion from Ultra-Sparse Samples",
        "authors": [
            "Hongyang R. Zhang",
            "Zhenshuo Zhang",
            "Huy L. Nguyen",
            "Guanghui Lan"
        ],
        "comments": "41 pages",
        "subjects": "Machine Learning (cs.LG); Optimization and Control (math.OC); Machine Learning (stat.ML)",
        "abstract": "Matrix completion is a classical problem that has received recurring interest across a wide range of fields. In this paper, we revisit this problem in an ultra-sparse sampling regime, where each entry of an unknown, $n\\times d$ matrix $M$ (with $n \\ge d$) is observed independently with probability $p = C / d$, for a fixed integer $C \\ge 2$. This setting is motivated by applications involving large, sparse panel datasets, where the number of rows far exceeds the number of columns. When each row contains only $C$ entries -- fewer than the rank of $M$ -- accurate imputation of $M$ is impossible. Instead, we estimate the row span of $M$ or the averaged second-moment matrix $T = M^{\\top} M / n$. The empirical second-moment matrix computed from observed entries exhibits non-random and sparse missingness. We propose an unbiased estimator that normalizes each nonzero entry of the second moment by its observed frequency, followed by gradient descent to impute the missing entries of $T$. The normalization divides a weighted sum of $n$ binomial random variables by the total number of ones. We show that the estimator is unbiased for any $p$ and enjoys low variance. When the row vectors of $M$ are drawn uniformly from a rank-$r$ factor model satisfying an incoherence condition, we prove that if $n \\ge O({d r^5 \\epsilon^{-2} C^{-2} \\log d})$, any local minimum of the gradient-descent objective is approximately global and recovers $T$ with error at most $\\epsilon^2$. Experiments on both synthetic and real-world data validate our approach. On three MovieLens datasets, our algorithm reduces bias by $88\\%$ relative to baseline estimators. We also empirically validate the linear sampling complexity of $n$ relative to $d$ on synthetic data. On an Amazon reviews dataset with sparsity $10^{-7}$, our method reduces the recovery error of $T$ by $59\\%$ and $M$ by $38\\%$ compared to baseline methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 36,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12227",
        "abs_url": "https://arxiv.org/abs/2601.12227",
        "pdf_url": "https://arxiv.org/pdf/2601.12227",
        "title": "Learning Longitudinal Health Representations from EHR and Wearable Data",
        "authors": [
            "Yuanyun Zhang",
            "Han Zhou",
            "Li Feng",
            "Yilin Hong",
            "Shi Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Foundation models trained on electronic health records show strong performance on many clinical prediction tasks but are limited by sparse and irregular documentation. Wearable devices provide dense continuous physiological signals but lack semantic grounding. Existing methods usually model these data sources separately or combine them through late fusion. We propose a multimodal foundation model that jointly represents electronic health records and wearable data as a continuous time latent process. The model uses modality specific encoders and a shared temporal backbone pretrained with self supervised and cross modal objectives. This design produces representations that are temporally coherent and clinically grounded. Across forecasting physiological and risk modeling tasks the model outperforms strong electronic health record only and wearable only baselines especially at long horizons and under missing data. These results show that joint electronic health record and wearable pretraining yields more faithful representations of longitudinal health.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 37,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12231",
        "abs_url": "https://arxiv.org/abs/2601.12231",
        "pdf_url": "https://arxiv.org/pdf/2601.12231",
        "title": "Wavelet-Aware Anomaly Detection in Multi-Channel User Logs via Deviation Modulation and Resolution-Adaptive Attention",
        "authors": [
            "Kaichuan Kong",
            "Dongjie Liu",
            "Xiaobo Jin",
            "Shijie Xu",
            "Guanggang Geng"
        ],
        "comments": "Accepted by ICASSP 2026. Copyright 2026 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works",
        "subjects": "Machine Learning (cs.LG); Cryptography and Security (cs.CR); Computation (stat.CO)",
        "abstract": "Insider threat detection is a key challenge in enterprise security, relying on user activity logs that capture rich and complex behavioral patterns. These logs are often multi-channel, non-stationary, and anomalies are rare, making anomaly detection challenging. To address these issues, we propose a novel framework that integrates wavelet-aware modulation, multi-resolution wavelet decomposition, and resolution-adaptive attention for robust anomaly detection. Our approach first applies a deviation-aware modulation scheme to suppress routine behaviors while amplifying anomalous deviations. Next, discrete wavelet transform (DWT) decomposes the log signals into multi-resolution representations, capturing both long-term trends and short-term anomalies. Finally, a learnable attention mechanism dynamically reweights the most discriminative frequency bands for detection. On the CERT r4.2 benchmark, our approach consistently outperforms existing baselines in precision, recall, and F1 score across various time granularities and scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 38,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12296",
        "abs_url": "https://arxiv.org/abs/2601.12296",
        "pdf_url": "https://arxiv.org/pdf/2601.12296",
        "title": "Distribution Shift Is Key to Learning Invariant Prediction",
        "authors": [
            "Hong Zheng",
            "Fei Teng"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "An interesting phenomenon arises: Empirical Risk Minimization (ERM) sometimes outperforms methods specifically designed for out-of-distribution tasks. This motivates an investigation into the reasons behind such behavior beyond algorithmic design. In this study, we find that one such reason lies in the distribution shift across training domains. A large degree of distribution shift can lead to better performance even under ERM. Specifically, we derive several theoretical and empirical findings demonstrating that distribution shift plays a crucial role in model learning and benefits learning invariant prediction. Firstly, the proposed upper bounds indicate that the degree of distribution shift directly affects the prediction ability of the learned models. If it is large, the models' ability can increase, approximating invariant prediction models that make stable predictions under arbitrary known or unseen domains; and vice versa. We also prove that, under certain data conditions, ERM solutions can achieve performance comparable to that of invariant prediction models. Secondly, the empirical validation results demonstrated that the predictions of learned models approximate those of Oracle or Optimal models, provided that the degree of distribution shift in the training data increases.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 39,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12305",
        "abs_url": "https://arxiv.org/abs/2601.12305",
        "pdf_url": "https://arxiv.org/pdf/2601.12305",
        "title": "Machine Learning as a Service (MLaaS) Dataset Generator Framework for IoT Environments",
        "authors": [
            "Deepak Kanneganti",
            "Sajib Mistry",
            "Sheik Fattah",
            "Joshua Boland",
            "Aneesh Krishna"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We propose a novel MLaaS Dataset Generator (MDG) framework that creates configurable and reproducible datasets for evaluating Machine Learning as a Service (MLaaS) selection and composition. MDG simulates realistic MLaaS behaviour by training and evaluating diverse model families across multiple real-world datasets and data distribution settings. It records detailed functional attributes, quality of service metrics, and composition-specific indicators, enabling systematic analysis of service performance and cross-service behaviour. Using MDG, we generate more than ten thousand MLaaS service instances and construct a large-scale benchmark dataset suitable for downstream evaluation. We also implement a built-in composition mechanism that models how services interact under varied Internet of Things conditions. Experiments demonstrate that datasets generated by MDG enhance selection accuracy and composition quality compared to existing baselines. MDG provides a practical and extensible foundation for advancing data-driven research on MLaaS selection and composition",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 40,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12322",
        "abs_url": "https://arxiv.org/abs/2601.12322",
        "pdf_url": "https://arxiv.org/pdf/2601.12322",
        "title": "Ordered Local Momentum for Asynchronous Distributed Learning under Arbitrary Delays",
        "authors": [
            "Chang-Wei Shi",
            "Shi-Shang Wang",
            "Wu-Jun Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Momentum SGD (MSGD) serves as a foundational optimizer in training deep models due to momentum's key role in accelerating convergence and enhancing generalization. Meanwhile, asynchronous distributed learning is crucial for training large-scale deep models, especially when the computing capabilities of the workers in the cluster are heterogeneous. To reduce communication frequency, local updates are widely adopted in distributed learning. However, how to implement asynchronous distributed MSGD with local updates remains unexplored. To solve this problem, we propose a novel method, called \\underline{or}dered \\underline{lo}cal \\underline{mo}mentum (OrLoMo), for asynchronous distributed learning. In OrLoMo, each worker runs MSGD locally. Then the local momentum from each worker will be aggregated by the server in order based on its global iteration index. To the best of our knowledge, OrLoMo is the first method to implement asynchronous distributed MSGD with local updates. We prove the convergence of OrLoMo for non-convex problems under arbitrary delays. Experiments validate that OrLoMo can outperform its synchronous counterpart and other asynchronous methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 41,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12341",
        "abs_url": "https://arxiv.org/abs/2601.12341",
        "pdf_url": "https://arxiv.org/pdf/2601.12341",
        "title": "Time-Continuous Modeling for Temporal Affective Pattern Recognition in LLMs",
        "authors": [
            "Rezky Kam",
            "Coddy N. Siswanto"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Emerging Technologies (cs.ET); Human-Computer Interaction (cs.HC); Systems and Control (eess.SY)",
        "abstract": "This paper introduces a dataset and conceptual framework for LLMs to mimic real world emotional dynamics through time and in-context learning leveraging physics-informed neural network, opening a possibility for interpretable dialogue modeling.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 42,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12355",
        "abs_url": "https://arxiv.org/abs/2601.12355",
        "pdf_url": "https://arxiv.org/pdf/2601.12355",
        "title": "LB-MCTS: Synergizing Large Language Models and Bayesian Optimization for Efficient CASH",
        "authors": [
            "Beicheng Xu",
            "Weitong Qian",
            "Lingching Tung",
            "Yupeng Lu",
            "Bin Cui"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "To lower the expertise barrier in machine learning, the AutoML community has focused on the CASH problem, a fundamental challenge that automates the process of algorithm selection and hyperparameter tuning. While traditional methods like Bayesian Optimization (BO) struggle with cold-start issues, Large Language Models (LLMs) can mitigate these via semantic priors. However, existing LLM-based optimizers generalize poorly to the high-dimensional, structured CASH space. We propose LB-MCTS, a framework synergizing LLMs and BO within a Monte Carlo Tree Search structure. It maximizes LLM reasoning with Selective Tuning Memory (STM) and explicit exploration-exploitation trade-off. It combines the strengths of both paradigms by dynamically shifting from LLM-driven to BO-driven proposals as data accumulates. Experiments on 104 AMLB datasets demonstrate the superiority of LB-MCTS over the competitive baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 43,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12362",
        "abs_url": "https://arxiv.org/abs/2601.12362",
        "pdf_url": "https://arxiv.org/pdf/2601.12362",
        "title": "Machine Learning-Based Framework for Real Time Detection and Early Prediction of Control Valve Stiction in Industrial Control Systems",
        "authors": [
            "Natthapong Promsricha",
            "Chotirawee Chatpattanasiri",
            "Nuttavut Kerdgongsup",
            "Stavroula Balabani"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Instrumentation and Detectors (physics.ins-det)",
        "abstract": "Control valve stiction, a friction that prevents smooth valve movement, is a common fault in industrial process systems that causes instability, equipment wear, and higher maintenance costs. Many plants still operate with conventional valves that lack real time monitoring, making early predictions challenging. This study presents a machine learning (ML) framework for detecting and predicting stiction using only routinely collected process signals: the controller output (OP) from control systems and the process variable (PV), such as flow rate. Three deep learning models were developed and compared: a Convolutional Neural Network (CNN), a hybrid CNN with a Support Vector Machine (CNN-SVM), and a Long Short-Term Memory (LSTM) network. To train these models, a data-driven labeling method based on slope ratio analysis was applied to a real oil and gas refinery dataset. The LSTM model achieved the highest accuracy and was able to predict stiction up to four hours in advance. To the best of the authors' knowledge, this is the first study to demonstrate ML based early prediction of control valve stiction from real industry data. The proposed framework can be integrated into existing control systems to support predictive maintenance, reduce downtime, and avoid unnecessary hardware replacement.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 44,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12380",
        "abs_url": "https://arxiv.org/abs/2601.12380",
        "pdf_url": "https://arxiv.org/pdf/2601.12380",
        "title": "Statistical-Neural Interaction Networks for Interpretable Mixed-Type Data Imputation",
        "authors": [
            "Ou Deng",
            "Shoji Nishimura",
            "Atsushi Ogihara",
            "Qun Jin"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Real-world tabular databases routinely combine continuous measurements and categorical records, yet missing entries are pervasive and can distort downstream analysis. We propose Statistical-Neural Interaction (SNI), an interpretable mixed-type imputation framework that couples correlation-derived statistical priors with neural feature attention through a Controllable-Prior Feature Attention (CPFA) module. CPFA learns head-wise prior-strength coefficients $\\{\\lambda_h\\}$ that softly regularize attention toward the prior while allowing data-driven deviations when nonlinear patterns appear to be present in the data. Beyond imputation, SNI aggregates attention maps into a directed feature-dependency matrix that summarizes which variables the imputer relied on, without requiring post-hoc explainers. We evaluate SNI against six baselines (Mean/Mode, MICE, KNN, MissForest, GAIN, MIWAE) on six datasets spanning ICU monitoring, population surveys, socio-economic statistics, and engineering applications. Under MCAR/strict-MAR at 30\\% missingness, SNI is generally competitive on continuous metrics but is often outperformed by accuracy-first baselines (MissForest, MIWAE) on categorical variables; in return, it provides intrinsic dependency diagnostics and explicit statistical-neural trade-off parameters. We additionally report MNAR stress tests (with a mask-aware variant) and discuss computational cost, limitations -- particularly for severely imbalanced categorical targets -- and deployment scenarios where interpretability may justify the trade-off.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 45,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12405",
        "abs_url": "https://arxiv.org/abs/2601.12405",
        "pdf_url": "https://arxiv.org/pdf/2601.12405",
        "title": "Explainable Machine Learning for Pediatric Dental Risk Stratification Using Socio-Demographic Determinants",
        "authors": [
            "Manasi Kanade",
            "Abhi Thakkar",
            "Gabriela Fernandes"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Background: Pediatric dental disease remains one of the most prevalent and inequitable chronic health conditions worldwide. Although strong epidemiological evidence links oral health outcomes to socio-economic and demographic determinants, most artificial intelligence (AI) applications in dentistry rely on image-based diagnosis and black-box prediction models, limiting transparency and ethical applicability in pediatric populations. Objective: This study aimed to develop and evaluate an explainable machine learning framework for pediatric dental risk stratification that prioritizes interpretability, calibration, and ethical deployment over maximal predictive accuracy. Methods: A supervised machine learning model was trained using population-level pediatric data including age, income-to-poverty ratio, race/ethnicity, gender, and medical history. Model performance was assessed using receiver operating characteristic (ROC) analysis and calibration curves. Explainability was achieved using SHapley Additive exPlanations (SHAP) to provide global and individual-level interpretation of predictions. Results: The model achieved modest discrimination (AUC = 0.61) with conservative calibration, underestimating risk at higher probability levels. SHAP analysis identified age and income-to-poverty ratio as the strongest contributors to predicted risk, followed by race/ethnicity and gender. Conclusion: Explainable machine learning enables transparent, prevention-oriented pediatric dental risk stratification and supports population screening and equitable resource allocation rather than diagnostic decision-making.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 46,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12426",
        "abs_url": "https://arxiv.org/abs/2601.12426",
        "pdf_url": "https://arxiv.org/pdf/2601.12426",
        "title": "Graph Attention Networks with Physical Constraints for Anomaly Detection",
        "authors": [
            "Mohammadhossein Homaei",
            "Iman Khazrak",
            "Ruben Molano",
            "Andres Caro",
            "Mar Avila"
        ],
        "comments": "7 Pages, 4 Figures, 5 Tables",
        "subjects": "Machine Learning (cs.LG); Cryptography and Security (cs.CR)",
        "abstract": "Water distribution systems (WDSs) face increasing cyber-physical risks, which make reliable anomaly detection essential. Many data-driven models ignore network topology and are hard to interpret, while model-based ones depend strongly on parameter accuracy. This work proposes a hydraulic-aware graph attention network using normalized conservation law violations as features. It combines mass and energy balance residuals with graph attention and bidirectional LSTM to learn spatio-temporal patterns. A multi-scale module aggregates detection scores from node to network level. On the BATADAL dataset, it reaches $F1=0.979$, showing $3.3$pp gain and high robustness under $15\\%$ parameter noise.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 47,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12467",
        "abs_url": "https://arxiv.org/abs/2601.12467",
        "pdf_url": "https://arxiv.org/pdf/2601.12467",
        "title": "Patch-Level Tokenization with CNN Encoders and Attention for Improved Transformer Time-Series Forecasting",
        "authors": [
            "Saurish Nagrath"
        ],
        "comments": "6 pages, 2 figures, 3 tables",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Transformer-based models have shown strong performance in time-series forecasting by leveraging self-attention to model long-range temporal dependencies. However, their effectiveness depends critically on the quality and structure of input representations derived from raw multivariate time-series data. This work proposes a two-stage forecasting framework that explicitly separates local temporal representation learning from global dependency modelling. In the first stage, a convolutional neural network (CNN) operates on fixed-length temporal patches to extract short-range temporal dynamics and non-linear feature interactions, producing compact patch-level token embeddings. Token-level self-attention is subsequently applied during representation learning to refine these embeddings by enabling interactions across temporal patches. In the second stage, a Transformer encoder processes the resulting token sequence to model inter-patch temporal dependencies and generate per-patch forecasts. Experiments conducted on synthetic multivariate time-series data with controlled static and dynamic factors demonstrate that the proposed patch-based tokenization strategy achieves competitive forecasting performance compared to convolutional and patch-based Transformer baselines. The results highlight the importance of structured temporal representations and show that decoupling local temporal encoding from global attention-based modelling yields more effective and stable time-series forecasting.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 48,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12502",
        "abs_url": "https://arxiv.org/abs/2601.12502",
        "pdf_url": "https://arxiv.org/pdf/2601.12502",
        "title": "Semidefinite Programming for Quantum Channel Learning",
        "authors": [
            "Mikhail Gennadievich Belov",
            "Victor Victorovich Dubov",
            "Vadim Konstantinovich Ivanov",
            "Alexander Yurievich Maslov",
            "Olga Vladimirovna Proshina",
            "Vladislav Gennadievich Malyshkin"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Numerical Analysis (math.NA); Quantum Physics (quant-ph)",
        "abstract": "The problem of reconstructing a quantum channel from a sample of classical data is considered. When the total fidelity can be represented as a ratio of two quadratic forms (e.g., in the case of mapping a mixed state to a pure state, projective operators, unitary learning, and others), Semidefinite Programming (SDP) can be applied to solve the fidelity optimization problem with respect to the Choi matrix. A remarkable feature of SDP is that the optimization is convex, which allows the problem to be efficiently solved by a variety of numerical algorithms. We have tested several commercially available SDP solvers, all of which allowed for the reconstruction of quantum channels of different forms. A notable feature is that the Kraus rank of the obtained quantum channel typically comprises less than a few percent of its maximal possible value. This suggests that a relatively small Kraus rank quantum channel is typically sufficient to describe experimentally observed classical data. The theory was also applied to the problem of reconstructing projective operators from data. Finally, we discuss a classical computational model based on quantum channel transformation, performed and calculated on a classical computer, possibly hardware-optimized.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 49,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12519",
        "abs_url": "https://arxiv.org/abs/2601.12519",
        "pdf_url": "https://arxiv.org/pdf/2601.12519",
        "title": "Learning Relativistic Geodesics and Chaotic Dynamics via Stabilized Lagrangian Neural Networks",
        "authors": [
            "Abdullah Umut Hamzaogullari",
            "Arkadas Ozakin"
        ],
        "comments": "21 pages",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Lagrangian Neural Networks (LNNs) can learn arbitrary Lagrangians from trajectory data, but their unusual optimization objective leads to significant training instabilities that limit their application to complex systems. We propose several improvements that address these fundamental challenges, namely, a Hessian regularization scheme that penalizes unphysical signatures in the Lagrangian's second derivatives with respect to velocities, preventing the network from learning unstable dynamics, activation functions that are better suited to the problem of learning Lagrangians, and a physics-aware coordinate scaling that improves stability. We systematically evaluate these techniques alongside previously proposed methods for improving stability. Our improved architecture successfully trains on systems of unprecedented complexity, including triple pendulums, and achieved 96.6\\% lower validation loss value and 90.68\\% better stability than baseline LNNs in double pendulum systems. With the improved framework, we show that our LNNs can learn Lagrangians representing geodesic motion in both non-relativistic and general relativistic settings. To deal with the relativistic setting, we extended our regularization to penalize violations of Lorentzian signatures, which allowed us to predict a geodesic Lagrangian under AdS\\textsubscript{4} spacetime metric directly from trajectory data, which to our knowledge has not been done in the literature before. This opens new possibilities for automated discovery of geometric structures in physics, including extraction of spacetime metric tensor components from geodesic trajectories. While our approach inherits some limitations of the original LNN framework, particularly the requirement for invertible Hessians, it significantly expands the practical applicability of LNNs for scientific discovery tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 50,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12525",
        "abs_url": "https://arxiv.org/abs/2601.12525",
        "pdf_url": "https://arxiv.org/pdf/2601.12525",
        "title": "Approximating splits for decision trees quickly in sparse data streams",
        "authors": [
            "Nikolaj Tatti"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Data Structures and Algorithms (cs.DS)",
        "abstract": "Decision trees are one of the most popular classifiers in the machine learning literature. While the most common decision tree learning algorithms treat data as a batch, numerous algorithms have been proposed to construct decision trees from a data stream. A standard training strategy involves augmenting the current tree by changing a leaf node into a split. Here we typically maintain counters in each leaf which allow us to determine the optimal split, and whether the split should be done. In this paper we focus on how to speed up the search for the optimal split when dealing with sparse binary features and a binary class. We focus on finding splits that have the approximately optimal information gain or Gini index. In both cases finding the optimal split can be done in $O(d)$ time, where $d$ is the number of features. We propose an algorithm that yields $(1 + \\alpha)$ approximation when using conditional entropy in amortized $O(\\alpha^{-1}(1 + m\\log d) \\log \\log n)$ time, where $m$ is the number of 1s in a data point, and $n$ is the number of data points. Similarly, for Gini index, we achieve $(1 + \\alpha)$ approximation in amortized $O(\\alpha^{-1} + m \\log d)$ time. Our approach is beneficial for sparse data where $m \\ll d$. In our experiments we find almost-optimal splits efficiently, faster than the baseline, overperforming the theoretical approximation guarantees.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 51,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12543",
        "abs_url": "https://arxiv.org/abs/2601.12543",
        "pdf_url": "https://arxiv.org/pdf/2601.12543",
        "title": "Press Start to Charge: Videogaming the Online Centralized Charging Scheduling Problem",
        "authors": [
            "Alireza Ghahtarani",
            "Martin Cousineau",
            "Amir-massoud Farahmand",
            "Jorge E. Mendoza"
        ],
        "comments": "41 pages",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We study the online centralized charging scheduling problem (OCCSP). In this problem, a central authority must decide, in real time, when to charge dynamically arriving electric vehicles (EVs), subject to capacity limits, with the objective of balancing load across a finite planning horizon. To solve the problem, we first gamify it; that is, we model it as a game where charging blocks are placed within temporal and capacity constraints on a grid. We design heuristic policies, train learning agents with expert demonstrations, and improve them using Dataset Aggregation (DAgger). From a theoretical standpoint, we show that gamification reduces model complexity and yields tighter generalization bounds than vector-based formulations. Experiments across multiple EV arrival patterns confirm that gamified learning enhances load balancing. In particular, the image-to-movement model trained with DAgger consistently outperforms heuristic baselines, vector-based approaches, and supervised learning agents, while also demonstrating robustness in sensitivity analyses. These operational gains translate into tangible economic value. In a real-world case study for the Greater Montréal Area (Québec, Canada) using utility cost data, the proposed methods lower system costs by tens of millions of dollars per year over the prevailing practice and show clear potential to delay costly grid upgrades.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 52,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12598",
        "abs_url": "https://arxiv.org/abs/2601.12598",
        "pdf_url": "https://arxiv.org/pdf/2601.12598",
        "title": "Dissecting Linear Recurrent Models: How Different Gating Strategies Drive Selectivity and Generalization",
        "authors": [
            "Younes Bouhadjar",
            "Maxime Fabre",
            "Felix Schmidt",
            "Emre Neftci"
        ],
        "comments": "11 pages, 4 figures and 4 tables",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
        "abstract": "Linear recurrent neural networks have emerged as efficient alternatives to the original Transformer's softmax attention mechanism, thanks to their highly parallelizable training and constant memory and computation requirements at inference. Iterative refinements of these models have introduced an increasing number of architectural mechanisms, leading to increased complexity and computational costs. Nevertheless, systematic direct comparisons among these models remain limited. Existing benchmark tasks are either too simplistic to reveal substantial differences or excessively resource-intensive for experimentation. In this work, we propose a refined taxonomy of linear recurrent models and introduce SelectivBench, a set of lightweight and customizable synthetic benchmark tasks for systematically evaluating sequence models. SelectivBench specifically evaluates selectivity in sequence models at small to medium scale, such as the capacity to focus on relevant inputs while ignoring context-based distractors. It employs rule-based grammars to generate sequences with adjustable complexity, incorporating irregular gaps that intentionally violate transition rules. Evaluations of linear recurrent models on SelectivBench reveal performance patterns consistent with results from large-scale language tasks. Our analysis clarifies the roles of essential architectural features: gating and rapid forgetting mechanisms facilitate recall, in-state channel mixing is unnecessary for selectivity, but critical for generalization, and softmax attention remains dominant due to its memory capacity scaling with sequence length. Our benchmark enables targeted, efficient exploration of linear recurrent models and provides a controlled setting for studying behaviors observed in large-scale evaluations. Code is available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 53,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12604",
        "abs_url": "https://arxiv.org/abs/2601.12604",
        "pdf_url": "https://arxiv.org/pdf/2601.12604",
        "title": "Beyond Softmax and Entropy: Improving Convergence Guarantees of Policy Gradients by f-SoftArgmax Parameterization with Coupled Regularization",
        "authors": [
            "Safwan Labbi",
            "Daniil Tiapkin",
            "Paul Mangold",
            "Eric Moulines"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Policy gradient methods are known to be highly sensitive to the choice of policy parameterization. In particular, the widely used softmax parameterization can induce ill-conditioned optimization landscapes and lead to exponentially slow convergence. Although this can be mitigated by preconditioning, this solution is often computationally expensive. Instead, we propose replacing the softmax with an alternative family of policy parameterizations based on the generalized f-softargmax. We further advocate coupling this parameterization with a regularizer induced by the same f-divergence, which improves the optimization landscape and ensures that the resulting regularized objective satisfies a Polyak-Lojasiewicz inequality. Leveraging this structure, we establish the first explicit non-asymptotic last-iterate convergence guarantees for stochastic policy gradient methods for finite MDPs without any form of preconditioning. We also derive sample-complexity bounds for the unregularized problem and show that f-PG, with Tsallis divergences achieves polynomial sample complexity in contrast to the exponential complexity incurred by the standard softmax parameterization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 54,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12612",
        "abs_url": "https://arxiv.org/abs/2601.12612",
        "pdf_url": "https://arxiv.org/pdf/2601.12612",
        "title": "What Trace Powers Reveal About Log-Determinants: Closed-Form Estimators, Certificates, and Failure Modes",
        "authors": [
            "Piyush Sao"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Computing $\\log\\det(A)$ for large symmetric positive definite matrices arises in Gaussian process inference and Bayesian model comparison. Standard methods combine matrix-vector products with polynomial approximations. We study a different model: access to trace powers $p_k = \\tr(A^k)$, natural when matrix powers are available. Classical moment-based approximations Taylor-expand $\\log(\\lambda)$ around the arithmetic mean. This requires $|\\lambda - \\AM| < \\AM$ and diverges when $\\kappa > 4$. We work instead with the moment-generating function $M(t) = \\E[X^t]$ for normalized eigenvalues $X = \\lambda/\\AM$. Since $M'(0) = \\E[\\log X]$, the log-determinant becomes $\\log\\det(A) = n(\\log \\AM + M'(0))$ -- the problem reduces to estimating a derivative at $t = 0$. Trace powers give $M(k)$ at positive integers, but interpolating $M(t)$ directly is ill-conditioned due to exponential growth. The transform $K(t) = \\log M(t)$ compresses this range. Normalization by $\\AM$ ensures $K(0) = K(1) = 0$. With these anchors fixed, we interpolate $K$ through $m+1$ consecutive integers and differentiate to estimate $K'(0)$. However, this local interpolation cannot capture arbitrary spectral features. We prove a fundamental limit: no continuous estimator using finitely many positive moments can be uniformly accurate over unbounded conditioning. Positive moments downweight the spectral tail; $K'(0) = \\E[\\log X]$ is tail-sensitive. This motivates guaranteed bounds. From the same traces we derive upper bounds on $(\\det A)^{1/n}$. Given a spectral floor $r \\leq \\lambda_{\\min}$, we obtain moment-constrained lower bounds, yielding a provable interval for $\\log\\det(A)$. A gap diagnostic indicates when to trust the point estimate and when to report bounds. All estimators and bounds cost $O(m)$, independent of $n$. For $m \\in \\{4, \\ldots, 8\\}$, this is effectively constant time.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 55,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12637",
        "abs_url": "https://arxiv.org/abs/2601.12637",
        "pdf_url": "https://arxiv.org/pdf/2601.12637",
        "title": "Topology-Aware Multiscale Mixture of Experts for Efficient Molecular Property Prediction",
        "authors": [
            "Long D. Nguyen",
            "Kelin Xia",
            "Binh P. Nguyen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Quantitative Methods (q-bio.QM)",
        "abstract": "Many molecular properties depend on 3D geometry, where non-covalent interactions, stereochemical effects, and medium- to long-range forces are determined by spatial distances and angles that cannot be uniquely captured by a 2D bond graph. Yet most 3D molecular graph neural networks still rely on globally fixed neighborhood heuristics, typically defined by distance cutoffs and maximum neighbor limits, to define local message-passing neighborhoods, leading to rigid, data-agnostic interaction budgets. We propose Multiscale Interaction Mixture of Experts (MI-MoE) to adapt interaction modeling across geometric regimes. Our contributions are threefold: (1) we introduce a distance-cutoff expert ensemble that explicitly captures short-, mid-, and long-range interactions without committing to a single cutoff; (2) we design a topological gating encoder that routes inputs to experts using filtration-based descriptors, including persistent homology features, summarizing how connectivity evolves across radii; and (3) we show that MI-MoE is a plug-in module that consistently improves multiple strong 3D molecular backbones across diverse molecular and polymer property prediction benchmark datasets, covering both regression and classification tasks. These results highlight topology-aware multiscale routing as an effective principle for 3D molecular graph learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 56,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12662",
        "abs_url": "https://arxiv.org/abs/2601.12662",
        "pdf_url": "https://arxiv.org/pdf/2601.12662",
        "title": "Decentralized Learning Strategies for Estimation Error Minimization with Graph Neural Networks",
        "authors": [
            "Xingran Chen",
            "Navid NaderiAlizadeh",
            "Alejandro Ribeiro",
            "Shirin Saeedi Bidokhti"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Signal Processing (eess.SP)",
        "abstract": "We address real-time sampling and estimation of autoregressive Markovian sources in dynamic yet structurally similar multi-hop wireless networks. Each node caches samples from others and communicates over wireless collision channels, aiming to minimize time-average estimation error via decentralized policies. Due to the high dimensionality of action spaces and complexity of network topologies, deriving optimal policies analytically is intractable. To address this, we propose a graphical multi-agent reinforcement learning framework for policy optimization. Theoretically, we demonstrate that our proposed policies are transferable, allowing a policy trained on one graph to be effectively applied to structurally similar graphs. Numerical experiments demonstrate that (i) our proposed policy outperforms state-of-the-art baselines; (ii) the trained policies are transferable to larger networks, with performance gains increasing with the number of agents; (iii) the graphical training procedure withstands non-stationarity, even when using independent learning techniques; and (iv) recurrence is pivotal in both independent learning and centralized training and decentralized execution, and improves the resilience to non-stationarity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 57,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12680",
        "abs_url": "https://arxiv.org/abs/2601.12680",
        "pdf_url": "https://arxiv.org/pdf/2601.12680",
        "title": "MetaToolAgent: Towards Generalizable Tool Usage in LLMs through Meta-Learning",
        "authors": [
            "Zheng Fang",
            "Wolfgang Mayer",
            "Zeyu Zhang",
            "Jian Wang",
            "Hong-Yu Zhang",
            "Wanli Li",
            "Zaiwen Feng"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Tool learning is increasingly important for large language models (LLMs) to effectively coordinate and utilize a diverse set of tools in order to solve complex real-world tasks. By selecting and integrating appropriate tools, LLMs extend their capabilities beyond pure language understanding to perform specialized functions. However, existing methods for tool selection often focus on limited tool sets and struggle to generalize to novel tools encountered in practical deployments. To address these challenges, we introduce a comprehensive dataset spanning 7 domains, containing 155 tools and 9,377 question-answer pairs, which simulates realistic integration scenarios. Additionally, we propose MetaToolAgent (MTA), a meta-learning approach designed to improve cross-tool generalization. Experimental results show that MTA significantly outperforms baseline methods on unseen tools, demonstrating its promise for building flexible and scalable systems that require dynamic tool coordination.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 58,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12699",
        "abs_url": "https://arxiv.org/abs/2601.12699",
        "pdf_url": "https://arxiv.org/pdf/2601.12699",
        "title": "Resource-Conscious RL Algorithms for Deep Brain Stimulation",
        "authors": [
            "Arkaprava Gupta",
            "Nicholas Carter",
            "William Zellers",
            "Prateek Ganguli",
            "Benedikt Dietrich",
            "Vibhor Krishna",
            "Parasara Sridhar Duggirala",
            "Samarjit Chakraborty"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Systems and Control (eess.SY)",
        "abstract": "Deep Brain Stimulation (DBS) has proven to be a promising treatment of Parkinson's Disease (PD). DBS involves stimulating specific regions of the brain's Basal Ganglia (BG) using electric impulses to alleviate symptoms of PD such as tremors, rigidity, and bradykinesia. Although most clinical DBS approaches today use a fixed frequency and amplitude, they suffer from side effects (such as slurring of speech) and shortened battery life of the implant. Reinforcement learning (RL) approaches have been used in recent research to perform DBS in a more adaptive manner to improve overall patient outcome. These RL algorithms are, however, too complex to be trained in vivo due to their long convergence time and requirement of high computational resources. We propose a new Time & Threshold-Triggered Multi-Armed Bandit (T3P MAB) RL approach for DBS that is more effective than existing algorithms. Further, our T3P agent is lightweight enough to be deployed in the implant, unlike current deep-RL strategies, and even forgoes the need for an offline training phase. Additionally, most existing RL approaches have focused on modulating only frequency or amplitude, and the possibility of tuning them together remains greatly unexplored in the literature. Our RL agent can tune both frequency and amplitude of DBS signals to the brain with better sample efficiency and requires minimal time to converge. We implement an MAB agent for DBS for the first time on hardware to report energy measurements and prove its suitability for resource-constrained platforms. Our T3P MAB algorithm is deployed on a variety of microcontroller unit (MCU) setups to show its efficiency in terms of power consumption as opposed to other existing RL approaches used in recent work.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 59,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12703",
        "abs_url": "https://arxiv.org/abs/2601.12703",
        "pdf_url": "https://arxiv.org/pdf/2601.12703",
        "title": "Towards Spectroscopy: Susceptibility Clusters in Language Models",
        "authors": [
            "Andrew Gordon",
            "Garrett Baker",
            "George Wang",
            "William Snell",
            "Stan van Wingerden",
            "Daniel Murfet"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Spectroscopy infers the internal structure of physical systems by measuring their response to perturbations. We apply this principle to neural networks: perturbing the data distribution by upweighting a token $y$ in context $x$, we measure the model's response via susceptibilities $\\chi_{xy}$, which are covariances between component-level observables and the perturbation computed over a localized Gibbs posterior via stochastic gradient Langevin dynamics (SGLD). Theoretically, we show that susceptibilities decompose as a sum over modes of the data distribution, explaining why tokens that follow their contexts \"for similar reasons\" cluster together in susceptibility space. Empirically, we apply this methodology to Pythia-14M, developing a conductance-based clustering algorithm that identifies 510 interpretable clusters ranging from grammatical patterns to code structure to mathematical notation. Comparing to sparse autoencoders, 50% of our clusters match SAE features, validating that both methods recover similar structure.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 60,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12704",
        "abs_url": "https://arxiv.org/abs/2601.12704",
        "pdf_url": "https://arxiv.org/pdf/2601.12704",
        "title": "Adaptively trained Physics-informed Radial Basis Function Neural Networks for Solving Multi-asset Option Pricing Problems",
        "authors": [
            "Yan Ma",
            "Yumeng Ren"
        ],
        "comments": "30 pages,16 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The present study investigates the numerical solution of Black-Scholes partial differential equation (PDE) for option valuation with multiple underlying assets. We develop a physics-informed (PI) machine learning algorithm based on a radial basis function neural network (RBFNN) that concurrently optimizes the network architecture and predicts the target option price. The physics-informed radial basis function neural network (PIRBFNN) combines the strengths of the traditional radial basis function collocation method and the physics-informed neural network machine learning approach to effectively solve PDE problems in the financial context. By employing a PDE residual-based technique to adaptively refine the distribution of hidden neurons during the training process, the PIRBFNN facilitates accurate and efficient handling of multidimensional option pricing models featuring non-smooth payoff conditions. The validity of the proposed method is demonstrated through a set of experiments encompassing a single-asset European put option, a double-asset exchange option, and a four-asset basket call option.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 61,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12706",
        "abs_url": "https://arxiv.org/abs/2601.12706",
        "pdf_url": "https://arxiv.org/pdf/2601.12706",
        "title": "Trend-Adjusted Time Series Models with an Application to Gold Price Forecasting",
        "authors": [
            "Sina Kazemdehbashi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Time series data play a critical role in various fields, including finance, healthcare, marketing, and engineering. A wide range of techniques (from classical statistical models to neural network-based approaches such as Long Short-Term Memory (LSTM)) have been employed to address time series forecasting challenges. In this paper, we reframe time series forecasting as a two-part task: (1) predicting the trend (directional movement) of the time series at the next time step, and (2) forecasting the quantitative value at the next time step. The trend can be predicted using a binary classifier, while quantitative values can be forecasted using models such as LSTM and Bidirectional Long Short-Term Memory (Bi-LSTM). Building on this reframing, we propose the Trend-Adjusted Time Series (TATS) model, which adjusts the forecasted values based on the predicted trend provided by the binary classifier. We validate the proposed approach through both theoretical analysis and empirical evaluation. The TATS model is applied to a volatile financial time series (the daily gold price) with the objective of forecasting the next days price. Experimental results demonstrate that TATS consistently outperforms standard LSTM and Bi-LSTM models by achieving significantly lower forecasting error. In addition, our results indicate that commonly used metrics such as MSE and MAE are insufficient for fully assessing time series model performance. Therefore, we also incorporate trend detection accuracy, which measures how effectively a model captures trends in a time series.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 62,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12707",
        "abs_url": "https://arxiv.org/abs/2601.12707",
        "pdf_url": "https://arxiv.org/pdf/2601.12707",
        "title": "Decoding Rewards in Competitive Games: Inverse Game Theory with Entropy Regularization",
        "authors": [
            "Junyi Liao",
            "Zihan Zhu",
            "Ethan Fang",
            "Zhuoran Yang",
            "Vahid Tarokh"
        ],
        "comments": "Extended journal version of ICML 2025 paper. Submitted to Operations Research",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Estimating the unknown reward functions driving agents' behaviors is of central interest in inverse reinforcement learning and game theory. To tackle this problem, we develop a unified framework for reward function recovery in two-player zero-sum matrix games and Markov games with entropy regularization, where we aim to reconstruct the underlying reward functions given observed players' strategies and actions. This task is challenging due to the inherent ambiguity of inverse problems, the non-uniqueness of feasible rewards, and limited observational data coverage. To address these challenges, we establish the reward function's identifiability using the quantal response equilibrium (QRE) under linear assumptions. Building upon this theoretical foundation, we propose a novel algorithm to learn reward functions from observed actions. Our algorithm works in both static and dynamic settings and is adaptable to incorporate different methods, such as Maximum Likelihood Estimation (MLE). We provide strong theoretical guarantees for the reliability and sample efficiency of our algorithm. Further, we conduct extensive numerical studies to demonstrate the practical effectiveness of the proposed framework, offering new insights into decision-making in competitive environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 63,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12730",
        "abs_url": "https://arxiv.org/abs/2601.12730",
        "pdf_url": "https://arxiv.org/pdf/2601.12730",
        "title": "Distribution-Centric Policy Optimization Dominates Exploration-Exploitation Trade-off",
        "authors": [
            "Zhaochun Li",
            "Chen Wang",
            "Jionghao Bai",
            "Shisheng Cui",
            "Ge Lan",
            "Zhou Zhao",
            "Yue Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The exploration-exploitation (EE) trade-off is a central challenge in reinforcement learning (RL) for large language models (LLMs). With Group Relative Policy Optimization (GRPO), training tends to be exploitation driven: entropy decreases monotonically, samples convergence, and exploration fades. Most existing fixes are \\textbf{sample-centric}: they seek or bonus rare samples, assuming exploration comes from novel trajectories and tokens. These heuristics depend on the \"luck\" of informative samples, lack principled control of the policy, and often yield limited or inconsistent gains. In this work, we are the first to introduce a \\textbf{distribution-centric} perspective for RL, in which exploration is always guided by a \"better\" target distribution, and reveal that a policy's ability to resist entropy collapse is governed by the distribution itself rather than individual samples. Building on this insight, we propose Distribution-Centric Policy Optimization (DCPO), which reformulates entropy regulation as distribution-level regularization. DCPO achieves controllable entropy fully on-policy without sampling from external distributions, enabling efficient exploration while maintaining training stability. Across multiple models and seven benchmarks, DCPO improves over GRPO by about 20\\% on average. Overall, DCPO replaces sample-level heuristics with distribution-level principles, offering a theoretically grounded and flexible framework for controllable exploration and a stronger EE trade-off. The code is available in this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 64,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12751",
        "abs_url": "https://arxiv.org/abs/2601.12751",
        "pdf_url": "https://arxiv.org/pdf/2601.12751",
        "title": "A Boolean Function-Theoretic Framework for Expressivity in GNNs with Applications to Fair Graph Mining",
        "authors": [
            "Manjish Pal"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We propose a novel expressivity framework for Graph Neural Networks (GNNs) grounded in Boolean function theory, enabling a fine-grained analysis of their ability to capture complex subpopulation structures. We introduce the notion of \\textit{Subpopulation Boolean Isomorphism} (SBI) as an invariant that strictly subsumes existing expressivity measures such as Weisfeiler-Lehman (WL), biconnectivity-based, and homomorphism-based frameworks. Our theoretical results identify Fourier degree, circuit class (AC$^0$, NC$^1$), and influence as key barriers to expressivity in fairness-aware GNNs. We design a circuit-traversal-based fairness algorithm capable of handling subpopulations defined by high-complexity Boolean functions, such as parity, which break existing baselines. Experiments on real-world graphs show that our method achieves low fairness gaps across intersectional groups where state-of-the-art methods fail, providing the first principled treatment of GNN expressivity tailored to fairness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 65,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12775",
        "abs_url": "https://arxiv.org/abs/2601.12775",
        "pdf_url": "https://arxiv.org/pdf/2601.12775",
        "title": "Eddy-Resolving Global Ocean Forecasting with Multi-Scale Graph Neural Networks",
        "authors": [
            "Yuta Hirabayashi",
            "Daisuke Matusoka",
            "Konobu Kimura"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Research on data-driven ocean models has progressed rapidly in recent years; however, the application of these models to global eddy-resolving ocean forecasting remains limited. The accurate representation of ocean dynamics across a wide range of spatial scales remains a major challenge in such applications. This study proposes a multi-scale graph neural network-based ocean model for 10-day global forecasting that improves short-term prediction skill and enhances the representation of multi-scale ocean variability. The model employs an encoder-processor-decoder architecture and uses two spherical meshes with different resolutions to better capture the multi-scale nature of ocean dynamics. In addition, the model incorporates surface atmospheric variables along with ocean state variables as node inputs to improve short-term prediction accuracy by representing atmospheric forcing. Evaluation using surface kinetic energy spectra and case studies shows that the model accurately represents a broad range of spatial scales, while root mean square error comparisons demonstrate improved skill in short-term predictions. These results indicate that the proposed model delivers more accurate short-term forecasts and improved representation of multi-scale ocean dynamics, thereby highlighting its potential to advance data-driven, eddy-resolving global ocean forecasting.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 66,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12807",
        "abs_url": "https://arxiv.org/abs/2601.12807",
        "pdf_url": "https://arxiv.org/pdf/2601.12807",
        "title": "Semi-supervised Instruction Tuning for Large Language Models on Text-Attributed Graphs",
        "authors": [
            "Zixing Song",
            "Irwin King"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The emergent reasoning capabilities of Large Language Models (LLMs) offer a transformative paradigm for analyzing text-attributed graphs. While instruction tuning is the prevailing method for adapting pre-trained LLMs to graph learning tasks like node classification, it requires a substantial volume of annotated (INSTRUCTION, OUTPUT) pairs deriving from labeled nodes. This requirement is particularly prohibitive in the social domain, where obtaining expert labels for sensitive or evolving content is costly and slow. Furthermore, standard graph instruction tuning fails to exploit the vast amount of unlabeled nodes, which contain latent correlations due to edge connections that are beneficial for downstream predictions. To bridge this gap, we propose a novel Semi-supervised Instruction Tuning pipeline for Graph Learning, named SIT-Graph. Notably, SIT-Graph is model-agnostic and can be seamlessly integrated into any graph instruction tuning method that utilizes LLMs as the predictor. SIT-Graph operates via an iterative self-training process. Initially, the model is fine-tuned using instruction pairs constructed solely from the labeled nodes. Then it generates confidence-filtered pseudo-responses for unlabeled nodes to strategically augment the dataset for the next round of fine-tuning. Finally, this iterative refinement progressively aligns the LLM with the underlying node correlations. Extensive experiments demonstrate that when incorporated into state-of-the-art graph instruction tuning methods, SIT-Graph significantly enhances their performance on text-attributed graph benchmarks, achieving over 20% improvement under the low label ratio settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 67,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12816",
        "abs_url": "https://arxiv.org/abs/2601.12816",
        "pdf_url": "https://arxiv.org/pdf/2601.12816",
        "title": "Fisher-Orthogonal Projected Natural Gradient Descent for Continual Learning",
        "authors": [
            "Ishir Garg",
            "Neel Kolhe",
            "Andy Peng",
            "Rohan Gopalam"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Continual learning aims to enable neural networks to acquire new knowledge on sequential tasks. However, the key challenge in such settings is to learn new tasks without catastrophically forgetting previously learned tasks. We propose the Fisher-Orthogonal Projected Natural Gradient Descent (FOPNG) optimizer, which enforces Fisher-orthogonal constraints on parameter updates to preserve old task performance while learning new tasks. Unlike existing methods that operate in Euclidean parameter space, FOPNG projects gradients onto the Fisher-orthogonal complement of previous task gradients. This approach unifies natural gradient descent with orthogonal gradient methods within an information-geometric framework. The resulting update direction is invariant under reparameterization, guarantees descent in the Fisher metric, and helps preserve prior task outputs. We provide theoretical analysis establishing the properties of the projected update, describe efficient and practical implementations using the diagonal Fisher, and demonstrate strong results on standard continual learning benchmarks such as Permuted-MNIST, Split-MNIST, Rotated-MNIST, Split-CIFAR10, and Split-CIFAR100.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 68,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12839",
        "abs_url": "https://arxiv.org/abs/2601.12839",
        "pdf_url": "https://arxiv.org/pdf/2601.12839",
        "title": "Knowledge-Integrated Representation Learning for Crypto Anomaly Detection under Extreme Label Scarcity; Relational Domain-Logic Integration with Retrieval-Grounded Context and Path-Level Explanations",
        "authors": [
            "Gyuyeon Na",
            "Minjung Park",
            "Soyoun Kim",
            "Jungbin Shin",
            "Sangmi Chai"
        ],
        "comments": "Gyuyeon Na, Minjung Park, Soyoun Kim contributed equally to this work",
        "subjects": "Machine Learning (cs.LG); Risk Management (q-fin.RM)",
        "abstract": "Detecting anomalous trajectories in decentralized crypto networks is fundamentally challenged by extreme label scarcity and the adaptive evasion strategies of illicit actors. While Graph Neural Networks (GNNs) effectively capture local structural patterns, they struggle to internalize multi hop, logic driven motifs such as fund dispersal and layering that characterize sophisticated money laundering, limiting their forensic accountability under regulations like the FATF Travel Rule. To address this limitation, we propose Relational Domain Logic Integration (RDLI), a framework that embeds expert derived heuristics as differentiable, logic aware latent signals within representation learning. Unlike static rule based approaches, RDLI enables the detection of complex transactional flows that evade standard message passing. To further account for market volatility, we incorporate a Retrieval Grounded Context (RGC) module that conditions anomaly scoring on regulatory and macroeconomic context, mitigating false positives caused by benign regime shifts. Under extreme label scarcity (0.01%), RDLI outperforms state of the art GNN baselines by 28.9% in F1 score. A micro expert user study further confirms that RDLI path level explanations significantly improve trustworthiness, perceived usefulness, and clarity compared to existing methods, highlighting the importance of integrating domain logic with contextual grounding for both accuracy and explainability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 69,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12859",
        "abs_url": "https://arxiv.org/abs/2601.12859",
        "pdf_url": "https://arxiv.org/pdf/2601.12859",
        "title": "Generating Cyclic Conformers with Flow Matching in Cremer-Pople Coordinates",
        "authors": [
            "Luca Schaufelberger",
            "Aline Hartgers",
            "Kjell Jorner"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Chemical Physics (physics.chem-ph)",
        "abstract": "Cyclic molecules are ubiquitous across applications in chemistry and biology. Their restricted conformational flexibility provides structural pre-organization that is key to their function in drug discovery and catalysis. However, reliably sampling the conformer ensembles of ring systems remains challenging. Here, we introduce PuckerFlow, a generative machine learning model that performs flow matching on the Cremer-Pople space, a low-dimensional internal coordinate system capturing the relevant degrees of freedom of rings. Our approach enables generation of valid closed rings by design and demonstrates strong performance in generating conformers that are both diverse and precise. We show that PuckerFlow outperforms other conformer generation methods on nearly all quantitative metrics and illustrate the potential of PuckerFlow for ring systems relevant to chemical applications, particularly in catalysis and drug discovery. This work enables efficient and reliable conformer generation of cyclic structures, paving the way towards modeling structure-property relationships and the property-guided generation of rings across a wide range of applications in chemistry and biology.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 70,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12893",
        "abs_url": "https://arxiv.org/abs/2601.12893",
        "pdf_url": "https://arxiv.org/pdf/2601.12893",
        "title": "AdaNODEs: Test Time Adaptation for Time Series Forecasting Using Neural ODEs",
        "authors": [
            "Ting Dang",
            "Soumyajit Chatterjee",
            "Hong Jia",
            "Yu Wu",
            "Flora Salim",
            "Fahim Kawsar"
        ],
        "comments": "Accepted by ICASSP 2026",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Test time adaptation (TTA) has emerged as a promising solution to adapt pre-trained models to new, unseen data distributions using unlabeled target domain data. However, most TTA methods are designed for independent data, often overlooking the time series data and rarely addressing forecasting tasks. This paper presents AdaNODEs, an innovative source-free TTA method tailored explicitly for time series forecasting. By leveraging Neural Ordinary Differential Equations (NODEs), we propose a novel adaptation framework that accommodates the unique characteristics of distribution shifts in time series data. Moreover, we innovatively propose a new loss function to tackle TTA for forecasting tasks. AdaNODEs only requires updating limited model parameters, showing effectiveness in capturing temporal dependencies while avoiding significant memory usage. Extensive experiments with one- and high-dimensional data demonstrate that AdaNODEs offer relative improvements of 5.88\\% and 28.4\\% over the SOTA baselines, especially demonstrating robustness across higher severity distribution shifts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 71,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12900",
        "abs_url": "https://arxiv.org/abs/2601.12900",
        "pdf_url": "https://arxiv.org/pdf/2601.12900",
        "title": "Supervised Learning for the (s,S) Inventory Model with General Interarrival Demands and General Lead Times",
        "authors": [
            "Eliran Sherzer",
            "Yonit Barron"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The continuous-review (s,S) inventory model is a cornerstone of stochastic inventory theory, yet its analysis becomes analytically intractable when dealing with non-Markovian systems. In such systems, evaluating long-run performance measures typically relies on costly simulation. This paper proposes a supervised learning framework via a neural network model for approximating stationary performance measures of (s,S) inventory systems with general distributions for the interarrival time between demands and lead times under lost sales. Simulations are first used to generate training labels, after which the neural network is trained. After training, the neural network provides almost instantaneous predictions of various metrics of the system, such as the stationary distribution of inventory levels, the expected cycle time, and the probability of lost sales. We find that using a small number of low-order moments of the distributions as input is sufficient to train the neural networks and to accurately capture the steady-state distribution. Extensive numerical experiments demonstrate high accuracy over a wide range of system parameters. As such, it effectively replaces repeated and costly simulation runs. Our framework is easily extendable to other inventory models, offering an efficient and fast alternative for analyzing complex stochastic systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 72,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12903",
        "abs_url": "https://arxiv.org/abs/2601.12903",
        "pdf_url": "https://arxiv.org/pdf/2601.12903",
        "title": "Deep Temporal Graph Clustering: A Comprehensive Benchmark and Datasets",
        "authors": [
            "Meng Liu",
            "Ke Liang",
            "Siwei Wang",
            "Xingchen Hu",
            "Sihang Zhou",
            "Xinwang Liu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Temporal Graph Clustering (TGC) is a new task with little attention, focusing on node clustering in temporal graphs. Compared with existing static graph clustering, it can find the balance between time requirement and space requirement (Time-Space Balance) through the interaction sequence-based batch-processing pattern. However, there are two major challenges that hinder the development of TGC, i.e., inapplicable clustering techniques and inapplicable datasets. To address these challenges, we propose a comprehensive benchmark, called BenchTGC. Specially, we design a BenchTGC Framework to illustrate the paradigm of temporal graph clustering and improve existing clustering techniques to fit temporal graphs. In addition, we also discuss problems with public temporal graph datasets and develop multiple datasets suitable for TGC task, called BenchTGC Datasets. According to extensive experiments, we not only verify the advantages of BenchTGC, but also demonstrate the necessity and importance of TGC task. We wish to point out that the dynamically changing and complex scenarios in real world are the foundation of temporal graph clustering. The code and data is available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 73,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12917",
        "abs_url": "https://arxiv.org/abs/2601.12917",
        "pdf_url": "https://arxiv.org/pdf/2601.12917",
        "title": "CooperLLM: Cloud-Edge-End Cooperative Federated Fine-tuning for LLMs via ZOO-based Gradient Correction",
        "authors": [
            "He Sun",
            "Jinrui Zhou",
            "Li Li",
            "Mingjun Xiao"
        ],
        "comments": "14 pages, 9 figures, under review",
        "subjects": "Machine Learning (cs.LG); Distributed, Parallel, and Cluster Computing (cs.DC)",
        "abstract": "Large Language Models (LLMs) perform well on many NLP tasks, but fine-tuning them on resource-constrained mobile devices is challenging due to high memory and computation costs, despite growing demands for privacy-preserving personalization. Federated Learning (FL) enables local-data training, yet existing methods either rely on memory-intensive backpropagation or use zeroth-order optimization (ZOO), which avoids backward passes but suffers from slow convergence and degraded accuracy. We propose CooperLLM, a cloud-assisted edge-end cooperative federated fine-tuning framework that combines ZOO on mobile devices with cloud-guided gradient rectification. Mobile clients perform lightweight ZOO updates on private data, while the cloud fine-tunes on auxiliary public data using backpropagation and injects guided perturbations to rectify local updates, improving convergence and accuracy without violating privacy. To address system bottlenecks, CooperLLM introduces pipeline scheduling and adaptive compression to overlap computation and communication and reduce memory usage. Experiments on multiple Transformer models and datasets show that CooperLLM reduces on-device memory by up to $86.4\\%$, accelerates convergence by $8.8 \\times$, and improves accuracy by up to 10 percentage points over state-of-the-art ZOO-based baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 74,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12928",
        "abs_url": "https://arxiv.org/abs/2601.12928",
        "pdf_url": "https://arxiv.org/pdf/2601.12928",
        "title": "An efficient heuristic for geometric analysis of cell deformations",
        "authors": [
            "Yaima Paz Soto",
            "Silena Herold Garcia",
            "Ximo Gual-Arnau",
            "Antoni Jaume-i-Capó",
            "Manuel González-Hidalgo"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)",
        "abstract": "Sickle cell disease causes erythrocytes to become sickle-shaped, affecting their movement in the bloodstream and reducing oxygen delivery. It has a high global prevalence and places a significant burden on healthcare systems, especially in resource-limited regions. Automated classification of sickle cells in blood images is crucial, allowing the specialist to reduce the effort required and avoid errors when quantifying the deformed cells and assessing the severity of a crisis. Recent studies have proposed various erythrocyte representation and classification methods. Since classification depends solely on cell shape, a suitable approach models erythrocytes as closed planar curves in shape space. This approach employs elastic distances between shapes, which are invariant under rotations, translations, scaling, and reparameterizations, ensuring consistent distance measurements regardless of the curves' position, starting point, or traversal speed. While previous methods exploiting shape space distances had achieved high accuracy, we refined the model by considering the geometric characteristics of healthy and sickled erythrocytes. Our method proposes (1) to employ a fixed parameterization based on the major axis of each cell to compute distances and (2) to align each cell with two templates using this parameterization before computing distances. Aligning shapes to templates before distance computation, a concept successfully applied in areas such as molecular dynamics, and using a fixed parameterization, instead of minimizing distances across all possible parameterizations, simplifies calculations. This strategy achieves 96.03\\% accuracy rate in both supervised classification and unsupervised clustering. Our method ensures efficient erythrocyte classification, maintaining or improving accuracy over shape space models while significantly reducing computational costs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 75,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12931",
        "abs_url": "https://arxiv.org/abs/2601.12931",
        "pdf_url": "https://arxiv.org/pdf/2601.12931",
        "title": "Online Continual Learning for Time Series: a Natural Score-driven Approach",
        "authors": [
            "Edoardo Urettini",
            "Daniele Atzeni",
            "Ioanna-Yvonni Tsaknaki",
            "Antonio Carta"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "Online continual learning (OCL) methods adapt to changing environments without forgetting past knowledge. Similarly, online time series forecasting (OTSF) is a real-world problem where data evolve in time and success depends on both rapid adaptation and long-term memory. Indeed, time-varying and regime-switching forecasting models have been extensively studied, offering a strong justification for the use of OCL in these settings. Building on recent work that applies OCL to OTSF, this paper aims to strengthen the theoretical and practical connections between time series methods and OCL. First, we reframe neural network optimization as a parameter filtering problem, showing that natural gradient descent is a score-driven method and proving its information-theoretic optimality. Then, we show that using a Student's t likelihood in addition to natural gradient induces a bounded update, which improves robustness to outliers. Finally, we introduce Natural Score-driven Replay (NatSR), which combines our robust optimizer with a replay buffer and a dynamic scale heuristic that improves fast adaptation at regime drifts. Empirical results demonstrate that NatSR achieves stronger forecasting performance than more complex state-of-the-art methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 76,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12965",
        "abs_url": "https://arxiv.org/abs/2601.12965",
        "pdf_url": "https://arxiv.org/pdf/2601.12965",
        "title": "Deterministic Dynamics of Sampling Processes in Score-Based Diffusion Models with Multiplicative Noise Conditioning",
        "authors": [
            "Doheon Kim"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Score-based diffusion models generate new samples by learning the score function associated with a diffusion process. While the effectiveness of these models can be theoretically explained using differential equations related to the sampling process, previous work by Song and Ermon (2020) demonstrated that neural networks using multiplicative noise conditioning can still generate satisfactory samples. In this setup, the model is expressed as the product of two functions: one depending on the spatial variable and the other on the noise magnitude. This structure limits the model's ability to represent a more general relationship between the spatial variable and the noise, indicating that it cannot fully learn the correct score. Despite this limitation, the models perform well in practice. In this work, we provide a theoretical explanation for this phenomenon by studying the deterministic dynamics of the associated differential equations, offering insight into how the model operates.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 77,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12971",
        "abs_url": "https://arxiv.org/abs/2601.12971",
        "pdf_url": "https://arxiv.org/pdf/2601.12971",
        "title": "Architecture-Optimization Co-Design for Physics-Informed Neural Networks Via Attentive Representations and Conflict-Resolved Gradients",
        "authors": [
            "Pancheng Niu",
            "Jun Guo",
            "Qiaolin He",
            "Yongming Chen",
            "Yanchao Shi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Physics-Informed Neural Networks (PINNs) provide a learning-based framework for solving partial differential equations (PDEs) by embedding governing physical laws into neural network training. In practice, however, their performance is often hindered by limited representational capacity and optimization difficulties caused by competing physical constraints and conflicting gradients. In this work, we study PINN training from a unified architecture-optimization perspective. We first propose a layer-wise dynamic attention mechanism to enhance representational flexibility, resulting in the Layer-wise Dynamic Attention PINN (LDA-PINN). We then reformulate PINN training as a multi-task learning problem and introduce a conflict-resolved gradient update strategy to alleviate gradient interference, leading to the Gradient-Conflict-Resolved PINN (GC-PINN). By integrating these two components, we develop the Architecture-Conflict-Resolved PINN (ACR-PINN), which combines attentive representations with conflict-aware optimization while preserving the standard PINN loss formulation. Extensive experiments on benchmark PDEs, including the Burgers, Helmholtz, Klein-Gordon, and lid-driven cavity flow problems, demonstrate that ACR-PINN achieves faster convergence and significantly lower relative $L_2$ and $L_\\infty$ errors than standard PINNs. These results highlight the effectiveness of architecture-optimization co-design for improving the robustness and accuracy of PINN-based solvers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 78,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12988",
        "abs_url": "https://arxiv.org/abs/2601.12988",
        "pdf_url": "https://arxiv.org/pdf/2601.12988",
        "title": "PaperGuide: Making Small Language-Model Paper-Reading Agents More Efficient",
        "authors": [
            "Zijian Wang",
            "Tiancheng Huang",
            "Hanqi Li",
            "Da Ma",
            "Lu Chen",
            "Kai Yu"
        ],
        "comments": "35 pages, 9 figures, 7 tables",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The accelerating growth of the scientific literature makes it increasingly difficult for researchers to track new advances through manual reading alone. Recent progress in large language models (LLMs) has therefore spurred interest in autonomous agents that can read scientific papers and extract task-relevant information. However, most existing approaches rely either on heavily engineered prompting or on a conventional SFT-RL training pipeline, both of which often lead to excessive and low-yield exploration. Drawing inspiration from cognitive science, we propose PaperCompass, a framework that mitigates these issues by separating high-level planning from fine-grained execution. PaperCompass first drafts an explicit plan that outlines the intended sequence of actions, and then performs detailed reasoning to instantiate each step by selecting the parameters for the corresponding function calls. To train such behavior, we introduce Draft-and-Follow Policy Optimization (DFPO), a tailored RL method that jointly optimizes both the draft plan and the final solution. DFPO can be viewed as a lightweight form of hierarchical reinforcement learning, aimed at narrowing the `knowing-doing' gap in LLMs. We provide a theoretical analysis that establishes DFPO's favorable optimization properties, supporting a stable and reliable training process. Experiments on paper-based question answering (Paper-QA) benchmarks show that PaperCompass improves efficiency over strong baselines without sacrificing performance, achieving results comparable to much larger models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 79,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13020",
        "abs_url": "https://arxiv.org/abs/2601.13020",
        "pdf_url": "https://arxiv.org/pdf/2601.13020",
        "title": "PASs-MoE: Mitigating Misaligned Co-drift among Router and Experts via Pathway Activation Subspaces for Continual Learning",
        "authors": [
            "Zhiyan Hou",
            "Haiyun Guo",
            "Haokai Ma",
            "Yandu Sun",
            "Yonghui Yang",
            "Jinqiao Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Continual instruction tuning (CIT) requires multimodal large language models (MLLMs) to adapt to a stream of tasks without forgetting prior capabilities. A common strategy is to isolate updates by routing inputs to different LoRA experts. However, existing LoRA-based Mixture-of-Experts (MoE) methods often jointly update the router and experts in an indiscriminate way, causing the router's preferences to co-drift with experts' adaptation pathways and gradually deviate from early-stage input-expert specialization. We term this phenomenon Misaligned Co-drift, which blurs expert responsibilities and exacerbates this http URL address this, we introduce the pathway activation subspace (PASs), a LoRA-induced subspace that reflects which low-rank pathway directions an input activates in each expert, providing a capability-aligned coordinate system for routing and preservation. Based on PASs, we propose a fixed-capacity PASs-based MoE-LoRA method with two components: PAS-guided Reweighting, which calibrates routing using each expert's pathway activation signals, and PAS-aware Rank Stabilization, which selectively stabilizes rank directions important to previous tasks. Experiments on a CIT benchmark show that our approach consistently outperforms a range of conventional continual learning baselines and MoE-LoRA variants in both accuracy and anti-forgetting without adding parameters. Our code will be released upon acceptance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 80,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13021",
        "abs_url": "https://arxiv.org/abs/2601.13021",
        "pdf_url": "https://arxiv.org/pdf/2601.13021",
        "title": "Enhancing Generalization in Sickle Cell Disease Diagnosis through Ensemble Methods and Feature Importance Analysis",
        "authors": [
            "Nataša Petrović",
            "Gabriel Moyà-Alcover",
            "Antoni Jaume-i-Capó",
            "Jose Maria Buades Rubio"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "This work presents a novel approach for selecting the optimal ensemble-based classification method and features with a primarly focus on achieving generalization, based on the state-of-the-art, to provide diagnostic support for Sickle Cell Disease using peripheral blood smear images of red blood cells. We pre-processed and segmented the microscopic images to ensure the extraction of high-quality features. To ensure the reliability of our proposed system, we conducted an in-depth analysis of interpretability. Leveraging techniques established in the literature, we extracted features from blood cells and employed ensemble machine learning methods to classify their morphology. Furthermore, we have devised a methodology to identify the most critical features for classification, aimed at reducing complexity and training time and enhancing interpretability in opaque models. Lastly, we validated our results using a new dataset, where our model overperformed state-of-the-art models in terms of generalization. The results of classifier ensembled of Random Forest and Extra Trees classifier achieved an harmonic mean of precision and recall (F1-score) of 90.71\\% and a Sickle Cell Disease diagnosis support score (SDS-score) of 93.33\\%. These results demonstrate notable enhancement from previous ones with Gradient Boosting classifier (F1-score 87.32\\% and SDS-score 89.51\\%). To foster scientific progress, we have made available the parameters for each model, the implemented code library, and the confusion matrices with the raw data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 81,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13054",
        "abs_url": "https://arxiv.org/abs/2601.13054",
        "pdf_url": "https://arxiv.org/pdf/2601.13054",
        "title": "TinyML-Enabled IoT for Sustainable Precision Irrigation",
        "authors": [
            "Kamogelo Taueatsoala",
            "Caitlyn Daniels",
            "Angelina J. Ramsunar",
            "Petrus Bronkhorst",
            "Absalom E. Ezugwu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Small-scale farming communities are disproportionately affected by water scarcity, erratic climate patterns, and a lack of access to advanced, affordable agricultural technologies. To address these challenges, this paper presents a novel, edge-first IoT framework that integrates Tiny Machine Learning (TinyML) for intelligent, offline-capable precision irrigation. The proposed four-layer architecture leverages low-cost hardware, an ESP32 microcontroller as an edge inference node, and a Raspberry Pi as a local edge server to enable autonomous decision-making without cloud dependency. The system utilizes capacitive soil moisture, temperature, humidity, pH, and ambient light sensors for environmental monitoring. A rigorous comparative analysis of ensemble models identified gradient boosting as superior, achieving an R^2 score of 0.9973 and a Mean Absolute Percentage Error (MAPE) of 0.99%, outperforming a random forest model (R^2 = 0.9916, MAPE = 1.81%). This optimized model was converted and deployed as a lightweight TinyML inference engine on the ESP32 and predicts irrigation needs with exceptional accuracy (MAPE < 1%). Local communication is facilitated by an MQTT-based LAN protocol, ensuring reliable operation in areas with limited or no internet connectivity. Experimental validation in a controlled environment demonstrated a significant reduction in water usage compared to traditional methods, while the system's low-power design and offline functionality confirm its viability for sustainable, scalable deployment in resource-constrained rural settings. This work provides a practical, cost-effective blueprint for bridging the technological divide in agriculture and enhancing water-use efficiency through on-device artificial intelligence.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 82,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13100",
        "abs_url": "https://arxiv.org/abs/2601.13100",
        "pdf_url": "https://arxiv.org/pdf/2601.13100",
        "title": "Recursive Meta-Distillation: An Axiomatic Framework for Iterative Knowledge Refinement",
        "authors": [
            "Aaron R. Flouro",
            "Shawn P. Chadwick"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Recent work in probability-domain knowledge distillation has established axiomatic frameworks for temperature scaling, multi-teacher aggregation, and bias-variance trade-offs in single-stage settings. However, the mathematical behavior of recursive or multi-generation distillation remains poorly understood, with prior approaches relying primarily on empirical heuristics. In this work, we introduce an axiomatic and operator-theoretic framework for recursive meta-distillation, formalizing iterative knowledge distillation as a sequence of probability-distribution operators with explicit anchoring to base teachers. We define structural axioms for valid meta-teacher construction and prove the existence of non-trivial operator families satisfying these axioms without specifying particular algorithms or loss functions. Under mild realizability and convexity assumptions, we show that anchored recursive distillation induces contraction in KL divergence, yielding geometric convergence to base teacher distributions and a unique, globally attractive fixed point. The contribution is foundational rather than algorithmic: the framework characterizes when recursive distillation is mathematically well-posed and convergent rather than error-accumulating, independent of model architecture, optimization details, or specific operator instantiations. These results provide a theoretical basis for understanding stability, bias-variance behavior, and failure modes in iterative and multi-teacher distillation under capacity constraints.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 83,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13143",
        "abs_url": "https://arxiv.org/abs/2601.13143",
        "pdf_url": "https://arxiv.org/pdf/2601.13143",
        "title": "FastAV: Efficient Token Pruning for Audio-Visual Large Language Model Inference",
        "authors": [
            "Chaeyoung Jung",
            "Youngjoon Jang",
            "Seungwoo Lee",
            "Joon Son Chung"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In this work, we present FastAV, the first token pruning framework tailored for audio-visual large language models (AV-LLMs). While token pruning has been actively explored in standard large language models (LLMs) and vision-language models (LVLMs), its application to AV-LLMs has received little attention, even though multimodal integration substantially increases their token demands. To address this gap, we introduce a pruning strategy that utilizes attention weights to identify tokens emphasized at different stages and estimates their importance. Building on this analysis, FastAV applies a two-stage pruning strategy: (1) global pruning in intermediate layers to remove broadly less influential tokens, and (2) fine pruning in later layers considering the impact on next token generation. Notably, our method does not rely on full attention maps, which makes it fully compatible with efficient attention mechanisms such as FlashAttention. Extensive experiments demonstrate that FastAV reduces FLOPs by more than 40% on two representative AV-LLMs, while preserving or even improving model performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 84,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13160",
        "abs_url": "https://arxiv.org/abs/2601.13160",
        "pdf_url": "https://arxiv.org/pdf/2601.13160",
        "title": "Training instability in deep learning follows low-dimensional dynamical principles",
        "authors": [
            "Zhipeng Zhang",
            "Zhenjie Yao",
            "Kai Li",
            "Lei Yang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Deep learning systems achieve remarkable empirical performance, yet the stability of the training process itself remains poorly understood. Training unfolds as a high-dimensional dynamical system in which small perturbations to optimization, data, parameters, or learning signals can induce abrupt and irreversible collapse, undermining reproducibility and scalability. We propose a unified dynamical perspective that characterizes training stability as an intrinsic property of learning systems, organized along four interacting dimensions: optimization, environmental/data, parametric, and learning-signal stability. We operationalize this perspective through controlled perturbation auditing of training trajectories, probing how learning dynamics respond to structured disturbances without modifying learning algorithms. Across reinforcement learning and large language model training, we identify three recurring regularities: high final performance is frequently decoupled from training stability; controlled stochasticity consistently buffers learning dynamics across paradigms; and deviations in low-dimensional latent meta-states systematically precede observable performance collapse. Together, these findings establish training stability as a measurable and comparable dynamical property of learning systems, providing a descriptive foundation for studying learning dynamics beyond final performance outcomes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 85,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13162",
        "abs_url": "https://arxiv.org/abs/2601.13162",
        "pdf_url": "https://arxiv.org/pdf/2601.13162",
        "title": "NeuroShield: A Neuro-Symbolic Framework for Adversarial Robustness",
        "authors": [
            "Ali Shafiee Sarvestani",
            "Jason Schmidt",
            "Arman Roohi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Emerging Technologies (cs.ET)",
        "abstract": "Adversarial vulnerability and lack of interpretability are critical limitations of deep neural networks, especially in safety-sensitive settings such as autonomous driving. We introduce \\DesignII, a neuro-symbolic framework that integrates symbolic rule supervision into neural networks to enhance both adversarial robustness and explainability. Domain knowledge is encoded as logical constraints over appearance attributes such as shape and color, and enforced through semantic and symbolic logic losses applied during training. Using the GTSRB dataset, we evaluate robustness against FGSM and PGD attacks at a standard $\\ell_\\infty$ perturbation budget of $\\varepsilon = 8/255$. Relative to clean training, standard adversarial training provides modest improvements in robustness ($\\sim$10 percentage points). Conversely, our FGSM-Neuro-Symbolic and PGD-Neuro-Symbolic models achieve substantially larger gains, improving adversarial accuracy by 18.1\\% and 17.35\\% over their corresponding adversarial-training baselines, representing roughly a three-fold larger robustness gain than standard adversarial training provides when both are measured relative to the same clean-training baseline, without reducing clean-sample accuracy. Compared to transformer-based defenses such as LNL-MoEx, which require heavy architectures and extensive data augmentation, our PGD-Neuro-Symbolic variant attains comparable or superior robustness using a ResNet18 backbone trained for 10 epochs. These results show that symbolic reasoning offers an effective path to robust and interpretable AI.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 86,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13190",
        "abs_url": "https://arxiv.org/abs/2601.13190",
        "pdf_url": "https://arxiv.org/pdf/2601.13190",
        "title": "LAViG-FLOW: Latent Autoregressive Video Generation for Fluid Flow Simulations",
        "authors": [
            "Vittoria De Pellegrini",
            "Tariq Alkhalifah"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Fluid Dynamics (physics.flu-dyn)",
        "abstract": "Modeling and forecasting subsurface multiphase fluid flow fields underpin applications ranging from geological CO2 sequestration (GCS) operations to geothermal production. This is essential for ensuring both operational performance and long-term safety. While high fidelity multiphase simulators are widely used for this purpose, they become prohibitively expensive once many forward runs are required for inversion purposes and quantify uncertainty. To tackle this challenge we propose LAViG-FLOW, a latent autoregressive video generation diffusion framework that explicitly learns the coupled evolution of saturation and pressure fields. Each state variable is compressed by a dedicated 2D autoencoder, and a Video Diffusion Transformer (VDiT) models their coupled distribution across time. We first train the model on a given time horizon to learn their coupled relationship and then fine-tune it autoregressively so it can extrapolate beyond the observed time window. Evaluated on an open-source CO2 sequestration dataset, LAViG-FLOW generates saturation and pressure fields that stay consistent across time while running orders of magnitude faster than traditional numerical solvers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 87,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13243",
        "abs_url": "https://arxiv.org/abs/2601.13243",
        "pdf_url": "https://arxiv.org/pdf/2601.13243",
        "title": "A Comprehensive Evaluation of LLM Reasoning: From Single-Model to Multi-Agent Paradigms",
        "authors": [
            "Yapeng Li",
            "Jiakuo Yu",
            "Zhixin Liu",
            "Xinnan Liu",
            "Jing Yu",
            "Songze Li",
            "Tonghua Su"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Large Language Models (LLMs) are increasingly deployed as reasoning systems, where reasoning paradigms - such as Chain-of-Thought (CoT) and multi-agent systems (MAS) - play a critical role, yet their relative effectiveness and cost-accuracy trade-offs remain poorly understood. In this work, we conduct a comprehensive and unified evaluation of reasoning paradigms, spanning direct single-model generation, CoT-augmented single-model reasoning, and representative MAS workflows, characterizing their reasoning performance across a diverse suite of closed-form benchmarks. Beyond overall performance, we probe role-specific capability demands in MAS using targeted role isolation analyses, and analyze cost-accuracy trade-offs to identify which MAS workflows offer a favorable balance between cost and accuracy, and which incur prohibitive overhead for marginal gains. We further introduce MIMeBench, a new open-ended benchmark that targets two foundational yet underexplored semantic capabilities - semantic abstraction and contrastive discrimination - thereby providing an alternative evaluation axis beyond closed-form accuracy and enabling fine-grained assessment of semantic competence that is difficult to capture with existing benchmarks. Our results show that increased structural complexity does not consistently lead to improved reasoning performance, with its benefits being highly dependent on the properties and suitability of the reasoning paradigm itself. The codes are released at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 88,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13244",
        "abs_url": "https://arxiv.org/abs/2601.13244",
        "pdf_url": "https://arxiv.org/pdf/2601.13244",
        "title": "Do Instruction-Tuned Models Always Perform Better Than Base Models? Evidence from Math and Domain-Shifted Benchmarks",
        "authors": [
            "Prateek Munjal",
            "Clement Christophe",
            "Ronnie Rajan",
            "Praveenkumar Kanithi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Instruction finetuning is standard practice for improving LLM performance, yet it remains unclear whether it enhances reasoning or merely induces surface-level pattern matching. We investigate this by evaluating base and instruction-tuned models on standard math benchmarks, structurally perturbed variants, and domain-shifted tasks. Our analysis highlights two key (often overlooked) limitations of instruction tuning. First, the performance advantage is unstable and depends heavily on evaluation settings. In zero-shot CoT settings on GSM8K, base models consistently outperform instruction-tuned variants, with drops as high as 32.67\\% (Llama3-70B). Instruction-tuned models only match or exceed this performance when provided with few-shot exemplars, suggesting a reliance on specific prompting patterns rather than intrinsic reasoning. Second, tuning gains are brittle under distribution shift. Our results show that base models surpass instruction-tuned variants on the domain-specific MedCalc benchmark. Additionally, instruction-tuned models show sharp declines on perturbed datasets, indicating sensitivity to prompt structure over robust reasoning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 89,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13272",
        "abs_url": "https://arxiv.org/abs/2601.13272",
        "pdf_url": "https://arxiv.org/pdf/2601.13272",
        "title": "Multi-level Monte Carlo Dropout for Efficient Uncertainty Quantification",
        "authors": [
            "Aaron Pim",
            "Tristan Pryer"
        ],
        "comments": "26 pages, 11 figures",
        "subjects": "Machine Learning (cs.LG); Computation (stat.CO); Machine Learning (stat.ML)",
        "abstract": "We develop a multilevel Monte Carlo (MLMC) framework for uncertainty quantification with Monte Carlo dropout. Treating dropout masks as a source of epistemic randomness, we define a fidelity hierarchy by the number of stochastic forward passes used to estimate predictive moments. We construct coupled coarse--fine estimators by reusing dropout masks across fidelities, yielding telescoping MLMC estimators for both predictive means and predictive variances that remain unbiased for the corresponding dropout-induced quantities while reducing sampling variance at fixed evaluation budget. We derive explicit bias, variance and effective cost expressions, together with sample-allocation rules across levels. Numerical experiments on forward and inverse PINNs--Uzawa benchmarks confirm the predicted variance rates and demonstrate efficiency gains over single-level MC-dropout at matched cost.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 90,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13284",
        "abs_url": "https://arxiv.org/abs/2601.13284",
        "pdf_url": "https://arxiv.org/pdf/2601.13284",
        "title": "Balancing Classification and Calibration Performance in Decision-Making LLMs via Calibration Aware Reinforcement Learning",
        "authors": [
            "Duygu Nur Yaldiz",
            "Evangelia Spiliopoulou",
            "Zheng Qi",
            "Siddharth Varia",
            "Srikanth Doss",
            "Nikolaos Pappas"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Large language models (LLMs) are increasingly deployed in decision-making tasks, where not only accuracy but also reliable confidence estimates are essential. Well-calibrated confidence enables downstream systems to decide when to trust a model and when to defer to fallback mechanisms. In this work, we conduct a systematic study of calibration in two widely used fine-tuning paradigms: supervised fine-tuning (SFT) and reinforcement learning with verifiable rewards (RLVR). We show that while RLVR improves task performance, it produces extremely overconfident models, whereas SFT yields substantially better calibration, even under distribution shift, though with smaller performance gains. Through targeted experiments, we diagnose RLVR's failure, showing that decision tokens act as extraction steps of the decision in reasoning traces and do not carry confidence information, which prevents reinforcement learning from surfacing calibrated alternatives. Based on this insight, we propose a calibration-aware reinforcement learning formulation that directly adjusts decision-token probabilities. Our method preserves RLVR's accuracy level while mitigating overconfidence, reducing ECE scores up to 9 points.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 91,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13303",
        "abs_url": "https://arxiv.org/abs/2601.13303",
        "pdf_url": "https://arxiv.org/pdf/2601.13303",
        "title": "Verifying Local Robustness of Pruned Safety-Critical Networks",
        "authors": [
            "Minh Le",
            "Phuong Cao"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Formal verification of Deep Neural Networks (DNNs) is essential for safety-critical applications, ranging from surgical robotics to NASA JPL autonomous systems. However, the computational cost of verifying large-scale models remains a significant barrier to adoption. This paper investigates the impact of pruning on formal local robustness certificates with different ratios. Using the state-of-the-art $\\alpha,\\beta$-CROWN verifier, we evaluate ResNet4 models across varying pruning ratios on MNIST and, more importantly, on the NASA JPL Mars Frost Identification datasets. Our findings demonstrate a non-linear relationship: light pruning (40%) in MNIST and heavy pruning (70%-90%) in JPL improve verifiability, allowing models to outperform unpruned baselines in proven $L_\\infty$ robustness properties. This suggests that reduced connectivity simplifies the search space for formal solvers and that the optimal pruning ratio varies significantly between datasets. This research highlights the complex nature of model compression, offering critical insights into selecting the optimal pruning ratio for deploying efficient, yet formally verified, DNNs in high-stakes environments where reliability is non-negotiable.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 92,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13350",
        "abs_url": "https://arxiv.org/abs/2601.13350",
        "pdf_url": "https://arxiv.org/pdf/2601.13350",
        "title": "Beyond Mapping : Domain-Invariant Representations via Spectral Embedding of Optimal Transport Plans",
        "authors": [
            "Abdel Djalil Sad Saoud",
            "Fred Maurice Ngolè Mboula",
            "Hanane Slimani"
        ],
        "comments": "5 pages, 2 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Distributional shifts between training and inference time data remain a central challenge in machine learning, often leading to poor performance. It motivated the study of principled approaches for domain alignment, such as optimal transport based unsupervised domain adaptation, that relies on approximating Monge map using transport plans, which is sensitive to the transport problem regularization strategy and hyperparameters, and might yield biased domains alignment. In this work, we propose to interpret smoothed transport plans as adjacency matrices of bipartite graphs connecting source to target domain and derive domain-invariant samples' representations through spectral embedding. We evaluate our approach on acoustic adaptation benchmarks for music genre recognition, music-speech discrimination, as well as electrical cable defect detection and classification tasks using time domain reflection in different diagnosis settings, achieving overall strong performances.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 93,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13357",
        "abs_url": "https://arxiv.org/abs/2601.13357",
        "pdf_url": "https://arxiv.org/pdf/2601.13357",
        "title": "On the Relation of State Space Models and Hidden Markov Models",
        "authors": [
            "Aydin Ghojogh",
            "M.Hadi Sepanj",
            "Benyamin Ghojogh"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL); Audio and Speech Processing (eess.AS); Systems and Control (eess.SY)",
        "abstract": "State Space Models (SSMs) and Hidden Markov Models (HMMs) are foundational frameworks for modeling sequential data with latent variables and are widely used in signal processing, control theory, and machine learning. Despite their shared temporal structure, they differ fundamentally in the nature of their latent states, probabilistic assumptions, inference procedures, and training paradigms. Recently, deterministic state space models have re-emerged in natural language processing through architectures such as S4 and Mamba, raising new questions about the relationship between classical probabilistic SSMs, HMMs, and modern neural sequence models. In this paper, we present a unified and systematic comparison of HMMs, linear Gaussian state space models, Kalman filtering, and contemporary NLP state space models. We analyze their formulations through the lens of probabilistic graphical models, examine their inference algorithms -- including forward-backward inference and Kalman filtering -- and contrast their learning procedures via Expectation-Maximization and gradient-based optimization. By highlighting both structural similarities and semantic differences, we clarify when these models are equivalent, when they fundamentally diverge, and how modern NLP SSMs relate to classical probabilistic models. Our analysis bridges perspectives from control theory, probabilistic modeling, and modern deep learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 94,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13365",
        "abs_url": "https://arxiv.org/abs/2601.13365",
        "pdf_url": "https://arxiv.org/pdf/2601.13365",
        "title": "CausationEntropy: Pythonic Optimal Causation Entropy",
        "authors": [
            "Kevin Slote",
            "Jeremie Fish",
            "Erik Bollt"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Data Analysis, Statistics and Probability (physics.data-an)",
        "abstract": "Optimal Causation Entropy (oCSE) is a robust causal network modeling technique that reveals causal networks from dynamical systems and coupled oscillators, distinguishing direct from indirect paths. CausationEntropy is a Python package that implements oCSE and several of its significant optimizations and methodological extensions. In this paper, we introduce the version 1.1 release of CausationEntropy, which includes new synthetic data generators, plotting tools, and several advanced information-theoretical causal network discovery algorithms with criteria for estimating Gaussian, k-nearest neighbors (kNN), geometric k-nearest neighbors (geometric-kNN), kernel density (KDE) and Poisson entropic estimators. The package is easy to install from the PyPi software repository, is thoroughly documented, supplemented with extensive code examples, and is modularly structured to support future additions. The entire codebase is released under the MIT license and is available on GitHub and through PyPi Repository. We expect this package to serve as a benchmark tool for causal discovery in complex dynamical systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 95,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13422",
        "abs_url": "https://arxiv.org/abs/2601.13422",
        "pdf_url": "https://arxiv.org/pdf/2601.13422",
        "title": "TrustEnergy: A Unified Framework for Accurate and Reliable User-level Energy Usage Prediction",
        "authors": [
            "Dahai Yu",
            "Rongchao Xu",
            "Dingyi Zhuang",
            "Yuheng Bu",
            "Shenhao Wang",
            "Guang Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Energy usage prediction is important for various real-world applications, including grid management, infrastructure planning, and disaster response. Although a plethora of deep learning approaches have been proposed to perform this task, most of them either overlook the essential spatial correlations across households or fail to scale to individualized prediction, making them less effective for accurate fine-grained user-level prediction. In addition, due to the dynamic and uncertain nature of energy usage caused by various factors such as extreme weather events, quantifying uncertainty for reliable prediction is also significant, but it has not been fully explored in existing work. In this paper, we propose a unified framework called TrustEnergy for accurate and reliable user-level energy usage prediction. There are two key technical components in TrustEnergy, (i) a Hierarchical Spatiotemporal Representation module to efficiently capture both macro and micro energy usage patterns with a novel memory-augmented spatiotemporal graph neural network, and (ii) an innovative Sequential Conformalized Quantile Regression module to dynamically adjust uncertainty bounds to ensure valid prediction intervals over time, without making strong assumptions about the underlying data distribution. We implement and evaluate our TrustEnergy framework by working with an electricity provider in Florida, and the results show our TrustEnergy can achieve a 5.4% increase in prediction accuracy and 5.7% improvement in uncertainty quantification compared to state-of-the-art baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 96,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13445",
        "abs_url": "https://arxiv.org/abs/2601.13445",
        "pdf_url": "https://arxiv.org/pdf/2601.13445",
        "title": "BladeSDF : Unconditional and Conditional Generative Modeling of Representative Blade Geometries Using Signed Distance Functions",
        "authors": [
            "Ashish S. Nair",
            "Sandipp Krishnan Ravi",
            "Itzel Salgado",
            "Changjie Sun",
            "Sayan Ghosh",
            "Liping Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computational Physics (physics.comp-ph)",
        "abstract": "Generative AI has emerged as a transformative paradigm in engineering design, enabling automated synthesis and reconstruction of complex 3D geometries while preserving feasibility and performance relevance. This paper introduces a domain-specific implicit generative framework for turbine blade geometry using DeepSDF, addressing critical gaps in performance-aware modeling and manufacturable design generation. The proposed method leverages a continuous signed distance function (SDF) representation to reconstruct and generate smooth, watertight geometries with quantified accuracy. It establishes an interpretable, near-Gaussian latent space that aligns with blade-relevant parameters, such as taper and chord ratios, enabling controlled exploration and unconditional synthesis through interpolation and Gaussian sampling. In addition, a compact neural network maps engineering descriptors, such as maximum directional strains, to latent codes, facilitating the generation of performance-informed geometry. The framework achieves high reconstruction fidelity, with surface distance errors concentrated within $1\\%$ of the maximum blade dimension, and demonstrates robust generalization to unseen designs. By integrating constraints, objectives, and performance metrics, this approach advances beyond traditional 2D-guided or unconstrained 3D pipelines, offering a practical and interpretable solution for data-driven turbine blade modeling and concept generation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 97,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13448",
        "abs_url": "https://arxiv.org/abs/2601.13448",
        "pdf_url": "https://arxiv.org/pdf/2601.13448",
        "title": "Fairness-informed Pareto Optimization : An Efficient Bilevel Framework",
        "authors": [
            "Sofiane Tanji",
            "Samuel Vaiter",
            "Yassine Laguel"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Optimization and Control (math.OC); Machine Learning (stat.ML)",
        "abstract": "Despite their promise, fair machine learning methods often yield Pareto-inefficient models, in which the performance of certain groups can be improved without degrading that of others. This issue arises frequently in traditional in-processing approaches such as fairness-through-regularization. In contrast, existing Pareto-efficient approaches are biased towards a certain perspective on fairness and fail to adapt to the broad range of fairness metrics studied in the literature. In this paper, we present BADR, a simple framework to recover the optimal Pareto-efficient model for any fairness metric. Our framework recovers its models through a Bilevel Adaptive Rescalarisation procedure. The lower level is a weighted empirical risk minimization task where the weights are a convex combination of the groups, while the upper level optimizes the chosen fairness objective. We equip our framework with two novel large-scale, single-loop algorithms, BADR-GD and BADR-SGD, and establish their convergence guarantees. We release badr, an open-source Python toolbox implementing our framework for a variety of learning tasks and fairness metrics. Finally, we conduct extensive numerical experiments demonstrating the advantages of BADR over existing Pareto-efficient approaches to fairness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 98,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13456",
        "abs_url": "https://arxiv.org/abs/2601.13456",
        "pdf_url": "https://arxiv.org/pdf/2601.13456",
        "title": "Federated Learning Under Temporal Drift - Mitigating Catastrophic Forgetting via Experience Replay",
        "authors": [
            "Sahasra Kokkula",
            "Daniel David",
            "Aaditya Baruah"
        ],
        "comments": "8 pages, 5 figures. Course project for Neural Networks & Deep Learning COMSW4776 course at Columbia University",
        "subjects": "Machine Learning (cs.LG); Distributed, Parallel, and Cluster Computing (cs.DC)",
        "abstract": "Federated Learning struggles under temporal concept drift where client data distributions shift over time. We demonstrate that standard FedAvg suffers catastrophic forgetting under seasonal drift on Fashion-MNIST, with accuracy dropping from 74% to 28%. We propose client-side experience replay, where each client maintains a small buffer of past samples mixed with current data during local training. This simple approach requires no changes to server aggregation. Experiments show that a 50-sample-per-class buffer restores performance to 78-82%, effectively preventing forgetting. Our ablation study reveals a clear memory-accuracy trade-off as buffer size increases.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 99,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13463",
        "abs_url": "https://arxiv.org/abs/2601.13463",
        "pdf_url": "https://arxiv.org/pdf/2601.13463",
        "title": "Quantum Qualifiers for Neural Network Model Selection in Hadronic Physics",
        "authors": [
            "Brandon B. Le",
            "D. Keller"
        ],
        "comments": "12 pages, 5 figures. Proceedings for the 26th International Symposium on Spin Physics (SPIN2025), September 21-26, 2025; Qingdao, Shandong, China",
        "subjects": "Machine Learning (cs.LG); High Energy Physics - Phenomenology (hep-ph); Nuclear Theory (nucl-th); Quantum Physics (quant-ph)",
        "abstract": "As quantum machine-learning architectures mature, a central challenge is no longer their construction, but identifying the regimes in which they offer practical advantages over classical approaches. In this work, we introduce a framework for addressing this question in data-driven hadronic physics problems by developing diagnostic tools - centered on a quantitative quantum qualifier - that guide model selection between classical and quantum deep neural networks based on intrinsic properties of the data. Using controlled classification and regression studies, we show how relative model performance follows systematic trends in complexity, noise, and dimensionality, and how these trends can be distilled into a predictive criterion. We then demonstrate the utility of this approach through an application to Compton form factor extraction from deeply virtual Compton scattering, where the quantum qualifier identifies kinematic regimes favorable to quantum models. Together, these results establish a principled framework for deploying quantum machine-learning tools in precision hadronic physics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 100,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13476",
        "abs_url": "https://arxiv.org/abs/2601.13476",
        "pdf_url": "https://arxiv.org/pdf/2601.13476",
        "title": "A Unified Variational Imputation Framework for Electric Vehicle Charging Data Using Retrieval-Augmented Language Model",
        "authors": [
            "Jinhao Li",
            "Hao Wang"
        ],
        "comments": "15 pages",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "The reliability of data-driven applications in electric vehicle (EV) infrastructure, such as charging demand forecasting, hinges on the availability of complete, high-quality charging data. However, real-world EV datasets are often plagued by missing records, and existing imputation methods are ill-equipped for the complex, multimodal context of charging data, often relying on a restrictive one-model-per-station paradigm that ignores valuable inter-station correlations. To address these gaps, we develop a novel PRobabilistic variational imputation framework that leverages the power of large lAnguage models and retrIeval-augmented Memory (PRAIM). PRAIM employs a pre-trained language model to encode heterogeneous data, spanning time-series demand, calendar features, and geospatial context, into a unified, semantically rich representation. This is dynamically fortified by retrieval-augmented memory that retrieves relevant examples from the entire charging network, enabling a single, unified imputation model empowered by variational neural architecture to overcome data sparsity. Extensive experiments on four public datasets demonstrate that PRAIM significantly outperforms established baselines in both imputation accuracy and its ability to preserve the original data's statistical distribution, leading to substantial improvements in downstream forecasting performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 101,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13522",
        "abs_url": "https://arxiv.org/abs/2601.13522",
        "pdf_url": "https://arxiv.org/pdf/2601.13522",
        "title": "StoTAM: Stochastic Alternating Minimization for Tucker-Structured Tensor Sensing",
        "authors": [
            "Shuang Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Optimization and Control (math.OC)",
        "abstract": "Low-rank tensor sensing is a fundamental problem with broad applications in signal processing and machine learning. Among various tensor models, low-Tucker-rank tensors are particularly attractive for capturing multi-mode subspace structures in high-dimensional data. Existing recovery methods either operate on the full tensor variable with expensive tensor projections, or adopt factorized formulations that still rely on full-gradient computations, while most stochastic factorized approaches are restricted to tensor decomposition settings. In this work, we propose a stochastic alternating minimization algorithm that operates directly on the core tensor and factor matrices under a Tucker factorization. The proposed method avoids repeated tensor projections and enables efficient mini-batch updates on low-dimensional tensor factors. Numerical experiments on synthetic tensor sensing demonstrate that the proposed algorithm exhibits favorable convergence behavior in wall-clock time compared with representative stochastic tensor recovery baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 102,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13548",
        "abs_url": "https://arxiv.org/abs/2601.13548",
        "pdf_url": "https://arxiv.org/pdf/2601.13548",
        "title": "Patterning: The Dual of Interpretability",
        "authors": [
            "George Wang",
            "Daniel Murfet"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Mechanistic interpretability aims to understand how neural networks generalize beyond their training data by reverse-engineering their internal structures. We introduce patterning as the dual problem: given a desired form of generalization, determine what training data produces it. Our approach is based on susceptibilities, which measure how posterior expectation values of observables respond to infinitesimal shifts in the data distribution. Inverting this linear response relationship yields the data intervention that steers the model toward a target internal configuration. We demonstrate patterning in a small language model, showing that re-weighting training data along principal susceptibility directions can accelerate or delay the formation of structure, such as the induction circuit. In a synthetic parentheses balancing task where multiple algorithms achieve perfect training accuracy, we show that patterning can select which algorithm the model learns by targeting the local learning coefficient of each solution. These results establish that the same mathematical framework used to read internal structure can be inverted to write it.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 103,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13564",
        "abs_url": "https://arxiv.org/abs/2601.13564",
        "pdf_url": "https://arxiv.org/pdf/2601.13564",
        "title": "Multi-objective fluorescent molecule design with a data-physics dual-driven generative framework",
        "authors": [
            "Yanheng Li",
            "Zhichen Pu",
            "Lijiang Yang",
            "Zehao Zhou",
            "Yi Qin Gao"
        ],
        "comments": "Total 43 pages: 32 pages Main Text + 11 pages SI",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Chemical Physics (physics.chem-ph); Biomolecules (q-bio.BM)",
        "abstract": "Designing fluorescent small molecules with tailored optical and physicochemical properties requires navigating vast, underexplored chemical space while satisfying multiple objectives and constraints. Conventional generate-score-screen approaches become impractical under such realistic design specifications, owing to their low search efficiency, unreliable generalizability of machine-learning prediction, and the prohibitive cost of quantum chemical calculation. Here we present LUMOS, a data-and-physics driven framework for inverse design of fluorescent molecules. LUMOS couples generator and predictor within a shared latent representation, enabling direct specification-to-molecule design and efficient exploration. Moreover, LUMOS combines neural networks with a fast time-dependent density functional theory (TD-DFT) calculation workflow to build a suite of complementary predictors spanning different trade-offs in speed, accuracy, and generalizability, enabling reliable property prediction across diverse scenarios. Finally, LUMOS employs a property-guided diffusion model integrated with multi-objective evolutionary algorithms, enabling de novo design and molecular optimization under multiple objectives and constraints. Across comprehensive benchmarks, LUMOS consistently outperforms baseline models in terms of accuracy, generalizability and physical plausibility for fluorescence property prediction, and demonstrates superior performance in multi-objective scaffold- and fragment-level molecular optimization. Further validation using TD-DFT and molecular dynamics (MD) simulations demonstrates that LUMOS can generate valid fluorophores that meet various target specifications. Overall, these results establish LUMOS as a data-physics dual-driven framework for general fluorophore inverse design.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 104,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13569",
        "abs_url": "https://arxiv.org/abs/2601.13569",
        "pdf_url": "https://arxiv.org/pdf/2601.13569",
        "title": "DRGW: Learning Disentangled Representations for Robust Graph Watermarking",
        "authors": [
            "Jiasen Li",
            "Yanwei Liu",
            "Zhuoyi Shang",
            "Xiaoyan Gu",
            "Weiping Wang"
        ],
        "comments": "Published at The Web Conference 2026 (WWW '26)",
        "subjects": "Machine Learning (cs.LG); Cryptography and Security (cs.CR)",
        "abstract": "Graph-structured data is foundational to numerous web applications, and watermarking is crucial for protecting their intellectual property and ensuring data provenance. Existing watermarking methods primarily operate on graph structures or entangled graph representations, which compromise the transparency and robustness of watermarks due to the information coupling in representing graphs and uncontrollable discretization in transforming continuous numerical representations into graph structures. This motivates us to propose DRGW, the first graph watermarking framework that addresses these issues through disentangled representation learning. Specifically, we design an adversarially trained encoder that learns an invariant structural representation against diverse perturbations and derives a statistically independent watermark carrier, ensuring both robustness and transparency of watermarks. Meanwhile, we devise a graph-aware invertible neural network to provide a lossless channel for watermark embedding and extraction, guaranteeing high detectability and transparency of watermarks. Additionally, we develop a structure-aware editor that resolves the issue of latent modifications into discrete graph edits, ensuring robustness against structural perturbations. Experiments on diverse benchmark datasets demonstrate the superior effectiveness of DRGW.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 105,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13572",
        "abs_url": "https://arxiv.org/abs/2601.13572",
        "pdf_url": "https://arxiv.org/pdf/2601.13572",
        "title": "Behavior Knowledge Merge in Reinforced Agentic Models",
        "authors": [
            "Xiangchi Yuan",
            "Dachuan Shi",
            "Chunhui Zhang",
            "Zheyuan Liu",
            "Shenglong Yao",
            "Soroush Vosoughi",
            "Wenke Lee"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Reinforcement learning (RL) is central to post-training, particularly for agentic models that require specialized reasoning behaviors. In this setting, model merging offers a practical mechanism for integrating multiple RL-trained agents from different tasks into a single generalist model. However, existing merging methods are designed for supervised fine-tuning (SFT), and they are suboptimal to preserve task-specific capabilities on RL-trained agentic models. The root is a task-vector mismatch between RL and SFT: on-policy RL induces task vectors that are highly sparse and heterogeneous, whereas SFT-style merging implicitly assumes dense and globally comparable task vectors. When standard global averaging is applied under this mismatch, RL's non-overlapping task vectors that encode critical task-specific behaviors are reduced and parameter updates are diluted. To address this issue, we propose Reinforced Agent Merging (RAM), a distribution-aware merging framework explicitly designed for RL-trained agentic models. RAM disentangles shared and task-specific unique parameter updates, averaging shared components while selectively preserving and rescaling unique ones to counteract parameter update dilution. Experiments across multiple agent domains and model architectures demonstrate that RAM not only surpasses merging baselines, but also unlocks synergistic potential among agents to achieve performance superior to that of specialized agents in their domains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 106,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13580",
        "abs_url": "https://arxiv.org/abs/2601.13580",
        "pdf_url": "https://arxiv.org/pdf/2601.13580",
        "title": "Neural Organ Transplantation (NOT): Checkpoint-Based Modular Adaptation for Transformer Models",
        "authors": [
            "Ahmad Al-Zuraiqi"
        ],
        "comments": "27 pages, 8 figures, 16 tables. Decoder-only transformers (124M-20B parameters). Complete experimental results and reproducibility details in appendices. Code and checkpoints: this https URL",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "We introduce Neural Organ Transplantation (NOT), a modular adaptation framework that enables trained transformer layers to function as reusable transferable checkpoints for domain adaptation. Unlike conventional fine-tuning approaches that tightly couple trained parameters to specific model instances and training data, NOT extracts contiguous layer subsets (\"donor organs\") from pre-trained models, trains them independently on domain-specific data, and saves them as standalone checkpoint files that can be transplanted into compatible recipient models without access to the original training data. Through experiments on three decoder-only transformer architectures spanning 124M to 20B parameters (GPT-2, TinyLlama, and GPT-OSS), we demonstrate that donor transplantation substantially outperforms existing adaptation methods, achieving an order-of-magnitude improvement in perplexity over LoRA while training significantly faster. The method exhibits position dependence, with early insertion positions yielding optimal results. Cross-domain transfer at billion-parameter scale reveals unexpected regularization benefits. These findings demonstrate that transformer middle layers can support efficient modular transfer for decoder-only architectures, enabling privacy-preserving expertise sharing through checkpoint distribution. We note that this approach is currently limited to decoder-only models; preliminary experiments on encoder-based architectures show reduced effectiveness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 107,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13599",
        "abs_url": "https://arxiv.org/abs/2601.13599",
        "pdf_url": "https://arxiv.org/pdf/2601.13599",
        "title": "Diffusion In Diffusion: Breaking the Autoregressive Bottleneck in Block Diffusion Models",
        "authors": [
            "Linrui Ma",
            "Yufei Cui",
            "Kai Han",
            "Yunhe Wang"
        ],
        "comments": "Work In Progress",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Block diffusion language models, operating as semi-autoregressive paradigms, combine the strengths of both autoregressive and diffusion paradigms. However, their strict unidirectional block dependencies introduce irreversibility and sacrifice the global planning capabilities for which diffusion models are renowned. In order to address these issues, we propose Diffusion in Diffusion, a draft-then-refine framework designed to overcome the irreversibility and myopia problems inherent in block diffusion models. Our approach first employs block diffusion to generate rapid drafts using small blocks, then refines these drafts through global bidirectional diffusion with a larger bidirectional receptive field. We utilise snapshot confidence remasking to identify the most critical tokens that require modification, and apply mix-scale training to expand the block diffusion model's global capabilities. Empirical results demonstrate that our approach sets a new benchmark for discrete diffusion models on the OpenWebText dataset. Using just 26% of the fine-tuning budget of baseline models, we reduce generative perplexity from 25.7 to 21.9, significantly narrowing the performance gap with autoregressive models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 108,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13608",
        "abs_url": "https://arxiv.org/abs/2601.13608",
        "pdf_url": "https://arxiv.org/pdf/2601.13608",
        "title": "Fisher-Informed Parameterwise Aggregation for Federated Learning with Heterogeneous Data",
        "authors": [
            "Zhipeng Chang",
            "Ting He",
            "Wenrui Hao"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Federated learning aggregates model updates from distributed clients, but standard first order methods such as FedAvg apply the same scalar weight to all parameters from each client. Under non-IID data, these uniformly weighted updates can be strongly misaligned across clients, causing client drift and degrading the global model. Here we propose Fisher-Informed Parameterwise Aggregation (FIPA), a second-order aggregation method that replaces client-level scalar weights with parameter-specific Fisher Information Matrix (FIM) weights, enabling true parameter-level scaling that captures how each client's data uniquely influences different parameters. With low-rank approximation, FIPA remains communication- and computation-efficient. Across nonlinear function regression, PDE learning, and image classification, FIPA consistently improves over averaging-based aggregation, and can be effectively combined with state-of-the-art client-side optimization algorithms to further improve image classification accuracy. These results highlight the benefits of FIPA for federated learning under heterogeneous data distributions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 109,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13653",
        "abs_url": "https://arxiv.org/abs/2601.13653",
        "pdf_url": "https://arxiv.org/pdf/2601.13653",
        "title": "TimeART: Towards Agentic Time Series Reasoning via Tool-Augmentation",
        "authors": [
            "Xingjian Wu",
            "Junkai Lu",
            "Zhengyu Li",
            "Xiangfei Qiu",
            "Jilin Hu",
            "Chenjuan Guo",
            "Christian S. Jensen",
            "Bin Yang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Time series data widely exist in real-world cyber-physical systems. Though analyzing and interpreting them contributes to significant values, e.g, disaster prediction and financial risk control, current workflows mainly rely on human data scientists, which requires significant labor costs and lacks automation. To tackle this, we introduce TimeART, a framework fusing the analytical capability of strong out-of-the-box tools and the reasoning capability of Large Language Models (LLMs), which serves as a fully agentic data scientist for Time Series Question Answering (TSQA). To teach the LLM-based Time Series Reasoning Models (TSRMs) strategic tool-use, we also collect a 100k expert trajectory corpus called TimeToolBench. To enhance TSRMs' generalization capability, we then devise a four-stage training strategy, which boosts TSRMs through learning from their own early experiences and self-reflections. Experimentally, we train an 8B TSRM on TimeToolBench and equip it with the TimeART framework, and it achieves consistent state-of-the-art performance on multiple TSQA tasks, which pioneers a novel approach towards agentic time series reasoning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 110,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13676",
        "abs_url": "https://arxiv.org/abs/2601.13676",
        "pdf_url": "https://arxiv.org/pdf/2601.13676",
        "title": "Autoregressive deep learning for real-time simulation of soft tissue dynamics during virtual neurosurgery",
        "authors": [
            "Fabian Greifeneder",
            "Wolfgang Fenz",
            "Benedikt Alkin",
            "Johannes Brandstetter",
            "Michael Giretzlehner",
            "Philipp Moser"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Accurate simulation of brain deformation is a key component for developing realistic, interactive neurosurgical simulators, as complex nonlinear deformations must be captured to ensure realistic tool-tissue interactions. However, traditional numerical solvers often fall short in meeting real-time performance requirements. To overcome this, we introduce a deep learning-based surrogate model that efficiently simulates transient brain deformation caused by continuous interactions between surgical instruments and the virtual brain geometry. Building on Universal Physics Transformers, our approach operates directly on large-scale mesh data and is trained on an extensive dataset generated from nonlinear finite element simulations, covering a broad spectrum of temporal instrument-tissue interaction scenarios. To reduce the accumulation of errors in autoregressive inference, we propose a stochastic teacher forcing strategy applied during model training. Specifically, training consists of short stochastic rollouts in which the proportion of ground truth inputs is gradually decreased in favor of model-generated predictions. Our results show that the proposed surrogate model achieves accurate and efficient predictions across a range of transient brain deformation scenarios, scaling to meshes with up to 150,000 nodes. The introduced stochastic teacher forcing technique substantially improves long-term rollout stability, reducing the maximum prediction error from 6.7 mm to 3.5 mm. We further integrate the trained surrogate model into an interactive neurosurgical simulation environment, achieving runtimes below 10 ms per simulation step on consumer-grade inference hardware. Our proposed deep learning framework enables rapid, smooth and accurate biomechanical simulations of dynamic brain tissue deformation, laying the foundation for realistic surgical training environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 111,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13710",
        "abs_url": "https://arxiv.org/abs/2601.13710",
        "pdf_url": "https://arxiv.org/pdf/2601.13710",
        "title": "Who Should Have Surgery? A Comparative Study of GenAI vs Supervised ML for CRS Surgical Outcome Prediction",
        "authors": [
            "Sayeed Shafayet Chowdhury",
            "Snehasis Mukhopadhyay",
            "Shiaofen Fang",
            "Vijay R. Ramakrishnan"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Artificial intelligence has reshaped medical imaging, yet the use of AI on clinical data for prospective decision support remains limited. We study pre-operative prediction of clinically meaningful improvement in chronic rhinosinusitis (CRS), defining success as a more than 8.9-point reduction in SNOT-22 at 6 months (MCID). In a prospectively collected cohort where all patients underwent surgery, we ask whether models using only pre-operative clinical data could have identified those who would have poor outcomes, i.e. those who should have avoided surgery. We benchmark supervised ML (logistic regression, tree ensembles, and an in-house MLP) against generative AI (ChatGPT, Claude, Gemini, Perplexity), giving each the same structured inputs and constraining outputs to binary recommendations with confidence. Our best ML model (MLP) achieves 85 % accuracy with superior calibration and decision-curve net benefit. GenAI models underperform on discrimination and calibration across zero-shot setting. Notably, GenAI justifications align with clinician heuristics and the MLP's feature importance, repeatedly highlighting baseline SNOT-22, CT/endoscopy severity, polyp phenotype, and physchology/pain comorbidities. We provide a reproducible tabular-to-GenAI evaluation protocol and subgroup analyses. Findings support an ML-first, GenAI- augmented workflow: deploy calibrated ML for primary triage of surgical candidacy, with GenAI as an explainer to enhance transparency and shared decision-making.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 112,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13748",
        "abs_url": "https://arxiv.org/abs/2601.13748",
        "pdf_url": "https://arxiv.org/pdf/2601.13748",
        "title": "EEG-Titans: Long-Horizon Seizure Forecasting via Dual-Branch Attention and Neural Memory",
        "authors": [
            "Tien-Dat Pham",
            "Xuan-The Tran"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Human-Computer Interaction (cs.HC)",
        "abstract": "Accurate epileptic seizure prediction from electroencephalography (EEG) remains challenging because pre-ictal dynamics may span long time horizons while clinically relevant signatures can be subtle and transient. Many deep learning models face a persistent trade-off between capturing local spatiotemporal patterns and maintaining informative long-range context when operating on ultralong sequences. We propose EEG-Titans, a dualbranch architecture that incorporates a modern neural memory mechanism for long-context modeling. The model combines sliding-window attention to capture short-term anomalies with a recurrent memory pathway that summarizes slower, progressive trends over time. On the CHB-MIT scalp EEG dataset, evaluated under a chronological holdout protocol, EEG-Titans achieves 99.46% average segment-level sensitivity across 18 subjects. We further analyze safety-first operating points on artifact-prone recordings and show that a hierarchical context strategy extending the receptive field for high-noise subjects can markedly reduce false alarms (down to 0.00 FPR/h in an extreme outlier) without sacrificing sensitivity. These results indicate that memory-augmented long-context modeling can provide robust seizure forecasting under clinically constrained evaluation",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 113,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13776",
        "abs_url": "https://arxiv.org/abs/2601.13776",
        "pdf_url": "https://arxiv.org/pdf/2601.13776",
        "title": "Orthogonium : A Unified, Efficient Library of Orthogonal and 1-Lipschitz Building Blocks",
        "authors": [
            "Thibaut Boissin",
            "Franck Mamalet",
            "Valentin Lafargue",
            "Mathieu Serrurier"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Orthogonal and 1-Lipschitz neural network layers are essential building blocks in robust deep learning architectures, crucial for certified adversarial robustness, stable generative models, and reliable recurrent networks. Despite significant advancements, existing implementations remain fragmented, limited, and computationally demanding. To address these issues, we introduce Orthogonium , a unified, efficient, and comprehensive PyTorch library providing orthogonal and 1-Lipschitz layers. Orthogonium provides access to standard convolution features-including support for strides, dilation, grouping, and transposed-while maintaining strict mathematical guarantees. Its optimized implementations reduce overhead on large scale benchmarks such as ImageNet. Moreover, rigorous testing within the library has uncovered critical errors in existing implementations, emphasizing the importance of standardized and reliable tools. Orthogonium thus significantly lowers adoption barriers, enabling scalable experimentation and integration across diverse applications requiring orthogonality and robust Lipschitz constraints. Orthogonium is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 114,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13780",
        "abs_url": "https://arxiv.org/abs/2601.13780",
        "pdf_url": "https://arxiv.org/pdf/2601.13780",
        "title": "Principled Latent Diffusion for Graphs via Laplacian Autoencoders",
        "authors": [
            "Antoine Siraudin",
            "Christopher Morris"
        ],
        "comments": "Preprint, under review",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Graph diffusion models achieve state-of-the-art performance in graph generation but suffer from quadratic complexity in the number of nodes -- and much of their capacity is wasted modeling the absence of edges in sparse graphs. Inspired by latent diffusion in other modalities, a natural idea is to compress graphs into a low-dimensional latent space and perform diffusion there. However, unlike images or text, graph generation requires nearly lossless reconstruction, as even a single error in decoding an adjacency matrix can render the entire sample invalid. This challenge has remained largely unaddressed. We propose LG-Flow, a latent graph diffusion framework that directly overcomes these obstacles. A permutation-equivariant autoencoder maps each node into a fixed-dimensional embedding from which the full adjacency is provably recoverable, enabling near-lossless reconstruction for both undirected graphs and DAGs. The dimensionality of this latent representation scales linearly with the number of nodes, eliminating the quadratic bottleneck and making it feasible to train larger and more expressive models. In this latent space, we train a Diffusion Transformer with flow matching, enabling efficient and expressive graph generation. Our approach achieves competitive results against state-of-the-art graph diffusion models, while achieving up to $1000\\times$ speed-up.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 115,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13793",
        "abs_url": "https://arxiv.org/abs/2601.13793",
        "pdf_url": "https://arxiv.org/pdf/2601.13793",
        "title": "PAtt: A Pattern Attention Network for ETA Prediction Using Historical Speed Profiles",
        "authors": [
            "ByeoungDo Kim",
            "JunYeop Na",
            "Kyungwook Tak",
            "JunTae Kim",
            "DongHyeon Kim",
            "Duckky Kim"
        ],
        "comments": "7 pages, 3 figures, ITSC 2025, to be published",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In this paper, we propose an ETA model (Estimated Time of Arrival) that leverages an attention mechanism over historical road speed patterns. As autonomous driving and intelligent transportation systems become increasingly prevalent, the need for accurate and reliable ETA estimation has grown, playing a vital role in navigation, mobility planning, and traffic management. However, predicting ETA remains a challenging task due to the dynamic and complex nature of traffic flow. Traditional methods often combine real-time and historical traffic data in simplistic ways, or rely on complex rule-based computations. While recent deep learning models have shown potential, they often require high computational costs and do not effectively capture the spatio-temporal patterns crucial for ETA prediction. ETA prediction inherently involves spatio-temporal causality, and our proposed model addresses this by leveraging attention mechanisms to extract and utilize temporal features accumulated at each spatio-temporal point along a route. This architecture enables efficient and accurate ETA estimation while keeping the model lightweight and scalable. We validate our approach using real-world driving datasets and demonstrate that our approach outperforms existing baselines by effectively integrating road characteristics, real-time traffic conditions, and historical speed patterns in a task-aware manner.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 116,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13824",
        "abs_url": "https://arxiv.org/abs/2601.13824",
        "pdf_url": "https://arxiv.org/pdf/2601.13824",
        "title": "ELSA: Efficient LLM-Centric Split Aggregation for Privacy-Aware Hierarchical Federated Learning over Resource-Constrained Edge Networks",
        "authors": [
            "Xiaohong Yang",
            "Tong Xie",
            "Minghui Liwang",
            "Chikai Shang",
            "Yang Lu",
            "Zhenzhen Jiao",
            "Liqun Fu",
            "Seyyedali Hosseinalipour"
        ],
        "comments": "11 pages, 16 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Training large language models (LLMs) at the network edge faces fundamental challenges arising from device resource constraints, severe data heterogeneity, and heightened privacy risks. To address these, we propose ELSA (Efficient LLM-centric Split Aggregation), a novel framework that systematically integrates split learning (SL) and hierarchical federated learning (HFL) for distributed LLM fine-tuning over resource-constrained edge networks. ELSA introduces three key innovations. First, it employs a task-agnostic, behavior-aware client clustering mechanism that constructs semantic fingerprints using public probe inputs and symmetric KL divergence, further enhanced by prediction-consistency-based trust scoring and latency-aware edge assignment to jointly address data heterogeneity, client unreliability, and communication constraints. Second, it splits the LLM into three parts across clients and edge servers, with the cloud used only for adapter aggregation, enabling an effective balance between on-device computation cost and global convergence stability. Third, it incorporates a lightweight communication scheme based on computational sketches combined with semantic subspace orthogonal perturbation (SS-OP) to reduce communication overhead while mitigating privacy leakage during model exchanges. Experiments across diverse NLP tasks demonstrate that ELSA consistently outperforms state-of-the-art methods in terms of adaptability, convergence behavior, and robustness, establishing a scalable and privacy-aware solution for edge-side LLM fine-tuning under resource constraints.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 117,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13844",
        "abs_url": "https://arxiv.org/abs/2601.13844",
        "pdf_url": "https://arxiv.org/pdf/2601.13844",
        "title": "Optimal L2 Regularization in High-dimensional Continual Linear Regression",
        "authors": [
            "Gilad Karpel",
            "Edward Moroshko",
            "Ran Levinstein",
            "Ron Meir",
            "Daniel Soudry",
            "Itay Evron"
        ],
        "comments": "Accepted to ALT 2026",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We study generalization in an overparameterized continual linear regression setting, where a model is trained with L2 (isotropic) regularization across a sequence of tasks. We derive a closed-form expression for the expected generalization loss in the high-dimensional regime that holds for arbitrary linear teachers. We demonstrate that isotropic regularization mitigates label noise under both single-teacher and multiple i.i.d. teacher settings, whereas prior work accommodating multiple teachers either did not employ regularization or used memory-demanding methods. Furthermore, we prove that the optimal fixed regularization strength scales nearly linearly with the number of tasks $T$, specifically as $T/\\ln T$. To our knowledge, this is the first such result in theoretical continual learning. Finally, we validate our theoretical findings through experiments on linear regression and neural networks, illustrating how this scaling law affects generalization and offering a practical recipe for the design of continual learning systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 118,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13851",
        "abs_url": "https://arxiv.org/abs/2601.13851",
        "pdf_url": "https://arxiv.org/pdf/2601.13851",
        "title": "Inverting Self-Organizing Maps: A Unified Activation-Based Framework",
        "authors": [
            "Alessandro Londei",
            "Matteo Benati",
            "Denise Lanzieri",
            "Vittorio Loreto"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Self-Organizing Maps provide topology-preserving projections of high-dimensional data and have been widely used for visualization, clustering, and vector quantization. In this work, we show that the activation pattern of a SOM - the squared distances to its prototypes - can be inverted to recover the exact input under mild geometric conditions. This follows from a classical fact in Euclidean distance geometry: a point in $D$ dimensions is uniquely determined by its distances to $D{+}1$ affinely independent references. We derive the corresponding linear system and characterize the conditions under which the inversion is well-posed. Building upon this mechanism, we introduce the Manifold-Aware Unified SOM Inversion and Control (MUSIC) update rule, which enables controlled, semantically meaningful trajectories in latent space. MUSIC modifies squared distances to selected prototypes while preserving others, resulting in a deterministic geometric flow aligned with the SOM's piecewise-linear structure. Tikhonov regularization stabilizes the update rule and ensures smooth motion on high-dimensional datasets. Unlike variational or probabilistic generative models, MUSIC does not rely on sampling, latent priors, or encoder-decoder architectures. If no perturbation is applied, inversion recovers the exact input; when a target cluster or prototype is specified, MUSIC produces coherent semantic variations while remaining on the data manifold. This leads to a new perspective on data augmentation and controllable latent exploration based solely on prototype geometry. We validate the approach using synthetic Gaussian mixtures, the MNIST and the Faces in the Wild dataset. Across all settings, MUSIC produces smooth, interpretable trajectories that reveal the underlying geometry of the learned manifold, illustrating the advantages of SOM-based inversion over unsupervised clustering.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 119,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13892",
        "abs_url": "https://arxiv.org/abs/2601.13892",
        "pdf_url": "https://arxiv.org/pdf/2601.13892",
        "title": "Multi-Objective Hierarchical Optimization with Large Language Models",
        "authors": [
            "Andrej Schwanke",
            "Lyubomir Ivanov",
            "David Salinas",
            "Frank Hutter",
            "Arber Zela"
        ],
        "comments": "23 pages, 21 figures, 9 tables",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Despite their widespread adoption in various domains, especially due to their powerful reasoning capabilities, Large Language Models (LLMs) are not the off-the-shelf choice to drive multi-objective optimization yet. Conventional strategies rank high in benchmarks due to their intrinsic capabilities to handle numerical inputs and careful modelling choices that balance exploration and Pareto-front exploitation, as well as handle multiple (conflicting) objectives. In this paper, we close this gap by leveraging LLMs as surrogate models and candidate samplers inside a structured hierarchical search strategy. By adaptively partitioning the input space into disjoint hyperrectangular regions and ranking them with a composite score function, we restrict the generative process of the LLM to specific, high-potential sub-spaces, hence making the problem easier to solve as the LLM doesn't have to reason about the global structure of the problem, but only locally instead. We show that under standard regularity assumptions, our algorithm generates candidate solutions that converge to the true Pareto set in Hausdorff distance. Empirically, it consistently outperforms the global LLM-based multi-objective optimizer and is on par with standard evolutionary and Bayesian optimization algorithm on synthetic and real-world benchmarks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 120,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13897",
        "abs_url": "https://arxiv.org/abs/2601.13897",
        "pdf_url": "https://arxiv.org/pdf/2601.13897",
        "title": "TractRLFusion: A GPT-Based Multi-Critic Policy Fusion Framework for Fiber Tractography",
        "authors": [
            "Ankita Joshi",
            "Ashutosh Sharma",
            "Anoushkrit Goel",
            "Ranjeet Ranjan Jha",
            "Chirag Ahuja",
            "Arnav Bhavsar",
            "Aditya Nigam"
        ],
        "comments": "Accepted at 23rd IEEE International Symposium on Biomedical Imaging (ISBI), 2026",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Tractography plays a pivotal role in the non-invasive reconstruction of white matter fiber pathways, providing vital information on brain connectivity and supporting precise neurosurgical planning. Although traditional methods relied mainly on classical deterministic and probabilistic approaches, recent progress has benefited from supervised deep learning (DL) and deep reinforcement learning (DRL) to improve tract reconstruction. A persistent challenge in tractography is accurately reconstructing white matter tracts while minimizing spurious connections. To address this, we propose TractRLFusion, a novel GPT-based policy fusion framework that integrates multiple RL policies through a data-driven fusion strategy. Our method employs a two-stage training data selection process for effective policy fusion, followed by a multi-critic fine-tuning phase to enhance robustness and generalization. Experiments on HCP, ISMRM, and TractoInferno datasets demonstrate that TractRLFusion outperforms individual RL policies as well as state-of-the-art classical and DRL methods in accuracy and anatomical reliability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 121,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13953",
        "abs_url": "https://arxiv.org/abs/2601.13953",
        "pdf_url": "https://arxiv.org/pdf/2601.13953",
        "title": "Differentiable Logic Synthesis: Spectral Coefficient Selection via Sinkhorn-Constrained Composition",
        "authors": [
            "Gorgi Pavlov"
        ],
        "comments": "35 pages, 22 figures. Code available at this https URL",
        "subjects": "Machine Learning (cs.LG); Hardware Architecture (cs.AR); Logic in Computer Science (cs.LO)",
        "abstract": "Learning precise Boolean logic via gradient descent remains challenging: neural networks typically converge to \"fuzzy\" approximations that degrade under quantization. We introduce Hierarchical Spectral Composition, a differentiable architecture that selects spectral coefficients from a frozen Boolean Fourier basis and composes them via Sinkhorn-constrained routing with column-sign modulation. Our approach draws on recent insights from Manifold-Constrained Hyper-Connections (mHC), which demonstrated that projecting routing matrices onto the Birkhoff polytope preserves identity mappings and stabilizes large-scale training. We adapt this framework to logic synthesis, adding column-sign modulation to enable Boolean negation -- a capability absent in standard doubly stochastic routing. We validate our approach across four phases of increasing complexity: (1) For n=2 (16 Boolean operations over 4-dim basis), gradient descent achieves 100% accuracy with zero routing drift and zero-loss quantization to ternary masks. (2) For n=3 (10 three-variable operations), gradient descent achieves 76% accuracy, but exhaustive enumeration over 3^8 = 6561 configurations proves that optimal ternary masks exist for all operations (100% accuracy, 39% sparsity). (3) For n=4 (10 four-variable operations over 16-dim basis), spectral synthesis -- combining exact Walsh-Hadamard coefficients, ternary quantization, and MCMC refinement with parallel tempering -- achieves 100% accuracy on all operations. This progression establishes (a) that ternary polynomial threshold representations exist for all tested functions, and (b) that finding them requires methods beyond pure gradient descent as dimensionality grows. All operations enable single-cycle combinational logic inference at 10,959 MOps/s on GPU, demonstrating viability for hardware-efficient neuro-symbolic logic synthesis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 122,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2502.20966",
        "abs_url": "https://arxiv.org/abs/2502.20966",
        "pdf_url": "https://arxiv.org/pdf/2502.20966",
        "title": "Post-Hoc Uncertainty Quantification in Pre-Trained Neural Networks via Activation-Level Gaussian Processes",
        "authors": [
            "Richard Bergna",
            "Stefan Depeweg",
            "Sergio Calvo Ordonez",
            "Jonathan Plenk",
            "Alvaro Cartea",
            "Jose Miguel Hernandez-Lobato"
        ],
        "comments": "10 pages, 8 figures, 7th Symposium on Advances in Approximate Bayesian Inference",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Statistics Theory (math.ST)",
        "abstract": "Uncertainty quantification in neural networks through methods such as Dropout, Bayesian neural networks and Laplace approximations is either prone to underfitting or computationally demanding, rendering these approaches impractical for large-scale datasets. In this work, we address these shortcomings by shifting the focus from uncertainty in the weight space to uncertainty at the activation level, via Gaussian processes. More specifically, we introduce the Gaussian Process Activation function (GAPA) to capture neuron-level uncertainties. Our approach operates in a post-hoc manner, preserving the original mean predictions of the pre-trained neural network and thereby avoiding the underfitting issues commonly encountered in previous methods. We propose two methods. The first, GAPA-Free, employs empirical kernel learning from the training data for the hyperparameters and is highly efficient during training. The second, GAPA-Variational, learns the hyperparameters via gradient descent on the kernels, thus affording greater flexibility. Empirical results demonstrate that GAPA-Variational outperforms the Laplace approximation on most datasets in at least one of the uncertainty quantification metrics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 123,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11550",
        "abs_url": "https://arxiv.org/abs/2601.11550",
        "pdf_url": "https://arxiv.org/pdf/2601.11550",
        "title": "Uniqueness ratio as a predictor of a privacy leakage",
        "authors": [
            "Danah A. AlSalem AlKhashti"
        ],
        "comments": "",
        "subjects": "Databases (cs.DB); Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
        "abstract": "Identity leakage can emerge when independent databases are joined, even when each dataset is anonymized individually. While previous work focuses on post-join detection or complex privacy models, little attention has been given to simple, interpretable pre-join indicators that can warn data engineers and database administrators before integration occurs. This study investigates the uniqueness ratio of candidate join attributes as an early predictor of re-identification risk. Using synthetic multi-table datasets, we compute the uniqueness ratio of attribute combinations within each database and examine how these ratios correlate with identity exposure after the join. Experimental results show a strong relationship between high pre-join uniqueness and increased post-join leakage, measured by the proportion of records that become uniquely identifiable or fall into very small groups. Our findings demonstrate that uniqueness ratio offers an explainable and practical signal for assessing join induced privacy risk, providing a foundation for developing more comprehensive pre-join risk estimation models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 124,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11560",
        "abs_url": "https://arxiv.org/abs/2601.11560",
        "pdf_url": "https://arxiv.org/pdf/2601.11560",
        "title": "DeepEvidence: Empowering Biomedical Discovery with Deep Knowledge Graph Research",
        "authors": [
            "Zifeng Wang",
            "Zheng Chen",
            "Ziwei Yang",
            "Xuan Wang",
            "Qiao Jin",
            "Yifan Peng",
            "Zhiyong Lu",
            "Jimeng Sun"
        ],
        "comments": "",
        "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Biomedical knowledge graphs (KGs) encode vast, heterogeneous information spanning literature, genes, pathways, drugs, diseases, and clinical trials, but leveraging them collectively for scientific discovery remains difficult. Their structural differences, continual evolution, and limited cross-resource alignment require substantial manual integration, limiting the depth and scale of knowledge exploration. We introduce DeepEvidence, an AI-agent framework designed to perform Deep Research across various heterogeneous biomedical KGs. Unlike generic Deep Research systems that rely primarily on internet-scale text, DeepEvidence incorporates specialized knowledge-graph tooling and coordinated exploration strategies to systematically bridge heterogeneous resources. At its core is an orchestrator that directs two complementary agents: Breadth-First ReSearch (BFRS) for broad, multi-graph entity search, and Depth-First ReSearch (DFRS) for multi-hop, evidence-focused reasoning. An internal, incrementally built evidence graph provides a structured record of retrieved entities, relations, and supporting evidence. To operate at scale, DeepEvidence includes unified interfaces for querying diverse biomedical APIs and an execution sandbox that enables programmatic data retrieval, extraction, and analysis. Across established deep-reasoning benchmarks and four key stages of the biomedical discovery lifecycle: drug discovery, pre-clinical experimentation, clinical trial development, and evidence-based medicine, DeepEvidence demonstrates substantial gains in systematic exploration and evidence synthesis. These results highlight the potential of knowledge-graph-driven Deep Research to accelerate biomedical discovery.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 125,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11594",
        "abs_url": "https://arxiv.org/abs/2601.11594",
        "pdf_url": "https://arxiv.org/pdf/2601.11594",
        "title": "Multi-Scale Negative Coupled Information Systems (MNCIS): A Unified Spectral Topology Framework for Stability in Turbulence, AI, and Biology",
        "authors": [
            "Pengyue Hou"
        ],
        "comments": "Includes supplementary materials and code. Foundation and mathematical proofs can be found in the companion paper arXiv:2601.00638",
        "subjects": "Computational Physics (physics.comp-ph); Machine Learning (cs.LG); Adaptation and Self-Organizing Systems (nlin.AO); Biological Physics (physics.bio-ph)",
        "abstract": "Complex dynamical systems frequently encounter a recurrent structural instability: the collapse of the spectral gap, driving the system toward a low-dimensional \"Zero-Mode Attractor\" (e.g., spectral pile-up or over-smoothing). Building upon recent global well-posedness estimates [Hou, arXiv:2601.00638], this work generalizes the Multi-Scale Negative Coupled Information System (MNCIS) framework. We postulate that global stability requires an active topological operator -- Adaptive Spectral Negative Coupling (ASNC) -- functioning as a state-dependent high-pass filter that penalizes entropy accumulation at spectral boundaries. We validate this unified framework via three implementations:(1) Hydrodynamics: In 3D Navier-Stokes turbulence ($N=256^3$), ASNC acts as a global-enstrophy adaptive sub-grid scale (SGS) model, stabilizing the inviscid limit and preserving the Kolmogorov $-5/3$ inertial range without artificial hyper-viscosity.(2) Artificial Intelligence: Addressing Over-smoothing in Graph Neural Networks (GNNs), we implement ASNC as a parameter-free topological constraint. Unlike baselines (e.g., DeepGCNs) relying on dense residual connections to bypass signal decay, our framework enables the training of ultra-deep 64-layer networks without residual connections, maintaining perfectly stationary feature variance ($\\sigma^2 \\equiv 1.0$) on the ogbn-arxiv benchmark. (3) Biological Physics: In reaction-diffusion morphogenesis, it stabilizes Turing patterns against diffusive washout in high-entropy regimes. Our results suggest that the MNCIS framework provides a base-independent topological condition for distinguishing viable complex systems from those collapsing into thermal equilibrium, bridging physical stability and information persistence.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 126,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11595",
        "abs_url": "https://arxiv.org/abs/2601.11595",
        "pdf_url": "https://arxiv.org/pdf/2601.11595",
        "title": "Enhancing Model Context Protocol (MCP) with Context-Aware Server Collaboration",
        "authors": [
            "Meenakshi Amulya Jayanti",
            "X.Y. Han"
        ],
        "comments": "",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG); Software Engineering (cs.SE)",
        "abstract": "The Model Context Protocol (MCP) has emerged as a widely used framework for enabling LLM-based agents to communicate with external tools and services. The most common implementation of MCP, proposed by Anthropic, heavily relies on a Large Language Model (LLM) to decompose tasks and issue instructions to servers, which act as stateless executors. In particular, the agents, models, and servers are stateless and do not have access to a global context. However, in tasks involving LLM-driven coordination, it is natural that a Shared Context Store (SCS) could improve the efficiency and coherence of multi-agent workflows by reducing redundancy and enabling knowledge transfer between servers. Thus, in this work, we design and assess the performance of a Context-Aware MCP (CA-MCP) that offloads execution logic to specialized MCP servers that read from and write to a shared context memory, allowing them to coordinate more autonomously in real time. In this design, context management serves as the central mechanism that maintains continuity across task executions by tracking intermediate states and shared variables, thereby enabling persistent collaboration among agents without repeated prompting. We present experiments showing that the CA-MCP can outperform the traditional MCP by reducing the number of LLM calls required for complex tasks and decreasing the frequency of response failures when task conditions are not satisfied, thereby improving overall efficiency and responsiveness. In particular, we conducted experiments on the TravelPlanner and REALM-Bench benchmark datasets and observed statistically significant results indicating the potential advantages of incorporating a shared context store via CA-MCP in LLM-driven multi-agent systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 127,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11614",
        "abs_url": "https://arxiv.org/abs/2601.11614",
        "pdf_url": "https://arxiv.org/pdf/2601.11614",
        "title": "Multi-modal MRI-Based Alzheimer's Disease Diagnosis with Transformer-based Image Synthesis and Transfer Learning",
        "authors": [
            "Jason Qiu"
        ],
        "comments": "19 pages, 10 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)",
        "abstract": "Alzheimer's disease (AD) is a progressive neurodegenerative disorder in which pathological changes begin many years before the onset of clinical symptoms, making early detection essential for timely intervention. T1-weighted (T1w) Magnetic Resonance Imaging (MRI) is routinely used in clinical practice to identify macroscopic brain alterations, but these changes typically emerge relatively late in the disease course. Diffusion MRI (dMRI), in contrast, is sensitive to earlier microstructural abnormalities by probing water diffusion in brain tissue. dMRI metrics, including fractional anisotropy (FA) and mean diffusivity (MD), provide complementary information about white matter integrity and neurodegeneration. However, dMRI acquisitions are time-consuming and susceptible to motion artifacts, limiting their routine use in clinical populations. To bridge this gap, I propose a 3D TransUNet image synthesis framework that predicts FA and MD maps directly from T1w MRI. My model generates high-fidelity maps, achieving a structural similarity index (SSIM) exceeding 0.93 and a strong Pearson correlation (>0.94) with ground-truth dMRI. When integrated into a multi-modal diagnostic model, these synthetic features boost AD classification accuracy by 5% (78.75%->83.75%) and, most importantly, improve mild cognitive impairment (MCI) detection by 12.5%. This study demonstrates that high-quality diffusion microstructural information can be inferred from routinely acquired T1w MRI, effectively transferring the benefits of multi-modality imaging to settings where diffusion data are unavailable. By reducing scan time while preserving complementary structural and microstructural information, the proposed approach has the potential to improve the accessibility, efficiency, and accuracy of AD diagnosis in clinical practice.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 128,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11625",
        "abs_url": "https://arxiv.org/abs/2601.11625",
        "pdf_url": "https://arxiv.org/pdf/2601.11625",
        "title": "Reasoning Stabilization Point: A Training-Time Signal for Stable Evidence and Shortcut Reliance",
        "authors": [
            "Sahil Rajesh Dhayalkar"
        ],
        "comments": "8 pages, Submitted to ACL Rolling Review and is under review",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Fine-tuning pretrained language models can improve task performance while subtly altering the evidence a model relies on. We propose a training-time interpretability view that tracks token-level attributions across finetuning epochs. We define explanation driftas the epoch-to-epoch change in normalized token attributions on a fixed probe set, and introduce the Reasoning Stabilization Point(RSP), the earliest epoch after which drift remains consistently low. RSP is computed from within-run drift dynamics and requires no tuning on out-of-distribution data. Across multiple lightweight transformer classifiers and benchmark classification tasks, drift typically collapses into a low, stable regime early in training, while validation accuracy continues to change only marginally. In a controlled shortcut setting with label-correlated trigger tokens, attribution dynamics expose increasing reliance on the shortcut even when validation accuracy remains competitive. Overall, explanation drift provides a simple, low-cost diagnostic for monitoring how decision evidence evolves during fine-tuning and for selecting checkpoints in a stable-evidence regime.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 129,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11626",
        "abs_url": "https://arxiv.org/abs/2601.11626",
        "pdf_url": "https://arxiv.org/pdf/2601.11626",
        "title": "Concatenated Matrix SVD: Compression Bounds, Incremental Approximation, and Error-Constrained Clustering",
        "authors": [
            "Maksym Shamrai"
        ],
        "comments": "",
        "subjects": "Numerical Analysis (math.NA); Machine Learning (cs.LG)",
        "abstract": "Large collections of matrices arise throughout modern machine learning, signal processing, and scientific computing, where they are commonly compressed by concatenation followed by truncated singular value decomposition (SVD). This strategy enables parameter sharing and efficient reconstruction and has been widely adopted across domains ranging from multi-view learning and signal processing to neural network compression. However, it leaves a fundamental question unanswered: which matrices can be safely concatenated and compressed together under explicit reconstruction error constraints? Existing approaches rely on heuristic or architecture-specific grouping and provide no principled guarantees on the resulting SVD approximation error. In the present work, we introduce a theory-driven framework for compression-aware clustering of matrices under SVD compression constraints. Our analysis establishes new spectral bounds for horizontally concatenated matrices, deriving global upper bounds on the optimal rank-$r$ SVD reconstruction error from lower bounds on singular value growth. The first bound follows from Weyl-type monotonicity under blockwise extensions, while the second leverages singular values of incremental residuals to yield tighter, per-block guarantees. We further develop an efficient approximate estimator based on incremental truncated SVD that tracks dominant singular values without forming the full concatenated matrix. Therefore, we propose three clustering algorithms that merge matrices only when their predicted joint SVD compression error remains below a user-specified threshold. The algorithms span a trade-off between speed, provable accuracy, and scalability, enabling compression-aware clustering with explicit error control. Code is available online.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 130,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11629",
        "abs_url": "https://arxiv.org/abs/2601.11629",
        "pdf_url": "https://arxiv.org/pdf/2601.11629",
        "title": "Semantic Differentiation for Tackling Challenges in Watermarking Low-Entropy Constrained Generation Outputs",
        "authors": [
            "Nghia T. Le",
            "Alan Ritter",
            "Kartik Goyal"
        ],
        "comments": "18 pages, 4 figures",
        "subjects": "Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
        "abstract": "We demonstrate that while the current approaches for language model watermarking are effective for open-ended generation, they are inadequate at watermarking LM outputs for constrained generation tasks with low-entropy output spaces. Therefore, we devise SeqMark, a sequence-level watermarking algorithm with semantic differentiation that balances the output quality, watermark detectability, and imperceptibility. It improves on the shortcomings of the prevalent token-level watermarking algorithms that cause under-utilization of the sequence-level entropy available for constrained generation tasks. Moreover, we identify and improve upon a different failure mode we term region collapse, associated with prior sequence-level watermarking algorithms. This occurs because the pseudorandom partitioning of semantic space for watermarking in these approaches causes all high-probability outputs to collapse into either invalid or valid regions, leading to a trade-off in output quality and watermarking effectiveness. SeqMark instead, differentiates the high-probable output subspace and partitions it into valid and invalid regions, ensuring the even spread of high-quality outputs among all the regions. On various constrained generation tasks like machine translation, code generation, and abstractive summarization, SeqMark substantially improves watermark detection accuracy (up to 28% increase in F1) while maintaining high generation quality.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 131,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11642",
        "abs_url": "https://arxiv.org/abs/2601.11642",
        "pdf_url": "https://arxiv.org/pdf/2601.11642",
        "title": "PSSF: Early osteoarthritis detection using physical synthetic knee X-ray scans and AI radiomics models",
        "authors": [
            "Abbas Alzubaidi",
            "Ali Al-Bayaty"
        ],
        "comments": "16 pages, 6 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Image and Video Processing (eess.IV)",
        "abstract": "Knee osteoarthritis (OA) is a major cause of disability worldwide and is still largely assessed using subjective radiographic grading, most commonly the Kellgren-Lawrence (KL) scale. Artificial intelligence (AI) and radiomics offer quantitative tools for OA assessment but depend on large, well-annotated image datasets, mainly X-ray scans, that are often difficult to obtain because of privacy, governance and resourcing constraints. In this research, we introduce a physics-based synthetic simulation framework (PSSF) to fully generate controllable X-ray scans without patients' involvement and violating their privacy and institutional constraints. This PSSF is a 2D X-ray projection simulator of anteroposterior knee radiographs from a parametric anatomical model of the distal femur and proximal tibia. Using PSSF, we create a virtual cohort of 180 subjects (260 knees), each is imaged under three protocols (reference, low-dose, and geometry-shift). Medial joint regions are automatically localized, preprocessed, and processed with the Image Biomarker Standardisation Initiative (IBSI). Practically, three machine learning (ML) models are utilized, logistic regression, random forest, and gradient boosting, to train binary (KL-like \"0\" vs. \"2\") and three-class (0-2) prediction radiographic images. Robustness is assessed within IBSI protocol, cross-protocol, and multi-protocol scenarios. Finally, features stability is then evaluated using intraclass correlation coefficients across acquisition changes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 132,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11647",
        "abs_url": "https://arxiv.org/abs/2601.11647",
        "pdf_url": "https://arxiv.org/pdf/2601.11647",
        "title": "Reinforcement Learning for Dynamic Workflow Optimization in CI/CD Pipelines",
        "authors": [
            "Aniket Abhishek Soni",
            "Milan Parikh",
            "Rashi Nimesh Kumar Dhenia",
            "Jubin Abhishek Soni",
            "Ayush Raj Jha",
            "Sneja Mitinbhai Shah"
        ],
        "comments": "Accepted and presented at CICN 2025 (International Conference on Computational Intelligence and Communication Networks). 7 pages, 5 figures",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Continuous Integration and Continuous Deployment (CI/CD) pipelines are central to modern software delivery, yet their static workflows often introduce inefficiencies as systems scale. This paper proposes a reinforcement learning (RL) based approach to dynamically optimize CI/CD pipeline workflows. The pipeline is modeled as a Markov Decision Process, and an RL agent is trained to make runtime decisions such as selecting full, partial, or no test execution in order to maximize throughput while minimizing testing overhead. A configurable CI/CD simulation environment is developed to evaluate the approach across build, test, and deploy stages. Experimental results show that the RL optimized pipeline achieves up to a 30 percent improvement in throughput and approximately a 25 percent reduction in test execution time compared to static baselines, while maintaining a defect miss rate below 5 percent. The agent learns to selectively skip or abbreviate tests for low risk commits, accelerating feedback cycles without significantly increasing failure risk. These results demonstrate the potential of reinforcement learning to enable adaptive and intelligent DevOps workflows, providing a practical pathway toward more efficient, resilient, and sustainable CI/CD automation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 133,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11653",
        "abs_url": "https://arxiv.org/abs/2601.11653",
        "pdf_url": "https://arxiv.org/pdf/2601.11653",
        "title": "AI Agents Need Memory Control Over More Context",
        "authors": [
            "Fouad Bousetouane"
        ],
        "comments": "32 pages, 7 figures",
        "subjects": "Neurons and Cognition (q-bio.NC); Machine Learning (cs.LG); Multiagent Systems (cs.MA)",
        "abstract": "AI agents are increasingly used in long, multi-turn workflows in both research and enterprise settings. As interactions grow, agent behavior often degrades due to loss of constraint focus, error accumulation, and memory-induced drift. This problem is especially visible in real-world deployments where context evolves, distractions are introduced, and decisions must remain consistent over time. A common practice is to equip agents with persistent memory through transcript replay or retrieval-based mechanisms. While convenient, these approaches introduce unbounded context growth and are vulnerable to noisy recall and memory poisoning, leading to unstable behavior and increased drift. In this work, we introduce the Agent Cognitive Compressor (ACC), a bio-inspired memory controller that replaces transcript replay with a bounded internal state updated online at each turn. ACC separates artifact recall from state commitment, enabling stable conditioning while preventing unverified content from becoming persistent memory. We evaluate ACC using an agent-judge-driven live evaluation framework that measures both task outcomes and memory-driven anomalies across extended interactions. Across scenarios spanning IT operations, cybersecurity response, and healthcare workflows, ACC consistently maintains bounded memory and exhibits more stable multi-turn behavior, with significantly lower hallucination and drift than transcript replay and retrieval-based agents. These results show that cognitive compression provides a practical and effective foundation for reliable memory control in long-horizon AI agents.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 134,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11659",
        "abs_url": "https://arxiv.org/abs/2601.11659",
        "pdf_url": "https://arxiv.org/pdf/2601.11659",
        "title": "The Llama 4 Herd: Architecture, Training, Evaluation, and Deployment Notes",
        "authors": [
            "Aaron Adcock",
            "Aayushi Srivastava",
            "Abhimanyu Dubey",
            "Abhinav Jauhri",
            "Abhinav Pande",
            "Abhinav Pandey",
            "Abhinav Sharma",
            "Abhishek Kadian",
            "Abhishek Kumawat",
            "Adam Kelsey",
            "Adam Stelle",
            "Adeel Cheema",
            "Adela Kabiljo",
            "Adina Katz",
            "Adithya Gangidi",
            "Aditya Tayade",
            "Adolfo Victoria",
            "Adrian Samatan Alastuey",
            "Adrien Conrath",
            "Afroz Mohiuddin",
            "Ahmed Sharif",
            "Ahnaf Siddiqui",
            "Ahuva Goldstand",
            "Aijung Li",
            "Aidan Boyd",
            "Aidin Kazemi Daliri",
            "Aisha Iqbal",
            "Ajay Menon",
            "Ajit Mathews",
            "Akhil Mathur",
            "Akshat Agarwal",
            "Alan Schelten",
            "Alana Shine",
            "Alejandro Castillejo Muñoz",
            "Aleksei Guliaev",
            "Alex Radovic",
            "Alex Song",
            "Alex Vaughan",
            "Alexander Simeonov",
            "Alexandre Rezende",
            "Alexandre Rezende",
            "Alexei Baevski",
            "Alexey Roubaud",
            "Allen Ma",
            "Alvin Lee",
            "Alyssa Pereira",
            "Aman Ahmed",
            "Aman Shankar",
            "Amanda Kallet",
            "Amar Budhiraja",
            "Ameya Khandekar",
            "Amine Benhalloum",
            "Amir Gershman",
            "Amit Nagpal",
            "Amit Zohar",
            "Amr Sharaf",
            "Anant Desai",
            "Anastasia Razdaibiedina",
            "Anca Agape",
            "Andranik Kurghinyan",
            "Andre Perunicic",
            "Andrea Madotto",
            "Andrei Darabanov",
            "Andrés Alvarado",
            "Andrew Brown",
            "Andrew Cohen",
            "Andrew Fang",
            "Andrew Freeman",
            "Andrew Gallagher",
            "Andrew Gu",
            "Andrew Prasetyo Jo",
            "Andrew Ryan",
            "Andrew Steffen",
            "Andrew Wei",
            "Andrey Rusakov",
            "Andrii Golovei",
            "Andy Shang",
            "Angela Fan",
            "Angela Fan",
            "Angela Flewellen",
            "Animesh Pathak",
            "Anirudh Goyal",
            "Ankit Ramchandani",
            "Ankur Pai",
            "Ankur Singh",
            "Ankush Garg",
            "Anlu Xing",
            "Anna Cai",
            "Anna Grosul",
            "Anna Prochowska",
            "Anna Sun",
            "Annie Dong",
            "Annie Franco",
            "Anqi Hu",
            "Anshul Chawla",
            "Anthony Hartshorn",
            "Antonia Sheng",
            "Antony Thomas",
            "Anuj Goyal",
            "Anusha De"
        ],
        "comments": "15 pages",
        "subjects": "Software Engineering (cs.SE); Machine Learning (cs.LG)",
        "abstract": "This document consolidates publicly reported technical details about Metas Llama 4 model family. It summarizes (i) released variants (Scout and Maverick) and the broader herd context including the previewed Behemoth teacher model, (ii) architectural characteristics beyond a high-level MoE description covering routed/shared-expert structure, early-fusion multimodality, and long-context design elements reported for Scout (iRoPE and length generalization strategies), (iii) training disclosures spanning pre-training, mid-training for long-context extension, and post-training methodology (lightweight SFT, online RL, and lightweight DPO) as described in release materials, (iv) developer-reported benchmark results for both base and instruction-tuned checkpoints, and (v) practical deployment constraints observed across major serving environments, including provider-specific context limits and quantization packaging. The manuscript also summarizes licensing obligations relevant to redistribution and derivative naming, and reviews publicly described safeguards and evaluation practices. The goal is to provide a compact technical reference for researchers and practitioners who need precise, source-backed facts about Llama 4.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 135,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11691",
        "abs_url": "https://arxiv.org/abs/2601.11691",
        "pdf_url": "https://arxiv.org/pdf/2601.11691",
        "title": "Explainable histomorphology-based survival prediction of glioblastoma, IDH-wildtype",
        "authors": [
            "Jan-Philipp Redlich",
            "Friedrich Feuerhake",
            "Stefan Nikolin",
            "Nadine Sarah Schaadt",
            "Sarah Teuber-Hanselmann",
            "Joachim Weis",
            "Sabine Luttmann",
            "Andrea Eberle",
            "Christoph Buck",
            "Timm Intemann",
            "Pascal Birnstill",
            "Klaus Kraywinkel",
            "Jonas Ort",
            "Peter Boor",
            "André Homeyer"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)",
        "abstract": "Glioblastoma, IDH-wildtype (GBM-IDHwt) is the most common malignant brain tumor. Histomorphology is a crucial component of the integrated diagnosis of GBM-IDHwt. Artificial intelligence (AI) methods have shown promise to extract additional prognostic information from histological whole-slide images (WSI) of hematoxylin and eosin-stained glioblastoma tissue. Here, we present an explainable AI-based method to support systematic interpretation of histomorphological features associated with survival. It combines an explainable multiple instance learning (MIL) architecture with a sparse autoencoder (SAE) to relate human-interpretable visual patterns of tissue to survival. The MIL architecture directly identifies prognosis-relevant image tiles and the SAE maps these tiles post-hoc to visual patterns. The MIL method was trained and evaluated using a new real-world dataset that comprised 720 GBM-IDHwt cases from three hospitals and four cancer registries in Germany. The SAE was trained using 1878 WSIs of glioblastoma from five independent public data collections. Despite the many factors influencing survival time, our method showed some ability to discriminate between patients living less than 180 days or more than 360 days solely based on histomorphology (AUC: 0.67; 95% CI: 0.63-0.72). Cox proportional hazards regression confirmed a significant difference in survival time between the predicted groups after adjustment for established prognostic factors (hazard ratio: 1.47; 95% CI: 1.26-1.72). Our method identified multiple interpretable visual patterns associated with survival. Three neuropathologists separately found that 21 of the 24 most strongly associated patterns could be clearly attributed to seven histomorphological categories. Necrosis and hemorrhage appeared to be associated with shorter survival while highly cellular tumor areas were associated with longer survival.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 136,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11700",
        "abs_url": "https://arxiv.org/abs/2601.11700",
        "pdf_url": "https://arxiv.org/pdf/2601.11700",
        "title": "Telling Human and Machine Handwriting Apart",
        "authors": [
            "Luis A. Leiva",
            "Moises Diaz",
            "Nuwan T. Attygalle",
            "Miguel A. Ferrer",
            "Rejean Plamondon"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Handwriting movements can be leveraged as a unique form of behavioral biometrics, to verify whether a real user is operating a device or application. This task can be framed as a reverse Turing test in which a computer has to detect if an input instance has been generated by a human or artificially. To tackle this task, we study ten public datasets of handwritten symbols (isolated characters, digits, gestures, pointing traces, and signatures) that are artificially reproduced using seven different synthesizers, including, among others, the Kinematic Theory (Sigma h model), generative adversarial networks, Transformers, and Diffusion models. We train a shallow recurrent neural network that achieves excellent performance (98.3 percent Area Under the ROC Curve (AUC) score and 1.4 percent equal error rate on average across all synthesizers and datasets) using nonfeaturized trajectory data as input. In few-shot settings, we show that our classifier achieves such an excellent performance when trained on just 10 percent of the data, as evaluated on the remaining 90% of the data as a test set. We further challenge our classifier in out-of-domain settings, and observe very competitive results as well. Our work has implications for computerized systems that need to verify human presence, and adds an additional layer of security to keep attackers at bay.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 137,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11716",
        "abs_url": "https://arxiv.org/abs/2601.11716",
        "pdf_url": "https://arxiv.org/pdf/2601.11716",
        "title": "AllShowers: One model for all calorimeter showers",
        "authors": [
            "Thorsten Buss",
            "Henry Day-Hall",
            "Frank Gaede",
            "Gregor Kasieczka",
            "Katja Krüger"
        ],
        "comments": "",
        "subjects": "Instrumentation and Detectors (physics.ins-det); Machine Learning (cs.LG); High Energy Physics - Experiment (hep-ex); High Energy Physics - Phenomenology (hep-ph)",
        "abstract": "Accurate and efficient detector simulation is essential for modern collider experiments. To reduce the high computational cost, various fast machine learning surrogate models have been proposed. Traditional surrogate models for calorimeter shower modeling train separate networks for each particle species, limiting scalability and reuse. We introduce AllShowers, a unified generative model that simulates calorimeter showers across multiple particle types using a single generative model. AllShowers is a continuous normalizing flow model with a Transformer architecture, enabling it to generate complex spatial and energy correlations in variable-length point cloud representations of showers. Trained on a diverse dataset of simulated showers in the highly granular ILD detector, the model demonstrates the ability to generate realistic showers for electrons, photons, and charged and neutral hadrons across a wide range of incident energies and angles without retraining. In addition to unifying shower generation for multiple particle types, AllShowers surpasses the fidelity of previous single-particle-type models for hadronic showers. Key innovations include the use of a layer embedding, allowing the model to learn all relevant calorimeter layer properties; a custom attention masking scheme to reduce computational demands and introduce a helpful inductive bias; and a shower- and layer-wise optimal transport mapping to improve training convergence and sample quality. AllShowers marks a significant step towards a universal model for calorimeter shower simulations in collider experiments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 138,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11723",
        "abs_url": "https://arxiv.org/abs/2601.11723",
        "pdf_url": "https://arxiv.org/pdf/2601.11723",
        "title": "A Proof of Concept for a Digital Twin of an Ultrasonic Fermentation System",
        "authors": [
            "Francesco Saverio Sconocchia Pisoni",
            "Andrea Vitaletti",
            "Davide Appolloni",
            "Federico Ortenzi",
            "Blasco Morozzo della Rocca",
            "Mariano José Guillén",
            "Alessandro Contaldo"
        ],
        "comments": "23 pages, submitted to the 22nd International Conference on Intelligent Environments (IE 2026)",
        "subjects": "Emerging Technologies (cs.ET); Machine Learning (cs.LG)",
        "abstract": "This paper presents the design and implementation of a proof of concept digital twin for an innovative ultrasonic-enhanced beer-fermentation system, developed to enable intelligent monitoring, prediction, and actuation in yeast-growth environments. A traditional fermentation tank is equipped with a piezoelectric transducer able to irradiate the tank with ultrasonic waves, providing an external abiotic stimulus to enhance the growth of yeast and accelerate the fermentation process. At its core, the digital twin incorporates a predictive model that estimates yeast's culture density over time based on the surrounding environmental conditions. To this end, we implement, tailor and extend the model proposed in Palacios et al., allowing us to effectively handle the limited number of available training samples by using temperature, ultrasonic frequency, and duty cycle as inputs. The results obtained along with the assessment of model performance demonstrate the feasibility of the proposed approach.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 139,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11746",
        "abs_url": "https://arxiv.org/abs/2601.11746",
        "pdf_url": "https://arxiv.org/pdf/2601.11746",
        "title": "LIME-LLM: Probing Models with Fluent Counterfactuals, Not Broken Text",
        "authors": [
            "George Mihaila",
            "Suleyman Olcay Polat",
            "Poli Nemkova",
            "Himanshu Sharma",
            "Namratha V. Urs",
            "Mark V. Albert"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Local explanation methods such as LIME (Ribeiro et al., 2016) remain fundamental to trustworthy AI, yet their application to NLP is limited by a reliance on random token masking. These heuristic perturbations frequently generate semantically invalid, out-of-distribution inputs that weaken the fidelity of local surrogate models. While recent generative approaches such as LLiMe (Angiulli et al., 2025b) attempt to mitigate this by employing Large Language Models for neighborhood generation, they rely on unconstrained paraphrasing that introduces confounding variables, making it difficult to isolate specific feature contributions. We introduce LIME-LLM, a framework that replaces random noise with hypothesis-driven, controlled perturbations. By enforcing a strict \"Single Mask-Single Sample\" protocol and employing distinct neutral infill and boundary infill strategies, LIME-LLM constructs fluent, on-manifold neighborhoods that rigorously isolate feature effects. We evaluate our method against established baselines (LIME, SHAP, Integrated Gradients) and the generative LLiMe baseline across three diverse benchmarks: CoLA, SST-2, and HateXplain using human-annotated rationales as ground truth. Empirical results demonstrate that LIME-LLM establishes a new benchmark for black-box NLP explainability, achieving significant improvements in local explanation fidelity compared to both traditional perturbation-based methods and recent generative alternatives.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 140,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11762",
        "abs_url": "https://arxiv.org/abs/2601.11762",
        "pdf_url": "https://arxiv.org/pdf/2601.11762",
        "title": "Industry-Aligned Granular Topic Modeling",
        "authors": [
            "Sae Young Moon",
            "Myeongjun Erik Jang",
            "Haoyan Luo",
            "Chunyang Xiao",
            "Antonios Georgiadis",
            "Fran Silavong"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Topic modeling has extensive applications in text mining and data analysis across various industrial sectors. Although the concept of granularity holds significant value for business applications by providing deeper insights, the capability of topic modeling methods to produce granular topics has not been thoroughly explored. In this context, this paper introduces a framework called TIDE, which primarily provides a novel granular topic modeling method based on large language models (LLMs) as a core feature, along with other useful functionalities for business applications, such as summarizing long documents, topic parenting, and distillation. Through extensive experiments on a variety of public and real-world business datasets, we demonstrate that TIDE's topic modeling approach outperforms modern topic modeling methods, and our auxiliary components provide valuable support for dealing with industrial business scenarios. The TIDE framework is currently undergoing the process of being open sourced.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 141,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11775",
        "abs_url": "https://arxiv.org/abs/2601.11775",
        "pdf_url": "https://arxiv.org/pdf/2601.11775",
        "title": "Quantum Kernel Machine Learning for Autonomous Materials Science",
        "authors": [
            "Felix Adams",
            "Daiwei Zhu",
            "David W. Steuerman",
            "A. Gilad Kusne",
            "Ichiro Takeuchi"
        ],
        "comments": "",
        "subjects": "Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG); Quantum Physics (quant-ph)",
        "abstract": "Autonomous materials science, where active learning is used to navigate large compositional phase space, has emerged as a powerful vehicle to rapidly explore new materials. A crucial aspect of autonomous materials science is exploring new materials using as little data as possible. Gaussian process-based active learning allows effective charting of multi-dimensional parameter space with a limited number of training data, and thus is a common algorithmic choice for autonomous materials science. An integral part of the autonomous workflow is the application of kernel functions for quantifying similarities among measured data points. A recent theoretical breakthrough has shown that quantum kernel models can achieve similar performance with less training data than classical models. This signals the possible advantage of applying quantum kernel machine learning to autonomous materials discovery. In this work, we compare quantum and classical kernels for their utility in sequential phase space navigation for autonomous materials science. Specifically, we compute a quantum kernel and several classical kernels for x-ray diffraction patterns taken from an Fe-Ga-Pd ternary composition spread library. We conduct our study on both IonQ's Aria trapped ion quantum computer hardware and the corresponding classical noisy simulator. We experimentally verify that a quantum kernel model can outperform some classical kernel models. The results highlight the potential of quantum kernel machine learning methods for accelerating materials discovery and suggest complex x-ray diffraction data is a candidate for robust quantum kernel model advantage.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 142,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11790",
        "abs_url": "https://arxiv.org/abs/2601.11790",
        "pdf_url": "https://arxiv.org/pdf/2601.11790",
        "title": "Gradient-based Active Learning with Gaussian Processes for Global Sensitivity Analysis",
        "authors": [
            "Guerlain Lambert",
            "Céline Helbert",
            "Claire Lauvernet"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Methodology (stat.ME)",
        "abstract": "Global sensitivity analysis of complex numerical simulators is often limited by the small number of model evaluations that can be afforded. In such settings, surrogate models built from a limited set of simulations can substantially reduce the computational burden, provided that the design of computer experiments is enriched efficiently. In this context, we propose an active learning approach that, for a fixed evaluation budget, targets the most informative regions of the input space to improve sensitivity analysis accuracy. More specifically, our method builds on recent advances in active learning for sensitivity analysis (Sobol' indices and derivative-based global sensitivity measures, DGSM) that exploit derivatives obtained from a Gaussian process (GP) surrogate. By leveraging the joint posterior distribution of the GP gradient, we develop acquisition functions that better account for correlations between partial derivatives and their impact on the response surface, leading to a more comprehensive and robust methodology than existing DGSM-oriented criteria. The proposed approach is first compared to state-of-the-art methods on standard benchmark functions, and is then applied to a real environmental model of pesticide transfers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 143,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11822",
        "abs_url": "https://arxiv.org/abs/2601.11822",
        "pdf_url": "https://arxiv.org/pdf/2601.11822",
        "title": "RAPID-Serve: Resource-efficient and Accelerated P/D Intra-GPU Disaggregation",
        "authors": [
            "Amna Masood",
            "Pratishtha Gaur",
            "Nuwan Jayasena"
        ],
        "comments": "",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)",
        "abstract": "Two widely adopted techniques for LLM inference serving systems today are hybrid batching and disaggregated serving. A hybrid batch combines prefill and decode tokens of different requests in the same batch to improve resource utilization and throughput at the cost of increased latency per token. In contrast, disaggregated serving decouples compute-bound prefill and bandwidth-bound decode phases to optimize for service level objectives (SLOs) at the cost of resource under-utilization and KV-cache transfer overheads. To address the limitations of these techniques, we propose RAPID-Serve: a technique to concurrently execute prefill and decode on the same GPU(s) to meet latency SLOs while maintaining high throughput and efficient resource utilization. Furthermore, we propose Adaptive Resource Management for runtime compute resource allocation, optionally leveraging CU masking (a fine-grained Compute Unit partitioning feature on AMD Instinct\\textsuperscript{TM} GPUs). RAPID-Serve provides up to 4.1x (average 1.7x) unconstrained throughput improvement and 32x and higher (average 4.9x) throughput improvement under SLO constraints, showing it as an effective strategy compared to the state-of-the-art approaches, particularly in resource-constrained environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 144,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11859",
        "abs_url": "https://arxiv.org/abs/2601.11859",
        "pdf_url": "https://arxiv.org/pdf/2601.11859",
        "title": "Cascaded Transformer for Robust and Scalable SLA Decomposition via Amortized Optimization",
        "authors": [
            "Cyril Shih-Huan Hsu"
        ],
        "comments": "",
        "subjects": "Networking and Internet Architecture (cs.NI); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)",
        "abstract": "The evolution toward 6G networks increasingly relies on network slicing to provide tailored, End-to-End (E2E) logical networks over shared physical infrastructures. A critical challenge is effectively decomposing E2E Service Level Agreements (SLAs) into domain-specific SLAs, which current solutions handle through computationally intensive, iterative optimization processes that incur substantial latency and complexity. To address this, we introduce Casformer, a cascaded Transformer architecture designed for fast, optimization-free SLA decomposition. Casformer leverages historical domain feedback encoded through domain-specific Transformer encoders in its first layer, and integrates cross-domain dependencies using a Transformer-based aggregator in its second layer. The model is trained under a learning paradigm inspired by Domain-Informed Neural Networks (DINNs), incorporating risk-informed modeling and amortized optimization to learn a stable, forward-only SLA decomposition policy. Extensive evaluations demonstrate that Casformer achieves improved SLA decomposition quality against state-of-the-art optimization-based frameworks, while exhibiting enhanced scalability and robustness under volatile and noisy network conditions. In addition, its forward-only design reduces runtime complexity and simplifies deployment and maintenance. These insights reveal the potential of combining amortized optimization with Transformer-based sequence modeling to advance network automation, providing a scalable and efficient solution suitable for real-time SLA management in advanced 5G-and-beyond network environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 145,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11860",
        "abs_url": "https://arxiv.org/abs/2601.11860",
        "pdf_url": "https://arxiv.org/pdf/2601.11860",
        "title": "Adversarial Drift-Aware Predictive Transfer: Toward Durable Clinical AI",
        "authors": [
            "Xin Xiong",
            "Zijian Guo",
            "Haobo Zhu",
            "Chuan Hong",
            "Jordan W Smoller",
            "Tianxi Cai",
            "Molei Liu"
        ],
        "comments": "",
        "subjects": "Applications (stat.AP); Machine Learning (cs.LG); Methodology (stat.ME)",
        "abstract": "Clinical AI systems frequently suffer performance decay post-deployment due to temporal data shifts, such as evolving populations, diagnostic coding updates (e.g., ICD-9 to ICD-10), and systemic shocks like the COVID-19 pandemic. Addressing this ``aging'' effect via frequent retraining is often impractical due to computational costs and privacy constraints. To overcome these hurdles, we introduce Adversarial Drift-Aware Predictive Transfer (ADAPT), a novel framework designed to confer durability against temporal drift with minimal retraining. ADAPT innovatively constructs an uncertainty set of plausible future models by combining historical source models and limited current data. By optimizing worst-case performance over this set, it balances current accuracy with robustness against degradation due to future drifts. Crucially, ADAPT requires only summary-level model estimators from historical periods, preserving data privacy and ensuring operational simplicity. Validated on longitudinal suicide risk prediction using electronic health records from Mass General Brigham (2005--2021) and Duke University Health Systems, ADAPT demonstrated superior stability across coding transitions and pandemic-induced shifts. By minimizing annual performance decay without labeling or retraining future data, ADAPT offers a scalable pathway for sustaining reliable AI in high-stakes healthcare environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 146,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11905",
        "abs_url": "https://arxiv.org/abs/2601.11905",
        "pdf_url": "https://arxiv.org/pdf/2601.11905",
        "title": "LIBRA: Language Model Informed Bandit Recourse Algorithm for Personalized Treatment Planning",
        "authors": [
            "Junyu Cao",
            "Ruijiang Gao",
            "Esmaeil Keyvanshokooh",
            "Jianhao Ma"
        ],
        "comments": "50 pages. Previous version with human-AI collaboration: arXiv:2410.14640",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Statistics Theory (math.ST)",
        "abstract": "We introduce a unified framework that seamlessly integrates algorithmic recourse, contextual bandits, and large language models (LLMs) to support sequential decision-making in high-stakes settings such as personalized medicine. We first introduce the recourse bandit problem, where a decision-maker must select both a treatment action and a feasible, minimal modification to mutable patient features. To address this problem, we develop the Generalized Linear Recourse Bandit (GLRB) algorithm. Building on this foundation, we propose LIBRA, a Language Model-Informed Bandit Recourse Algorithm that strategically combines domain knowledge from LLMs with the statistical rigor of bandit learning. LIBRA offers three key guarantees: (i) a warm-start guarantee, showing that LIBRA significantly reduces initial regret when LLM recommendations are near-optimal; (ii) an LLM-effort guarantee, proving that the algorithm consults the LLM only $O(\\log^2 T)$ times, where $T$ is the time horizon, ensuring long-term autonomy; and (iii) a robustness guarantee, showing that LIBRA never performs worse than a pure bandit algorithm even when the LLM is unreliable. We further establish matching lower bounds that characterize the fundamental difficulty of the recourse bandit problem and demonstrate the near-optimality of our algorithms. Experiments on synthetic environments and a real hypertension-management case study confirm that GLRB and LIBRA improve regret, treatment quality, and sample efficiency compared with standard contextual bandits and LLM-only benchmarks. Our results highlight the promise of recourse-aware, LLM-assisted bandit algorithms for trustworthy LLM-bandits collaboration in personalized high-stakes decision-making.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 147,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11918",
        "abs_url": "https://arxiv.org/abs/2601.11918",
        "pdf_url": "https://arxiv.org/pdf/2601.11918",
        "title": "Effects of Gabor Filters on Classification Performance of CNNs Trained on a Limited Number of Conditions",
        "authors": [
            "Akito Morita",
            "Hirotsugu Okuno"
        ],
        "comments": "5 pages, 4 figures, 4 tables",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "In this study, we propose a technique to improve the accuracy and reduce the size of convolutional neural networks (CNNs) running on edge devices for real-world robot vision applications. CNNs running on edge devices must have a small architecture, and CNNs for robot vision applications involving on-site object recognition must be able to be trained efficiently to identify specific visual targets from data obtained under a limited variation of conditions. The visual nervous system (VNS) is a good example that meets the above requirements because it learns from few visual experiences. Therefore, we used a Gabor filter, a model of the feature extractor of the VNS, as a preprocessor for CNNs to investigate the accuracy of the CNNs trained with small amounts of data. To evaluate how well CNNs trained on image data acquired under a limited variation of conditions generalize to data acquired under other conditions, we created an image dataset consisting of images acquired from different camera positions, and investigated the accuracy of the CNNs that trained using images acquired at a certain distance. The results were compared after training on multiple CNN architectures with and without Gabor filters as preprocessing. The results showed that preprocessing with Gabor filters improves the generalization performance of CNNs and contributes to reducing the size of CNNs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 148,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11926",
        "abs_url": "https://arxiv.org/abs/2601.11926",
        "pdf_url": "https://arxiv.org/pdf/2601.11926",
        "title": "Harmonica: A Self-Adaptation Exemplar for Sustainable MLOps",
        "authors": [
            "Ananya Halgatti",
            "Shaunak Biswas",
            "Hiya Bhatt",
            "Srinivasan Rakhunathan",
            "Karthik Vaidhyanathan"
        ],
        "comments": "This paper has been accepted to SEAMS 2026 Artifact Track",
        "subjects": "Software Engineering (cs.SE); Machine Learning (cs.LG)",
        "abstract": "Machine learning enabled systems (MLS) often operate in settings where they regularly encounter uncertainties arising from changes in their surrounding environment. Without structured oversight, such changes can degrade model behavior, increase operational cost, and reduce the usefulness of deployed systems. Although Machine Learning Operations (MLOps) streamlines the lifecycle of ML models, it provides limited support for addressing runtime uncertainties that influence the longer term sustainability of MLS. To support continued viability, these systems need a mechanism that detects when execution drifts outside acceptable bounds and adjusts system behavior in response. Despite the growing interest in sustainable and self-adaptive MLS, there has been limited work towards exemplars that allow researchers to study these challenges in MLOps pipelines. This paper presents Harmonica, a self-adaptation exemplar built on the HarmonE approach, designed to enable the sustainable operation of such pipelines. Harmonica introduces structured adaptive control through MAPE-K loop, separating high-level adaptation policy from low-level tactic execution. It continuously monitors sustainability metrics, evaluates them against dynamic adaptation boundaries, and automatically triggers architectural tactics when thresholds are violated. We demonstrate the tool through case studies in time series regression and computer vision, examining its ability to improve system stability and reduce manual intervention. The results show that Harmonica offers a practical and reusable foundation for enabling adaptive behavior in MLS that rely on MLOps pipelines for sustained operation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 149,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11937",
        "abs_url": "https://arxiv.org/abs/2601.11937",
        "pdf_url": "https://arxiv.org/pdf/2601.11937",
        "title": "Impact of Circuit Depth versus Qubit Count on Variational Quantum Classifiers for Higgs Boson Signal Detection",
        "authors": [
            "Fatih Maulana"
        ],
        "comments": "13 Pages, 5 Figures, Code and Data Available at: this https URL",
        "subjects": "Quantum Physics (quant-ph); Machine Learning (cs.LG); High Energy Physics - Experiment (hep-ex)",
        "abstract": "High-Energy Physics (HEP) experiments, such as those at the Large Hadron Collider (LHC), generate massive datasets that challenge classical computational limits. Quantum Machine Learning (QML) offers a potential advantage in processing high-dimensional data; however, finding the optimal architecture for current Noisy Intermediate-Scale Quantum (NISQ) devices remains an open challenge. This study investigates the performance of Variational Quantum Classifiers (VQC) in detecting Higgs Boson signals using the ATLAS Higgs Boson Machine Learning Challenge 2014 experiment dataset. We implemented a dimensionality reduction pipeline using Principal Component Analysis (PCA) to map 30 physical features into 4-qubit and 8-qubit latent spaces. We benchmarked three configurations: (A) a shallow 4-qubit circuit, (B) a deep 4-qubit circuit with increased entanglement layers, and (C) an expanded 8-qubit circuit. Experimental results demonstrate that increasing circuit depth significantly improves performance, yielding the highest accuracy of 56.2% (Configuration B), compared to a baseline of 51.9%. Conversely, simply scaling to 8 qubits resulted in a performance degradation to 50.6% due to optimization challenges associated with Barren Plateaus in the larger Hilbert space. These findings suggest that for near-term quantum hardware, prioritizing circuit depth and entanglement capability is more critical than increasing qubit count for effective anomaly detection in HEP data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 150,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11995",
        "abs_url": "https://arxiv.org/abs/2601.11995",
        "pdf_url": "https://arxiv.org/pdf/2601.11995",
        "title": "Learning Audio-Visual Embeddings with Inferred Latent Interaction Graphs",
        "authors": [
            "Donghuo Zeng",
            "Hao Niu",
            "Yanan Wang",
            "Masato Taya"
        ],
        "comments": "16 pages, 5 figures, 2 tables",
        "subjects": "Multimedia (cs.MM); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG); Sound (cs.SD)",
        "abstract": "Learning robust audio-visual embeddings requires bringing genuinely related audio and visual signals together while filtering out incidental co-occurrences - background noise, unrelated elements, or unannotated events. Most contrastive and triplet-loss methods use sparse annotated labels per clip and treat any co-occurrence as semantic similarity. For example, a video labeled \"train\" might also contain motorcycle audio and visual, because \"motorcycle\" is not the chosen annotation; standard methods treat these co-occurrences as negatives to true motorcycle anchors elsewhere, creating false negatives and missing true cross-modal dependencies. We propose a framework that leverages soft-label predictions and inferred latent interactions to address these issues: (1) Audio-Visual Semantic Alignment Loss (AV-SAL) trains a teacher network to produce aligned soft-label distributions across modalities, assigning nonzero probability to co-occurring but unannotated events and enriching the supervision signal. (2) Inferred Latent Interaction Graph (ILI) applies the GRaSP algorithm to teacher soft labels to infer a sparse, directed dependency graph among classes. This graph highlights directional dependencies (e.g., \"Train (visual)\" -> \"Motorcycle (audio)\") that expose likely semantic or conditional relationships between classes; these are interpreted as estimated dependency patterns. (3) Latent Interaction Regularizer (LIR): A student network is trained with both metric loss and a regularizer guided by the ILI graph, pulling together embeddings of dependency-linked but unlabeled pairs in proportion to their soft-label probabilities. Experiments on AVE and VEGAS benchmarks show consistent improvements in mean average precision (mAP), demonstrating that integrating inferred latent interactions into embedding learning enhances robustness and semantic coherence.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 151,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.11996",
        "abs_url": "https://arxiv.org/abs/2601.11996",
        "pdf_url": "https://arxiv.org/pdf/2601.11996",
        "title": "MongoDB Injection Query Classification Model using MongoDB Log files as Training Data",
        "authors": [
            "Shaunak Perni",
            "Minal Shirodkar",
            "Ramdas Karmalli"
        ],
        "comments": "24 Pages, 5 Tables, 6 Figures, Journal",
        "subjects": "Cryptography and Security (cs.CR); Databases (cs.DB); Machine Learning (cs.LG)",
        "abstract": "NoSQL Injection attacks are a class of cybersecurity attacks where an attacker sends a specifically engineered query to a NoSQL database which then performs an unauthorized operation. To defend against such attacks, rule based systems were initially developed but then were found to be ineffective to innovative injection attacks hence a model based approach was developed. Most model based detection systems, during testing gave exponentially positive results but were trained only on the query statement sent to the server. However due to the scarcity of data and class imbalances these model based systems were found to be not effective against all attacks in the real world. This paper explores classifying NoSQL injection attacks sent to a MongoDB server based on Log Data, and other extracted features excluding raw query statements. The log data was collected from a simulated attack on an empty MongoDB server which was then processed and explored. A discriminant analysis was carried out to determine statistically significant features to discriminate between injection and benign queries resulting in a dataset of significant features. Several Machine learning based classification models using an AutoML library, \"FLAML\", as well as 6 manually programmed models were trained on this dataset , which were then trained on 50 randomized samples of data, cross validated and evaluated. The study found that the best model was the \"FLAML\" library's \"XGBoost limited depth\" model with an accuracy of 71%.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 152,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12023",
        "abs_url": "https://arxiv.org/abs/2601.12023",
        "pdf_url": "https://arxiv.org/pdf/2601.12023",
        "title": "A Kernel Approach for Semi-implicit Variational Inference",
        "authors": [
            "Longlin Yu",
            "Ziheng Cheng",
            "Shiyue Zhang",
            "Cheng Zhang"
        ],
        "comments": "40 pages, 15 figures. arXiv admin note: substantial text overlap with arXiv:2405.18997",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Semi-implicit variational inference (SIVI) enhances the expressiveness of variational families through hierarchical semi-implicit distributions, but the intractability of their densities makes standard ELBO-based optimization biased. Recent score-matching approaches to SIVI (SIVI-SM) address this issue via a minimax formulation, at the expense of an additional lower-level optimization problem. In this paper, we propose kernel semi-implicit variational inference (KSIVI), a principled and tractable alternative that eliminates the lower-level optimization by leveraging kernel methods. We show that when optimizing over a reproducing kernel Hilbert space, the lower-level problem admits an explicit solution, reducing the objective to the kernel Stein discrepancy (KSD). Exploiting the hierarchical structure of semi-implicit distributions, the resulting KSD objective can be efficiently optimized using stochastic gradient methods. We establish optimization guarantees via variance bounds on Monte Carlo gradient estimators and derive statistical generalization bounds of order $\\tilde{\\mathcal{O}}(1/\\sqrt{n})$. We further introduce a multi-layer hierarchical extension that improves expressiveness while preserving tractability. Empirical results on synthetic and real-world Bayesian inference tasks demonstrate the effectiveness of KSIVI.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 153,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12032",
        "abs_url": "https://arxiv.org/abs/2601.12032",
        "pdf_url": "https://arxiv.org/pdf/2601.12032",
        "title": "Speaking to Silicon: Neural Communication with Bitcoin Mining ASICs",
        "authors": [
            "Francisco Angulo de Lafuente",
            "Vladimir Veselov",
            "Richard Goodman"
        ],
        "comments": "13 pages, 6 figures, 15 tables. Machine-checked Lean 4 proofs available at this https URL. Validated across Antminer S9, Lucky Miner LV06, and Goldshell LB-Box platforms",
        "subjects": "Neural and Evolutionary Computing (cs.NE); Hardware Architecture (cs.AR); Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
        "abstract": "This definitive research memoria presents a comprehensive, mathematically verified paradigm for neural communication with Bitcoin mining Application-Specific Integrated Circuits (ASICs), integrating five complementary frameworks: thermodynamic reservoir computing, hierarchical number system theory, algorithmic analysis, network latency optimization, and machine-checked mathematical formalization. We establish that obsolete cryptocurrency mining hardware exhibits emergent computational properties enabling bidirectional information exchange between AI systems and silicon substrates. The research program demonstrates: (1) reservoir computing with NARMA-10 Normalized Root Mean Square Error (NRMSE) of 0.8661; (2) the Thermodynamic Probability Filter (TPF) achieving 92.19% theoretical energy reduction; (3) the Virtual Block Manager achieving +25% effective hashrate; and (4) hardware universality across multiple ASIC families including Antminer S9, Lucky Miner LV06, and Goldshell LB-Box. A significant contribution is the machine-checked mathematical formalization using Lean 4 and Mathlib, providing unambiguous definitions, machine-verified theorems, and reviewer-proof claims. Key theorems proven include: independence implies zero leakage, predictor beats baseline implies non-independence (the logical core of TPF), energy savings theoretical maximum, and Physical Unclonable Function (PUF) distinguishability witnesses. Vladimir Veselov's hierarchical number system theory explains why early-round information contains predictive power. This work establishes a new paradigm: treating ASICs not as passive computational substrates but as active conversational partners whose thermodynamic state encodes exploitable computational information.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 154,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12039",
        "abs_url": "https://arxiv.org/abs/2601.12039",
        "pdf_url": "https://arxiv.org/pdf/2601.12039",
        "title": "Nonlinear Dynamic Factor Analysis With a Transformer Network",
        "authors": [
            "Oliver Snellman"
        ],
        "comments": "Working paper. 88 pages, 57 figures, 14 tables. Earlier versions circulated as \"Nowcasting with a Transformer Network\" (first version: 26 Oct 2024)",
        "subjects": "Econometrics (econ.EM); Machine Learning (cs.LG)",
        "abstract": "The paper develops a Transformer architecture for estimating dynamic factors from multivariate time series data under flexible identification assumptions. Performance on small datasets is improved substantially by using a conventional factor model as prior information via a regularization term in the training objective. The results are interpreted with Attention matrices that quantify the relative importance of variables and their lags for the factor estimate. Time variation in Attention patterns can help detect regime switches and evaluate narratives. Monte Carlo experiments suggest that the Transformer is more accurate than the linear factor model, when the data deviate from linear-Gaussian assumptions. An empirical application uses the Transformer to construct a coincident index of U.S. real economic activity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 155,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12053",
        "abs_url": "https://arxiv.org/abs/2601.12053",
        "pdf_url": "https://arxiv.org/pdf/2601.12053",
        "title": "A New Strategy for Artificial Intelligence: Training Foundation Models Directly on Human Brain Data",
        "authors": [
            "Maël Donoso"
        ],
        "comments": "",
        "subjects": "Neurons and Cognition (q-bio.NC); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "While foundation models have achieved remarkable results across a diversity of domains, they still rely on human-generated data, such as text, as a fundamental source of knowledge. However, this data is ultimately the product of human brains, the filtered projection of a deeper neural complexity. In this paper, we explore a new strategy for artificial intelligence: moving beyond surface-level statistical regularities by training foundation models directly on human brain data. We hypothesize that neuroimaging data could open a window into elements of human cognition that are not accessible through observable actions, and argue that this additional knowledge could be used, alongside classical training data, to overcome some of the current limitations of foundation models. While previous research has demonstrated the possibility to train classical machine learning or deep learning models on neural patterns, this path remains largely unexplored for high-level cognitive functions. Here, we classify the current limitations of foundation models, as well as the promising brain regions and cognitive processes that could be leveraged to address them, along four levels: perception, valuation, execution, and integration. Then, we propose two methods that could be implemented to prioritize the use of limited neuroimaging data for strategically chosen, high-value steps in foundation model training: reinforcement learning from human brain (RLHB) and chain of thought from human brain (CoTHB). We also discuss the potential implications for agents, artificial general intelligence, and artificial superintelligence, as well as the ethical, social, and technical challenges and opportunities. We argue that brain-trained foundation models could represent a realistic and effective middle ground between continuing to scale current architectures and exploring alternative, neuroscience-inspired solutions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 156,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12117",
        "abs_url": "https://arxiv.org/abs/2601.12117",
        "pdf_url": "https://arxiv.org/pdf/2601.12117",
        "title": "Offline Policy Learning with Weight Clipping and Heaviside Composite Optimization",
        "authors": [
            "Jingren Liu",
            "Hanzhang Qin",
            "Junyi Liu",
            "Mabel C. Chou",
            "Jong-Shi Pang"
        ],
        "comments": "",
        "subjects": "Optimization and Control (math.OC); Machine Learning (cs.LG)",
        "abstract": "Offline policy learning aims to use historical data to learn an optimal personalized decision rule. In the standard estimate-then-optimize framework, reweighting-based methods (e.g., inverse propensity weighting or doubly robust estimators) are widely used to produce unbiased estimates of policy values. However, when the propensity scores of some treatments are small, these reweighting-based methods suffer from high variance in policy value estimation, which may mislead the downstream policy optimization and yield a learned policy with inferior value. In this paper, we systematically develop an offline policy learning algorithm based on a weight-clipping estimator that truncates small propensity scores via a clipping threshold chosen to minimize the mean squared error (MSE) in policy value estimation. Focusing on linear policies, we address the bilevel and discontinuous objective induced by weight-clipping-based policy optimization by reformulating the problem as a Heaviside composite optimization problem, which provides a rigorous computational framework. The reformulated policy optimization problem is then solved efficiently using the progressive integer programming method, making practical policy learning tractable. We establish an upper bound for the suboptimality of the proposed algorithm, which reveals how the reduction in MSE of policy value estimation, enabled by our proposed weight-clipping estimator, leads to improved policy learning performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 157,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12161",
        "abs_url": "https://arxiv.org/abs/2601.12161",
        "pdf_url": "https://arxiv.org/pdf/2601.12161",
        "title": "Streaming Operator Inference for Model Reduction of Large-Scale Dynamical Systems",
        "authors": [
            "Tomoki Koike",
            "Prakash Mohan",
            "Marc T. Henry de Frahan",
            "Julie Bessac",
            "Elizabeth Qian"
        ],
        "comments": "",
        "subjects": "Numerical Analysis (math.NA); Machine Learning (cs.LG); Dynamical Systems (math.DS); Computational Physics (physics.comp-ph)",
        "abstract": "Projection-based model reduction enables efficient simulation of complex dynamical systems by constructing low-dimensional surrogate models from high-dimensional data. The Operator Inference (OpInf) approach learns such reduced surrogate models through a two-step process: constructing a low-dimensional basis via Singular Value Decomposition (SVD) to compress the data, then solving a linear least-squares (LS) problem to infer reduced operators that govern the dynamics in this compressed space, all without access to the underlying code or full model operators, i.e., non-intrusively. Traditional OpInf operates as a batch learning method, where both the SVD and LS steps process all data simultaneously. This poses a barrier to deployment of the approach on large-scale applications where dataset sizes prevent the loading of all data into memory at once. Additionally, the traditional batch approach does not naturally allow model updates using new data acquired during online computation. To address these limitations, we propose Streaming OpInf, which learns reduced models from sequentially arriving data streams. Our approach employs incremental SVD for adaptive basis construction and recursive LS for streaming operator updates, eliminating the need to store complete data sets while enabling online model adaptation. The approach can flexibly combine different choices of streaming algorithms for numerical linear algebra: we systematically explore the impact of these choices both analytically and numerically to identify effective combinations for accurate reduced model learning. Numerical experiments on benchmark problems and a large-scale turbulent channel flow demonstrate that Streaming OpInf achieves accuracy comparable to batch OpInf while reducing memory requirements by over 99% and enabling dimension reductions exceeding 31,000x, resulting in orders-of-magnitude faster predictions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 158,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12219",
        "abs_url": "https://arxiv.org/abs/2601.12219",
        "pdf_url": "https://arxiv.org/pdf/2601.12219",
        "title": "Persistent Sheaf Laplacian Analysis of Protein Stability and Solubility Changes upon Mutation",
        "authors": [
            "Yiming Ren",
            "Junjie Wee",
            "Xi Chen",
            "Grace Qian",
            "Guo-Wei Wei"
        ],
        "comments": "",
        "subjects": "Spectral Theory (math.SP); Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)",
        "abstract": "Genetic mutations frequently disrupt protein structure, stability, and solubility, acting as primary drivers for a wide spectrum of diseases. Despite the critical importance of these molecular alterations, existing computational models often lack interpretability, and fail to integrate essential physicochemical interaction. To overcome these limitations, we propose SheafLapNet, a unified predictive framework grounded in the mathematical theory of Topological Deep Learning (TDL) and Persistent Sheaf Laplacian (PSL). Unlike standard Topological Data Analysis (TDA) tools such as persistent homology, which are often insensitive to heterogeneous information, PSL explicitly encodes specific physical and chemical information such as partial charges directly into the topological analysis. SheafLapNet synergizes these sheaf-theoretic invariants with advanced protein transformer features and auxiliary physical descriptors to capture intrinsic molecular interactions in a multiscale and mechanistic manner. To validate our framework, we employ rigorous benchmarks for both regression and classification tasks. For stability prediction, we utilize the comprehensive S2648 and S350 datasets. For solubility prediction, we employ the PON-Sol2 dataset, which provides annotations for increased, decreased, or neutral solubility changes. By integrating these multi-perspective features, SheafLapNet achieves state-of-the-art performance across these diverse benchmarks, demonstrating that sheaf-theoretic modeling significantly enhances both interpretability and generalizability in predicting mutation-induced structural and functional changes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 159,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12238",
        "abs_url": "https://arxiv.org/abs/2601.12238",
        "pdf_url": "https://arxiv.org/pdf/2601.12238",
        "title": "On the Provable Suboptimality of Momentum SGD in Nonstationary Stochastic Optimization",
        "authors": [
            "Sharan Sahu",
            "Cameron J. Hogan",
            "Martin T. Wells"
        ],
        "comments": "70 pages, 4 figures, 2 tables",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Optimization and Control (math.OC)",
        "abstract": "While momentum-based acceleration has been studied extensively in deterministic optimization problems, its behavior in nonstationary environments -- where the data distribution and optimal parameters drift over time -- remains underexplored. We analyze the tracking performance of Stochastic Gradient Descent (SGD) and its momentum variants (Polyak heavy-ball and Nesterov) under uniform strong convexity and smoothness in varying stepsize regimes. We derive finite-time bounds in expectation and with high probability for the tracking error, establishing a sharp decomposition into three components: a transient initialization term, a noise-induced variance term, and a drift-induced tracking lag. Crucially, our analysis uncovers a fundamental trade-off: while momentum can suppress gradient noise, it incurs an explicit penalty on the tracking capability. We show that momentum can substantially amplify drift-induced tracking error, with amplification that becomes unbounded as the momentum parameter approaches one, formalizing the intuition that using 'stale' gradients hinders adaptation to rapid regime shifts. Complementing these upper bounds, we establish minimax lower bounds for dynamic regret under gradient-variation constraints. These lower bounds prove that the inertia-induced penalty is not an artifact of analysis but an information-theoretic barrier: in drift-dominated regimes, momentum creates an unavoidable 'inertia window' that fundamentally degrades performance. Collectively, these results provide a definitive theoretical grounding for the empirical instability of momentum in dynamic environments and delineate the precise regime boundaries where SGD provably outperforms its accelerated counterparts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 160,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12247",
        "abs_url": "https://arxiv.org/abs/2601.12247",
        "pdf_url": "https://arxiv.org/pdf/2601.12247",
        "title": "Plan, Verify and Fill: A Structured Parallel Decoding Approach for Diffusion Language Models",
        "authors": [
            "Miao Li",
            "Hanyang Jiang",
            "Sikai Chen",
            "Hengyu Fu",
            "Yuhang Cai",
            "Baihe Huang",
            "Tinghan Ye",
            "Xuanzhou Chen",
            "Pascal Van Hentenryck"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Diffusion Language Models (DLMs) present a promising non-sequential paradigm for text generation, distinct from standard autoregressive (AR) approaches. However, current decoding strategies often adopt a reactive stance, underutilizing the global bidirectional context to dictate global trajectories. To address this, we propose Plan-Verify-Fill (PVF), a training-free paradigm that grounds planning via quantitative validation. PVF actively constructs a hierarchical skeleton by prioritizing high-leverage semantic anchors and employs a verification protocol to operationalize pragmatic structural stopping where further deliberation yields diminishing returns. Extensive evaluations on LLaDA-8B-Instruct and Dream-7B-Instruct demonstrate that PVF reduces the Number of Function Evaluations (NFE) by up to 65% compared to confidence-based parallel decoding across benchmark datasets, unlocking superior efficiency without compromising accuracy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 161,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12253",
        "abs_url": "https://arxiv.org/abs/2601.12253",
        "pdf_url": "https://arxiv.org/pdf/2601.12253",
        "title": "Federated Joint Learning for Domain and Class Generalization",
        "authors": [
            "Haoran Xu",
            "Jiaze Li",
            "Jianzhong Ju",
            "Zhenbo Luo"
        ],
        "comments": "ICASSP 2026",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Efficient fine-tuning of visual-language models like CLIP has become crucial due to their large-scale parameter size and extensive pretraining requirements. Existing methods typically address either the issue of unseen classes or unseen domains in isolation, without considering a joint framework for both. In this paper, we propose \\textbf{Fed}erated Joint Learning for \\textbf{D}omain and \\textbf{C}lass \\textbf{G}eneralization, termed \\textbf{FedDCG}, a novel approach that addresses both class and domain generalization in federated learning settings. Our method introduces a domain grouping strategy where class-generalized networks are trained within each group to prevent decision boundary confusion. During inference, we aggregate class-generalized results based on domain similarity, effectively integrating knowledge from both class and domain generalization. Specifically, a learnable network is employed to enhance class generalization capabilities, and a decoupling mechanism separates general and domain-specific knowledge, improving generalization to unseen domains. Extensive experiments across various datasets show that \\textbf{FedDCG} outperforms state-of-the-art baselines in terms of accuracy and robustness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 162,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12263",
        "abs_url": "https://arxiv.org/abs/2601.12263",
        "pdf_url": "https://arxiv.org/pdf/2601.12263",
        "title": "Multimodal Generative Engine Optimization: Rank Manipulation for Vision-Language Model Rankers",
        "authors": [
            "Yixuan Du",
            "Chenxiao Yu",
            "Haoyan Xu",
            "Ziyi Wang",
            "Yue Zhao",
            "Xiyang Hu"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Vision-Language Models (VLMs) are rapidly replacing unimodal encoders in modern retrieval and recommendation systems. While their capabilities are well-documented, their robustness against adversarial manipulation in competitive ranking scenarios remains largely unexplored. In this paper, we uncover a critical vulnerability in VLM-based product search: multimodal ranking attacks. We present Multimodal Generative Engine Optimization (MGEO), a novel adversarial framework that enables a malicious actor to unfairly promote a target product by jointly optimizing imperceptible image perturbations and fluent textual suffixes. Unlike existing attacks that treat modalities in isolation, MGEO employs an alternating gradient-based optimization strategy to exploit the deep cross-modal coupling within the VLM. Extensive experiments on real-world datasets using state-of-the-art models demonstrate that our coordinated attack significantly outperforms text-only and image-only baselines. These findings reveal that multimodal synergy, typically a strength of VLMs, can be weaponized to compromise the integrity of search rankings without triggering conventional content filters.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 163,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12279",
        "abs_url": "https://arxiv.org/abs/2601.12279",
        "pdf_url": "https://arxiv.org/pdf/2601.12279",
        "title": "HCFT: Hierarchical Convolutional Fusion Transformer for EEG Decoding",
        "authors": [
            "Haodong Zhang",
            "Jiapeng Zhu",
            "Yitong Chen",
            "Hongqi Li"
        ],
        "comments": "Submitted to IEEE Journals",
        "subjects": "Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)",
        "abstract": "Electroencephalography (EEG) decoding requires models that can effectively extract and integrate complex temporal, spectral, and spatial features from multichannel signals. To address this challenge, we propose a lightweight and generalizable decoding framework named Hierarchical Convolutional Fusion Transformer (HCFT), which combines dual-branch convolutional encoders and hierarchical Transformer blocks for multi-scale EEG representation learning. Specifically, the model first captures local temporal and spatiotemporal dynamics through time-domain and time-space convolutional branches, and then aligns these features via a cross-attention mechanism that enables interaction between branches at each stage. Subsequently, a hierarchical Transformer fusion structure is employed to encode global dependencies across all feature stages, while a customized Dynamic Tanh normalization module is introduced to replace traditional Layer Normalization in order to enhance training stability and reduce redundancy. Extensive experiments are conducted on two representative benchmark datasets, BCI Competition IV-2b and CHB-MIT, covering both event-related cross-subject classification and continuous seizure prediction tasks. Results show that HCFT achieves 80.83% average accuracy and a Cohen's kappa of 0.6165 on BCI IV-2b, as well as 99.10% sensitivity, 0.0236 false positives per hour, and 98.82% specificity on CHB-MIT, consistently outperforming over ten state-of-the-art baseline methods. Ablation studies confirm that each core component of the proposed framework contributes significantly to the overall decoding performance, demonstrating HCFT's effectiveness in capturing EEG dynamics and its potential for real-world BCI applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 164,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12289",
        "abs_url": "https://arxiv.org/abs/2601.12289",
        "pdf_url": "https://arxiv.org/pdf/2601.12289",
        "title": "ParaMETA: Towards Learning Disentangled Paralinguistic Speaking Styles Representations from Speech",
        "authors": [
            "Haowei Lou",
            "Hye-young Paik",
            "Wen Hu",
            "Lina Yao"
        ],
        "comments": "9 pages, 7 figures, Accepted to AAAI-26 (Main Technical Track)",
        "subjects": "Sound (cs.SD); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)",
        "abstract": "Learning representative embeddings for different types of speaking styles, such as emotion, age, and gender, is critical for both recognition tasks (e.g., cognitive computing and human-computer interaction) and generative tasks (e.g., style-controllable speech generation). In this work, we introduce ParaMETA, a unified and flexible framework for learning and controlling speaking styles directly from speech. Unlike existing methods that rely on single-task models or cross-modal alignment, ParaMETA learns disentangled, task-specific embeddings by projecting speech into dedicated subspaces for each type of style. This design reduces inter-task interference, mitigates negative transfer, and allows a single model to handle multiple paralinguistic tasks such as emotion, gender, age, and language classification. Beyond recognition, ParaMETA enables fine-grained style control in Text-To-Speech (TTS) generative models. It supports both speech- and text-based prompting and allows users to modify one speaking styles while preserving others. Extensive experiments demonstrate that ParaMETA outperforms strong baselines in classification accuracy and generates more natural and expressive speech, while maintaining a lightweight and efficient model suitable for real-world applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 165,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12307",
        "abs_url": "https://arxiv.org/abs/2601.12307",
        "pdf_url": "https://arxiv.org/pdf/2601.12307",
        "title": "Rethinking the Value of Multi-Agent Workflow: A Strong Single Agent Baseline",
        "authors": [
            "Jiawei Xu",
            "Arief Koesdwiady",
            "Sisong Bei",
            "Yan Han",
            "Baixiang Huang",
            "Dakuo Wang",
            "Yutong Chen",
            "Zheshen Wang",
            "Peihao Wang",
            "Pan Li",
            "Ying Ding"
        ],
        "comments": "",
        "subjects": "Multiagent Systems (cs.MA); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Recent advances in LLM-based multi-agent systems (MAS) show that workflows composed of multiple LLM agents with distinct roles, tools, and communication patterns can outperform single-LLM baselines on complex tasks. However, most frameworks are homogeneous, where all agents share the same base LLM and differ only in prompts, tools, and positions in the workflow. This raises the question of whether such workflows can be simulated by a single agent through multi-turn conversations. We investigate this across seven benchmarks spanning coding, mathematics, general question answering, domain-specific reasoning, and real-world planning and tool use. Our results show that a single agent can reach the performance of homogeneous workflows with an efficiency advantage from KV cache reuse, and can even match the performance of an automatically optimized heterogeneous workflow. Building on this finding, we propose \\textbf{OneFlow}, an algorithm that automatically tailors workflows for single-agent execution, reducing inference costs compared to existing automatic multi-agent design frameworks without trading off accuracy. These results position the single-LLM implementation of multi-agent workflows as a strong baseline for MAS research. We also note that single-LLM methods cannot capture heterogeneous workflows due to the lack of KV cache sharing across different LLMs, highlighting future opportunities in developing \\textit{truly} heterogeneous multi-agent systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 166,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12311",
        "abs_url": "https://arxiv.org/abs/2601.12311",
        "pdf_url": "https://arxiv.org/pdf/2601.12311",
        "title": "Cross-reality Location Privacy Protection in 6G-enabled Vehicular Metaverses: An LLM-enhanced Hybrid Generative Diffusion Model-based Approach",
        "authors": [
            "Xiaofeng Luo",
            "Jiayi He",
            "Jiawen Kang",
            "Ruichen Zhang",
            "Zhaoshui He",
            "Ekram Hossain",
            "Dong In Kim"
        ],
        "comments": "16 pages, 8 figures",
        "subjects": "Networking and Internet Architecture (cs.NI); Cryptography and Security (cs.CR); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)",
        "abstract": "The emergence of 6G-enabled vehicular metaverses enables Autonomous Vehicles (AVs) to operate across physical and virtual spaces through space-air-ground-sea integrated networks. The AVs can deploy AI agents powered by large AI models as personalized assistants, on edge servers to support intelligent driving decision making and enhanced on-board experiences. However, such cross-reality interactions may cause serious location privacy risks, as adversaries can infer AV trajectories by correlating the location reported when AVs request LBS in reality with the location of the edge servers on which their corresponding AI agents are deployed in virtuality. To address this challenge, we design a cross-reality location privacy protection framework based on hybrid actions, including continuous location perturbation in reality and discrete privacy-aware AI agent migration in virtuality. In this framework, a new privacy metric, termed cross-reality location entropy, is proposed to effectively quantify the privacy levels of AVs. Based on this metric, we formulate an optimization problem to optimize the hybrid action, focusing on achieving a balance between location protection, service latency reduction, and quality of service maintenance. To solve the complex mixed-integer problem, we develop a novel LLM-enhanced Hybrid Diffusion Proximal Policy Optimization (LHDPPO) algorithm, which integrates LLM-driven informative reward design to enhance environment understanding with double Generative Diffusion Models-based policy exploration to handle high-dimensional action spaces, thereby enabling reliable determination of optimal hybrid actions. Extensive experiments on real-world datasets demonstrate that the proposed framework effectively mitigates cross-reality location privacy leakage for AVs while maintaining strong user immersion within 6G-enabled vehicular metaverse scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 167,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12345",
        "abs_url": "https://arxiv.org/abs/2601.12345",
        "pdf_url": "https://arxiv.org/pdf/2601.12345",
        "title": "Adaptive Rotary Steering with Joint Autoregression for Robust Extraction of Closely Moving Speakers in Dynamic Scenarios",
        "authors": [
            "Jakob Kienegger",
            "Timo Gerkmann"
        ],
        "comments": "Accepted at IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2026",
        "subjects": "Audio and Speech Processing (eess.AS); Machine Learning (cs.LG); Sound (cs.SD)",
        "abstract": "Latest advances in deep spatial filtering for Ambisonics demonstrate strong performance in stationary multi-speaker scenarios by rotating the sound field toward a target speaker prior to multi-channel enhancement. For applicability in dynamic acoustic conditions with moving speakers, we propose to automate this rotary steering using an interleaved tracking algorithm conditioned on the target's initial direction. However, for nearby or crossing speakers, robust tracking becomes difficult and spatial cues less effective for enhancement. By incorporating the processed recording as additional guide into both algorithms, our novel joint autoregressive framework leverages temporal-spectral correlations of speech to resolve spatially challenging speaker constellations. Consequently, our proposed method significantly improves tracking and enhancement of closely spaced speakers, consistently outperforming comparable non-autoregressive methods on a synthetic dataset. Real-world recordings complement these findings in complex scenarios with multiple speaker crossings and varying speaker-to-array distances.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 168,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12354",
        "abs_url": "https://arxiv.org/abs/2601.12354",
        "pdf_url": "https://arxiv.org/pdf/2601.12354",
        "title": "Bone-conduction Guided Multimodal Speech Enhancement with Conditional Diffusion Models",
        "authors": [
            "Sina Khanagha",
            "Bunlong Lay",
            "Timo Gerkmann"
        ],
        "comments": "Accepted to IEEE ICASSP 2026",
        "subjects": "Audio and Speech Processing (eess.AS); Machine Learning (cs.LG); Sound (cs.SD)",
        "abstract": "Single-channel speech enhancement models face significant performance degradation in extremely noisy environments. While prior work has shown that complementary bone-conducted speech can guide enhancement, effective integration of this noise-immune modality remains a challenge. This paper introduces a novel multimodal speech enhancement framework that integrates bone-conduction sensors with air-conducted microphones using a conditional diffusion model. Our proposed model significantly outperforms previously established multimodal techniques and a powerful diffusion-based single-modal baseline across a wide range of acoustic conditions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 169,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12375",
        "abs_url": "https://arxiv.org/abs/2601.12375",
        "pdf_url": "https://arxiv.org/pdf/2601.12375",
        "title": "LiQSS: Post-Transformer Linear Quantum-Inspired State-Space Tensor Networks for Real-Time 6G",
        "authors": [
            "Farhad Rezazadeh",
            "Hatim Chergui",
            "Mehdi Bennis",
            "Houbing Song",
            "Lingjia Liu",
            "Dusit Niyato",
            "Merouane Debbah"
        ],
        "comments": "14 pages, 4 figures, 5 tables",
        "subjects": "Networking and Internet Architecture (cs.NI); Machine Learning (cs.LG)",
        "abstract": "Proactive and agentic control in Sixth-Generation (6G) Open Radio Access Networks (O-RAN) requires control-grade prediction under stringent Near-Real-Time (Near-RT) latency and computational constraints. While Transformer-based models are effective for sequence modeling, their quadratic complexity limits scalability in Near-RT RAN Intelligent Controller (RIC) analytics. This paper investigates a post-Transformer design paradigm for efficient radio telemetry forecasting. We propose a quantum-inspired many-body state-space tensor network that replaces self-attention with stable structured state-space dynamics kernels, enabling linear-time sequence modeling. Tensor-network factorizations in the form of Tensor Train (TT) / Matrix Product State (MPS) representations are employed to reduce parameterization and data movement in both input projections and prediction heads, while lightweight channel gating and mixing layers capture non-stationary cross-Key Performance Indicator (KPI) dependencies. The proposed model is instantiated as an agentic perceive-predict xApp and evaluated on a bespoke O-RAN KPI time-series dataset comprising 59,441 sliding windows across 13 KPIs, using Reference Signal Received Power (RSRP) forecasting as a representative use case. Our proposed Linear Quantum-Inspired State-Space (LiQSS) model is 10.8x-15.8x smaller and approximately 1.4x faster than prior structured state-space baselines. Relative to Transformer-based models, LiQSS achieves up to a 155x reduction in parameter count and up to 2.74x faster inference, without sacrificing forecasting accuracy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 170,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12400",
        "abs_url": "https://arxiv.org/abs/2601.12400",
        "pdf_url": "https://arxiv.org/pdf/2601.12400",
        "title": "BiCoLoR: Communication-Efficient Optimization with Bidirectional Compression and Local Training",
        "authors": [
            "Laurent Condat",
            "Artavazd Maranjyan",
            "Peter Richtárik"
        ],
        "comments": "",
        "subjects": "Optimization and Control (math.OC); Machine Learning (cs.LG)",
        "abstract": "Slow and costly communication is often the main bottleneck in distributed optimization, especially in federated learning where it occurs over wireless networks. We introduce BiCoLoR, a communication-efficient optimization algorithm that combines two widely used and effective strategies: local training, which increases computation between communication rounds, and compression, which encodes high-dimensional vectors into short bitstreams. While these mechanisms have been combined before, compression has typically been applied only to uplink (client-to-server) communication, leaving the downlink (server-to-client) side unaddressed. In practice, however, both directions are costly. We propose BiCoLoR, the first algorithm to combine local training with bidirectional compression using arbitrary unbiased compressors. This joint design achieves accelerated complexity guarantees in both convex and strongly convex heterogeneous settings. Empirically, BiCoLoR outperforms existing algorithms and establishes a new standard in communication efficiency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 171,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12407",
        "abs_url": "https://arxiv.org/abs/2601.12407",
        "pdf_url": "https://arxiv.org/pdf/2601.12407",
        "title": "De-Anonymization at Scale via Tournament-Style Attribution",
        "authors": [
            "Lirui Zhang",
            "Huishuai Zhang"
        ],
        "comments": "14 pages",
        "subjects": "Cryptography and Security (cs.CR); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "As LLMs rapidly advance and enter real-world use, their privacy implications are increasingly important. We study an authorship de-anonymization threat: using LLMs to link anonymous documents to their authors, potentially compromising settings such as double-blind peer review. We propose De-Anonymization at Scale (DAS), a large language model-based method for attributing authorship among tens of thousands of candidate texts. DAS uses a sequential progression strategy: it randomly partitions the candidate corpus into fixed-size groups, prompts an LLM to select the text most likely written by the same author as a query text, and iteratively re-queries the surviving candidates to produce a ranked top-k list. To make this practical at scale, DAS adds a dense-retrieval prefilter to shrink the search space and a majority-voting style aggregation over multiple independent runs to improve robustness and ranking precision. Experiments on anonymized review data show DAS can recover same-author texts from pools of tens of thousands with accuracy well above chance, demonstrating a realistic privacy risk for anonymous platforms. On standard authorship benchmarks (Enron emails and blog posts), DAS also improves both accuracy and scalability over prior approaches, highlighting a new LLM-enabled de-anonymization vulnerability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 172,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12433",
        "abs_url": "https://arxiv.org/abs/2601.12433",
        "pdf_url": "https://arxiv.org/pdf/2601.12433",
        "title": "Temporal Data and Short-Time Averages Improve Multiphase Mass Flow Metering",
        "authors": [
            "Amanda Nyholm",
            "Yessica Arellano",
            "Jinyu Liu",
            "Damian Krakowiak",
            "Pierluigi Salvo Rossi"
        ],
        "comments": "9 pages, 6 figures",
        "subjects": "Signal Processing (eess.SP); Machine Learning (cs.LG)",
        "abstract": "Reliable flow measurements are essential in many industries, but current instruments often fail to accurately estimate multiphase flows, which are frequently encountered in real-world operations. Combining machine learning (ML) algorithms with accurate single-phase flowmeters has therefore received extensive research attention in recent years. The Coriolis mass flowmeter is a widely used single-phase meter that provides direct mass flow measurements, which ML models can be trained to correct, thereby reducing measurement errors in multiphase conditions. This paper demonstrates that preserving temporal information significantly improves model performance in such scenarios. We compare a multilayer perceptron, a windowed multilayer perceptron, and a convolutional neural network (CNN) on three-phase air-water-oil flow data from 342 experiments. Whereas prior work typically compresses each experiment into a single averaged sample, we instead compute short-time averages from within each experiment and train models that preserve temporal information at several downsampling intervals. The CNN performed best at 0.25 Hz with approximately 95 % of relative errors below 13 %, a normalized root mean squared error of 0.03, and a mean absolute percentage error of approximately 4.3 %, clearly outperforming the best single-averaged model and demonstrating that short-time averaging within individual experiments is preferable. Results are consistent across multiple data splits and random seeds, demonstrating robustness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 173,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12460",
        "abs_url": "https://arxiv.org/abs/2601.12460",
        "pdf_url": "https://arxiv.org/pdf/2601.12460",
        "title": "TrojanPraise: Jailbreak LLMs via Benign Fine-Tuning",
        "authors": [
            "Zhixin Xie",
            "Xurui Song",
            "Jun Luo"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
        "abstract": "The demand of customized large language models (LLMs) has led to commercial LLMs offering black-box fine-tuning APIs, yet this convenience introduces a critical security loophole: attackers could jailbreak the LLMs by fine-tuning them with malicious data. Though this security issue has recently been exposed, the feasibility of such attacks is questionable as malicious training dataset is believed to be detectable by moderation models such as Llama-Guard-3. In this paper, we propose TrojanPraise, a novel finetuning-based attack exploiting benign and thus filter-approved data. Basically, TrojanPraise fine-tunes the model to associate a crafted word (e.g., \"bruaf\") with harmless connotations, then uses this word to praise harmful concepts, subtly shifting the LLM from refusal to compliance. To explain the attack, we decouple the LLM's internal representation of a query into two dimensions of knowledge and attitude. We demonstrate that successful jailbreak requires shifting the attitude while avoiding knowledge shift, a distortion in the model's understanding of the concept. To validate this attack, we conduct experiments on five opensource LLMs and two commercial LLMs under strict black-box settings. Results show that TrojanPraise achieves a maximum attack success rate of 95.88% while evading moderation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 174,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12483",
        "abs_url": "https://arxiv.org/abs/2601.12483",
        "pdf_url": "https://arxiv.org/pdf/2601.12483",
        "title": "A Mixture of Experts Vision Transformer for High-Fidelity Surface Code Decoding",
        "authors": [
            "Hoang Viet Nguyen",
            "Manh Hung Nguyen",
            "Hoang Ta",
            "Van Khu Vu",
            "Yeow Meng Chee"
        ],
        "comments": "16 pages, 7 figures",
        "subjects": "Quantum Physics (quant-ph); Information Theory (cs.IT); Machine Learning (cs.LG)",
        "abstract": "Quantum error correction is a key ingredient for large scale quantum computation, protecting logical information from physical noise by encoding it into many physical qubits. Topological stabilizer codes are particularly appealing due to their geometric locality and practical relevance. In these codes, stabilizer measurements yield a syndrome that must be decoded into a recovery operation, making decoding a central bottleneck for scalable real time operation. Existing decoders are commonly classified into two categories. Classical algorithmic decoders provide strong and well established baselines, but may incur substantial computational overhead at large code distances or under stringent latency constraints. Machine learning based decoders offer fast GPU inference and flexible function approximation, yet many approaches do not explicitly exploit the lattice geometry and local structure of topological codes, which can limit performance. In this work, we propose QuantumSMoE, a quantum vision transformer based decoder that incorporates code structure through plus shaped embeddings and adaptive masking to capture local interactions and lattice connectivity, and improves scalability via a mixture of experts layer with a novel auxiliary loss. Experiments on the toric code demonstrate that QuantumSMoE outperforms state-of-the-art machine learning decoders as well as widely used classical baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 175,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12507",
        "abs_url": "https://arxiv.org/abs/2601.12507",
        "pdf_url": "https://arxiv.org/pdf/2601.12507",
        "title": "SDCoNet: Saliency-Driven Multi-Task Collaborative Network for Remote Sensing Object Detection",
        "authors": [
            "Ruo Qi",
            "Linhui Dai",
            "Yusong Qin",
            "Chaolei Yang",
            "Yanshan Li"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "In remote sensing images, complex backgrounds, weak object signals, and small object scales make accurate detection particularly challenging, especially under low-quality imaging conditions. A common strategy is to integrate single-image super-resolution (SR) before detection; however, such serial pipelines often suffer from misaligned optimization objectives, feature redundancy, and a lack of effective interaction between SR and detection. To address these issues, we propose a Saliency-Driven multi-task Collaborative Network (SDCoNet) that couples SR and detection through implicit feature sharing while preserving task specificity. SDCoNet employs the swin transformer-based shared encoder, where hierarchical window-shifted self-attention supports cross-task feature collaboration and adaptively balances the trade-off between texture refinement and semantic representation. In addition, a multi-scale saliency prediction module produces importance scores to select key tokens, enabling focused attention on weak object regions, suppression of background clutter, and suppression of adverse features introduced by multi-task coupling. Furthermore, a gradient routing strategy is introduced to mitigate optimization conflicts. It first stabilizes detection semantics and subsequently routes SR gradients along a detection-oriented direction, enabling the framework to guide the SR branch to generate high-frequency details that are explicitly beneficial for detection. Experiments on public datasets, including NWPU VHR-10-Split, DOTAv1.5-Split, and HRSSD-Split, demonstrate that the proposed method, while maintaining competitive computational efficiency, significantly outperforms existing mainstream algorithms in small object detection on low-quality remote sensing images. Our code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 176,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12587",
        "abs_url": "https://arxiv.org/abs/2601.12587",
        "pdf_url": "https://arxiv.org/pdf/2601.12587",
        "title": "A Theory of Diversity for Random Matrices with Applications to In-Context Learning of Schrödinger Equations",
        "authors": [
            "Frank Cole",
            "Yulong Lu",
            "Shaurya Sehgal"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "We address the following question: given a collection $\\{\\mathbf{A}^{(1)}, \\dots, \\mathbf{A}^{(N)}\\}$ of independent $d \\times d$ random matrices drawn from a common distribution $\\mathbb{P}$, what is the probability that the centralizer of $\\{\\mathbf{A}^{(1)}, \\dots, \\mathbf{A}^{(N)}\\}$ is trivial? We provide lower bounds on this probability in terms of the sample size $N$ and the dimension $d$ for several families of random matrices which arise from the discretization of linear Schrödinger operators with random potentials. When combined with recent work on machine learning theory, our results provide guarantees on the generalization ability of transformer-based neural networks for in-context learning of Schrödinger equations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 177,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12600",
        "abs_url": "https://arxiv.org/abs/2601.12600",
        "pdf_url": "https://arxiv.org/pdf/2601.12600",
        "title": "SSVD-O: Parameter-Efficient Fine-Tuning with Structured SVD for Speech Recognition",
        "authors": [
            "Pu Wang",
            "Shinji Watanabe",
            "Hugo Van hamme"
        ],
        "comments": "Accepted by IEEE ICASSP 2026",
        "subjects": "Sound (cs.SD); Computation and Language (cs.CL); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)",
        "abstract": "Parameter-efficient fine-tuning (PEFT) is a scalable approach for adapting large speech foundation models to new domains. While methods such as LoRA and its state-of-the-art variants reduce adaptation costs, they typically allocate parameters uniformly across model subspaces, which limits their efficiency and scalability in speech applications. Building on our prior work, this paper introduces SSVD-Outer (SSVD-O), an extension of the structured SVD-guided (SSVD) fine-tuning method. SSVD-O combines input acoustic feature space-associated inner transformations with output semantic feature space-associated outer transformations to enable scalable and balanced adaptation. We conduct the first systematic analysis of parameter budget allocation across model subspaces in PEFT for automatic speech recognition (ASR), and investigate the trade-off between learning and forgetting under constrained resources. SSVD-O is benchmarked against LoRA, DoRA, PiSSA, and SSVD on domain-shifted ASR tasks, including child speech and regional accents, across model scales from 0.1B to 2B within the ESPnet framework. Experimental results show that SSVD-O consistently narrows the performance gap to full fine-tuning while improving generalization and mitigating catastrophic forgetting.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 178,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12603",
        "abs_url": "https://arxiv.org/abs/2601.12603",
        "pdf_url": "https://arxiv.org/pdf/2601.12603",
        "title": "onepot CORE -- an enumerated chemical space to streamline drug discovery, enabled by automated small molecule synthesis and AI",
        "authors": [
            "Andrei S. Tyrin",
            "Brandon Wang",
            "Manuel Muñoz",
            "Samuel H. Foxman",
            "Daniil A. Boiko"
        ],
        "comments": "",
        "subjects": "Chemical Physics (physics.chem-ph); Machine Learning (cs.LG)",
        "abstract": "The design-make-test-analyze cycle in early-stage drug discovery remains constrained primarily by the \"make\" step: small-molecule synthesis is slow, costly, and difficult to scale or automate across diverse chemotypes. Enumerated chemical spaces aim to reduce this bottleneck by predefining synthesizable regions of chemical space from available building blocks and reliable reactions, yet existing commercial spaces are still limited by long turnaround times, narrow reaction scope, and substantial manual decision-making in route selection and execution. Here we present the first version of onepot CORE, an enumerated chemical space containing 3.4B molecules and corresponding on-demand synthesis product enabled by an automated synthesis platform and an AI chemist, Phil, that designs, executes, and analyzes experiments. onepot CORE is constructed by (i) selecting a reaction set commonly used in medicinal chemistry, (ii) sourcing and curating building blocks from supplier catalogs, (iii) enumerating candidate products, and (iv) applying ML-based feasibility assessment to prioritize compounds for robust execution. In the current release, the space is supported by seven reactions. We describe an end-to-end workflow - from route selection and automated liquid handling through workup and purification. We further report validation across operational metrics (success rate, timelines, purity, and identity), including NMR confirmation for a representative set of synthesized compounds and assay suitability demonstrated using a series of DPP4 inhibitors. Collectively, onepot CORE illustrates a path toward faster, more reliable access to diverse small molecules, supporting accelerated discovery in pharmaceuticals and beyond.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 179,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12610",
        "abs_url": "https://arxiv.org/abs/2601.12610",
        "pdf_url": "https://arxiv.org/pdf/2601.12610",
        "title": "HERMES: A Unified Open-Source Framework for Realtime Multimodal Physiological Sensing, Edge AI, and Intervention in Closed-Loop Smart Healthcare Applications",
        "authors": [
            "Maxim Yudayev",
            "Juha Carlon",
            "Diwas Lamsal",
            "Vayalet Stefanova",
            "Benjamin Filtjens"
        ],
        "comments": "Submitted to ACM SenSys '26, 12 pages (excl. references), 9 figures",
        "subjects": "Systems and Control (eess.SY); Machine Learning (cs.LG)",
        "abstract": "Intelligent assistive technologies are increasingly recognized as critical daily-use enablers for people with disabilities and age-related functional decline. Longitudinal studies, curation of quality datasets, live monitoring in activities of daily living, and intelligent intervention devices, share the largely unsolved need in reliable high-throughput multimodal sensing and processing. Streaming large heterogeneous data from distributed sensors, historically closed-source environments, and limited prior works on realtime closed-loop AI methodologies, inhibit such applications. To accelerate the emergence of clinical deployments, we deliver HERMES - an open-source high-performance Python framework for continuous multimodal sensing and AI processing at the edge. It enables synchronized data collection, and realtime streaming inference with user PyTorch models, on commodity computing devices. HERMES is applicable to fixed-lab and free-living environments, of distributed commercial and custom sensors. It is the first work to offer a holistic methodology that bridges cross-disciplinary gaps in real-world implementation strategies, and guides downstream AI model development. Its application on the closed-loop intelligent prosthesis use case illustrates the process of suitable AI model development from the generated constraints and trade-offs. Validation on the use case, with 4 synchronized hosts cooperatively capturing 18 wearable and off-body modalities, demonstrates performance and relevance of HERMES to the trajectory of the intelligent healthcare domain.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 180,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12614",
        "abs_url": "https://arxiv.org/abs/2601.12614",
        "pdf_url": "https://arxiv.org/pdf/2601.12614",
        "title": "Deterministic and probabilistic neural surrogates of global hybrid-Vlasov simulations",
        "authors": [
            "Daniel Holmberg",
            "Ivan Zaitsev",
            "Markku Alho",
            "Ioanna Bouri",
            "Fanni Franssila",
            "Haewon Jeong",
            "Minna Palmroth",
            "Teemu Roos"
        ],
        "comments": "",
        "subjects": "Space Physics (physics.space-ph); Machine Learning (cs.LG); Plasma Physics (physics.plasm-ph)",
        "abstract": "Hybrid-Vlasov simulations resolve ion-kinetic effects for modeling the solar wind-magnetosphere interaction, but even 5D (2D + 3V) simulations are computationally expensive. We show that graph-based machine learning emulators can learn the spatiotemporal evolution of electromagnetic fields and lower order moments of ion velocity distribution in the near-Earth space environment from four 5D Vlasiator runs performed with identical steady solar wind conditions. The initial ion number density is systematically varied, while the grid spacing is held constant, to scan the ratio of the characteristic ion skin depth to the numerical grid size. Using a graph neural network architecture operating on the 2D spatial simulation grid comprising 670k cells, we demonstrate that both a deterministic forecasting model (Graph-FM) and a probabilistic ensemble forecasting model (Graph-EFM) based on a latent variable formulation are capable of producing accurate predictions of future plasma states. A divergence penalty is incorporated during training to encourage divergence-freeness in the magnetic fields and improve physical consistency. For the probabilistic model, a continuous ranked probability score objective is added to improve the calibration of the ensemble forecasts. When trained, the emulators achieve more than two orders of magnitude speedup in generating the next time step relative to the original simulation on a single GPU compared to 100 CPUs for the Vlasiator runs, while closely matching physical magnetospheric response of the different runs. These results demonstrate that machine learning offers a way to make hybrid-Vlasov simulation tractable for real-time use while providing forecast uncertainty.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 181,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12621",
        "abs_url": "https://arxiv.org/abs/2601.12621",
        "pdf_url": "https://arxiv.org/pdf/2601.12621",
        "title": "Learning Deterministic Finite-State Machines from the Prefixes of a Single String is NP-Complete",
        "authors": [
            "Radu Cosmin Dumitru",
            "Ryo Yoshinaka",
            "Ayumi Shinohara"
        ],
        "comments": "12 pages, 4 figures",
        "subjects": "Formal Languages and Automata Theory (cs.FL); Machine Learning (cs.LG)",
        "abstract": "It is well known that computing a minimum DFA consistent with a given set of positive and negative examples is NP-hard. Previous work has identified conditions on the input sample under which the problem becomes tractable or remains hard. In this paper, we study the computational complexity of the case where the input sample is prefix-closed. This formulation is equivalent to computing a minimum Moore machine consistent with observations along its runs. We show that the problem is NP-hard to approximate when the sample set consists of all prefixes of binary strings. Furthermore, we show that the problem remains NP-hard as a decision problem even when the sample set consists of the prefixes of a single binary string. Our argument also extends to the corresponding problem for Mealy machines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 182,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12630",
        "abs_url": "https://arxiv.org/abs/2601.12630",
        "pdf_url": "https://arxiv.org/pdf/2601.12630",
        "title": "Reorienting off-path Nudged Elastic Bands (RONEB) via Minimum Mode Following",
        "authors": [
            "Rohit Goswami",
            "Miha Gunde",
            "Hannes Jónsson"
        ],
        "comments": "25 pages. 11 figures",
        "subjects": "Chemical Physics (physics.chem-ph); Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG); Computational Physics (physics.comp-ph)",
        "abstract": "Accurate determination of transition states remains central to understanding reaction kinetics. Double-ended methods like the Nudged Elastic Band (NEB) ensure relevant transition states and paths, but incur high computational costs and suffer stagnation on flat or rough potential energy surfaces. Conversely, single-ended eigenmode-following techniques offer efficiency but cannot often be constrained between specific states. Here, we present the Reorienting Off-path Nudged Elastic Bands (RONEB), an adaptive hybrid algorithm that integrates the double ended nature of the NEB with the acceleration of single ended Min-Mode Following methods. RONEB provides stability based on the history of the path optimization, relative force triggering, and an alignment-based back-off penalty to dynamically decouple the climbing image from the elastic band constraints. We benchmark the method against the standard Climbing Image NEB (CI-NEB) across the Baker-Chan transition state test set using the PET-MAD machine-learned potential and the OptBench Pt(111) heptamer island surface diffusion set. A Bayesian analysis of the performance data quantifies a median reduction in gradient calls of 46.3% [95% CrI: -54.7%, -36.9%] relative to the baseline, while surface diffusion tests reveal a 28% reduction across 59 metallic rearrangement mechanisms. These results establish RONEB as a highly effective tool for high-throughput automated chemical discovery.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 183,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12639",
        "abs_url": "https://arxiv.org/abs/2601.12639",
        "pdf_url": "https://arxiv.org/pdf/2601.12639",
        "title": "Objective Matters: Fine-Tuning Objectives Shape Safety, Robustness, and Persona Drift",
        "authors": [
            "Daniel Vennemeyer",
            "Punya Syon Pandey",
            "Phan Anh Duong",
            "Michael Umeokoli",
            "Samuel Ratnam"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Fine-tuning LLMs on benign data can still degrade alignment and adversarial robustness, yet direct analysis of the role of fine-tuning objectives in shaping these safety outcomes remain limited. We present a controlled comparison of six fine-tuning objectives -- Supervised Fine-Tuning, Direct Preference Optimization, Conditional Fine-Tuning, Inoculation Prompting, Odds Ratio Preference Optimization, and KL-regularized fine-tuning -- holding data, domain, architecture, and optimization fixed. Across closed-form reasoning and open-ended generation tasks, we find that objective choice induces systematic, scale-dependent shifts along the safety-capability frontier. At small training budgets, robustness is similar across objectives but capability differs. At larger budgets, objectives diverge sharply: supervised and preference-based tuning tightly couple capability gains to increased adversarial vulnerability and persona drift, while objectives that constrain learning signals -- especially ORPO and KL-regularization -- substantially mitigate both. Fine-tuning objectives therefore matter little for safety at small scales but become a primary driver of adversarial robustness and latent persona stability as training scale increases.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 184,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12660",
        "abs_url": "https://arxiv.org/abs/2601.12660",
        "pdf_url": "https://arxiv.org/pdf/2601.12660",
        "title": "Toward Faithful Explanations in Acoustic Anomaly Detection",
        "authors": [
            "Maab Elrashid",
            "Anthony Deschênes",
            "Cem Subakan",
            "Mirco Ravanelli",
            "Rémi Georges",
            "Michael Morin"
        ],
        "comments": "Accepted at the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2026. Code: this https URL",
        "subjects": "Sound (cs.SD); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)",
        "abstract": "Interpretability is essential for user trust in real-world anomaly detection applications. However, deep learning models, despite their strong performance, often lack transparency. In this work, we study the interpretability of autoencoder-based models for audio anomaly detection, by comparing a standard autoencoder (AE) with a mask autoencoder (MAE) in terms of detection performance and interpretability. We applied several attribution methods, including error maps, saliency maps, SmoothGrad, Integrated Gradients, GradSHAP, and Grad-CAM. Although MAE shows a slightly lower detection, it consistently provides more faithful and temporally precise explanations, suggesting a better alignment with true anomalies. To assess the relevance of the regions highlighted by the explanation method, we propose a perturbation-based faithfulness metric that replaces them with their reconstructions to simulate normal input. Our findings, based on experiments in a real industrial scenario, highlight the importance of incorporating interpretability into anomaly detection pipelines and show that masked training improves explanation quality without compromising performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 185,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12663",
        "abs_url": "https://arxiv.org/abs/2601.12663",
        "pdf_url": "https://arxiv.org/pdf/2601.12663",
        "title": "Energy-Efficient Prediction in Textile Manufacturing: Enhancing Accuracy and Data Efficiency With Ensemble Deep Transfer Learning",
        "authors": [
            "Yan-Chen Chen",
            "Wei-Yu Chiu",
            "Qun-Yu Wang",
            "Jing-Wei Chen",
            "Hao-Ting Zhao"
        ],
        "comments": "26 pages, 11 figures",
        "subjects": "Signal Processing (eess.SP); Machine Learning (cs.LG)",
        "abstract": "Traditional textile factories consume substantial energy, making energy-efficient production optimization crucial for sustainability and cost reduction. Meanwhile, deep neural networks (DNNs), which are effective for factory output prediction and operational optimization, require extensive historical data, posing challenges due to high sensor deployment and data collection costs. To address this, we propose Ensemble Deep Transfer Learning (EDTL), a novel framework that enhances prediction accuracy and data efficiency by integrating transfer learning with an ensemble strategy and a feature alignment layer. EDTL pretrains DNN models on data-rich production lines (source domain) and adapts them to data-limited lines (target domain), reducing dependency on large datasets. Experiments on real-world textile factory datasets show that EDTL improves prediction accuracy by 5.66% and enhances model robustness by 3.96% compared to conventional DNNs, particularly in data-limited scenarios (20%-40% data availability). This research contributes to energy-efficient textile manufacturing by enabling accurate predictions with fewer data requirements, providing a scalable and cost-effective solution for smart production systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 186,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12671",
        "abs_url": "https://arxiv.org/abs/2601.12671",
        "pdf_url": "https://arxiv.org/pdf/2601.12671",
        "title": "Exploiting Test-Time Augmentation in Federated Learning for Brain Tumor MRI Classification",
        "authors": [
            "Thamara Leandra de Deus Melo",
            "Rodrigo Moreira",
            "Larissa Ferreira Rodrigues Moreira",
            "André Ricardo Backes"
        ],
        "comments": "21st International Conference on Computer Vision Theory and Applications (VISAPP 2026), 9-11 March 2026, Marbella, Spain",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Efficient brain tumor diagnosis is crucial for early treatment; however, it is challenging because of lesion variability and image complexity. We evaluated convolutional neural networks (CNNs) in a federated learning (FL) setting, comparing models trained on original versus preprocessed MRI images (resizing, grayscale conversion, normalization, filtering, and histogram equalization). Preprocessing alone yielded negligible gains; combined with test-time augmentation (TTA), it delivered consistent, statistically significant improvements in federated MRI classification (p<0.001). In practice, TTA should be the default inference strategy in FL-based medical imaging; when the computational budget permits, pairing TTA with light preprocessing provides additional reliable gains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 187,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12693",
        "abs_url": "https://arxiv.org/abs/2601.12693",
        "pdf_url": "https://arxiv.org/pdf/2601.12693",
        "title": "BlocksecRT-DETR: Decentralized Privacy-Preserving and Token-Efficient Federated Transformer Learning for Secure Real-Time Object Detection in ITS",
        "authors": [
            "Mohoshin Ara Tahera",
            "Sabbir Rahman",
            "Shuvalaxmi Dass",
            "Sharif Ullah",
            "Mahmoud Abouyessef"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
        "abstract": "Federated real-time object detection using transformers in Intelligent Transportation Systems (ITS) faces three major challenges: (1) missing-class non-IID data heterogeneity from geographically diverse traffic environments, (2) latency constraints on edge hardware for high-capacity transformer models, and (3) privacy and security risks from untrusted client updates and centralized aggregation. We propose BlockSecRT-DETR, a BLOCKchain-SECured Real-Time Object DEtection TRansformer framework for ITS that provides a decentralized, token-efficient, and privacy-preserving federated training solution using RT-DETR transformer, incorporating a blockchain-secured update validation mechanism for trustworthy aggregation. In this framework, challenges (1) and (2) are jointly addressed through a unified client-side design that integrates RT-DETR training with a Token Engineering Module (TEM). TEM prunes low-utility tokens, reducing encoder complexity and latency on edge hardware, while aggregated updates mitigate non-IID data heterogeneity across clients. To address challenge (3), BlockSecRT-DETR incorporates a decentralized blockchain-secured update validation mechanism that enables tamper-proof, privacy-preserving, and trust-free authenticated model aggregation without relying on a central server. We evaluated the proposed framework under a missing-class Non-IID partition of the KITTI dataset and conducted a blockchain case study to quantify security overhead. TEM improves inference latency by 17.2% and reduces encoder FLOPs by 47.8%, while maintaining global detection accuracy (89.20% mAP@0.5). The blockchain integration adds 400 ms per round, and the ledger size remains under 12 KB due to metadata-only on-chain storage.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 188,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12714",
        "abs_url": "https://arxiv.org/abs/2601.12714",
        "pdf_url": "https://arxiv.org/pdf/2601.12714",
        "title": "P2L-CA: An Effective Parameter Tuning Framework for Rehearsal-Free Multi-Label Class-Incremental Learning",
        "authors": [
            "Songlin Dong",
            "Jiangyang Li",
            "Chenhao Ding",
            "Zhiheng Ma",
            "Haoyu Luo",
            "Yuhang He",
            "Yihong Gong"
        ],
        "comments": "12 pages, 5 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Multi-label Class-Incremental Learning aims to continuously recognize novel categories in complex scenes where multiple objects co-occur. However, existing approaches often incur high computational costs due to full-parameter fine-tuning and substantial storage overhead from memory buffers, or they struggle to address feature confusion and domain discrepancies adequately. To overcome these limitations, we introduce P2L-CA, a parameter-efficient framework that integrates a Prompt-to-Label module with a Continuous Adapter module. The P2L module leverages class-specific prompts to disentangle multi-label representations while incorporating linguistic priors to enforce stable semantic-visual alignment. Meanwhile, the CA module employs lightweight adapters to mitigate domain gaps between pre-trained models and downstream tasks, thereby enhancing model plasticity. Extensive experiments across standard and challenging MLCIL settings on MS-COCO and PASCAL VOC show that P2L-CA not only achieves substantial improvements over state-of-the-art methods but also demonstrates strong generalization in CIL scenarios, all while requiring minimal trainable parameters and eliminating the need for memory buffers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 189,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12752",
        "abs_url": "https://arxiv.org/abs/2601.12752",
        "pdf_url": "https://arxiv.org/pdf/2601.12752",
        "title": "SoundPlot: An Open-Source Framework for Birdsong Acoustic Analysis and Neural Synthesis with Interactive 3D Visualization",
        "authors": [
            "Naqcho Ali Mehdi",
            "Mohammad Adeel",
            "Aizaz Ali Larik"
        ],
        "comments": "",
        "subjects": "Sound (cs.SD); Machine Learning (cs.LG)",
        "abstract": "We present SoundPlot, an open-source framework for analyzing avian vocalizations through acoustic feature extraction, dimensionality reduction, and neural audio synthesis. The system transforms audio signals into a multi-dimensional acoustic feature space, enabling real-time visualization of temporal dynamics in 3D using web-based interactive graphics. Our framework implements a complete analysis-synthesis pipeline that extracts spectral features (centroid, bandwidth, contrast), pitch contours via probabilistic YIN (pYIN), and mel-frequency cepstral coefficients (MFCCs), mapping them to a unified timbre space for visualization. Audio reconstruction employs the Griffin-Lim phase estimation algorithm applied to mel spectrograms. The accompanying this http URL-based interface provides dual-viewport visualization comparing original and synthesized audio trajectories with independent playback controls. We demonstrate the framework's capabilities through comprehensive waveform analysis, spectrogram comparisons, and feature space evaluation using Principal Component Analysis (PCA). Quantitative evaluation shows mel spectrogram correlation scores exceeding 0.92, indicating high-fidelity preservation of perceptual acoustic structure. SoundPlot is released under the MIT License to facilitate research in bioacoustics, audio signal processing, and computational ethology.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 190,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12804",
        "abs_url": "https://arxiv.org/abs/2601.12804",
        "pdf_url": "https://arxiv.org/pdf/2601.12804",
        "title": "SL-CBM: Enhancing Concept Bottleneck Models with Semantic Locality for Better Interpretability",
        "authors": [
            "Hanwei Zhang",
            "Luo Cheng",
            "Rui Wen",
            "Yang Zhang",
            "Lijun Zhang",
            "Holger Hermanns"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Explainable AI (XAI) is crucial for building transparent and trustworthy machine learning systems, especially in high-stakes domains. Concept Bottleneck Models (CBMs) have emerged as a promising ante-hoc approach that provides interpretable, concept-level explanations by explicitly modeling human-understandable concepts. However, existing CBMs often suffer from poor locality faithfulness, failing to spatially align concepts with meaningful image regions, which limits their interpretability and reliability. In this work, we propose SL-CBM (CBM with Semantic Locality), a novel extension that enforces locality faithfulness by generating spatially coherent saliency maps at both concept and class levels. SL-CBM integrates a 1x1 convolutional layer with a cross-attention mechanism to enhance alignment between concepts, image regions, and final predictions. Unlike prior methods, SL-CBM produces faithful saliency maps inherently tied to the model's internal reasoning, facilitating more effective debugging and intervention. Extensive experiments on image datasets demonstrate that SL-CBM substantially improves locality faithfulness, explanation quality, and intervention efficacy while maintaining competitive classification accuracy. Our ablation studies highlight the importance of contrastive and entropy-based regularization for balancing accuracy, sparsity, and faithfulness. Overall, SL-CBM bridges the gap between concept-based reasoning and spatial explainability, setting a new standard for interpretable and trustworthy concept-based models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 191,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12809",
        "abs_url": "https://arxiv.org/abs/2601.12809",
        "pdf_url": "https://arxiv.org/pdf/2601.12809",
        "title": "Left-Right Symmetry Breaking in CLIP-style Vision-Language Models Trained on Synthetic Spatial-Relation Data",
        "authors": [
            "Takaki Yamamoto",
            "Chihiro Noguchi",
            "Toshihiro Tanizawa"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Spatial understanding remains a key challenge in vision-language models. Yet it is still unclear whether such understanding is truly acquired, and if so, through what mechanisms. We present a controllable 1D image-text testbed to probe how left-right relational understanding emerges in Transformer-based vision and text encoders trained with a CLIP-style contrastive objective. We train lightweight Transformer-based vision and text encoders end-to-end on paired descriptions of one- and two-object scenes and evaluate generalization to unseen object pairs while systematically varying label and layout diversity. We find that contrastive training learns left-right relations and that label diversity, more than layout diversity, is the primary driver of generalization in this setting. To gain the mechanistic understanding, we perform an attention decomposition and show that interactions between positional and token embeddings induce a horizontal attention gradient that breaks left-right symmetry in the encoders; ablating this contribution substantially reduces left-right discrimination. Our results provide a mechanistic insight of when and how CLIP-style models acquire relational competence.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 192,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12856",
        "abs_url": "https://arxiv.org/abs/2601.12856",
        "pdf_url": "https://arxiv.org/pdf/2601.12856",
        "title": "Mining Citywide Dengue Spread Patterns in Singapore Through Hotspot Dynamics from Open Web Data",
        "authors": [
            "Liping Huang",
            "Gaoxi Xiao",
            "Stefan Ma",
            "Hechang Chen",
            "Shisong Tang",
            "Flora Salim"
        ],
        "comments": "9 pages, 9 figures. It's accepted by WWW 2026 Web4Good Track. To make accessible earlier, authors would like to put it on arxiv before the conference",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Dengue, a mosquito-borne disease, continues to pose a persistent public health challenge in urban areas, particularly in tropical regions such as Singapore. Effective and affordable control requires anticipating where transmission risks are likely to emerge so that interventions can be deployed proactively rather than reactively. This study introduces a novel framework that uncovers and exploits latent transmission links between urban regions, mined directly from publicly available dengue case data. Instead of treating cases as isolated reports, we model how hotspot formation in one area is influenced by epidemic dynamics in neighboring regions. While mosquito movement is highly localized, long-distance transmission is often driven by human mobility, and in our case study, the learned network aligns closely with commuting flows, providing an interpretable explanation for citywide spread. These hidden links are optimized through gradient descent and used not only to forecast hotspot status but also to verify the consistency of spreading patterns, by examining the stability of the inferred network across consecutive weeks. Case studies on Singapore during 2013-2018 and 2020 show that four weeks of hotspot history are sufficient to achieve an average F-score of 0.79. Importantly, the learned transmission links align with commuting flows, highlighting the interpretable interplay between hidden epidemic spread and human mobility. By shifting from simply reporting dengue cases to mining and validating hidden spreading dynamics, this work transforms open web-based case data into a predictive and explanatory resource. The proposed framework advances epidemic modeling while providing a scalable, low-cost tool for public health planning, early intervention, and urban resilience.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 193,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12866",
        "abs_url": "https://arxiv.org/abs/2601.12866",
        "pdf_url": "https://arxiv.org/pdf/2601.12866",
        "title": "PDFInspect: A Unified Feature Extraction Framework for Malicious Document Detection",
        "authors": [
            "Sharmila S P"
        ],
        "comments": "6 pages, 2 figures, paper accepted in COMSNETS 2026 conference",
        "subjects": "Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
        "abstract": "The increasing prevalence of malicious Portable Document Format (PDF) files necessitates robust and comprehensive feature extraction techniques for effective detection and analysis. This work presents a unified framework that integrates graph-based, structural, and metadata-driven analysis to generate a rich feature representation for each PDF document. The system extracts text from PDF pages and constructs undirected graphs based on pairwise word relationships, enabling the computation of graph-theoretic features such as node count, edge density, and clustering coefficient. Simultaneously, the framework parses embedded metadata to quantify character distributions, entropy patterns, and inconsistencies across fields such as author, title, and producer. Temporal features are derived from creation and modification timestamps to capture behavioral signatures, while structural elements including, object streams, fonts, and embedded images, are quantified to reflect document complexity. Boolean flags for potentially malicious PDF constructs (e.g., JavaScript, launch actions) are also extracted. Together, these features form a high-dimensional vector representation (170 dimensions) that is well-suited for downstream tasks such as malware classification, anomaly detection, and forensic analysis. The proposed approach is scalable, extensible, and designed to support real-world PDF threat intelligence workflows.6",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 194,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12868",
        "abs_url": "https://arxiv.org/abs/2601.12868",
        "pdf_url": "https://arxiv.org/pdf/2601.12868",
        "title": "Race, Ethnicity and Their Implication on Bias in Large Language Models",
        "authors": [
            "Shiyue Hu",
            "Ruizhe Li",
            "Yanjun Gao"
        ],
        "comments": "Work in process",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Large language models (LLMs) increasingly operate in high-stakes settings including healthcare and medicine, where demographic attributes such as race and ethnicity may be explicitly stated or implicitly inferred from text. However, existing studies primarily document outcome-level disparities, offering limited insight into internal mechanisms underlying these effects. We present a mechanistic study of how race and ethnicity are represented and operationalized within LLMs. Using two publicly available datasets spanning toxicity-related generation and clinical narrative understanding tasks, we analyze three open-source models with a reproducible interpretability pipeline combining probing, neuron-level attribution, and targeted intervention. We find that demographic information is distributed across internal units with substantial cross-model variation. Although some units encode sensitive or stereotype-related associations from pretraining, identical demographic cues can induce qualitatively different behaviors. Interventions suppressing such neurons reduce bias but leave substantial residual effects, suggesting behavioral rather than representational change and motivating more systematic mitigation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 195,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12886",
        "abs_url": "https://arxiv.org/abs/2601.12886",
        "pdf_url": "https://arxiv.org/pdf/2601.12886",
        "title": "Communication Methods in Multi-Agent Reinforcement Learning",
        "authors": [
            "Christoph Wittner"
        ],
        "comments": "12 pages, 2 figures",
        "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Multi-agent reinforcement learning is a promising research area that extends established reinforcement learning approaches to problems formulated as multi-agent systems. Recently, a multitude of communication methods have been introduced to this field to address problems such as partially observable environments, non-stationarity, and exponentially growing action spaces. Communication further enables efficient cooperation among all agents interacting in an environment. This work aims at providing an overview of communication techniques in multi-agent reinforcement learning. By an in-depth analysis of 29 publications on this topic, the strengths and weaknesses of explicit, implicit, attention-based, graph-based, and hierarchical/role-based communication are evaluated. The results of this comparison show that there is no general, optimal communication framework for every problem. On the contrary, the choice of communication depends heavily on the problem at hand. The comparison also highlights the importance of communication methods with low computational overhead to enable scalability to environments where many agents interact. Finally, the paper discusses current research gaps, emphasizing the need for standardized benchmarking of system-level metrics and improved robustness under realistic communication conditions to enhance the real-world applicability of these approaches.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 196,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12913",
        "abs_url": "https://arxiv.org/abs/2601.12913",
        "pdf_url": "https://arxiv.org/pdf/2601.12913",
        "title": "Actionable Interpretability Must Be Defined in Terms of Symmetries",
        "authors": [
            "Pietro Barbiero",
            "Mateo Espinosa Zarlenga",
            "Francesco Giannini",
            "Alberto Termine",
            "Filippo Bonchi",
            "Mateja Jamnik",
            "Giuseppe Marra"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)",
        "abstract": "This paper argues that interpretability research in Artificial Intelligence is fundamentally ill-posed as existing definitions of interpretability are not *actionable*: they fail to provide formal principles from which concrete modelling and inferential rules can be derived. We posit that for a definition of interpretability to be actionable, it must be given in terms of *symmetries*. We hypothesise that four symmetries suffice to (i) motivate core interpretability properties, (ii) characterize the class of interpretable models, and (iii) derive a unified formulation of interpretable inference (e.g., alignment, interventions, and counterfactuals) as a form of Bayesian inversion.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 197,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12918",
        "abs_url": "https://arxiv.org/abs/2601.12918",
        "pdf_url": "https://arxiv.org/pdf/2601.12918",
        "title": "Dynamic Hand Gesture Recognition for Robot Manipulator Tasks",
        "authors": [
            "Dharmendra Sharma",
            "Peeyush Thakur",
            "Sandeep Gupta",
            "Narendra Kumar Dhar",
            "Laxmidhar Behera"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
        "abstract": "This paper proposes a novel approach to recognizing dynamic hand gestures facilitating seamless interaction between humans and robots. Here, each robot manipulator task is assigned a specific gesture. There may be several such tasks, hence, several gestures. These gestures may be prone to several dynamic variations. All such variations for different gestures shown to the robot are accurately recognized in real-time using the proposed unsupervised model based on the Gaussian Mixture model. The accuracy during training and real-time testing prove the efficacy of this methodology.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 198,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12945",
        "abs_url": "https://arxiv.org/abs/2601.12945",
        "pdf_url": "https://arxiv.org/pdf/2601.12945",
        "title": "A Component-Based Survey of Interactions between Large Language Models and Multi-Armed Bandits",
        "authors": [
            "Miao Xie",
            "Siguang Chen",
            "Chunli Lv"
        ],
        "comments": "27 pages, 6 table",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Large language models (LLMs) have become powerful and widely used systems for language understanding and generation, while multi-armed bandit (MAB) algorithms provide a principled framework for adaptive decision-making under uncertainty. This survey explores the potential at the intersection of these two fields. As we know, it is the first survey to systematically review the bidirectional interaction between large language models and multi-armed bandits at the component level. We highlight the bidirectional benefits: MAB algorithms address critical LLM challenges, spanning from pre-training to retrieval-augmented generation (RAG) and personalization. Conversely, LLMs enhance MAB systems by redefining core components such as arm definition and environment modeling, thereby improving decision-making in sequential tasks. We analyze existing LLM-enhanced bandit systems and bandit-enhanced LLM systems, providing insights into their design, methodologies, and performance. Key challenges and representative findings are identified to help guide future research. An accompanying GitHub repository that indexes relevant literature is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 199,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12981",
        "abs_url": "https://arxiv.org/abs/2601.12981",
        "pdf_url": "https://arxiv.org/pdf/2601.12981",
        "title": "Early Prediction of Type 2 Diabetes Using Multimodal data and Tabular Transformers",
        "authors": [
            "Sulaiman Khan",
            "Md. Rafiul Biswas",
            "Zubair Shah"
        ],
        "comments": "08 pages, 06 figures, accepted for publication in FLLM2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "This study introduces a novel approach for early Type 2 Diabetes Mellitus (T2DM) risk prediction using a tabular transformer (TabTrans) architecture to analyze longitudinal patient data. By processing patients` longitudinal health records and bone-related tabular data, our model captures complex, long-range dependencies in disease progression that conventional methods often overlook. We validated our TabTrans model on a retrospective Qatar BioBank (QBB) cohort of 1,382 subjects, comprising 725 men (146 diabetic, 579 healthy) and 657 women (133 diabetic, 524 healthy). The study integrated electronic health records (EHR) with dual-energy X-ray absorptiometry (DXA) data. To address class imbalance, we employed SMOTE and SMOTE-ENN resampling techniques. The proposed model`s performance is evaluated against conventional machine learning (ML) and generative AI models, including Claude 3.5 Sonnet (Anthropic`s constitutional AI), GPT-4 (OpenAI`s generative pre-trained transformer), and Gemini Pro (Google`s multimodal language model). Our TabTrans model demonstrated superior predictive performance, achieving ROC AUC $\\geq$ 79.7 % for T2DM prediction compared to both generative AI models and conventional ML approaches. Feature interpretation analysis identified key risk indicators, with visceral adipose tissue (VAT) mass and volume, ward bone mineral density (BMD) and bone mineral content (BMC), T and Z-scores, and L1-L4 scores emerging as the most important predictors associated with diabetes development in Qatari adults. These findings demonstrate the significant potential of TabTrans for analyzing complex tabular healthcare data, providing a powerful tool for proactive T2DM management and personalized clinical interventions in the Qatari population. Index Terms: tabular transformers, multimodal data, DXA data, diabetes, T2DM, feature interpretation, tabular data",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 200,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12990",
        "abs_url": "https://arxiv.org/abs/2601.12990",
        "pdf_url": "https://arxiv.org/pdf/2601.12990",
        "title": "Beyond Visual Realism: Toward Reliable Financial Time Series Generation",
        "authors": [
            "Fan Zhang",
            "Jiabin Luo",
            "Zheng Zhang",
            "Shuanghong Huang",
            "Zhipeng Liu",
            "Yu Chen"
        ],
        "comments": "Accepted by ICASSP 2026",
        "subjects": "Statistical Finance (q-fin.ST); Machine Learning (cs.LG)",
        "abstract": "Generative models for financial time series often create data that look realistic and even reproduce stylized facts such as fat tails or volatility clustering. However, these apparent successes break down under trading backtests: models like GANs or WGAN-GP frequently collapse, yielding extreme and unrealistic results that make the synthetic data unusable in practice. We identify the root cause in the neglect of financial asymmetry and rare tail events, which strongly affect market risk but are often overlooked by objectives focusing on distribution matching. To address this, we introduce the Stylized Facts Alignment GAN (SFAG), which converts key stylized facts into differentiable structural constraints and jointly optimizes them with adversarial loss. This multi-constraint design ensures that generated series remain aligned with market dynamics not only in plots but also in backtesting. Experiments on the Shanghai Composite Index (2004--2024) show that while baseline GANs produce unstable and implausible trading outcomes, SFAG generates synthetic data that preserve stylized facts and support robust momentum strategy performance. Our results highlight that structure-preserving objectives are essential to bridge the gap between superficial realism and practical usability in financial generative modeling.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 201,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.12996",
        "abs_url": "https://arxiv.org/abs/2601.12996",
        "pdf_url": "https://arxiv.org/pdf/2601.12996",
        "title": "OFA-MAS: One-for-All Multi-Agent System Topology Design based on Mixture-of-Experts Graph Generative Models",
        "authors": [
            "Shiyuan Li",
            "Yixin Liu",
            "Yu Zheng",
            "Mei Li",
            "Quoc Viet Hung Nguyen",
            "Shirui Pan"
        ],
        "comments": "Accepted by WWW 2026",
        "subjects": "Multiagent Systems (cs.MA); Machine Learning (cs.LG)",
        "abstract": "Multi-Agent Systems (MAS) offer a powerful paradigm for solving complex problems, yet their performance is critically dependent on the design of their underlying collaboration topology. As MAS become increasingly deployed in web services (e.g., search engines), designing adaptive topologies for diverse cross-domain user queries becomes essential. Current graph learning-based design methodologies often adhere to a \"one-for-one\" paradigm, where a specialized model is trained for each specific task domain. This approach suffers from poor generalization to unseen domains and fails to leverage shared structural knowledge across different tasks. To address this, we propose OFA-TAD, a one-for-all framework that generates adaptive collaboration graphs for any task described in natural language through a single universal model. Our approach integrates a Task-Aware Graph State Encoder (TAGSE) that filters task-relevant node information via sparse gating, and a Mixture-of-Experts (MoE) architecture that dynamically selects specialized sub-networks to drive node and edge prediction. We employ a three-stage training strategy: unconditional pre-training on canonical topologies for structural priors, large-scale conditional pre-training on LLM-generated datasets for task-topology mappings, and supervised fine-tuning on empirically validated graphs. Experiments across six diverse benchmarks show that OFA-TAD significantly outperforms specialized one-for-one models, generating highly adaptive MAS topologies. Code: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 202,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13035",
        "abs_url": "https://arxiv.org/abs/2601.13035",
        "pdf_url": "https://arxiv.org/pdf/2601.13035",
        "title": "SASA: Semantic-Aware Contrastive Learning Framework with Separated Attention for Triple Classification",
        "authors": [
            "Xu Xiaodan",
            "Hu Xiaolin"
        ],
        "comments": "in progress",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Knowledge Graphs~(KGs) often suffer from unreliable knowledge, which restricts their utility. Triple Classification~(TC) aims to determine the validity of triples from KGs. Recently, text-based methods learn entity and relation representations from natural language descriptions, significantly improving the generalization capabilities of TC models and setting new benchmarks in performance. However, there are still two critical challenges. First, existing methods often ignore the effective semantic interaction among different KG components. Second, most approaches adopt single binary classification training objective, leading to insufficient semantic representation learning. To address these challenges, we propose \\textbf{SASA}, a novel framework designed to enhance TC models via separated attention mechanism and semantic-aware contrastive learning~(CL). Specifically, we first propose separated attention mechanism to encode triples into decoupled contextual representations and then fuse them through a more effective interactive way. Then, we introduce semantic-aware hierarchical CL as auxiliary training objective to guide models in improving their discriminative capabilities and achieving sufficient semantic learning, considering both local level and global level CL. Experimental results across two benchmark datasets demonstrate that SASA significantly outperforms state-of-the-art methods. In terms of accuracy, we advance the state-of-the-art by +5.9\\% on FB15k-237 and +3.4\\% on YAGO3-10.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 203,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13079",
        "abs_url": "https://arxiv.org/abs/2601.13079",
        "pdf_url": "https://arxiv.org/pdf/2601.13079",
        "title": "Polychronous Wave Computing: Timing-Native Address Selection in Spiking Networks",
        "authors": [
            "Natalila G. Berloff"
        ],
        "comments": "23 pages, Supplementary Materials are available at this https URL",
        "subjects": "Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Optics (physics.optics)",
        "abstract": "Spike timing offers a combinatorial address space, suggesting that timing-based spiking inference can be executed as lookup and routing rather than as dense multiply--accumulate. Yet most neuromorphic and photonic systems still digitize events into timestamps, bins, or rates and then perform selection in clocked logic. We introduce Polychronous Wave Computing (PWC), a timing-native address-selection primitive that maps relative spike latencies directly to a discrete output route in the wave domain. Spike times are phase-encoded in a rotating frame and processed by a programmable multiport interferometer that evaluates K template correlations in parallel; a driven--dissipative winner-take-all stage then performs a physical argmax, emitting a one-hot output port. We derive the operating envelope imposed by phase wrapping and mutual coherence, and collapse timing jitter, static phase mismatch, and dephasing into a single effective phase-noise budget whose induced winner--runner-up margin predicts boundary-first failures and provides an intensity-only calibration target. Simulations show that nonlinear competition improves routing fidelity compared with noisy linear intensity readout, and that hardware-in-the-loop phase tuning rescues a temporal-order gate from 55.9% to 97.2% accuracy under strong static mismatch. PWC provides a fast routing coprocessor for LUT-style spiking networks and sparse top-1 gates (e.g., mixture-of-experts routing) across polaritonic, photonic, and oscillator platforms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 204,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13082",
        "abs_url": "https://arxiv.org/abs/2601.13082",
        "pdf_url": "https://arxiv.org/pdf/2601.13082",
        "title": "Adversarial News and Lost Profits: Manipulating Headlines in LLM-Driven Algorithmic Trading",
        "authors": [
            "Advije Rizvani",
            "Giovanni Apruzzese",
            "Pavel Laskov"
        ],
        "comments": "This work has been accepted for publication at the IEEE Conference on Secure and Trustworthy Machine Learning (SaTML). The final version will be available on IEEE Xplore",
        "subjects": "Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
        "abstract": "Large Language Models (LLMs) are increasingly adopted in the financial domain. Their exceptional capabilities to analyse textual data make them well-suited for inferring the sentiment of finance-related news. Such feedback can be leveraged by algorithmic trading systems (ATS) to guide buy/sell decisions. However, this practice bears the risk that a threat actor may craft \"adversarial news\" intended to mislead an LLM. In particular, the news headline may include \"malicious\" content that remains invisible to human readers but which is still ingested by the LLM. Although prior work has studied textual adversarial examples, their system-wide impact on LLM-supported ATS has not yet been quantified in terms of monetary risk. To address this threat, we consider an adversary with no direct access to an ATS but able to alter stock-related news headlines on a single day. We evaluate two human-imperceptible manipulations in a financial context: Unicode homoglyph substitutions that misroute models during stock-name recognition, and hidden-text clauses that alter the sentiment of the news headline. We implement a realistic ATS in Backtrader that fuses an LSTM-based price forecast with LLM-derived sentiment (FinBERT, FinGPT, FinLLaMA, and six general-purpose LLMs), and quantify monetary impact using portfolio metrics. Experiments on real-world data show that manipulating a one-day attack over 14 months can reliably mislead LLMs and reduce annual returns by up to 17.7 percentage points. To assess real-world feasibility, we analyze popular scraping libraries and trading platforms and survey 27 FinTech practitioners, confirming our hypotheses. We notified trading platform owners of this security issue.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 205,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13097",
        "abs_url": "https://arxiv.org/abs/2601.13097",
        "pdf_url": "https://arxiv.org/pdf/2601.13097",
        "title": "RM -RF: Reward Model for Run-Free Unit Test Evaluation",
        "authors": [
            "Elena Bruches",
            "Daniil Grebenkin",
            "Mikhail Klementev",
            "Vadim Alperovich",
            "Roman Derunets",
            "Dari Baturova",
            "Georgy Mkrtchyan",
            "Oleg Sedukhin",
            "Ivan Bondarenko",
            "Nikolay Bushkov",
            "Stanislav Moiseev"
        ],
        "comments": "This paper has been accepted for publication at the 33rd IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER 2026)",
        "subjects": "Software Engineering (cs.SE); Machine Learning (cs.LG)",
        "abstract": "We present RM-RF, a lightweight reward model for run-free evaluation of automatically generated unit tests. Instead of repeatedly compiling and executing candidate tests, RM-RF predicts - from source and test code alone - three execution-derived signals: (1) whether the augmented test suite compiles and runs successfully, (2) whether the generated test cases increase code coverage, and (3) whether the generated test cases improve the mutation kill rate. To train and evaluate RM-RF we assemble a multilingual dataset (Java, Python, Go) of focal files, test files, and candidate test additions labeled by an execution-based pipeline, and we release an associated dataset and methodology for comparative evaluation. We tested multiple model families and tuning regimes (zero-shot, full fine-tuning, and PEFT via LoRA), achieving an average F1 of 0.69 across the three targets. Compared to conventional compile-and-run instruments, RM-RF provides substantially lower latency and infrastructure cost while delivering competitive predictive fidelity, enabling fast, scalable feedback for large-scale test generation and RL-based code optimization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 206,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13102",
        "abs_url": "https://arxiv.org/abs/2601.13102",
        "pdf_url": "https://arxiv.org/pdf/2601.13102",
        "title": "Approximate full conformal prediction in RKHS",
        "authors": [
            "Davidson Lova Razafindrakoto",
            "Alain Celisse",
            "Jérôme Lacaille"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Statistics Theory (math.ST)",
        "abstract": "Full conformal prediction is a framework that implicitly formulates distribution-free confidence prediction regions for a wide range of estimators. However, a classical limitation of the full conformal framework is the computation of the confidence prediction regions, which is usually impossible since it requires training infinitely many estimators (for real-valued prediction for instance). The main purpose of the present work is to describe a generic strategy for designing a tight approximation to the full conformal prediction region that can be efficiently computed. Along with this approximate confidence region, a theoretical quantification of the tightness of this approximation is developed, depending on the smoothness assumptions on the loss and score functions. The new notion of thickness is introduced for quantifying the discrepancy between the approximate confidence region and the full conformal one.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 207,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13144",
        "abs_url": "https://arxiv.org/abs/2601.13144",
        "pdf_url": "https://arxiv.org/pdf/2601.13144",
        "title": "Forecasting Continuum Intensity for Solar Active Region Emergence Prediction using Transformers",
        "authors": [
            "Jonas Tirona",
            "Sarang Patil",
            "Spiridon Kasapis",
            "Eren Dogan",
            "John Stefan",
            "Irina N. Kitiashvili",
            "Alexander G. Kosovichev",
            "Mengjia Xu"
        ],
        "comments": "30 pages, 7 figures, submitted to JGR: Machine Learning and Computation",
        "subjects": "Solar and Stellar Astrophysics (astro-ph.SR); Machine Learning (cs.LG)",
        "abstract": "Early and accurate prediction of solar active region (AR) emergence is crucial for space weather forecasting. Building on established Long Short-Term Memory (LSTM) based approaches for forecasting the continuum intensity decrease associated with AR emergence, this work expands the modeling with new architectures and targets. We investigate a sliding-window Transformer architecture to forecast continuum intensity evolution up to 12 hours ahead using data from 46 ARs observed by SDO/HMI. We conduct a systematic ablation study to evaluate two key components: (1) the inclusion of a temporal 1D convolutional (Conv1D) front-end and (2) a novel `Early Detection' architecture featuring attention biases and a timing-aware loss function. Our best-performing model, combining the Early Detection architecture without the Conv1D layer, achieved a Root Mean Square Error (RMSE) of 0.1189 (representing a 10.6% improvement over the LSTM baseline) and an average advance warning time of 4.73 hours (timing difference of -4.73h), even under a stricter emergence criterion than previous studies. While the Transformer demonstrates superior aggregate timing and accuracy, we note that this high-sensitivity detection comes with increased variance compared to smoother baseline models. However, this volatility is a necessary trade-off for operational warning systems: the model's ability to detect micro-changes in precursor signals enables significantly earlier detection, outweighing the cost of increased noise. Our results demonstrate that Transformer architectures modified with early detection biases, when used without temporal smoothing layers, provide a high-sensitivity alternative for forecasting AR emergence that prioritizes advance warning over statistical smoothness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 208,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13145",
        "abs_url": "https://arxiv.org/abs/2601.13145",
        "pdf_url": "https://arxiv.org/pdf/2601.13145",
        "title": "SolARED: Solar Active Region Emergence Dataset for Machine Learning Aided Predictions",
        "authors": [
            "Spiridon Kasapis",
            "Eren Dogan",
            "Irina N. Kitiashvili",
            "Alexander G. Kosovichev",
            "John T. Stefan",
            "Jake D. Butler",
            "Jonas Tirona",
            "Sarang Patil",
            "Mengjia Xu"
        ],
        "comments": "15 pages, 6 figures, submitted to the Springer Nature - Solar Physics Journal",
        "subjects": "Solar and Stellar Astrophysics (astro-ph.SR); Machine Learning (cs.LG)",
        "abstract": "The development of accurate forecasts of solar eruptive activity has become increasingly important for preventing potential impacts on space technologies and exploration. Therefore, it is crucial to detect Active Regions (ARs) before they start forming on the solar surface. This will enable the development of early-warning capabilities for upcoming space weather disturbances. For this reason, we prepared the Solar Active Region Emergence Dataset (SolARED). The dataset is derived from full-disk maps of the Doppler velocity, magnetic field, and continuum intensity, obtained by the Helioseismic and Magnetic Imager (HMI) onboard the Solar Dynamics Observatory (SDO). SolARED includes time series of remapped, tracked, and binned data that characterize the evolution of acoustic power of solar oscillations, unsigned magnetic flux, and continuum intensity for 50 large ARs before, during, and after their emergence on the solar surface, as well as surrounding areas observed on the solar disc between 2010 and 2023. The resulting ML-ready SolARED dataset is designed to support enhancements of predictive capabilities, enabling the development of operational forecasts for the emergence of active regions. The SolARED dataset is available at this https URL, through an interactive visualization web application.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 209,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13155",
        "abs_url": "https://arxiv.org/abs/2601.13155",
        "pdf_url": "https://arxiv.org/pdf/2601.13155",
        "title": "Probe and Skip: Self-Predictive Token Skipping for Efficient Long-Context LLM Inference",
        "authors": [
            "Zimeng Wu",
            "Donghao Wang",
            "Chaozhe Jin",
            "Jiaxin Chen",
            "Yunhong Wang"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Long-context inference enhances the reasoning capability of Large Language Models (LLMs) while incurring significant computational overhead. Token-oriented methods, such as pruning and skipping, have shown promise in reducing inference latency, but still suffer from inherently limited acceleration potential, outdated proxy signals, and redundancy interference, thus yielding suboptimal speed-accuracy trade-offs. To address these challenges, we propose SPTS (Self-Predictive Token Skipping), a training-free framework for efficient long-context LLM inference. Specifically, motivated by the thought of probing the influence of targeted skipping layers, we design two component-specific strategies for selective token skipping: Partial Attention Probing (PAP) for multi-head attention, which selects informative tokens by performing partial forward attention computation, and Low-rank Transformation Probing (LTP) for feed forward network, which constructs a low-rank proxy network to predict token transformations. Furthermore, a Multi-Stage Delayed Pruning (MSDP) strategy reallocates the skipping budget and progressively prunes redundant tokens across layers. Extensive experiments demonstrate the effectiveness of our method, achieving up to 2.46$\\times$ and 2.29$\\times$ speedups for prefilling and end-to-end generation, respectively, while maintaining state-of-the-art model performance. The source code will be publicly available upon paper acceptance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 210,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13191",
        "abs_url": "https://arxiv.org/abs/2601.13191",
        "pdf_url": "https://arxiv.org/pdf/2601.13191",
        "title": "Empirical Risk Minimization with $f$-Divergence Regularization",
        "authors": [
            "Francisco Daunas",
            "Iñaki Esnaola",
            "Samir M. Perlaza",
            "H. Vincent Poor"
        ],
        "comments": "Submitted to IEEE Transactions on Information Theory. arXiv admin note: substantial text overlap with arXiv:2502.14544, arXiv:2508.03314",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "In this paper, the solution to the empirical risk minimization problem with $f$-divergence regularization (ERM-$f$DR) is presented and conditions under which the solution also serves as the solution to the minimization of the expected empirical risk subject to an $f$-divergence constraint are established. The proposed approach extends applicability to a broader class of $f$-divergences than previously reported and yields theoretical results that recover previously known results. Additionally, the difference between the expected empirical risk of the ERM-$f$DR solution and that of its reference measure is characterized, providing insights into previously studied cases of $f$-divergences. A central contribution is the introduction of the normalization function, a mathematical object that is critical in both the dual formulation and practical computation of the ERM-$f$DR solution. This work presents an implicit characterization of the normalization function as a nonlinear ordinary differential equation (ODE), establishes its key properties, and subsequently leverages them to construct a numerical algorithm for approximating the normalization factor under mild assumptions. Further analysis demonstrates structural equivalences between ERM-$f$DR problems with different $f$-divergences via transformations of the empirical risk. Finally, the proposed algorithm is used to compute the training and test risks of ERM-$f$DR solutions under different $f$-divergence regularizers. This numerical example highlights the practical implications of choosing different functions $f$ in ERM-$f$DR problems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 211,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13235",
        "abs_url": "https://arxiv.org/abs/2601.13235",
        "pdf_url": "https://arxiv.org/pdf/2601.13235",
        "title": "RubRIX: Rubric-Driven Risk Mitigation in Caregiver-AI Interactions",
        "authors": [
            "Drishti Goel",
            "Jeongah Lee",
            "Qiuyue Joy Zhong",
            "Violeta J. Rodriguez",
            "Daniel S. Brown",
            "Ravi Karkar",
            "Dong Whi Yoo",
            "Koustuv Saha"
        ],
        "comments": "",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computers and Society (cs.CY); Machine Learning (cs.LG); Social and Information Networks (cs.SI)",
        "abstract": "Caregivers seeking AI-mediated support express complex needs -- information-seeking, emotional validation, and distress cues -- that warrant careful evaluation of response safety and appropriateness. Existing AI evaluation frameworks, primarily focused on general risks (toxicity, hallucinations, policy violations, etc), may not adequately capture the nuanced risks of LLM-responses in caregiving-contexts. We introduce RubRIX (Rubric-based Risk Index), a theory-driven, clinician-validated framework for evaluating risks in LLM caregiving responses. Grounded in the Elements of an Ethic of Care, RubRIX operationalizes five empirically-derived risk dimensions: Inattention, Bias & Stigma, Information Inaccuracy, Uncritical Affirmation, and Epistemic Arrogance. We evaluate six state-of-the-art LLMs on over 20,000 caregiver queries from Reddit and ALZConnected. Rubric-guided refinement consistently reduced risk-components by 45-98% after one iteration across models. This work contributes a methodological approach for developing domain-sensitive, user-centered evaluation frameworks for high-burden contexts. Our findings highlight the importance of domain-sensitive, interactional risk evaluation for the responsible deployment of LLMs in caregiving support contexts. We release benchmark datasets to enable future research on contextual risk evaluation in AI-mediated support.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 212,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13240",
        "abs_url": "https://arxiv.org/abs/2601.13240",
        "pdf_url": "https://arxiv.org/pdf/2601.13240",
        "title": "KOCO-BENCH: Can Large Language Models Leverage Domain Knowledge in Software Development?",
        "authors": [
            "Xue Jiang",
            "Jiaru Qian",
            "Xianjie Shi",
            "Chenjie Li",
            "Hao Zhu",
            "Ziyu Wang",
            "Jielun Zhang",
            "Zheyu Zhao",
            "Kechi Zhang",
            "Jia Li",
            "Wenpin Jiao",
            "Zhi Jin",
            "Ge Li",
            "Yihong Dong"
        ],
        "comments": "",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Large language models (LLMs) excel at general programming but struggle with domain-specific software development, necessitating domain specialization methods for LLMs to learn and utilize domain knowledge and data. However, existing domain-specific code benchmarks cannot evaluate the effectiveness of domain specialization methods, which focus on assessing what knowledge LLMs possess rather than how they acquire and apply new knowledge, lacking explicit knowledge corpora for developing domain specialization methods. To this end, we present KOCO-BENCH, a novel benchmark designed for evaluating domain specialization methods in real-world software development. KOCO-BENCH contains 6 emerging domains with 11 software frameworks and 25 projects, featuring curated knowledge corpora alongside multi-granularity evaluation tasks including domain code generation (from function-level to project-level with rigorous test suites) and domain knowledge understanding (via multiple-choice Q&A). Unlike previous benchmarks that only provide test sets for direct evaluation, KOCO-BENCH requires acquiring and applying diverse domain knowledge (APIs, rules, constraints, etc.) from knowledge corpora to solve evaluation tasks. Our evaluations reveal that KOCO-BENCH poses significant challenges to state-of-the-art LLMs. Even with domain specialization methods (e.g., SFT, RAG, kNN-LM) applied, improvements remain marginal. Best-performing coding agent, Claude Code, achieves only 34.2%, highlighting the urgent need for more effective domain specialization methods. We release KOCO-BENCH, evaluation code, and baselines to advance further research at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 213,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13251",
        "abs_url": "https://arxiv.org/abs/2601.13251",
        "pdf_url": "https://arxiv.org/pdf/2601.13251",
        "title": "Beyond Cosine Similarity: Taming Semantic Drift and Antonym Intrusion in a 15-Million Node Turkish Synonym Graph",
        "authors": [
            "Ebubekir Tosun",
            "Mehmet Emin Buldur",
            "Özay Ezerceli",
            "Mahmoud ElHussieni"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Neural embeddings have a notorious blind spot: they can't reliably tell synonyms apart from antonyms. Consequently, increasing similarity thresholds often fails to prevent opposites from being grouped together. We've built a large-scale semantic clustering system specifically designed to tackle this problem head on. Our pipeline chews through 15 million lexical items, evaluates a massive 520 million potential relationships, and ultimately generates 2.9 million high-precision semantic clusters. The system makes three primary contributions. First, we introduce a labeled dataset of 843,000 concept pairs spanning synonymy, antonymy, and co-hyponymy, constructed via Gemini 2.5-Flash LLM augmentation and verified using human-curated dictionary resources. Second, we propose a specialized three-way semantic relation discriminator that achieves 90% macro-F1, enabling robust disambiguation beyond raw embedding similarity. Third, we introduce a novel soft-to-hard clustering algorithm that mitigates semantic drift preventing erroneous transitive chains (e.g., hot -> spicy -> pain -> depression) while simultaneously resolving polysemy. Our approach employs a topology-aware two-stage expansion-pruning procedure with topological voting, ensuring that each term is assigned to exactly one semantically coherent cluster. The resulting resource enables high-precision semantic search and retrieval-augmented generation, particularly for morphologically rich and low-resource languages where existing synonym databases remain sparse.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 214,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13253",
        "abs_url": "https://arxiv.org/abs/2601.13253",
        "pdf_url": "https://arxiv.org/pdf/2601.13253",
        "title": "A Hybrid Protocol for Large-Scale Semantic Dataset Generation in Low-Resource Languages: The Turkish Semantic Relations Corpus",
        "authors": [
            "Ebubekir Tosun",
            "Mehmet Emin Buldur",
            "Özay Ezerceli",
            "Mahmoud ElHussieni"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "We present a hybrid methodology for generating large-scale semantic relationship datasets in low-resource languages, demonstrated through a comprehensive Turkish semantic relations corpus. Our approach integrates three phases: (1) FastText embeddings with Agglomerative Clustering to identify semantic clusters, (2) Gemini 2.5-Flash for automated semantic relationship classification, and (3) integration with curated dictionary sources. The resulting dataset comprises 843,000 unique Turkish semantic pairs across three relationship types (synonyms, antonyms, co-hyponyms) representing a 10x scale increase over existing resources at minimal cost ($65). We validate the dataset through two downstream tasks: an embedding model achieving 90% top-1 retrieval accuracy and a classification model attaining 90% F1-macro. Our scalable protocol addresses critical data scarcity in Turkish NLP and demonstrates applicability to other low-resource languages. We publicly release the dataset and models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 215,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13256",
        "abs_url": "https://arxiv.org/abs/2601.13256",
        "pdf_url": "https://arxiv.org/pdf/2601.13256",
        "title": "Deep Neural networks for solving high-dimensional parabolic partial differential equations",
        "authors": [
            "Wenzhong Zhang",
            "Zhenyuan Hu",
            "Wei Cai",
            "George EM Karniadakis"
        ],
        "comments": "",
        "subjects": "Numerical Analysis (math.NA); Machine Learning (cs.LG)",
        "abstract": "The numerical solution of high dimensional partial differential equations (PDEs) is severely constrained by the curse of dimensionality (CoD), rendering classical grid--based methods impractical beyond a few dimensions. In recent years, deep neural networks have emerged as a promising mesh free alternative, enabling the approximation of PDE solutions in tens to thousands of dimensions. This review provides a tutorial--oriented introduction to neural--network--based methods for solving high dimensional parabolic PDEs, emphasizing conceptual clarity and methodological connections. We organize the literature around three unifying paradigms: (i) PDE residual--based approaches, including physicsinformed neural networks and their high dimensional variants; (ii) stochastic methods derived from Feynman--Kac and backward stochastic differential equation formulations; and (iii) hybrid derivative--free random difference approaches designed to alleviate the computational cost of derivatives in high dimensions. For each paradigm, we outline the underlying mathematical formulation, algorithmic implementation, and practical strengths and limitations. Representative benchmark problems--including Hamilton--Jacobi--Bellman and Black--Scholes equations in up to 1000 dimensions --illustrate the scalability, effectiveness, and accuracy of the methods. The paper concludes with a discussion of open challenges and future directions for reliable and scalable solvers of high dimensional PDEs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 216,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13260",
        "abs_url": "https://arxiv.org/abs/2601.13260",
        "pdf_url": "https://arxiv.org/pdf/2601.13260",
        "title": "Stop Taking Tokenizers for Granted: They Are Core Design Decisions in Large Language Models",
        "authors": [
            "Sawsan Alqahtani",
            "Mir Tafseer Nayeem",
            "Md Tahmid Rahman Laskar",
            "Tasnim Mohiuddin",
            "M Saiful Bari"
        ],
        "comments": "Accepted to EACL 2026 (long, main). The first two authors contributed equally",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Tokenization underlies every large language model, yet it remains an under-theorized and inconsistently designed component. Common subword approaches such as Byte Pair Encoding (BPE) offer scalability but often misalign with linguistic structure, amplify bias, and waste capacity across languages and domains. This paper reframes tokenization as a core modeling decision rather than a preprocessing step. We argue for a context-aware framework that integrates tokenizer and model co-design, guided by linguistic, domain, and deployment considerations. Standardized evaluation and transparent reporting are essential to make tokenization choices accountable and comparable. Treating tokenization as a core design problem, not a technical afterthought, can yield language technologies that are fairer, more efficient, and more adaptable.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 217,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13294",
        "abs_url": "https://arxiv.org/abs/2601.13294",
        "pdf_url": "https://arxiv.org/pdf/2601.13294",
        "title": "The Tag is the Signal: URL-Agnostic Credibility Scoring for Messages on Telegram",
        "authors": [
            "Yipeng Wang",
            "Huy Gia Han Vu",
            "Mohit Singhal"
        ],
        "comments": "",
        "subjects": "Social and Information Networks (cs.SI); Computers and Society (cs.CY); Machine Learning (cs.LG)",
        "abstract": "Telegram has become one of the leading platforms for disseminating misinformational messages. However, many existing pipelines still classify each message's credibility based on the reputation of its associated domain names or its lexical features. Such methods work well on traditional long-form news articles published by well-known sources, but high-risk posts on Telegram are short and URL-sparse, leading to failures for link-based and standard TF-IDF models. To this end, we propose the TAG2CRED pipeline, a method designed for such short, convoluted messages. Our model will directly score each post based on the tags assigned to the text. We designed a concise label system that covers the dimensions of theme, claim type, call to action, and evidence. The fine-tuned large language model (LLM) assigns tags to messages and then maps these tags to calibrated risk scores in the [0,1] interval through L2-regularized logistic regression. We evaluated 87,936 Telegram messages associated with Media Bias/Fact Check (MBFC), using URL masking and domain disjoint splits. The results showed that the ROC-AUC of the TAG2CRED model reached 0.871, the macro-F1 value was 0.787, and the Brier score was 0.167, outperforming the baseline TF-IDF (macro-F1 value 0.737, Brier score 0.248); at the same time, the number of features used in this model is much smaller, and the generalization ability on infrequent domains is stronger. The performance of the stacked ensemble model (TF-IDF + TAG2CRED + SBERT) was further improved over the baseline SBERT. ROC-AUC reached 0.901, and the macro-F1 value was 0.813 (Brier score 0.114). This indicates that style labels and lexical features may capture different but complementary dimensions of information risk.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 218,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13308",
        "abs_url": "https://arxiv.org/abs/2601.13308",
        "pdf_url": "https://arxiv.org/pdf/2601.13308",
        "title": "Scaling laws for amplitude surrogates",
        "authors": [
            "Henning Bahl",
            "Victor Bresó-Pla",
            "Anja Butter",
            "Joaquín Iturriza Ramirez"
        ],
        "comments": "45 pages, 20 figures",
        "subjects": "High Energy Physics - Phenomenology (hep-ph); Machine Learning (cs.LG)",
        "abstract": "Scaling laws describing the dependence of neural network performance on the amount of training data, the spent compute, and the network size have emerged across a huge variety of machine learning task and datasets. In this work, we systematically investigate these scaling laws in the context of amplitude surrogates for particle physics. We show that the scaling coefficients are connected to the number of external particles of the process. Our results demonstrate that scaling laws are a useful tool to achieve desired precision targets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 219,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13331",
        "abs_url": "https://arxiv.org/abs/2601.13331",
        "pdf_url": "https://arxiv.org/pdf/2601.13331",
        "title": "MultiST: A Cross-Attention-Based Multimodal Model for Spatial Transcriptomic",
        "authors": [
            "Wei Wang",
            "Quoc-Toan Ly",
            "Chong Yu",
            "Jun Bai"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Spatial transcriptomics (ST) enables transcriptome-wide profiling while preserving the spatial context of tissues, offering unprecedented opportunities to study tissue organization and cell-cell interactions in situ. Despite recent advances, existing methods often lack effective integration of histological morphology with molecular profiles, relying on shallow fusion strategies or omitting tissue images altogether, which limits their ability to resolve ambiguous spatial domain boundaries. To address this challenge, we propose MultiST, a unified multimodal framework that jointly models spatial topology, gene expression, and tissue morphology through cross-attention-based fusion. MultiST employs graph-based gene encoders with adversarial alignment to learn robust spatial representations, while integrating color-normalized histological features to capture molecular-morphological dependencies and refine domain boundaries. We evaluated the proposed method on 13 diverse ST datasets spanning two organs, including human brain cortex and breast cancer tissue. MultiST yields spatial domains with clearer and more coherent boundaries than existing methods, leading to more stable pseudotime trajectories and more biologically interpretable cell-cell interaction patterns. The MultiST framework and source code are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 220,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13359",
        "abs_url": "https://arxiv.org/abs/2601.13359",
        "pdf_url": "https://arxiv.org/pdf/2601.13359",
        "title": "Sockpuppetting: Jailbreaking LLMs Without Optimization Through Output Prefix Injection",
        "authors": [
            "Asen Dotsinski",
            "Panagiotis Eustratiadis"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
        "abstract": "As open-weight large language models (LLMs) increase in capabilities, safeguarding them against malicious prompts and understanding possible attack vectors becomes ever more important. While automated jailbreaking methods like GCG [Zou et al., 2023] remain effective, they often require substantial computational resources and specific expertise. We introduce \"sockpuppetting'', a simple method for jailbreaking open-weight LLMs by inserting an acceptance sequence (e.g., \"Sure, here is how to...'') at the start of a model's output and allowing it to complete the response. Requiring only a single line of code and no optimization, sockpuppetting achieves up to 80% higher attack success rate (ASR) than GCG on Qwen3-8B in per-prompt comparisons. We also explore a hybrid approach that optimizes the adversarial suffix within the assistant message block rather than the user prompt, increasing ASR by 64% over GCG on Llama-3.1-8B in a prompt-agnostic setting. The results establish sockpuppetting as an effective low-cost attack accessible to unsophisticated adversaries, highlighting the need for defences against output-prefix injection in open-weight models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 221,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13362",
        "abs_url": "https://arxiv.org/abs/2601.13362",
        "pdf_url": "https://arxiv.org/pdf/2601.13362",
        "title": "Improving Geopolitical Forecasts with Bayesian Networks",
        "authors": [
            "Matthew Martin"
        ],
        "comments": "34 pages, 3 figures",
        "subjects": "Applications (stat.AP); Machine Learning (cs.LG)",
        "abstract": "This study explores how Bayesian networks (BNs) can improve forecast accuracy compared to logistic regression and recalibration and aggregation methods, using data from the Good Judgment Project. Regularized logistic regression models and a baseline recalibrated aggregate were compared to two types of BNs: structure-learned BNs with arcs between predictors, and naive BNs. Four predictor variables were examined: absolute difference from the aggregate, forecast value, days prior to question close, and mean standardized Brier score. Results indicated the recalibrated aggregate achieved the highest accuracy (AUC = 0.985), followed by both types of BNs, then the logistic regression models. Performance of the BNs was likely harmed by reduced information from the discretization process and violation of the assumption of linearity likely harmed the logistic regression models. Future research should explore hybrid approaches combining BNs with logistic regression, examine additional predictor variables, and account for hierarchical data dependencies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 222,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13368",
        "abs_url": "https://arxiv.org/abs/2601.13368",
        "pdf_url": "https://arxiv.org/pdf/2601.13368",
        "title": "Recurrent Confidence Chain: Temporal-Aware Uncertainty Quantification in Large Language Models",
        "authors": [
            "Zhenjiang Mao",
            "Anirudhh Venkat"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "As reasoning modules, such as the chain-of-thought mechanism, are applied to large language models, they achieve strong performance on various tasks such as answering common-sense questions and solving math problems. The main challenge now is to assess the uncertainty of answers, which can help prevent misleading or serious hallucinations for users. Although current methods analyze long reasoning sequences by filtering unrelated tokens and examining potential connections between nearby tokens or sentences, the temporal spread of confidence is often overlooked. This oversight can lead to inflated overall confidence, even when earlier steps exhibit very low confidence. To address this issue, we propose a novel method that incorporates inter-step attention to analyze semantic correlations across steps. For handling long-horizon responses, we introduce a hidden confidence mechanism to retain historical confidence information, which is then combined with stepwise confidence to produce a more accurate overall estimate. We evaluate our method on the GAOKAO math benchmark and the CLadder causal reasoning dataset using mainstream open-source large language models. Our approach is shown to outperform state-of-the-art methods by achieving a superior balance between predictive quality and calibration, demonstrated by strong performance on both Negative Log-Likelihood and Expected Calibration Error.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 223,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13387",
        "abs_url": "https://arxiv.org/abs/2601.13387",
        "pdf_url": "https://arxiv.org/pdf/2601.13387",
        "title": "Confidence over Time: Confidence Calibration with Temporal Logic for Large Language Model Reasoning",
        "authors": [
            "Zhenjiang Mao",
            "Anirudhh Venkat",
            "Artem Bisliouk",
            "Akshat Kothiyal",
            "Sindhura Kumbakonam Subramanian",
            "Saithej Singhu",
            "Ivan Ruchkin"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Large Language Models (LLMs) increasingly rely on long-form, multi-step reasoning to solve complex tasks such as mathematical problem solving and scientific question answering. Despite strong performance, existing confidence estimation methods typically reduce an entire reasoning process to a single scalar score, ignoring how confidence evolves throughout the generation. As a result, these methods are often sensitive to superficial factors such as response length or verbosity, and struggle to distinguish correct reasoning from confidently stated errors. We propose to characterize the stepwise confidence signal using Signal Temporal Logic (STL). Using a discriminative STL mining procedure, we discover temporal formulas that distinguish confidence signals of correct and incorrect responses. Our analysis found that the STL patterns generalize across tasks, and numeric parameters exhibit sensitivity to individual questions. Based on these insights, we develop a confidence estimation approach that informs STL blocks with parameter hypernetworks. Experiments on multiple reasoning tasks show our confidence scores are more calibrated than the baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 224,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13410",
        "abs_url": "https://arxiv.org/abs/2601.13410",
        "pdf_url": "https://arxiv.org/pdf/2601.13410",
        "title": "Classifiers in High Dimensional Hilbert Metrics",
        "authors": [
            "Aditya Acharya",
            "Auguste H. Gezalyan",
            "David M. Mount"
        ],
        "comments": "",
        "subjects": "Computational Geometry (cs.CG); Machine Learning (cs.LG)",
        "abstract": "Classifying points in high dimensional spaces is a fundamental geometric problem in machine learning. In this paper, we address classifying points in the $d$-dimensional Hilbert polygonal metric. The Hilbert metric is a generalization of the Cayley-Klein hyperbolic distance to arbitrary convex bodies and has a diverse range of applications in machine learning and convex geometry. We first present an efficient LP-based algorithm in the metric for the large-margin SVM problem. Our algorithm runs in time polynomial to the number of points, bounding facets, and dimension. This is a significant improvement on previous works, which either provide no theoretical guarantees on running time, or suffer from exponential runtime. We also consider the closely related Funk metric. We also present efficient algorithms for the soft-margin SVM problem and for nearest neighbor-based classification in the Hilbert metric.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 225,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13433",
        "abs_url": "https://arxiv.org/abs/2601.13433",
        "pdf_url": "https://arxiv.org/pdf/2601.13433",
        "title": "Trust Me, I'm an Expert: Decoding and Steering Authority Bias in Large Language Models",
        "authors": [
            "Priyanka Mary Mammen",
            "Emil Joswin",
            "Shankar Venkitachalam"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Prior research demonstrates that performance of language models on reasoning tasks can be influenced by suggestions, hints and endorsements. However, the influence of endorsement source credibility remains underexplored. We investigate whether language models exhibit systematic bias based on the perceived expertise of the provider of the endorsement. Across 4 datasets spanning mathematical, legal, and medical reasoning, we evaluate 11 models using personas representing four expertise levels per domain. Our results reveal that models are increasingly susceptible to incorrect/misleading endorsements as source expertise increases, with higher-authority sources inducing not only accuracy degradation but also increased confidence in wrong answers. We also show that this authority bias is mechanistically encoded within the model and a model can be steered away from the bias, thereby improving its performance even when an expert gives a misleading endorsement.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 226,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13436",
        "abs_url": "https://arxiv.org/abs/2601.13436",
        "pdf_url": "https://arxiv.org/pdf/2601.13436",
        "title": "Distribution-Free Confidence Ellipsoids for Ridge Regression with PAC Bounds",
        "authors": [
            "Szabolcs Szentpéteri",
            "Balázs Csanád Csáji"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Signal Processing (eess.SP); Systems and Control (eess.SY); Statistics Theory (math.ST)",
        "abstract": "Linearly parametrized models are widely used in control and signal processing, with the least-squares (LS) estimate being the archetypical solution. When the input is insufficiently exciting, the LS problem may be unsolvable or numerically unstable. This issue can be resolved through regularization, typically with ridge regression. Although regularized estimators reduce the variance error, it remains important to quantify their estimation uncertainty. A possible approach for linear regression is to construct confidence ellipsoids with the Sign-Perturbed Sums (SPS) ellipsoidal outer approximation (EOA) algorithm. The SPS EOA builds non-asymptotic confidence ellipsoids under the assumption that the noises are independent and symmetric about zero. This paper introduces an extension of the SPS EOA algorithm to ridge regression, and derives probably approximately correct (PAC) upper bounds for the resulting region sizes. Compared with previous analyses, our result explicitly show how the regularization parameter affects the region sizes, and provide tighter bounds under weaker excitation assumptions. Finally, the practical effect of regularization is also demonstrated via simulation experiments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 227,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13458",
        "abs_url": "https://arxiv.org/abs/2601.13458",
        "pdf_url": "https://arxiv.org/pdf/2601.13458",
        "title": "Labels or Preferences? Budget-Constrained Learning with Human Judgments over AI-Generated Outputs",
        "authors": [
            "Zihan Dong",
            "Ruijia Wu",
            "Linjun Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Statistics Theory (math.ST)",
        "abstract": "The increasing reliance on human preference feedback to judge AI-generated pseudo labels has created a pressing need for principled, budget-conscious data acquisition strategies. We address the crucial question of how to optimally allocate a fixed annotation budget between ground-truth labels and pairwise preferences in AI. Our solution, grounded in semi-parametric inference, casts the budget allocation problem as a monotone missing data framework. Building on this formulation, we introduce Preference-Calibrated Active Learning (PCAL), a novel method that learns the optimal data acquisition strategy and develops a statistically efficient estimator for functionals of the data distribution. Theoretically, we prove the asymptotic optimality of our PCAL estimator and establish a key robustness guarantee that ensures robust performance even with poorly estimated nuisance models. Our flexible framework applies to a general class of problems, by directly optimizing the estimator's variance instead of requiring a closed-form solution. This work provides a principled and statistically efficient approach for budget-constrained learning in modern AI. Simulations and real-data analysis demonstrate the practical benefits and superior performance of our proposed method.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 228,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13489",
        "abs_url": "https://arxiv.org/abs/2601.13489",
        "pdf_url": "https://arxiv.org/pdf/2601.13489",
        "title": "Bridging the Gap Between Estimated and True Regret Towards Reliable Regret Estimation in Deep Learning based Mechanism Design",
        "authors": [
            "Shuyuan You",
            "Zhiqiang Zhuang",
            "Kewen Wang",
            "Zhe Wang"
        ],
        "comments": "",
        "subjects": "Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG); General Economics (econ.GN)",
        "abstract": "Recent advances, such as RegretNet, ALGnet, RegretFormer and CITransNet, use deep learning to approximate optimal multi item auctions by relaxing incentive compatibility (IC) and measuring its violation via ex post regret. However, the true accuracy of these regret estimates remains unclear. Computing exact regret is computationally intractable, and current models rely on gradient based optimizers whose outcomes depend heavily on hyperparameter choices. Through extensive experiments, we reveal that existing methods systematically underestimate actual regret (In some models, the true regret is several hundred times larger than the reported regret), leading to overstated claims of IC and revenue. To address this issue, we derive a lower bound on regret and introduce an efficient item wise regret approximation. Building on this, we propose a guided refinement procedure that substantially improves regret estimation accuracy while reducing computational cost. Our method provides a more reliable foundation for evaluating incentive compatibility in deep learning based auction mechanisms and highlights the need to reassess prior performance claims in this area.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 229,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13519",
        "abs_url": "https://arxiv.org/abs/2601.13519",
        "pdf_url": "https://arxiv.org/pdf/2601.13519",
        "title": "Small Gradient Norm Regret for Online Convex Optimization",
        "authors": [
            "Wenzhi Gao",
            "Chang He",
            "Madeleine Udell"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Optimization and Control (math.OC)",
        "abstract": "This paper introduces a new problem-dependent regret measure for online convex optimization with smooth losses. The notion, which we call the $G^\\star$ regret, depends on the cumulative squared gradient norm evaluated at the decision in hindsight $\\sum_{t=1}^T \\|\\nabla \\ell(x^\\star)\\|^2$. We show that the $G^\\star$ regret strictly refines the existing $L^\\star$ (small loss) regret, and that it can be arbitrarily sharper when the losses have vanishing curvature around the hindsight decision. We establish upper and lower bounds on the $G^\\star$ regret and extend our results to dynamic regret and bandit settings. As a byproduct, we refine the existing convergence analysis of stochastic optimization algorithms in the interpolation regime. Some experiments validate our theoretical findings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 230,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13542",
        "abs_url": "https://arxiv.org/abs/2601.13542",
        "pdf_url": "https://arxiv.org/pdf/2601.13542",
        "title": "Refined Gradient-Based Temperature Optimization for the Replica-Exchange Monte-Carlo Method",
        "authors": [
            "Tatsuya Miyata",
            "Shunta Arai",
            "Satoshi Takabe"
        ],
        "comments": "15 pages",
        "subjects": "Computational Physics (physics.comp-ph); Machine Learning (cs.LG)",
        "abstract": "The replica-exchange Monte-Carlo (RXMC) method is a powerful Markov-chain Monte-Carlo algorithm for sampling from multi-modal distributions, which are challenging for conventional methods. The sampling efficiency of the RXMC method depends highly on the selection of the temperatures, and finding optimal temperatures remains a challenge. In this study, we propose a refined online temperature selection method by extending the gradient-based optimization framework proposed previously. Building upon the existing temperature update approach, we introduce a reparameterization technique to strictly enforce physical constraints, such as the monotonic ordering of inverse temperatures, which were not explicitly addressed in the original formulation. The proposed method defines the variance of acceptance rates between adjacent replicas as a loss function, estimates its gradient using differential information from the sampling process, and optimizes the temperatures via gradient descent. We demonstrate the effectiveness of our method through experiments on benchmark spin systems, including the two-dimensional ferromagnetic Ising model, the two-dimensional ferromagnetic XY model, and the three-dimensional Edwards-Anderson model. Our results show that the method successfully achieves uniform acceptance rates and reduces round-trip times across the temperature space. Furthermore, our proposed method offers a significant advantage over recently proposed policy gradient method that require careful hyperparameter tuning, while simultaneously preventing the constraint violations that destabilize optimization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 231,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13602",
        "abs_url": "https://arxiv.org/abs/2601.13602",
        "pdf_url": "https://arxiv.org/pdf/2601.13602",
        "title": "An Elementary Approach to Scheduling in Generative Diffusion Models",
        "authors": [
            "Qiang Sun",
            "H. Vincent Poor",
            "Wenyi Zhang"
        ],
        "comments": "",
        "subjects": "Information Theory (cs.IT); Machine Learning (cs.LG)",
        "abstract": "An elementary approach to characterizing the impact of noise scheduling and time discretization in generative diffusion models is developed. Considering a simplified model where the source distribution is multivariate Gaussian with a given covariance matrix, the explicit closed-form evolution trajectory of the distributions across reverse sampling steps is derived, and consequently, the Kullback-Leibler (KL) divergence between the source distribution and the reverse sampling output is obtained. The effect of the number of time discretization steps on the convergence of this KL divergence is studied via the Euler-Maclaurin expansion. An optimization problem is formulated, and its solution noise schedule is obtained via calculus of variations, shown to follow a tangent law whose coefficient is determined by the eigenvalues of the source covariance matrix. For an alternative scenario, more realistic in practice, where pretrained models have been obtained for some given noise schedules, the KL divergence also provides a measure to compare different time discretization strategies in reverse sampling. Experiments across different datasets and pretrained models demonstrate that the time discretization strategy selected by our approach consistently outperforms baseline and search-based strategies, particularly when the budget on the number of function evaluations is very tight.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 232,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13604",
        "abs_url": "https://arxiv.org/abs/2601.13604",
        "pdf_url": "https://arxiv.org/pdf/2601.13604",
        "title": "Optimizing Parallel Schemes with Lyapunov Exponents and kNN-LLE Estimation",
        "authors": [
            "Mudassir Shams",
            "Andrei Velichko",
            "Bruno Carpentieri"
        ],
        "comments": "25 pages, 9 figures, 10 tables",
        "subjects": "Numerical Analysis (math.NA); Machine Learning (cs.LG); Dynamical Systems (math.DS)",
        "abstract": "Inverse parallel schemes remain indispensable tools for computing the roots of nonlinear systems, yet their dynamical behavior can be unexpectedly rich, ranging from strong contraction to oscillatory or chaotic transients depending on the choice of algorithmic parameters and initial states. A unified analytical-data-driven methodology for identifying, measuring, and reducing such instabilities in a family of uni-parametric inverse parallel solvers is presented in this study. On the theoretical side, we derive stability and bifurcation characterizations of the underlying iterative maps, identifying parameter regions associated with periodic or chaotic behavior. On the computational side, we introduce a micro-series pipeline based on kNN-driven estimation of the local largest Lyapunov exponent (LLE), applied to scalar time series derived from solver trajectories. The resulting sliding-window Lyapunov profiles provide fine-grained, real-time diagnostics of contractive or unstable phases and reveal transient behaviors not captured by coarse linearized analysis. Leveraging this correspondence, we introduce a Lyapunov-informed parameter selection strategy that identifies solver settings associated with stable behavior, particularly when the estimated LLE indicates persistent instability. Comprehensive experiments on ensembles of perturbed initial guesses demonstrate close agreement between the theoretical stability diagrams and empirical Lyapunov profiles, and show that the proposed adaptive mechanism significantly improves robustness. The study establishes micro-series Lyapunov analysis as a practical, interpretable tool for constructing self-stabilizing root-finding schemes and opens avenues for extending such diagnostics to higher-dimensional or noise-contaminated problems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 233,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13642",
        "abs_url": "https://arxiv.org/abs/2601.13642",
        "pdf_url": "https://arxiv.org/pdf/2601.13642",
        "title": "Sample Complexity of Average-Reward Q-Learning: From Single-agent to Federated Reinforcement Learning",
        "authors": [
            "Yuchen Jiao",
            "Jiin Woo",
            "Gen Li",
            "Gauri Joshi",
            "Yuejie Chi"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Average-reward reinforcement learning offers a principled framework for long-term decision-making by maximizing the mean reward per time step. Although Q-learning is a widely used model-free algorithm with established sample complexity in discounted and finite-horizon Markov decision processes (MDPs), its theoretical guarantees for average-reward settings remain limited. This work studies a simple but effective Q-learning algorithm for average-reward MDPs with finite state and action spaces under the weakly communicating assumption, covering both single-agent and federated scenarios. For the single-agent case, we show that Q-learning with carefully chosen parameters achieves sample complexity $\\widetilde{O}\\left(\\frac{|\\mathcal{S}||\\mathcal{A}|\\|h^{\\star}\\|_{\\mathsf{sp}}^3}{\\varepsilon^3}\\right)$, where $\\|h^{\\star}\\|_{\\mathsf{sp}}$ is the span norm of the bias function, improving previous results by at least a factor of $\\frac{\\|h^{\\star}\\|_{\\mathsf{sp}}^2}{\\varepsilon^2}$. In the federated setting with $M$ agents, we prove that collaboration reduces the per-agent sample complexity to $\\widetilde{O}\\left(\\frac{|\\mathcal{S}||\\mathcal{A}|\\|h^{\\star}\\|_{\\mathsf{sp}}^3}{M\\varepsilon^3}\\right)$, with only $\\widetilde{O}\\left(\\frac{\\|h^{\\star}\\|_{\\mathsf{sp}}}{\\varepsilon}\\right)$ communication rounds required. These results establish the first federated Q-learning algorithm for average-reward MDPs, with provable efficiency in both sample and communication complexity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 234,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13644",
        "abs_url": "https://arxiv.org/abs/2601.13644",
        "pdf_url": "https://arxiv.org/pdf/2601.13644",
        "title": "Towards Token-Level Text Anomaly Detection",
        "authors": [
            "Yang Cao",
            "Bicheng Yu",
            "Sikun Yang",
            "Ming Liu",
            "Yujiu Yang"
        ],
        "comments": "WWW 2026",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Despite significant progress in text anomaly detection for web applications such as spam filtering and fake news detection, existing methods are fundamentally limited to document-level analysis, unable to identify which specific parts of a text are anomalous. We introduce token-level anomaly detection, a novel paradigm that enables fine-grained localization of anomalies within text. We formally define text anomalies at both document and token-levels, and propose a unified detection framework that operates across multiple levels. To facilitate research in this direction, we collect and annotate three benchmark datasets spanning spam, reviews and grammar errors with token-level labels. Experimental results demonstrate that our framework get better performance than other 6 baselines, opening new possibilities for precise anomaly localization in text. All the codes and data are publicly available on this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 235,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13662",
        "abs_url": "https://arxiv.org/abs/2601.13662",
        "pdf_url": "https://arxiv.org/pdf/2601.13662",
        "title": "Reinforcement Learning for Opportunistic Routing in Software-Defined LEO-Terrestrial Systems",
        "authors": [
            "Sivaram Krishnan",
            "Zhouyou Gu",
            "Jihong Park",
            "Sung-Min Oh",
            "Jinho Choi"
        ],
        "comments": "",
        "subjects": "Networking and Internet Architecture (cs.NI); Machine Learning (cs.LG)",
        "abstract": "The proliferation of large-scale low Earth orbit (LEO) satellite constellations is driving the need for intelligent routing strategies that can effectively deliver data to terrestrial networks under rapidly time-varying topologies and intermittent gateway visibility. Leveraging the global control capabilities of a geostationary (GEO)-resident software-defined networking (SDN) controller, we introduce opportunistic routing, which aims to minimize delivery delay by forwarding packets to any currently available ground gateways rather than fixed destinations. This makes it a promising approach for achieving low-latency and robust data delivery in highly dynamic LEO networks. Specifically, we formulate a constrained stochastic optimization problem and employ a residual reinforcement learning framework to optimize opportunistic routing for reducing transmission delay. Simulation results over multiple days of orbital data demonstrate that our method achieves significant improvements in queue length reduction compared to classical backpressure and other well-known queueing algorithms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 236,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13704",
        "abs_url": "https://arxiv.org/abs/2601.13704",
        "pdf_url": "https://arxiv.org/pdf/2601.13704",
        "title": "Performance and Complexity Trade-off Optimization of Speech Models During Training",
        "authors": [
            "Esteban Gómez",
            "Tom Bäckström"
        ],
        "comments": "",
        "subjects": "Sound (cs.SD); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)",
        "abstract": "In speech machine learning, neural network models are typically designed by choosing an architecture with fixed layer sizes and structure. These models are then trained to maximize performance on metrics aligned with the task's objective. While the overall architecture is usually guided by prior knowledge of the task, the sizes of individual layers are often chosen heuristically. However, this approach does not guarantee an optimal trade-off between performance and computational complexity; consequently, post hoc methods such as weight quantization or model pruning are typically employed to reduce computational cost. This occurs because stochastic gradient descent (SGD) methods can only optimize differentiable functions, while factors influencing computational complexity, such as layer sizes and floating-point operations per second (FLOP/s), are non-differentiable and require modifying the model structure during training. We propose a reparameterization technique based on feature noise injection that enables joint optimization of performance and computational complexity during training using SGD-based methods. Unlike traditional pruning methods, our approach allows the model size to be dynamically optimized for a target performance-complexity trade-off, without relying on heuristic criteria to select which weights or structures to remove. We demonstrate the effectiveness of our method through three case studies, including a synthetic example and two practical real-world applications: voice activity detection and audio anti-spoofing. The code related to our work is publicly available to encourage further research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 237,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13708",
        "abs_url": "https://arxiv.org/abs/2601.13708",
        "pdf_url": "https://arxiv.org/pdf/2601.13708",
        "title": "Generative Adversarial Networks for Resource State Generation",
        "authors": [
            "Shahbaz Shaik",
            "Sourav Chatterjee",
            "Sayantan Pramanik",
            "Indranil Chakrabarty"
        ],
        "comments": "",
        "subjects": "Quantum Physics (quant-ph); Machine Learning (cs.LG)",
        "abstract": "We introduce a physics-informed Generative Adversarial Network framework that recasts quantum resource-state generation as an inverse-design task. By embedding task-specific utility functions into training, the model learns to generate valid two-qubit states optimized for teleportation and entanglement broadcasting. Comparing decomposition-based and direct-generation architectures reveals that structural enforcement of Hermiticity, trace-one, and positivity yields higher fidelity and training stability than loss-only approaches. The framework reproduces theoretical resource boundaries for Werner-like and Bell-diagonal states with fidelities exceeding ~98%, establishing adversarial learning as a lightweight yet effective method for constraint-driven quantum-state discovery. This approach provides a scalable foundation for automated design of tailored quantum resources for information-processing applications, exemplified with teleportation and broadcasting of entanglement, and it opens up the possibility of using such states in efficient quantum network design.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 238,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13713",
        "abs_url": "https://arxiv.org/abs/2601.13713",
        "pdf_url": "https://arxiv.org/pdf/2601.13713",
        "title": "SWE-Tester: Training Open-Source LLMs for Issue Reproduction in Real-World Repositories",
        "authors": [
            "Aditya Bharat Soni",
            "Rajat Ghosh",
            "Vaishnavi Bhargava",
            "Valerie Chen",
            "Debojyoti Dutta"
        ],
        "comments": "",
        "subjects": "Software Engineering (cs.SE); Machine Learning (cs.LG)",
        "abstract": "Software testing is crucial for ensuring the correctness and reliability of software systems. Automated generation of issue reproduction tests from natural language issue descriptions enhances developer productivity by simplifying root cause analysis, promotes test-driven development -- \"test first, write code later\", and can be used for improving the effectiveness of automated issue resolution systems like coding agents. Existing methods proposed for this task predominantly rely on closed-source LLMs, with limited exploration of open models. To address this, we propose SWE-Tester -- a novel pipeline for training open-source LLMs to generate issue reproduction tests. First, we curate a high-quality training dataset of 41K instances from 2.6K open-source GitHub repositories and use it to train LLMs of varying sizes and families. The fine-tuned models achieve absolute improvements of up to 10\\% in success rate and 21\\% in change coverage on SWT-Bench Verified. Further analysis shows consistent improvements with increased inference-time compute, more data, and larger models. These results highlight the effectiveness of our framework for advancing open-source LLMs in this domain.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 239,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13731",
        "abs_url": "https://arxiv.org/abs/2601.13731",
        "pdf_url": "https://arxiv.org/pdf/2601.13731",
        "title": "Breaking the Data Barrier in Learning Symbolic Computation: A Case Study on Variable Ordering Suggestion for Cylindrical Algebraic Decomposition",
        "authors": [
            "Rui-Juan Jing",
            "Yuegang Zhao",
            "Changbo Chen"
        ],
        "comments": "",
        "subjects": "Symbolic Computation (cs.SC); Machine Learning (cs.LG)",
        "abstract": "Symbolic computation, powered by modern computer algebra systems, has important applications in mathematical reasoning through exact deep computations. The efficiency of symbolic computation is largely constrained by such deep computations in high dimension. This creates a fundamental barrier on labelled data acquisition if leveraging supervised deep learning to accelerate symbolic computation. Cylindrical algebraic decomposition (CAD) is a pillar symbolic computation method for reasoning with first-order logic formulas over reals with many applications in formal verification and automatic theorem proving. Variable orderings have a huge impact on its efficiency. Impeded by the difficulty to acquire abundant labelled data, existing learning-based approaches are only competitive with the best expert-based heuristics. In this work, we address this problem by designing a series of intimately connected tasks for which a large amount of annotated data can be easily obtained. We pre-train a Transformer model with these data and then fine-tune it on the datasets for CAD ordering. Experiments on publicly available CAD ordering datasets show that on average the orderings predicted by the new model are significantly better than those suggested by the best heuristic methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 240,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13745",
        "abs_url": "https://arxiv.org/abs/2601.13745",
        "pdf_url": "https://arxiv.org/pdf/2601.13745",
        "title": "Variational Dual-path Attention Network for CSI-Based Gesture Recognition",
        "authors": [
            "N.Zhang"
        ],
        "comments": "8 pages, 7 figures, 2 tables",
        "subjects": "Networking and Internet Architecture (cs.NI); Machine Learning (cs.LG)",
        "abstract": "Wi-Fi gesture recognition based on Channel State Information (CSI) is challenged by high-dimensional noise and resource constraints on edge devices. Prevailing end-to-end models tightly couple feature extraction with classification, overlooking the inherent time-frequency sparsity of CSI and leading to redundancy and poor generalization. To address this, this paper proposes a lightweight feature preprocessing module--the Variational Dual-path Attention Network (VDAN). It performs structured feature refinement through frequency-domain filtering and temporal detection. Variational inference is introduced to model the uncertainty in attention weights, thereby enhancing robustness to noise. The design principles of the module are explained from the perspectives of the information bottleneck and regularization. Experiments on a public dataset demonstrate that the learned attention weights align with the physical sparse characteristics of CSI, verifying its interpretability. This work provides an efficient and explainable front-end processing solution for resource-constrained wireless sensing systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 241,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13751",
        "abs_url": "https://arxiv.org/abs/2601.13751",
        "pdf_url": "https://arxiv.org/pdf/2601.13751",
        "title": "HiT: History-Injection Transformers for Onboard Continuous Flood Change Detection",
        "authors": [
            "Daniel Kyselica",
            "Jonáš Herec",
            "Oliver Kutis",
            "Rado Pitoňák"
        ],
        "comments": "19 pages, 9 figures, submitted to conference",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Natural disaster monitoring through continuous satellite observation requires processing multi-temporal data under strict operational constraints. This paper addresses flood detection, a critical application for hazard management, by developing an onboard change detection system that operates within the memory and computational limits of small satellites. We propose History Injection mechanism for Transformer models (HiT), that maintains historical context from previous observations while reducing data storage by over 99\\% of original image size. Moreover, testing on the STTORM-CD flood dataset confirms that the HiT mechanism within the Prithvi-tiny foundation model maintains detection accuracy compared to the bitemporal baseline. The proposed HiT-Prithvi model achieved 43 FPS on Jetson Orin Nano, a representative onboard hardware used in nanosats. This work establishes a practical framework for satellite-based continuous monitoring of natural disasters, supporting real-time hazard assessment without dependency on ground-based processing infrastructure. Architecture as well as model checkpoints is available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 242,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13798",
        "abs_url": "https://arxiv.org/abs/2601.13798",
        "pdf_url": "https://arxiv.org/pdf/2601.13798",
        "title": "Insight: Interpretable Semantic Hierarchies in Vision-Language Encoders",
        "authors": [
            "Kai Wittenmayer",
            "Sukrut Rao",
            "Amin Parchami-Araghi",
            "Bernt Schiele",
            "Jonas Fischer"
        ],
        "comments": "32 pages, 24 figures, 3 tables",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Language-aligned vision foundation models perform strongly across diverse downstream tasks. Yet, their learned representations remain opaque, making interpreting their decision-making hard. Recent works decompose these representations into human-interpretable concepts, but provide poor spatial grounding and are limited to image classification tasks. In this work, we propose Insight, a language-aligned concept foundation model that provides fine-grained concepts, which are human-interpretable and spatially grounded in the input image. We leverage a hierarchical sparse autoencoder and a foundation model with strong semantic representations to automatically extract concepts at various granularities. Examining local co-occurrence dependencies of concepts allows us to define concept relationships. Through these relations we further improve concept naming and obtain richer explanations. On benchmark data, we show that Insight provides performance on classification and segmentation that is competitive with opaque foundation models while providing fine-grained, high quality concept-based explanations. Code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 243,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13806",
        "abs_url": "https://arxiv.org/abs/2601.13806",
        "pdf_url": "https://arxiv.org/pdf/2601.13806",
        "title": "Knowledge Graph-Assisted LLM Post-Training for Enhanced Legal Reasoning",
        "authors": [
            "Dezhao Song",
            "Guglielmo Bonifazi",
            "Frank Schilder",
            "Jonathan Richard Schwarz"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "LLM post-training has primarily relied on large text corpora and human feedback, without capturing the structure of domain knowledge. This has caused models to struggle dealing with complex reasoning tasks, especially for high-stakes professional domains. In Law, reasoning requires deep understanding of the relations between various legal concepts, a key component missing in current LLM post-training. In this paper, we propose a knowledge graph (KG)-assisted approach for enhancing LLMs' reasoning capability in Legal that is generalizable to other high-stakes domains. We model key legal concepts by following the \\textbf{IRAC} (Issue, Rule, Analysis and Conclusion) framework, and construct a KG with 12K legal cases. We then produce training data using our IRAC KG, and conduct both Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) with three state-of-the-art (SOTA) LLMs (30B, 49B and 70B), varying architecture and base model family. Our post-trained models obtained better average performance on 4/5 diverse legal benchmarks (14 tasks) than baselines. In particular, our 70B DPO model achieved the best score on 4/6 reasoning tasks, among baselines and a 141B SOTA legal LLM, demonstrating the effectiveness of our KG for enhancing LLMs' legal reasoning capability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 244,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13817",
        "abs_url": "https://arxiv.org/abs/2601.13817",
        "pdf_url": "https://arxiv.org/pdf/2601.13817",
        "title": "Device Association and Resource Allocation for Hierarchical Split Federated Learning in Space-Air-Ground Integrated Network",
        "authors": [
            "Haitao Zhao",
            "Xiaoyu Tang",
            "Bo Xu",
            "Jinlong Sun",
            "Linghao Zhang"
        ],
        "comments": "",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)",
        "abstract": "6G facilitates deployment of Federated Learning (FL) in the Space-Air-Ground Integrated Network (SAGIN), yet FL confronts challenges such as resource constrained and unbalanced data distribution. To address these issues, this paper proposes a Hierarchical Split Federated Learning (HSFL) framework and derives its upper bound of loss function. To minimize the weighted sum of training loss and latency, we formulate a joint optimization problem that integrates device association, model split layer selection, and resource allocation. We decompose the original problem into several subproblems, where an iterative optimization algorithm for device association and resource allocation based on brute-force split point search is proposed. Simulation results demonstrate that the proposed algorithm can effectively balance training efficiency and model accuracy for FL in SAGIN.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 245,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13849",
        "abs_url": "https://arxiv.org/abs/2601.13849",
        "pdf_url": "https://arxiv.org/pdf/2601.13849",
        "title": "Co-Initialization of Control Filter and Secondary Path via Meta-Learning for Active Noise Control",
        "authors": [
            "Ziyi Yang",
            "Li Rao",
            "Zhengding Luo",
            "Dongyuan Shi",
            "Qirui Huang",
            "Woon-Seng Gan"
        ],
        "comments": "",
        "subjects": "Audio and Speech Processing (eess.AS); Machine Learning (cs.LG); Signal Processing (eess.SP)",
        "abstract": "Active noise control (ANC) must adapt quickly when the acoustic environment changes, yet early performance is largely dictated by initialization. We address this with a Model-Agnostic Meta-Learning (MAML) co-initialization that jointly sets the control filter and the secondary-path model for FxLMS-based ANC while keeping the runtime algorithm unchanged. The initializer is pre-trained on a small set of measured paths using short two-phase inner loops that mimic identification followed by residual-noise reduction, and is applied by simply setting the learned initial coefficients. In an online secondary path modeling FxLMS testbed, it yields lower early-stage error, shorter time-to-target, reduced auxiliary-noise energy, and faster recovery after path changes than a baseline without re-initialization. The method provides a simple fast start for feedforward ANC under environment changes, requiring a small set of paths to pre-train.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 246,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13874",
        "abs_url": "https://arxiv.org/abs/2601.13874",
        "pdf_url": "https://arxiv.org/pdf/2601.13874",
        "title": "Unified Unbiased Variance Estimation for MMD: Robust Finite-Sample Performance with Imbalanced Data and Exact Acceleration under Null and Alternative Hypotheses",
        "authors": [
            "Shijie Zhong",
            "Jiangfeng Fu",
            "Yikun Yang"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "The maximum mean discrepancy (MMD) is a kernel-based nonparametric statistic for two-sample testing, whose inferential accuracy depends critically on variance characterization. Existing work provides various finite-sample estimators of the MMD variance, often differing under the null and alternative hypotheses and across balanced or imbalanced sampling schemes. In this paper, we study the variance of the MMD statistic through its U-statistic representation and Hoeffding decomposition, and establish a unified finite-sample characterization covering different hypotheses and sample configurations. Building on this analysis, we propose an exact acceleration method for the univariate case under the Laplacian kernel, which reduces the overall computational complexity from $\\mathcal O(n^2)$ to $\\mathcal O(n \\log n)$.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 247,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13926",
        "abs_url": "https://arxiv.org/abs/2601.13926",
        "pdf_url": "https://arxiv.org/pdf/2601.13926",
        "title": "SCG With Your Phone: Diagnosis of Rhythmic Spectrum Disorders in Field Conditions",
        "authors": [
            "Peter Golenderov",
            "Yaroslav Matushenko",
            "Anastasia Tushina",
            "Michal Barodkin"
        ],
        "comments": "",
        "subjects": "Quantitative Methods (q-bio.QM); Machine Learning (cs.LG); Signal Processing (eess.SP)",
        "abstract": "Aortic valve opening (AO) events are crucial for detecting frequency and rhythm disorders, especially in real-world settings where seismocardiography (SCG) signals collected via consumer smartphones are subject to noise, motion artifacts, and variability caused by device heterogeneity. In this work, we present a robust deep-learning framework for SCG segmentation and rhythm analysis using accelerometer recordings obtained with consumer smartphones. We develop an enhanced U-Net v3 architecture that integrates multi-scale convolutions, residual connections, and attention gates, enabling reliable segmentation of noisy SCG signals. A dedicated post-processing pipeline converts probability masks into precise AO timestamps, whereas a novel adaptive 3D-to-1D projection method ensures robustness to arbitrary smartphone orientation. Experimental results demonstrate that the proposed method achieves consistently high accuracy and robustness across various device types and unsupervised data-collection conditions. Our approach enables practical, low-cost, and automated cardiac-rhythm monitoring using everyday mobile devices, paving the way for scalable, field-deployable cardiovascular assessment and future multimodal diagnostic systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 248,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13931",
        "abs_url": "https://arxiv.org/abs/2601.13931",
        "pdf_url": "https://arxiv.org/pdf/2601.13931",
        "title": "Towards Effective Negation Modeling in Joint Audio-Text Models for Music",
        "authors": [
            "Yannis Vasilakis",
            "Rachel Bittner",
            "Johan Pauwels"
        ],
        "comments": "Accepted at IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2026",
        "subjects": "Sound (cs.SD); Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "Joint audio-text models are widely used for music retrieval, yet they struggle with semantic phenomena such as negation. Negation is fundamental for distinguishing the absence (or presence) of musical elements (e.g., \"with vocals\" vs. \"without vocals\"), but current systems fail to represent this reliably. In this work, we investigate and mitigate this limitation by training CLAP models from scratch on the Million Song Dataset with LP-MusicCaps-MSD captions. We introduce negation through text augmentation and a dissimilarity-based contrastive loss, designed to explicitly separate original and negated captions in the joint embedding space. To evaluate progress, we propose two protocols that frame negation modeling as retrieval and binary classification tasks. Experiments demonstrate that both methods, individually and combined, improve negation handling while largely preserving retrieval performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 249,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13945",
        "abs_url": "https://arxiv.org/abs/2601.13945",
        "pdf_url": "https://arxiv.org/pdf/2601.13945",
        "title": "Efficient Coordination with the System-Level Shared State: An Embodied-AI Native Modular Framework",
        "authors": [
            "Yixuan Deng",
            "Tongrun Wu",
            "Donghao Wu",
            "Zeyu Wei",
            "Jiayuan Wang",
            "Zhenglong Sun",
            "Yuqing Tang",
            "Xiaoqiang Ji"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
        "abstract": "As Embodied AI systems move from research prototypes to real world deployments, they tend to evolve rapidly while remaining reliable under workload changes and partial failures. In practice, many deployments are only partially decoupled: middleware moves messages, but shared context and feedback semantics are implicit, causing interface drift, cross-module interference, and brittle recovery at scale. We present ANCHOR, a modular framework that makes decoupling and robustness explicit system-level primitives. ANCHOR separates (i) Canonical Records, an evolvable contract for the standardized shared state, from (ii) a communication bus for many-to-many dissemination and feedback-oriented coordination, forming an inspectable end-to-end loop. We validate closed-loop feasibility on a de-identified workflow instantiation, characterize latency distributions under varying payload sizes and publish rates, and demonstrate automatic stream resumption after hard crashes and restarts even with shared-memory loss. Overall, ANCHOR turns ad-hoc integration glue into explicit contracts, enabling controlled degradation under load and self-healing recovery for scalable deployment of closed-loop AI systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 250,
        "date": "2026-01-21",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-21?abs=True",
        "arxiv_id": "2601.13969",
        "abs_url": "https://arxiv.org/abs/2601.13969",
        "pdf_url": "https://arxiv.org/pdf/2601.13969",
        "title": "Autonomous Knowledge Graph Exploration with Adaptive Breadth-Depth Retrieval",
        "authors": [
            "Joaquín Polonuer",
            "Lucas Vittor",
            "Iñaki Arango",
            "Ayush Noori",
            "David A. Clifton",
            "Luciano Del Corro",
            "Marinka Zitnik"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "Retrieving evidence for language model queries from knowledge graphs requires balancing broad search across the graph with multi-hop traversal to follow relational links. Similarity-based retrievers provide coverage but remain shallow, whereas traversal-based methods rely on selecting seed nodes to start exploration, which can fail when queries span multiple entities and relations. We introduce ARK: Adaptive Retriever of Knowledge, an agentic KG retriever that gives a language model control over this breadth-depth tradeoff using a two-operation toolset: global lexical search over node descriptors and one-hop neighborhood exploration that composes into multi-hop traversal. ARK alternates between breadth-oriented discovery and depth-oriented expansion without depending on a fragile seed selection, a pre-set hop depth, or requiring retrieval training. ARK adapts tool use to queries, using global search for language-heavy queries and neighborhood exploration for relation-heavy queries. On STaRK, ARK reaches 59.1% average Hit@1 and 67.4 average MRR, improving average Hit@1 by up to 31.4% and average MRR by up to 28.0% over retrieval-based and agentic training-free methods. Finally, we distill ARK's tool-use trajectories from a large teacher into an 8B model via label-free imitation, improving Hit@1 by +7.0, +26.6, and +13.5 absolute points over the base 8B model on AMAZON, MAG, and PRIME datasets, respectively, while retaining up to 98.5% of the teacher's Hit@1 rate.",
        "gemini2.5flash": "",
        "overall_idea": ""
    }
]