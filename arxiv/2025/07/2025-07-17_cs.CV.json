[
    {
        "order": 1,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11549",
        "abs_url": "https://arxiv.org/abs/2507.11549",
        "pdf_url": "https://arxiv.org/pdf/2507.11549",
        "title": "An Memory-Efficient Framework for Deformable Transformer with Neural Architecture Search",
        "authors": [
            "Wendong Mao",
            "Mingfan Zhao",
            "Jianfeng Guan",
            "Qiwei Dong",
            "Zhongfeng Wang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Deformable Attention Transformers (DAT) have shown remarkable performance in computer vision tasks by adaptively focusing on informative image regions. However, their data-dependent sampling mechanism introduces irregular memory access patterns, posing significant challenges for efficient hardware deployment. Existing acceleration methods either incur high hardware overhead or compromise model accuracy. To address these issues, this paper proposes a hardware-friendly optimization framework for DAT. First, a neural architecture search (NAS)-based method with a new slicing strategy is proposed to automatically divide the input feature into uniform patches during the inference process, avoiding memory conflicts without modifying model architecture. The method explores the optimal slice configuration by jointly optimizing hardware cost and inference accuracy. Secondly, an FPGA-based verification system is designed to test the performance of this framework on edge-side hardware. Algorithm experiments on the ImageNet-1K dataset demonstrate that our hardware-friendly framework can maintain have only 0.2% accuracy drop compared to the baseline DAT. Hardware experiments on Xilinx FPGA show the proposed method reduces DRAM access times to 18% compared with existing DAT acceleration methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 2,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11550",
        "abs_url": "https://arxiv.org/abs/2507.11550",
        "pdf_url": "https://arxiv.org/pdf/2507.11550",
        "title": "Deformable Dynamic Convolution for Accurate yet Efficient Spatio-Temporal Traffic Prediction",
        "authors": [
            "Hyeonseok Jin",
            "Geonmin Kim",
            "Kyungbaek Kim"
        ],
        "comments": "7 pages",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Spatio-temporal traffic prediction plays a key role in intelligent transportation systems by enabling accurate prediction in complex urban areas. Although not only accuracy but also efficiency for scalability is important, some previous methods struggle to capture heterogeneity such as varying traffic patterns across regions and time periods. Moreover, Graph Neural Networks (GNNs), which are the mainstream of traffic prediction, not only require predefined adjacency matrix, but also limit scalability to large-scale data containing many nodes due to their inherent complexity. To overcome these limitations, we propose Deformable Dynamic Convolution Network (DDCN) for accurate yet efficient traffic prediction. Traditional Convolutional Neural Networks (CNNs) are limited in modeling non-Euclidean spatial structures and spatio-temporal heterogeneity, DDCN overcomes these challenges by dynamically applying deformable filters based on offset. Specifically, DDCN decomposes transformer-style CNN to encoder-decoder structure, and applies proposed approaches to the spatial and spatio-temporal attention blocks of the encoder to emphasize important features. The decoder, composed of feed-forward module, complements the output of the encoder. This novel structure make DDCN can perform accurate yet efficient traffic prediction. In comprehensive experiments on four real-world datasets, DDCN achieves competitive performance, emphasizing the potential and effectiveness of CNN-based approaches for spatio-temporal traffic prediction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 3,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11554",
        "abs_url": "https://arxiv.org/abs/2507.11554",
        "pdf_url": "https://arxiv.org/pdf/2507.11554",
        "title": "Inversion-DPO: Precise and Efficient Post-Training for Diffusion Models",
        "authors": [
            "Zejian Li",
            "Yize Li",
            "Chenye Meng",
            "Zhongni Liu",
            "Yang Ling",
            "Shengyuan Zhang",
            "Guang Yang",
            "Changyuan Yang",
            "Zhiyuan Yang",
            "Lingyun Sun"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Recent advancements in diffusion models (DMs) have been propelled by alignment methods that post-train models to better conform to human preferences. However, these approaches typically require computation-intensive training of a base model and a reward model, which not only incurs substantial computational overhead but may also compromise model accuracy and training efficiency. To address these limitations, we propose Inversion-DPO, a novel alignment framework that circumvents reward modeling by reformulating Direct Preference Optimization (DPO) with DDIM inversion for DMs. Our method conducts intractable posterior sampling in Diffusion-DPO with the deterministic inversion from winning and losing samples to noise and thus derive a new post-training paradigm. This paradigm eliminates the need for auxiliary reward models or inaccurate appromixation, significantly enhancing both precision and efficiency of training. We apply Inversion-DPO to a basic task of text-to-image generation and a challenging task of compositional image generation. Extensive experiments show substantial performance improvements achieved by Inversion-DPO compared to existing post-training methods and highlight the ability of the trained generative models to generate high-fidelity compositionally coherent images. For the post-training of compostitional image geneation, we curate a paired dataset consisting of 11,140 images with complex structural annotations and comprehensive scores, designed to enhance the compositional capabilities of generative models. Inversion-DPO explores a new avenue for efficient, high-precision alignment in diffusion models, advancing their applicability to complex realistic generation tasks. Our code is available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 4,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11558",
        "abs_url": "https://arxiv.org/abs/2507.11558",
        "pdf_url": "https://arxiv.org/pdf/2507.11558",
        "title": "Reprogramming Vision Foundation Models for Spatio-Temporal Forecasting",
        "authors": [
            "Changlu Chen",
            "Yanbin Liu",
            "Chaoxi Niu",
            "Ling Chen",
            "Tianqing Zhu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Foundation models have achieved remarkable success in natural language processing and computer vision, demonstrating strong capabilities in modeling complex patterns. While recent efforts have explored adapting large language models (LLMs) for time-series forecasting, LLMs primarily capture one-dimensional sequential dependencies and struggle to model the richer spatio-temporal (ST) correlations essential for accurate ST forecasting. In this paper, we present \\textbf{ST-VFM}, a novel framework that systematically reprograms Vision Foundation Models (VFMs) for general-purpose spatio-temporal forecasting. While VFMs offer powerful spatial priors, two key challenges arise when applying them to ST tasks: (1) the lack of inherent temporal modeling capacity and (2) the modality gap between visual and ST data. To address these, ST-VFM adopts a \\emph{dual-branch architecture} that integrates raw ST inputs with auxiliary ST flow inputs, where the flow encodes lightweight temporal difference signals interpretable as dynamic spatial cues. To effectively process these dual-branch inputs, ST-VFM introduces two dedicated reprogramming stages. The \\emph{pre-VFM reprogramming} stage applies a Temporal-Aware Token Adapter to embed temporal context and align both branches into VFM-compatible feature spaces. The \\emph{post-VFM reprogramming} stage introduces a Bilateral Cross-Prompt Coordination module, enabling dynamic interaction between branches through prompt-based conditioning, thus enriching joint representation learning without modifying the frozen VFM backbone. Extensive experiments on ten spatio-temporal datasets show that ST-VFM outperforms state-of-the-art baselines, demonstrating effectiveness and robustness across VFM backbones (e.g., DINO, CLIP, DEIT) and ablation studies, establishing it as a strong general framework for spatio-temporal forecasting.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 5,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11562",
        "abs_url": "https://arxiv.org/abs/2507.11562",
        "pdf_url": "https://arxiv.org/pdf/2507.11562",
        "title": "Expert Operational GANS: Towards Real-Color Underwater Image Restoration",
        "authors": [
            "Ozer Can Devecioglu",
            "Serkan Kiranyaz",
            "Mehmet Yamac",
            "Moncef Gabbouj"
        ],
        "comments": "6 pages",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Image and Video Processing (eess.IV)",
        "abstract": "The wide range of deformation artifacts that arise from complex light propagation, scattering, and depth-dependent attenuation makes the underwater image restoration to remain a challenging problem. Like other single deep regressor networks, conventional GAN-based restoration methods struggle to perform well across this heterogeneous domain, since a single generator network is typically insufficient to capture the full range of visual degradations. In order to overcome this limitation, we propose xOp-GAN, a novel GAN model with several expert generator networks, each trained solely on a particular subset with a certain image quality. Thus, each generator can learn to maximize its restoration performance for a particular quality range. Once a xOp-GAN is trained, each generator can restore the input image and the best restored image can then be selected by the discriminator based on its perceptual confidence score. As a result, xOP-GAN is the first GAN model with multiple generators where the discriminator is being used during the inference of the regression task. Experimental results on benchmark Large Scale Underwater Image (LSUI) dataset demonstrates that xOp-GAN achieves PSNR levels up to 25.16 dB, surpassing all single-regressor models by a large margin even, with reduced complexity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 6,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11571",
        "abs_url": "https://arxiv.org/abs/2507.11571",
        "pdf_url": "https://arxiv.org/pdf/2507.11571",
        "title": "Data-Driven Meta-Analysis and Public-Dataset Evaluation for Sensor-Based Gait Age Estimation",
        "authors": [
            "Varun Velankar"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)",
        "abstract": "Estimating a person's age from their gait has important applications in healthcare, security and human-computer interaction. In this work, we review fifty-nine studies involving over seventy-five thousand subjects recorded with video, wearable and radar sensors. We observe that convolutional neural networks produce an average error of about 4.2 years, inertial-sensor models about 4.5 years and multi-sensor fusion as low as 3.4 years, with notable differences between lab and real-world data. We then analyse sixty-three thousand eight hundred forty-six gait cycles from the OU-ISIR Large-Population dataset to quantify correlations between age and five key metrics: stride length, walking speed, step cadence, step-time variability and joint-angle entropy, with correlation coefficients of at least 0.27. Next, we fine-tune a ResNet34 model and apply Grad-CAM to reveal that the network attends to the knee and pelvic regions, consistent with known age-related gait changes. Finally, on a one hundred thousand sample subset of the VersatileGait database, we compare support vector machines, decision trees, random forests, multilayer perceptrons and convolutional neural networks, finding that deep networks achieve up to 96 percent accuracy while processing each sample in under 0.1 seconds. By combining a broad meta-analysis with new large-scale experiments and interpretable visualizations, we establish solid performance baselines and practical guidelines for reducing gait-age error below three years in real-world scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 7,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11575",
        "abs_url": "https://arxiv.org/abs/2507.11575",
        "pdf_url": "https://arxiv.org/pdf/2507.11575",
        "title": "What cat is that? A re-id model for feral cats",
        "authors": [
            "Victor Caquilpan"
        ],
        "comments": "Master's project",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Feral cats exert a substantial and detrimental impact on Australian wildlife, placing them among the most dangerous invasive species worldwide. Therefore, closely monitoring these cats is essential labour in minimising their effects. In this context, the potential application of Re-Identification (re-ID) emerges to enhance monitoring activities for these animals, utilising images captured by camera traps. This project explores different CV approaches to create a re-ID model able to identify individual feral cats in the wild. The main approach consists of modifying a part-pose guided network (PPGNet) model, initially used in the re-ID of Amur tigers, to be applicable for feral cats. This adaptation, resulting in PPGNet-Cat, which incorporates specific modifications to suit the characteristics of feral cats images. Additionally, various experiments were conducted, particularly exploring contrastive learning approaches such as ArcFace loss. The main results indicate that PPGNet-Cat excels in identifying feral cats, achieving high performance with a mean Average Precision (mAP) of 0.86 and a rank-1 accuracy of 0.95. These outcomes establish PPGNet-Cat as a competitive model within the realm of re-ID.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 8,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11579",
        "abs_url": "https://arxiv.org/abs/2507.11579",
        "pdf_url": "https://arxiv.org/pdf/2507.11579",
        "title": "SketchDNN: Joint Continuous-Discrete Diffusion for CAD Sketch Generation",
        "authors": [
            "Sathvik Chereddy",
            "John Femiani"
        ],
        "comments": "17 pages, 63 figures, Proceedings of the 42nd International Conference on Machine Learning (ICML2025)",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "We present SketchDNN, a generative model for synthesizing CAD sketches that jointly models both continuous parameters and discrete class labels through a unified continuous-discrete diffusion process. Our core innovation is Gaussian-Softmax diffusion, where logits perturbed with Gaussian noise are projected onto the probability simplex via a softmax transformation, facilitating blended class labels for discrete variables. This formulation addresses 2 key challenges, namely, the heterogeneity of primitive parameterizations and the permutation invariance of primitives in CAD sketches. Our approach significantly improves generation quality, reducing Fréchet Inception Distance (FID) from 16.04 to 7.80 and negative log-likelihood (NLL) from 84.8 to 81.33, establishing a new state-of-the-art in CAD sketch generation on the SketchGraphs dataset.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 9,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11638",
        "abs_url": "https://arxiv.org/abs/2507.11638",
        "pdf_url": "https://arxiv.org/pdf/2507.11638",
        "title": "Interpretable Prediction of Lymph Node Metastasis in Rectal Cancer MRI Using Variational Autoencoders",
        "authors": [
            "Benjamin Keel",
            "Aaron Quyn",
            "David Jayne",
            "Maryam Mohsin",
            "Samuel D. Relton"
        ],
        "comments": "Published in Medical Image Understanding and Analysis (MIUA) 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Effective treatment for rectal cancer relies on accurate lymph node metastasis (LNM) staging. However, radiological criteria based on lymph node (LN) size, shape and texture morphology have limited diagnostic accuracy. In this work, we investigate applying a Variational Autoencoder (VAE) as a feature encoder model to replace the large pre-trained Convolutional Neural Network (CNN) used in existing approaches. The motivation for using a VAE is that the generative model aims to reconstruct the images, so it directly encodes visual features and meaningful patterns across the data. This leads to a disentangled and structured latent space which can be more interpretable than a CNN. Models are deployed on an in-house MRI dataset with 168 patients who did not undergo neo-adjuvant treatment. The post-operative pathological N stage was used as the ground truth to evaluate model predictions. Our proposed model 'VAE-MLP' achieved state-of-the-art performance on the MRI dataset, with cross-validated metrics of AUC 0.86 +/- 0.05, Sensitivity 0.79 +/- 0.06, and Specificity 0.85 +/- 0.05. Code is available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 10,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11642",
        "abs_url": "https://arxiv.org/abs/2507.11642",
        "pdf_url": "https://arxiv.org/pdf/2507.11642",
        "title": "Posture-Driven Action Intent Inference for Playing style and Fatigue Assessment",
        "authors": [
            "Abhishek Jaiswal",
            "Nisheeth Srivastava"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Posture-based mental state inference has significant potential in diagnosing fatigue, preventing injury, and enhancing performance across various domains. Such tools must be research-validated with large datasets before being translated into practice. Unfortunately, such vision diagnosis faces serious challenges due to the sensitivity of human subject data. To address this, we identify sports settings as a viable alternative for accumulating data from human subjects experiencing diverse emotional states. We test our hypothesis in the game of cricket and present a posture-based solution to identify human intent from activity videos. Our method achieves over 75\\% F1 score and over 80\\% AUC-ROC in discriminating aggressive and defensive shot intent through motion analysis. These findings indicate that posture leaks out strong signals for intent inference, even with inherent noise in the data pipeline. Furthermore, we utilize existing data statistics as weak supervision to validate our findings, offering a potential solution for overcoming data labelling limitations. This research contributes to generalizable techniques for sports analytics and also opens possibilities for applying human behavior analysis across various fields.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 11,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11653",
        "abs_url": "https://arxiv.org/abs/2507.11653",
        "pdf_url": "https://arxiv.org/pdf/2507.11653",
        "title": "VISTA: Monocular Segmentation-Based Mapping for Appearance and View-Invariant Global Localization",
        "authors": [
            "Hannah Shafferman",
            "Annika Thomas",
            "Jouko Kinnari",
            "Michael Ricard",
            "Jose Nino",
            "Jonathan How"
        ],
        "comments": "9 pages, 6 figures. This work has been submitted to the IEEE for possible publication",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
        "abstract": "Global localization is critical for autonomous navigation, particularly in scenarios where an agent must localize within a map generated in a different session or by another agent, as agents often have no prior knowledge about the correlation between reference frames. However, this task remains challenging in unstructured environments due to appearance changes induced by viewpoint variation, seasonal changes, spatial aliasing, and occlusions -- known failure modes for traditional place recognition methods. To address these challenges, we propose VISTA (View-Invariant Segmentation-Based Tracking for Frame Alignment), a novel open-set, monocular global localization framework that combines: 1) a front-end, object-based, segmentation and tracking pipeline, followed by 2) a submap correspondence search, which exploits geometric consistencies between environment maps to align vehicle reference frames. VISTA enables consistent localization across diverse camera viewpoints and seasonal changes, without requiring any domain-specific training or finetuning. We evaluate VISTA on seasonal and oblique-angle aerial datasets, achieving up to a 69% improvement in recall over baseline methods. Furthermore, we maintain a compact object-based map that is only 0.6% the size of the most memory-conservative baseline, making our approach capable of real-time implementation on resource-constrained platforms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 12,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11730",
        "abs_url": "https://arxiv.org/abs/2507.11730",
        "pdf_url": "https://arxiv.org/pdf/2507.11730",
        "title": "Seeing the Signs: A Survey of Edge-Deployable OCR Models for Billboard Visibility Analysis",
        "authors": [
            "Maciej Szankin",
            "Vidhyananth Venkatasamy",
            "Lihang Ying"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Outdoor advertisements remain a critical medium for modern marketing, yet accurately verifying billboard text visibility under real-world conditions is still challenging. Traditional Optical Character Recognition (OCR) pipelines excel at cropped text recognition but often struggle with complex outdoor scenes, varying fonts, and weather-induced visual noise. Recently, multimodal Vision-Language Models (VLMs) have emerged as promising alternatives, offering end-to-end scene understanding with no explicit detection step. This work systematically benchmarks representative VLMs - including Qwen 2.5 VL 3B, InternVL3, and SmolVLM2 - against a compact CNN-based OCR baseline (PaddleOCRv4) across two public datasets (ICDAR 2015 and SVT), augmented with synthetic weather distortions to simulate realistic degradation. Our results reveal that while selected VLMs excel at holistic scene reasoning, lightweight CNN pipelines still achieve competitive accuracy for cropped text at a fraction of the computational cost-an important consideration for edge deployment. To foster future research, we release our weather-augmented benchmark and evaluation code publicly.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 13,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11761",
        "abs_url": "https://arxiv.org/abs/2507.11761",
        "pdf_url": "https://arxiv.org/pdf/2507.11761",
        "title": "Beyond Task-Specific Reasoning: A Unified Conditional Generative Framework for Abstract Visual Reasoning",
        "authors": [
            "Fan Shi",
            "Bin Li",
            "Xiangyang Xue"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Abstract visual reasoning (AVR) enables humans to quickly discover and generalize abstract rules to new scenarios. Designing intelligent systems with human-like AVR abilities has been a long-standing topic in the artificial intelligence community. Deep AVR solvers have recently achieved remarkable success in various AVR tasks. However, they usually use task-specific designs or parameters in different tasks. In such a paradigm, solving new tasks often means retraining the model, and sometimes retuning the model architectures, which increases the cost of solving AVR problems. In contrast to task-specific approaches, this paper proposes a novel Unified Conditional Generative Solver (UCGS), aiming to address multiple AVR tasks in a unified framework. First, we prove that some well-known AVR tasks can be reformulated as the problem of estimating the predictability of target images in problem panels. Then, we illustrate that, under the proposed framework, training one conditional generative model can solve various AVR tasks. The experiments show that with a single round of multi-task training, UCGS demonstrates abstract reasoning ability across various AVR tasks. Especially, UCGS exhibits the ability of zero-shot reasoning, enabling it to perform abstract reasoning on problems from unseen AVR tasks in the testing phase.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 14,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11834",
        "abs_url": "https://arxiv.org/abs/2507.11834",
        "pdf_url": "https://arxiv.org/pdf/2507.11834",
        "title": "CorrMoE: Mixture of Experts with De-stylization Learning for Cross-Scene and Cross-Domain Correspondence Pruning",
        "authors": [
            "Peiwen Xia",
            "Tangfei Liao",
            "Wei Zhu",
            "Danhuai Zhao",
            "Jianjun Ke",
            "Kaihao Zhang",
            "Tong Lu",
            "Tao Wang"
        ],
        "comments": "Accepted by ECAI 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Establishing reliable correspondences between image pairs is a fundamental task in computer vision, underpinning applications such as 3D reconstruction and visual localization. Although recent methods have made progress in pruning outliers from dense correspondence sets, they often hypothesize consistent visual domains and overlook the challenges posed by diverse scene structures. In this paper, we propose CorrMoE, a novel correspondence pruning framework that enhances robustness under cross-domain and cross-scene variations. To address domain shift, we introduce a De-stylization Dual Branch, performing style mixing on both implicit and explicit graph features to mitigate the adverse influence of domain-specific representations. For scene diversity, we design a Bi-Fusion Mixture of Experts module that adaptively integrates multi-perspective features through linear-complexity attention and dynamic expert routing. Extensive experiments on benchmark datasets demonstrate that CorrMoE achieves superior accuracy and generalization compared to state-of-the-art methods. The code and pre-trained models are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 15,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11845",
        "abs_url": "https://arxiv.org/abs/2507.11845",
        "pdf_url": "https://arxiv.org/pdf/2507.11845",
        "title": "ProtoConNet: Prototypical Augmentation and Alignment for Open-Set Few-Shot Image Classification",
        "authors": [
            "Kexuan Shi",
            "Zhuang Qi",
            "Jingjing Zhu",
            "Lei Meng",
            "Yaochen Zhang",
            "Haibei Huang",
            "Xiangxu Meng"
        ],
        "comments": "Accepted in ChinaMM and recommended to Displays",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Open-set few-shot image classification aims to train models using a small amount of labeled data, enabling them to achieve good generalization when confronted with unknown environments. Existing methods mainly use visual information from a single image to learn class representations to distinguish known from unknown categories. However, these methods often overlook the benefits of integrating rich contextual information. To address this issue, this paper proposes a prototypical augmentation and alignment method, termed ProtoConNet, which incorporates background information from different samples to enhance the diversity of the feature space, breaking the spurious associations between context and image subjects in few-shot scenarios. Specifically, it consists of three main modules: the clustering-based data selection (CDS) module mines diverse data patterns while preserving core features; the contextual-enhanced semantic refinement (CSR) module builds a context dictionary to integrate into image representations, which boosts the model's robustness in various scenarios; and the prototypical alignment (PA) module reduces the gap between image representations and class prototypes, amplifying feature distances for known and unknown classes. Experimental results from two datasets verified that ProtoConNet enhances the effectiveness of representation learning in few-shot scenarios and identifies open-set samples, making it superior to existing methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 16,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11892",
        "abs_url": "https://arxiv.org/abs/2507.11892",
        "pdf_url": "https://arxiv.org/pdf/2507.11892",
        "title": "From Coarse to Nuanced: Cross-Modal Alignment of Fine-Grained Linguistic Cues and Visual Salient Regions for Dynamic Emotion Recognition",
        "authors": [
            "Yu Liu",
            "Leyuan Qu",
            "Hanlei Shi",
            "Di Gao",
            "Yuhua Zheng",
            "Taihao Li"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)",
        "abstract": "Dynamic Facial Expression Recognition (DFER) aims to identify human emotions from temporally evolving facial movements and plays a critical role in affective computing. While recent vision-language approaches have introduced semantic textual descriptions to guide expression recognition, existing methods still face two key limitations: they often underutilize the subtle emotional cues embedded in generated text, and they have yet to incorporate sufficiently effective mechanisms for filtering out facial dynamics that are irrelevant to emotional expression. To address these gaps, We propose GRACE, Granular Representation Alignment for Cross-modal Emotion recognition that integrates dynamic motion modeling, semantic text refinement, and token-level cross-modal alignment to facilitate the precise localization of emotionally salient spatiotemporal features. Our method constructs emotion-aware textual descriptions via a Coarse-to-fine Affective Text Enhancement (CATE) module and highlights expression-relevant facial motion through a motion-difference weighting mechanism. These refined semantic and visual signals are aligned at the token level using entropy-regularized optimal transport. Experiments on three benchmark datasets demonstrate that our method significantly improves recognition performance, particularly in challenging settings with ambiguous or imbalanced emotion classes, establishing new state-of-the-art (SOTA) results in terms of both UAR and WAR.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 17,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11893",
        "abs_url": "https://arxiv.org/abs/2507.11893",
        "pdf_url": "https://arxiv.org/pdf/2507.11893",
        "title": "Spatial Frequency Modulation for Semantic Segmentation",
        "authors": [
            "Linwei Chen",
            "Ying Fu",
            "Lin Gu",
            "Dezhi Zheng",
            "Jifeng Dai"
        ],
        "comments": "Accept by TPAMI 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "High spatial frequency information, including fine details like textures, significantly contributes to the accuracy of semantic segmentation. However, according to the Nyquist-Shannon Sampling Theorem, high-frequency components are vulnerable to aliasing or distortion when propagating through downsampling layers such as strided-convolution. Here, we propose a novel Spatial Frequency Modulation (SFM) that modulates high-frequency features to a lower frequency before downsampling and then demodulates them back during upsampling. Specifically, we implement modulation through adaptive resampling (ARS) and design a lightweight add-on that can densely sample the high-frequency areas to scale up the signal, thereby lowering its frequency in accordance with the Frequency Scaling Property. We also propose Multi-Scale Adaptive Upsampling (MSAU) to demodulate the modulated feature and recover high-frequency information through non-uniform upsampling This module further improves segmentation by explicitly exploiting information interaction between densely and sparsely resampled areas at multiple scales. Both modules can seamlessly integrate with various architectures, extending from convolutional neural networks to transformers. Feature visualization and analysis confirm that our method effectively alleviates aliasing while successfully retaining details after demodulation. Finally, we validate the broad applicability and effectiveness of SFM by extending it to image classification, adversarial robustness, instance segmentation, and panoptic segmentation tasks. The code is available at \\href{this https URL}{this https URL}.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 18,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11910",
        "abs_url": "https://arxiv.org/abs/2507.11910",
        "pdf_url": "https://arxiv.org/pdf/2507.11910",
        "title": "SEPose: A Synthetic Event-based Human Pose Estimation Dataset for Pedestrian Monitoring",
        "authors": [
            "Kaustav Chanda",
            "Aayush Atul Verma",
            "Arpitsinh Vaghela",
            "Yezhou Yang",
            "Bharatesh Chakravarthi"
        ],
        "comments": "Accepted at the 28th IEEE International Conference on Intelligent Transportation Systems (ITSC 2025)",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Event-based sensors have emerged as a promising solution for addressing challenging conditions in pedestrian and traffic monitoring systems. Their low-latency and high dynamic range allow for improved response time in safety-critical situations caused by distracted walking or other unusual movements. However, the availability of data covering such scenarios remains limited. To address this gap, we present SEPose -- a comprehensive synthetic event-based human pose estimation dataset for fixed pedestrian perception generated using dynamic vision sensors in the CARLA simulator. With nearly 350K annotated pedestrians with body pose keypoints from the perspective of fixed traffic cameras, SEPose is a comprehensive synthetic multi-person pose estimation dataset that spans busy and light crowds and traffic across diverse lighting and weather conditions in 4-way intersections in urban, suburban, and rural environments. We train existing state-of-the-art models such as RVT and YOLOv8 on our dataset and evaluate them on real event-based data to demonstrate the sim-to-real generalization capabilities of the proposed dataset.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 19,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11931",
        "abs_url": "https://arxiv.org/abs/2507.11931",
        "pdf_url": "https://arxiv.org/pdf/2507.11931",
        "title": "Dark-EvGS: Event Camera as an Eye for Radiance Field in the Dark",
        "authors": [
            "Jingqian Wu",
            "Peiqi Duan",
            "Zongqiang Wang",
            "Changwei Wang",
            "Boxin Shi",
            "Edmund Y. Lam"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In low-light environments, conventional cameras often struggle to capture clear multi-view images of objects due to dynamic range limitations and motion blur caused by long exposure. Event cameras, with their high-dynamic range and high-speed properties, have the potential to mitigate these issues. Additionally, 3D Gaussian Splatting (GS) enables radiance field reconstruction, facilitating bright frame synthesis from multiple viewpoints in low-light conditions. However, naively using an event-assisted 3D GS approach still faced challenges because, in low light, events are noisy, frames lack quality, and the color tone may be inconsistent. To address these issues, we propose Dark-EvGS, the first event-assisted 3D GS framework that enables the reconstruction of bright frames from arbitrary viewpoints along the camera trajectory. Triplet-level supervision is proposed to gain holistic knowledge, granular details, and sharp scene rendering. The color tone matching block is proposed to guarantee the color consistency of the rendered frames. Furthermore, we introduce the first real-captured dataset for the event-guided bright frame synthesis task via 3D GS-based radiance field reconstruction. Experiments demonstrate that our method achieves better results than existing methods, conquering radiance field reconstruction under challenging low-light conditions. The code and sample data are included in the supplementary material.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 20,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11932",
        "abs_url": "https://arxiv.org/abs/2507.11932",
        "pdf_url": "https://arxiv.org/pdf/2507.11932",
        "title": "Hyperphantasia: A Benchmark for Evaluating the Mental Visualization Capabilities of Multimodal LLMs",
        "authors": [
            "Mohammad Shahab Sepehri",
            "Berk Tinaz",
            "Zalan Fabian",
            "Mahdi Soltanolkotabi"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Mental visualization, the ability to construct and manipulate visual representations internally, is a core component of human cognition and plays a vital role in tasks involving reasoning, prediction, and abstraction. Despite the rapid progress of Multimodal Large Language Models (MLLMs), current benchmarks primarily assess passive visual perception, offering limited insight into the more active capability of internally constructing visual patterns to support problem solving. Yet mental visualization is a critical cognitive skill in humans, supporting abilities such as spatial navigation, predicting physical trajectories, and solving complex visual problems through imaginative simulation. To bridge this gap, we introduce Hyperphantasia, a synthetic benchmark designed to evaluate the mental visualization abilities of MLLMs through four carefully constructed puzzles. Each task is procedurally generated and presented at three difficulty levels, enabling controlled analysis of model performance across increasing complexity. Our comprehensive evaluation of state-of-the-art models reveals a substantial gap between the performance of humans and MLLMs. Additionally, we explore the potential of reinforcement learning to improve visual simulation capabilities. Our findings suggest that while some models exhibit partial competence in recognizing visual patterns, robust mental visualization remains an open challenge for current MLLMs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 21,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11947",
        "abs_url": "https://arxiv.org/abs/2507.11947",
        "pdf_url": "https://arxiv.org/pdf/2507.11947",
        "title": "RaDL: Relation-aware Disentangled Learning for Multi-Instance Text-to-Image Generation",
        "authors": [
            "Geon Park",
            "Seon Bin Kim",
            "Gunho Jung",
            "Seong-Whan Lee"
        ],
        "comments": "6 Pages",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "With recent advancements in text-to-image (T2I) models, effectively generating multiple instances within a single image prompt has become a crucial challenge. Existing methods, while successful in generating positions of individual instances, often struggle to account for relationship discrepancy and multiple attributes leakage. To address these limitations, this paper proposes the relation-aware disentangled learning (RaDL) framework. RaDL enhances instance-specific attributes through learnable parameters and generates relation-aware image features via Relation Attention, utilizing action verbs extracted from the global prompt. Through extensive evaluations on benchmarks such as COCO-Position, COCO-MIG, and DrawBench, we demonstrate that RaDL outperforms existing methods, showing significant improvements in positional accuracy, multiple attributes consideration, and the relationships between instances. Our results present RaDL as the solution for generating images that consider both the relationships and multiple attributes of each instance within the multi-instance image.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 22,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11955",
        "abs_url": "https://arxiv.org/abs/2507.11955",
        "pdf_url": "https://arxiv.org/pdf/2507.11955",
        "title": "Prototypical Progressive Alignment and Reweighting for Generalizable Semantic Segmentation",
        "authors": [
            "Yuhang Zhang",
            "Zhengyu Zhang",
            "Muxin Liao",
            "Shishun Tian",
            "Wenbin Zou",
            "Lu Zhang",
            "Chen Xu"
        ],
        "comments": "This paper was accepted by IEEE Transactions on Intelligent Transportation Systems",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Generalizable semantic segmentation aims to perform well on unseen target domains, a critical challenge due to real-world applications requiring high generalizability. Class-wise prototypes, representing class centroids, serve as domain-invariant cues that benefit generalization due to their stability and semantic consistency. However, this approach faces three challenges. First, existing methods often adopt coarse prototypical alignment strategies, which may hinder performance. Second, naive prototypes computed by averaging source batch features are prone to overfitting and may be negatively affected by unrelated source data. Third, most methods treat all source samples equally, ignoring the fact that different features have varying adaptation difficulties. To address these limitations, we propose a novel framework for generalizable semantic segmentation: Prototypical Progressive Alignment and Reweighting (PPAR), leveraging the strong generalization ability of the CLIP model. Specifically, we define two prototypes: the Original Text Prototype (OTP) and Visual Text Prototype (VTP), generated via CLIP to serve as a solid base for alignment. We then introduce a progressive alignment strategy that aligns features in an easy-to-difficult manner, reducing domain gaps gradually. Furthermore, we propose a prototypical reweighting mechanism that estimates the reliability of source data and adjusts its contribution, mitigating the effect of irrelevant or harmful features (i.e., reducing negative transfer). We also provide a theoretical analysis showing the alignment between our method and domain generalization theory. Extensive experiments across multiple benchmarks demonstrate that PPAR achieves state-of-the-art performance, validating its effectiveness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 23,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11967",
        "abs_url": "https://arxiv.org/abs/2507.11967",
        "pdf_url": "https://arxiv.org/pdf/2507.11967",
        "title": "Language-Guided Contrastive Audio-Visual Masked Autoencoder with Automatically Generated Audio-Visual-Text Triplets from Videos",
        "authors": [
            "Yuchi Ishikawa",
            "Shota Nakada",
            "Hokuto Munakata",
            "Kazuhiro Saito",
            "Tatsuya Komatsu",
            "Yoshimitsu Aoki"
        ],
        "comments": "Interspeech 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Audio and Speech Processing (eess.AS); Image and Video Processing (eess.IV)",
        "abstract": "In this paper, we propose Language-Guided Contrastive Audio-Visual Masked Autoencoders (LG-CAV-MAE) to improve audio-visual representation learning. LG-CAV-MAE integrates a pretrained text encoder into contrastive audio-visual masked autoencoders, enabling the model to learn across audio, visual and text modalities. To train LG-CAV-MAE, we introduce an automatic method to generate audio-visual-text triplets from unlabeled videos. We first generate frame-level captions using an image captioning model and then apply CLAP-based filtering to ensure strong alignment between audio and captions. This approach yields high-quality audio-visual-text triplets without requiring manual annotations. We evaluate LG-CAV-MAE on audio-visual retrieval tasks, as well as an audio-visual classification task. Our method significantly outperforms existing approaches, achieving up to a 5.6% improvement in recall@10 for retrieval tasks and a 3.2% improvement for the classification task.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 24,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11968",
        "abs_url": "https://arxiv.org/abs/2507.11968",
        "pdf_url": "https://arxiv.org/pdf/2507.11968",
        "title": "Watch, Listen, Understand, Mislead: Tri-modal Adversarial Attacks on Short Videos for Content Appropriateness Evaluation",
        "authors": [
            "Sahid Hossain Mustakim",
            "S M Jishanul Islam",
            "Ummay Maria Muna",
            "Montasir Chowdhury",
            "Mohammed Jawwadul Islam",
            "Sadia Ahmmed",
            "Tashfia Sikder",
            "Syed Tasdid Azam Dhrubo",
            "Swakkhar Shatabda"
        ],
        "comments": "Accepted as long paper, SVU Workshop at ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Multimodal Large Language Models (MLLMs) are increasingly used for content moderation, yet their robustness in short-form video contexts remains underexplored. Current safety evaluations often rely on unimodal attacks, failing to address combined attack vulnerabilities. In this paper, we introduce a comprehensive framework for evaluating the tri-modal safety of MLLMs. First, we present the Short-Video Multimodal Adversarial (SVMA) dataset, comprising diverse short-form videos with human-guided synthetic adversarial attacks. Second, we propose ChimeraBreak, a novel tri-modal attack strategy that simultaneously challenges visual, auditory, and semantic reasoning pathways. Extensive experiments on state-of-the-art MLLMs reveal significant vulnerabilities with high Attack Success Rates (ASR). Our findings uncover distinct failure modes, showing model biases toward misclassifying benign or policy-violating content. We assess results using LLM-as-a-judge, demonstrating attack reasoning efficacy. Our dataset and findings provide crucial insights for developing more robust and safe MLLMs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 25,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11969",
        "abs_url": "https://arxiv.org/abs/2507.11969",
        "pdf_url": "https://arxiv.org/pdf/2507.11969",
        "title": "GS-Bias: Global-Spatial Bias Learner for Single-Image Test-Time Adaptation of Vision-Language Models",
        "authors": [
            "Zhaohong Huang",
            "Yuxin Zhang",
            "Jingjing Xie",
            "Fei Chao",
            "Rongrong Ji"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Recent advances in test-time adaptation (TTA) for Vision-Language Models (VLMs) have garnered increasing attention, particularly through the use of multiple augmented views of a single image to boost zero-shot generalization. Unfortunately, existing methods fail to strike a satisfactory balance between performance and efficiency, either due to excessive overhead of tuning text prompts or unstable benefits from handcrafted, training-free visual feature enhancement. In this paper, we present Global-Spatial Bias Learner (GS-Bias), an efficient and effective TTA paradigm that incorporates two learnable biases during TTA, unfolded as the global bias and spatial bias. Particularly, the global bias captures the global semantic features of a test image by learning consistency across augmented views, while spatial bias learns the semantic coherence between regions in the image's spatial visual representation. It is worth highlighting that these two sets of biases are directly added to the logits outputed by the pretrained VLMs, which circumvent the full backpropagation through VLM that hinders the efficiency of existing TTA methods. This endows GS-Bias with extremely high efficiency while achieving state-of-the-art performance on 15 benchmark datasets. For example, it achieves a 2.23% improvement over TPT in cross-dataset generalization and a 2.72% improvement in domain generalization, while requiring only 6.5% of TPT's memory usage on ImageNet.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 26,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11980",
        "abs_url": "https://arxiv.org/abs/2507.11980",
        "pdf_url": "https://arxiv.org/pdf/2507.11980",
        "title": "EC-Diff: Fast and High-Quality Edge-Cloud Collaborative Inference for Diffusion Models",
        "authors": [
            "Jiajian Xie",
            "Shengyu Zhang",
            "Zhou Zhao",
            "Fan Wu",
            "Fei Wu"
        ],
        "comments": "21 pages, 8 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Diffusion Models have shown remarkable proficiency in image and video synthesis. As model size and latency increase limit user experience, hybrid edge-cloud collaborative framework was recently proposed to realize fast inference and high-quality generation, where the cloud model initiates high-quality semantic planning and the edge model expedites later-stage refinement. However, excessive cloud denoising prolongs inference time, while insufficient steps cause semantic ambiguity, leading to inconsistency in edge model output. To address these challenges, we propose EC-Diff that accelerates cloud inference through gradient-based noise estimation while identifying the optimal point for cloud-edge handoff to maintain generation quality. Specifically, we design a K-step noise approximation strategy to reduce cloud inference frequency by using noise gradients between steps and applying cloud inference periodically to adjust errors. Then we design a two-stage greedy search algorithm to efficiently find the optimal parameters for noise approximation and edge model switching. Extensive experiments demonstrate that our method significantly enhances generation quality compared to edge inference, while achieving up to an average $2\\times$ speedup in inference compared to cloud inference. Video samples and source code are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 27,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11985",
        "abs_url": "https://arxiv.org/abs/2507.11985",
        "pdf_url": "https://arxiv.org/pdf/2507.11985",
        "title": "Unsupervised Part Discovery via Descriptor-Based Masked Image Restoration with Optimized Constraints",
        "authors": [
            "Jiahao Xia",
            "Yike Wu",
            "Wenjian Huang",
            "Jianguo Zhang",
            "Jian Zhang"
        ],
        "comments": "Accepted to ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Part-level features are crucial for image understanding, but few studies focus on them because of the lack of fine-grained labels. Although unsupervised part discovery can eliminate the reliance on labels, most of them cannot maintain robustness across various categories and scenarios, which restricts their application range. To overcome this limitation, we present a more effective paradigm for unsupervised part discovery, named Masked Part Autoencoder (MPAE). It first learns part descriptors as well as a feature map from the inputs and produces patch features from a masked version of the original images. Then, the masked regions are filled with the learned part descriptors based on the similarity between the local features and descriptors. By restoring these masked patches using the part descriptors, they become better aligned with their part shapes, guided by appearance features from unmasked patches. Finally, MPAE robustly discovers meaningful parts that closely match the actual object shapes, even in complex scenarios. Moreover, several looser yet more effective constraints are proposed to enable MPAE to identify the presence of parts across various scenarios and categories in an unsupervised manner. This provides the foundation for addressing challenges posed by occlusion and for exploring part similarity across multiple categories. Extensive experiments demonstrate that our method robustly discovers meaningful parts across various categories and scenarios. The code is available at the project this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 28,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11986",
        "abs_url": "https://arxiv.org/abs/2507.11986",
        "pdf_url": "https://arxiv.org/pdf/2507.11986",
        "title": "Style Composition within Distinct LoRA modules for Traditional Art",
        "authors": [
            "Jaehyun Lee",
            "Wonhark Park",
            "Wonsik Shin",
            "Hyunho Lee",
            "Hyoung Min Na",
            "Nojun Kwak"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Diffusion-based text-to-image models have achieved remarkable results in synthesizing diverse images from text prompts and can capture specific artistic styles via style personalization. However, their entangled latent space and lack of smooth interpolation make it difficult to apply distinct painting techniques in a controlled, regional manner, often causing one style to dominate. To overcome this, we propose a zero-shot diffusion pipeline that naturally blends multiple styles by performing style composition on the denoised latents predicted during the flow-matching denoising process of separately trained, style-specialized models. We leverage the fact that lower-noise latents carry stronger stylistic information and fuse them across heterogeneous diffusion pipelines using spatial masks, enabling precise, region-specific style control. This mechanism preserves the fidelity of each individual style while allowing user-guided mixing. Furthermore, to ensure structural coherence across different models, we incorporate depth-map conditioning via ControlNet into the diffusion framework. Qualitative and quantitative experiments demonstrate that our method successfully achieves region-specific style mixing according to the given masks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 29,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11990",
        "abs_url": "https://arxiv.org/abs/2507.11990",
        "pdf_url": "https://arxiv.org/pdf/2507.11990",
        "title": "ID-EA: Identity-driven Text Enhancement and Adaptation with Textual Inversion for Personalized Text-to-Image Generation",
        "authors": [
            "Hyun-Jun Jin",
            "Young-Eun Kim",
            "Seong-Whan Lee"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Recently, personalized portrait generation with a text-to-image diffusion model has significantly advanced with Textual Inversion, emerging as a promising approach for creating high-fidelity personalized images. Despite its potential, current Textual Inversion methods struggle to maintain consistent facial identity due to semantic misalignments between textual and visual embedding spaces regarding identity. We introduce ID-EA, a novel framework that guides text embeddings to align with visual identity embeddings, thereby improving identity preservation in a personalized generation. ID-EA comprises two key components: the ID-driven Enhancer (ID-Enhancer) and the ID-conditioned Adapter (ID-Adapter). First, the ID-Enhancer integrates identity embeddings with a textual ID anchor, refining visual identity embeddings derived from a face recognition model using representative text embeddings. Then, the ID-Adapter leverages the identity-enhanced embedding to adapt the text condition, ensuring identity preservation by adjusting the cross-attention module in the pre-trained UNet model. This process encourages the text features to find the most related visual clues across the foreground snippets. Extensive quantitative and qualitative evaluations demonstrate that ID-EA substantially outperforms state-of-the-art methods in identity preservation metrics while achieving remarkable computational efficiency, generating personalized portraits approximately 15 times faster than existing approaches.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 30,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11994",
        "abs_url": "https://arxiv.org/abs/2507.11994",
        "pdf_url": "https://arxiv.org/pdf/2507.11994",
        "title": "SAMST: A Transformer framework based on SAM pseudo label filtering for remote sensing semi-supervised semantic segmentation",
        "authors": [
            "Jun Yin",
            "Fei Wu",
            "Yupeng Ren",
            "Jisheng Huang",
            "Qiankun Li",
            "Heng jin",
            "Jianhai Fu",
            "Chanjie Cui"
        ],
        "comments": "IGARSS2025 accepted, Correspondence: fujianhai2024@gmail.com (J.F.), cuichj@mail2.this http URL (C.C.)",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Public remote sensing datasets often face limitations in universality due to resolution variability and inconsistent land cover category definitions. To harness the vast pool of unlabeled remote sensing data, we propose SAMST, a semi-supervised semantic segmentation method. SAMST leverages the strengths of the Segment Anything Model (SAM) in zero-shot generalization and boundary detection. SAMST iteratively refines pseudo-labels through two main components: supervised model self-training using both labeled and pseudo-labeled data, and a SAM-based Pseudo-label Refiner. The Pseudo-label Refiner comprises three modules: a Threshold Filter Module for preprocessing, a Prompt Generation Module for extracting connected regions and generating prompts for SAM, and a Label Refinement Module for final label stitching. By integrating the generalization power of large models with the training efficiency of small models, SAMST improves pseudo-label accuracy, thereby enhancing overall model performance. Experiments on the Potsdam dataset validate the effectiveness and feasibility of SAMST, demonstrating its potential to address the challenges posed by limited labeled data in remote sensing semantic segmentation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 31,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12001",
        "abs_url": "https://arxiv.org/abs/2507.12001",
        "pdf_url": "https://arxiv.org/pdf/2507.12001",
        "title": "AU-Blendshape for Fine-grained Stylized 3D Facial Expression Manipulation",
        "authors": [
            "Hao Li",
            "Ju Dai",
            "Feng Zhou",
            "Kaida Ning",
            "Lei Li",
            "Junjun Pan"
        ],
        "comments": "ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "While 3D facial animation has made impressive progress, challenges still exist in realizing fine-grained stylized 3D facial expression manipulation due to the lack of appropriate datasets. In this paper, we introduce the AUBlendSet, a 3D facial dataset based on AU-Blendshape representation for fine-grained facial expression manipulation across identities. AUBlendSet is a blendshape data collection based on 32 standard facial action units (AUs) across 500 identities, along with an additional set of facial postures annotated with detailed AUs. Based on AUBlendSet, we propose AUBlendNet to learn AU-Blendshape basis vectors for different character styles. AUBlendNet predicts, in parallel, the AU-Blendshape basis vectors of the corresponding style for a given identity mesh, thereby achieving stylized 3D emotional facial manipulation. We comprehensively validate the effectiveness of AUBlendSet and AUBlendNet through tasks such as stylized facial expression manipulation, speech-driven emotional facial animation, and emotion recognition data augmentation. Through a series of qualitative and quantitative experiments, we demonstrate the potential and importance of AUBlendSet and AUBlendNet in 3D facial animation tasks. To the best of our knowledge, AUBlendSet is the first dataset, and AUBlendNet is the first network for continuous 3D facial expression manipulation for any identity through facial AUs. Our source code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 32,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12006",
        "abs_url": "https://arxiv.org/abs/2507.12006",
        "pdf_url": "https://arxiv.org/pdf/2507.12006",
        "title": "Frequency-Dynamic Attention Modulation for Dense Prediction",
        "authors": [
            "Linwei Chen",
            "Lin Gu",
            "Ying Fu"
        ],
        "comments": "Accepted by ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Vision Transformers (ViTs) have significantly advanced computer vision, demonstrating strong performance across various tasks. However, the attention mechanism in ViTs makes each layer function as a low-pass filter, and the stacked-layer architecture in existing transformers suffers from frequency vanishing. This leads to the loss of critical details and textures. We propose a novel, circuit-theory-inspired strategy called Frequency-Dynamic Attention Modulation (FDAM), which can be easily plugged into ViTs. FDAM directly modulates the overall frequency response of ViTs and consists of two techniques: Attention Inversion (AttInv) and Frequency Dynamic Scaling (FreqScale). Since circuit theory uses low-pass filters as fundamental elements, we introduce AttInv, a method that generates complementary high-pass filtering by inverting the low-pass filter in the attention matrix, and dynamically combining the two. We further design FreqScale to weight different frequency components for fine-grained adjustments to the target response function. Through feature similarity analysis and effective rank evaluation, we demonstrate that our approach avoids representation collapse, leading to consistent performance improvements across various models, including SegFormer, DeiT, and MaskDINO. These improvements are evident in tasks such as semantic segmentation, object detection, and instance segmentation. Additionally, we apply our method to remote sensing detection, achieving state-of-the-art results in single-scale settings. The code is available at \\href{this https URL}{this https URL}.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 33,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12008",
        "abs_url": "https://arxiv.org/abs/2507.12008",
        "pdf_url": "https://arxiv.org/pdf/2507.12008",
        "title": "Dual form Complementary Masking for Domain-Adaptive Image Segmentation",
        "authors": [
            "Jiawen Wang",
            "Yinda Chen",
            "Xiaoyu Liu",
            "Che Liu",
            "Dong Liu",
            "Jianqing Gao",
            "Zhiwei Xiong"
        ],
        "comments": "Accepted by ICML 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Recent works have correlated Masked Image Modeling (MIM) with consistency regularization in Unsupervised Domain Adaptation (UDA). However, they merely treat masking as a special form of deformation on the input images and neglect the theoretical analysis, which leads to a superficial understanding of masked reconstruction and insufficient exploitation of its potential in enhancing feature extraction and representation learning. In this paper, we reframe masked reconstruction as a sparse signal reconstruction problem and theoretically prove that the dual form of complementary masks possesses superior capabilities in extracting domain-agnostic image features. Based on this compelling insight, we propose MaskTwins, a simple yet effective UDA framework that integrates masked reconstruction directly into the main training pipeline. MaskTwins uncovers intrinsic structural patterns that persist across disparate domains by enforcing consistency between predictions of images masked in complementary ways, enabling domain generalization in an end-to-end manner. Extensive experiments verify the superiority of MaskTwins over baseline methods in natural and biological image segmentation. These results demonstrate the significant advantages of MaskTwins in extracting domain-invariant features without the need for separate pre-training, offering a new paradigm for domain-adaptive segmentation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 34,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12009",
        "abs_url": "https://arxiv.org/abs/2507.12009",
        "pdf_url": "https://arxiv.org/pdf/2507.12009",
        "title": "Deep Neural Encoder-Decoder Model to Relate fMRI Brain Activity with Naturalistic Stimuli",
        "authors": [
            "Florian David",
            "Michael Chan",
            "Elenor Morgenroth",
            "Patrik Vuilleumier",
            "Dimitri Van De Ville"
        ],
        "comments": "Accepted in International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC)",
        "abstract": "We propose an end-to-end deep neural encoder-decoder model to encode and decode brain activity in response to naturalistic stimuli using functional magnetic resonance imaging (fMRI) data. Leveraging temporally correlated input from consecutive film frames, we employ temporal convolutional layers in our architecture, which effectively allows to bridge the temporal resolution gap between natural movie stimuli and fMRI acquisitions. Our model predicts activity of voxels in and around the visual cortex and performs reconstruction of corresponding visual inputs from neural activity. Finally, we investigate brain regions contributing to visual decoding through saliency maps. We find that the most contributing regions are the middle occipital area, the fusiform area, and the calcarine, respectively employed in shape perception, complex recognition (in particular face perception), and basic visual features such as edges and contrasts. These functions being strongly solicited are in line with the decoder's capability to reconstruct edges, faces, and contrasts. All in all, this suggests the possibility to probe our understanding of visual processing in films using as a proxy the behaviour of deep learning models such as the one proposed in this paper.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 35,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12017",
        "abs_url": "https://arxiv.org/abs/2507.12017",
        "pdf_url": "https://arxiv.org/pdf/2507.12017",
        "title": "SS-DC: Spatial-Spectral Decoupling and Coupling Across Visible-Infrared Gap for Domain Adaptive Object Detection",
        "authors": [
            "Xiwei Zhang",
            "Chunjin Yang",
            "Yiming Xiao",
            "Runtong Zhang",
            "Fanman Meng"
        ],
        "comments": "8 main-pages, 3 reference-pages, 5 figures, 6 tables",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Unsupervised domain adaptive object detection (UDAOD) from the visible domain to the infrared (RGB-IR) domain is challenging. Existing methods regard the RGB domain as a unified domain and neglect the multiple subdomains within it, such as daytime, nighttime, and foggy scenes. We argue that decoupling the domain-invariant (DI) and domain-specific (DS) features across these multiple subdomains is beneficial for RGB-IR domain adaptation. To this end, this paper proposes a new SS-DC framework based on a decoupling-coupling strategy. In terms of decoupling, we design a Spectral Adaptive Idempotent Decoupling (SAID) module in the aspect of spectral decomposition. Due to the style and content information being highly embedded in different frequency bands, this module can decouple DI and DS components more accurately and interpretably. A novel filter bank-based spectral processing paradigm and a self-distillation-driven decoupling loss are proposed to improve the spectral domain decoupling. In terms of coupling, a new spatial-spectral coupling method is proposed, which realizes joint coupling through spatial and spectral DI feature pyramids. Meanwhile, this paper introduces DS from decoupling to reduce the domain bias. Extensive experiments demonstrate that our method can significantly improve the baseline performance and outperform existing UDAOD methods on multiple RGB-IR datasets, including a new experimental protocol proposed in this paper based on the FLIR-ADAS dataset.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 36,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12022",
        "abs_url": "https://arxiv.org/abs/2507.12022",
        "pdf_url": "https://arxiv.org/pdf/2507.12022",
        "title": "Dataset Ownership Verification for Pre-trained Masked Models",
        "authors": [
            "Yuechen Xie",
            "Jie Song",
            "Yicheng Shan",
            "Xiaoyan Zhang",
            "Yuanyu Wan",
            "Shengxuming Zhang",
            "Jiarui Duan",
            "Mingli Song"
        ],
        "comments": "Accepted by ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "High-quality open-source datasets have emerged as a pivotal catalyst driving the swift advancement of deep learning, while facing the looming threat of potential exploitation. Protecting these datasets is of paramount importance for the interests of their owners. The verification of dataset ownership has evolved into a crucial approach in this domain; however, existing verification techniques are predominantly tailored to supervised models and contrastive pre-trained models, rendering them ill-suited for direct application to the increasingly prevalent masked models. In this work, we introduce the inaugural methodology addressing this critical, yet unresolved challenge, termed Dataset Ownership Verification for Masked Modeling (DOV4MM). The central objective is to ascertain whether a suspicious black-box model has been pre-trained on a particular unlabeled dataset, thereby assisting dataset owners in safeguarding their rights. DOV4MM is grounded in our empirical observation that when a model is pre-trained on the target dataset, the difficulty of reconstructing masked information within the embedding space exhibits a marked contrast to models not pre-trained on that dataset. We validated the efficacy of DOV4MM through ten masked image models on ImageNet-1K and four masked language models on WikiText-103. The results demonstrate that DOV4MM rejects the null hypothesis, with a $p$-value considerably below 0.05, surpassing all prior approaches. Code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 37,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12023",
        "abs_url": "https://arxiv.org/abs/2507.12023",
        "pdf_url": "https://arxiv.org/pdf/2507.12023",
        "title": "MVAR: MultiVariate AutoRegressive Air Pollutants Forecasting Model",
        "authors": [
            "Xu Fan",
            "Zhihao Wang",
            "Yuetan Lin",
            "Yan Zhang",
            "Yang Xiang",
            "Hao Li"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Air pollutants pose a significant threat to the environment and human health, thus forecasting accurate pollutant concentrations is essential for pollution warnings and policy-making. Existing studies predominantly focus on single-pollutant forecasting, neglecting the interactions among different pollutants and their diverse spatial responses. To address the practical needs of forecasting multivariate air pollutants, we propose MultiVariate AutoRegressive air pollutants forecasting model (MVAR), which reduces the dependency on long-time-window inputs and boosts the data utilization efficiency. We also design the Multivariate Autoregressive Training Paradigm, enabling MVAR to achieve 120-hour long-term sequential forecasting. Additionally, MVAR develops Meteorological Coupled Spatial Transformer block, enabling the flexible coupling of AI-based meteorological forecasts while learning the interactions among pollutants and their diverse spatial responses. As for the lack of standardized datasets in air pollutants forecasting, we construct a comprehensive dataset covering 6 major pollutants across 75 cities in North China from 2018 to 2023, including ERA5 reanalysis data and FuXi-2.0 forecast data. Experimental results demonstrate that the proposed model outperforms state-of-the-art methods and validate the effectiveness of the proposed architecture.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 38,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12026",
        "abs_url": "https://arxiv.org/abs/2507.12026",
        "pdf_url": "https://arxiv.org/pdf/2507.12026",
        "title": "3D-MoRe: Unified Modal-Contextual Reasoning for Embodied Question Answering",
        "authors": [
            "Rongtao Xu",
            "Han Gao",
            "Mingming Yu",
            "Dong An",
            "Shunpeng Chen",
            "Changwei Wang",
            "Li Guo",
            "Xiaodan Liang",
            "Shibiao Xu"
        ],
        "comments": "Accepted by IROS 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "With the growing need for diverse and scalable data in indoor scene tasks, such as question answering and dense captioning, we propose 3D-MoRe, a novel paradigm designed to generate large-scale 3D-language datasets by leveraging the strengths of foundational models. The framework integrates key components, including multi-modal embedding, cross-modal interaction, and a language model decoder, to process natural language instructions and 3D scene data. This approach facilitates enhanced reasoning and response generation in complex 3D environments. Using the ScanNet 3D scene dataset, along with text annotations from ScanQA and ScanRefer, 3D-MoRe generates 62,000 question-answer (QA) pairs and 73,000 object descriptions across 1,513 scenes. We also employ various data augmentation techniques and implement semantic filtering to ensure high-quality data. Experiments on ScanQA demonstrate that 3D-MoRe significantly outperforms state-of-the-art baselines, with the CIDEr score improving by 2.15\\%. Similarly, on ScanRefer, our approach achieves a notable increase in CIDEr@0.5 by 1.84\\%, highlighting its effectiveness in both tasks. Our code and generated datasets will be publicly released to benefit the community, and both can be accessed on the this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 39,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12027",
        "abs_url": "https://arxiv.org/abs/2507.12027",
        "pdf_url": "https://arxiv.org/pdf/2507.12027",
        "title": "SGLoc: Semantic Localization System for Camera Pose Estimation from 3D Gaussian Splatting Representation",
        "authors": [
            "Beining Xu",
            "Siting Zhu",
            "Hesheng Wang"
        ],
        "comments": "8 pages, 2 figures, IROS 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
        "abstract": "We propose SGLoc, a novel localization system that directly regresses camera poses from 3D Gaussian Splatting (3DGS) representation by leveraging semantic information. Our method utilizes the semantic relationship between 2D image and 3D scene representation to estimate the 6DoF pose without prior pose information. In this system, we introduce a multi-level pose regression strategy that progressively estimates and refines the pose of query image from the global 3DGS map, without requiring initial pose priors. Moreover, we introduce a semantic-based global retrieval algorithm that establishes correspondences between 2D (image) and 3D (3DGS map). By matching the extracted scene semantic descriptors of 2D query image and 3DGS semantic representation, we align the image with the local region of the global 3DGS map, thereby obtaining a coarse pose estimation. Subsequently, we refine the coarse pose by iteratively optimizing the difference between the query image and the rendered image from 3DGS. Our SGLoc demonstrates superior performance over baselines on 12scenes and 7scenes datasets, showing excellent capabilities in global localization without initial pose prior. Code will be available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 40,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12029",
        "abs_url": "https://arxiv.org/abs/2507.12029",
        "pdf_url": "https://arxiv.org/pdf/2507.12029",
        "title": "Intra-view and Inter-view Correlation Guided Multi-view Novel Class Discovery",
        "authors": [
            "Xinhang Wan",
            "Jiyuan Liu",
            "Qian Qu",
            "Suyuan Liu",
            "Chuyu Zhang",
            "Fangdi Wang",
            "Xinwang Liu",
            "En Zhu",
            "Kunlun He"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "In this paper, we address the problem of novel class discovery (NCD), which aims to cluster novel classes by leveraging knowledge from disjoint known classes. While recent advances have made significant progress in this area, existing NCD methods face two major limitations. First, they primarily focus on single-view data (e.g., images), overlooking the increasingly common multi-view data, such as multi-omics datasets used in disease diagnosis. Second, their reliance on pseudo-labels to supervise novel class clustering often results in unstable performance, as pseudo-label quality is highly sensitive to factors such as data noise and feature dimensionality. To address these challenges, we propose a novel framework named Intra-view and Inter-view Correlation Guided Multi-view Novel Class Discovery (IICMVNCD), which is the first attempt to explore NCD in multi-view setting so far. Specifically, at the intra-view level, leveraging the distributional similarity between known and novel classes, we employ matrix factorization to decompose features into view-specific shared base matrices and factor matrices. The base matrices capture distributional consistency among the two datasets, while the factor matrices model pairwise relationships between samples. At the inter-view level, we utilize view relationships among known classes to guide the clustering of novel classes. This includes generating predicted labels through the weighted fusion of factor matrices and dynamically adjusting view weights of known classes based on the supervision loss, which are then transferred to novel class learning. Experimental results validate the effectiveness of our proposed approach.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 41,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12049",
        "abs_url": "https://arxiv.org/abs/2507.12049",
        "pdf_url": "https://arxiv.org/pdf/2507.12049",
        "title": "MoViAD: Modular Visual Anomaly Detection",
        "authors": [
            "Manuel Barusco",
            "Francesco Borsatti",
            "Arianna Stropeni",
            "Davide Dalle Pezze",
            "Gian Antonio Susto"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "VAD is a critical field in machine learning focused on identifying deviations from normal patterns in images, often challenged by the scarcity of anomalous data and the need for unsupervised training. To accelerate research and deployment in this domain, we introduce MoViAD, a comprehensive and highly modular library designed to provide fast and easy access to state-of-the-art VAD models, trainers, datasets, and VAD utilities. MoViAD supports a wide array of scenarios, including continual, semi-supervised, few-shots, noisy, and many more. In addition, it addresses practical deployment challenges through dedicated Edge and IoT settings, offering optimized models and backbones, along with quantization and compression utilities for efficient on-device execution and distributed inference. MoViAD integrates a selection of backbones, robust evaluation VAD metrics (pixel-level and image-level) and useful profiling tools for efficiency analysis. The library is designed for fast, effortless deployment, enabling machine learning engineers to easily use it for their specific setup with custom models, datasets, and backbones. At the same time, it offers the flexibility and extensibility researchers need to develop and experiment with new methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 42,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12060",
        "abs_url": "https://arxiv.org/abs/2507.12060",
        "pdf_url": "https://arxiv.org/pdf/2507.12060",
        "title": "InstructFLIP: Exploring Unified Vision-Language Model for Face Anti-spoofing",
        "authors": [
            "Kun-Hsiang Lin",
            "Yu-Wen Tseng",
            "Kang-Yang Huang",
            "Jhih-Ciang Wu",
            "Wen-Huang Cheng"
        ],
        "comments": "Accepted by MM'25",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Multimedia (cs.MM)",
        "abstract": "Face anti-spoofing (FAS) aims to construct a robust system that can withstand diverse attacks. While recent efforts have concentrated mainly on cross-domain generalization, two significant challenges persist: limited semantic understanding of attack types and training redundancy across domains. We address the first by integrating vision-language models (VLMs) to enhance the perception of visual input. For the second challenge, we employ a meta-domain strategy to learn a unified model that generalizes well across multiple domains. Our proposed InstructFLIP is a novel instruction-tuned framework that leverages VLMs to enhance generalization via textual guidance trained solely on a single domain. At its core, InstructFLIP explicitly decouples instructions into content and style components, where content-based instructions focus on the essential semantics of spoofing, and style-based instructions consider variations related to the environment and camera characteristics. Extensive experiments demonstrate the effectiveness of InstructFLIP by outperforming SOTA models in accuracy and substantially reducing training redundancy across diverse domains in FAS. Project website is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 43,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12062",
        "abs_url": "https://arxiv.org/abs/2507.12062",
        "pdf_url": "https://arxiv.org/pdf/2507.12062",
        "title": "MS-DETR: Towards Effective Video Moment Retrieval and Highlight Detection by Joint Motion-Semantic Learning",
        "authors": [
            "Hongxu Ma",
            "Guanshuo Wang",
            "Fufu Yu",
            "Qiong Jia",
            "Shouhong Ding"
        ],
        "comments": "Accepted by ACM MM'25",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Video Moment Retrieval (MR) and Highlight Detection (HD) aim to pinpoint specific moments and assess clip-wise relevance based on the text query. While DETR-based joint frameworks have made significant strides, there remains untapped potential in harnessing the intricate relationships between temporal motion and spatial semantics within video content. In this paper, we propose the Motion-Semantics DETR (MS-DETR), a framework that captures rich motion-semantics features through unified learning for MR/HD tasks. The encoder first explicitly models disentangled intra-modal correlations within motion and semantics dimensions, guided by the given text queries. Subsequently, the decoder utilizes the task-wise correlation across temporal motion and spatial semantics dimensions to enable precise query-guided localization for MR and refined highlight boundary delineation for HD. Furthermore, we observe the inherent sparsity dilemma within the motion and semantics dimensions of MR/HD datasets. To address this issue, we enrich the corpus from both dimensions by generation strategies and propose contrastive denoising learning to ensure the above components learn robustly and effectively. Extensive experiments on four MR/HD benchmarks demonstrate that our method outperforms existing state-of-the-art models by a margin. Our code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 44,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12083",
        "abs_url": "https://arxiv.org/abs/2507.12083",
        "pdf_url": "https://arxiv.org/pdf/2507.12083",
        "title": "Foresight in Motion: Reinforcing Trajectory Prediction with Reward Heuristics",
        "authors": [
            "Muleilan Pei",
            "Shaoshuai Shi",
            "Xuesong Chen",
            "Xu Liu",
            "Shaojie Shen"
        ],
        "comments": "Accepted by ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
        "abstract": "Motion forecasting for on-road traffic agents presents both a significant challenge and a critical necessity for ensuring safety in autonomous driving systems. In contrast to most existing data-driven approaches that directly predict future trajectories, we rethink this task from a planning perspective, advocating a \"First Reasoning, Then Forecasting\" strategy that explicitly incorporates behavior intentions as spatial guidance for trajectory prediction. To achieve this, we introduce an interpretable, reward-driven intention reasoner grounded in a novel query-centric Inverse Reinforcement Learning (IRL) scheme. Our method first encodes traffic agents and scene elements into a unified vectorized representation, then aggregates contextual features through a query-centric paradigm. This enables the derivation of a reward distribution, a compact yet informative representation of the target agent's behavior within the given scene context via IRL. Guided by this reward heuristic, we perform policy rollouts to reason about multiple plausible intentions, providing valuable priors for subsequent trajectory generation. Finally, we develop a hierarchical DETR-like decoder integrated with bidirectional selective state space models to produce accurate future trajectories along with their associated probabilities. Extensive experiments on the large-scale Argoverse and nuScenes motion forecasting datasets demonstrate that our approach significantly enhances trajectory prediction confidence, achieving highly competitive performance relative to state-of-the-art methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 45,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12087",
        "abs_url": "https://arxiv.org/abs/2507.12087",
        "pdf_url": "https://arxiv.org/pdf/2507.12087",
        "title": "YOLOv8-SMOT: An Efficient and Robust Framework for Real-Time Small Object Tracking via Slice-Assisted Training and Adaptive Association",
        "authors": [
            "Xiang Yu",
            "Xinyao Liu",
            "Guang Liang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Tracking small, agile multi-objects (SMOT), such as birds, from an Unmanned Aerial Vehicle (UAV) perspective is a highly challenging computer vision task. The difficulty stems from three main sources: the extreme scarcity of target appearance features, the complex motion entanglement caused by the combined dynamics of the camera and the targets themselves, and the frequent occlusions and identity ambiguity arising from dense flocking behavior. This paper details our championship-winning solution in the MVA 2025 \"Finding Birds\" Small Multi-Object Tracking Challenge (SMOT4SB), which adopts the tracking-by-detection paradigm with targeted innovations at both the detection and association levels. On the detection side, we propose a systematic training enhancement framework named \\textbf{SliceTrain}. This framework, through the synergy of 'deterministic full-coverage slicing' and 'slice-level stochastic augmentation, effectively addresses the problem of insufficient learning for small objects in high-resolution image training. On the tracking side, we designed a robust tracker that is completely independent of appearance information. By integrating a \\textbf{motion direction maintenance (EMA)} mechanism and an \\textbf{adaptive similarity metric} combining \\textbf{bounding box expansion and distance penalty} into the OC-SORT framework, our tracker can stably handle irregular motion and maintain target identities. Our method achieves state-of-the-art performance on the SMOT4SB public test set, reaching an SO-HOTA score of \\textbf{55.205}, which fully validates the effectiveness and advancement of our framework in solving complex real-world SMOT problems. The source code will be made available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 46,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12095",
        "abs_url": "https://arxiv.org/abs/2507.12095",
        "pdf_url": "https://arxiv.org/pdf/2507.12095",
        "title": "BRUM: Robust 3D Vehicle Reconstruction from 360 Sparse Images",
        "authors": [
            "Davide Di Nucci",
            "Matteo Tomei",
            "Guido Borghi",
            "Luca Ciuffreda",
            "Roberto Vezzani",
            "Rita Cucchiara"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Accurate 3D reconstruction of vehicles is vital for applications such as vehicle inspection, predictive maintenance, and urban planning. Existing methods like Neural Radiance Fields and Gaussian Splatting have shown impressive results but remain limited by their reliance on dense input views, which hinders real-world applicability. This paper addresses the challenge of reconstructing vehicles from sparse-view inputs, leveraging depth maps and a robust pose estimation architecture to synthesize novel views and augment training data. Specifically, we enhance Gaussian Splatting by integrating a selective photometric loss, applied only to high-confidence pixels, and replacing standard Structure-from-Motion pipelines with the DUSt3R architecture to improve camera pose estimation. Furthermore, we present a novel dataset featuring both synthetic and real-world public transportation vehicles, enabling extensive evaluation of our approach. Experimental results demonstrate state-of-the-art performance across multiple benchmarks, showcasing the method's ability to achieve high-quality reconstructions even under constrained input conditions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 47,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12103",
        "abs_url": "https://arxiv.org/abs/2507.12103",
        "pdf_url": "https://arxiv.org/pdf/2507.12103",
        "title": "DeepShade: Enable Shade Simulation by Text-conditioned Image Generation",
        "authors": [
            "Longchao Da",
            "Xiangrui Liu",
            "Mithun Shivakoti",
            "Thirulogasankar Pranav Kutralingam",
            "Yezhou Yang",
            "Hua Wei"
        ],
        "comments": "7pages, 4 figures. Accepted to IJCAI 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Computers and Society (cs.CY)",
        "abstract": "Heatwaves pose a significant threat to public health, especially as global warming intensifies. However, current routing systems (e.g., online maps) fail to incorporate shade information due to the difficulty of estimating shades directly from noisy satellite imagery and the limited availability of training data for generative models. In this paper, we address these challenges through two main contributions. First, we build an extensive dataset covering diverse longitude-latitude regions, varying levels of building density, and different urban layouts. Leveraging Blender-based 3D simulations alongside building outlines, we capture building shadows under various solar zenith angles throughout the year and at different times of day. These simulated shadows are aligned with satellite images, providing a rich resource for learning shade patterns. Second, we propose the DeepShade, a diffusion-based model designed to learn and synthesize shade variations over time. It emphasizes the nuance of edge features by jointly considering RGB with the Canny edge layer, and incorporates contrastive learning to capture the temporal change rules of shade. Then, by conditioning on textual descriptions of known conditions (e.g., time of day, solar angles), our framework provides improved performance in generating shade images. We demonstrate the utility of our approach by using our shade predictions to calculate shade ratios for real-world route planning in Tempe, Arizona. We believe this work will benefit society by providing a reference for urban planning in extreme heat weather and its potential practical applications in the environment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 48,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12105",
        "abs_url": "https://arxiv.org/abs/2507.12105",
        "pdf_url": "https://arxiv.org/pdf/2507.12105",
        "title": "Out-of-distribution data supervision towards biomedical semantic segmentation",
        "authors": [
            "Yiquan Gao",
            "Duohui Xu"
        ],
        "comments": "This paper was published in Proceedings of SPIE Volume 13442 and is reprinted with permission. The official version is available at this https URL. One personal copy is allowed. Reproduction, distribution, or commercial use is prohibited",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Biomedical segmentation networks easily suffer from the unexpected misclassification between foreground and background objects when learning on limited and imperfect medical datasets. Inspired by the strong power of Out-of-Distribution (OoD) data on other visual tasks, we propose a data-centric framework, Med-OoD to address this issue by introducing OoD data supervision into fully-supervised biomedical segmentation with none of the following needs: (i) external data sources, (ii) feature regularization objectives, (iii) additional annotations. Our method can be seamlessly integrated into segmentation networks without any modification on the architectures. Extensive experiments show that Med-OoD largely prevents various segmentation networks from the pixel misclassification on medical images and achieves considerable performance improvements on Lizard dataset. We also present an emerging learning paradigm of training a medical segmentation network completely using OoD data devoid of foreground class labels, surprisingly turning out 76.1% mIoU as test result. We hope this learning paradigm will attract people to rethink the roles of OoD data. Code is made available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 49,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12107",
        "abs_url": "https://arxiv.org/abs/2507.12107",
        "pdf_url": "https://arxiv.org/pdf/2507.12107",
        "title": "Non-Adaptive Adversarial Face Generation",
        "authors": [
            "Sunpill Kim",
            "Seunghun Paik",
            "Chanwoo Hwang",
            "Minsu Kim",
            "Jae Hong Seo"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)",
        "abstract": "Adversarial attacks on face recognition systems (FRSs) pose serious security and privacy threats, especially when these systems are used for identity verification. In this paper, we propose a novel method for generating adversarial faces-synthetic facial images that are visually distinct yet recognized as a target identity by the FRS. Unlike iterative optimization-based approaches (e.g., gradient descent or other iterative solvers), our method leverages the structural characteristics of the FRS feature space. We figure out that individuals sharing the same attribute (e.g., gender or race) form an attributed subsphere. By utilizing such subspheres, our method achieves both non-adaptiveness and a remarkably small number of queries. This eliminates the need for relying on transferability and open-source surrogate models, which have been a typical strategy when repeated adaptive queries to commercial FRSs are impossible. Despite requiring only a single non-adaptive query consisting of 100 face images, our method achieves a high success rate of over 93% against AWS's CompareFaces API at its default threshold. Furthermore, unlike many existing attacks that perturb a given image, our method can deliberately produce adversarial faces that impersonate the target identity while exhibiting high-level attributes chosen by the adversary.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 50,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12114",
        "abs_url": "https://arxiv.org/abs/2507.12114",
        "pdf_url": "https://arxiv.org/pdf/2507.12114",
        "title": "LidarPainter: One-Step Away From Any Lidar View To Novel Guidance",
        "authors": [
            "Yuzhou Ji",
            "Ke Ma",
            "Hong Cai",
            "Anchun Zhang",
            "Lizhuang Ma",
            "Xin Tan"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Dynamic driving scene reconstruction is of great importance in fields like digital twin system and autonomous driving simulation. However, unacceptable degradation occurs when the view deviates from the input trajectory, leading to corrupted background and vehicle models. To improve reconstruction quality on novel trajectory, existing methods are subject to various limitations including inconsistency, deformation, and time consumption. This paper proposes LidarPainter, a one-step diffusion model that recovers consistent driving views from sparse LiDAR condition and artifact-corrupted renderings in real-time, enabling high-fidelity lane shifts in driving scene reconstruction. Extensive experiments show that LidarPainter outperforms state-of-the-art methods in speed, quality and resource efficiency, specifically 7 x faster than StreetCrafter with only one fifth of GPU memory required. LidarPainter also supports stylized generation using text prompts such as \"foggy\" and \"night\", allowing for a diverse expansion of the existing asset library.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 51,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12123",
        "abs_url": "https://arxiv.org/abs/2507.12123",
        "pdf_url": "https://arxiv.org/pdf/2507.12123",
        "title": "Open-Vocabulary Indoor Object Grounding with 3D Hierarchical Scene Graph",
        "authors": [
            "Sergey Linok",
            "Gleb Naumov"
        ],
        "comments": "13 pages, 5 figures, 2 tables",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We propose OVIGo-3DHSG method - Open-Vocabulary Indoor Grounding of objects using 3D Hierarchical Scene Graph. OVIGo-3DHSG represents an extensive indoor environment over a Hierarchical Scene Graph derived from sequences of RGB-D frames utilizing a set of open-vocabulary foundation models and sensor data processing. The hierarchical representation explicitly models spatial relations across floors, rooms, locations, and objects. To effectively address complex queries involving spatial reference to other objects, we integrate the hierarchical scene graph with a Large Language Model for multistep reasoning. This integration leverages inter-layer (e.g., room-to-object) and intra-layer (e.g., object-to-object) connections, enhancing spatial contextual understanding. We investigate the semantic and geometry accuracy of hierarchical representation on Habitat Matterport 3D Semantic multi-floor scenes. Our approach demonstrates efficient scene comprehension and robust object grounding compared to existing methods. Overall OVIGo-3DHSG demonstrates strong potential for applications requiring spatial reasoning and understanding of indoor environments. Related materials can be found at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 52,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12125",
        "abs_url": "https://arxiv.org/abs/2507.12125",
        "pdf_url": "https://arxiv.org/pdf/2507.12125",
        "title": "Block-based Symmetric Pruning and Fusion for Efficient Vision Transformers",
        "authors": [
            "Yi-Kuan Hsieh",
            "Jun-Wei Hsieh",
            "Xin Li",
            "Yu-Ming Chang",
            "Yu-Chee Tseng"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Vision Transformer (ViT) has achieved impressive results across various vision tasks, yet its high computational cost limits practical applications. Recent methods have aimed to reduce ViT's $O(n^2)$ complexity by pruning unimportant tokens. However, these techniques often sacrifice accuracy by independently pruning query (Q) and key (K) tokens, leading to performance degradation due to overlooked token interactions. To address this limitation, we introduce a novel {\\bf Block-based Symmetric Pruning and Fusion} for efficient ViT (BSPF-ViT) that optimizes the pruning of Q/K tokens jointly. Unlike previous methods that consider only a single direction, our approach evaluates each token and its neighbors to decide which tokens to retain by taking token interaction into account. The retained tokens are compressed through a similarity fusion step, preserving key information while reducing computational costs. The shared weights of Q/K tokens create a symmetric attention matrix, allowing pruning only the upper triangular part for speed up. BSPF-ViT consistently outperforms state-of-the-art ViT methods at all pruning levels, increasing ImageNet classification accuracy by 1.3% on DeiT-T and 2.0% on DeiT-S, while reducing computational overhead by 50%. It achieves 40% speedup with improved accuracy across various ViTs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 53,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12135",
        "abs_url": "https://arxiv.org/abs/2507.12135",
        "pdf_url": "https://arxiv.org/pdf/2507.12135",
        "title": "Learning Pixel-adaptive Multi-layer Perceptrons for Real-time Image Enhancement",
        "authors": [
            "Junyu Lou",
            "Xiaorui Zhao",
            "Kexuan Shi",
            "Shuhang Gu"
        ],
        "comments": "Accepted to ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Deep learning-based bilateral grid processing has emerged as a promising solution for image enhancement, inherently encoding spatial and intensity information while enabling efficient full-resolution processing through slicing operations. However, existing approaches are limited to linear affine transformations, hindering their ability to model complex color relationships. Meanwhile, while multi-layer perceptrons (MLPs) excel at non-linear mappings, traditional MLP-based methods employ globally shared parameters, which is hard to deal with localized variations. To overcome these dual challenges, we propose a Bilateral Grid-based Pixel-Adaptive Multi-layer Perceptron (BPAM) framework. Our approach synergizes the spatial modeling of bilateral grids with the non-linear capabilities of MLPs. Specifically, we generate bilateral grids containing MLP parameters, where each pixel dynamically retrieves its unique transformation parameters and obtain a distinct MLP for color mapping based on spatial coordinates and intensity values. In addition, we propose a novel grid decomposition strategy that categorizes MLP parameters into distinct types stored in separate subgrids. Multi-channel guidance maps are used to extract category-specific parameters from corresponding subgrids, ensuring effective utilization of color information during slicing while guiding precise parameter generation. Extensive experiments on public datasets demonstrate that our method outperforms state-of-the-art methods in performance while maintaining real-time processing capabilities.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 54,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12137",
        "abs_url": "https://arxiv.org/abs/2507.12137",
        "pdf_url": "https://arxiv.org/pdf/2507.12137",
        "title": "AD-GS: Object-Aware B-Spline Gaussian Splatting for Self-Supervised Autonomous Driving",
        "authors": [
            "Jiawei Xu",
            "Kai Deng",
            "Zexin Fan",
            "Shenlong Wang",
            "Jin Xie",
            "Jian Yang"
        ],
        "comments": "Accepted by ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Modeling and rendering dynamic urban driving scenes is crucial for self-driving simulation. Current high-quality methods typically rely on costly manual object tracklet annotations, while self-supervised approaches fail to capture dynamic object motions accurately and decompose scenes properly, resulting in rendering artifacts. We introduce AD-GS, a novel self-supervised framework for high-quality free-viewpoint rendering of driving scenes from a single log. At its core is a novel learnable motion model that integrates locality-aware B-spline curves with global-aware trigonometric functions, enabling flexible yet precise dynamic object modeling. Rather than requiring comprehensive semantic labeling, AD-GS automatically segments scenes into objects and background with the simplified pseudo 2D segmentation, representing objects using dynamic Gaussians and bidirectional temporal visibility masks. Further, our model incorporates visibility reasoning and physically rigid regularization to enhance robustness. Extensive evaluations demonstrate that our annotation-free model significantly outperforms current state-of-the-art annotation-free methods and is competitive with annotation-dependent approaches.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 55,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12138",
        "abs_url": "https://arxiv.org/abs/2507.12138",
        "pdf_url": "https://arxiv.org/pdf/2507.12138",
        "title": "Neural Human Pose Prior",
        "authors": [
            "Michal Heker",
            "Sefy Kararlitsky",
            "David Tolpin"
        ],
        "comments": "Work in progress",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "We introduce a principled, data-driven approach for modeling a neural prior over human body poses using normalizing flows. Unlike heuristic or low-expressivity alternatives, our method leverages RealNVP to learn a flexible density over poses represented in the 6D rotation format. We address the challenge of modeling distributions on the manifold of valid 6D rotations by inverting the Gram-Schmidt process during training, enabling stable learning while preserving downstream compatibility with rotation-based frameworks. Our architecture and training pipeline are framework-agnostic and easily reproducible. We demonstrate the effectiveness of the learned prior through both qualitative and quantitative evaluations, and we analyze its impact via ablation studies. This work provides a sound probabilistic foundation for integrating pose priors into human motion capture and reconstruction pipelines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 56,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12157",
        "abs_url": "https://arxiv.org/abs/2507.12157",
        "pdf_url": "https://arxiv.org/pdf/2507.12157",
        "title": "Fine-Grained Image Recognition from Scratch with Teacher-Guided Data Augmentation",
        "authors": [
            "Edwin Arkel Rios",
            "Fernando Mikael",
            "Oswin Gosal",
            "Femiloye Oyerinde",
            "Hao-Chun Liang",
            "Bo-Cheng Lai",
            "Min-Chun Hu"
        ],
        "comments": "Main: 10 pages, 2 figures, 4 tables",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Fine-grained image recognition (FGIR) aims to distinguish visually similar sub-categories within a broader class, such as identifying bird species. While most existing FGIR methods rely on backbones pretrained on large-scale datasets like ImageNet, this dependence limits adaptability to resource-constrained environments and hinders the development of task-specific architectures tailored to the unique challenges of FGIR. In this work, we challenge the conventional reliance on pretrained models by demonstrating that high-performance FGIR systems can be trained entirely from scratch. We introduce a novel training framework, TGDA, that integrates data-aware augmentation with weak supervision via a fine-grained-aware teacher model, implemented through knowledge distillation. This framework unlocks the design of task-specific and hardware-aware architectures, including LRNets for low-resolution FGIR and ViTFS, a family of Vision Transformers optimized for efficient inference. Extensive experiments across three FGIR benchmarks over diverse settings involving low-resolution and high-resolution inputs show that our method consistently matches or surpasses state-of-the-art pretrained counterparts. In particular, in the low-resolution setting, LRNets trained with TGDA improve accuracy by up to 23\\% over prior methods while requiring up to 20.6x less parameters, lower FLOPs, and significantly less training data. Similarly, ViTFS-T can match the performance of a ViT B-16 pretrained on ImageNet-21k while using 15.3x fewer trainable parameters and requiring orders of magnitudes less data. These results highlight TGDA's potential as an adaptable alternative to pretraining, paving the way for more efficient fine-grained vision systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 57,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12177",
        "abs_url": "https://arxiv.org/abs/2507.12177",
        "pdf_url": "https://arxiv.org/pdf/2507.12177",
        "title": "Hybrid Ensemble Approaches: Optimal Deep Feature Fusion and Hyperparameter-Tuned Classifier Ensembling for Enhanced Brain Tumor Classification",
        "authors": [
            "Zahid Ullah",
            "Dragan Pamucar",
            "Jihie Kim"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Magnetic Resonance Imaging (MRI) is widely recognized as the most reliable tool for detecting tumors due to its capability to produce detailed images that reveal their presence. However, the accuracy of diagnosis can be compromised when human specialists evaluate these images. Factors such as fatigue, limited expertise, and insufficient image detail can lead to errors. For example, small tumors might go unnoticed, or overlap with healthy brain regions could result in misidentification. To address these challenges and enhance diagnostic precision, this study proposes a novel double ensembling framework, consisting of ensembled pre-trained deep learning (DL) models for feature extraction and ensembled fine-tuned hyperparameter machine learning (ML) models to efficiently classify brain tumors. Specifically, our method includes extensive preprocessing and augmentation, transfer learning concepts by utilizing various pre-trained deep convolutional neural networks and vision transformer networks to extract deep features from brain MRI, and fine-tune hyperparameters of ML classifiers. Our experiments utilized three different publicly available Kaggle MRI brain tumor datasets to evaluate the pre-trained DL feature extractor models, ML classifiers, and the effectiveness of an ensemble of deep features along with an ensemble of ML classifiers for brain tumor classification. Our results indicate that the proposed feature fusion and classifier fusion improve upon the state of the art, with hyperparameter fine-tuning providing a significant enhancement over the ensemble method. Additionally, we present an ablation study to illustrate how each component contributes to accurate brain tumor classification.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 58,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12188",
        "abs_url": "https://arxiv.org/abs/2507.12188",
        "pdf_url": "https://arxiv.org/pdf/2507.12188",
        "title": "Wavelet-based Decoupling Framework for low-light Stereo Image Enhancement",
        "authors": [
            "Shuangli Du",
            "Siming Yan",
            "Zhenghao Shi",
            "Zhenzhen You",
            "Lu Sun"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Image and Video Processing (eess.IV)",
        "abstract": "Low-light images suffer from complex degradation, and existing enhancement methods often encode all degradation factors within a single latent space. This leads to highly entangled features and strong black-box characteristics, making the model prone to shortcut learning. To mitigate the above issues, this paper proposes a wavelet-based low-light stereo image enhancement method with feature space decoupling. Our insight comes from the following findings: (1) Wavelet transform enables the independent processing of low-frequency and high-frequency information. (2) Illumination adjustment can be achieved by adjusting the low-frequency component of a low-light image, extracted through multi-level wavelet decomposition. Thus, by using wavelet transform the feature space is decomposed into a low-frequency branch for illumination adjustment and multiple high-frequency branches for texture enhancement. Additionally, stereo low-light image enhancement can extract useful cues from another view to improve enhancement. To this end, we propose a novel high-frequency guided cross-view interaction module (HF-CIM) that operates within high-frequency branches rather than across the entire feature space, effectively extracting valuable image details from the other view. Furthermore, to enhance the high-frequency information, a detail and texture enhancement module (DTEM) is proposed based on cross-attention mechanism. The model is trained on a dataset consisting of images with uniform illumination and images with non-uniform illumination. Experimental results on both real and synthetic images indicate that our algorithm offers significant advantages in light adjustment while effectively recovering high-frequency information. The code and dataset are publicly available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 59,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12195",
        "abs_url": "https://arxiv.org/abs/2507.12195",
        "pdf_url": "https://arxiv.org/pdf/2507.12195",
        "title": "Revealing the Ancient Beauty: Digital Reconstruction of Temple Tiles using Computer Vision",
        "authors": [
            "Arkaprabha Basu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Modern digitised approaches have dramatically changed the preservation and restoration of cultural treasures, integrating computer scientists into multidisciplinary projects with ease. Machine learning, deep learning, and computer vision techniques have revolutionised developing sectors like 3D reconstruction, picture inpainting,IoT-based methods, genetic algorithms, and image processing with the integration of computer scientists into multidisciplinary initiatives. We suggest three cutting-edge techniques in recognition of the special qualities of Indian monuments, which are famous for their architectural skill and aesthetic appeal. First is the Fractal Convolution methodology, a segmentation method based on image processing that successfully reveals subtle architectural patterns within these irreplaceable cultural buildings. The second is a revolutionary Self-Sensitive Tile Filling (SSTF) method created especially for West Bengal's mesmerising Bankura Terracotta Temples with a brand-new data augmentation method called MosaicSlice on the third. Furthermore, we delve deeper into the Super Resolution strategy to upscale the images without losing significant amount of quality. Our methods allow for the development of seamless region-filling and highly detailed tiles while maintaining authenticity using a novel data augmentation strategy within affordable costs introducing automation. By providing effective solutions that preserve the delicate balance between tradition and innovation, this study improves the subject and eventually ensures unrivalled efficiency and aesthetic excellence in cultural heritage protection. The suggested approaches advance the field into an era of unmatched efficiency and aesthetic quality while carefully upholding the delicate equilibrium between tradition and innovation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 60,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12201",
        "abs_url": "https://arxiv.org/abs/2507.12201",
        "pdf_url": "https://arxiv.org/pdf/2507.12201",
        "title": "RODS: Robust Optimization Inspired Diffusion Sampling for Detecting and Reducing Hallucination in Generative Models",
        "authors": [
            "Yiqi Tian",
            "Pengfei Jin",
            "Mingze Yuan",
            "Na Li",
            "Bo Zeng",
            "Quanzheng Li"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Optimization and Control (math.OC)",
        "abstract": "Diffusion models have achieved state-of-the-art performance in generative modeling, yet their sampling procedures remain vulnerable to hallucinations, often stemming from inaccuracies in score approximation. In this work, we reinterpret diffusion sampling through the lens of optimization and introduce RODS (Robust Optimization-inspired Diffusion Sampler), a novel method that detects and corrects high-risk sampling steps using geometric cues from the loss landscape. RODS enforces smoother sampling trajectories and adaptively adjusts perturbations, reducing hallucinations without retraining and at minimal additional inference cost. Experiments on AFHQv2, FFHQ, and 11k-hands demonstrate that RODS improves both sampling fidelity and robustness, detecting over 70% of hallucinated samples and correcting more than 25%, all while avoiding the introduction of new artifacts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 61,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12232",
        "abs_url": "https://arxiv.org/abs/2507.12232",
        "pdf_url": "https://arxiv.org/pdf/2507.12232",
        "title": "MGFFD-VLM: Multi-Granularity Prompt Learning for Face Forgery Detection with VLM",
        "authors": [
            "Tao Chen",
            "Jingyi Zhang",
            "Decheng Liu",
            "Chunlei Peng"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Recent studies have utilized visual large language models (VLMs) to answer not only \"Is this face a forgery?\" but also \"Why is the face a forgery?\" These studies introduced forgery-related attributes, such as forgery location and type, to construct deepfake VQA datasets and train VLMs, achieving high accuracy while providing human-understandable explanatory text descriptions. However, these methods still have limitations. For example, they do not fully leverage face quality-related attributes, which are often abnormal in forged faces, and they lack effective training strategies for forgery-aware VLMs. In this paper, we extend the VQA dataset to create DD-VQA+, which features a richer set of attributes and a more diverse range of samples. Furthermore, we introduce a novel forgery detection framework, MGFFD-VLM, which integrates an Attribute-Driven Hybrid LoRA Strategy to enhance the capabilities of Visual Large Language Models (VLMs). Additionally, our framework incorporates Multi-Granularity Prompt Learning and a Forgery-Aware Training Strategy. By transforming classification and forgery segmentation results into prompts, our method not only improves forgery classification but also enhances interpretability. To further boost detection performance, we design multiple forgery-related auxiliary losses. Experimental results demonstrate that our approach surpasses existing methods in both text-based forgery judgment and analysis, achieving superior accuracy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 62,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12236",
        "abs_url": "https://arxiv.org/abs/2507.12236",
        "pdf_url": "https://arxiv.org/pdf/2507.12236",
        "title": "Generate to Ground: Multimodal Text Conditioning Boosts Phrase Grounding in Medical Vision-Language Models",
        "authors": [
            "Felix Nützel",
            "Mischa Dombrowski",
            "Bernhard Kainz"
        ],
        "comments": "20 pages, 6 figures. To appear in Proc. MIDL 2025 (PMLR)",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Phrase grounding, i.e., mapping natural language phrases to specific image regions, holds significant potential for disease localization in medical imaging through clinical reports. While current state-of-the-art methods rely on discriminative, self-supervised contrastive models, we demonstrate that generative text-to-image diffusion models, leveraging cross-attention maps, can achieve superior zero-shot phrase grounding performance. Contrary to prior assumptions, we show that fine-tuning diffusion models with a frozen, domain-specific language model, such as CXR-BERT, substantially outperforms domain-agnostic counterparts. This setup achieves remarkable improvements, with mIoU scores doubling those of current discriminative methods. These findings highlight the underexplored potential of generative models for phrase grounding tasks. To further enhance performance, we introduce Bimodal Bias Merging (BBM), a novel post-processing technique that aligns text and image biases to identify regions of high certainty. BBM refines cross-attention maps, achieving even greater localization accuracy. Our results establish generative approaches as a more effective paradigm for phrase grounding in the medical imaging domain, paving the way for more robust and interpretable applications in clinical practice. The source code and model weights are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 63,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12245",
        "abs_url": "https://arxiv.org/abs/2507.12245",
        "pdf_url": "https://arxiv.org/pdf/2507.12245",
        "title": "Calisthenics Skills Temporal Video Segmentation",
        "authors": [
            "Antonio Finocchiaro",
            "Giovanni Maria Farinella",
            "Antonino Furnari"
        ],
        "comments": "9 pages, 6 figures, In Proceedings of the 19th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications - Volume 2",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Calisthenics is a fast-growing bodyweight discipline that consists of different categories, one of which is focused on skills. Skills in calisthenics encompass both static and dynamic elements performed by athletes. The evaluation of static skills is based on their difficulty level and the duration of the hold. Automated tools able to recognize isometric skills from a video by segmenting them to estimate their duration would be desirable to assist athletes in their training and judges during competitions. Although the video understanding literature on action recognition through body pose analysis is rich, no previous work has specifically addressed the problem of calisthenics skill temporal video segmentation. This study aims to provide an initial step towards the implementation of automated tools within the field of Calisthenics. To advance knowledge in this context, we propose a dataset of video footage of static calisthenics skills performed by athletes. Each video is annotated with a temporal segmentation which determines the extent of each skill. We hence report the results of a baseline approach to address the problem of skill temporal segmentation on the proposed dataset. The results highlight the feasibility of the proposed problem, while there is still room for improvement.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 64,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12248",
        "abs_url": "https://arxiv.org/abs/2507.12248",
        "pdf_url": "https://arxiv.org/pdf/2507.12248",
        "title": "Comparative Analysis of CNN Performance in Keras, PyTorch and JAX on PathMNIST",
        "authors": [
            "Anida Nezović",
            "Jalal Romano",
            "Nada Marić",
            "Medina Kapo",
            "Amila Akagić"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Deep learning has significantly advanced the field of medical image classification, particularly with the adoption of Convolutional Neural Networks (CNNs). Various deep learning frameworks such as Keras, PyTorch and JAX offer unique advantages in model development and deployment. However, their comparative performance in medical imaging tasks remains underexplored. This study presents a comprehensive analysis of CNN implementations across these frameworks, using the PathMNIST dataset as a benchmark. We evaluate training efficiency, classification accuracy and inference speed to assess their suitability for real-world applications. Our findings highlight the trade-offs between computational speed and model accuracy, offering valuable insights for researchers and practitioners in medical image analysis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 65,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12269",
        "abs_url": "https://arxiv.org/abs/2507.12269",
        "pdf_url": "https://arxiv.org/pdf/2507.12269",
        "title": "Site-Level Fine-Tuning with Progressive Layer Freezing: Towards Robust Prediction of Bronchopulmonary Dysplasia from Day-1 Chest Radiographs in Extremely Preterm Infants",
        "authors": [
            "Sybelle Goedicke-Fritz",
            "Michelle Bous",
            "Annika Engel",
            "Matthias Flotho",
            "Pascal Hirsch",
            "Hannah Wittig",
            "Dino Milanovic",
            "Dominik Mohr",
            "Mathias Kaspar",
            "Sogand Nemat",
            "Dorothea Kerner",
            "Arno Bücker",
            "Andreas Keller",
            "Sascha Meyer",
            "Michael Zemlin",
            "Philipp Flotho"
        ],
        "comments": "S.G.-F., M.B., and A.E. contributed equally to this work and share first authorship. M.Z. and P.F. contributed equally to this work and share senior authorship",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Bronchopulmonary dysplasia (BPD) is a chronic lung disease affecting 35% of extremely low birth weight infants. Defined by oxygen dependence at 36 weeks postmenstrual age, it causes lifelong respiratory complications. However, preventive interventions carry severe risks, including neurodevelopmental impairment, ventilator-induced lung injury, and systemic complications. Therefore, early BPD prognosis and prediction of BPD outcome is crucial to avoid unnecessary toxicity in low risk infants. Admission radiographs of extremely preterm infants are routinely acquired within 24h of life and could serve as a non-invasive prognostic tool. In this work, we developed and investigated a deep learning approach using chest X-rays from 163 extremely low-birth-weight infants ($\\leq$32 weeks gestation, 401-999g) obtained within 24 hours of birth. We fine-tuned a ResNet-50 pretrained specifically on adult chest radiographs, employing progressive layer freezing with discriminative learning rates to prevent overfitting and evaluated a CutMix augmentation and linear probing. For moderate/severe BPD outcome prediction, our best performing model with progressive freezing, linear probing and CutMix achieved an AUROC of 0.78 $\\pm$ 0.10, balanced accuracy of 0.69 $\\pm$ 0.10, and an F1-score of 0.67 $\\pm$ 0.11. In-domain pre-training significantly outperformed ImageNet initialization (p = 0.031) which confirms domain-specific pretraining to be important for BPD outcome prediction. Routine IRDS grades showed limited prognostic value (AUROC 0.57 $\\pm$ 0.11), confirming the need of learned markers. Our approach demonstrates that domain-specific pretraining enables accurate BPD prediction from routine day-1 radiographs. Through progressive freezing and linear probing, the method remains computationally feasible for site-level implementation and future federated learning deployments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 66,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12283",
        "abs_url": "https://arxiv.org/abs/2507.12283",
        "pdf_url": "https://arxiv.org/pdf/2507.12283",
        "title": "FADE: Adversarial Concept Erasure in Flow Models",
        "authors": [
            "Zixuan Fu",
            "Yan Ren",
            "Finn Carter",
            "Chenyue Wang",
            "Ze Niu",
            "Dacheng Yu",
            "Emily Davis",
            "Bo Zhang"
        ],
        "comments": "Camera Ready",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Diffusion models have demonstrated remarkable image generation capabilities, but also pose risks in privacy and fairness by memorizing sensitive concepts or perpetuating biases. We propose a novel \\textbf{concept erasure} method for text-to-image diffusion models, designed to remove specified concepts (e.g., a private individual or a harmful stereotype) from the model's generative repertoire. Our method, termed \\textbf{FADE} (Fair Adversarial Diffusion Erasure), combines a trajectory-aware fine-tuning strategy with an adversarial objective to ensure the concept is reliably removed while preserving overall model fidelity. Theoretically, we prove a formal guarantee that our approach minimizes the mutual information between the erased concept and the model's outputs, ensuring privacy and fairness. Empirically, we evaluate FADE on Stable Diffusion and FLUX, using benchmarks from prior work (e.g., object, celebrity, explicit content, and style erasure tasks from MACE). FADE achieves state-of-the-art concept removal performance, surpassing recent baselines like ESD, UCE, MACE, and ANT in terms of removal efficacy and image quality. Notably, FADE improves the harmonic mean of concept removal and fidelity by 5--10\\% over the best prior method. We also conduct an ablation study to validate each component of FADE, confirming that our adversarial and trajectory-preserving objectives each contribute to its superior performance. Our work sets a new standard for safe and fair generative modeling by unlearning specified concepts without retraining from scratch.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 67,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12292",
        "abs_url": "https://arxiv.org/abs/2507.12292",
        "pdf_url": "https://arxiv.org/pdf/2507.12292",
        "title": "Efficient Calisthenics Skills Classification through Foreground Instance Selection and Depth Estimation",
        "authors": [
            "Antonio Finocchiaro",
            "Giovanni Maria Farinella",
            "Antonino Furnari"
        ],
        "comments": "13 pages, 4 figures, In International Conference on Image Analysis and Processing",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Calisthenics skill classification is the computer vision task of inferring the skill performed by an athlete from images, enabling automatic performance assessment and personalized analytics. Traditional methods for calisthenics skill recognition are based on pose estimation methods to determine the position of skeletal data from images, which is later fed to a classification algorithm to infer the performed skill. Despite the progress in human pose estimation algorithms, they still involve high computational costs, long inference times, and complex setups, which limit the applicability of such approaches in real-time applications or mobile devices. This work proposes a direct approach to calisthenics skill recognition, which leverages depth estimation and athlete patch retrieval to avoid the computationally expensive human pose estimation module. Using Depth Anything V2 for depth estimation and YOLOv10 for athlete localization, we segment the subject from the background rather than relying on traditional pose estimation techniques. This strategy increases efficiency, reduces inference time, and improves classification accuracy. Our approach significantly outperforms skeleton-based methods, achieving 38.3x faster inference with RGB image patches and improved classification accuracy with depth patches (0.837 vs. 0.815). Beyond these performance gains, the modular design of our pipeline allows for flexible replacement of components, enabling future enhancements and adaptation to real-world applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 68,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12318",
        "abs_url": "https://arxiv.org/abs/2507.12318",
        "pdf_url": "https://arxiv.org/pdf/2507.12318",
        "title": "Compositional Discrete Latent Code for High Fidelity, Productive Diffusion Models",
        "authors": [
            "Samuel Lavoie",
            "Michael Noukhovitch",
            "Aaron Courville"
        ],
        "comments": "In submission, 22 pages, 7 tables, 12 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "We argue that diffusion models' success in modeling complex distributions is, for the most part, coming from their input conditioning. This paper investigates the representation used to condition diffusion models from the perspective that ideal representations should improve sample fidelity, be easy to generate, and be compositional to allow out-of-training samples generation. We introduce Discrete Latent Code (DLC), an image representation derived from Simplicial Embeddings trained with a self-supervised learning objective. DLCs are sequences of discrete tokens, as opposed to the standard continuous image embeddings. They are easy to generate and their compositionality enables sampling of novel images beyond the training distribution. Diffusion models trained with DLCs have improved generation fidelity, establishing a new state-of-the-art for unconditional image generation on ImageNet. Additionally, we show that composing DLCs allows the image generator to produce out-of-distribution samples that coherently combine the semantics of images in diverse ways. Finally, we showcase how DLCs can enable text-to-image generation by leveraging large-scale pretrained language models. We efficiently finetune a text diffusion language model to generate DLCs that produce novel samples outside of the image generator training distribution.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 69,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12336",
        "abs_url": "https://arxiv.org/abs/2507.12336",
        "pdf_url": "https://arxiv.org/pdf/2507.12336",
        "title": "Unsupervised Monocular 3D Keypoint Discovery from Multi-View Diffusion Priors",
        "authors": [
            "Subin Jeon",
            "In Cho",
            "Junyoung Hong",
            "Seon Joo Kim"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "This paper introduces KeyDiff3D, a framework for unsupervised monocular 3D keypoints estimation that accurately predicts 3D keypoints from a single image. While previous methods rely on manual annotations or calibrated multi-view images, both of which are expensive to collect, our method enables monocular 3D keypoints estimation using only a collection of single-view images. To achieve this, we leverage powerful geometric priors embedded in a pretrained multi-view diffusion model. In our framework, this model generates multi-view images from a single image, serving as a supervision signal to provide 3D geometric cues to our model. We also use the diffusion model as a powerful 2D multi-view feature extractor and construct 3D feature volumes from its intermediate representations. This transforms implicit 3D priors learned by the diffusion model into explicit 3D features. Beyond accurate keypoints estimation, we further introduce a pipeline that enables manipulation of 3D objects generated by the diffusion model. Experimental results on diverse aspects and datasets, including Human3.6M, Stanford Dogs, and several in-the-wild and out-of-domain datasets, highlight the effectiveness of our method in terms of accuracy, generalization, and its ability to enable manipulation of 3D objects generated by the diffusion model from a single image.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 70,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12344",
        "abs_url": "https://arxiv.org/abs/2507.12344",
        "pdf_url": "https://arxiv.org/pdf/2507.12344",
        "title": "Improving Lightweight Weed Detection via Knowledge Distillation",
        "authors": [
            "Ahmet Oğuz Saltık",
            "Max Voigt",
            "Sourav Modak",
            "Mike Beckworth",
            "Anthony Stein"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Weed detection is a critical component of precision agriculture, facilitating targeted herbicide application and reducing environmental impact. However, deploying accurate object detection models on resource-limited platforms remains challenging, particularly when differentiating visually similar weed species commonly encountered in plant phenotyping applications. In this work, we investigate Channel-wise Knowledge Distillation (CWD) and Masked Generative Distillation (MGD) to enhance the performance of lightweight models for real-time smart spraying systems. Utilizing YOLO11x as the teacher model and YOLO11n as both reference and student, both CWD and MGD effectively transfer knowledge from the teacher to the student model. Our experiments, conducted on a real-world dataset comprising sugar beet crops and four weed types (Cirsium, Convolvulus, Fallopia, and Echinochloa), consistently show increased AP50 across all classes. The distilled CWD student model achieves a notable improvement of 2.5% and MGD achieves 1.9% in mAP50 over the baseline without increasing model complexity. Additionally, we validate real-time deployment feasibility by evaluating the student YOLO11n model on Jetson Orin Nano and Raspberry Pi 5 embedded devices, performing five independent runs to evaluate performance stability across random seeds. These findings confirm CWD and MGD as an effective, efficient, and practical approach for improving deep learning-based weed detection accuracy in precision agriculture and plant phenotyping scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 71,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12359",
        "abs_url": "https://arxiv.org/abs/2507.12359",
        "pdf_url": "https://arxiv.org/pdf/2507.12359",
        "title": "Cluster Contrast for Unsupervised Visual Representation Learning",
        "authors": [
            "Nikolaos Giakoumoglou",
            "Tania Stathaki"
        ],
        "comments": "ICIP 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "We introduce Cluster Contrast (CueCo), a novel approach to unsupervised visual representation learning that effectively combines the strengths of contrastive learning and clustering methods. Inspired by recent advancements, CueCo is designed to simultaneously scatter and align feature representations within the feature space. This method utilizes two neural networks, a query and a key, where the key network is updated through a slow-moving average of the query outputs. CueCo employs a contrastive loss to push dissimilar features apart, enhancing inter-class separation, and a clustering objective to pull together features of the same cluster, promoting intra-class compactness. Our method achieves 91.40% top-1 classification accuracy on CIFAR-10, 68.56% on CIFAR-100, and 78.65% on ImageNet-100 using linear evaluation with a ResNet-18 backbone. By integrating contrastive learning with clustering, CueCo sets a new direction for advancing unsupervised visual representation learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 72,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12382",
        "abs_url": "https://arxiv.org/abs/2507.12382",
        "pdf_url": "https://arxiv.org/pdf/2507.12382",
        "title": "Text-driven Multiplanar Visual Interaction for Semi-supervised Medical Image Segmentation",
        "authors": [
            "Kaiwen Huang",
            "Yi Zhou",
            "Huazhu Fu",
            "Yizhe Zhang",
            "Chen Gong",
            "Tao Zhou"
        ],
        "comments": "10 pages; 2 figures; Have been accepted by MICCAI 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Semi-supervised medical image segmentation is a crucial technique for alleviating the high cost of data annotation. When labeled data is limited, textual information can provide additional context to enhance visual semantic understanding. However, research exploring the use of textual data to enhance visual semantic embeddings in 3D medical imaging tasks remains scarce. In this paper, we propose a novel text-driven multiplanar visual interaction framework for semi-supervised medical image segmentation (termed Text-SemiSeg), which consists of three main modules: Text-enhanced Multiplanar Representation (TMR), Category-aware Semantic Alignment (CSA), and Dynamic Cognitive Augmentation (DCA). Specifically, TMR facilitates text-visual interaction through planar mapping, thereby enhancing the category awareness of visual features. CSA performs cross-modal semantic alignment between the text features with introduced learnable variables and the intermediate layer of visual features. DCA reduces the distribution discrepancy between labeled and unlabeled data through their interaction, thus improving the model's robustness. Finally, experiments on three public datasets demonstrate that our model effectively enhances visual features with textual information and outperforms other methods. Our code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 73,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12396",
        "abs_url": "https://arxiv.org/abs/2507.12396",
        "pdf_url": "https://arxiv.org/pdf/2507.12396",
        "title": "OD-VIRAT: A Large-Scale Benchmark for Object Detection in Realistic Surveillance Environments",
        "authors": [
            "Hayat Ullah",
            "Abbas Khan",
            "Arslan Munir",
            "Hari Kalva"
        ],
        "comments": "14 pages",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Realistic human surveillance datasets are crucial for training and evaluating computer vision models under real-world conditions, facilitating the development of robust algorithms for human and human-interacting object detection in complex environments. These datasets need to offer diverse and challenging data to enable a comprehensive assessment of model performance and the creation of more reliable surveillance systems for public safety. To this end, we present two visual object detection benchmarks named OD-VIRAT Large and OD-VIRAT Tiny, aiming at advancing visual understanding tasks in surveillance imagery. The video sequences in both benchmarks cover 10 different scenes of human surveillance recorded from significant height and distance. The proposed benchmarks offer rich annotations of bounding boxes and categories, where OD-VIRAT Large has 8.7 million annotated instances in 599,996 images and OD-VIRAT Tiny has 288,901 annotated instances in 19,860 images. This work also focuses on benchmarking state-of-the-art object detection architectures, including RETMDET, YOLOX, RetinaNet, DETR, and Deformable-DETR on this object detection-specific variant of VIRAT dataset. To the best of our knowledge, it is the first work to examine the performance of these recently published state-of-the-art object detection architectures on realistic surveillance imagery under challenging conditions such as complex backgrounds, occluded objects, and small-scale objects. The proposed benchmarking and experimental settings will help in providing insights concerning the performance of selected object detection models and set the base for developing more efficient and robust object detection architectures.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 74,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12414",
        "abs_url": "https://arxiv.org/abs/2507.12414",
        "pdf_url": "https://arxiv.org/pdf/2507.12414",
        "title": "AutoVDC: Automated Vision Data Cleaning Using Vision-Language Models",
        "authors": [
            "Santosh Vasa",
            "Aditi Ramadwar",
            "Jnana Rama Krishna Darabattula",
            "Md Zafar Anwar",
            "Stanislaw Antol",
            "Andrei Vatavu",
            "Thomas Monninger",
            "Sihao Ding"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)",
        "abstract": "Training of autonomous driving systems requires extensive datasets with precise annotations to attain robust performance. Human annotations suffer from imperfections, and multiple iterations are often needed to produce high-quality datasets. However, manually reviewing large datasets is laborious and expensive. In this paper, we introduce AutoVDC (Automated Vision Data Cleaning) framework and investigate the utilization of Vision-Language Models (VLMs) to automatically identify erroneous annotations in vision datasets, thereby enabling users to eliminate these errors and enhance data quality. We validate our approach using the KITTI and nuImages datasets, which contain object detection benchmarks for autonomous driving. To test the effectiveness of AutoVDC, we create dataset variants with intentionally injected erroneous annotations and observe the error detection rate of our approach. Additionally, we compare the detection rates using different VLMs and explore the impact of VLM fine-tuning on our pipeline. The results demonstrate our method's high performance in error detection and data cleaning experiments, indicating its potential to significantly improve the reliability and accuracy of large-scale production datasets in autonomous driving.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 75,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12416",
        "abs_url": "https://arxiv.org/abs/2507.12416",
        "pdf_url": "https://arxiv.org/pdf/2507.12416",
        "title": "QuRe: Query-Relevant Retrieval through Hard Negative Sampling in Composed Image Retrieval",
        "authors": [
            "Jaehyun Kwak",
            "Ramahdani Muhammad Izaaz Inhar",
            "Se-Young Yun",
            "Sung-Ju Lee"
        ],
        "comments": "Accepted to ICML 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Composed Image Retrieval (CIR) retrieves relevant images based on a reference image and accompanying text describing desired modifications. However, existing CIR methods only focus on retrieving the target image and disregard the relevance of other images. This limitation arises because most methods employing contrastive learning-which treats the target image as positive and all other images in the batch as negatives-can inadvertently include false negatives. This may result in retrieving irrelevant images, reducing user satisfaction even when the target image is retrieved. To address this issue, we propose Query-Relevant Retrieval through Hard Negative Sampling (QuRe), which optimizes a reward model objective to reduce false negatives. Additionally, we introduce a hard negative sampling strategy that selects images positioned between two steep drops in relevance scores following the target image, to effectively filter false negatives. In order to evaluate CIR models on their alignment with human satisfaction, we create Human-Preference FashionIQ (HP-FashionIQ), a new dataset that explicitly captures user preferences beyond target retrieval. Extensive experiments demonstrate that QuRe achieves state-of-the-art performance on FashionIQ and CIRR datasets while exhibiting the strongest alignment with human preferences on the HP-FashionIQ dataset. The source code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 76,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12420",
        "abs_url": "https://arxiv.org/abs/2507.12420",
        "pdf_url": "https://arxiv.org/pdf/2507.12420",
        "title": "InterpIoU: Rethinking Bounding Box Regression with Interpolation-Based IoU Optimization",
        "authors": [
            "Haoyuan Liu",
            "Hiroshi Watanabe"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Bounding box regression (BBR) is fundamental to object detection, where the regression loss is crucial for accurate localization. Existing IoU-based losses often incorporate handcrafted geometric penalties to address IoU's non-differentiability in non-overlapping cases and enhance BBR performance. However, these penalties are sensitive to box shape, size, and distribution, often leading to suboptimal optimization for small objects and undesired behaviors such as bounding box enlargement due to misalignment with the IoU objective. To address these limitations, we propose InterpIoU, a novel loss function that replaces handcrafted geometric penalties with a term based on the IoU between interpolated boxes and the target. By using interpolated boxes to bridge the gap between predictions and ground truth, InterpIoU provides meaningful gradients in non-overlapping cases and inherently avoids the box enlargement issue caused by misaligned penalties. Simulation results further show that IoU itself serves as an ideal regression target, while existing geometric penalties are both unnecessary and suboptimal. Building on InterpIoU, we introduce Dynamic InterpIoU, which dynamically adjusts interpolation coefficients based on IoU values, enhancing adaptability to scenarios with diverse object distributions. Experiments on COCO, VisDrone, and PASCAL VOC show that our methods consistently outperform state-of-the-art IoU-based losses across various detection frameworks, with particularly notable improvements in small object detection, confirming their effectiveness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 77,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12426",
        "abs_url": "https://arxiv.org/abs/2507.12426",
        "pdf_url": "https://arxiv.org/pdf/2507.12426",
        "title": "DVFL-Net: A Lightweight Distilled Video Focal Modulation Network for Spatio-Temporal Action Recognition",
        "authors": [
            "Hayat Ullah",
            "Muhammad Ali Shafique",
            "Abbas Khan",
            "Arslan Munir"
        ],
        "comments": "17 pages",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The landscape of video recognition has evolved significantly, shifting from traditional Convolutional Neural Networks (CNNs) to Transformer-based architectures for improved accuracy. While 3D CNNs have been effective at capturing spatiotemporal dynamics, recent Transformer models leverage self-attention to model long-range spatial and temporal dependencies. Despite achieving state-of-the-art performance on major benchmarks, Transformers remain computationally expensive, particularly with dense video data. To address this, we propose a lightweight Video Focal Modulation Network, DVFL-Net, which distills spatiotemporal knowledge from a large pre-trained teacher into a compact nano student model, enabling efficient on-device deployment. DVFL-Net utilizes knowledge distillation and spatial-temporal feature modulation to significantly reduce computation while preserving high recognition performance. We employ forward Kullback-Leibler (KL) divergence alongside spatio-temporal focal modulation to effectively transfer both local and global context from the Video-FocalNet Base (teacher) to the proposed VFL-Net (student). We evaluate DVFL-Net on UCF50, UCF101, HMDB51, SSV2, and Kinetics-400, benchmarking it against recent state-of-the-art methods in Human Action Recognition (HAR). Additionally, we conduct a detailed ablation study analyzing the impact of forward KL divergence. The results confirm the superiority of DVFL-Net in achieving an optimal balance between performance and efficiency, demonstrating lower memory usage, reduced GFLOPs, and strong accuracy, making it a practical solution for real-time HAR applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 78,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12433",
        "abs_url": "https://arxiv.org/abs/2507.12433",
        "pdf_url": "https://arxiv.org/pdf/2507.12433",
        "title": "Traffic-Aware Pedestrian Intention Prediction",
        "authors": [
            "Fahimeh Orvati Nia",
            "Hai Lin"
        ],
        "comments": "6 pages, 4 figures. Accepted to the American Control Conference (ACC) 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Systems and Control (eess.SY)",
        "abstract": "Accurate pedestrian intention estimation is crucial for the safe navigation of autonomous vehicles (AVs) and hence attracts a lot of research attention. However, current models often fail to adequately consider dynamic traffic signals and contextual scene information, which are critical for real-world applications. This paper presents a Traffic-Aware Spatio-Temporal Graph Convolutional Network (TA-STGCN) that integrates traffic signs and their states (Red, Yellow, Green) into pedestrian intention prediction. Our approach introduces the integration of dynamic traffic signal states and bounding box size as key features, allowing the model to capture both spatial and temporal dependencies in complex urban environments. The model surpasses existing methods in accuracy. Specifically, TA-STGCN achieves a 4.75% higher accuracy compared to the baseline model on the PIE dataset, demonstrating its effectiveness in improving pedestrian intention prediction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 79,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12441",
        "abs_url": "https://arxiv.org/abs/2507.12441",
        "pdf_url": "https://arxiv.org/pdf/2507.12441",
        "title": "Describe Anything Model for Visual Question Answering on Text-rich Images",
        "authors": [
            "Yen-Linh Vu",
            "Dinh-Thang Duong",
            "Truong-Binh Duong",
            "Anh-Khoi Nguyen",
            "Thanh-Huy Nguyen",
            "Le Thien Phuc Nguyen",
            "Jianhua Xing",
            "Xingjian Li",
            "Tianyang Wang",
            "Ulas Bagci",
            "Min Xu"
        ],
        "comments": "11 pages, 5 figures. Accepted to VisionDocs @ ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Recent progress has been made in region-aware vision-language modeling, particularly with the emergence of the Describe Anything Model (DAM). DAM is capable of generating detailed descriptions of any specific image areas or objects without the need for additional localized image-text alignment supervision. We hypothesize that such region-level descriptive capability is beneficial for the task of Visual Question Answering (VQA), especially in challenging scenarios involving images with dense text. In such settings, the fine-grained extraction of textual information is crucial to producing correct answers. Motivated by this, we introduce DAM-QA, a framework with a tailored evaluation protocol, developed to investigate and harness the region-aware capabilities from DAM for the text-rich VQA problem that requires reasoning over text-based information within images. DAM-QA incorporates a mechanism that aggregates answers from multiple regional views of image content, enabling more effective identification of evidence that may be tied to text-related elements. Experiments on six VQA benchmarks show that our approach consistently outperforms the baseline DAM, with a notable 7+ point gain on DocVQA. DAM-QA also achieves the best overall performance among region-aware models with fewer parameters, significantly narrowing the gap with strong generalist VLMs. These results highlight the potential of DAM-like models for text-rich and broader VQA tasks when paired with efficient usage and integration strategies. Our code is publicly available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 80,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12449",
        "abs_url": "https://arxiv.org/abs/2507.12449",
        "pdf_url": "https://arxiv.org/pdf/2507.12449",
        "title": "Vision-based Perception for Autonomous Vehicles in Obstacle Avoidance Scenarios",
        "authors": [
            "Van-Hoang-Anh Phan",
            "Chi-Tam Nguyen",
            "Doan-Trung Au",
            "Thanh-Danh Phan",
            "Minh-Thien Duong",
            "My-Ha Le"
        ],
        "comments": "7 pages, 6 figures, 4 tables, HSI 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Obstacle avoidance is essential for ensuring the safety of autonomous vehicles. Accurate perception and motion planning are crucial to enabling vehicles to navigate complex environments while avoiding collisions. In this paper, we propose an efficient obstacle avoidance pipeline that leverages a camera-only perception module and a Frenet-Pure Pursuit-based planning strategy. By integrating advancements in computer vision, the system utilizes YOLOv11 for object detection and state-of-the-art monocular depth estimation models, such as Depth Anything V2, to estimate object distances. A comparative analysis of these models provides valuable insights into their accuracy, efficiency, and robustness in real-world conditions. The system is evaluated in diverse scenarios on a university campus, demonstrating its effectiveness in handling various obstacles and enhancing autonomous navigation. The video presenting the results of the obstacle avoidance experiments is available at: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 81,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12455",
        "abs_url": "https://arxiv.org/abs/2507.12455",
        "pdf_url": "https://arxiv.org/pdf/2507.12455",
        "title": "Mitigating Object Hallucinations via Sentence-Level Early Intervention",
        "authors": [
            "Shangpin Peng",
            "Senqiao Yang",
            "Li Jiang",
            "Zhuotao Tian"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Multimodal large language models (MLLMs) have revolutionized cross-modal understanding but continue to struggle with hallucinations - fabricated content contradicting visual inputs. Existing hallucination mitigation methods either incur prohibitive computational costs or introduce distribution mismatches between training data and model outputs. We identify a critical insight: hallucinations predominantly emerge at the early stages of text generation and propagate through subsequent outputs. To address this, we propose **SENTINEL** (**S**entence-level **E**arly i**N**tervention **T**hrough **IN**-domain pr**E**ference **L**earning), a framework that eliminates dependency on human annotations. Specifically, we first bootstrap high-quality in-domain preference pairs by iteratively sampling model outputs, validating object existence through cross-checking with two open-vocabulary detectors, and classifying sentences into hallucinated/non-hallucinated categories. Subsequently, we use context-coherent positive samples and hallucinated negative samples to build context-aware preference data iteratively. Finally, we train models using a context-aware preference loss (C-DPO) that emphasizes discriminative learning at the sentence level where hallucinations initially manifest. Experimental results show that SENTINEL can reduce hallucinations by over 90\\% compared to the original model and outperforms the previous state-of-the-art method on both hallucination benchmarks and general capabilities benchmarks, demonstrating its superiority and generalization ability. The models, datasets, and code are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 82,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12461",
        "abs_url": "https://arxiv.org/abs/2507.12461",
        "pdf_url": "https://arxiv.org/pdf/2507.12461",
        "title": "Interpreting Radiologist's Intention from Eye Movements in Chest X-ray Diagnosis",
        "authors": [
            "Trong-Thang Pham",
            "Anh Nguyen",
            "Zhigang Deng",
            "Carol C. Wu",
            "Hien Van Nguyen",
            "Ngan Le"
        ],
        "comments": "ACM MM 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Radiologists rely on eye movements to navigate and interpret medical images. A trained radiologist possesses knowledge about the potential diseases that may be present in the images and, when searching, follows a mental checklist to locate them using their gaze. This is a key observation, yet existing models fail to capture the underlying intent behind each fixation. In this paper, we introduce a deep learning-based approach, RadGazeIntent, designed to model this behavior: having an intention to find something and actively searching for it. Our transformer-based architecture processes both the temporal and spatial dimensions of gaze data, transforming fine-grained fixation features into coarse, meaningful representations of diagnostic intent to interpret radiologists' goals. To capture the nuances of radiologists' varied intention-driven behaviors, we process existing medical eye-tracking datasets to create three intention-labeled subsets: RadSeq (Systematic Sequential Search), RadExplore (Uncertainty-driven Exploration), and RadHybrid (Hybrid Pattern). Experimental results demonstrate RadGazeIntent's ability to predict which findings radiologists are examining at specific moments, outperforming baseline methods across all intention-labeled datasets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 83,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12462",
        "abs_url": "https://arxiv.org/abs/2507.12462",
        "pdf_url": "https://arxiv.org/pdf/2507.12462",
        "title": "SpatialTrackerV2: 3D Point Tracking Made Easy",
        "authors": [
            "Yuxi Xiao",
            "Jianyuan Wang",
            "Nan Xue",
            "Nikita Karaev",
            "Yuri Makarov",
            "Bingyi Kang",
            "Xing Zhu",
            "Hujun Bao",
            "Yujun Shen",
            "Xiaowei Zhou"
        ],
        "comments": "International Conference on Computer Vision, ICCV 2025. Huggingface Demo: this https URL, Code: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We present SpatialTrackerV2, a feed-forward 3D point tracking method for monocular videos. Going beyond modular pipelines built on off-the-shelf components for 3D tracking, our approach unifies the intrinsic connections between point tracking, monocular depth, and camera pose estimation into a high-performing and feedforward 3D point tracker. It decomposes world-space 3D motion into scene geometry, camera ego-motion, and pixel-wise object motion, with a fully differentiable and end-to-end architecture, allowing scalable training across a wide range of datasets, including synthetic sequences, posed RGB-D videos, and unlabeled in-the-wild footage. By learning geometry and motion jointly from such heterogeneous data, SpatialTrackerV2 outperforms existing 3D tracking methods by 30%, and matches the accuracy of leading dynamic 3D reconstruction approaches while running 50$\\times$ faster.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 84,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12463",
        "abs_url": "https://arxiv.org/abs/2507.12463",
        "pdf_url": "https://arxiv.org/pdf/2507.12463",
        "title": "MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior Understanding",
        "authors": [
            "Renjie Li",
            "Ruijie Ye",
            "Mingyang Wu",
            "Hao Frank Yang",
            "Zhiwen Fan",
            "Hezhen Hu",
            "Zhengzhong Tu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Humans are integral components of the transportation ecosystem, and understanding their behaviors is crucial to facilitating the development of safe driving systems. Although recent progress has explored various aspects of human behavior$\\unicode{x2014}$such as motion, trajectories, and intention$\\unicode{x2014}$a comprehensive benchmark for evaluating human behavior understanding in autonomous driving remains unavailable. In this work, we propose $\\textbf{MMHU}$, a large-scale benchmark for human behavior analysis featuring rich annotations, such as human motion and trajectories, text description for human motions, human intention, and critical behavior labels relevant to driving safety. Our dataset encompasses 57k human motion clips and 1.73M frames gathered from diverse sources, including established driving datasets such as Waymo, in-the-wild videos from YouTube, and self-collected data. A human-in-the-loop annotation pipeline is developed to generate rich behavior captions. We provide a thorough dataset analysis and benchmark multiple tasks$\\unicode{x2014}$ranging from motion prediction to motion generation and human behavior question answering$\\unicode{x2014}$thereby offering a broad evaluation suite. Project page : this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 85,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12464",
        "abs_url": "https://arxiv.org/abs/2507.12464",
        "pdf_url": "https://arxiv.org/pdf/2507.12464",
        "title": "CytoSAE: Interpretable Cell Embeddings for Hematology",
        "authors": [
            "Muhammed Furkan Dasdelen",
            "Hyesu Lim",
            "Michele Buck",
            "Katharina S. Götze",
            "Carsten Marr",
            "Steffen Schneider"
        ],
        "comments": "11 pages, 5 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)",
        "abstract": "Sparse autoencoders (SAEs) emerged as a promising tool for mechanistic interpretability of transformer-based foundation models. Very recently, SAEs were also adopted for the visual domain, enabling the discovery of visual concepts and their patch-wise attribution to tokens in the transformer model. While a growing number of foundation models emerged for medical imaging, tools for explaining their inferences are still lacking. In this work, we show the applicability of SAEs for hematology. We propose CytoSAE, a sparse autoencoder which is trained on over 40,000 peripheral blood single-cell images. CytoSAE generalizes to diverse and out-of-domain datasets, including bone marrow cytology, where it identifies morphologically relevant concepts which we validated with medical experts. Furthermore, we demonstrate scenarios in which CytoSAE can generate patient-specific and disease-specific concepts, enabling the detection of pathognomonic cells and localized cellular abnormalities at the patch level. We quantified the effect of concepts on a patient-level AML subtype classification task and show that CytoSAE concepts reach performance comparable to the state-of-the-art, while offering explainability on the sub-cellular level. Source code and model weights are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 86,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12465",
        "abs_url": "https://arxiv.org/abs/2507.12465",
        "pdf_url": "https://arxiv.org/pdf/2507.12465",
        "title": "PhysX: Physical-Grounded 3D Asset Generation",
        "authors": [
            "Ziang Cao",
            "Zhaoxi Chen",
            "Liang Pan",
            "Ziwei Liu"
        ],
        "comments": "Project page: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "3D modeling is moving from virtual to physical. Existing 3D generation primarily emphasizes geometries and textures while neglecting physical-grounded modeling. Consequently, despite the rapid development of 3D generative models, the synthesized 3D assets often overlook rich and important physical properties, hampering their real-world application in physical domains like simulation and embodied AI. As an initial attempt to address this challenge, we propose \\textbf{PhysX}, an end-to-end paradigm for physical-grounded 3D asset generation. 1) To bridge the critical gap in physics-annotated 3D datasets, we present PhysXNet - the first physics-grounded 3D dataset systematically annotated across five foundational dimensions: absolute scale, material, affordance, kinematics, and function description. In particular, we devise a scalable human-in-the-loop annotation pipeline based on vision-language models, which enables efficient creation of physics-first assets from raw 3D assets.2) Furthermore, we propose \\textbf{PhysXGen}, a feed-forward framework for physics-grounded image-to-3D asset generation, injecting physical knowledge into the pre-trained 3D structural space. Specifically, PhysXGen employs a dual-branch architecture to explicitly model the latent correlations between 3D structures and physical properties, thereby producing 3D assets with plausible physical predictions while preserving the native geometry quality. Extensive experiments validate the superior performance and promising generalization capability of our framework. All the code, data, and models will be released to facilitate future research in generative physical AI.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 87,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11551",
        "abs_url": "https://arxiv.org/abs/2507.11551",
        "pdf_url": "https://arxiv.org/pdf/2507.11551",
        "title": "Landmark Detection for Medical Images using a General-purpose Segmentation Model",
        "authors": [
            "Ekaterina Stansfield",
            "Jennifer A. Mitterer",
            "Abdulrahman Altahhan"
        ],
        "comments": "13 pages, 8 figures, 2 tables. Submitted to ICONIP 2025",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Radiographic images are a cornerstone of medical diagnostics in orthopaedics, with anatomical landmark detection serving as a crucial intermediate step for information extraction. General-purpose foundational segmentation models, such as SAM (Segment Anything Model), do not support landmark segmentation out of the box and require prompts to function. However, in medical imaging, the prompts for landmarks are highly specific. Since SAM has not been trained to recognize such landmarks, it cannot generate accurate landmark segmentations for diagnostic purposes. Even MedSAM, a medically adapted variant of SAM, has been trained to identify larger anatomical structures, such as organs and their parts, and lacks the fine-grained precision required for orthopaedic pelvic landmarks. To address this limitation, we propose leveraging another general-purpose, non-foundational model: YOLO. YOLO excels in object detection and can provide bounding boxes that serve as input prompts for SAM. While YOLO is efficient at detection, it is significantly outperformed by SAM in segmenting complex structures. In combination, these two models form a reliable pipeline capable of segmenting not only a small pilot set of eight anatomical landmarks but also an expanded set of 72 landmarks and 16 regions with complex outlines, such as the femoral cortical bone and the pelvic inlet. By using YOLO-generated bounding boxes to guide SAM, we trained the hybrid model to accurately segment orthopaedic pelvic radiographs. Our results show that the proposed combination of YOLO and SAM yields excellent performance in detecting anatomical landmarks and intricate outlines in orthopaedic pelvic radiographs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 88,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11557",
        "abs_url": "https://arxiv.org/abs/2507.11557",
        "pdf_url": "https://arxiv.org/pdf/2507.11557",
        "title": "3D Wavelet Latent Diffusion Model for Whole-Body MR-to-CT Modality Translation",
        "authors": [
            "Jiaxu Zheng",
            "Meiman He",
            "Xuhui Tang",
            "Xiong Wang",
            "Tuoyu Cao",
            "Tianyi Zeng",
            "Lichi Zhang",
            "Chenyu You"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Magnetic Resonance (MR) imaging plays an essential role in contemporary clinical diagnostics. It is increasingly integrated into advanced therapeutic workflows, such as hybrid Positron Emission Tomography/Magnetic Resonance (PET/MR) imaging and MR-only radiation therapy. These integrated approaches are critically dependent on accurate estimation of radiation attenuation, which is typically facilitated by synthesizing Computed Tomography (CT) images from MR scans to generate attenuation maps. However, existing MR-to-CT synthesis methods for whole-body imaging often suffer from poor spatial alignment between the generated CT and input MR images, and insufficient image quality for reliable use in downstream clinical tasks. In this paper, we present a novel 3D Wavelet Latent Diffusion Model (3D-WLDM) that addresses these limitations by performing modality translation in a learned latent space. By incorporating a Wavelet Residual Module into the encoder-decoder architecture, we enhance the capture and reconstruction of fine-scale features across image and latent spaces. To preserve anatomical integrity during the diffusion process, we disentangle structural and modality-specific characteristics and anchor the structural component to prevent warping. We also introduce a Dual Skip Connection Attention mechanism within the diffusion model, enabling the generation of high-resolution CT images with improved representation of bony structures and soft-tissue contrast.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 89,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11561",
        "abs_url": "https://arxiv.org/abs/2507.11561",
        "pdf_url": "https://arxiv.org/pdf/2507.11561",
        "title": "Predicting Pulmonary Hypertension in Newborns: A Multi-view VAE Approach",
        "authors": [
            "Lucas Erlacher",
            "Samuel Ruipérez-Campillo",
            "Holger Michel",
            "Sven Wellmann",
            "Thomas M. Sutter",
            "Ece Ozkan",
            "Julia E. Vogt"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Pulmonary hypertension (PH) in newborns is a critical condition characterized by elevated pressure in the pulmonary arteries, leading to right ventricular strain and heart failure. While right heart catheterization (RHC) is the diagnostic gold standard, echocardiography is preferred due to its non-invasive nature, safety, and accessibility. However, its accuracy highly depends on the operator, making PH assessment subjective. While automated detection methods have been explored, most models focus on adults and rely on single-view echocardiographic frames, limiting their performance in diagnosing PH in newborns. While multi-view echocardiography has shown promise in improving PH assessment, existing models struggle with generalizability. In this work, we employ a multi-view variational autoencoder (VAE) for PH prediction using echocardiographic videos. By leveraging the VAE framework, our model captures complex latent representations, improving feature extraction and robustness. We compare its performance against single-view and supervised learning approaches. Our results show improved generalization and classification accuracy, highlighting the effectiveness of multi-view learning for robust PH assessment in newborns.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 90,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11569",
        "abs_url": "https://arxiv.org/abs/2507.11569",
        "pdf_url": "https://arxiv.org/pdf/2507.11569",
        "title": "Are Vision Foundation Models Ready for Out-of-the-Box Medical Image Registration?",
        "authors": [
            "Hanxue Gu",
            "Yaqian Chen",
            "Nicholas Konz",
            "Qihang Li",
            "Maciej A. Mazurowski"
        ],
        "comments": "3 figures, 9 pages",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Foundation models, pre-trained on large image datasets and capable of capturing rich feature representations, have recently shown potential for zero-shot image registration. However, their performance has mostly been tested in the context of rigid or less complex structures, such as the brain or abdominal organs, and it remains unclear whether these models can handle more challenging, deformable anatomy. Breast MRI registration is particularly difficult due to significant anatomical variation between patients, deformation caused by patient positioning, and the presence of thin and complex internal structure of fibroglandular tissue, where accurate alignment is crucial. Whether foundation model-based registration algorithms can address this level of complexity remains an open question. In this study, we provide a comprehensive evaluation of foundation model-based registration algorithms for breast MRI. We assess five pre-trained encoders, including DINO-v2, SAM, MedSAM, SSLSAM, and MedCLIP, across four key breast registration tasks that capture variations in different years and dates, sequences, modalities, and patient disease status (lesion versus no lesion). Our results show that foundation model-based algorithms such as SAM outperform traditional registration baselines for overall breast alignment, especially under large domain shifts, but struggle with capturing fine details of fibroglandular tissue. Interestingly, additional pre-training or fine-tuning on medical or breast-specific images in MedSAM and SSLSAM, does not improve registration performance and may even decrease it in some cases. Further work is needed to understand how domain-specific training influences registration and to explore targeted strategies that improve both global alignment and fine structure accuracy. We also publicly release our code at \\href{this https URL}{Github}.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 91,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11625",
        "abs_url": "https://arxiv.org/abs/2507.11625",
        "pdf_url": "https://arxiv.org/pdf/2507.11625",
        "title": "MapIQ: Benchmarking Multimodal Large Language Models for Map Question Answering",
        "authors": [
            "Varun Srivastava",
            "Fan Lei",
            "Srija Mukhopadhyay",
            "Vivek Gupta",
            "Ross Maciejewski"
        ],
        "comments": "Published as a conference paper at COLM 2025",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Recent advancements in multimodal large language models (MLLMs) have driven researchers to explore how well these models read data visualizations, e.g., bar charts, scatter plots. More recently, attention has shifted to visual question answering with maps (Map-VQA). However, Map-VQA research has primarily focused on choropleth maps, which cover only a limited range of thematic categories and visual analytical tasks. To address these gaps, we introduce MapIQ, a benchmark dataset comprising 14,706 question-answer pairs across three map types: choropleth maps, cartograms, and proportional symbol maps spanning topics from six distinct themes (e.g., housing, crime). We evaluate multiple MLLMs using six visual analytical tasks, comparing their performance against one another and a human baseline. An additional experiment examining the impact of map design changes (e.g., altered color schemes, modified legend designs, and removal of map elements) provides insights into the robustness and sensitivity of MLLMs, their reliance on internal geographic knowledge, and potential avenues for improving Map-VQA performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 92,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11690",
        "abs_url": "https://arxiv.org/abs/2507.11690",
        "pdf_url": "https://arxiv.org/pdf/2507.11690",
        "title": "The Impact of Coreset Selection on Spurious Correlations and Group Robustness",
        "authors": [
            "Amaya Dharmasiri",
            "William Yang",
            "Polina Kirichenko",
            "Lydia Liu",
            "Olga Russakovsky"
        ],
        "comments": "10 pages, 9 additional pages for Appendix",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Coreset selection methods have shown promise in reducing the training data size while maintaining model performance for data-efficient machine learning. However, as many datasets suffer from biases that cause models to learn spurious correlations instead of causal features, it is important to understand whether and how dataset reduction methods may perpetuate, amplify, or mitigate these biases. In this work, we conduct the first comprehensive analysis of the implications of data selection on the spurious bias levels of the selected coresets and the robustness of downstream models trained on them. We use an extensive experimental setting spanning ten different spurious correlations benchmarks, five score metrics to characterize sample importance/ difficulty, and five data selection policies across a broad range of coreset sizes. Thereby, we unravel a series of nontrivial nuances in interactions between sample difficulty and bias alignment, as well as dataset bias and resultant model robustness. For example, we find that selecting coresets using embedding-based sample characterization scores runs a comparatively lower risk of inadvertently exacerbating bias than selecting using characterizations based on learning dynamics. Most importantly, our analysis reveals that although some coreset selection methods could achieve lower bias levels by prioritizing difficult samples, they do not reliably guarantee downstream robustness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 93,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11711",
        "abs_url": "https://arxiv.org/abs/2507.11711",
        "pdf_url": "https://arxiv.org/pdf/2507.11711",
        "title": "Image-Based Multi-Survey Classification of Light Curves with a Pre-Trained Vision Transformer",
        "authors": [
            "Daniel Moreno-Cartagena",
            "Guillermo Cabrera-Vives",
            "Alejandra M. Muñoz Arancibia",
            "Pavlos Protopapas",
            "Francisco Förster",
            "Márcio Catelan",
            "A. Bayo",
            "Pablo A. Estévez",
            "P. Sánchez-Sáez",
            "Franz E. Bauer",
            "M. Pavez-Herrera",
            "L. Hernández-García",
            "Gonzalo Rojas"
        ],
        "comments": "Accepted at the 2025 Workshop on Machine Learning for Astrophysics at the International Conference on Machine Learning (ICML)",
        "subjects": "Instrumentation and Methods for Astrophysics (astro-ph.IM); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We explore the use of Swin Transformer V2, a pre-trained vision Transformer, for photometric classification in a multi-survey setting by leveraging light curves from the Zwicky Transient Facility (ZTF) and the Asteroid Terrestrial-impact Last Alert System (ATLAS). We evaluate different strategies for integrating data from these surveys and find that a multi-survey architecture which processes them jointly achieves the best performance. These results highlight the importance of modeling survey-specific characteristics and cross-survey interactions, and provide guidance for building scalable classifiers for future time-domain astronomy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 94,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11821",
        "abs_url": "https://arxiv.org/abs/2507.11821",
        "pdf_url": "https://arxiv.org/pdf/2507.11821",
        "title": "MNIST-Gen: A Modular MNIST-Style Dataset Generation Using Hierarchical Semantics, Reinforcement Learning, and Category Theory",
        "authors": [
            "Pouya Shaeri",
            "Arash Karimi",
            "Ariane Middel"
        ],
        "comments": "Submitted to a computer science conference",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC)",
        "abstract": "Neural networks are often benchmarked using standard datasets such as MNIST, FashionMNIST, or other variants of MNIST, which, while accessible, are limited to generic classes such as digits or clothing items. For researchers working on domain-specific tasks, such as classifying trees, food items, or other real-world objects, these data sets are insufficient and irrelevant. Additionally, creating and publishing a custom dataset can be time consuming, legally constrained, or beyond the scope of individual projects. We present MNIST-Gen, an automated, modular, and adaptive framework for generating MNIST-style image datasets tailored to user-specified categories using hierarchical semantic categorization. The system combines CLIP-based semantic understanding with reinforcement learning and human feedback to achieve intelligent categorization with minimal manual intervention. Our hierarchical approach supports complex category structures with semantic characteristics, enabling fine-grained subcategorization and multiple processing modes: individual review for maximum control, smart batch processing for large datasets, and fast batch processing for rapid creation. Inspired by category theory, MNIST-Gen models each data transformation stage as a composable morphism, enhancing clarity, modularity, and extensibility. As proof of concept, we generate and benchmark two novel datasets-\\textit{Tree-MNIST} and \\textit{Food-MNIST}-demonstrating MNIST-Gen's utility for producing task-specific evaluation data while achieving 85\\% automatic categorization accuracy and 80\\% time savings compared to manual approaches.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 95,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11852",
        "abs_url": "https://arxiv.org/abs/2507.11852",
        "pdf_url": "https://arxiv.org/pdf/2507.11852",
        "title": "Towards Autonomous Riding: A Review of Perception, Planning, and Control in Intelligent Two-Wheelers",
        "authors": [
            "Mohammed Hassanin",
            "Mohammad Abu Alsheikh",
            "Carlos C. N. Kuhn",
            "Damith Herath",
            "Dinh Thai Hoang",
            "Ibrahim Radwan"
        ],
        "comments": "17 pages",
        "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The rapid adoption of micromobility solutions, particularly two-wheeled vehicles like e-scooters and e-bikes, has created an urgent need for reliable autonomous riding (AR) technologies. While autonomous driving (AD) systems have matured significantly, AR presents unique challenges due to the inherent instability of two-wheeled platforms, limited size, limited power, and unpredictable environments, which pose very serious concerns about road users' safety. This review provides a comprehensive analysis of AR systems by systematically examining their core components, perception, planning, and control, through the lens of AD technologies. We identify critical gaps in current AR research, including a lack of comprehensive perception systems for various AR tasks, limited industry and government support for such developments, and insufficient attention from the research community. The review analyses the gaps of AR from the perspective of AD to highlight promising research directions, such as multimodal sensor techniques for lightweight platforms and edge deep learning architectures. By synthesising insights from AD research with the specific requirements of AR, this review aims to accelerate the development of safe, efficient, and scalable autonomous riding systems for future urban mobility.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 96,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11853",
        "abs_url": "https://arxiv.org/abs/2507.11853",
        "pdf_url": "https://arxiv.org/pdf/2507.11853",
        "title": "A Spatial-Physics Informed Model for 3D Spiral Sample Scanned by SQUID Microscopy",
        "authors": [
            "J. Senthilnath",
            "Jayasanker Jayabalan",
            "Zhuoyi Lin",
            "Aye Phyu Phyu Aung",
            "Chen Hao",
            "Kaixin Xu",
            "Yeow Kheng Lim",
            "F. C. Wellstood"
        ],
        "comments": "copyright 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works",
        "subjects": "Instrumentation and Detectors (physics.ins-det); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The development of advanced packaging is essential in the semiconductor manufacturing industry. However, non-destructive testing (NDT) of advanced packaging becomes increasingly challenging due to the depth and complexity of the layers involved. In such a scenario, Magnetic field imaging (MFI) enables the imaging of magnetic fields generated by currents. For MFI to be effective in NDT, the magnetic fields must be converted into current density. This conversion has typically relied solely on a Fast Fourier Transform (FFT) for magnetic field inversion; however, the existing approach does not consider eddy current effects or image misalignment in the test setup. In this paper, we present a spatial-physics informed model (SPIM) designed for a 3D spiral sample scanned using Superconducting QUantum Interference Device (SQUID) microscopy. The SPIM encompasses three key components: i) magnetic image enhancement by aligning all the \"sharp\" wire field signals to mitigate the eddy current effect using both in-phase (I-channel) and quadrature-phase (Q-channel) images; (ii) magnetic image alignment that addresses skew effects caused by any misalignment of the scanning SQUID microscope relative to the wire segments; and (iii) an inversion method for converting magnetic fields to magnetic currents by integrating the Biot-Savart Law with FFT. The results show that the SPIM improves I-channel sharpness by 0.3% and reduces Q-channel sharpness by 25%. Also, we were able to remove rotational and skew misalignments of 0.30 in a real image. Overall, SPIM highlights the potential of combining spatial analysis with physics-driven models in practical applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 97,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11900",
        "abs_url": "https://arxiv.org/abs/2507.11900",
        "pdf_url": "https://arxiv.org/pdf/2507.11900",
        "title": "CompressedVQA-HDR: Generalized Full-reference and No-reference Quality Assessment Models for Compressed High Dynamic Range Videos",
        "authors": [
            "Wei Sun",
            "Linhan Cao",
            "Kang Fu",
            "Dandan Zhu",
            "Jun Jia",
            "Menghan Hu",
            "Xiongkuo Min",
            "Guangtao Zhai"
        ],
        "comments": "CompressedVQA-HDR won first place in the FR track of the Generalizable HDR & SDR Video Quality Measurement Grand Challenge at IEEE ICME 2025",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Video compression is a standard procedure applied to all videos to minimize storage and transmission demands while preserving visual quality as much as possible. Therefore, evaluating the visual quality of compressed videos is crucial for guiding the practical usage and further development of video compression algorithms. Although numerous compressed video quality assessment (VQA) methods have been proposed, they often lack the generalization capability needed to handle the increasing diversity of video types, particularly high dynamic range (HDR) content. In this paper, we introduce CompressedVQA-HDR, an effective VQA framework designed to address the challenges of HDR video quality assessment. Specifically, we adopt the Swin Transformer and SigLip 2 as the backbone networks for the proposed full-reference (FR) and no-reference (NR) VQA models, respectively. For the FR model, we compute deep structural and textural similarities between reference and distorted frames using intermediate-layer features extracted from the Swin Transformer as its quality-aware feature representation. For the NR model, we extract the global mean of the final-layer feature maps from SigLip 2 as its quality-aware representation. To mitigate the issue of limited HDR training data, we pre-train the FR model on a large-scale standard dynamic range (SDR) VQA dataset and fine-tune it on the HDRSDR-VQA dataset. For the NR model, we employ an iterative mixed-dataset training strategy across multiple compressed VQA datasets, followed by fine-tuning on the HDRSDR-VQA dataset. Experimental results show that our models achieve state-of-the-art performance compared to existing FR and NR VQA models. Moreover, CompressedVQA-HDR-FR won first place in the FR track of the Generalizable HDR & SDR Video Quality Measurement Grand Challenge at IEEE ICME 2025. The code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 98,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11936",
        "abs_url": "https://arxiv.org/abs/2507.11936",
        "pdf_url": "https://arxiv.org/pdf/2507.11936",
        "title": "A Survey of Deep Learning for Geometry Problem Solving",
        "authors": [
            "Jianzhe Ma",
            "Wenxuan Wang",
            "Qin Jin"
        ],
        "comments": "Work in progress",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Geometry problem solving is a key area of mathematical reasoning, which is widely involved in many important fields such as education, mathematical ability assessment of artificial intelligence, and multimodal ability assessment. In recent years, the rapid development of deep learning technology, especially the rise of multimodal large language models, has triggered a widespread research boom. This paper provides a survey of the applications of deep learning in geometry problem solving, including (i) a comprehensive summary of the relevant tasks in geometry problem solving; (ii) a thorough review of related deep learning methods; (iii) a detailed analysis of evaluation metrics and methods; and (iv) a critical discussion of the current challenges and future directions that can be explored. Our goal is to provide a comprehensive and practical reference of deep learning for geometry problem solving to promote further developments in this field. We create a continuously updated list of papers on GitHub: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 99,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11938",
        "abs_url": "https://arxiv.org/abs/2507.11938",
        "pdf_url": "https://arxiv.org/pdf/2507.11938",
        "title": "A Multi-Level Similarity Approach for Single-View Object Grasping: Matching, Planning, and Fine-Tuning",
        "authors": [
            "Hao Chen",
            "Takuya Kiyokawa",
            "Zhengtao Hu",
            "Weiwei Wan",
            "Kensuke Harada"
        ],
        "comments": "Accepted by IEEE T-RO",
        "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Grasping unknown objects from a single view has remained a challenging topic in robotics due to the uncertainty of partial observation. Recent advances in large-scale models have led to benchmark solutions such as GraspNet-1Billion. However, such learning-based approaches still face a critical limitation in performance robustness for their sensitivity to sensing noise and environmental changes. To address this bottleneck in achieving highly generalized grasping, we abandon the traditional learning framework and introduce a new perspective: similarity matching, where similar known objects are utilized to guide the grasping of unknown target objects. We newly propose a method that robustly achieves unknown-object grasping from a single viewpoint through three key steps: 1) Leverage the visual features of the observed object to perform similarity matching with an existing database containing various object models, identifying potential candidates with high similarity; 2) Use the candidate models with pre-existing grasping knowledge to plan imitative grasps for the unknown target object; 3) Optimize the grasp quality through a local fine-tuning process. To address the uncertainty caused by partial and noisy observation, we propose a multi-level similarity matching framework that integrates semantic, geometric, and dimensional features for comprehensive evaluation. Especially, we introduce a novel point cloud geometric descriptor, the C-FPFH descriptor, which facilitates accurate similarity assessment between partial point clouds of observed objects and complete point clouds of database models. In addition, we incorporate the use of large language models, introduce the semi-oriented bounding box, and develop a novel point cloud registration approach based on plane detection to enhance matching accuracy under single-view conditions. Videos are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 100,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11939",
        "abs_url": "https://arxiv.org/abs/2507.11939",
        "pdf_url": "https://arxiv.org/pdf/2507.11939",
        "title": "POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering",
        "authors": [
            "Yichen Xu",
            "Liangyu Chen",
            "Liang Zhang",
            "Wenxuan Wang",
            "Qin Jin"
        ],
        "comments": "Work in Progress",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM)",
        "abstract": "Charts are a universally adopted medium for interpreting and communicating data. However, existing chart understanding benchmarks are predominantly English-centric, limiting their accessibility and applicability to global audiences. In this paper, we present PolyChartQA, the first large-scale multilingual chart question answering benchmark covering 22,606 charts and 26,151 question-answering pairs across 10 diverse languages. PolyChartQA is built using a decoupled pipeline that separates chart data from rendering code, allowing multilingual charts to be flexibly generated by simply translating the data and reusing the code. We leverage state-of-the-art LLM-based translation and enforce rigorous quality control in the pipeline to ensure the linguistic and semantic consistency of the generated multilingual charts. PolyChartQA facilitates systematic evaluation of multilingual chart understanding. Experiments on both open- and closed-source large vision-language models reveal a significant performance gap between English and other languages, especially low-resource ones with non-Latin scripts. This benchmark lays a foundation for advancing globally inclusive vision-language models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 101,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11943",
        "abs_url": "https://arxiv.org/abs/2507.11943",
        "pdf_url": "https://arxiv.org/pdf/2507.11943",
        "title": "Effective Fine-Tuning of Vision Transformers with Low-Rank Adaptation for Privacy-Preserving Image Classification",
        "authors": [
            "Haiwei Lin",
            "Shoko Imaizumi",
            "Hitoshi Kiya"
        ],
        "comments": "3 pages, 3 figures, conference",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We propose a low-rank adaptation method for training privacy-preserving vision transformer (ViT) models that efficiently freezes pre-trained ViT model weights. In the proposed method, trainable rank decomposition matrices are injected into each layer of the ViT architecture, and moreover, the patch embedding layer is not frozen, unlike in the case of the conventional low-rank adaptation methods. The proposed method allows us not only to reduce the number of trainable parameters but to also maintain almost the same accuracy as that of full-time tuning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 102,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11949",
        "abs_url": "https://arxiv.org/abs/2507.11949",
        "pdf_url": "https://arxiv.org/pdf/2507.11949",
        "title": "MOSPA: Human Motion Generation Driven by Spatial Audio",
        "authors": [
            "Shuyang Xu",
            "Zhiyang Dou",
            "Mingyi Shi",
            "Liang Pan",
            "Leo Ho",
            "Jingbo Wang",
            "Yuan Liu",
            "Cheng Lin",
            "Yuexin Ma",
            "Wenping Wang",
            "Taku Komura"
        ],
        "comments": "",
        "subjects": "Graphics (cs.GR); Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
        "abstract": "Enabling virtual humans to dynamically and realistically respond to diverse auditory stimuli remains a key challenge in character animation, demanding the integration of perceptual modeling and motion synthesis. Despite its significance, this task remains largely unexplored. Most previous works have primarily focused on mapping modalities like speech, audio, and music to generate human motion. As of yet, these models typically overlook the impact of spatial features encoded in spatial audio signals on human motion. To bridge this gap and enable high-quality modeling of human movements in response to spatial audio, we introduce the first comprehensive Spatial Audio-Driven Human Motion (SAM) dataset, which contains diverse and high-quality spatial audio and motion data. For benchmarking, we develop a simple yet effective diffusion-based generative framework for human MOtion generation driven by SPatial Audio, termed MOSPA, which faithfully captures the relationship between body motion and spatial audio through an effective fusion mechanism. Once trained, MOSPA could generate diverse realistic human motions conditioned on varying spatial audio inputs. We perform a thorough investigation of the proposed dataset and conduct extensive experiments for benchmarking, where our method achieves state-of-the-art performance on this task. Our model and dataset will be open-sourced upon acceptance. Please refer to our supplementary video for more details.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 103,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.11971",
        "abs_url": "https://arxiv.org/abs/2507.11971",
        "pdf_url": "https://arxiv.org/pdf/2507.11971",
        "title": "HPR3D: Hierarchical Proxy Representation for High-Fidelity 3D Reconstruction and Controllable Editing",
        "authors": [
            "Tielong Wang",
            "Yuxuan Xiong",
            "Jinfan Liu",
            "Zhifan Zhang",
            "Ye Chen",
            "Yue Shi",
            "Bingbing Ni"
        ],
        "comments": "",
        "subjects": "Graphics (cs.GR); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Current 3D representations like meshes, voxels, point clouds, and NeRF-based neural implicit fields exhibit significant limitations: they are often task-specific, lacking universal applicability across reconstruction, generation, editing, and driving. While meshes offer high precision, their dense vertex data complicates editing; NeRFs deliver excellent rendering but suffer from structural ambiguity, hindering animation and manipulation; all representations inherently struggle with the trade-off between data complexity and fidelity. To overcome these issues, we introduce a novel 3D Hierarchical Proxy Node representation. Its core innovation lies in representing an object's shape and texture via a sparse set of hierarchically organized (tree-structured) proxy nodes distributed on its surface and interior. Each node stores local shape and texture information (implicitly encoded by a small MLP) within its neighborhood. Querying any 3D coordinate's properties involves efficient neural interpolation and lightweight decoding from relevant nearby and parent nodes. This framework yields a highly compact representation where nodes align with local semantics, enabling direct drag-and-edit manipulation, and offers scalable quality-complexity control. Extensive experiments across 3D reconstruction and editing demonstrate our method's expressive efficiency, high-fidelity rendering quality, and superior editability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 104,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12012",
        "abs_url": "https://arxiv.org/abs/2507.12012",
        "pdf_url": "https://arxiv.org/pdf/2507.12012",
        "title": "Identifying Signatures of Image Phenotypes to Track Treatment Response in Liver Disease",
        "authors": [
            "Matthias Perkonigg",
            "Nina Bastati",
            "Ahmed Ba-Ssalamah",
            "Peter Mesenbrink",
            "Alexander Goehler",
            "Miljen Martic",
            "Xiaofei Zhou",
            "Michael Trauner",
            "Georg Langs"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Quantifiable image patterns associated with disease progression and treatment response are critical tools for guiding individual treatment, and for developing novel therapies. Here, we show that unsupervised machine learning can identify a pattern vocabulary of liver tissue in magnetic resonance images that quantifies treatment response in diffuse liver disease. Deep clustering networks simultaneously encode and cluster patches of medical images into a low-dimensional latent space to establish a tissue vocabulary. The resulting tissue types capture differential tissue change and its location in the liver associated with treatment response. We demonstrate the utility of the vocabulary on a randomized controlled trial cohort of non-alcoholic steatohepatitis patients. First, we use the vocabulary to compare longitudinal liver change in a placebo and a treatment cohort. Results show that the method identifies specific liver tissue change pathways associated with treatment, and enables a better separation between treatment groups than established non-imaging measures. Moreover, we show that the vocabulary can predict biopsy derived features from non-invasive imaging data. We validate the method on a separate replication cohort to demonstrate the applicability of the proposed method.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 105,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12042",
        "abs_url": "https://arxiv.org/abs/2507.12042",
        "pdf_url": "https://arxiv.org/pdf/2507.12042",
        "title": "Stereo Sound Event Localization and Detection with Onscreen/offscreen Classification",
        "authors": [
            "Kazuki Shimada",
            "Archontis Politis",
            "Iran R. Roman",
            "Parthasaarathy Sudarsanam",
            "David Diaz-Guerra",
            "Ruchi Pandey",
            "Kengo Uchida",
            "Yuichiro Koyama",
            "Naoya Takahashi",
            "Takashi Shibuya",
            "Shusuke Takahashi",
            "Tuomas Virtanen",
            "Yuki Mitsufuji"
        ],
        "comments": "5 pages, 2 figures",
        "subjects": "Sound (cs.SD); Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM); Audio and Speech Processing (eess.AS); Image and Video Processing (eess.IV)",
        "abstract": "This paper presents the objective, dataset, baseline, and metrics of Task 3 of the DCASE2025 Challenge on sound event localization and detection (SELD). In previous editions, the challenge used four-channel audio formats of first-order Ambisonics (FOA) and microphone array. In contrast, this year's challenge investigates SELD with stereo audio data (termed stereo SELD). This change shifts the focus from more specialized 360° audio and audiovisual scene analysis to more commonplace audio and media scenarios with limited field-of-view (FOV). Due to inherent angular ambiguities in stereo audio data, the task focuses on direction-of-arrival (DOA) estimation in the azimuth plane (left-right axis) along with distance estimation. The challenge remains divided into two tracks: audio-only and audiovisual, with the audiovisual track introducing a new sub-task of onscreen/offscreen event classification necessitated by the limited FOV. This challenge introduces the DCASE2025 Task3 Stereo SELD Dataset, whose stereo audio and perspective video clips are sampled and converted from the STARSS23 recordings. The baseline system is designed to process stereo audio and corresponding video frames as inputs. In addition to the typical SELD event classification and localization, it integrates onscreen/offscreen classification for the audiovisual track. The evaluation metrics have been modified to introduce an onscreen/offscreen accuracy metric, which assesses the models' ability to identify which sound sources are onscreen. In the experimental evaluation, the baseline system performs reasonably well with the stereo audio data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 106,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12050",
        "abs_url": "https://arxiv.org/abs/2507.12050",
        "pdf_url": "https://arxiv.org/pdf/2507.12050",
        "title": "IDFace: Face Template Protection for Efficient and Secure Identification",
        "authors": [
            "Sunpill Kim",
            "Seunghun Paik",
            "Chanwoo Hwang",
            "Dongsoo Kim",
            "Junbum Shin",
            "Jae Hong Seo"
        ],
        "comments": "Accepted to ICCV 2025",
        "subjects": "Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "As face recognition systems (FRS) become more widely used, user privacy becomes more important. A key privacy issue in FRS is protecting the user's face template, as the characteristics of the user's face image can be recovered from the template. Although recent advances in cryptographic tools such as homomorphic encryption (HE) have provided opportunities for securing the FRS, HE cannot be used directly with FRS in an efficient plug-and-play manner. In particular, although HE is functionally complete for arbitrary programs, it is basically designed for algebraic operations on encrypted data of predetermined shape, such as a polynomial ring. Thus, a non-tailored combination of HE and the system can yield very inefficient performance, and many previous HE-based face template protection methods are hundreds of times slower than plain systems without protection. In this study, we propose IDFace, a new HE-based secure and efficient face identification method with template protection. IDFace is designed on the basis of two novel techniques for efficient searching on a (homomorphically encrypted) biometric database with an angular metric. The first technique is a template representation transformation that sharply reduces the unit cost for the matching test. The second is a space-efficient encoding that reduces wasted space from the encryption algorithm, thus saving the number of operations on encrypted templates. Through experiments, we show that IDFace can identify a face template from among a database of 1M encrypted templates in 126ms, showing only 2X overhead compared to the identification over plaintexts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 107,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12092",
        "abs_url": "https://arxiv.org/abs/2507.12092",
        "pdf_url": "https://arxiv.org/pdf/2507.12092",
        "title": "Benchmarking and Explaining Deep Learning Cortical Lesion MRI Segmentation in Multiple Sclerosis",
        "authors": [
            "Nataliia Molchanova",
            "Alessandro Cagol",
            "Mario Ocampo-Pineda",
            "Po-Jui Lu",
            "Matthias Weigel",
            "Xinjie Chen",
            "Erin Beck",
            "Charidimos Tsagkas",
            "Daniel Reich",
            "Colin Vanden Bulcke",
            "Anna Stolting",
            "Serena Borrelli",
            "Pietro Maggi",
            "Adrien Depeursinge",
            "Cristina Granziera",
            "Henning Mueller",
            "Pedro M. Gordaliza",
            "Meritxell Bach Cuadra"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Cortical lesions (CLs) have emerged as valuable biomarkers in multiple sclerosis (MS), offering high diagnostic specificity and prognostic relevance. However, their routine clinical integration remains limited due to subtle magnetic resonance imaging (MRI) appearance, challenges in expert annotation, and a lack of standardized automated methods. We propose a comprehensive multi-centric benchmark of CL detection and segmentation in MRI. A total of 656 MRI scans, including clinical trial and research data from four institutions, were acquired at 3T and 7T using MP2RAGE and MPRAGE sequences with expert-consensus annotations. We rely on the self-configuring nnU-Net framework, designed for medical imaging segmentation, and propose adaptations tailored to the improved CL detection. We evaluated model generalization through out-of-distribution testing, demonstrating strong lesion detection capabilities with an F1-score of 0.64 and 0.5 in and out of the domain, respectively. We also analyze internal model features and model errors for a better understanding of AI decision-making. Our study examines how data variability, lesion ambiguity, and protocol differences impact model performance, offering future recommendations to address these barriers to clinical adoption. To reinforce the reproducibility, the implementation and models will be publicly accessible and ready to use at this https URL and this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 108,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12132",
        "abs_url": "https://arxiv.org/abs/2507.12132",
        "pdf_url": "https://arxiv.org/pdf/2507.12132",
        "title": "DoRF: Doppler Radiance Fields for Robust Human Activity Recognition Using Wi-Fi",
        "authors": [
            "Navid Hasanzadeh",
            "Shahrokh Valaee"
        ],
        "comments": "",
        "subjects": "Signal Processing (eess.SP); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Wi-Fi Channel State Information (CSI) has gained increasing interest for remote sensing applications. Recent studies show that Doppler velocity projections extracted from CSI can enable human activity recognition (HAR) that is robust to environmental changes and generalizes to new users. However, despite these advances, generalizability still remains insufficient for practical deployment. Inspired by neural radiance fields (NeRF), which learn a volumetric representation of a 3D scene from 2D images, this work proposes a novel approach to reconstruct an informative 3D latent motion representation from one-dimensional Doppler velocity projections extracted from Wi-Fi CSI. The resulting latent representation is then used to construct a uniform Doppler radiance field (DoRF) of the motion, providing a comprehensive view of the performed activity and improving the robustness to environmental variability. The results show that the proposed approach noticeably enhances the generalization accuracy of Wi-Fi-based HAR, highlighting the strong potential of DoRFs for practical sensing applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 109,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12145",
        "abs_url": "https://arxiv.org/abs/2507.12145",
        "pdf_url": "https://arxiv.org/pdf/2507.12145",
        "title": "PRISM: Distributed Inference for Foundation Models at Edge",
        "authors": [
            "Muhammad Azlan Qazi",
            "Alexandros Iosifidis",
            "Qi Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Foundation models (FMs) have achieved remarkable success across a wide range of applications, from image classification to natural langurage processing, but pose significant challenges for deployment at edge. This has sparked growing interest in developing practical and efficient strategies for bringing foundation models to edge environments. In this work, we propose PRISM, a communication-efficient and compute-aware strategy for distributed Transformer inference on edge devices. Our method leverages a Segment Means representation to approximate intermediate output features, drastically reducing inter-device communication. Additionally, we restructure the self-attention mechanism to eliminate redundant computations caused by per-device Key/Value calculation in position-wise partitioning and design a partition-aware causal masking scheme tailored for autoregressive models. We evaluate PRISM on ViT, BERT, and GPT-2 across diverse datasets, namely CIFAR-10, CIFAR-100, ImageNet-1k, GLUE, and CBT. Our results demonstrate substantial reductions in communication overhead (up to 99.2% for BERT at compression rate CR = 128) and per-device computation (51.24% for BERT at the same setting), with only minor accuracy degradation. This method offers a scalable and practical solution for deploying foundation models in distributed resource-constrained environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 110,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12297",
        "abs_url": "https://arxiv.org/abs/2507.12297",
        "pdf_url": "https://arxiv.org/pdf/2507.12297",
        "title": "RegCL: Continual Adaptation of Segment Anything Model via Model Merging",
        "authors": [
            "Yuan-Chen Shu",
            "Zhiwei Lin",
            "Yongtao Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "To address the performance limitations of the Segment Anything Model (SAM) in specific domains, existing works primarily adopt adapter-based one-step adaptation paradigms. However, some of these methods are specific developed for specific domains. If used on other domains may lead to performance degradation. This issue of catastrophic forgetting severely limits the model's scalability. To address this issue, this paper proposes RegCL, a novel non-replay continual learning (CL) framework designed for efficient multi-domain knowledge integration through model merging. Specifically, RegCL incorporates the model merging algorithm into the continual learning paradigm by merging the parameters of SAM's adaptation modules (e.g., LoRA modules) trained on different domains. The merging process is guided by weight optimization, which minimizes prediction discrepancies between the merged model and each of the domain-specific models. RegCL effectively consolidates multi-domain knowledge while maintaining parameter efficiency, i.e., the model size remains constant regardless of the number of tasks, and no historical data storage is required. Experimental results demonstrate that RegCL achieves favorable continual learning performance across multiple downstream datasets, validating its effectiveness in dynamic scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 111,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12305",
        "abs_url": "https://arxiv.org/abs/2507.12305",
        "pdf_url": "https://arxiv.org/pdf/2507.12305",
        "title": "PROL : Rehearsal Free Continual Learning in Streaming Data via Prompt Online Learning",
        "authors": [
            "M. Anwar Ma'sum",
            "Mahardhika Pratama",
            "Savitha Ramasamy",
            "Lin Liu",
            "Habibullah Habibullah",
            "Ryszard Kowalczyk"
        ],
        "comments": "ICCV 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The data privacy constraint in online continual learning (OCL), where the data can be seen only once, complicates the catastrophic forgetting problem in streaming data. A common approach applied by the current SOTAs in OCL is with the use of memory saving exemplars or features from previous classes to be replayed in the current task. On the other hand, the prompt-based approach performs excellently in continual learning but with the cost of a growing number of trainable parameters. The first approach may not be applicable in practice due to data openness policy, while the second approach has the issue of throughput associated with the streaming data. In this study, we propose a novel prompt-based method for online continual learning that includes 4 main components: (1) single light-weight prompt generator as a general knowledge, (2) trainable scaler-and-shifter as specific knowledge, (3) pre-trained model (PTM) generalization preserving, and (4) hard-soft updates mechanism. Our proposed method achieves significantly higher performance than the current SOTAs in CIFAR100, ImageNet-R, ImageNet-A, and CUB dataset. Our complexity analysis shows that our method requires a relatively smaller number of parameters and achieves moderate training time, inference time, and throughput. For further study, the source code of our method is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 112,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12366",
        "abs_url": "https://arxiv.org/abs/2507.12366",
        "pdf_url": "https://arxiv.org/pdf/2507.12366",
        "title": "FactorHD: A Hyperdimensional Computing Model for Multi-Object Multi-Class Representation and Factorization",
        "authors": [
            "Yifei Zhou",
            "Xuchu Huang",
            "Chenyu Ni",
            "Min Zhou",
            "Zheyu Yan",
            "Xunzhao Yin",
            "Cheng Zhuo"
        ],
        "comments": "7 pages, 5 figures, 2 tables, to be published in the 62nd DAC (Design Automation Conference) proceedings",
        "subjects": "Symbolic Computation (cs.SC); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Neuro-symbolic artificial intelligence (neuro-symbolic AI) excels in logical analysis and reasoning. Hyperdimensional Computing (HDC), a promising brain-inspired computational model, is integral to neuro-symbolic AI. Various HDC models have been proposed to represent class-instance and class-class relations, but when representing the more complex class-subclass relation, where multiple objects associate different levels of classes and subclasses, they face challenges for factorization, a crucial task for neuro-symbolic AI systems. In this article, we propose FactorHD, a novel HDC model capable of representing and factorizing the complex class-subclass relation efficiently. FactorHD features a symbolic encoding method that embeds an extra memorization clause, preserving more information for multiple objects. In addition, it employs an efficient factorization algorithm that selectively eliminates redundant classes by identifying the memorization clause of the target class. Such model significantly enhances computing efficiency and accuracy in representing and factorizing multiple objects with class-subclass relation, overcoming limitations of existing HDC models such as \"superposition catastrophe\" and \"the problem of 2\". Evaluations show that FactorHD achieves approximately 5667x speedup at a representation size of 10^9 compared to existing HDC models. When integrated with the ResNet-18 neural network, FactorHD achieves 92.48% factorization accuracy on the Cifar-10 dataset.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 113,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12417",
        "abs_url": "https://arxiv.org/abs/2507.12417",
        "pdf_url": "https://arxiv.org/pdf/2507.12417",
        "title": "Spontaneous Spatial Cognition Emerges during Egocentric Video Viewing through Non-invasive BCI",
        "authors": [
            "Weichen Dai",
            "Yuxuan Huang",
            "Li Zhu",
            "Dongjun Liu",
            "Yu Zhang",
            "Qibin Zhao",
            "Andrzej Cichocki",
            "Fabio Babiloni",
            "Ke Li",
            "Jianyu Qiu",
            "Gangyong Jia",
            "Wanzeng Kong",
            "Qing Wu"
        ],
        "comments": "",
        "subjects": "Neurons and Cognition (q-bio.NC); Computer Vision and Pattern Recognition (cs.CV); Signal Processing (eess.SP)",
        "abstract": "Humans possess a remarkable capacity for spatial cognition, allowing for self-localization even in novel or unfamiliar environments. While hippocampal neurons encoding position and orientation are well documented, the large-scale neural dynamics supporting spatial representation, particularly during naturalistic, passive experience, remain poorly understood. Here, we demonstrate for the first time that non-invasive brain-computer interfaces (BCIs) based on electroencephalography (EEG) can decode spontaneous, fine-grained egocentric 6D pose, comprising three-dimensional position and orientation, during passive viewing of egocentric video. Despite EEG's limited spatial resolution and high signal noise, we find that spatially coherent visual input (i.e., continuous and structured motion) reliably evokes decodable spatial representations, aligning with participants' subjective sense of spatial engagement. Decoding performance further improves when visual input is presented at a frame rate of 100 ms per image, suggesting alignment with intrinsic neural temporal dynamics. Using gradient-based backpropagation through a neural decoding model, we identify distinct EEG channels contributing to position -- and orientation specific -- components, revealing a distributed yet complementary neural encoding scheme. These findings indicate that the brain's spatial systems operate spontaneously and continuously, even under passive conditions, challenging traditional distinctions between active and passive spatial cognition. Our results offer a non-invasive window into the automatic construction of egocentric spatial maps and advance our understanding of how the human mind transforms everyday sensory experience into structured internal representations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 114,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12427",
        "abs_url": "https://arxiv.org/abs/2507.12427",
        "pdf_url": "https://arxiv.org/pdf/2507.12427",
        "title": "Unit-Based Histopathology Tissue Segmentation via Multi-Level Feature Representation",
        "authors": [
            "Ashkan Shakarami",
            "Azade Farshad",
            "Yousef Yeganeh",
            "Lorenzo Nicole",
            "Peter Schuffler",
            "Stefano Ghidoni",
            "Nassir Navab"
        ],
        "comments": "12 pages, 6 figures",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "We propose UTS, a unit-based tissue segmentation framework for histopathology that classifies each fixed-size 32 * 32 tile, rather than each pixel, as the segmentation unit. This approach reduces annotation effort and improves computational efficiency without compromising accuracy. To implement this approach, we introduce a Multi-Level Vision Transformer (L-ViT), which benefits the multi-level feature representation to capture both fine-grained morphology and global tissue context. Trained to segment breast tissue into three categories (infiltrating tumor, non-neoplastic stroma, and fat), UTS supports clinically relevant tasks such as tumor-stroma quantification and surgical margin assessment. Evaluated on 386,371 tiles from 459 H&E-stained regions, it outperforms U-Net variants and transformer-based baselines. Code and Dataset will be available at GitHub.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 115,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-17?abs=True",
        "arxiv_id": "2507.12440",
        "abs_url": "https://arxiv.org/abs/2507.12440",
        "pdf_url": "https://arxiv.org/pdf/2507.12440",
        "title": "EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos",
        "authors": [
            "Ruihan Yang",
            "Qinxi Yu",
            "Yecheng Wu",
            "Rui Yan",
            "Borui Li",
            "An-Chieh Cheng",
            "Xueyan Zou",
            "Yunhao Fang",
            "Hongxu Yin",
            "Sifei Liu",
            "Song Han",
            "Yao Lu",
            "Xiaolong Wang"
        ],
        "comments": "More videos can be found on our website: this https URL",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Real robot data collection for imitation learning has led to significant advancements in robotic manipulation. However, the requirement for robot hardware in the process fundamentally constrains the scale of the data. In this paper, we explore training Vision-Language-Action (VLA) models using egocentric human videos. The benefit of using human videos is not only for their scale but more importantly for the richness of scenes and tasks. With a VLA trained on human video that predicts human wrist and hand actions, we can perform Inverse Kinematics and retargeting to convert the human actions to robot actions. We fine-tune the model using a few robot manipulation demonstrations to obtain the robot policy, namely EgoVLA. We propose a simulation benchmark called Ego Humanoid Manipulation Benchmark, where we design diverse bimanual manipulation tasks with demonstrations. We fine-tune and evaluate EgoVLA with Ego Humanoid Manipulation Benchmark and show significant improvements over baselines and ablate the importance of human data. Videos can be found on our website: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    }
]