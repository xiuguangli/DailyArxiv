[
    {
        "order": 1,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05300",
        "abs_url": "https://arxiv.org/abs/2601.05300",
        "pdf_url": "https://arxiv.org/pdf/2601.05300",
        "title": "TIME: Temporally Intelligent Meta-reasoning Engine for Context Triggered Explicit Reasoning",
        "authors": [
            "Susmit Das"
        ],
        "comments": "14 pages, 3 figures with 27 page appendix. See this https URL and this https URL for associated code",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
        "abstract": "Reasoning oriented large language models often expose explicit \"thinking\" as long, turn-global traces at the start of every response, either always on or toggled externally at inference time. While useful for arithmetic, programming, and problem solving, this design is costly, blurs claim level auditability, and cannot re-trigger explicit reasoning once the model begins presenting. Dialogue models are also largely blind to temporal structure, treating replies after seconds and replies after weeks as equivalent unless time is stated in text. We introduce TIME, the Temporally Intelligent Meta-reasoning Engine, a behavioral alignment framework that treats explicit reasoning as a context sensitive resource driven by discourse and temporal cues. TIME augments dialogue with optional ISO 8601 <time> tags, tick turns that represent silent gaps, and short <think> blocks that can appear anywhere in a reply. A four-phase curriculum including a small, maximally diverse full-batch alignment step trains Qwen3 dense models to invoke brief, in-place reasoning bursts and keep user facing text compact. We evaluate with TIMEBench, a temporally grounded dialogue benchmark probing chronology, commonsense under gaps and offsets, anomaly detection, and continuity. Across 4B to 32B scales, TIME improves TIMEBench scores over base Qwen3 in both thinking and no-thinking modes while reducing reasoning tokens by about an order of magnitude. Our training data and code are available at this https URL and TIMEBench is available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 2,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05304",
        "abs_url": "https://arxiv.org/abs/2601.05304",
        "pdf_url": "https://arxiv.org/pdf/2601.05304",
        "title": "Ontology Neural Networks for Topologically Conditioned Constraint Satisfaction",
        "authors": [
            "Jaehong Oh"
        ],
        "comments": "12 pages, 11 figures",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Neuro-symbolic reasoning systems face fundamental challenges in maintaining semantic coherence while satisfying physical and logical constraints. Building upon our previous work on Ontology Neural Networks, we present an enhanced framework that integrates topological conditioning with gradient stabilization mechanisms. The approach employs Forman-Ricci curvature to capture graph topology, Deep Delta Learning for stable rank-one perturbations during constraint projection, and Covariance Matrix Adaptation Evolution Strategy for parameter optimization. Experimental evaluation across multiple problem sizes demonstrates that the method achieves mean energy reduction to 1.15 compared to baseline values of 11.68, with 95 percent success rate in constraint satisfaction tasks. The framework exhibits seed-independent convergence and graceful scaling behavior up to twenty-node problems, suggesting that topological structure can inform gradient-based optimization without sacrificing interpretability or computational efficiency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 3,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05352",
        "abs_url": "https://arxiv.org/abs/2601.05352",
        "pdf_url": "https://arxiv.org/pdf/2601.05352",
        "title": "When the Server Steps In: Calibrated Updates for Fair Federated Learning",
        "authors": [
            "Tianrun Yu",
            "Kaixiang Zhao",
            "Cheng Zhang",
            "Anjun Gao",
            "Yueyang Quan",
            "Zhuqing Liu",
            "Minghong Fang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Cryptography and Security (cs.CR); Information Retrieval (cs.IR); Social and Information Networks (cs.SI)",
        "abstract": "Federated learning (FL) has emerged as a transformative distributed learning paradigm, enabling multiple clients to collaboratively train a global model under the coordination of a central server without sharing their raw training data. While FL offers notable advantages, it faces critical challenges in ensuring fairness across diverse demographic groups. To address these fairness concerns, various fairness-aware debiasing methods have been proposed. However, many of these approaches either require modifications to clients' training protocols or lack flexibility in their aggregation strategies. In this work, we address these limitations by introducing EquFL, a novel server-side debiasing method designed to mitigate bias in FL systems. EquFL operates by allowing the server to generate a single calibrated update after receiving model updates from the clients. This calibrated update is then integrated with the aggregated client updates to produce an adjusted global model that reduces bias. Theoretically, we establish that EquFL converges to the optimal global model achieved by FedAvg and effectively reduces fairness loss over training rounds. Empirically, we demonstrate that EquFL significantly mitigates bias within the system, showcasing its practical effectiveness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 4,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05353",
        "abs_url": "https://arxiv.org/abs/2601.05353",
        "pdf_url": "https://arxiv.org/pdf/2601.05353",
        "title": "GlyRAG: Context-Aware Retrieval-Augmented Framework for Blood Glucose Forecasting",
        "authors": [
            "Shovito Barua Soumma",
            "Hassan Ghasemzadeh"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Information Theory (cs.IT)",
        "abstract": "Accurate forecasting of blood glucose from CGM is essential for preventing dysglycemic events, thus enabling proactive diabetes management. However, current forecasting models treat blood glucose readings captured using CGMs as a numerical sequence, either ignoring context or relying on additional sensors/modalities that are difficult to collect and deploy at scale. Recently, LLMs have shown promise for time-series forecasting tasks, yet their role as agentic context extractors in diabetes care remains largely unexplored. To address these limitations, we propose GlyRAG, a context-aware, retrieval-augmented forecasting framework that derives semantic understanding of blood glucose dynamics directly from CGM traces without requiring additional sensor modalities. GlyRAG employs an LLM as a contextualization agent to generate clinical summaries. These summaries are embedded by a language model and fused with patch-based glucose representations in a multimodal transformer architecture with a cross translation loss aligining textual and physiological embeddings. A retrieval module then identifies similar historical episodes in the learned embedding space and uses cross-attention to integrate these case-based analogues prior to making a forecasting inference. Extensive evaluations on two T1D cohorts show that GlyRAG consistently outperforms state-of-the art methods, achieving up to 39% lower RMSE and a further 1.7% reduction in RMSE over the baseline. Clinical evaluation shows that GlyRAG places 85% predictions in safe zones and achieves 51% improvement in predicting dysglycemic events across both cohorts. These results indicate that LLM-based contextualization and retrieval over CGM traces can enhance the accuracy and clinical reliability of long-horizon glucose forecasting without the need for extra sensors, thus supporting future agentic decision-support tools for diabetes management.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 5,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05371",
        "abs_url": "https://arxiv.org/abs/2601.05371",
        "pdf_url": "https://arxiv.org/pdf/2601.05371",
        "title": "The Kernel Manifold: A Geometric Approach to Gaussian Process Model Selection",
        "authors": [
            "Md Shafiqul Islam",
            "Shakti Prasad Padhy",
            "Douglas Allaire",
            "Raymundo Arróyave"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Methodology (stat.ME)",
        "abstract": "Gaussian Process (GP) regression is a powerful nonparametric Bayesian framework, but its performance depends critically on the choice of covariance kernel. Selecting an appropriate kernel is therefore central to model quality, yet remains one of the most challenging and computationally expensive steps in probabilistic modeling. We present a Bayesian optimization framework built on kernel-of-kernels geometry, using expected divergence-based distances between GP priors to explore kernel space efficiently. A multidimensional scaling (MDS) embedding of this distance matrix maps a discrete kernel library into a continuous Euclidean manifold, enabling smooth BO. In this formulation, the input space comprises kernel compositions, the objective is the log marginal likelihood, and featurization is given by the MDS coordinates. When the divergence yields a valid metric, the embedding preserves geometry and produces a stable BO landscape. We demonstrate the approach on synthetic benchmarks, real-world time-series datasets, and an additive manufacturing case study predicting melt-pool geometry, achieving superior predictive accuracy and uncertainty calibration relative to baselines including Large Language Model (LLM)-guided search. This framework establishes a reusable probabilistic geometry for kernel search, with direct relevance to GP modeling and deep kernel learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 6,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05378",
        "abs_url": "https://arxiv.org/abs/2601.05378",
        "pdf_url": "https://arxiv.org/pdf/2601.05378",
        "title": "Inverting Non-Injective Functions with Twin Neural Network Regression",
        "authors": [
            "Sebastian J. Wetzel"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Robotics (cs.RO)",
        "abstract": "Non-injective functions are not invertible. However, non-injective functions can be restricted to sub-domains on which they are locally injective and surjective and thus invertible if the dimensionality between input and output spaces are the same. Further, even if the dimensionalities do not match it is often possible to choose a preferred solution from many possible solutions. Twin neural network regression is naturally capable of incorporating these properties to invert non-injective functions. Twin neural network regression is trained to predict adjustments to well known input variables $\\mathbf{x}^{\\text{anchor}}$ to obtain an estimate for an unknown $\\mathbf{x}^{\\text{new}}$ under a change of the target variable from $\\mathbf{y}^{\\text{anchor}}$ to $\\mathbf{y}^{\\text{new}}$. In combination with k-nearest neighbor search, I propose a deterministic framework that finds input parameters to a given target variable of non-injective functions. The method is demonstrated by inverting non-injective functions describing toy problems and robot arm control that are a) defined by data or b) known as mathematical formula.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 7,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05383",
        "abs_url": "https://arxiv.org/abs/2601.05383",
        "pdf_url": "https://arxiv.org/pdf/2601.05383",
        "title": "Imitation Learning for Combinatorial Optimisation under Uncertainty",
        "authors": [
            "Prakash Gawas",
            "Antoine Legrain",
            "Louis-Martin Rousseau"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Optimization and Control (math.OC)",
        "abstract": "Imitation learning (IL) provides a data-driven framework for approximating policies for large-scale combinatorial optimisation problems formulated as sequential decision problems (SDPs), where exact solution methods are computationally intractable. A central but underexplored aspect of IL in this context is the role of the \\emph{expert} that generates training demonstrations. Existing studies employ a wide range of expert constructions, yet lack a unifying framework to characterise their modelling assumptions, computational properties, and impact on learning performance. This paper introduces a systematic taxonomy of experts for IL in combinatorial optimisation under uncertainty. Experts are classified along three dimensions: (i) their treatment of uncertainty, including myopic, deterministic, full-information, two-stage stochastic, and multi-stage stochastic formulations; (ii) their level of optimality, distinguishing task-optimal and approximate experts; and (iii) their interaction mode with the learner, ranging from one-shot supervision to iterative, interactive schemes. Building on this taxonomy, we propose a generalised Dataset Aggregation (DAgger) algorithm that supports multiple expert queries, expert aggregation, and flexible interaction strategies. The proposed framework is evaluated on a dynamic physician-to-patient assignment problem with stochastic arrivals and capacity constraints. Computational experiments compare learning outcomes across expert types and interaction regimes. The results show that policies learned from stochastic experts consistently outperform those learned from deterministic or full-information experts, while interactive learning improves solution quality using fewer expert demonstrations. Aggregated deterministic experts provide an effective alternative when stochastic optimisation becomes computationally challenging.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 8,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05391",
        "abs_url": "https://arxiv.org/abs/2601.05391",
        "pdf_url": "https://arxiv.org/pdf/2601.05391",
        "title": "DynaSTy: A Framework for SpatioTemporal Node Attribute Prediction in Dynamic Graphs",
        "authors": [
            "Namrata Banerji",
            "Tanya Berger-Wolf"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Accurate multistep forecasting of node-level attributes on dynamic graphs is critical for applications ranging from financial trust networks to biological networks. Existing spatiotemporal graph neural networks typically assume a static adjacency matrix. In this work, we propose an end-to-end dynamic edge-biased spatiotemporal model that ingests a multi-dimensional timeseries of node attributes and a timeseries of adjacency matrices, to predict multiple future steps of node attributes. At each time step, our transformer-based model injects the given adjacency as an adaptable attention bias, allowing the model to focus on relevant neighbors as the graph evolves. We further deploy a masked node-time pretraining objective that primes the encoder to reconstruct missing features, and train with scheduled sampling and a horizon-weighted loss to mitigate compounding error over long horizons. Unlike prior work, our model accommodates dynamic graphs that vary across input samples, enabling forecasting in multi-system settings such as brain networks across different subjects, financial systems in different contexts, or evolving social systems. Empirical results demonstrate that our method consistently outperforms strong baselines on Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 9,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05407",
        "abs_url": "https://arxiv.org/abs/2601.05407",
        "pdf_url": "https://arxiv.org/pdf/2601.05407",
        "title": "Interactive Distillation for Cooperative Multi-Agent Reinforcement Learning",
        "authors": [
            "Minwoo Cho",
            "Batuhan Altundas",
            "Matthew Gombolay"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Multiagent Systems (cs.MA)",
        "abstract": "Knowledge distillation (KD) has the potential to accelerate MARL by employing a centralized teacher for decentralized students but faces key bottlenecks. Specifically, there are (1) challenges in synthesizing high-performing teaching policies in complex domains, (2) difficulties when teachers must reason in out-of-distribution (OOD) states, and (3) mismatches between the decentralized students' and the centralized teacher's observation spaces. To address these limitations, we propose HINT (Hierarchical INteractive Teacher-based transfer), a novel KD framework for MARL in a centralized training, decentralized execution setup. By leveraging hierarchical RL, HINT provides a scalable, high-performing teacher. Our key innovation, pseudo off-policy RL, enables the teacher policy to be updated using both teacher and student experience, thereby improving OOD adaptation. HINT also applies performance-based filtering to retain only outcome-relevant guidance, reducing observation mismatches. We evaluate HINT on challenging cooperative domains (e.g., FireCommander for resource allocation, MARINE for tactical combat). Across these benchmarks, HINT outperforms baselines, achieving improvements of 60% to 165% in success rate.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 10,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05420",
        "abs_url": "https://arxiv.org/abs/2601.05420",
        "pdf_url": "https://arxiv.org/pdf/2601.05420",
        "title": "Efficient Inference for Noisy LLM-as-a-Judge Evaluation",
        "authors": [
            "Yiqun T Chen",
            "Sizhu Lu",
            "Sijia Li",
            "Moran Guo",
            "Shengyi Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Applications (stat.AP); Methodology (stat.ME)",
        "abstract": "Large language models (LLMs) are increasingly used as automatic evaluators of generative AI outputs, a paradigm often referred to as \"LLM-as-a-judge.\" In practice, LLM judges are imperfect predictions for the underlying truth and can exhibit systematic, non-random errors. Two main approaches have recently been proposed to address this issue: (i) direct measurementerror correction based on misclassification models such as Rogan-Gladen-style estimators, and (ii) surrogate-outcome approaches such as prediction-powered inference (PPI), which correct bias by calibrating prediction residuals on a small set of gold-standard human labels. In this paper, we systematically study the performance of these two approaches for estimating mean parameters (e.g., average benchmark scores or pairwise win rates). Leveraging tools from semiparametric efficiency theory, we unify the two classes of estimators by deriving explicit forms of efficient influence function (EIF)-based efficient estimators and characterize conditions under which PPI-style estimators attain strictly smaller asymptotic variance than measurement-error corrections. We verify our theoretical results in simulations and demonstrate the methods on real-data examples. We provide an implementation of the benchmarked methods and comparison utilities at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 11,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05431",
        "abs_url": "https://arxiv.org/abs/2601.05431",
        "pdf_url": "https://arxiv.org/pdf/2601.05431",
        "title": "Prediction of Fault Slip Tendency in CO${_2}$ Storage using Data-space Inversion",
        "authors": [
            "Xiaowen He",
            "Su Jiang",
            "Louis J. Durlofsky"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Accurately assessing the potential for fault slip is essential in many subsurface operations. Conventional model-based history matching methods, which entail the generation of posterior geomodels calibrated to observed data, can be challenging to apply in coupled flow-geomechanics problems with faults. In this work, we implement a variational autoencoder (VAE)-based data-space inversion (DSI) framework to predict pressure, stress and strain fields, and fault slip tendency, in CO${_2}$ storage projects. The main computations required by the DSI workflow entail the simulation of O(1000) prior geomodels. The posterior distributions for quantities of interest are then inferred directly from prior simulation results and observed data, without the need to generate posterior geomodels. The model used here involves a synthetic 3D system with two faults. Realizations of heterogeneous permeability and porosity fields are generated using geostatistical software, and uncertain geomechanical and fault parameters are sampled for each realization from prior distributions. Coupled flow-geomechanics simulations for these geomodels are conducted using GEOS. A VAE with stacked convolutional long short-term memory layers is trained, using the prior simulation results, to represent pressure, strain, effective normal stress and shear stress fields in terms of latent variables. The VAE parameterization is used with DSI for posterior predictions, with monitoring wells providing observed pressure and strain data. Posterior results for synthetic true models demonstrate that the DSI-VAE framework gives accurate predictions for pressure, strain, and stress fields and for fault slip tendency. The framework is also shown to reduce uncertainty in key geomechanical and fault parameters.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 12,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05451",
        "abs_url": "https://arxiv.org/abs/2601.05451",
        "pdf_url": "https://arxiv.org/pdf/2601.05451",
        "title": "RingSQL: Generating Synthetic Data with Schema-Independent Templates for Text-to-SQL Reasoning Models",
        "authors": [
            "Marko Sterbentz",
            "Kevin Cushing",
            "Cameron Barrie",
            "Kristian J. Hammond"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
        "abstract": "Recent advances in text-to-SQL systems have been driven by larger models and improved datasets, yet progress is still limited by the scarcity of high-quality training data. Manual data creation is expensive, and existing synthetic methods trade off reliability and scalability. Template-based approaches ensure correct SQL but require schema-specific templates, while LLM-based generation scales easily but lacks quality and correctness guarantees. We introduce RingSQL, a hybrid data generation framework that combines schema-independent query templates with LLM-based paraphrasing of natural language questions. This approach preserves SQL correctness across diverse schemas while providing broad linguistic variety. In our experiments, we find that models trained using data produced by RingSQL achieve an average gain in accuracy of +2.3% across six text-to-SQL benchmarks when compared to models trained on other synthetic data. We make our code available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 13,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05475",
        "abs_url": "https://arxiv.org/abs/2601.05475",
        "pdf_url": "https://arxiv.org/pdf/2601.05475",
        "title": "MaxCode: A Max-Reward Reinforcement Learning Framework for Automated Code Optimization",
        "authors": [
            "Jiefu Ou",
            "Sapana Chaudhary",
            "Kaj Bostrom",
            "Nathaniel Weir",
            "Shuai Zhang",
            "Huzefa Rangwala",
            "George Karypis"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
        "abstract": "Large Language Models (LLMs) demonstrate strong capabilities in general coding tasks but encounter two key challenges when optimizing code: (i) the complexity of writing optimized code (such as performant CUDA kernels and competition-level CPU code) requires expertise in systems, algorithms and specific languages and (ii) requires interpretation of performance metrics like timing and device utilization beyond binary correctness. In this work, we explore inference-time search algorithms that guide the LLM to discover better solutions through iterative refinement based on execution feedback. Our approach, called MaxCode unifies existing search methods under a max-reward reinforcement learning framework, making the observation and action-value functions modular for modification. To enhance the observation space, we integrate a natural language critique model that converts raw execution feedback into diagnostic insights about errors and performance bottlenecks, and the best-discounted reward seen so far. Together, these provide richer input to the code proposal function. To improve exploration during search, we train a generative reward-to-go model using action values from rollouts to rerank potential solutions. Testing on the KernelBench (CUDA) and PIE (C++) optimization benchmarks shows that MaxCode improves optimized code performance compared to baselines, achieving 20.3% and 10.1% relative improvements in absolute speedup value and relative speedup ranking, respectively.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 14,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05501",
        "abs_url": "https://arxiv.org/abs/2601.05501",
        "pdf_url": "https://arxiv.org/pdf/2601.05501",
        "title": "Hi-ZFO: Hierarchical Zeroth- and First-Order LLM Fine-Tuning via Importance-Guided Tensor Selection",
        "authors": [
            "Feihu Jin",
            "Ying Tan"
        ],
        "comments": "13 pages, 4 figures",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
        "abstract": "Fine-tuning large language models (LLMs) using standard first-order (FO) optimization often drives training toward sharp, poorly generalizing minima. Conversely, zeroth-order (ZO) methods offer stronger exploratory behavior without relying on explicit gradients, yet suffer from slow convergence. More critically, our analysis reveals that in generative tasks, the vast output and search space significantly amplify estimation variance, rendering ZO methods both noisy and inefficient. To address these challenges, we propose \\textbf{Hi-ZFO} (\\textbf{Hi}erarchical \\textbf{Z}eroth- and \\textbf{F}irst-\\textbf{O}rder optimization), a hybrid framework designed to synergize the precision of FO gradients with the exploratory capability of ZO estimation. Hi-ZFO adaptively partitions the model through layer-wise importance profiling, applying precise FO updates to critical layers while leveraging ZO optimization for less sensitive ones. Notably, ZO in Hi-ZFO is not merely a memory-saving surrogate; it is intentionally introduced as a source of \"beneficial stochasticity\" to help the model escape the local minima where pure FO optimization tends to stagnate. Validated across diverse generative, mathematical, and code reasoning tasks, Hi-ZFO consistently achieves superior performance while significantly reducing the training time. These results demonstrate the effectiveness of hierarchical hybrid optimization for LLM fine-tuning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 15,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05521",
        "abs_url": "https://arxiv.org/abs/2601.05521",
        "pdf_url": "https://arxiv.org/pdf/2601.05521",
        "title": "Toward an Integrated Cross-Urban Accident Prevention System: A Multi-Task Spatial-Temporal Learning Framework for Urban Safety Management",
        "authors": [
            "Jiayu Fang",
            "Zhiqi Shao",
            "Haoning Xi",
            "Boris Choy",
            "Junbin Gao"
        ],
        "comments": "38pages, 18figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The development of a cross-city accident prevention system is particularly challenging due to the heterogeneity, inconsistent reporting, and inherently clustered, sparse, cyclical, and noisy nature of urban accident data. These intrinsic data properties, combined with fragmented governance and incompatible reporting standards, have long hindered the creation of an integrated, cross-city accident prevention framework. To address this gap, we propose the Mamba Local-ttention Spatial-Temporal Network MLA-STNet, a unified system that formulates accident risk prediction as a multi-task learning problem across multiple cities. MLA-STNet integrates two complementary modules: (i)the Spatio-Temporal Geographical Mamba-Attention (STG-MA), which suppresses unstable spatio-temporal fluctuations and strengthens long-range temporal dependencies; and (ii) the Spatio-Temporal Semantic Mamba-Attention (STS-MA), which mitigates cross-city heterogeneity through a shared-parameter design that jointly trains all cities while preserving individual semantic representation spaces. We validate the proposed framework through 75 experiments under two forecasting scenarios, full-day and high-frequency accident periods, using real-world datasets from New York City and Chicago. Compared with the state-of-the-art baselines, MLA-STNet achieves up to 6% lower RMSE, 8% higher Recall, and 5% higher MAP, while maintaining less than 1% performance variation under 50% input noise. These results demonstrate that MLA-STNet effectively unifies heterogeneous urban datasets within a scalable, robust, and interpretable Cross-City Accident Prevention System, paving the way for coordinated and data-driven urban safety management.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 16,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05537",
        "abs_url": "https://arxiv.org/abs/2601.05537",
        "pdf_url": "https://arxiv.org/pdf/2601.05537",
        "title": "Scalable Heterogeneous Graph Learning via Heterogeneous-aware Orthogonal Prototype Experts",
        "authors": [
            "Wei Zhou",
            "Hong Huang",
            "Ruize Shi",
            "Bang Liu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Heterogeneous Graph Neural Networks(HGNNs) have advanced mainly through better encoders, yet their decoding/projection stage still relies on a single shared linear head, assuming it can map rich node embeddings to labels. We call this the Linear Projection Bottleneck: in heterogeneous graphs, contextual diversity and long-tail shifts make a global head miss fine semantics, overfit hub nodes, and underserve tail nodes. While Mixture-of-Experts(MoE) could help, naively applying it clashes with structural imbalance and risks expert collapse. We propose a Heterogeneous-aware Orthogonal Prototype Experts framework named HOPE, a plug-and-play replacement for the standard prediction head. HOPE uses learnable prototype-based routing to assign instances to experts by similarity, letting expert usage follow the natural long-tail distribution, and adds expert orthogonalization to encourage diversity and prevent collapse. Experiments on four real datasets show consistent gains across SOTA HGNN backbones with minimal overhead.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 17,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05544",
        "abs_url": "https://arxiv.org/abs/2601.05544",
        "pdf_url": "https://arxiv.org/pdf/2601.05544",
        "title": "Buffered AUC maximization for scoring systems via mixed-integer optimization",
        "authors": [
            "Moe Shiina",
            "Shunnosuke Ikeda",
            "Yuichi Takano"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Optimization and Control (math.OC); Applications (stat.AP)",
        "abstract": "A scoring system is a linear classifier composed of a small number of explanatory variables, each assigned a small integer coefficient. This system is highly interpretable and allows predictions to be made with simple manual calculations without the need for a calculator. Several previous studies have used mixed-integer optimization (MIO) techniques to develop scoring systems for binary classification; however, they have not focused on directly maximizing AUC (i.e., area under the receiver operating characteristic curve), even though AUC is recognized as an essential evaluation metric for scoring systems. Our goal herein is to establish an effective MIO framework for constructing scoring systems that directly maximize the buffered AUC (bAUC) as the tightest concave lower bound on AUC. Our optimization model is formulated as a mixed-integer linear optimization (MILO) problem that maximizes bAUC subject to a group sparsity constraint for limiting the number of questions in the scoring system. Computational experiments using publicly available real-world datasets demonstrate that our MILO method can build scoring systems with superior AUC values compared to the baseline methods based on regularization and stepwise regression. This research contributes to the advancement of MIO techniques for developing highly interpretable classification models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 18,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05583",
        "abs_url": "https://arxiv.org/abs/2601.05583",
        "pdf_url": "https://arxiv.org/pdf/2601.05583",
        "title": "Learn to Evolve: Self-supervised Neural JKO Operator for Wasserstein Gradient Flow",
        "authors": [
            "Xue Feng",
            "Li Wang",
            "Deanna Needell",
            "Rongjie Lai"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Numerical Analysis (math.NA)",
        "abstract": "The Jordan-Kinderlehrer-Otto (JKO) scheme provides a stable variational framework for computing Wasserstein gradient flows, but its practical use is often limited by the high computational cost of repeatedly solving the JKO subproblems. We propose a self-supervised approach for learning a JKO solution operator without requiring numerical solutions of any JKO trajectories. The learned operator maps an input density directly to the minimizer of the corresponding JKO subproblem, and can be iteratively applied to efficiently generate the gradient-flow evolution. A key challenge is that only a number of initial densities are typically available for training. To address this, we introduce a Learn-to-Evolve algorithm that jointly learns the JKO operator and its induced trajectories by alternating between trajectory generation and operator updates. As training progresses, the generated data increasingly approximates true JKO trajectories. Meanwhile, this Learn-to-Evolve strategy serves as a natural form of data augmentation, significantly enhancing the generalization ability of the learned operator. Numerical experiments demonstrate the accuracy, stability, and robustness of the proposed method across various choices of energies and initial conditions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 19,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05586",
        "abs_url": "https://arxiv.org/abs/2601.05586",
        "pdf_url": "https://arxiv.org/pdf/2601.05586",
        "title": "Poisson Hyperplane Processes with Rectified Linear Units",
        "authors": [
            "Shufei Ge",
            "Shijia Wang",
            "Lloyd Elliott"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Methodology (stat.ME); Machine Learning (stat.ML)",
        "abstract": "Neural networks have shown state-of-the-art performances in various classification and regression tasks. Rectified linear units (ReLU) are often used as activation functions for the hidden layers in a neural network model. In this article, we establish the connection between the Poisson hyperplane processes (PHP) and two-layer ReLU neural networks. We show that the PHP with a Gaussian prior is an alternative probabilistic representation to a two-layer ReLU neural network. In addition, we show that a two-layer neural network constructed by PHP is scalable to large-scale problems via the decomposition propositions. Finally, we propose an annealed sequential Monte Carlo algorithm for Bayesian inference. Our numerical experiments demonstrate that our proposed method outperforms the classic two-layer ReLU neural network. The implementation of our proposed model is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 20,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05593",
        "abs_url": "https://arxiv.org/abs/2601.05593",
        "pdf_url": "https://arxiv.org/pdf/2601.05593",
        "title": "PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning",
        "authors": [
            "Jingcheng Hu",
            "Yinmin Zhang",
            "Shijie Shang",
            "Xiaobo Yang",
            "Yue Peng",
            "Zhewei Huang",
            "Hebin Zhou",
            "Xin Wu",
            "Jie Cheng",
            "Fanqi Wan",
            "Xiangwen Kong",
            "Chengyuan Yao",
            "Kaiwen Yan",
            "Ailin Huang",
            "Hongyu Zhou",
            "Qi Han",
            "Zheng Ge",
            "Daxin Jiang",
            "Xiangyu Zhang",
            "Heung-Yeung Shum"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We introduce Parallel Coordinated Reasoning (PaCoRe), a training-and-inference framework designed to overcome a central limitation of contemporary language models: their inability to scale test-time compute (TTC) far beyond sequential reasoning under a fixed context window. PaCoRe departs from the traditional sequential paradigm by driving TTC through massive parallel exploration coordinated via a message-passing architecture in multiple rounds. Each round launches many parallel reasoning trajectories, compacts their findings into context-bounded messages, and synthesizes these messages to guide the next round and ultimately produce the final answer. Trained end-to-end with large-scale, outcome-based reinforcement learning, the model masters the synthesis abilities required by PaCoRe and scales to multi-million-token effective TTC without exceeding context limits. The approach yields strong improvements across diverse domains, and notably pushes reasoning beyond frontier systems in mathematics: an 8B model reaches 94.5% on HMMT 2025, surpassing GPT-5's 93.2% by scaling effective TTC to roughly two million tokens. We open-source model checkpoints, training data, and the full inference pipeline to accelerate follow-up work.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 21,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05597",
        "abs_url": "https://arxiv.org/abs/2601.05597",
        "pdf_url": "https://arxiv.org/pdf/2601.05597",
        "title": "Good Allocations from Bad Estimates",
        "authors": [
            "Sílvia Casacuberta",
            "Moritz Hardt"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computers and Society (cs.CY)",
        "abstract": "Conditional average treatment effect (CATE) estimation is the de facto gold standard for targeting a treatment to a heterogeneous population. The method estimates treatment effects up to an error $\\epsilon > 0$ in each of $M$ different strata of the population, targeting individuals in decreasing order of estimated treatment effect until the budget runs out. In general, this method requires $O(M/\\epsilon^2)$ samples. This is best possible if the goal is to estimate all treatment effects up to an $\\epsilon$ error. In this work, we show how to achieve the same total treatment effect as CATE with only $O(M/\\epsilon)$ samples for natural distributions of treatment effects. The key insight is that coarse estimates suffice for near-optimal treatment allocations. In addition, we show that budget flexibility can further reduce the sample complexity of allocation. Finally, we evaluate our algorithm on various real-world RCT datasets. In all cases, it finds nearly optimal treatment allocations with surprisingly few samples. Our work highlights the fundamental distinction between treatment effect estimation and treatment allocation: the latter requires far fewer samples.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 22,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05607",
        "abs_url": "https://arxiv.org/abs/2601.05607",
        "pdf_url": "https://arxiv.org/pdf/2601.05607",
        "title": "Orchestrating Tokens and Sequences: Dynamic Hybrid Policy Optimization for RLVR",
        "authors": [
            "Zijun Min",
            "Bingshuai Liu",
            "Ante Wang",
            "Long Zhang",
            "Anxiang Zeng",
            "Haibo Zhang",
            "Jinsong Su"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) offers a promising framework for optimizing large language models in reasoning tasks. However, existing RLVR algorithms focus on different granularities, and each has complementary strengths and limitations. Group Relative Policy Optimization (GRPO) updates the policy with token-level importance ratios, which preserves fine-grained credit assignment but often suffers from high variance and instability. In contrast, Group Sequence Policy Optimization (GSPO) applies single sequence-level importance ratios across all tokens in a response that better matches sequence-level rewards, but sacrifices token-wise credit assignment. In this paper, we propose Dynamic Hybrid Policy Optimization (DHPO) to bridge GRPO and GSPO within a single clipped surrogate objective. DHPO combines token-level and sequence-level importance ratios using weighting mechanisms. We explore two variants of the mixing mechanism, including an averaged mixing and an entropy-guided mixing. To further stabilize training, we employ a branch-specific clipping strategy that constrains token-level and sequence-level ratios within separate trust regions before mixing, preventing outliers in either branch from dominating the update. Across seven challenging mathematical reasoning benchmarks, experiments on both dense and MoE models from the Qwen3 series show that DHPO consistently outperforms GRPO and GSPO. We will release our code upon acceptance of this paper.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 23,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05616",
        "abs_url": "https://arxiv.org/abs/2601.05616",
        "pdf_url": "https://arxiv.org/pdf/2601.05616",
        "title": "Dual-Phase LLM Reasoning: Self-Evolved Mathematical Frameworks",
        "authors": [
            "ShaoZhen Liu",
            "Xinting Huang",
            "Houwen Peng",
            "Xin Chen",
            "Xinyang Song",
            "Qi Li",
            "Zhenan Sun"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In recent years, large language models (LLMs) have demonstrated significant potential in complex reasoning tasks like mathematical problem-solving. However, existing research predominantly relies on reinforcement learning (RL) frameworks while overlooking supervised fine-tuning (SFT) methods. This paper proposes a new two-stage training framework that enhances models' self-correction capabilities through self-generated long chain-of-thought (CoT) data. During the first stage, a multi-turn dialogue strategy guides the model to generate CoT data incorporating verification, backtracking, subgoal decomposition, and backward reasoning, with predefined rules filtering high-quality samples for supervised fine-tuning. The second stage employs a difficulty-aware rejection sampling mechanism to dynamically optimize data distribution, strengthening the model's ability to handle complex problems. The approach generates reasoning chains extended over 4 times longer while maintaining strong scalability, proving that SFT effectively activates models' intrinsic reasoning capabilities and provides a resource-efficient pathway for complex task optimization. Experimental results demonstrate performance improvements on mathematical benchmarks including GSM8K and MATH500, with the fine-tuned model achieving a substantial improvement on competition-level problems like AIME24. Code will be open-sourced.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 24,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05647",
        "abs_url": "https://arxiv.org/abs/2601.05647",
        "pdf_url": "https://arxiv.org/pdf/2601.05647",
        "title": "Transformer Is Inherently a Causal Learner",
        "authors": [
            "Xinyue Wang",
            "Stephen Wang",
            "Biwei Huang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "We reveal that transformers trained in an autoregressive manner naturally encode time-delayed causal structures in their learned representations. When predicting future values in multivariate time series, the gradient sensitivities of transformer outputs with respect to past inputs directly recover the underlying causal graph, without any explicit causal objectives or structural constraints. We prove this connection theoretically under standard identifiability conditions and develop a practical extraction method using aggregated gradient attributions. On challenging cases such as nonlinear dynamics, long-term dependencies, and non-stationary systems, this approach greatly surpasses the performance of state-of-the-art discovery algorithms, especially as data heterogeneity increases, exhibiting scaling potential where causal accuracy improves with data volume and heterogeneity, a property traditional methods lack. This unifying view lays the groundwork for a future paradigm where causal discovery operates through the lens of foundation models, and foundation models gain interpretability and enhancement through the lens of causality.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 25,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05650",
        "abs_url": "https://arxiv.org/abs/2601.05650",
        "pdf_url": "https://arxiv.org/pdf/2601.05650",
        "title": "From Global to Local: Cluster-Aware Learning for Wi-Fi Fingerprinting Indoor Localisation",
        "authors": [
            "Miguel Matey-Sanz",
            "Joaquín Torres-Sospedra",
            "Joaquín Huerta",
            "Sergio Trilles"
        ],
        "comments": "20 pages, 9 figures, 6 tables",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Wi-Fi fingerprinting remains one of the most practical solutions for indoor positioning, however, its performance is often limited by the size and heterogeneity of fingerprint datasets, strong Received Signal Strength Indicator variability, and the ambiguity introduced in large and multi-floor environments. These factors significantly degrade localisation accuracy, particularly when global models are applied without considering structural constraints. This paper introduces a clustering-based method that structures the fingerprint dataset prior to localisation. Fingerprints are grouped using either spatial or radio features, and clustering can be applied at the building or floor level. In the localisation phase, a clustering estimation procedure based on the strongest access points assigns unseen fingerprints to the most relevant cluster. Localisation is then performed only within the selected clusters, allowing learning models to operate on reduced and more coherent subsets of data. The effectiveness of the method is evaluated on three public datasets and several machine learning models. Results show a consistent reduction in localisation errors, particularly under building-level strategies, but at the cost of reducing the floor detection accuracy. These results demonstrate that explicitly structuring datasets through clustering is an effective and flexible approach for scalable indoor positioning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 26,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05679",
        "abs_url": "https://arxiv.org/abs/2601.05679",
        "pdf_url": "https://arxiv.org/pdf/2601.05679",
        "title": "Do Sparse Autoencoders Identify Reasoning Features in Language Models?",
        "authors": [
            "George Ma",
            "Zhongyuan Liang",
            "Irene Y. Chen",
            "Somayeh Sojoudi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We investigate whether sparse autoencoders (SAEs) identify genuine reasoning features in large language models (LLMs). Starting from features selected using standard contrastive activation methods, we introduce a falsification-oriented framework that combines causal token injection experiments and LLM-guided falsification to test whether feature activation reflects reasoning processes or superficial linguistic correlates. Across 20 configurations spanning multiple model families, layers, and reasoning datasets, we find that identified reasoning features are highly sensitive to token-level interventions. Injecting a small number of feature-associated tokens into non-reasoning text is sufficient to elicit strong activation for 59% to 94% of features, indicating reliance on lexical artifacts. For the remaining features that are not explained by simple token triggers, LLM-guided falsification consistently produces non-reasoning inputs that activate the feature and reasoning inputs that do not, with no analyzed feature satisfying our criteria for genuine reasoning behavior. Steering these features yields minimal changes or slight degradations in benchmark performance. Together, these results suggest that SAE features identified by contrastive approaches primarily capture linguistic correlates of reasoning rather than the underlying reasoning computations themselves.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 27,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05684",
        "abs_url": "https://arxiv.org/abs/2601.05684",
        "pdf_url": "https://arxiv.org/pdf/2601.05684",
        "title": "FLRQ: Faster LLM Quantization with Flexible Low-Rank Matrix Sketching",
        "authors": [
            "Hongyaoxing Gul",
            "Lijuan Hu",
            "Shuzi Niu",
            "Fangfang Liu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Traditional post-training quantization (PTQ) is considered an effective approach to reduce model size and accelerate inference of large-scale language models (LLMs). However, existing low-rank PTQ methods require costly fine-tuning to determine a compromise rank for diverse data and layers in large models, failing to exploit their full potential. Additionally, the current SVD-based low-rank approximation compounds the computational overhead. In this work, we thoroughly analyze the varying effectiveness of low-rank approximation across different layers in representative models. Accordingly, we introduce \\underline{F}lexible \\underline{L}ow-\\underline{R}ank \\underline{Q}uantization (FLRQ), a novel solution designed to quickly identify the accuracy-optimal ranks and aggregate them to achieve minimal storage combinations. FLRQ comprises two powerful components, Rank1-Sketch-based Flexible Rank Selection (R1-FLR) and Best Low-rank Approximation under Clipping (BLC). R1-FLR applies the R1-Sketch with Gaussian projection for the fast low-rank approximation, enabling outlier-aware rank extraction for each layer. Meanwhile, BLC aims at minimizing the low-rank quantization error under the scaling and clipping strategy through an iterative method. FLRQ demonstrates strong effectiveness and robustness in comprehensive experiments, achieving state-of-the-art performance in both quantization quality and algorithm efficiency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 28,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05759",
        "abs_url": "https://arxiv.org/abs/2601.05759",
        "pdf_url": "https://arxiv.org/pdf/2601.05759",
        "title": "Variational Autoencoders for P-wave Detection on Strong Motion Earthquake Spectrograms",
        "authors": [
            "Turkan Simge Ispak",
            "Salih Tileylioglu",
            "Erdem Akagunduz"
        ],
        "comments": "13 pages, 8 figures, 3 tables",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Accurate P-wave detection is critical for earthquake early warning, yet strong-motion records pose challenges due to high noise levels, limited labeled data, and complex waveform characteristics. This study reframes P-wave arrival detection as a self-supervised anomaly detection task to evaluate how architectural variations regulate the trade-off between reconstruction fidelity and anomaly discrimination. Through a comprehensive grid search of 492 Variational Autoencoder configurations, we show that while skip connections minimize reconstruction error (Mean Absolute Error approximately 0.0012), they induce \"overgeneralization\", allowing the model to reconstruct noise and masking the detection signal. In contrast, attention mechanisms prioritize global context over local detail and yield the highest detection performance with an area-under-the-curve of 0.875. The attention-based Variational Autoencoder achieves an area-under-the-curve of 0.91 in the 0 to 40-kilometer near-source range, demonstrating high suitability for immediate early warning applications. These findings establish that architectural constraints favoring global context over pixel-perfect reconstruction are essential for robust, self-supervised P-wave detection.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 29,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05770",
        "abs_url": "https://arxiv.org/abs/2601.05770",
        "pdf_url": "https://arxiv.org/pdf/2601.05770",
        "title": "Weights to Code: Extracting Interpretable Algorithms from the Discrete Transformer",
        "authors": [
            "Yifan Zhang",
            "Wei Bi",
            "Kechi Zhang",
            "Dongming Jin",
            "Jie Fu",
            "Zhi Jin"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
        "abstract": "Algorithm extraction aims to synthesize executable programs directly from models trained on specific algorithmic tasks, enabling de novo algorithm discovery without relying on human-written code. However, extending this paradigm to Transformer is hindered by superposition, where entangled features encoded in overlapping directions obstruct the extraction of symbolic expressions. In this work, we propose the Discrete Transformer, an architecture explicitly engineered to bridge the gap between continuous representations and discrete symbolic logic. By enforcing a strict functional disentanglement, which constrains Numerical Attention to information routing and Numerical MLP to element-wise arithmetic, and employing temperature-annealed sampling, our method effectively facilitates the extraction of human-readable programs. Empirically, the Discrete Transformer not only achieves performance comparable to RNN-based baselines but crucially extends interpretability to continuous variable domains. Moreover, our analysis of the annealing process shows that the efficient discrete search undergoes a clear phase transition from exploration to exploitation. We further demonstrate that our method enables fine-grained control over synthesized programs by imposing inductive biases. Collectively, these findings establish the Discrete Transformer as a robust framework for demonstration-free algorithm discovery, offering a rigorous pathway toward Transformer interpretability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 30,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05807",
        "abs_url": "https://arxiv.org/abs/2601.05807",
        "pdf_url": "https://arxiv.org/pdf/2601.05807",
        "title": "Fusion Matters: Length-Aware Analysis of Positional-Encoding Fusion in Transformers",
        "authors": [
            "Mohamed Amine Hallam",
            "Kuo-Kun Tseng"
        ],
        "comments": "10 pages, 5 figures. Code and reproduction materials available on GitHub",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
        "abstract": "Transformers require positional encodings to represent sequence order, yet most prior work focuses on designing new positional encodings rather than examining how positional information is fused with token embeddings. In this paper, we study whether the fusion mechanism itself affects performance, particularly in long-sequence settings. We conduct a controlled empirical study comparing three canonical fusion strategies--element-wise addition, concatenation with projection, and scalar gated fusion--under identical Transformer architectures, data splits, and random seeds. Experiments on three text classification datasets spanning short (AG News), medium (IMDB), and long (ArXiv) sequences show that fusion choice has negligible impact on short texts but produces consistent gains on long documents. To verify that these gains are structural rather than stochastic, we perform paired-seed analysis and cross-dataset comparison across sequence-length regimes. Additional experiments on the ArXiv dataset indicate that the benefit of learnable fusion generalizes across multiple positional encoding families. Finally, we explore a lightweight convolutional gating mechanism that introduces local inductive bias at the fusion level, evaluated on long documents only. Our results indicate that positional-encoding fusion is a non-trivial design choice for long-sequence Transformers and should be treated as an explicit modeling decision rather than a fixed default.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 31,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05811",
        "abs_url": "https://arxiv.org/abs/2601.05811",
        "pdf_url": "https://arxiv.org/pdf/2601.05811",
        "title": "Learning Reconstructive Embeddings in Reproducing Kernel Hilbert Spaces via the Representer Theorem",
        "authors": [
            "Enrique Feito-Casares",
            "Francisco M. Melgarejo-Meseguer",
            "José-Luis Rojo-Álvarez"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Motivated by the growing interest in representation learning approaches that uncover the latent structure of high-dimensional data, this work proposes new algorithms for reconstruction-based manifold learning within Reproducing-Kernel Hilbert Spaces (RKHS). Each observation is first reconstructed as a linear combination of the other samples in the RKHS, by optimizing a vector form of the Representer Theorem for their autorepresentation property. A separable operator-valued kernel extends the formulation to vector-valued data while retaining the simplicity of a single scalar similarity function. A subsequent kernel-alignment task projects the data into a lower-dimensional latent space whose Gram matrix aims to match the high-dimensional reconstruction kernel, thus transferring the auto-reconstruction geometry of the RKHS to the embedding. Therefore, the proposed algorithms represent an extended approach to the autorepresentation property, exhibited by many natural data, by using and adapting well-known results of Kernel Learning Theory. Numerical experiments on both simulated (concentric circles and swiss-roll) and real (cancer molecular activity and IoT network intrusions) datasets provide empirical evidence of the practical effectiveness of the proposed approach.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 32,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05812",
        "abs_url": "https://arxiv.org/abs/2601.05812",
        "pdf_url": "https://arxiv.org/pdf/2601.05812",
        "title": "Detecting Autism Spectrum Disorder with Deep Eye Movement Features",
        "authors": [
            "Zhanpei Huang",
            "Taochen chen",
            "Fangqing Gu",
            "Yiqun Zhang"
        ],
        "comments": "Accepted to CIS 2025",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Autism Spectrum Disorder (ASD) is a neurodevelopmental disorder characterized by deficits in social communication and behavioral patterns. Eye movement data offers a non-invasive diagnostic tool for ASD detection, as it is inherently discrete and exhibits short-term temporal dependencies, reflecting localized gaze focus between fixation points. These characteristics enable the data to provide deeper insights into subtle behavioral markers, distinguishing ASD-related patterns from typical development. Eye movement signals mainly contain short-term and localized dependencies. However, despite the widespread application of stacked attention layers in Transformer-based models for capturing long-range dependencies, our experimental results indicate that this approach yields only limited benefits when applied to eye movement data. This may be because discrete fixation points and short-term dependencies in gaze focus reduce the utility of global attention mechanisms, making them less efficient than architectures focusing on local temporal patterns. To efficiently capture subtle and complex eye movement patterns, distinguishing ASD from typically developing (TD) individuals, a discrete short-term sequential (DSTS) modeling framework is designed with Class-aware Representation and Imbalance-aware Mechanisms. Through extensive experiments on several eye movement datasets, DSTS outperforms both traditional machine learning techniques and more sophisticated deep learning models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 33,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05814",
        "abs_url": "https://arxiv.org/abs/2601.05814",
        "pdf_url": "https://arxiv.org/pdf/2601.05814",
        "title": "A Dual Pipeline Machine Learning Framework for Automated Multi Class Sleep Disorder Screening Using Hybrid Resampling and Ensemble Learning",
        "authors": [
            "Md Sultanul Islam Ovi",
            "Muhsina Tarannum Munfa",
            "Miftahul Alam Adib",
            "Syed Sabbir Hasan"
        ],
        "comments": "32 pages, 5 figures, 14 tables",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Accurate classification of sleep disorders, particularly insomnia and sleep apnea, is important for reducing long term health risks and improving patient quality of life. However, clinical sleep studies are resource intensive and are difficult to scale for population level screening. This paper presents a Dual Pipeline Machine Learning Framework for multi class sleep disorder screening using the Sleep Health and Lifestyle dataset. The framework consists of two parallel processing streams: a statistical pipeline that targets linear separability using Mutual Information and Linear Discriminant Analysis, and a wrapper based pipeline that applies Boruta feature selection with an autoencoder for non linear representation learning. To address class imbalance, we use the hybrid SMOTETomek resampling strategy. In experiments, Extra Trees and K Nearest Neighbors achieved an accuracy of 98.67%, outperforming recent baselines on the same dataset. Statistical testing using the Wilcoxon Signed Rank Test indicates that the improvement over baseline configurations is significant, and inference latency remains below 400 milliseconds. These results suggest that the proposed dual pipeline design supports accurate and efficient automated screening for non invasive sleep disorder risk stratification.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 34,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05845",
        "abs_url": "https://arxiv.org/abs/2601.05845",
        "pdf_url": "https://arxiv.org/pdf/2601.05845",
        "title": "A New Family of Poisson Non-negative Matrix Factorization Methods Using the Shifted Log Link",
        "authors": [
            "Eric Weine",
            "Peter Carbonetto",
            "Rafael A. Irizarry",
            "Matthew Stephens"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Methodology (stat.ME)",
        "abstract": "Poisson non-negative matrix factorization (NMF) is a widely used method to find interpretable \"parts-based\" decompositions of count data. While many variants of Poisson NMF exist, existing methods assume that the \"parts\" in the decomposition combine additively. This assumption may be natural in some settings, but not in others. Here we introduce Poisson NMF with the shifted-log link function to relax this assumption. The shifted-log link function has a single tuning parameter, and as this parameter varies the model changes from assuming that parts combine additively (i.e., standard Poisson NMF) to assuming that parts combine more multiplicatively. We provide an algorithm to fit this model by maximum likelihood, and also an approximation that substantially reduces computation time for large, sparse datasets (computations scale with the number of non-zero entries in the data matrix). We illustrate these new methods on a variety of real datasets. Our examples show how the choice of link function in Poisson NMF can substantively impact the results, and how in some settings the use of a shifted-log link function may improve interpretability compared with the standard, additive link.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 35,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05889",
        "abs_url": "https://arxiv.org/abs/2601.05889",
        "pdf_url": "https://arxiv.org/pdf/2601.05889",
        "title": "GlueNN: gluing patchwise analytic solutions with neural networks",
        "authors": [
            "Doyoung Kim",
            "Donghee Lee",
            "Hye-Sung Lee",
            "Jiheon Lee",
            "Jaeok Yi"
        ],
        "comments": "7 pages, 3 figures",
        "subjects": "Machine Learning (cs.LG); Cosmology and Nongalactic Astrophysics (astro-ph.CO); Computational Physics (physics.comp-ph)",
        "abstract": "In many problems in physics and engineering, one encounters complicated differential equations with strongly scale-dependent terms for which exact analytical or numerical solutions are not available. A common strategy is to divide the domain into several regions (patches) and simplify the equation in each region. When approximate analytic solutions can be obtained in each patch, they are then matched at the interfaces to construct a global solution. However, this patching procedure can fail to reproduce the correct solution, since the approximate forms may break down near the matching boundaries. In this work, we propose a learning framework in which the integration constants of asymptotic analytic solutions are promoted to scale-dependent functions. By constraining these coefficient functions with the original differential equation over the domain, the network learns a globally valid solution that smoothly interpolates between asymptotic regimes, eliminating the need for arbitrary boundary matching. We demonstrate the effectiveness of this framework in representative problems from chemical kinetics and cosmology, where it accurately reproduces global solutions and outperforms conventional matching procedures.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 36,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05913",
        "abs_url": "https://arxiv.org/abs/2601.05913",
        "pdf_url": "https://arxiv.org/pdf/2601.05913",
        "title": "Distilling Lightweight Domain Experts from Large ML Models by Identifying Relevant Subspaces",
        "authors": [
            "Pattarawat Chormai",
            "Ali Hashemi",
            "Klaus-Robert Müller",
            "Grégoire Montavon"
        ],
        "comments": "20 pages + supplement",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Knowledge distillation involves transferring the predictive capabilities of large, high-performing AI models (teachers) to smaller models (students) that can operate in environments with limited computing power. In this paper, we address the scenario in which only a few classes and their associated intermediate concepts are relevant to distill. This scenario is common in practice, yet few existing distillation methods explicitly focus on the relevant subtask. To address this gap, we introduce 'SubDistill', a new distillation algorithm with improved numerical properties that only distills the relevant components of the teacher model at each layer. Experiments on CIFAR-100 and ImageNet with Convolutional and Transformer models demonstrate that SubDistill outperforms existing layer-wise distillation techniques on a representative set of subtasks. Our benchmark evaluations are complemented by Explainable AI analyses showing that our distilled student models more closely match the decision structure of the original teacher model.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 37,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05929",
        "abs_url": "https://arxiv.org/abs/2601.05929",
        "pdf_url": "https://arxiv.org/pdf/2601.05929",
        "title": "Prophet as a Repro ducible Forecasting Framework: A Methodological Guide for Business and Financial Analytics",
        "authors": [
            "Sidney Shapiro",
            "Burhanuddin Panvelwala"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Reproducibility remains a persistent challenge in forecasting research and practice, particularly in business and financial analytics where forecasts inform high-stakes decisions. Traditional forecasting methods, while theoretically interpretable, often require extensive manual tuning and are difficult to replicate in proprietary environments. Machine learning approaches offer predictive flexibility but introduce challenges related to interpretability, stochastic training procedures, and cross-environment reproducibility. This paper examines Prophet, an open-source forecasting framework developed by Meta, as a reproducibility-enabling solution that balances interpretability, standardized workflows, and accessibility. Rather than proposing a new algorithm, this study evaluates how Prophet's additive structure, open-source implementation, and standardized workflow contribute to transparent and replicable forecasting practice. Using publicly available financial and retail datasets, we compare Prophet's performance and interpretability with multiple ARIMA specifications (auto-selected, manually specified, and seasonal variants) and Random Forest under a controlled and fully documented experimental design. This multi-model comparison provides a robust assessment of Prophet's relative performance and reproducibility advantages. Through concrete Python examples, we demonstrate how Prophet facilitates efficient forecasting workflows and integration with analytical pipelines. The study positions Prophet within the broader context of reproducible research. It highlights Prophet's role as a methodological building block that supports verification, auditability, and methodological rigor. This work provides researchers and practitioners with a practical reference framework for reproducible forecasting in Python-based research workflows.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 38,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05956",
        "abs_url": "https://arxiv.org/abs/2601.05956",
        "pdf_url": "https://arxiv.org/pdf/2601.05956",
        "title": "On the Robustness of Age for Learning-Based Wireless Scheduling in Unknown Environments",
        "authors": [
            "Juaren Steiger",
            "Bin Li"
        ],
        "comments": "technical report of conference paper",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The constrained combinatorial multi-armed bandit model has been widely employed to solve problems in wireless networking and related areas, including the problem of wireless scheduling for throughput optimization under unknown channel conditions. Most work in this area uses an algorithm design strategy that combines a bandit learning algorithm with the virtual queue technique to track the throughput constraint violation. These algorithms seek to minimize the virtual queue length in their algorithm design. However, in networks where channel conditions change abruptly, the resulting constraints may become infeasible, leading to unbounded growth in virtual queue lengths. In this paper, we make the key observation that the dynamics of the head-of-line age, i.e. the age of the oldest packet in the virtual queue, make it more robust when used in algorithm design compared to the virtual queue length. We therefore design a learning-based scheduling policy that uses the head-of-line age in place of the virtual queue length. We show that our policy matches state-of-the-art performance under i.i.d. network conditions. Crucially, we also show that the system remains stable even under abrupt changes in channel conditions and can rapidly recover from periods of constraint infeasibility.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 39,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05984",
        "abs_url": "https://arxiv.org/abs/2601.05984",
        "pdf_url": "https://arxiv.org/pdf/2601.05984",
        "title": "Community-Based Model Sharing and Generalisation: Anomaly Detection in IoT Temperature Sensor Networks",
        "authors": [
            "Sahibzada Saadoon Hammad",
            "Joaquín Huerta Guijarro",
            "Francisco Ramos",
            "Michael Gould Carlson",
            "Sergio Trilles Oliver"
        ],
        "comments": "20 pages, 9 figures, Journal submission",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The rapid deployment of Internet of Things (IoT) devices has led to large-scale sensor networks that monitor environmental and urban phenomena in real time. Communities of Interest (CoIs) provide a promising paradigm for organising heterogeneous IoT sensor networks by grouping devices with similar operational and environmental characteristics. This work presents an anomaly detection framework based on the CoI paradigm by grouping sensors into communities using a fused similarity matrix that incorporates temporal correlations via Spearman coefficients, spatial proximity using Gaussian distance decay, and elevation similarities. For each community, representative stations based on the best silhouette are selected and three autoencoder architectures (BiLSTM, LSTM, and MLP) are trained using Bayesian hyperparameter optimization with expanding window cross-validation and tested on stations from the same cluster and the best representative stations of other clusters. The models are trained on normal temperature patterns of the data and anomalies are detected through reconstruction error analysis. Experimental results show a robust within-community performance across the evaluated configurations, while variations across communities are observed. Overall, the results support the applicability of community-based model sharing in reducing computational overhead and to analyse model generalisability across IoT sensor networks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 40,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.06016",
        "abs_url": "https://arxiv.org/abs/2601.06016",
        "pdf_url": "https://arxiv.org/pdf/2601.06016",
        "title": "LookAroundNet: Extending Temporal Context with Transformers for Clinically Viable EEG Seizure Detection",
        "authors": [
            "Þór Sverrisson",
            "Steinn Guðmundsson"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Automated seizure detection from electroencephalography (EEG) remains difficult due to the large variability of seizure dynamics across patients, recording conditions, and clinical settings. We introduce LookAroundNet, a transformer-based seizure detector that uses a wider temporal window of EEG data to model seizure activity. The seizure detector incorporates EEG signals before and after the segment of interest, reflecting how clinicians use surrounding context when interpreting EEG recordings. We evaluate the proposed method on multiple EEG datasets spanning diverse clinical environments, patient populations, and recording modalities, including routine clinical EEG and long-term ambulatory recordings, in order to study performance across varying data distributions. The evaluation includes publicly available datasets as well as a large proprietary collection of home EEG recordings, providing complementary views of controlled clinical data and unconstrained home-monitoring conditions. Our results show that LookAroundNet achieves strong performance across datasets, generalizes well to previously unseen recording conditions, and operates with computational costs compatible with real-world clinical deployment. The results indicate that extended temporal context, increased training data diversity, and model ensembling are key factors for improving performance. This work contributes to moving automatic seizure detection models toward clinically viable solutions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 41,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2509.12515",
        "abs_url": "https://arxiv.org/abs/2509.12515",
        "pdf_url": "https://arxiv.org/pdf/2509.12515",
        "title": "Rapid Adaptation of SpO2 Estimation to Wearable Devices via Transfer Learning on Low-Sampling-Rate PPG",
        "authors": [
            "Zequan Liang",
            "Ruoyu Zhang",
            "Wei Shao",
            "krishna Karthik",
            "Ehsan Kourkchi",
            "Setareh Rafatirad",
            "Houman Homayoun"
        ],
        "comments": "In the proceedings of IEEE-EMBS International Conference on Body Sensor Networks 2025",
        "subjects": "Signal Processing (eess.SP); Machine Learning (cs.LG)",
        "abstract": "Blood oxygen saturation (SpO2) is a vital marker for healthcare monitoring. Traditional SpO2 estimation methods often rely on complex clinical calibration, making them unsuitable for low-power, wearable applications. In this paper, we propose a transfer learning-based framework for the rapid adaptation of SpO2 estimation to energy-efficient wearable devices using low-sampling-rate (25Hz) dual-channel photoplethysmography (PPG). We first pretrain a bidirectional Long Short-Term Memory (BiLSTM) model with self-attention on a public clinical dataset, then fine-tune it using data collected from our wearable We-Be band and an FDA-approved reference pulse oximeter. Experimental results show that our approach achieves a mean absolute error (MAE) of 2.967% on the public dataset and 2.624% on the private dataset, significantly outperforming traditional calibration and non-transferred machine learning baselines. Moreover, using 25Hz PPG reduces power consumption by 40% compared to 100Hz, excluding baseline draw. Our method also attains an MAE of 3.284% in instantaneous SpO2 prediction, effectively capturing rapid fluctuations. These results demonstrate the rapid adaptation of accurate, low-power SpO2 monitoring on wearable devices without the need for clinical calibration.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 42,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2509.12518",
        "abs_url": "https://arxiv.org/abs/2509.12518",
        "pdf_url": "https://arxiv.org/pdf/2509.12518",
        "title": "Generalizable Blood Pressure Estimation from Multi-Wavelength PPG Using Curriculum-Adversarial Learning",
        "authors": [
            "Zequan Liang",
            "Ruoyu Zhang",
            "Wei Shao",
            "Mahdi Pirayesh Shirazi Nejad",
            "Ehsan Kourkchi",
            "Setareh Rafatirad",
            "Houman Homayoun"
        ],
        "comments": "In the proceedings of IEEE-EMBS International Conference on Body Sensor Networks 2025",
        "subjects": "Signal Processing (eess.SP); Machine Learning (cs.LG)",
        "abstract": "Accurate and generalizable blood pressure (BP) estimation is vital for the early detection and management of cardiovascular diseases. In this study, we enforce subject-level data splitting on a public multi-wavelength photoplethysmography (PPG) dataset and propose a generalizable BP estimation framework based on curriculum-adversarial learning. Our approach combines curriculum learning, which transitions from hypertension classification to BP regression, with domain-adversarial training that confuses subject identity to encourage the learning of subject-invariant features. Experiments show that multi-channel fusion consistently outperforms single-channel models. On the four-wavelength PPG dataset, our method achieves strong performance under strict subject-level splitting, with mean absolute errors (MAE) of 14.2mmHg for systolic blood pressure (SBP) and 6.4mmHg for diastolic blood pressure (DBP). Additionally, ablation studies validate the effectiveness of both the curriculum and adversarial components. These results highlight the potential of leveraging complementary information in multi-wavelength PPG and curriculum-adversarial strategies for accurate and robust BP estimation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 43,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05261",
        "abs_url": "https://arxiv.org/abs/2601.05261",
        "pdf_url": "https://arxiv.org/pdf/2601.05261",
        "title": "Improving User Experience with Personalized Review Ranking and Summarization",
        "authors": [
            "Muhammad Mufti",
            "Omar Hammad",
            "Mahfuzur Rahman"
        ],
        "comments": "",
        "subjects": "Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "Online consumer reviews play a crucial role in guiding purchase decisions by offering insights into product quality, usability, and performance. However, the increasing volume of user-generated reviews has led to information overload, making it difficult for consumers to identify content that aligns with their specific preferences. Existing review ranking systems typically rely on metrics such as helpfulness votes, star ratings, and recency, but these fail to capture individual user interests and often treat textual sentiment and rating signals separately. This research addresses these limitations by proposing a personalized framework that integrates review ranking and abstractive summarization to enhance decision-making efficiency. The proposed system begins by modeling each user's sentiment through a hybrid analysis of star ratings and review content. Simultaneously, user preferences were derived from historical reviews using sentence embeddings and clustering, forming semantic profiles aligned with thematic and sentiment dimensions. A relevance scoring algorithm matched these profiles with unseen reviews based on sentiment and aspect similarity. Top-matched reviews were then summarized to reflect individual interests. A user study with 70 participants demonstrated that the personalized approach improved satisfaction, perceived relevance, and decision-making confidence, while reducing time spent reading. The results highlight the method's effectiveness in alleviating information overload and delivering content tailored to user-specific preferences, emphasizing its value in enhancing user experience in review-rich decision-making environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 44,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05267",
        "abs_url": "https://arxiv.org/abs/2601.05267",
        "pdf_url": "https://arxiv.org/pdf/2601.05267",
        "title": "Transforming User Defined Criteria into Explainable Indicators with an Integrated LLM AHP System",
        "authors": [
            "Geonwoo Bang",
            "Dongho Kim",
            "Moohong Min"
        ],
        "comments": "",
        "subjects": "Information Retrieval (cs.IR); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Evaluating complex texts across domains requires converting user defined criteria into quantitative, explainable indicators, which is a persistent challenge in search and recommendation systems. Single prompt LLM evaluations suffer from complexity and latency issues, while criterion specific decomposition approaches rely on naive averaging or opaque black-box aggregation methods. We present an interpretable aggregation framework combining LLM scoring with the Analytic Hierarchy Process. Our method generates criterion specific scores via LLM as judge, measures discriminative power using Jensen Shannon distance, and derives statistically grounded weights through AHP pairwise comparison matrices. Experiments on Amazon review quality assessment and depression related text scoring demonstrate that our approach achieves high explainability and operational efficiency while maintaining comparable predictive power, making it suitable for real time latency sensitive web services.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 45,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05271",
        "abs_url": "https://arxiv.org/abs/2601.05271",
        "pdf_url": "https://arxiv.org/pdf/2601.05271",
        "title": "Enhancing Foundation Models in Transaction Understanding with LLM-based Sentence Embeddings",
        "authors": [
            "Xiran Fan",
            "Zhimeng Jiang",
            "Chin-Chia Michael Yeh",
            "Yuzhong Chen",
            "Yingtong Dou",
            "Menghai Pan",
            "Yan Zheng"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "The ubiquity of payment networks generates vast transactional data encoding rich consumer and merchant behavioral patterns. Recent foundation models for transaction analysis process tabular data sequentially but rely on index-based representations for categorical merchant fields, causing substantial semantic information loss by converting rich textual data into discrete tokens. While Large Language Models (LLMs) can address this limitation through superior semantic understanding, their computational overhead challenges real-time financial deployment. We introduce a hybrid framework that uses LLM-generated embeddings as semantic initializations for lightweight transaction models, balancing interpretability with operational efficiency. Our approach employs multi-source data fusion to enrich merchant categorical fields and a one-word constraint principle for consistent embedding generation across LLM architectures. We systematically address data quality through noise filtering and context-aware enrichment. Experiments on large-scale transaction datasets demonstrate significant performance improvements across multiple transaction understanding tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 46,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05274",
        "abs_url": "https://arxiv.org/abs/2601.05274",
        "pdf_url": "https://arxiv.org/pdf/2601.05274",
        "title": "On the use of case estimate and transactional payment data in neural networks for individual loss reserving",
        "authors": [
            "Benjamin Avanzi",
            "Matthew Lambrianidis",
            "Greg Taylor",
            "Bernard Wong"
        ],
        "comments": "",
        "subjects": "Statistical Finance (q-fin.ST); Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "The use of neural networks trained on individual claims data has become increasingly popular in the actuarial reserving literature. We consider how to best input historical payment data in neural network models. Additionally, case estimates are also available in the format of a time series, and we extend our analysis to assessing their predictive power. In this paper, we compare a feed-forward neural network trained on summarised transactions to a recurrent neural network equipped to analyse a claim's entire payment history and/or case estimate development history. We draw conclusions from training and comparing the performance of the models on multiple, comparable highly complex datasets simulated from SPLICE (Avanzi, Taylor and Wang, 2023). We find evidence that case estimates will improve predictions significantly, but that equipping the neural network with memory only leads to meagre improvements. Although the case estimation process and quality will vary significantly between insurers, we provide a standardised methodology for assessing their value.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 47,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05276",
        "abs_url": "https://arxiv.org/abs/2601.05276",
        "pdf_url": "https://arxiv.org/pdf/2601.05276",
        "title": "Channel Selected Stratified Nested Cross Validation for Clinically Relevant EEG Based Parkinsons Disease Detection",
        "authors": [
            "Nicholas R. Rasmussen",
            "Rodrigue Rizk",
            "Longwei Wang",
            "Arun Singh",
            "KC Santosh"
        ],
        "comments": "Submitted to IEEE Conference -> posting to Arxiv as normal",
        "subjects": "Signal Processing (eess.SP); Machine Learning (cs.LG)",
        "abstract": "The early detection of Parkinsons disease remains a critical challenge in clinical neuroscience, with electroencephalography offering a noninvasive and scalable pathway toward population level screening. While machine learning has shown promise in this domain, many reported results suffer from methodological flaws, most notably patient level data leakage, inflating performance estimates and limiting clinical translation. To address these modeling pitfalls, we propose a unified evaluation framework grounded in nested cross validation and incorporating three complementary safeguards: (i) patient level stratification to eliminate subject overlap and ensure unbiased generalization, (ii) multi layered windowing to harmonize heterogeneous EEG recordings while preserving temporal dynamics, and (iii) inner loop channel selection to enable principled feature reduction without information leakage. Applied across three independent datasets with a heterogeneous number of channels, a convolutional neural network trained under this framework achieved 80.6% accuracy and demonstrated state of the art performance under held out population block testing, comparable to other methods in the literature. This performance underscores the necessity of nested cross validation as a safeguard against bias and as a principled means of selecting the most relevant information for patient level decisions, providing a reproducible foundation that can extend to other biomedical signal analysis domains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 48,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05289",
        "abs_url": "https://arxiv.org/abs/2601.05289",
        "pdf_url": "https://arxiv.org/pdf/2601.05289",
        "title": "A universal vision transformer for fast calorimeter simulations",
        "authors": [
            "Luigi Favaro",
            "Andrea Giammanco",
            "Claudius Krause"
        ],
        "comments": "37 pages, 15 figures, 8 tables",
        "subjects": "High Energy Physics - Phenomenology (hep-ph); Machine Learning (cs.LG); High Energy Physics - Experiment (hep-ex); Instrumentation and Detectors (physics.ins-det)",
        "abstract": "The high-dimensional complex nature of detectors makes fast calorimeter simulations a prime application for modern generative machine learning. Vision transformers (ViTs) can emulate the Geant4 response with unmatched accuracy and are not limited to regular geometries. Starting from the CaloDREAM architecture, we demonstrate the robustness and scalability of ViTs on regular and irregular geometries, and multiple detectors. Our results show that ViTs generate electromagnetic and hadronic showers statistically indistinguishable from Geant4 in multiple evaluation metrics, while maintaining the generation time in the $\\mathcal{O}(10-100)$ ms on a single GPU. Furthermore, we show that pretraining on a large dataset and fine-tuning on the target geometry leads to reduced training costs and higher data efficiency, or altogether improves the fidelity of generated showers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 49,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05297",
        "abs_url": "https://arxiv.org/abs/2601.05297",
        "pdf_url": "https://arxiv.org/pdf/2601.05297",
        "title": "Machine learning assisted state prediction of misspecified linear dynamical system via modal reduction",
        "authors": [
            "Rohan Vitthal Thorat",
            "Rajdip Nayek"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Accurate prediction of structural dynamics is imperative for preserving digital twin fidelity throughout operational lifetimes. Parametric models with fixed nominal parameters often omit critical physical effects due to simplifications in geometry, material behavior, damping, or boundary conditions, resulting in model form errors (MFEs) that impair predictive accuracy. This work introduces a comprehensive framework for MFE estimation and correction in high-dimensional finite element (FE) based structural dynamical systems. The Gaussian Process Latent Force Model (GPLFM) represents discrepancies non-parametrically in the reduced modal domain, allowing a flexible data-driven characterization of unmodeled dynamics. A linear Bayesian filtering approach jointly estimates system states and discrepancies, incorporating epistemic and aleatoric uncertainties. To ensure computational tractability, the FE system is projected onto a reduced modal basis, and a mesh-invariant neural network maps modal states to discrepancy estimates, permitting model rectification across different FE discretizations without retraining. Validation is undertaken across five MFE scenarios-including incorrect beam theory, damping misspecification, misspecified boundary condition, unmodeled material nonlinearity, and local damage demonstrating the surrogate model's substantial reduction of displacement and rotation prediction errors under unseen excitations. The proposed methodology offers a potential means to uphold digital twin accuracy amid inherent modeling uncertainties.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 50,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05299",
        "abs_url": "https://arxiv.org/abs/2601.05299",
        "pdf_url": "https://arxiv.org/pdf/2601.05299",
        "title": "Optimizing Digital Adjudication through Social Network Analysis: An Empirical Study of Credit Card Disputes in Beijing",
        "authors": [
            "Chung Han Tsai",
            "ChengTo Lin",
            "Chung Han Tsai",
            "ChengTo Lin",
            "Baowen Zhang",
            "Qingyue Deng",
            "Yunhui Zhao",
            "Zhijia Song",
            "Baowen Zhang",
            "Qingyue Deng",
            "Yunhui Zhao",
            "Zhijia Song"
        ],
        "comments": "",
        "subjects": "Social and Information Networks (cs.SI); Machine Learning (cs.LG)",
        "abstract": "Amid the rapid digitalization of judicial systems, the integration of big data into adjudication remains underexplored, particularly in uncovering the structural logic of legal applications. This study bridges this gap by employing social network analysis (SNA) to examine credit card disputes involving personal information protection adjudicated in Beijing. By constructing a legal citation network, we reveal the latent patterns of substantive and procedural law application. The findings demonstrate that SNA can effectively identify core legal norms and typify cases, offering a robust methodological framework for optimizing 'Digital Court' systems. These insights provide practical pathways for enhancing judicial efficiency and consistency through data-driven case retrieval and holistic judicial information networks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 51,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05335",
        "abs_url": "https://arxiv.org/abs/2601.05335",
        "pdf_url": "https://arxiv.org/pdf/2601.05335",
        "title": "Generalized Canonical Polyadic Tensor Decompositions with General Symmetry",
        "authors": [
            "Alex Mulrooney",
            "David Hong"
        ],
        "comments": "This work has been submitted to the IEEE for possible publication. 11 pages, 5 figures",
        "subjects": "Numerical Analysis (math.NA); Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Canonical Polyadic (CP) tensor decomposition is a workhorse algorithm for discovering underlying low-dimensional structure in tensor data. This is accomplished in conventional CP decomposition by fitting a low-rank tensor to data with respect to the least-squares loss. Generalized CP (GCP) decompositions generalize this approach by allowing general loss functions that can be more appropriate, e.g., to model binary and count data or to improve robustness to outliers. However, GCP decompositions do not explicitly account for any symmetry in the tensors, which commonly arises in modern applications. For example, a tensor formed by stacking the adjacency matrices of a dynamic graph over time will naturally exhibit symmetry along the two modes corresponding to the graph nodes. In this paper, we develop a symmetric GCP (SymGCP) decomposition that allows for general forms of symmetry, i.e., symmetry along any subset of the modes. SymGCP accounts for symmetry by enforcing the corresponding symmetry in the decomposition. We derive gradients for SymGCP that enable its efficient computation via all-at-once optimization with existing tensor kernels. The form of the gradients also leads to various stochastic approximations that enable us to develop stochastic SymGCP algorithms that can scale to large tensors. We demonstrate the utility of the proposed SymGCP algorithms with a variety of experiments on both synthetic and real data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 52,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05366",
        "abs_url": "https://arxiv.org/abs/2601.05366",
        "pdf_url": "https://arxiv.org/pdf/2601.05366",
        "title": "Lost in Execution: On the Multilingual Robustness of Tool Calling in Large Language Models",
        "authors": [
            "Zheng Luo",
            "T Pranav Kutralingam",
            "Ogochukwu N Okoani",
            "Wanpeng Xu",
            "Hua Wei",
            "Xiyang Hu"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Large Language Models (LLMs) are increasingly deployed as agents that invoke external tools through structured function calls. While recent work reports strong tool-calling performance under standard English-centric evaluations, the robustness of tool calling under multilingual user interactions remains underexplored. In this work, we introduce MLCL, a diagnostic benchmark, and conduct a systematic evaluation of multilingual tool calling across Chinese, Hindi, and the low-resource language Igbo. Through fine-grained error analysis, we show that many failures occur despite correct intent understanding and tool selection. We identify parameter value language mismatch as a dominant failure mode, where models generate semantically appropriate parameter values in the user's language, violating language-invariant execution conventions. We further evaluate several inference-time system strategies and find that while these strategies substantially reduce language-induced execution errors, none of them can fully recover English-level performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 53,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05392",
        "abs_url": "https://arxiv.org/abs/2601.05392",
        "pdf_url": "https://arxiv.org/pdf/2601.05392",
        "title": "Archetypal cases for questionnaires with nominal multiple choice questions",
        "authors": [
            "Aleix Alcacer",
            "Irene Epifanio"
        ],
        "comments": "Statistical Methods for Data Analysis and Decision Sciences. Third Conference of the Statistics and Data Science Group of the Italian Statistical Society. Milan, April 2-3, 2025",
        "subjects": "Methodology (stat.ME); Machine Learning (cs.LG)",
        "abstract": "Archetypal analysis serves as an exploratory tool that interprets a collection of observations as convex combinations of pure (extreme) patterns. When these patterns correspond to actual observations within the sample, they are termed archetypoids. For the first time, we propose applying archetypoid analysis to nominal observations, specifically for identifying archetypal cases from questionnaires featuring nominal multiple-choice questions with a single possible answer. This approach can enhance our understanding of a nominal data set, similar to its application in multivariate contexts. We compare this methodology with the use of archetype analysis and probabilistic archetypal analysis and demonstrate the benefits of this methodology using a real-world example: the German credit dataset.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 54,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05428",
        "abs_url": "https://arxiv.org/abs/2601.05428",
        "pdf_url": "https://arxiv.org/pdf/2601.05428",
        "title": "Dynamic Inclusion and Bounded Multi-Factor Tilts for Robust Portfolio Construction",
        "authors": [
            "Roberto Garrone"
        ],
        "comments": "28 pages, 7 figures, algorithmic portfolio construction framework emphasizing robustness, explicit constraints, and implementability",
        "subjects": "Optimization and Control (math.OC); Machine Learning (cs.LG)",
        "abstract": "This paper proposes a portfolio construction framework designed to remain robust under estimation error, non-stationarity, and realistic trading constraints. The methodology combines dynamic asset eligibility, deterministic rebalancing, and bounded multi-factor tilts applied to an equal-weight baseline. Asset eligibility is formalized as a state-dependent constraint on portfolio construction, allowing factor exposure to adjust endogenously in response to observable market conditions such as liquidity, volatility, and cross-sectional breadth. Rather than estimating expected returns or covariances, the framework relies on cross-sectional rankings and hard structural bounds to control concentration, turnover, and fragility. The resulting approach is fully algorithmic, transparent, and directly implementable. It provides a robustness-oriented alternative to parametric optimization and unconstrained multi-factor models, particularly suited for long-horizon allocations where stability and operational feasibility are primary objectives.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 55,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05441",
        "abs_url": "https://arxiv.org/abs/2601.05441",
        "pdf_url": "https://arxiv.org/pdf/2601.05441",
        "title": "A brief note on learning problem with global perspectives",
        "authors": [
            "Getachew K. Befekadu"
        ],
        "comments": "7 Pages with 1 Figure",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Optimization and Control (math.OC)",
        "abstract": "This brief note considers the problem of learning with dynamic-optimizing principal-agent setting, in which the agents are allowed to have global perspectives about the learning process, i.e., the ability to view things according to their relative importances or in their true relations based-on some aggregated information shared by the principal. Whereas, the principal, which is exerting an influence on the learning process of the agents in the aggregation, is primarily tasked to solve a high-level optimization problem posed as an empirical-likelihood estimator under conditional moment restrictions model that also accounts information about the agents' predictive performances on out-of-samples as well as a set of private datasets available only to the principal. In particular, we present a coherent mathematical argument which is necessary for characterizing the learning process behind this abstract principal-agent learning framework, although we acknowledge that there are a few conceptual and theoretical issues still need to be addressed.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 56,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05444",
        "abs_url": "https://arxiv.org/abs/2601.05444",
        "pdf_url": "https://arxiv.org/pdf/2601.05444",
        "title": "What Functions Does XGBoost Learn?",
        "authors": [
            "Dohyeong Ki",
            "Adityanand Guntuboyina"
        ],
        "comments": "",
        "subjects": "Statistics Theory (math.ST); Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "This paper establishes a rigorous theoretical foundation for the function class implicitly learned by XGBoost, bridging the gap between its empirical success and our theoretical understanding. We introduce an infinite-dimensional function class $\\mathcal{F}^{d, s}_{\\infty-\\text{ST}}$ that extends finite ensembles of bounded-depth regression trees, together with a complexity measure $V^{d, s}_{\\infty-\\text{XGB}}(\\cdot)$ that generalizes the $L^1$ regularization penalty used in XGBoost. We show that every optimizer of the XGBoost objective is also an optimizer of an equivalent penalized regression problem over $\\mathcal{F}^{d, s}_{\\infty-\\text{ST}}$ with penalty $V^{d, s}_{\\infty-\\text{XGB}}(\\cdot)$, providing an interpretation of XGBoost as implicitly targeting a broader function class. We also develop a smoothness-based interpretation of $\\mathcal{F}^{d, s}_{\\infty-\\text{ST}}$ and $V^{d, s}_{\\infty-\\text{XGB}}(\\cdot)$ in terms of Hardy--Krause variation. We prove that the least squares estimator over $\\{f \\in \\mathcal{F}^{d, s}_{\\infty-\\text{ST}}: V^{d, s}_{\\infty-\\text{XGB}}(f) \\le V\\}$ achieves a nearly minimax-optimal rate of convergence $n^{-2/3} (\\log n)^{4(\\min(s, d) - 1)/3}$, thereby avoiding the curse of dimensionality. Our results provide the first rigorous characterization of the function space underlying XGBoost, clarify its connection to classical notions of variation, and identify an important open problem: whether the XGBoost algorithm itself achieves minimax optimality over this class.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 57,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05445",
        "abs_url": "https://arxiv.org/abs/2601.05445",
        "pdf_url": "https://arxiv.org/pdf/2601.05445",
        "title": "Knowledge-Driven Multi-Turn Jailbreaking on Large Language Models",
        "authors": [
            "Songze Li",
            "Ruishi He",
            "Xiaojun Jia",
            "Jun Wang",
            "Zhihui Fu"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
        "abstract": "Large Language Models (LLMs) face a significant threat from multi-turn jailbreak attacks, where adversaries progressively steer conversations to elicit harmful outputs. However, the practical effectiveness of existing attacks is undermined by several critical limitations: they struggle to maintain a coherent progression over long interactions, often losing track of what has been accomplished and what remains to be done; they rely on rigid or pre-defined patterns, and fail to adapt to the LLM's dynamic and unpredictable conversational state. To address these shortcomings, we introduce Mastermind, a multi-turn jailbreak framework that adopts a dynamic and self-improving approach. Mastermind operates in a closed loop of planning, execution, and reflection, enabling it to autonomously build and refine its knowledge of model vulnerabilities through interaction. It employs a hierarchical planning architecture that decouples high-level attack objectives from low-level tactical execution, ensuring long-term focus and coherence. This planning is guided by a knowledge repository that autonomously discovers and refines effective attack patterns by reflecting on interactive experiences. Mastermind leverages this accumulated knowledge to dynamically recombine and adapt attack vectors, dramatically improving both effectiveness and resilience. We conduct comprehensive experiments against state-of-the-art models, including GPT-5 and Claude 3.7 Sonnet. The results demonstrate that Mastermind significantly outperforms existing baselines, achieving substantially higher attack success rates and harmfulness ratings. Moreover, our framework exhibits notable resilience against multiple advanced defense mechanisms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 58,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05498",
        "abs_url": "https://arxiv.org/abs/2601.05498",
        "pdf_url": "https://arxiv.org/pdf/2601.05498",
        "title": "Prompt-Free SAM-Based Multi-Task Framework for Breast Ultrasound Lesion Segmentation and Classification",
        "authors": [
            "Samuel E. Johnny",
            "Bernes L. Atabonfack",
            "Israel Alagbe",
            "Assane Gueye"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Accurate tumor segmentation and classification in breast ultrasound (BUS) imaging remain challenging due to low contrast, speckle noise, and diverse lesion morphology. This study presents a multi-task deep learning framework that jointly performs lesion segmentation and diagnostic classification using embeddings from the Segment Anything Model (SAM) vision encoder. Unlike prompt-based SAM variants, our approach employs a prompt-free, fully supervised adaptation where high-dimensional SAM features are decoded through either a lightweight convolutional head or a UNet-inspired decoder for pixel-wise segmentation. The classification branch is enhanced via mask-guided attention, allowing the model to focus on lesion-relevant features while suppressing background artifacts. Experiments on the PRECISE 2025 breast ultrasound dataset, split per class into 80 percent training and 20 percent testing, show that the proposed method achieves a Dice Similarity Coefficient (DSC) of 0.887 and an accuracy of 92.3 percent, ranking among the top entries on the PRECISE challenge leaderboard. These results demonstrate that SAM-based representations, when coupled with segmentation-guided learning, significantly improve both lesion delineation and diagnostic prediction in breast ultrasound imaging.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 59,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05528",
        "abs_url": "https://arxiv.org/abs/2601.05528",
        "pdf_url": "https://arxiv.org/pdf/2601.05528",
        "title": "Autonomous Probe Microscopy with Robust Bag-of-Features Multi-Objective Bayesian Optimization: Pareto-Front Mapping of Nanoscale Structure-Property Trade-Offs",
        "authors": [
            "Kamyar Barakati",
            "Haochen Zhu",
            "C Charlotte Buchanan",
            "Dustin A Gilbert",
            "Philip Rack",
            "Sergei V. Kalinin"
        ],
        "comments": "25 pages, 5 figures",
        "subjects": "Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG); Data Analysis, Statistics and Probability (physics.data-an)",
        "abstract": "Combinatorial materials libraries are an efficient route to generate large families of candidate compositions, but their impact is often limited by the speed and depth of characterization and by the difficulty of extracting actionable structure-property relations from complex characterization data. Here we develop an autonomous scanning probe microscopy (SPM) framework that integrates automated atomic force and magnetic force microscopy (AFM/MFM) to rapidly explore magnetic and structural properties across combinatorial spread libraries. To enable automated exploration of systems without a clear optimization target, we introduce a combination of a static physics-informed bag-of-features (BoF) representation of measured surface morphology and magnetic structure with multi-objective Bayesian optimization (MOBO) to discover the relative significance and robustness of features. The resulting closed-loop workflow selectively samples the compositional gradient and reconstructs feature landscapes consistent with dense grid \"ground truth\" measurements. The resulting Pareto structure reveals where multiple nanoscale objectives are simultaneously optimized, where trade-offs between roughness, coherence, and magnetic contrast are unavoidable, and how families of compositions cluster into distinct functional regimes, thereby turning multi-feature imaging data into interpretable maps of competing structure-property trends. While demonstrated for Au-Co-Ni and AFM/MFM, the approach is general and can be extended to other combinatorial systems, imaging modalities, and feature sets, illustrating how feature-based MOBO and autonomous SPM can transform microscopy images from static data products into active feedback for real-time, multi-objective materials discovery.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 60,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05531",
        "abs_url": "https://arxiv.org/abs/2601.05531",
        "pdf_url": "https://arxiv.org/pdf/2601.05531",
        "title": "DNATokenizer: A GPU-First Byte-to-Identifier Tokenizer for High-Throughput DNA Language Models",
        "authors": [
            "Eliatan Niktab",
            "Hardip Patel"
        ],
        "comments": "",
        "subjects": "Genomics (q-bio.GN); Machine Learning (cs.LG)",
        "abstract": "Tokenization sits at the boundary between high-throughput genomic input and GPU compute, posing challenges in both algorithm design and system throughput. Overlapping k-mer tokenization can introduce information leakage under masked language modeling (MLM) and may degrade downstream accuracy. Single-nucleotide tokenization avoids leakage and preserves per-base fidelity, but it greatly increases sequence length for attention-based architectures. Non-overlapping k-mers and byte-pair encoding (BPE) provide compression and avoid leakage, at the cost of boundary sensitivity or reduced interpretability. Empirically, the choice of tokenization interacts strongly with model architecture and task requirements. At the system level, however, standard string tokenizers and host-bound vocabulary lookups dominate wall-clock time once inputs reach billions of bases, regardless of the tokenization algorithm. We present DNATok, a high-performance, GPU-first tokenization system that replaces general-purpose string processing with byte lookup table (LUT)-based identifier streaming and an overlapped host-to-device (H2D)/compute pipeline using pinned memory and architectural parallelism. DNATok is vocabulary-agnostic: it accelerates single-nucleotide, non-overlapping k-mer, and BPE tokenization, and integrates as a drop-in systems layer beneath genomic foundation models. DNATok achieves 84-95x higher encoding throughput than optimized Hugging Face baselines and up to 1.9x higher H2D throughput. End-to-end streaming reaches 1.27-1.84e8 tokens/s depending on configuration, effectively removing tokenization as a bottleneck for production-scale training and inference.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 61,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05577",
        "abs_url": "https://arxiv.org/abs/2601.05577",
        "pdf_url": "https://arxiv.org/pdf/2601.05577",
        "title": "Autonomous Discovery of the Ising Model's Critical Parameters with Reinforcement Learning",
        "authors": [
            "Hai Man",
            "Chaobo Wang",
            "Jia-Rui Li",
            "Yuping Tian",
            "Shu-Gang Chen"
        ],
        "comments": "37 pages, 9 figures. This is the Accepted Manuscript of an article published in J. Stat. Mech",
        "subjects": "Statistical Mechanics (cond-mat.stat-mech); Machine Learning (cs.LG); Computational Physics (physics.comp-ph)",
        "abstract": "Traditional methods for determining critical parameters are often influenced by human factors. This research introduces a physics-inspired adaptive reinforcement learning framework that enables agents to autonomously interact with physical environments, simultaneously identifying both the critical temperature and various types of critical exponents in the Ising model with precision. Interestingly, our algorithm exhibits search behavior reminiscent of phase transitions, efficiently converging to target parameters regardless of initial conditions. Experimental results demonstrate that this method significantly outperforms traditional approaches, particularly in environments with strong perturbations. This study not only incorporates physical concepts into machine learning to enhance algorithm interpretability but also establishes a new paradigm for scientific exploration, transitioning from manual analysis to autonomous AI discovery.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 62,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05599",
        "abs_url": "https://arxiv.org/abs/2601.05599",
        "pdf_url": "https://arxiv.org/pdf/2601.05599",
        "title": "Quantifying and Inducing Shape Bias in CNNs via Max-Pool Dilation",
        "authors": [
            "Takito Sawada",
            "Akinori Iwata",
            "Masahiro Okuda"
        ],
        "comments": "Accepted to IEVC 2026. 4 pages, 1 figure, 3 tables",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Convolutional Neural Networks (CNNs) are known to exhibit a strong texture bias, favoring local patterns over global shape information--a tendency inherent to their convolutional architecture. While this bias is beneficial for texture-rich natural images, it often degrades performance on shape-dominant data such as illustrations and sketches. Although prior work has proposed shape-biased models to mitigate this issue, these approaches lack a quantitative metric for identifying which datasets would actually benefit from such modifications. To address this gap, we propose a data-driven metric that quantifies the shape-texture balance of a dataset by computing the Structural Similarity Index (SSIM) between each image's luminance channel and its L0-smoothed counterpart. Building on this metric, we further introduce a computationally efficient adaptation method that promotes shape bias by modifying the dilation of max-pooling operations while keeping convolutional weights frozen. Experimental results show that this approach consistently improves classification accuracy on shape-dominant datasets, particularly in low-data regimes where full fine-tuning is impractical, requiring training only the final classification layer.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 63,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05637",
        "abs_url": "https://arxiv.org/abs/2601.05637",
        "pdf_url": "https://arxiv.org/pdf/2601.05637",
        "title": "GenCtrl -- A Formal Controllability Toolkit for Generative Models",
        "authors": [
            "Emily Cheng",
            "Carmen Amo Alonso",
            "Federico Danieli",
            "Arno Blaas",
            "Luca Zappella",
            "Pau Rodriguez",
            "Xavier Suau"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Systems and Control (eess.SY)",
        "abstract": "As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 64,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05641",
        "abs_url": "https://arxiv.org/abs/2601.05641",
        "pdf_url": "https://arxiv.org/pdf/2601.05641",
        "title": "Multilingual Amnesia: On the Transferability of Unlearning in Multilingual LLMs",
        "authors": [
            "Alireza Dehghanpour Farashah",
            "Aditi Khandelwal",
            "Marylou Fauchard",
            "Zhuan Shi",
            "Negar Rostamzadeh",
            "Golnoosh Farnadi"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "As multilingual large language models become more widely used, ensuring their safety and fairness across diverse linguistic contexts presents unique challenges. While existing research on machine unlearning has primarily focused on monolingual settings, typically English, multilingual environments introduce additional complexities due to cross-lingual knowledge transfer and biases embedded in both pretraining and fine-tuning data. In this work, we study multilingual unlearning using the Aya-Expanse 8B model under two settings: (1) data unlearning and (2) concept unlearning. We extend benchmarks for factual knowledge and stereotypes to ten languages through translation: English, French, Arabic, Japanese, Russian, Farsi, Korean, Hindi, Hebrew, and Indonesian. These languages span five language families and a wide range of resource levels. Our experiments show that unlearning in high-resource languages is generally more stable, with asymmetric transfer effects observed between typologically related languages. Furthermore, our analysis of linguistic distances indicates that syntactic similarity is the strongest predictor of cross-lingual unlearning behavior.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 65,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05663",
        "abs_url": "https://arxiv.org/abs/2601.05663",
        "pdf_url": "https://arxiv.org/pdf/2601.05663",
        "title": "Tracing Stereotypes in Pre-trained Transformers: From Biased Neurons to Fairer Models",
        "authors": [
            "Gianmario Voria",
            "Moses Openja",
            "Foutse Khomh",
            "Gemma Catolino",
            "Fabio Palomba"
        ],
        "comments": "",
        "subjects": "Software Engineering (cs.SE); Machine Learning (cs.LG)",
        "abstract": "The advent of transformer-based language models has reshaped how AI systems process and generate text. In software engineering (SE), these models now support diverse activities, accelerating automation and decision-making. Yet, evidence shows that these models can reproduce or amplify social biases, raising fairness concerns. Recent work on neuron editing has shown that internal activations in pre-trained transformers can be traced and modified to alter model behavior. Building on the concept of knowledge neurons, neurons that encode factual information, we hypothesize the existence of biased neurons that capture stereotypical associations within pre-trained transformers. To test this hypothesis, we build a dataset of biased relations, i.e., triplets encoding stereotypes across nine bias types, and adapt neuron attribution strategies to trace and suppress biased neurons in BERT models. We then assess the impact of suppression on SE tasks. Our findings show that biased knowledge is localized within small neuron subsets, and suppressing them substantially reduces bias with minimal performance loss. This demonstrates that bias in transformers can be traced and mitigated at the neuron level, offering an interpretable approach to fairness in SE.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 66,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05741",
        "abs_url": "https://arxiv.org/abs/2601.05741",
        "pdf_url": "https://arxiv.org/pdf/2601.05741",
        "title": "ViTNT-FIQA: Training-Free Face Image Quality Assessment with Vision Transformers",
        "authors": [
            "Guray Ozgur",
            "Eduarda Caldeira",
            "Tahar Chettaoui",
            "Jan Niklas Kolf",
            "Marco Huber",
            "Naser Damer",
            "Fadi Boutros"
        ],
        "comments": "Accepted at WACV Workshops",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Face Image Quality Assessment (FIQA) is essential for reliable face recognition systems. Current approaches primarily exploit only final-layer representations, while training-free methods require multiple forward passes or backpropagation. We propose ViTNT-FIQA, a training-free approach that measures the stability of patch embedding evolution across intermediate Vision Transformer (ViT) blocks. We demonstrate that high-quality face images exhibit stable feature refinement trajectories across blocks, while degraded images show erratic transformations. Our method computes Euclidean distances between L2-normalized patch embeddings from consecutive transformer blocks and aggregates them into image-level quality scores. We empirically validate this correlation on a quality-labeled synthetic dataset with controlled degradation levels. Unlike existing training-free approaches, ViTNT-FIQA requires only a single forward pass without backpropagation or architectural modifications. Through extensive evaluation on eight benchmarks (LFW, AgeDB-30, CFP-FP, CALFW, Adience, CPLFW, XQLFW, IJB-C), we show that ViTNT-FIQA achieves competitive performance with state-of-the-art methods while maintaining computational efficiency and immediate applicability to any pre-trained ViT-based face recognition model.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 67,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05789",
        "abs_url": "https://arxiv.org/abs/2601.05789",
        "pdf_url": "https://arxiv.org/pdf/2601.05789",
        "title": "SAFE: Secure and Accurate Federated Learning for Privacy-Preserving Brain-Computer Interfaces",
        "authors": [
            "Tianwang Jia",
            "Xiaoqing Chen",
            "Dongrui Wu"
        ],
        "comments": "12 pages, 9 figures",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Electroencephalogram (EEG)-based brain-computer interfaces (BCIs) are widely adopted due to their efficiency and portability; however, their decoding algorithms still face multiple challenges, including inadequate generalization, adversarial vulnerability, and privacy leakage. This paper proposes Secure and Accurate FEderated learning (SAFE), a federated learning-based approach that protects user privacy by keeping data local during model training. SAFE employs local batch-specific normalization to mitigate cross-subject feature distribution shifts and hence improves model generalization. It further enhances adversarial robustness by introducing perturbations in both the input space and the parameter space through federated adversarial training and adversarial weight perturbation. Experiments on five EEG datasets from motor imagery (MI) and event-related potential (ERP) BCI paradigms demonstrated that SAFE consistently outperformed 14 state-of-the-art approaches in both decoding accuracy and adversarial robustness, while ensuring privacy protection. Notably, it even outperformed centralized training approaches that do not consider privacy protection at all. To our knowledge, SAFE is the first algorithm to simultaneously achieve high decoding accuracy, strong adversarial robustness, and reliable privacy protection without using any calibration data from the target subject, making it highly desirable for real-world BCIs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 68,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05794",
        "abs_url": "https://arxiv.org/abs/2601.05794",
        "pdf_url": "https://arxiv.org/pdf/2601.05794",
        "title": "Simplify-This: A Comparative Analysis of Prompt-Based and Fine-Tuned LLMs",
        "authors": [
            "Eilam Cohen",
            "Itamar Bul",
            "Danielle Inbar",
            "Omri Loewenbach"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Large language models (LLMs) enable strong text generation, and in general there is a practical tradeoff between fine-tuning and prompt engineering. We introduce Simplify-This, a comparative study evaluating both paradigms for text simplification with encoder-decoder LLMs across multiple benchmarks, using a range of evaluation metrics. Fine-tuned models consistently deliver stronger structural simplification, whereas prompting often attains higher semantic similarity scores yet tends to copy inputs. A human evaluation favors fine-tuned outputs overall. We release code, a cleaned derivative dataset used in our study, checkpoints of fine-tuned models, and prompt templates to facilitate reproducibility and future work.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 69,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05810",
        "abs_url": "https://arxiv.org/abs/2601.05810",
        "pdf_url": "https://arxiv.org/pdf/2601.05810",
        "title": "SceneFoundry: Generating Interactive Infinite 3D Worlds",
        "authors": [
            "ChunTeng Chen",
            "YiChen Hsu",
            "YiWen Liu",
            "WeiFang Sun",
            "TsaiChing Ni",
            "ChunYi Lee",
            "Min Sun",
            "YuanFu Yang"
        ],
        "comments": "15 pages",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)",
        "abstract": "The ability to automatically generate large-scale, interactive, and physically realistic 3D environments is crucial for advancing robotic learning and embodied intelligence. However, existing generative approaches often fail to capture the functional complexity of real-world interiors, particularly those containing articulated objects with movable parts essential for manipulation and navigation. This paper presents SceneFoundry, a language-guided diffusion framework that generates apartment-scale 3D worlds with functionally articulated furniture and semantically diverse layouts for robotic training. From natural language prompts, an LLM module controls floor layout generation, while diffusion-based posterior sampling efficiently populates the scene with articulated assets from large-scale 3D repositories. To ensure physical usability, SceneFoundry employs differentiable guidance functions to regulate object quantity, prevent articulation collisions, and maintain sufficient walkable space for robotic navigation. Extensive experiments demonstrate that our framework generates structurally valid, semantically coherent, and functionally interactive environments across diverse scene types and conditions, enabling scalable embodied AI research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 70,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05858",
        "abs_url": "https://arxiv.org/abs/2601.05858",
        "pdf_url": "https://arxiv.org/pdf/2601.05858",
        "title": "CLewR: Curriculum Learning with Restarts for Machine Translation Preference Learning",
        "authors": [
            "Alexandra Dragomir",
            "Florin Brad",
            "Radu Tudor Ionescu"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Large language models (LLMs) have demonstrated competitive performance in zero-shot multilingual machine translation (MT). Some follow-up works further improved MT performance via preference optimization, but they leave a key aspect largely underexplored: the order in which data samples are given during training. We address this topic by integrating curriculum learning into various state-of-the-art preference optimization algorithms to boost MT performance. We introduce a novel curriculum learning strategy with restarts (CLewR), which reiterates easy-to-hard curriculum multiple times during training to effectively mitigate the catastrophic forgetting of easy examples. We demonstrate consistent gains across several model families (Gemma2, Qwen2.5, Llama3.1) and preference optimization techniques. We publicly release our code at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 71,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05868",
        "abs_url": "https://arxiv.org/abs/2601.05868",
        "pdf_url": "https://arxiv.org/pdf/2601.05868",
        "title": "Sequential Bayesian Optimal Experimental Design in Infinite Dimensions via Policy Gradient Reinforcement Learning",
        "authors": [
            "Kaichen Shen",
            "Peng Chen"
        ],
        "comments": "",
        "subjects": "Optimization and Control (math.OC); Machine Learning (cs.LG)",
        "abstract": "Sequential Bayesian optimal experimental design (SBOED) for PDE-governed inverse problems is computationally challenging, especially for infinite-dimensional random field parameters. High-fidelity approaches require repeated forward and adjoint PDE solves inside nested Bayesian inversion and design loops. We formulate SBOED as a finite-horizon Markov decision process and learn an amortized design policy via policy-gradient reinforcement learning (PGRL), enabling online design selection from the experiment history without repeatedly solving an SBOED optimization problem. To make policy training and reward evaluation scalable, we combine dual dimension reduction -- active subspace projection for the parameter and principal component analysis for the state -- with an adjusted derivative-informed latent attention neural operator (LANO) surrogate that predicts both the parameter-to-solution map and its Jacobian. We use a Laplace-based D-optimality reward while noting that, in general, other expected-information-gain utilities such as KL divergence can also be used within the same framework. We further introduce an eigenvalue-based evaluation strategy that uses prior samples as proxies for maximum a posteriori (MAP) points, avoiding repeated MAP solves while retaining accurate information-gain estimates. Numerical experiments on sequential multi-sensor placement for contaminant source tracking demonstrate approximately $100\\times$ speedup over high-fidelity finite element methods, improved performance over random sensor placements, and physically interpretable policies that discover an ``upstream'' tracking strategy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 72,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05905",
        "abs_url": "https://arxiv.org/abs/2601.05905",
        "pdf_url": "https://arxiv.org/pdf/2601.05905",
        "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
        "authors": [
            "Haoming Xu",
            "Ningyuan Zhao",
            "Yunzhi Yao",
            "Weihong Xu",
            "Hongru Wang",
            "Xinle Deng",
            "Shumin Deng",
            "Jeff Z. Pan",
            "Huajun Chen",
            "Ningyu Zhang"
        ],
        "comments": "Work in progress",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG); Multiagent Systems (cs.MA)",
        "abstract": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 73,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05910",
        "abs_url": "https://arxiv.org/abs/2601.05910",
        "pdf_url": "https://arxiv.org/pdf/2601.05910",
        "title": "Multi-task Modeling for Engineering Applications with Sparse Data",
        "authors": [
            "Yigitcan Comlek",
            "R. Murali Krishnan",
            "Sandipp Krishnan Ravi",
            "Amin Moghaddas",
            "Rafael Giorjao",
            "Michael Eff",
            "Anirban Samaddar",
            "Nesar S. Ramachandra",
            "Sandeep Madireddy",
            "Liping Wang"
        ],
        "comments": "15 pages, 5 figures, 6 tables",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Applications (stat.AP)",
        "abstract": "Modern engineering and scientific workflows often require simultaneous predictions across related tasks and fidelity levels, where high-fidelity data is scarce and expensive, while low-fidelity data is more abundant. This paper introduces an Multi-Task Gaussian Processes (MTGP) framework tailored for engineering systems characterized by multi-source, multi-fidelity data, addressing challenges of data sparsity and varying task correlations. The proposed framework leverages inter-task relationships across outputs and fidelity levels to improve predictive performance and reduce computational costs. The framework is validated across three representative scenarios: Forrester function benchmark, 3D ellipsoidal void modeling, and friction-stir welding. By quantifying and leveraging inter-task relationships, the proposed MTGP framework offers a robust and scalable solution for predictive modeling in domains with significant computational and experimental costs, supporting informed decision-making and efficient resource utilization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 74,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05930",
        "abs_url": "https://arxiv.org/abs/2601.05930",
        "pdf_url": "https://arxiv.org/pdf/2601.05930",
        "title": "Can We Predict Before Executing Machine Learning Agents?",
        "authors": [
            "Jingsheng Zheng",
            "Jintian Zhang",
            "Yujie Luo",
            "Yuren Mao",
            "Yunjun Gao",
            "Lun Du",
            "Huajun Chen",
            "Ningyu Zhang"
        ],
        "comments": "Work in progress",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multiagent Systems (cs.MA)",
        "abstract": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 75,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05946",
        "abs_url": "https://arxiv.org/abs/2601.05946",
        "pdf_url": "https://arxiv.org/pdf/2601.05946",
        "title": "A Critical Examination of Active Learning Workflows in Materials Science",
        "authors": [
            "Akhil S. Nair",
            "Lucas Foppa"
        ],
        "comments": "",
        "subjects": "Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG)",
        "abstract": "Active learning (AL) plays a critical role in materials science, enabling applications such as the construction of machine-learning interatomic potentials for atomistic simulations and the operation of self-driving laboratories. Despite its widespread use, the reliability and effectiveness of AL workflows depend on implicit design assumptions that are rarely examined systematically. Here, we critically assess AL workflows deployed in materials science and investigate how key design choices, such as surrogate models, sampling strategies, uncertainty quantification and evaluation metrics, relate to their performance. By identifying common pitfalls and discussing practical mitigation strategies, we provide guidance to practitioners for the efficient design, assessment, and interpretation of AL workflows in materials science.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 76,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05975",
        "abs_url": "https://arxiv.org/abs/2601.05975",
        "pdf_url": "https://arxiv.org/pdf/2601.05975",
        "title": "DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management",
        "authors": [
            "Kieran Wood",
            "Stephen J. Roberts",
            "Stefan Zohren"
        ],
        "comments": "",
        "subjects": "Trading and Market Microstructure (q-fin.TR); Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "We propose DeePM (Deep Portfolio Manager), a structured deep-learning macro portfolio manager trained end-to-end to maximize a robust, risk-adjusted utility. DeePM addresses three fundamental challenges in financial learning: (1) it resolves the asynchronous \"ragged filtration\" problem via a Directed Delay (Causal Sieve) mechanism that prioritizes causal impulse-response learning over information freshness; (2) it combats low signal-to-noise ratios via a Macroeconomic Graph Prior, regularizing cross-asset dependence according to economic first principles; and (3) it optimizes a distributionally robust objective where a smooth worst-window penalty serves as a differentiable proxy for Entropic Value-at-Risk (EVaR) - a window-robust utility encouraging strong performance in the most adverse historical subperiods. In large-scale backtests from 2010-2025 on 50 diversified futures with highly realistic transaction costs, DeePM attains net risk-adjusted returns that are roughly twice those of classical trend-following strategies and passive benchmarks, solely using daily closing prices. Furthermore, DeePM improves upon the state-of-the-art Momentum Transformer architecture by roughly fifty percent. The model demonstrates structural resilience across the 2010s \"CTA (Commodity Trading Advisor) Winter\" and the post-2020 volatility regime shift, maintaining consistent performance through the pandemic, inflation shocks, and the subsequent higher-for-longer environment. Ablation studies confirm that strictly lagged cross-sectional attention, graph prior, principled treatment of transaction costs, and robust minimax optimization are the primary drivers of this generalization capability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 77,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05978",
        "abs_url": "https://arxiv.org/abs/2601.05978",
        "pdf_url": "https://arxiv.org/pdf/2601.05978",
        "title": "AWaRe-SAC: Proactive Slice Admission Control under Weather-Induced Capacity Uncertainty",
        "authors": [
            "Dror Jacoby",
            "Yanzhi Li",
            "Shuyue Yu",
            "Nicola Di Cicco",
            "Hagit Messer",
            "Gil Zussman",
            "Igor Kadota"
        ],
        "comments": "",
        "subjects": "Networking and Internet Architecture (cs.NI); Machine Learning (cs.LG)",
        "abstract": "As emerging applications demand higher throughput and lower latencies, operators are increasingly deploying millimeter-wave (mmWave) links within x-haul transport networks, spanning fronthaul, midhaul, and backhaul segments. However, the inherent susceptibility of mmWave frequencies to weather-related attenuation, particularly rain fading, complicates the maintenance of stringent Quality of Service (QoS) requirements. This creates a critical challenge: making admission decisions under uncertainty regarding future network capacity. To address this, we develop a proactive slice admission control framework for mmWave x-haul networks subject to rain-induced fluctuations. Our objective is to improve network performance, ensure QoS, and optimize revenue, thereby surpassing the limitations of standard reactive approaches. The proposed framework integrates a deep learning predictor of future network conditions with a proactive Q-learning-based slice admission control mechanism. We validate our solution using real-world data from a mmWave x-haul deployment in a dense urban area, incorporating realistic models of link capacity attenuation and dynamic slice demands. Extensive evaluations demonstrate that our proactive solution achieves 2-3x higher long-term average revenue under dynamic link conditions, providing a scalable and resilient framework for adaptive admission control.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 78,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.05988",
        "abs_url": "https://arxiv.org/abs/2601.05988",
        "pdf_url": "https://arxiv.org/pdf/2601.05988",
        "title": "CyberGFM: Graph Foundation Models for Lateral Movement Detection in Enterprise Networks",
        "authors": [
            "Isaiah J. King",
            "Bernardo Trindade",
            "Benjamin Bowman",
            "H. Howie Huang"
        ],
        "comments": "17 pages; 11 figures; 8 tables",
        "subjects": "Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
        "abstract": "Representing networks as a graph and training a link prediction model using benign connections is an effective method of anomaly-based intrusion detection. Existing works using this technique have shown great success using temporal graph neural networks and skip-gram-based approaches on random walks. However, random walk-based approaches are unable to incorporate rich edge data, while the GNN-based approaches require large amounts of memory to train. In this work, we propose extending the original insight from random walk-based skip-grams--that random walks through a graph are analogous to sentences in a corpus--to the more modern transformer-based foundation models. Using language models that take advantage of GPU optimizations, we can quickly train a graph foundation model to predict missing tokens in random walks through a network of computers. The graph foundation model is then finetuned for link prediction and used as a network anomaly detector. This new approach allows us to combine the efficiency of random walk-based methods and the rich semantic representation of deep learning methods. This system, which we call CyberGFM, achieved state-of-the-art results on three widely used network anomaly detection datasets, delivering a up to 2$\\times$ improvement in average precision. We found that CyberGFM outperforms all prior works in unsupervised link prediction for network anomaly detection, using the same number of parameters, and with equal or better efficiency than the previous best approaches.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 79,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.06009",
        "abs_url": "https://arxiv.org/abs/2601.06009",
        "pdf_url": "https://arxiv.org/pdf/2601.06009",
        "title": "Detecting Stochasticity in Discrete Signals via Nonparametric Excursion Theorem",
        "authors": [
            "Sunia Tanweer",
            "Firas A. Khasawneh"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Signal Processing (eess.SP); Probability (math.PR); Applications (stat.AP)",
        "abstract": "We develop a practical framework for distinguishing diffusive stochastic processes from deterministic signals using only a single discrete time series. Our approach is based on classical excursion and crossing theorems for continuous semimartingales, which correlates number $N_\\varepsilon$ of excursions of magnitude at least $\\varepsilon$ with the quadratic variation $[X]_T$ of the process. The scaling law holds universally for all continuous semimartingales with finite quadratic variation, including general Ito diffusions with nonlinear or state-dependent volatility, but fails sharply for deterministic systems -- thereby providing a theoretically-certfied method of distinguishing between these dynamics, as opposed to the subjective entropy or recurrence based state of the art methods. We construct a robust data-driven diffusion test. The method compares the empirical excursion counts against the theoretical expectation. The resulting ratio $K(\\varepsilon)=N_{\\varepsilon}^{\\mathrm{emp}}/N_{\\varepsilon}^{\\mathrm{theory}}$ is then summarized by a log-log slope deviation measuring the $\\varepsilon^{-2}$ law that provides a classification into diffusion-like or not. We demonstrate the method on canonical stochastic systems, some periodic and chaotic maps and systems with additive white noise, as well as the stochastic Duffing system. The approach is nonparametric, model-free, and relies only on the universal small-scale structure of continuous semimartingales.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 80,
        "date": "2026-01-12",
        "date_url": "https://arxiv.org/catchup/cs.LG/2026-01-12?abs=True",
        "arxiv_id": "2601.06025",
        "abs_url": "https://arxiv.org/abs/2601.06025",
        "pdf_url": "https://arxiv.org/pdf/2601.06025",
        "title": "Manifold limit for the training of shallow graph convolutional neural networks",
        "authors": [
            "Johanna Tengler",
            "Christoph Brune",
            "José A. Iglesias"
        ],
        "comments": "44 pages, 0 figures, 1 table",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Functional Analysis (math.FA); Optimization and Control (math.OC)",
        "abstract": "We study the discrete-to-continuum consistency of the training of shallow graph convolutional neural networks (GCNNs) on proximity graphs of sampled point clouds under a manifold assumption. Graph convolution is defined spectrally via the graph Laplacian, whose low-frequency spectrum approximates that of the Laplace-Beltrami operator of the underlying smooth manifold, and shallow GCNNs of possibly infinite width are linear functionals on the space of measures on the parameter space. From this functional-analytic perspective, graph signals are seen as spatial discretizations of functions on the manifold, which leads to a natural notion of training data consistent across graph resolutions. To enable convergence results, the continuum parameter space is chosen as a weakly compact product of unit balls, with Sobolev regularity imposed on the output weight and bias, but not on the convolutional parameter. The corresponding discrete parameter spaces inherit the corresponding spectral decay, and are additionally restricted by a frequency cutoff adapted to the informative spectral window of the graph Laplacians. Under these assumptions, we prove $\\Gamma$-convergence of regularized empirical risk minimization functionals and corresponding convergence of their global minimizers, in the sense of weak convergence of the parameter measures and uniform convergence of the functions over compact sets. This provides a formalization of mesh and sample independence for the training of such networks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    }
]