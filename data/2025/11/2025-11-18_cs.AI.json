[
    {
        "order": 1,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11591",
        "abs_url": "https://arxiv.org/abs/2511.11591",
        "pdf_url": "https://arxiv.org/pdf/2511.11591",
        "title": "LLM-Generated Negative News Headlines Dataset: Creation and Benchmarking Against Real Journalism",
        "authors": [
            "Olusola Babalola",
            "Bolanle Ojokoh",
            "Olutayo Boyinbode"
        ],
        "comments": "50 pages, 19 figures, 9 tables",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "This research examines the potential of datasets generated by Large Language Models (LLMs) to support Natural Language Processing (NLP) tasks, aiming to overcome challenges related to data acquisition and privacy concerns associated with real-world data. Focusing on negative valence text, a critical component of sentiment analysis, we explore the use of LLM-generated synthetic news headlines as an alternative to real-world data. A specialized corpus of negative news headlines was created using tailored prompts to capture diverse negative sentiments across various societal domains. The synthetic headlines were validated by expert review and further analyzed in embedding space to assess their alignment with real-world negative news in terms of content, tone, length, and style. Key metrics such as correlation with real headlines, perplexity, coherence, and realism were evaluated. The synthetic dataset was benchmarked against two sets of real news headlines using evaluations including the Comparative Perplexity Test, Comparative Readability Test, Comparative POS Profiling, BERTScore, and Comparative Semantic Similarity. Results show the generated headlines match real headlines with the only marked divergence being in the proper noun score of the POS profile test.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 2,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11597",
        "abs_url": "https://arxiv.org/abs/2511.11597",
        "pdf_url": "https://arxiv.org/pdf/2511.11597",
        "title": "CLINB: A Climate Intelligence Benchmark for Foundational Models",
        "authors": [
            "Michelle Chen Huebscher",
            "Katharine Mach",
            "Aleksandar Stanić",
            "Markus Leippold",
            "Ben Gaiarin",
            "Zeke Hausfather",
            "Elisa Rawat",
            "Erich Fischer",
            "Massimiliano Ciaramita",
            "Joeri Rogelj",
            "Christian Buck",
            "Lierni Sestorain Saralegui",
            "Reto Knutti"
        ],
        "comments": "Questions, system prompt and model judge prompts available here: this https URL",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Evaluating how Large Language Models (LLMs) handle complex, specialized knowledge remains a critical challenge. We address this through the lens of climate change by introducing CLINB, a benchmark that assesses models on open-ended, grounded, multimodal question answering tasks with clear requirements for knowledge quality and evidential support. CLINB relies on a dataset of real users' questions and evaluation rubrics curated by leading climate scientists. We implement and validate a model-based evaluation process and evaluate several frontier models. Our findings reveal a critical dichotomy. Frontier models demonstrate remarkable knowledge synthesis capabilities, often exhibiting PhD-level understanding and presentation quality. They outperform \"hybrid\" answers curated by domain experts assisted by weaker models. However, this performance is countered by failures in grounding. The quality of evidence varies, with substantial hallucination rates for references and images. We argue that bridging this gap between knowledge synthesis and verifiable attribution is essential for the deployment of AI in scientific workflows and that reliable, interpretable benchmarks like CLINB are needed to progress towards building trustworthy AI systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 3,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11599",
        "abs_url": "https://arxiv.org/abs/2511.11599",
        "pdf_url": "https://arxiv.org/pdf/2511.11599",
        "title": "SynBullying: A Multi LLM Synthetic Conversational Dataset for Cyberbullying Detectio",
        "authors": [
            "Arefeh Kazemi",
            "Hamza Qadeer",
            "Joachim Wagner",
            "Hossein Hosseini",
            "Sri Balaaji Natarajan Kalaivendan",
            "Brian Davis"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computers and Society (cs.CY)",
        "abstract": "We introduce SynBullying, a synthetic multi-LLM conversational dataset for studying and detecting cyberbullying (CB). SynBullying provides a scalable and ethically safe alternative to human data collection by leveraging large language models (LLMs) to simulate realistic bullying interactions. The dataset offers (i) conversational structure, capturing multi-turn exchanges rather than isolated posts; (ii) context-aware annotations, where harmfulness is assessed within the conversational flow considering context, intent, and discourse dynamics; and (iii) fine-grained labeling, covering various CB categories for detailed linguistic and behavioral analysis. We evaluate SynBullying across five dimensions, including conversational structure, lexical patterns, sentiment/toxicity, role dynamics, harm intensity, and CB-type distribution. We further examine its utility by testing its performance as standalone training data and as an augmentation source for CB classification.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 4,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11600",
        "abs_url": "https://arxiv.org/abs/2511.11600",
        "pdf_url": "https://arxiv.org/pdf/2511.11600",
        "title": "CausalGuard: A Smart System for Detecting and Preventing False Information in Large Language Models",
        "authors": [
            "Piyushkumar Patel"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)",
        "abstract": "While large language models have transformed how we interact with AI systems, they have a critical weakness: they confidently state false information that sounds entirely plausible. This \"hallucination\" problem has become a major barrier to using these models where accuracy matters most. Existing solutions either require retraining the entire model, add significant computational costs, or miss the root causes of why these hallucinations occur in the first place. We present CausalGuard, a new approach that combines causal reasoning with symbolic logic to catch and prevent hallucinations as they happen. Unlike previous methods that only check outputs after generation, our system understands the causal chain that leads to false statements and intervenes early in the process. CausalGuard works through two complementary paths: one that traces causal relationships between what the model knows and what it generates, and another that checks logical consistency using automated reasoning. Testing across twelve different benchmarks, we found that CausalGuard correctly identifies hallucinations 89.3\\% of the time while missing only 8.3\\% of actual hallucinations. More importantly, it reduces false claims by nearly 80\\% while keeping responses natural and helpful. The system performs especially well on complex reasoning tasks where multiple steps of logic are required. Because CausalGuard shows its reasoning process, it works well in sensitive areas like medical diagnosis or financial analysis where understanding why a decision was made matters as much as the decision itself.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 5,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11611",
        "abs_url": "https://arxiv.org/abs/2511.11611",
        "pdf_url": "https://arxiv.org/pdf/2511.11611",
        "title": "Quantifying Skill and Chance: A Unified Framework for the Geometry of Games",
        "authors": [
            "David H. Silver"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multiagent Systems (cs.MA)",
        "abstract": "We introduce a quantitative framework for separating skill and chance in games by modeling them as complementary sources of control over stochastic decision trees. We define the Skill-Luck Index S(G) in [-1, 1] by decomposing game outcomes into skill leverage K and luck leverage L. Applying this to 30 games reveals a continuum from pure chance (coin toss, S = -1) through mixed domains such as backgammon (S = 0, Sigma = 1.20) to pure skill (chess, S = +1, Sigma = 0). Poker exhibits moderate skill dominance (S = 0.33) with K = 0.40 +/- 0.03 and Sigma = 0.80. We further introduce volatility Sigma to quantify outcome uncertainty over successive turns. The framework extends to general stochastic decision systems, enabling principled comparisons of player influence, game balance, and predictive stability, with applications to game design, AI evaluation, and risk assessment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 6,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11752",
        "abs_url": "https://arxiv.org/abs/2511.11752",
        "pdf_url": "https://arxiv.org/pdf/2511.11752",
        "title": "Towards autonomous quantum physics research using LLM agents with access to intelligent tools",
        "authors": [
            "Sören Arlt",
            "Xuemei Gu",
            "Mario Krenn"
        ],
        "comments": "24 pages, 5 figures",
        "subjects": "Artificial Intelligence (cs.AI); Digital Libraries (cs.DL); Quantum Physics (quant-ph)",
        "abstract": "Artificial intelligence (AI) is used in numerous fields of science, yet the initial research questions and targets are still almost always provided by human researchers. AI-generated creative ideas in science are rare and often vague, so that it remains a human task to execute them. Automating idea generation and implementation in one coherent system would significantly shift the role of humans in the scientific process. Here we present AI-Mandel, an LLM agent that can generate and implement ideas in quantum physics. AI-Mandel formulates ideas from the literature and uses a domain-specific AI tool to turn them into concrete experiment designs that can readily be implemented in laboratories. The generated ideas by AI-Mandel are often scientifically interesting - for two of them we have already written independent scientific follow-up papers. The ideas include new variations of quantum teleportation, primitives of quantum networks in indefinite causal orders, and new concepts of geometric phases based on closed loops of quantum information transfer. AI-Mandel is a prototypical demonstration of an AI physicist that can generate and implement concrete, actionable ideas. Building such a system is not only useful to accelerate science, but it also reveals concrete open challenges on the path to human-level artificial scientists.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 7,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11770",
        "abs_url": "https://arxiv.org/abs/2511.11770",
        "pdf_url": "https://arxiv.org/pdf/2511.11770",
        "title": "Learning to Refine: An Agentic RL Approach for Iterative SPARQL Query Construction",
        "authors": [
            "Floris Vossebeld",
            "Shenghui Wang"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Generating complex, logically-sound SPARQL queries for multi-hop questions remains a critical bottleneck for Knowledge Graph Question Answering, as the brittle nature of one-shot generation by Large Language Models (LLMs) hinders reliable interaction with structured data. Current methods lack the adaptive policies needed to dynamically debug queries based on real-time execution feedback. This paper introduces a novel agentic framework where an LLM learns a resilient policy for the sequential process of iterative SPARQL construction. We show that a compact 3B-parameter model, trained exclusively via outcome-driven Reinforcement Learning (GRPO) without supervised fine-tuning, can learn effective policies for this task, discovering how to systematically recover from execution errors and refine its queries toward a correct answer. On a curated, executable single-answer subset of LC-QuAD 2.0, our agent achieves 49.7\\% accuracy post-entity-linking, a significant 17.5 percentage point improvement over the strongest iterative zero-shot baseline. Further analysis reveals that while the agent's capability is driven by RL, its performance is enhanced by an explicit deliberative reasoning step that acts as a cognitive scaffold to improve policy precision. This work presents a generalizable blueprint for teaching agents to master formal, symbolic tools through interaction, bridging the gap between probabilistic LLMs and the structured world of Knowledge Graphs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 8,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11773",
        "abs_url": "https://arxiv.org/abs/2511.11773",
        "pdf_url": "https://arxiv.org/pdf/2511.11773",
        "title": "On the Measure of a Model: From Intelligence to Generality",
        "authors": [
            "Ruchira Dhar",
            "Ninell Oldenburg",
            "Anders Soegaard"
        ],
        "comments": "Accepted at EurIPS Workshop on \"The Science of Benchmarking and Evaluating AI\"",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Benchmarks such as ARC, Raven-inspired tests, and the Blackbird Task are widely used to evaluate the intelligence of large language models (LLMs). Yet, the concept of intelligence remains elusive- lacking a stable definition and failing to predict performance on practical tasks such as question answering, summarization, or coding. Optimizing for such benchmarks risks misaligning evaluation with real-world utility. Our perspective is that evaluation should be grounded in generality rather than abstract notions of intelligence. We identify three assumptions that often underpin intelligence-focused evaluation: generality, stability, and realism. Through conceptual and formal analysis, we show that only generality withstands conceptual and empirical scrutiny. Intelligence is not what enables generality; generality is best understood as a multitask learning problem that directly links evaluation to measurable performance breadth and reliability. This perspective reframes how progress in AI should be assessed and proposes generality as a more stable foundation for evaluating capability across diverse and evolving tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 9,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11816",
        "abs_url": "https://arxiv.org/abs/2511.11816",
        "pdf_url": "https://arxiv.org/pdf/2511.11816",
        "title": "Do LLMs Really Struggle at NL-FOL Translation? Revealing their Strengths via a Novel Benchmarking Strategy",
        "authors": [
            "Andrea Brunello",
            "Luca Geatti",
            "Michele Mignani",
            "Angelo Montanari",
            "Nicola Saccomanno"
        ],
        "comments": "Full version of the paper accepted for publication at The 40th Annual AAAI Conference on Artificial Intelligence (AAAI 2026)",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Logic in Computer Science (cs.LO)",
        "abstract": "Due to its expressiveness and unambiguous nature, First-Order Logic (FOL) is a powerful formalism for representing concepts expressed in natural language (NL). This is useful, e.g., for specifying and verifying desired system properties. While translating FOL into human-readable English is relatively straightforward, the inverse problem, converting NL to FOL (NL-FOL translation), has remained a longstanding challenge, for both humans and machines. Although the emergence of Large Language Models (LLMs) promised a breakthrough, recent literature provides contrasting results on their ability to perform NL-FOL translation. In this work, we provide a threefold contribution. First, we critically examine existing datasets and protocols for evaluating NL-FOL translation performance, revealing key limitations that may cause a misrepresentation of LLMs' actual capabilities. Second, to overcome these shortcomings, we propose a novel evaluation protocol explicitly designed to distinguish genuine semantic-level logical understanding from superficial pattern recognition, memorization, and dataset contamination. Third, using this new approach, we show that state-of-the-art, dialogue-oriented LLMs demonstrate strong NL-FOL translation skills and a genuine grasp of sentence-level logic, whereas embedding-centric models perform markedly worse.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 10,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11899",
        "abs_url": "https://arxiv.org/abs/2511.11899",
        "pdf_url": "https://arxiv.org/pdf/2511.11899",
        "title": "End to End AI System for Surgical Gesture Sequence Recognition and Clinical Outcome Prediction",
        "authors": [
            "Xi Li",
            "Nicholas Matsumoto",
            "Ujjwal Pasupulety",
            "Atharva Deo",
            "Cherine Yang",
            "Jay Moran",
            "Miguel E. Hernandez",
            "Peter Wager",
            "Jasmine Lin",
            "Jeanine Kim",
            "Alvin C. Goh",
            "Christian Wagner",
            "Geoffrey A. Sonn",
            "Andrew J. Hung"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Fine-grained analysis of intraoperative behavior and its impact on patient outcomes remain a longstanding challenge. We present Frame-to-Outcome (F2O), an end-to-end system that translates tissue dissection videos into gesture sequences and uncovers patterns associated with postoperative outcomes. Leveraging transformer-based spatial and temporal modeling and frame-wise classification, F2O robustly detects consecutive short (~2 seconds) gestures in the nerve-sparing step of robot-assisted radical prostatectomy (AUC: 0.80 frame-level; 0.81 video-level). F2O-derived features (gesture frequency, duration, and transitions) predicted postoperative outcomes with accuracy comparable to human annotations (0.79 vs. 0.75; overlapping 95% CI). Across 25 shared features, effect size directions were concordant with small differences (~ 0.07), and strong correlation (r = 0.96, p < 1e-14). F2O also captured key patterns linked to erectile function recovery, including prolonged tissue peeling and reduced energy use. By enabling automatic interpretable assessment, F2O establishes a foundation for data-driven surgical feedback and prospective clinical decision support.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 11,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11914",
        "abs_url": "https://arxiv.org/abs/2511.11914",
        "pdf_url": "https://arxiv.org/pdf/2511.11914",
        "title": "Forgetting-MarI: LLM Unlearning via Marginal Information Regularization",
        "authors": [
            "Shizhou Xu",
            "Yuan Ni",
            "Stefan Broecker",
            "Thomas Strohmer"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Cryptography and Security (cs.CR); Information Theory (cs.IT); Machine Learning (cs.LG)",
        "abstract": "As AI models are trained on ever-expanding datasets, the ability to remove the influence of specific data from trained models has become essential for privacy protection and regulatory compliance. Unlearning addresses this challenge by selectively removing parametric knowledge from the trained models without retraining from scratch, which is critical for resource-intensive models such as Large Language Models (LLMs). Existing unlearning methods often degrade model performance by removing more information than necessary when attempting to ''forget'' specific data. We introduce Forgetting-MarI, an LLM unlearning framework that provably removes only the additional (marginal) information contributed by the data to be unlearned, while preserving the information supported by the data to be retained. By penalizing marginal information, our method yields an explicit upper bound on the unlearn dataset's residual influence in the trained models, providing provable undetectability. Extensive experiments confirm that our approach outperforms current state-of-the-art unlearning methods, delivering reliable forgetting and better preserved general model performance across diverse benchmarks. This advancement represents an important step toward making AI systems more controllable and compliant with privacy and copyright regulations without compromising their effectiveness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 12,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11916",
        "abs_url": "https://arxiv.org/abs/2511.11916",
        "pdf_url": "https://arxiv.org/pdf/2511.11916",
        "title": "An Analysis of Architectural Impact on LLM-based Abstract Visual Reasoning: A Systematic Benchmark on RAVEN-FAIR",
        "authors": [
            "Sinan Urgun",
            "Seçkin Arı"
        ],
        "comments": "23 pages, 9 figures",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "This study aims to systematically evaluate the performance of large language models (LLMs) in abstract visual reasoning problems. We examined four LLM models (GPT-4.1-Mini, Claude-3.5-Haiku, Gemini-1.5-Flash, Llama-3.3-70b) utilizing four different reasoning architectures (single-shot, embedding-controlled repetition, self-reflection, and multi-agent) on the RAVEN-FAIR dataset. Visual responses generated through a three-stage process (JSON extraction, LLM reasoning, and Tool Function) were evaluated using SSIM and LPIPS metrics; Chain-of-Thought scores and error types (semantic hallucination, numeric misperception) were analyzed. Results demonstrate that GPT-4.1-Mini consistently achieved the highest overall accuracy across all architectures, indicating a strong reasoning capability. While the multi-agent architecture occasionally altered semantic and numeric balance across models, these effects were not uniformly beneficial. Instead, each model exhibited distinct sensitivity patterns to architectural design, underscoring that reasoning effectiveness remains model-specific. Variations in response coverage further emerged as a confounding factor that complicates direct cross-architecture comparison. To estimate the upper-bound performance of each configuration, we report the best of five independent runs, representing a best-case scenario rather than an averaged outcome. This multi-run strategy aligns with recent recommendations, which emphasize that single-run evaluations are fragile and may lead to unreliable conclusions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 13,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11921",
        "abs_url": "https://arxiv.org/abs/2511.11921",
        "pdf_url": "https://arxiv.org/pdf/2511.11921",
        "title": "Looking Forward: Challenges and Opportunities in Agentic AI Reliability",
        "authors": [
            "Liudong Xing",
            "Janet"
        ],
        "comments": "13 pages, 6 figures; This is a preprint of a chapter accepted for publication in Generative and Agentic AI Reliability: Architectures, Challenges, and Trust for Autonomous Systems, published by SpringerNature",
        "subjects": "Artificial Intelligence (cs.AI); Emerging Technologies (cs.ET); Performance (cs.PF)",
        "abstract": "This chapter presents perspectives for challenges and future development in building reliable AI systems, particularly, agentic AI systems. Several open research problems related to mitigating the risks of cascading failures are discussed. The chapter also sheds lights on research challenges and opportunities in aspects including dynamic environments, inconsistent task execution, unpredictable emergent behaviors, as well as resource-intensive reliability mechanisms. In addition, several research directions along the line of testing and evaluating reliability of agentic AI systems are also discussed.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 14,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11924",
        "abs_url": "https://arxiv.org/abs/2511.11924",
        "pdf_url": "https://arxiv.org/pdf/2511.11924",
        "title": "A Neuromorphic Architecture for Scalable Event-Based Control",
        "authors": [
            "Yongkang Huo",
            "Fulvio Forni",
            "Rodolphe Sepulchre"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "This paper introduces the ``rebound Winner-Take-All (RWTA)\" motif as the basic element of a scalable neuromorphic control architecture. From the cellular level to the system level, the resulting architecture combines the reliability of discrete computation and the tunability of continuous regulation: it inherits the discrete computation capabilities of winner-take-all state machines and the continuous tuning capabilities of excitable biophysical circuits. The proposed event-based framework addresses continuous rhythmic generation and discrete decision-making in a unified physical modeling language. We illustrate the versatility, robustness, and modularity of the architecture through the nervous system design of a snake robot.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 15,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11945",
        "abs_url": "https://arxiv.org/abs/2511.11945",
        "pdf_url": "https://arxiv.org/pdf/2511.11945",
        "title": "Augmenting The Weather: A Hybrid Counterfactual-SMOTE Algorithm for Improving Crop Growth Prediction When Climate Changes",
        "authors": [
            "Mohammed Temraz",
            "Mark T Keane"
        ],
        "comments": "31 pages, 8 figures",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "In recent years, humanity has begun to experience the catastrophic effects of climate change as economic sectors (such as agriculture) struggle with unpredictable and extreme weather events. Artificial Intelligence (AI) should help us handle these climate challenges but its most promising solutions are not good at dealing with climate-disrupted data; specifically, machine learning methods that work from historical data-distributions, are not good at handling out-of-distribution, outlier events. In this paper, we propose a novel data augmentation method, that treats the predictive problems around climate change as being, in part, due to class-imbalance issues; that is, prediction from historical datasets is difficult because, by definition, they lack sufficient minority-class instances of \"climate outlier events\". This novel data augmentation method -- called Counterfactual-Based SMOTE (CFA-SMOTE) -- combines an instance-based counterfactual method from Explainable AI (XAI) with the well-known class-imbalance method, SMOTE. CFA-SMOTE creates synthetic data-points representing outlier, climate-events that augment the dataset to improve predictive performance. We report comparative experiments using this CFA-SMOTE method, comparing it to benchmark counterfactual and class-imbalance methods under different conditions (i.e., class-imbalance ratios). The focal climate-change domain used relies on predicting grass growth on Irish dairy farms, during Europe-wide drought and forage crisis of 2018.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 16,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11954",
        "abs_url": "https://arxiv.org/abs/2511.11954",
        "pdf_url": "https://arxiv.org/pdf/2511.11954",
        "title": "LLM-Assisted Formalization Enables Deterministic Detection of Statutory Inconsistency in the Internal Revenue Code",
        "authors": [
            "Borchuluun Yadamsuren",
            "Steven Keith Platt",
            "Miguel Diaz"
        ],
        "comments": "29 pages, 3 appendices with Prolog code and full codebase available at: this https URL",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "This study introduces a hybrid neuro-symbolic framework that achieves deterministic detection of statutory inconsistency in complex law. We use the U.S. Internal Revenue Code (IRC) as a case study because its complexity makes it a fertile domain for identifying conflicts. Our research offers a solution for detecting inconsistent provisions by combining Large Language Models (LLMs) with symbolic logic. LLM-based methods can support compliance, fairness, and statutory drafting, yet tax-specific applications remain sparse. A key challenge is that such models struggle with hierarchical processing and deep structured reasoning, especially over long text. This research addresses these gaps through experiments using GPT-4o, GPT-5, and Prolog. GPT-4o was first used to translate Section 121 into Prolog rules and refine them in SWISH. These rules were then incorporated into prompts to test whether Prolog-augmented prompting improved GPT-4o's inconsistency detection. GPT-4o, whether prompted with natural language alone or with Prolog augmentation, detected the inconsistency in only one of three strategies (33 percent accuracy), but its reasoning quality differed: natural-language prompting achieved 100 percent rule coverage, while Prolog-augmented prompting achieved 66 percent, indicating more incomplete statutory analysis. In contrast to probabilistic prompting, the hybrid Prolog model produced deterministic and reproducible results. Guided by GPT-5 for refinement, the model formalized the IRC section's competing interpretations and successfully detected an inconsistency zone. Validation tests confirm that the Prolog implementation is accurate, internally consistent, deterministic, and capable of autonomously identifying inconsistencies. These findings show that LLM-assisted formalization, anchored in symbolic logic, enables transparent and reliable statutory inconsistency detection.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 17,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11990",
        "abs_url": "https://arxiv.org/abs/2511.11990",
        "pdf_url": "https://arxiv.org/pdf/2511.11990",
        "title": "Improving Autoformalization Using Direct Dependency Retrieval",
        "authors": [
            "Shaoqi Wang",
            "Lu Yu",
            "Chunjie Yang"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "The convergence of deep learning and formal mathematics has spurred research in formal verification. Statement autoformalization, a crucial first step in this process, aims to translate informal descriptions into machine-verifiable representations but remains a significant challenge. The core difficulty lies in the fact that existing methods often suffer from a lack of contextual awareness, leading to hallucination of formal definitions and theorems. Furthermore, current retrieval-augmented approaches exhibit poor precision and recall for formal library dependency retrieval, and lack the scalability to effectively leverage ever-growing public datasets. To bridge this gap, we propose a novel retrieval-augmented framework based on DDR (\\textit{Direct Dependency Retrieval}) for statement autoformalization. Our DDR method directly generates candidate library dependencies from natural language mathematical descriptions and subsequently verifies their existence within the formal library via an efficient suffix array check. Leveraging this efficient search mechanism, we constructed a dependency retrieval dataset of over 500,000 samples and fine-tuned a high-precision DDR model. Experimental results demonstrate that our DDR model significantly outperforms SOTA methods in both retrieval precision and recall. Consequently, an autoformalizer equipped with DDR shows consistent performance advantages in both single-attempt accuracy and multi-attempt stability compared to models using traditional selection-based RAG methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 18,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12003",
        "abs_url": "https://arxiv.org/abs/2511.12003",
        "pdf_url": "https://arxiv.org/pdf/2511.12003",
        "title": "Look As You Think: Unifying Reasoning and Visual Evidence Attribution for Verifiable Document RAG via Reinforcement Learning",
        "authors": [
            "Shuochen Liu",
            "Pengfei Luo",
            "Chao Zhang",
            "Yuhao Chen",
            "Haotian Zhang",
            "Qi Liu",
            "Xin Kou",
            "Tong Xu",
            "Enhong Chen"
        ],
        "comments": "Poster of AAAI'2026",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Aiming to identify precise evidence sources from visual documents, visual evidence attribution for visual document retrieval-augmented generation (VD-RAG) ensures reliable and verifiable predictions from vision-language models (VLMs) in multimodal question answering. Most existing methods adopt end-to-end training to facilitate intuitive answer verification. However, they lack fine-grained supervision and progressive traceability throughout the reasoning process. In this paper, we introduce the Chain-of-Evidence (CoE) paradigm for VD-RAG. CoE unifies Chain-of-Thought (CoT) reasoning and visual evidence attribution by grounding reference elements in reasoning steps to specific regions with bounding boxes and page indexes. To enable VLMs to generate such evidence-grounded reasoning, we propose Look As You Think (LAT), a reinforcement learning framework that trains models to produce verifiable reasoning paths with consistent attribution. During training, LAT evaluates the attribution consistency of each evidence region and provides rewards only when the CoE trajectory yields correct answers, encouraging process-level self-verification. Experiments on vanilla Qwen2.5-VL-7B-Instruct with Paper- and Wiki-VISA benchmarks show that LAT consistently improves the vanilla model in both single- and multi-image settings, yielding average gains of 8.23% in soft exact match (EM) and 47.0% in IoU@0.5. Meanwhile, LAT not only outperforms the supervised fine-tuning baseline, which is trained to directly produce answers with attribution, but also exhibits stronger generalization across domains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 19,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12060",
        "abs_url": "https://arxiv.org/abs/2511.12060",
        "pdf_url": "https://arxiv.org/pdf/2511.12060",
        "title": "Intelligent Collaborative Optimization for Rubber Tyre Film Production Based on Multi-path Differentiated Clipping Proximal Policy Optimization",
        "authors": [
            "Yinghao Ruan",
            "Wei Pang",
            "Shuaihao Liu",
            "Huili Yang",
            "Leyi Han",
            "Xinghui Dong"
        ],
        "comments": "10 pages",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "The advent of smart manufacturing is addressing the limitations of traditional centralized scheduling and inflexible production line configurations in the rubber tyre industry, especially in terms of coping with dynamic production demands. Contemporary tyre manufacturing systems form complex networks of tightly coupled subsystems pronounced nonlinear interactions and emergent dynamics. This complexity renders the effective coordination of multiple subsystems, posing an essential yet formidable task. For high-dimensional, multi-objective optimization problems in this domain, we introduce a deep reinforcement learning algorithm: Multi-path Differentiated Clipping Proximal Policy Optimization (MPD-PPO). This algorithm employs a multi-branch policy architecture with differentiated gradient clipping constraints to ensure stable and efficient high-dimensional policy updates. Validated through experiments on width and thickness control in rubber tyre film production, MPD-PPO demonstrates substantial improvements in both tuning accuracy and operational efficiency. The framework successfully tackles key challenges, including high dimensionality, multi-objective trade-offs, and dynamic adaptation, thus delivering enhanced performance and production stability for real-time industrial deployment in tyre manufacturing.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 20,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12063",
        "abs_url": "https://arxiv.org/abs/2511.12063",
        "pdf_url": "https://arxiv.org/pdf/2511.12063",
        "title": "Bayesian Optimization in Language Space: An Eval-Efficient AI Self-Improvement Framework",
        "authors": [
            "Enoch Hyunwook Kang",
            "Hema Yoganarasimhan"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Large Language Models (LLMs) have recently enabled self-improving AI, i.e., AI that iteratively generates, evaluates, and refines its own outcomes. Recent studies have shown that self-improving AI focusing on prompt optimization can outperform state-of-the-art reinforcement-learning fine-tuned LLMs. Here, their `performance' is typically measured by query efficiency - the number of LLM-generated solution samples required to meet a certain performance threshold. However, in many societal applications, the primary limitation is not generating new solutions but evaluating them. For instance, evaluating an ad's effectiveness requires significant human feedback, which is far more costly and time-consuming than generating a candidate ad. To optimize for the evaluation efficiency objective, a natural approach is to extend Bayesian Optimization (BO), a framework proven optimal for evaluation efficiency, to the language domain. However, the difficulty of directly estimating suitable acquisition functions in LLMs' minds makes this extension challenging. This paper overcomes this challenge by proving that the combination of the simple and widely used Best-of-N selection strategy and simple textual gradients (i.e., textual edits from a critic model) statistically emulates the behavior of the gradients on the canonical UCB acquisition function, which induces optimal exploration in terms of evaluation efficiency. Based on this result, we propose TextGrad-Best-of-N Bayesian Optimization (T-BoN BO), a simple and eval-efficient language-space Bayesian optimization framework for AI self-improvement. We also empirically validate T-BoN BO by applying it to automated ad alignment tasks for persona distribution, demonstrating its superior performance compared to popular state-of-the-art baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 21,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12083",
        "abs_url": "https://arxiv.org/abs/2511.12083",
        "pdf_url": "https://arxiv.org/pdf/2511.12083",
        "title": "No-Regret Strategy Solving in Imperfect-Information Games via Pre-Trained Embedding",
        "authors": [
            "Yanchang Fu",
            "Shengda Liu",
            "Pei Xu",
            "Kaiqi Huang"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "High-quality information set abstraction remains a core challenge in solving large-scale imperfect-information extensive-form games (IIEFGs)-such as no-limit Texas Hold'em-where the finite nature of spatial resources hinders strategy solving over the full game. State-of-the-art AI methods rely on pre-trained discrete clustering for abstraction, yet their hard classification irreversibly loses critical information: specifically, the quantifiable subtle differences between information sets-vital for strategy solving-thereby compromising the quality of such solving. Inspired by the word embedding paradigm in natural language processing, this paper proposes the Embedding CFR algorithm, a novel approach for solving strategies in IIEFGs within an embedding space. The algorithm pre-trains and embeds features of isolated information sets into an interconnected low-dimensional continuous space, where the resulting vectors more precisely capture both the distinctions and connections between information sets. Embedding CFR presents a strategy-solving process driven by regret accumulation and strategy updates within this embedding space, with accompanying theoretical analysis verifying its capacity to reduce cumulative regret. Experiments on poker show that with the same spatial overhead, Embedding CFR achieves significantly faster exploitability convergence compared to cluster-based abstraction algorithms, confirming its effectiveness. Furthermore, to our knowledge, it is the first algorithm in poker AI that pre-trains information set abstractions through low-dimensional embedding for strategy solving.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 22,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12089",
        "abs_url": "https://arxiv.org/abs/2511.12089",
        "pdf_url": "https://arxiv.org/pdf/2511.12089",
        "title": "KrwEmd: Revising the Imperfect-Recall Abstraction from Forgetting Everything",
        "authors": [
            "Yanchang Fu",
            "Qiyue Yin",
            "Shengda Liu",
            "Pei Xu",
            "Kaiqi Huang"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT)",
        "abstract": "Excessive abstraction is a critical challenge in hand abstraction-a task specific to games like Texas hold'em-when solving large-scale imperfect-information games, as it impairs AI performance. This issue arises from extreme implementations of imperfect-recall abstraction, which entirely discard historical information. This paper presents KrwEmd, the first practical algorithm designed to address this problem. We first introduce the k-recall winrate feature, which not only qualitatively distinguishes signal observation infosets by leveraging both future and, crucially, historical game information, but also quantitatively captures their similarity. We then develop the KrwEmd algorithm, which clusters signal observation infosets using earth mover's distance to measure discrepancies between their features. Experimental results demonstrate that KrwEmd significantly improves AI gameplay performance compared to existing algorithms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 23,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12113",
        "abs_url": "https://arxiv.org/abs/2511.12113",
        "pdf_url": "https://arxiv.org/pdf/2511.12113",
        "title": "MetaGDPO: Alleviating Catastrophic Forgetting with Metacognitive Knowledge through Group Direct Preference Optimization",
        "authors": [
            "Lanxue Zhang",
            "Yuqiang Xie",
            "Fang Fang",
            "Fanglong Dong",
            "Rui Liu",
            "Yanan Cao"
        ],
        "comments": "23 pages, 10 figures, AAAI 2026",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Large Language Models demonstrate strong reasoning capabilities, which can be effectively compressed into smaller models. However, existing datasets and fine-tuning approaches still face challenges that lead to catastrophic forgetting, particularly for models smaller than 8B. First, most datasets typically ignore the relationship between training data knowledge and the model's inherent abilities, making it difficult to preserve prior knowledge. Second, conventional training objectives often fail to constrain inherent knowledge preservation, which can result in forgetting of previously learned skills. To address these issues, we propose a comprehensive solution that alleviates catastrophic forgetting from both the data and fine-tuning approach perspectives. On the data side, we construct a dataset of 5K instances that covers multiple reasoning tasks and incorporates metacognitive knowledge, making it more tolerant and effective for distillation into smaller models. We annotate the metacognitive knowledge required to solve each question and filter the data based on task knowledge and the model's inherent skills. On the training side, we introduce GDPO (Group Direction Preference Optimization), which is better suited for resource-limited scenarios and can efficiently approximate the performance of GRPO. Guided by the large model and by implicitly constraining the optimization path through a reference model, GDPO enables more effective knowledge transfer from the large model and constrains excessive parameter drift. Extensive experiments demonstrate that our approach significantly alleviates catastrophic forgetting and improves reasoning performance on smaller models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 24,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12135",
        "abs_url": "https://arxiv.org/abs/2511.12135",
        "pdf_url": "https://arxiv.org/pdf/2511.12135",
        "title": "RTMol: Rethinking Molecule-text Alignment in a Round-trip View",
        "authors": [
            "Letian Chen",
            "Runhan Shi",
            "Gufeng Yu",
            "Yang Yang"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Aligning molecular sequence representations (e.g., SMILES notations) with textual descriptions is critical for applications spanning drug discovery, materials design, and automated chemical literature analysis. Existing methodologies typically treat molecular captioning (molecule-to-text) and text-based molecular design (text-to-molecule) as separate tasks, relying on supervised fine-tuning or contrastive learning pipelines. These approaches face three key limitations: (i) conventional metrics like BLEU prioritize linguistic fluency over chemical accuracy, (ii) training datasets frequently contain chemically ambiguous narratives with incomplete specifications, and (iii) independent optimization of generation directions leads to bidirectional inconsistency. To address these issues, we propose RTMol, a bidirectional alignment framework that unifies molecular captioning and text-to-SMILES generation through self-supervised round-trip learning. The framework introduces novel round-trip evaluation metrics and enables unsupervised training for molecular captioning without requiring paired molecule-text corpora. Experiments demonstrate that RTMol enhances bidirectional alignment performance by up to 47% across various LLMs, establishing an effective paradigm for joint molecule-text understanding and generation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 25,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12169",
        "abs_url": "https://arxiv.org/abs/2511.12169",
        "pdf_url": "https://arxiv.org/pdf/2511.12169",
        "title": "Incremental Maintenance of DatalogMTL Materialisations",
        "authors": [
            "Kaiyue Zhao",
            "Dingqi Chen",
            "Shaoyu Wang",
            "Pan Hu"
        ],
        "comments": "Accepted as oral paper at the main track of AAAI 2026",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "DatalogMTL extends the classical Datalog language with metric temporal logic (MTL), enabling expressive reasoning over temporal data. While existing reasoning approaches, such as materialisation based and automata based methods, offer soundness and completeness, they lack support for handling efficient dynamic updates, a crucial requirement for real-world applications that involve frequent data updates. In this work, we propose DRedMTL, an incremental reasoning algorithm for DatalogMTL with bounded intervals. Our algorithm builds upon the classical DRed algorithm, which incrementally updates the materialisation of a Datalog program. Unlike a Datalog materialisation which is in essence a finite set of facts, a DatalogMTL materialisation has to be represented as a finite set of facts plus periodic intervals indicating how the full materialisation can be constructed through unfolding. To cope with this, our algorithm is equipped with specifically designed operators to efficiently handle such periodic representations of DatalogMTL materialisations. We have implemented this approach and tested it on several publicly available datasets. Experimental results show that DRedMTL often significantly outperforms rematerialisation, sometimes by orders of magnitude.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 26,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12208",
        "abs_url": "https://arxiv.org/abs/2511.12208",
        "pdf_url": "https://arxiv.org/pdf/2511.12208",
        "title": "Debate over Mixed-knowledge: A Robust Multi-Agent Framework for Incomplete Knowledge Graph Question Answering",
        "authors": [
            "Jilong Liu",
            "Pengyang Shao",
            "Wei Qin",
            "Fei Liu",
            "Yonghui Yang",
            "Richang Hong"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Knowledge Graph Question Answering (KGQA) aims to improve factual accuracy by leveraging structured knowledge. However, real-world Knowledge Graphs (KGs) are often incomplete, leading to the problem of Incomplete KGQA (IKGQA). A common solution is to incorporate external data to fill knowledge gaps, but existing methods lack the capacity to adaptively and contextually fuse multiple sources, failing to fully exploit their complementary strengths. To this end, we propose Debate over Mixed-knowledge (DoM), a novel framework that enables dynamic integration of structured and unstructured knowledge for IKGQA. Built upon the Multi-Agent Debate paradigm, DoM assigns specialized agents to perform inference over knowledge graphs and external texts separately, and coordinates their outputs through iterative interaction. It decomposes the input question into sub-questions, retrieves evidence via dual agents (KG and Retrieval-Augmented Generation, RAG), and employs a judge agent to evaluate and aggregate intermediate answers. This collaboration exploits knowledge complementarity and enhances robustness to KG incompleteness. In addition, existing IKGQA datasets simulate incompleteness by randomly removing triples, failing to capture the irregular and unpredictable nature of real-world knowledge incompleteness. To address this, we introduce a new dataset, Incomplete Knowledge Graph WebQuestions, constructed by leveraging real-world knowledge updates. These updates reflect knowledge beyond the static scope of KGs, yielding a more realistic and challenging benchmark. Through extensive experiments, we show that DoM consistently outperforms state-of-the-art baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 27,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12214",
        "abs_url": "https://arxiv.org/abs/2511.12214",
        "pdf_url": "https://arxiv.org/pdf/2511.12214",
        "title": "ViTE: Virtual Graph Trajectory Expert Router for Pedestrian Trajectory Prediction",
        "authors": [
            "Ruochen Li",
            "Zhanxing Zhu",
            "Tanqiu Qiao",
            "Hubert P. H. Shum"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Pedestrian trajectory prediction is critical for ensuring safety in autonomous driving, surveillance systems, and urban planning applications. While early approaches primarily focus on one-hop pairwise relationships, recent studies attempt to capture high-order interactions by stacking multiple Graph Neural Network (GNN) layers. However, these approaches face a fundamental trade-off: insufficient layers may lead to under-reaching problems that limit the model's receptive field, while excessive depth can result in prohibitive computational costs. We argue that an effective model should be capable of adaptively modeling both explicit one-hop interactions and implicit high-order dependencies, rather than relying solely on architectural depth. To this end, we propose ViTE (Virtual graph Trajectory Expert router), a novel framework for pedestrian trajectory prediction. ViTE consists of two key modules: a Virtual Graph that introduces dynamic virtual nodes to model long-range and high-order interactions without deep GNN stacks, and an Expert Router that adaptively selects interaction experts based on social context using a Mixture-of-Experts design. This combination enables flexible and scalable reasoning across varying interaction patterns. Experiments on three benchmarks (ETH/UCY, NBA, and SDD) demonstrate that our method consistently achieves state-of-the-art performance, validating both its effectiveness and practical efficiency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 28,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12239",
        "abs_url": "https://arxiv.org/abs/2511.12239",
        "pdf_url": "https://arxiv.org/pdf/2511.12239",
        "title": "Beyond World Models: Rethinking Understanding in AI Models",
        "authors": [
            "Tarun Gupta",
            "Danish Pruthi"
        ],
        "comments": "Accepted to AAAI 2026 (Main Track)",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "World models have garnered substantial interest in the AI community. These are internal representations that simulate aspects of the external world, track entities and states, capture causal relationships, and enable prediction of consequences. This contrasts with representations based solely on statistical correlations. A key motivation behind this research direction is that humans possess such mental world models, and finding evidence of similar representations in AI models might indicate that these models \"understand\" the world in a human-like way. In this paper, we use case studies from the philosophy of science literature to critically examine whether the world model framework adequately characterizes human-level understanding. We focus on specific philosophical analyses where the distinction between world model capabilities and human understanding is most pronounced. While these represent particular views of understanding rather than universal definitions, they help us explore the limits of world models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 29,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12254",
        "abs_url": "https://arxiv.org/abs/2511.12254",
        "pdf_url": "https://arxiv.org/pdf/2511.12254",
        "title": "Mobile-Agent-RAG: Driving Smart Multi-Agent Coordination with Contextual Knowledge Empowerment for Long-Horizon Mobile Automation",
        "authors": [
            "Yuxiang Zhou",
            "Jichang Li",
            "Yanhao Zhang",
            "Haonan Lu",
            "Guanbin Li"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)",
        "abstract": "Mobile agents show immense potential, yet current state-of-the-art (SoTA) agents exhibit inadequate success rates on real-world, long-horizon, cross-application tasks. We attribute this bottleneck to the agents' excessive reliance on static, internal knowledge within MLLMs, which leads to two critical failure points: 1) strategic hallucinations in high-level planning and 2) operational errors during low-level execution on user interfaces (UI). The core insight of this paper is that high-level planning and low-level UI operations require fundamentally distinct types of knowledge. Planning demands high-level, strategy-oriented experiences, whereas operations necessitate low-level, precise instructions closely tied to specific app UIs. Motivated by these insights, we propose Mobile-Agent-RAG, a novel hierarchical multi-agent framework that innovatively integrates dual-level retrieval augmentation. At the planning stage, we introduce Manager-RAG to reduce strategic hallucinations by retrieving human-validated comprehensive task plans that provide high-level guidance. At the execution stage, we develop Operator-RAG to improve execution accuracy by retrieving the most precise low-level guidance for accurate atomic actions, aligned with the current app and subtask. To accurately deliver these knowledge types, we construct two specialized retrieval-oriented knowledge bases. Furthermore, we introduce Mobile-Eval-RAG, a challenging benchmark for evaluating such agents on realistic multi-app, long-horizon tasks. Extensive experiments demonstrate that Mobile-Agent-RAG significantly outperforms SoTA baselines, improving task completion rate by 11.0% and step efficiency by 10.2%, establishing a robust paradigm for context-aware, reliable multi-agent mobile automation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 30,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12271",
        "abs_url": "https://arxiv.org/abs/2511.12271",
        "pdf_url": "https://arxiv.org/pdf/2511.12271",
        "title": "MoralReason: Generalizable Moral Decision Alignment For LLM Agents Using Reasoning-Level Reinforcement Learning",
        "authors": [
            "Zhiyu An",
            "Wan Du"
        ],
        "comments": "Accepted for AAAI 2026",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Large language models are increasingly influencing human moral decisions, yet current approaches focus primarily on evaluating rather than actively steering their moral decisions. We formulate this as an out-of-distribution moral alignment problem, where LLM agents must learn to apply consistent moral reasoning frameworks to scenarios beyond their training distribution. We introduce Moral-Reason-QA, a novel dataset extending 680 human-annotated, high-ambiguity moral scenarios with framework-specific reasoning traces across utilitarian, deontological, and virtue ethics, enabling systematic evaluation of moral generalization in realistic decision contexts. Our learning approach employs Group Relative Policy Optimization with composite rewards that simultaneously optimize decision alignment and framework-specific reasoning processes to facilitate learning of the underlying moral frameworks. Experimental results demonstrate successful generalization to unseen moral scenarios, with softmax-normalized alignment scores improving by +0.757 for utilitarian and +0.450 for deontological frameworks when tested on out-of-distribution evaluation sets. The experiments also reveal training challenges and promising directions that inform future research. These findings establish that LLM agents can be systematically trained to internalize and apply specific moral frameworks to novel situations, providing a critical foundation for AI safety as language models become more integrated into human decision-making processes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 31,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12306",
        "abs_url": "https://arxiv.org/abs/2511.12306",
        "pdf_url": "https://arxiv.org/pdf/2511.12306",
        "title": "UpBench: A Dynamically Evolving Real-World Labor-Market Agentic Benchmark Framework Built for Human-Centric AI",
        "authors": [
            "Darvin Yi",
            "Teng Liu",
            "Mattie Terzolo",
            "Lance Hasson",
            "Ayan Sinh",
            "Pablo Mendes",
            "Andrew Rabinovich"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computers and Society (cs.CY)",
        "abstract": "As large language model (LLM) agents increasingly undertake digital work, reliable frameworks are needed to evaluate their real-world competence, adaptability, and capacity for human collaboration. Existing benchmarks remain largely static, synthetic, or domain-limited, providing limited insight into how agents perform in dynamic, economically meaningful environments. We introduce UpBench, a dynamically evolving benchmark grounded in real jobs drawn from the global Upwork labor marketplace. Each task corresponds to a verified client transaction, anchoring evaluation in genuine work activity and financial outcomes. UpBench employs a rubric-based evaluation framework, in which expert freelancers decompose each job into detailed, verifiable acceptance criteria and assess AI submissions with per-criterion feedback. This structure enables fine-grained analysis of model strengths, weaknesses, and instruction-following fidelity beyond binary pass/fail metrics. Human expertise is integrated throughout the data pipeline (from job curation and rubric construction to evaluation) ensuring fidelity to real professional standards and supporting research on human-AI collaboration. By regularly refreshing tasks to reflect the evolving nature of online work, UpBench provides a scalable, human-centered foundation for evaluating agentic systems in authentic labor-market contexts, offering a path toward a collaborative framework, where AI amplifies human capability through partnership rather than replacement.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 32,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12344",
        "abs_url": "https://arxiv.org/abs/2511.12344",
        "pdf_url": "https://arxiv.org/pdf/2511.12344",
        "title": "Reward and Guidance through Rubrics: Promoting Exploration to Improve Multi-Domain Reasoning",
        "authors": [
            "Baolong Bi",
            "Shenghua Liu",
            "Yiwei Wang",
            "Siqian Tong",
            "Lingrui Mei",
            "Yuyao Ge",
            "Yilong Xu",
            "Jiafeng Guo",
            "Xueqi Cheng"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Recent advances in reinforcement learning (RL) have significantly improved the complex reasoning capabilities of large language models (LLMs). Despite these successes, existing methods mainly focus on single-domain RL (e.g., mathematics) with verifiable rewards (RLVR), and their reliance on purely online RL frameworks restricts the exploration space, thereby limiting reasoning performance. In this paper, we address these limitations by leveraging rubrics to provide both fine-grained reward signals and offline guidance. We propose $\\textbf{RGR-GRPO}$ (Reward and Guidance through Rubrics), a rubric-driven RL framework for multi-domain reasoning. RGR-GRPO enables LLMs to receive dense and informative rewards while exploring a larger solution space during GRPO training. Extensive experiments across 14 benchmarks spanning multiple domains demonstrate that RGR-GRPO consistently outperforms RL methods that rely solely on alternative reward schemes or offline guidance. Compared with verifiable online RL baseline, RGR-GRPO achieves average improvements of +7.0%, +5.4%, +8.4%, and +6.6% on mathematics, physics, chemistry, and general reasoning tasks, respectively. Notably, RGR-GRPO maintains stable entropy fluctuations during off-policy training and achieves superior pass@k performance, reflecting sustained exploration and effective breakthrough beyond existing performance bottlenecks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 33,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12359",
        "abs_url": "https://arxiv.org/abs/2511.12359",
        "pdf_url": "https://arxiv.org/pdf/2511.12359",
        "title": "More Than Irrational: Modeling Belief-Biased Agents",
        "authors": [
            "Yifan Zhu",
            "Sammie Katt",
            "Samuel Kaski"
        ],
        "comments": "13 pages, 8 figures. Accepted at the 40th Annual AAAI Conference on Artificial Intelligence (AAAI 2026)",
        "subjects": "Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)",
        "abstract": "Despite the explosive growth of AI and the technologies built upon it, predicting and inferring the sub-optimal behavior of users or human collaborators remains a critical challenge. In many cases, such behaviors are not a result of irrationality, but rather a rational decision made given inherent cognitive bounds and biased beliefs about the world. In this paper, we formally introduce a class of computational-rational (CR) user models for cognitively-bounded agents acting optimally under biased beliefs. The key novelty lies in explicitly modeling how a bounded memory process leads to a dynamically inconsistent and biased belief state and, consequently, sub-optimal sequential decision-making. We address the challenge of identifying the latent user-specific bound and inferring biased belief states from passive observations on the fly. We argue that for our formalized CR model family with an explicit and parameterized cognitive process, this challenge is tractable. To support our claim, we propose an efficient online inference method based on nested particle filtering that simultaneously tracks the user's latent belief state and estimates the unknown cognitive bound from a stream of observed actions. We validate our approach in a representative navigation task using memory decay as an example of a cognitive bound. With simulations, we show that (1) our CR model generates intuitively plausible behaviors corresponding to different levels of memory capacity, and (2) our inference method accurately and efficiently recovers the ground-truth cognitive bounds from limited observations ($\\le 100$ steps). We further demonstrate how this approach provides a principled foundation for developing adaptive AI assistants, enabling adaptive assistance that accounts for the user's memory limitations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 34,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12378",
        "abs_url": "https://arxiv.org/abs/2511.12378",
        "pdf_url": "https://arxiv.org/pdf/2511.12378",
        "title": "Learning to Trust: Bayesian Adaptation to Varying Suggester Reliability in Sequential Decision Making",
        "authors": [
            "Dylan M. Asmar",
            "Mykel J. Kochenderfer"
        ],
        "comments": "Under Review",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Autonomous agents operating in sequential decision-making tasks under uncertainty can benefit from external action suggestions, which provide valuable guidance but inherently vary in reliability. Existing methods for incorporating such advice typically assume static and known suggester quality parameters, limiting practical deployment. We introduce a framework that dynamically learns and adapts to varying suggester reliability in partially observable environments. First, we integrate suggester quality directly into the agent's belief representation, enabling agents to infer and adjust their reliance on suggestions through Bayesian inference over suggester types. Second, we introduce an explicit ``ask'' action allowing agents to strategically request suggestions at critical moments, balancing informational gains against acquisition costs. Experimental evaluation demonstrates robust performance across varying suggester qualities, adaptation to changing reliability, and strategic management of suggestion requests. This work provides a foundation for adaptive human-agent collaboration by addressing suggestion uncertainty in uncertain environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 35,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12439",
        "abs_url": "https://arxiv.org/abs/2511.12439",
        "pdf_url": "https://arxiv.org/pdf/2511.12439",
        "title": "Multi-agent Self-triage System with Medical Flowcharts",
        "authors": [
            "Yujia Liu",
            "Sophia Yu",
            "Hongyue Jin",
            "Jessica Wen",
            "Alexander Qian",
            "Terrence Lee",
            "Mattheus Ramsis",
            "Gi Won Choi",
            "Lianhui Qin",
            "Xin Liu",
            "Edward J. Wang"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)",
        "abstract": "Online health resources and large language models (LLMs) are increasingly used as a first point of contact for medical decision-making, yet their reliability in healthcare remains limited by low accuracy, lack of transparency, and susceptibility to unverified information. We introduce a proof-of-concept conversational self-triage system that guides LLMs with 100 clinically validated flowcharts from the American Medical Association, providing a structured and auditable framework for patient decision support. The system leverages a multi-agent framework consisting of a retrieval agent, a decision agent, and a chat agent to identify the most relevant flowchart, interpret patient responses, and deliver personalized, patient-friendly recommendations, respectively. Performance was evaluated at scale using synthetic datasets of simulated conversations. The system achieved 95.29% top-3 accuracy in flowchart retrieval (N=2,000) and 99.10% accuracy in flowchart navigation across varied conversational styles and conditions (N=37,200). By combining the flexibility of free-text interaction with the rigor of standardized clinical protocols, this approach demonstrates the feasibility of transparent, accurate, and generalizable AI-assisted self-triage, with potential to support informed patient decision-making while improving healthcare resource utilization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 36,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12485",
        "abs_url": "https://arxiv.org/abs/2511.12485",
        "pdf_url": "https://arxiv.org/pdf/2511.12485",
        "title": "ARCHE: A Novel Task to Evaluate LLMs on Latent Reasoning Chain Extraction",
        "authors": [
            "Pengze Li",
            "Jiaqi Liu",
            "Junchi Yu",
            "Lihao Liu",
            "Mingyu Ding",
            "Wanli Ouyang",
            "Shixiang Tang",
            "Xi Chen"
        ],
        "comments": "Accepted to AAAI 2026",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Large language models (LLMs) are increasingly used in scientific domains. While they can produce reasoning-like content via methods such as chain-of-thought prompting, these outputs are typically unstructured and informal, obscuring whether models truly understand the fundamental reasoning paradigms that underpin scientific inference. To address this, we introduce a novel task named Latent Reasoning Chain Extraction (ARCHE), in which models must decompose complex reasoning arguments into combinations of standard reasoning paradigms in the form of a Reasoning Logic Tree (RLT). In RLT, all reasoning steps are explicitly categorized as one of three variants of Peirce's fundamental inference modes: deduction, induction, or abduction. To facilitate this task, we release ARCHE Bench, a new benchmark derived from 70 Nature Communications articles, including more than 1,900 references and 38,000 viewpoints. We propose two logic-aware evaluation metrics: Entity Coverage (EC) for content completeness and Reasoning Edge Accuracy (REA) for step-by-step logical validity. Evaluations on 10 leading LLMs on ARCHE Bench reveal that models exhibit a trade-off between REA and EC, and none are yet able to extract a complete and standard reasoning chain. These findings highlight a substantial gap between the abilities of current reasoning models and the rigor required for scientific argumentation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 37,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12563",
        "abs_url": "https://arxiv.org/abs/2511.12563",
        "pdf_url": "https://arxiv.org/pdf/2511.12563",
        "title": "LOBERT: Generative AI Foundation Model for Limit Order Book Messages",
        "authors": [
            "Eljas Linna",
            "Kestutis Baltakys",
            "Alexandros Iosifidis",
            "Juho Kanniainen"
        ],
        "comments": "Submission for NeurIPS 2025 GenAI in Finance Workshop",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Modeling the dynamics of financial Limit Order Books (LOB) at the message level is challenging due to irregular event timing, rapid regime shifts, and the reactions of high-frequency traders to visible order flow. Previous LOB models require cumbersome data representations and lack adaptability outside their original tasks, leading us to introduce LOBERT, a general-purpose encoder-only foundation model for LOB data suitable for downstream fine-tuning. LOBERT adapts the original BERT architecture for LOB data by using a novel tokenization scheme that treats complete multi-dimensional messages as single tokens while retaining continuous representations of price, volume, and time. With these methods, LOBERT achieves leading performance in tasks such as predicting mid-price movements and next messages, while reducing the required context length compared to previous methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 38,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12579",
        "abs_url": "https://arxiv.org/abs/2511.12579",
        "pdf_url": "https://arxiv.org/pdf/2511.12579",
        "title": "Enhancing Conversational Recommender Systems with Tree-Structured Knowledge and Pretrained Language Models",
        "authors": [
            "Yongwen Ren",
            "Chao Wang",
            "Peng Du",
            "Chuan Qin",
            "Dazhong Shen",
            "Hui Xiong"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Recent advances in pretrained language models (PLMs) have significantly improved conversational recommender systems (CRS), enabling more fluent and context-aware interactions. To further enhance accuracy and mitigate hallucination, many methods integrate PLMs with knowledge graphs (KGs), but face key challenges: failing to fully exploit PLM reasoning over graph relationships, indiscriminately incorporating retrieved knowledge without context filtering, and neglecting collaborative preferences in multi-turn dialogues. To this end, we propose PCRS-TKA, a prompt-based framework employing retrieval-augmented generation to integrate PLMs with KGs. PCRS-TKA constructs dialogue-specific knowledge trees from KGs and serializes them into texts, enabling structure-aware reasoning while capturing rich entity semantics. Our approach selectively filters context-relevant knowledge and explicitly models collaborative preferences using specialized supervision signals. A semantic alignment module harmonizes heterogeneous inputs, reducing noise and enhancing accuracy. Extensive experiments demonstrate that PCRS-TKA consistently outperforms all baselines in both recommendation and conversational quality.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 39,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12677",
        "abs_url": "https://arxiv.org/abs/2511.12677",
        "pdf_url": "https://arxiv.org/pdf/2511.12677",
        "title": "Dynamic Tree Databases in Automated Planning",
        "authors": [
            "Oliver Joergensen",
            "Dominik Drexler",
            "Jendrik Seipp"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "A central challenge in scaling up explicit state-space search for large tasks is compactly representing the set of generated states. Tree databases, a data structure from model checking, require constant space per generated state in the best case, but they need a large preallocation of memory. We propose a novel dynamic variant of tree databases for compressing state sets over propositional and numeric variables and prove that it maintains the desirable properties of the static counterpart. Our empirical evaluation of state compression techniques for grounded and lifted planning on classical and numeric planning tasks reveals compression ratios of several orders of magnitude, often with negligible runtime overhead.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 40,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12754",
        "abs_url": "https://arxiv.org/abs/2511.12754",
        "pdf_url": "https://arxiv.org/pdf/2511.12754",
        "title": "Adaptively Coordinating with Novel Partners via Learned Latent Strategies",
        "authors": [
            "Benjamin Li",
            "Shuyang Shi",
            "Lucia Romero",
            "Huao Li",
            "Yaqi Xie",
            "Woojun Kim",
            "Stefanos Nikolaidis",
            "Michael Lewis",
            "Katia Sycara",
            "Simon Stepputtis"
        ],
        "comments": "Accepted to NeurIPS 2025",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multiagent Systems (cs.MA)",
        "abstract": "Adaptation is the cornerstone of effective collaboration among heterogeneous team members. In human-agent teams, artificial agents need to adapt to their human partners in real time, as individuals often have unique preferences and policies that may change dynamically throughout interactions. This becomes particularly challenging in tasks with time pressure and complex strategic spaces, where identifying partner behaviors and selecting suitable responses is difficult. In this work, we introduce a strategy-conditioned cooperator framework that learns to represent, categorize, and adapt to a broad range of potential partner strategies in real-time. Our approach encodes strategies with a variational autoencoder to learn a latent strategy space from agent trajectory data, identifies distinct strategy types through clustering, and trains a cooperator agent conditioned on these clusters by generating partners of each strategy type. For online adaptation to novel partners, we leverage a fixed-share regret minimization algorithm that dynamically infers and adjusts the partner's strategy estimation during interaction. We evaluate our method in a modified version of the Overcooked domain, a complex collaborative cooking environment that requires effective coordination among two players with a diverse potential strategy space. Through these experiments and an online user study, we demonstrate that our proposed agent achieves state of the art performance compared to existing baselines when paired with novel human, and agent teammates.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 41,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12759",
        "abs_url": "https://arxiv.org/abs/2511.12759",
        "pdf_url": "https://arxiv.org/pdf/2511.12759",
        "title": "Optimal Foraging in Memory Retrieval: Evaluating Random Walks and Metropolis-Hastings Sampling in Modern Semantic Spaces",
        "authors": [
            "James Moore"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Human memory retrieval often resembles ecological foraging where animals search for food in a patchy environment. Optimal foraging means following the Marginal Value Theorem (MVT), in which individuals exploit a patch of semantically related concepts until it becomes less rewarding and then switch to a new cluster. While human behavioral data suggests foraging-like patterns in semantic fluency tasks, it remains unclear whether modern high-dimensional embedding spaces provide representations that allow algorithms to match observed human behavior. Using state-of-the-art embeddings and prior semantic fluency data, I find that random walks on these embedding spaces produce results consistent with optimal foraging and the MVT. Surprisingly, introducing Metropolis-Hastings sampling, an adaptive algorithm expected to model strategic acceptance and rejection of new clusters, does not produce results consistent with human behavior. These findings challenge the assumption that more complex sampling mechanisms inherently lead to better cognitive models of memory retrieval. Instead, they show that appropriately structured embeddings, even with simple sampling, can produce near-optimal foraging dynamics. This supports the perspective of Hills (2012) rather than Abbott (2015), demonstrating that modern embeddings can approximate human memory foraging without relying on complex acceptance criteria.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 42,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12769",
        "abs_url": "https://arxiv.org/abs/2511.12769",
        "pdf_url": "https://arxiv.org/pdf/2511.12769",
        "title": "Event-CausNet: Unlocking Causal Knowledge from Text with Large Language Models for Reliable Spatio-Temporal Forecasting",
        "authors": [
            "Luyao Niu",
            "Zepu Wang",
            "Shuyi Guan",
            "Yang Liu",
            "Peng Sun"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "While spatio-temporal Graph Neural Networks (GNNs) excel at modeling recurring traffic patterns, their reliability plummets during non-recurring events like accidents. This failure occurs because GNNs are fundamentally correlational models, learning historical patterns that are invalidated by the new causal factors introduced during disruptions. To address this, we propose Event-CausNet, a framework that uses a Large Language Model to quantify unstructured event reports, builds a causal knowledge base by estimating average treatment effects, and injects this knowledge into a dual-stream GNN-LSTM network using a novel causal attention mechanism to adjust and enhance the forecast. Experiments on a real-world dataset demonstrate that Event-CausNet achieves robust performance, reducing prediction error (MAE) by up to 35.87%, significantly outperforming state-of-the-art baselines. Our framework bridges the gap between correlational models and causal reasoning, providing a solution that is more accurate and transferable, while also offering crucial interpretability, providing a more reliable foundation for real-world traffic management during critical disruptions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 43,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12792",
        "abs_url": "https://arxiv.org/abs/2511.12792",
        "pdf_url": "https://arxiv.org/pdf/2511.12792",
        "title": "Multi-Agent Reinforcement Learning for Heterogeneous Satellite Cluster Resources Optimization",
        "authors": [
            "Mohamad A. Hady",
            "Siyi Hu",
            "Mahardhika Pratama",
            "Zehong Cao",
            "Ryszard Kowalczyk"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "This work investigates resource optimization in heterogeneous satellite clusters performing autonomous Earth Observation (EO) missions using Reinforcement Learning (RL). In the proposed setting, two optical satellites and one Synthetic Aperture Radar (SAR) satellite operate cooperatively in low Earth orbit to capture ground targets and manage their limited onboard resources efficiently. Traditional optimization methods struggle to handle the real-time, uncertain, and decentralized nature of EO operations, motivating the use of RL and Multi-Agent Reinforcement Learning (MARL) for adaptive decision-making. This study systematically formulates the optimization problem from single-satellite to multi-satellite scenarios, addressing key challenges including energy and memory constraints, partial observability, and agent heterogeneity arising from diverse payload capabilities. Using a near-realistic simulation environment built on the Basilisk and BSK-RL frameworks, we evaluate the performance and stability of state-of-the-art MARL algorithms such as MAPPO, HAPPO, and HATRPO. Results show that MARL enables effective coordination across heterogeneous satellites, balancing imaging performance and resource utilization while mitigating non-stationarity and inter-agent reward coupling. The findings provide practical insights into scalable, autonomous satellite operations and contribute a foundation for future research on intelligent EO mission planning under heterogeneous and dynamic conditions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 44,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12793",
        "abs_url": "https://arxiv.org/abs/2511.12793",
        "pdf_url": "https://arxiv.org/pdf/2511.12793",
        "title": "Neuro-Logic Lifelong Learning",
        "authors": [
            "Bowen He",
            "Xiaoan Xu",
            "Alper Kamil Bozkurt",
            "Vahid Tarokh",
            "Juncheng Dong"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Solving Inductive Logic Programming (ILP) problems with neural networks is a key challenge in Neural-Symbolic Ar- tificial Intelligence (AI). While most research has focused on designing novel network architectures for individual prob- lems, less effort has been devoted to exploring new learning paradigms involving a sequence of problems. In this work, we investigate lifelong learning ILP, which leverages the com- positional and transferable nature of logic rules for efficient learning of new problems. We introduce a compositional framework, demonstrating how logic rules acquired from ear- lier tasks can be efficiently reused in subsequent ones, leading to improved scalability and performance. We formalize our approach and empirically evaluate it on sequences of tasks. Experimental results validate the feasibility and advantages of this paradigm, opening new directions for continual learn- ing in Neural-Symbolic AI.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 45,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12844",
        "abs_url": "https://arxiv.org/abs/2511.12844",
        "pdf_url": "https://arxiv.org/pdf/2511.12844",
        "title": "Mapping fNIRS Signals to Agent Performance: Toward Reinforcement Learning from Neural Feedback",
        "authors": [
            "Julia Santaniello",
            "Matthew Russell",
            "Benson Jiang",
            "Donatello Sassaroli",
            "Robert Jacob",
            "Jivko SInapov"
        ],
        "comments": "Accepted to the Association for the Advancement of Artificial Intelligence (AAAI) 2026. To appear in the AAAI 2026 Proceedings",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Reinforcement Learning from Human Feedback (RLHF) is a methodology that aligns agent behavior with human preferences by integrating human feedback into the agent's training process. We introduce a possible framework that employs passive Brain-Computer Interfaces (BCI) to guide agent training from implicit neural signals. We present and release a novel dataset of functional near-infrared spectroscopy (fNIRS) recordings collected from 25 human participants across three domains: a Pick-and-Place Robot, Lunar Lander, and Flappy Bird. We train classifiers to predict levels of agent performance (optimal, sub-optimal, or worst-case) from windows of preprocessed fNIRS feature vectors, achieving an average F1 score of 67% for binary classification and 46% for multi-class models averaged across conditions and domains. We also train regressors to predict the degree of deviation between an agent's chosen action and a set of near-optimal policies, providing a continuous measure of performance. We evaluate cross-subject generalization and demonstrate that fine-tuning pre-trained models with a small sample of subject-specific data increases average F1 scores by 17% and 41% for binary and multi-class models, respectively. Our work demonstrates that mapping implicit fNIRS signals to agent performance is feasible and can be improved, laying the foundation for future brain-driven RLHF systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 46,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12867",
        "abs_url": "https://arxiv.org/abs/2511.12867",
        "pdf_url": "https://arxiv.org/pdf/2511.12867",
        "title": "Bootstrapping LLMs via Preference-Based Policy Optimization",
        "authors": [
            "Chen Jia"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Bootstrapping large language models (LLMs) through preference-based policy optimization offers a promising direction for aligning model behavior with human preferences without relying on extensive manual annotations. In this work, we propose a novel preference-based policy optimization (PbPO) framework that formulates the learning process as a min-max game between the main policy and a reward model (RM). The RM is constrained within a confidence set derived from preference data to ensure reliable exploitation. Our iterative online algorithm actively collects preference data through guided exploration of the evolving policy, enabling continual self-improvement of both the policy and the RM. We provide theoretical guarantees for our method, establishing high-probability regret bounds for both settings with sequence-level RM and token-level RM, demonstrating its effectiveness in bootstrapping LLMs. Extensive experiments on five benchmarks show that our approach consistently outperforms existing state-of-the-art preference optimization techniques.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 47,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12876",
        "abs_url": "https://arxiv.org/abs/2511.12876",
        "pdf_url": "https://arxiv.org/pdf/2511.12876",
        "title": "Think, Speak, Decide: Language-Augmented Multi-Agent Reinforcement Learning for Economic Decision-Making",
        "authors": [
            "Heyang Ma",
            "Qirui Mi",
            "Qipeng Yang",
            "Zijun Fan",
            "Bo Li",
            "Haifeng Zhang"
        ],
        "comments": "Extended version of a submission to AAAI 2026",
        "subjects": "Artificial Intelligence (cs.AI); General Economics (econ.GN)",
        "abstract": "Economic decision-making depends not only on structured signals such as prices and taxes, but also on unstructured language, including peer dialogue and media narratives. While multi-agent reinforcement learning (MARL) has shown promise in optimizing economic decisions, it struggles with the semantic ambiguity and contextual richness of language. We propose LAMP (Language-Augmented Multi-Agent Policy), a framework that integrates language into economic decision-making and narrows the gap to real-world settings. LAMP follows a Think-Speak-Decide pipeline: (1) Think interprets numerical observations to extract short-term shocks and long-term trends, caching high-value reasoning trajectories; (2) Speak crafts and exchanges strategic messages based on reasoning, updating beliefs by parsing peer communications; and (3) Decide fuses numerical data, reasoning, and reflections into a MARL policy to optimize language-augmented decision-making. Experiments in economic simulation show that LAMP outperforms both MARL and LLM-only baselines in cumulative return (+63.5%, +34.0%), robustness (+18.8%, +59.4%), and interpretability. These results demonstrate the potential of language-augmented policies to deliver more effective and robust economic strategies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 48,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12901",
        "abs_url": "https://arxiv.org/abs/2511.12901",
        "pdf_url": "https://arxiv.org/pdf/2511.12901",
        "title": "Online Learning of HTN Methods for integrated LLM-HTN Planning",
        "authors": [
            "Yuesheng Xu",
            "Hector Munoz-Avila"
        ],
        "comments": "The Twelfth Annual Conference on Advances in Cognitive Systems (ACS-2025)",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "We present online learning of Hierarchical Task Network (HTN) methods in the context of integrated HTN planning and LLM-based chatbots. Methods indicate when and how to decompose tasks into subtasks. Our method learner is built on top of the ChatHTN planner. ChatHTN queries ChatGPT to generate a decomposition of a task into primitive tasks when no applicable method for the task is available. In this work, we extend ChatHTN. Namely, when ChatGPT generates a task decomposition, ChatHTN learns from it, akin to memoization. However, unlike memoization, it learns a generalized method that applies not only to the specific instance encountered, but to other instances of the same task. We conduct experiments on two domains and demonstrate that our online learning procedure reduces the number of calls to ChatGPT while solving at least as many problems, and in some cases, even more.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 49,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12913",
        "abs_url": "https://arxiv.org/abs/2511.12913",
        "pdf_url": "https://arxiv.org/pdf/2511.12913",
        "title": "CoS: Towards Optimal Event Scheduling via Chain-of-Scheduling",
        "authors": [
            "Yiming Zhao",
            "Jiwei Tang",
            "Shimin Di",
            "Libin Zheng",
            "Jianxing Yu",
            "Jian Yin"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Recommending event schedules is a key issue in Event-based Social Networks (EBSNs) in order to maintain user activity. An effective recommendation is required to maximize the user's preference, subjecting to both time and geographical constraints. Existing methods face an inherent trade-off among efficiency, effectiveness, and generalization, due to the NP-hard nature of the problem. This paper proposes the Chain-of-Scheduling (CoS) framework, which activates the event scheduling capability of Large Language Models (LLMs) through a guided, efficient scheduling process. CoS enhances LLM by formulating the schedule task into three atomic stages, i.e., exploration, verification and integration. Then we enable the LLMs to generate CoS autonomously via Knowledge Distillation (KD). Experimental results show that CoS achieves near-theoretical optimal effectiveness with high efficiency on three real-world datasets in a interpretable manner. Moreover, it demonstrates strong zero-shot learning ability on out-of-domain data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 50,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12916",
        "abs_url": "https://arxiv.org/abs/2511.12916",
        "pdf_url": "https://arxiv.org/pdf/2511.12916",
        "title": "Fault2Flow: An AlphaEvolve-Optimized Human-in-the-Loop Multi-Agent System for Fault-to-Workflow Automation",
        "authors": [
            "Yafang Wang",
            "Yangjie Tian",
            "Xiaoyu Shen",
            "Gaoyang Zhang",
            "Jiaze Sun",
            "He Zhang",
            "Ruohua Xu",
            "Feng Zhao"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Power grid fault diagnosis is a critical process hindered by its reliance on manual, error-prone methods. Technicians must manually extract reasoning logic from dense regulations and attempt to combine it with tacit expert knowledge, which is inefficient, error-prone, and lacks maintainability as ragulations are updated and experience evolves. While Large Language Models (LLMs) have shown promise in parsing unstructured text, no existing framework integrates these two disparate knowledge sources into a single, verified, and executable workflow. To bridge this gap, we propose Fault2Flow, an LLM-based multi-agent system. Fault2Flow systematically: (1) extracts and structures regulatory logic into PASTA-formatted fault trees; (2) integrates expert knowledge via a human-in-the-loop interface for verification; (3) optimizes the reasoning logic using a novel AlphaEvolve module; and (4) synthesizes the final, verified logic into an n8n-executable workflow. Experimental validation on transformer fault diagnosis datasets confirms 100\\% topological consistency and high semantic fidelity. Fault2Flow establishes a reproducible path from fault analysis to operational automation, substantially reducing expert workload.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 51,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12937",
        "abs_url": "https://arxiv.org/abs/2511.12937",
        "pdf_url": "https://arxiv.org/pdf/2511.12937",
        "title": "Yanyun-3: Enabling Cross-Platform Strategy Game Operation with Vision-Language Models",
        "authors": [
            "Guoyan Wang",
            "Yanyan Huang",
            "Chunlin Chen",
            "Lifeng Wang",
            "Yuxiang Sun"
        ],
        "comments": "32 pages, 13 figures",
        "subjects": "Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Automated operation in cross-platform strategy games demands agents with robust generalization across diverse user interfaces and dynamic battlefield conditions. While vision-language models (VLMs) have shown considerable promise in multimodal reasoning, their application to complex human-computer interaction scenarios--such as strategy gaming--remains largely unexplored. Here, we introduce Yanyun-3, a general-purpose agent framework that, for the first time, enables autonomous cross-platform operation across three heterogeneous strategy game environments. By integrating the vision-language reasoning of Qwen2.5-VL with the precise execution capabilities of UI-TARS, Yanyun-3 successfully performs core tasks including target localization, combat resource allocation, and area control. Through systematic ablation studies, we evaluate the effects of various multimodal data combinations--static images, multi-image sequences, and videos--and propose the concept of combination granularity to differentiate between intra-sample fusion and inter-sample mixing strategies. We find that a hybrid strategy, which fuses multi-image and video data while mixing in static images (MV+S), substantially outperforms full fusion: it reduces inference time by 63% and boosts the BLEU-4 score by a factor of 12 (from 4.81% to 62.41%, approximately 12.98x). Operating via a closed-loop pipeline of screen capture, model inference, and action execution, the agent demonstrates strong real-time performance and cross-platform generalization. Beyond providing an efficient solution for strategy game automation, our work establishes a general paradigm for enhancing VLM performance through structured multimodal data organization, offering new insights into the interplay between static perception and dynamic reasoning in embodied intelligence.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 52,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12963",
        "abs_url": "https://arxiv.org/abs/2511.12963",
        "pdf_url": "https://arxiv.org/pdf/2511.12963",
        "title": "MedRule-KG: A Knowledge-Graph--Steered Scaffold for Reliable Mathematical and Biomedical Reasoning",
        "authors": [
            "Crystal Su"
        ],
        "comments": "AAAI 2026 Workshop AI2ASE",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "We study how to impose domain-consistent structure on large language models (LLMs) used for scientific reasoning and early-stage drug discovery. We present MedRule-KG, a compact knowledge-graph scaffold paired with a lightweight verifier that steers generation toward mathematically and biomedically valid outputs. The system injects curated symbolic facts into prompts and then enforces rule satisfaction with a deterministic checker. We formalize generation as constrained inference, introduce a soft guidance surrogate suitable for decoding, and perform a thorough statistical analysis with uncertainty quantification. Across 90 tasks spanning reaction feasibility, metabolic compatibility, and toxicity screening, MedRule-KG reduces violation counts by 83.2\\% relative to a strong chain-of-thought baseline while improving exact match. Results remain stable under stratification and scale with dataset size, and the verifier adds negligible latency, making the approach practical for interactive design.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 53,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12997",
        "abs_url": "https://arxiv.org/abs/2511.12997",
        "pdf_url": "https://arxiv.org/pdf/2511.12997",
        "title": "WebCoach: Self-Evolving Web Agents with Cross-Session Memory Guidance",
        "authors": [
            "Genglin Liu",
            "Shijie Geng",
            "Sha Li",
            "Hejie Cui",
            "Sarah Zhang",
            "Xin Liu",
            "Tianyi Liu"
        ],
        "comments": "18 pages; work in progress",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Multimodal LLM-powered agents have recently demonstrated impressive capabilities in web navigation, enabling agents to complete complex browsing tasks across diverse domains. However, current agents struggle with repetitive errors and lack the ability to learn from past experiences across sessions, limiting their long-term robustness and sample efficiency. We introduce WebCoach, a model-agnostic self-evolving framework that equips web browsing agents with persistent cross-session memory, enabling improved long-term planning, reflection, and continual learning without retraining. WebCoach consists of three key components: (1) a WebCondenser, which standardizes raw navigation logs into concise summaries; (2) an External Memory Store, which organizes complete trajectories as episodic experiences; and (3) a Coach, which retrieves relevant experiences based on similarity and recency, and decides whether to inject task-specific advice into the agent via runtime hooks. This design empowers web agents to access long-term memory beyond their native context window, improving robustness in complex browsing tasks. Moreover, WebCoach achieves self-evolution by continuously curating episodic memory from new navigation trajectories, enabling agents to improve over time without retraining. Evaluations on the WebVoyager benchmark demonstrate that WebCoach consistently improves the performance of browser-use agents across three different LLM backbones. With a 38B model, it increases task success rates from 47% to 61% while reducing or maintaining the average number of steps. Notably, smaller base models with WebCoach achieve performance comparable to the same web agent using GPT-4o.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 54,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13007",
        "abs_url": "https://arxiv.org/abs/2511.13007",
        "pdf_url": "https://arxiv.org/pdf/2511.13007",
        "title": "GEM: Generative Entropy-Guided Preference Modeling for Few-shot Alignment of LLMs",
        "authors": [
            "Yiyang Zhao",
            "Huiyu Bai",
            "Xuejiao Zhao"
        ],
        "comments": "This paper has been accepted by AAAI 2026-AIA and designated as an oral presentation paper",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Alignment of large language models (LLMs) with human preferences typically relies on supervised reward models or external judges that demand abundant annotations. However, in fields that rely on professional knowledge, such as medicine and law, such large-scale preference labels are often unachievable. In this paper, we propose a generative entropy-guided preference modeling approach named GEM for LLMs aligment at low-resource and domain-specific scenarios. Instead of training a discriminative reward model on preference data, we directly train the LLM to internalize a closed-loop optimization architecture that can extract and exploit the multi-dimensional, fine-grained cognitive signals implicit in human preferences. Specifically, our Cognitive Filtering module, based on entropy theory in decision making, first leverages Chain-of-Thought (CoT) prompting to generate diverse candidate reasoning chains (CoTs) from preference data. Subsequently, it introduces a token scoring mechanism to rank and weight the sampled CoTs, boosting the importance of high-confidence answers and strategically high-entropy tokens. Building on these filtered preferences, we fine-tune the LLM using a novel self-evaluated group advantage algorithm, SEGA, which effectively aggregates group-level cognitive signals and transforms the entropy-based scores into implicit rewards for policy optimization. In these ways, GEM empowers the LLM to rely on its own judgments and establishes an entropy-guided closed-loop cognitive optimization framework, enabling highly efficient few-shot alignment of LLMs. Experiments on general benchmarks and domain-specific tasks (such as mathematical reasoning and medical dialogues) demonstrate that our GEM achieves significant improvements with few-shot preference data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 55,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13021",
        "abs_url": "https://arxiv.org/abs/2511.13021",
        "pdf_url": "https://arxiv.org/pdf/2511.13021",
        "title": "PragWorld: A Benchmark Evaluating LLMs' Local World Model under Minimal Linguistic Alterations and Conversational Dynamics",
        "authors": [
            "Sachin Vashistha",
            "Aryan Bibhuti",
            "Atharva Naik",
            "Martin Tutek",
            "Somak Aditya"
        ],
        "comments": "23 pages, 15 tables, 10 figures; AAAI 2026 Conference Main Track (oral)",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Real-world conversations are rich with pragmatic elements, such as entity mentions, references, and implicatures. Understanding such nuances is a requirement for successful natural communication, and often requires building a local world model which encodes such elements and captures the dynamics of their evolving states. However, it is not well-understood whether language models (LMs) construct or maintain a robust implicit representation of conversations. In this work, we evaluate the ability of LMs to encode and update their internal world model in dyadic conversations and test their malleability under linguistic alterations. To facilitate this, we apply seven minimal linguistic alterations to conversations sourced from popular datasets and construct two benchmarks comprising yes-no questions. We evaluate a wide range of open and closed source LMs and observe that they struggle to maintain robust accuracy. Our analysis unveils that LMs struggle to memorize crucial details, such as tracking entities under linguistic alterations to conversations. We then propose a dual-perspective interpretability framework which identifies transformer layers that are useful or harmful and highlights linguistic alterations most influenced by harmful layers, typically due to encoding spurious signals or relying on shortcuts. Inspired by these insights, we propose two layer-regularization based fine-tuning strategies that suppress the effect of the harmful layers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 56,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13027",
        "abs_url": "https://arxiv.org/abs/2511.13027",
        "pdf_url": "https://arxiv.org/pdf/2511.13027",
        "title": "Scaling Generative Verifiers For Natural Language Mathematical Proof Verification And Selection",
        "authors": [
            "Sadegh Mahdavi",
            "Branislav Kisacanin",
            "Shubham Toshniwal",
            "Wei Du",
            "Ivan Moshkov",
            "George Armstrong",
            "Renjie Liao",
            "Christos Thrampoulidis",
            "Igor Gitman"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Large language models have achieved remarkable success on final-answer mathematical problems, largely due to the ease of applying reinforcement learning with verifiable rewards. However, the reasoning underlying these solutions is often flawed. Advancing to rigorous proof-based mathematics requires reliable proof verification capabilities. We begin by analyzing multiple evaluation setups and show that focusing on a single benchmark can lead to brittle or misleading conclusions. To address this, we evaluate both proof-based and final-answer reasoning to obtain a more reliable measure of model performance. We then scale two major generative verification methods (GenSelect and LLM-as-a-Judge) to millions of tokens and identify their combination as the most effective framework for solution verification and selection. We further show that the choice of prompt for LLM-as-a-Judge significantly affects the model's performance, but reinforcement learning can reduce this sensitivity. However, despite improving proof-level metrics, reinforcement learning does not enhance final-answer precision, indicating that current models often reward stylistic or procedural correctness rather than mathematical validity. Our results establish practical guidelines for designing and evaluating scalable proof-verification and selection systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 57,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13087",
        "abs_url": "https://arxiv.org/abs/2511.13087",
        "pdf_url": "https://arxiv.org/pdf/2511.13087",
        "title": "MEGA-GUI: Multi-stage Enhanced Grounding Agents for GUI Elements",
        "authors": [
            "SeokJoo Kwak",
            "Jihoon Kim",
            "Boyoun Kim",
            "Jung Jae Yoon",
            "Wooseok Jang",
            "Jeonghoon Hong",
            "Jaeho Yang",
            "Yeong-Dae Kwon"
        ],
        "comments": "26 pages, 7 figures. Code available at this https URL",
        "subjects": "Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Graphical User Interface (GUI) grounding - the task of mapping natural language instructions to screen coordinates - is essential for autonomous agents and accessibility technologies. Existing systems rely on monolithic models or one-shot pipelines that lack modularity and fail under visual clutter and ambiguous instructions. We introduce MEGA-GUI, a multi-stage framework that separates grounding into coarse Region-of-Interest (ROI) selection and fine-grained element grounding, orchestrated by specialized vision-language agents. MEGA-GUI features a bidirectional ROI zoom algorithm that mitigates spatial dilution and a context-aware rewriting agent that reduces semantic ambiguity. Our analysis reveals complementary strengths and weaknesses across vision-language models at different visual scales, and we show that leveraging this modular structure achieves consistently higher accuracy than monolithic approaches. On the visually dense ScreenSpot-Pro benchmark, MEGA-GUI attains 73.18% accuracy, and on the semantically complex OSWorld-G benchmark it reaches 68.63%, surpassing previously reported results. Code and the Grounding Benchmark Toolkit (GBT) are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 58,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13091",
        "abs_url": "https://arxiv.org/abs/2511.13091",
        "pdf_url": "https://arxiv.org/pdf/2511.13091",
        "title": "STEP: Success-Rate-Aware Trajectory-Efficient Policy Optimization",
        "authors": [
            "Yuhan Chen",
            "Yuxuan Liu",
            "Long Zhang",
            "Pengzhi Gao",
            "Jian Luan",
            "Wei Liu"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Multi-turn interaction remains challenging for online reinforcement learning. A common solution is trajectory-level optimization, which treats each trajectory as a single training sample. However, this approach can be inefficient and yield misleading learning signals: it applies uniform sampling across tasks regardless of difficulty, penalizes correct intermediate actions in failed trajectories, and incurs high sample-collection costs. To address these issues, we propose STEP (Success-rate-aware Trajectory-Efficient Policy optimization), a framework that dynamically allocates sampling based on per-task success rates and performs step-level optimization. STEP maintains a smoothed success-rate record to guide adaptive trajectory resampling, allocating more effort to harder tasks. It then computes success-rate-weighted advantages and decomposes trajectories into step-level samples. Finally, it applies a step-level GRPO augmentation to refine updates for low-success tasks. Experiments on OSWorld and AndroidWorld show that STEP substantially improves sample efficiency and training stability over trajectory-level GRPO, converging faster and generalizing better under the same sampling budget.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 59,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13131",
        "abs_url": "https://arxiv.org/abs/2511.13131",
        "pdf_url": "https://arxiv.org/pdf/2511.13131",
        "title": "MM-Telco: Benchmarks and Multimodal Large Language Models for Telecom Applications",
        "authors": [
            "Gagan Raj Gupta",
            "Anshul Kumar",
            "Manish Rai",
            "Apu Chakraborty",
            "Ashutosh Modi",
            "Abdelaali Chaoub",
            "Soumajit Pramanik",
            "Moyank Giri",
            "Yashwanth Holla",
            "Sunny Kumar",
            "M. V. Kiran Sooraj"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Emerging Technologies (cs.ET); Networking and Internet Architecture (cs.NI)",
        "abstract": "Large Language Models (LLMs) have emerged as powerful tools for automating complex reasoning and decision-making tasks. In telecommunications, they hold the potential to transform network optimization, automate troubleshooting, enhance customer support, and ensure regulatory compliance. However, their deployment in telecom is hindered by domain-specific challenges that demand specialized adaptation. To overcome these challenges and to accelerate the adaptation of LLMs for telecom, we propose MM-Telco, a comprehensive suite of multimodal benchmarks and models tailored for the telecom domain. The benchmark introduces various tasks (both text based and image based) that address various practical real-life use cases such as network operations, network management, improving documentation quality, and retrieval of relevant text and images. Further, we perform baseline experiments with various LLMs and VLMs. The models fine-tuned on our dataset exhibit a significant boost in performance. Our experiments also help analyze the weak areas in the working of current state-of-art multimodal LLMs, thus guiding towards further development and research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 60,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13137",
        "abs_url": "https://arxiv.org/abs/2511.13137",
        "pdf_url": "https://arxiv.org/pdf/2511.13137",
        "title": "Conditional Diffusion Model for Multi-Agent Dynamic Task Decomposition",
        "authors": [
            "Yanda Zhu",
            "Yuanyang Zhu",
            "Daoyi Dong",
            "Caihua Chen",
            "Chunlin Chen"
        ],
        "comments": "AAAI 2026",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Task decomposition has shown promise in complex cooperative multi-agent reinforcement learning (MARL) tasks, which enables efficient hierarchical learning for long-horizon tasks in dynamic and uncertain environments. However, learning dynamic task decomposition from scratch generally requires a large number of training samples, especially exploring the large joint action space under partial observability. In this paper, we present the Conditional Diffusion Model for Dynamic Task Decomposition (C$\\text{D}^\\text{3}$T), a novel two-level hierarchical MARL framework designed to automatically infer subtask and coordination patterns. The high-level policy learns subtask representation to generate a subtask selection strategy based on subtask effects. To capture the effects of subtasks on the environment, C$\\text{D}^\\text{3}$T predicts the next observation and reward using a conditional diffusion model. At the low level, agents collaboratively learn and share specialized skills within their assigned subtasks. Moreover, the learned subtask representation is also used as additional semantic information in a multi-head attention mixing network to enhance value decomposition and provide an efficient reasoning bridge between individual and joint value functions. Experimental results on various benchmarks demonstrate that C$\\text{D}^\\text{3}$T achieves better performance than existing baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 61,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13160",
        "abs_url": "https://arxiv.org/abs/2511.13160",
        "pdf_url": "https://arxiv.org/pdf/2511.13160",
        "title": "InteractiveGNNExplainer: A Visual Analytics Framework for Multi-Faceted Understanding and Probing of Graph Neural Network Predictions",
        "authors": [
            "TC Singh",
            "Sougata Mukherjea"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Graph Neural Networks (GNNs) excel in graph-based learning tasks, but their complex, non-linear operations often render them as opaque \"black boxes\". This opacity hinders user trust, complicates debugging, bias detection, and adoption in critical domains requiring explainability. This paper introduces InteractiveGNNExplainer, a visual analytics framework to enhance GNN explainability, focusing on node classification. Our system uniquely integrates coordinated interactive views (dynamic graph layouts, embedding projections, feature inspection, neighborhood analysis) with established post-hoc (GNNExplainer) and intrinsic (GAT attention) explanation techniques. Crucially, it incorporates interactive graph editing, allowing users to perform a \"what-if\" analysis by perturbing graph structures and observing immediate impacts on GNN predictions and explanations. We detail the system architecture and, through case studies on Cora and CiteSeer datasets, demonstrate how InteractiveGNNExplainer facilitates in-depth misclassification diagnosis, comparative analysis of GCN versus GAT behaviors, and rigorous probing of model sensitivity. These capabilities foster a deeper, multifaceted understanding of GNN predictions, contributing to more transparent, trustworthy, and robust graph analysis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 62,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13193",
        "abs_url": "https://arxiv.org/abs/2511.13193",
        "pdf_url": "https://arxiv.org/pdf/2511.13193",
        "title": "Cost-Effective Communication: An Auction-based Method for Language Agent Interaction",
        "authors": [
            "Yijia Fan",
            "Jusheng Zhang",
            "Kaitong Cai",
            "Jing Yang",
            "Chengpei Tang",
            "Jian Wang",
            "Keze Wang"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Multi-agent systems (MAS) built on large language models (LLMs) often suffer from inefficient \"free-for-all\" communication, leading to exponential token costs and low signal-to-noise ratios that hinder their practical deployment. We challenge the notion that more communication is always beneficial, hypothesizing instead that the core issue is the absence of resource rationality. We argue that \"free\" communication, by ignoring the principle of scarcity, inherently breeds inefficiency and unnecessary expenses. To address this, we introduce the Dynamic Auction-based Language Agent (DALA), a novel framework that treats communication bandwidth as a scarce and tradable resource. Specifically, our DALA regards inter-agent communication as a centralized auction, where agents learn to bid for the opportunity to speak based on the predicted value density of their messages. Thus, our DALA intrinsically encourages agents to produce concise, informative messages while filtering out low-value communication. Extensive and comprehensive experiments demonstrate that our economically-driven DALA achieves new state-of-the-art performance across seven challenging reasoning benchmarks, including 84.32% on MMLU and a 91.21% pass@1 rate on HumanEval. Note that this is accomplished with remarkable efficiency, i.e., our DALA uses only 6.25 million tokens, a fraction of the resources consumed by current state-of-the-art methods on GSM8K. Further analysis reveals that our DALA cultivates the emergent skill of strategic silence, effectively adapting its communication strategies from verbosity to silence in a dynamical manner via resource constraints.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 63,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13214",
        "abs_url": "https://arxiv.org/abs/2511.13214",
        "pdf_url": "https://arxiv.org/pdf/2511.13214",
        "title": "Learning to Solve Resource-Constrained Project Scheduling Problems with Duration Uncertainty using Graph Neural Networks",
        "authors": [
            "Guillaume Infantes",
            "Stéphanie Roussel",
            "Antoine Jacquet",
            "Emmanuel Benazera"
        ],
        "comments": "Accepted at ICTAI 2025 Conference",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "The Resource-Constrained Project Scheduling Problem (RCPSP) is a classical scheduling problem that has received significant attention due to of its numerous applications in industry. However, in practice, task durations are subject to uncertainty that must be considered in order to propose resilient scheduling. In this paper, we address the RCPSP variant with uncertain tasks duration (modeled using known probabilities) and aim to minimize the overall expected project duration. Our objective is to produce a baseline schedule that can be reused multiple times in an industrial setting regardless of the actual duration scenario. We leverage Graph Neural Networks in conjunction with Deep Reinforcement Learning (DRL) to develop an effective policy for task scheduling. This policy operates similarly to a priority dispatch rule and is paired with a Serial Schedule Generation Scheme to produce a schedule. Our empirical evaluation on standard benchmarks demonstrates the approach's superiority in terms of performance and its ability to generalize. The developed framework, Wheatley, is made publicly available online to facilitate further research and reproducibility.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 64,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13226",
        "abs_url": "https://arxiv.org/abs/2511.13226",
        "pdf_url": "https://arxiv.org/pdf/2511.13226",
        "title": "Informative Communication of Robot Plans",
        "authors": [
            "Michele Persiani",
            "Thomas Hellstrom"
        ],
        "comments": "Conference: PAAMS 2022, 20th International Conference on Practical Applications of Agents and Multi-Agent Systems",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "When a robot is asked to verbalize its plan it can do it in many ways. For example, a seemingly natural strategy is incremental, where the robot verbalizes its planned actions in plan order. However, an important aspect of this type of strategy is that it misses considerations on what is effectively informative to communicate, because not considering what the user knows prior to explanations. In this paper we propose a verbalization strategy to communicate robot plans informatively, by measuring the information gain that verbalizations have against a second-order theory of mind of the user capturing his prior knowledge on the robot. As shown in our experiments, this strategy allows to understand the robot's goal much quicker than by using strategies such as increasing or decreasing plan order. In addition, following our formulation we hint to what is informative and why when a robot communicates its plan.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 65,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13288",
        "abs_url": "https://arxiv.org/abs/2511.13288",
        "pdf_url": "https://arxiv.org/pdf/2511.13288",
        "title": "Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO",
        "authors": [
            "Haoyang Hong",
            "Jiajun Yin",
            "Yuan Wang",
            "Jingnan Liu",
            "Zhe Chen",
            "Ailing Yu",
            "Ji Li",
            "Zhiling Ye",
            "Hansong Xiao",
            "Yefei Chen",
            "Hualei Zhou",
            "Yun Yue",
            "Minghui Yang",
            "Chunxiao Guo",
            "Junwei Liu",
            "Peng Wei",
            "Jinjie Gu"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Multi-agent systems perform well on general reasoning tasks. However, the lack of training in specialized areas hinders their accuracy. Current training methods train a unified large language model (LLM) for all agents in the system. This may limit the performances due to different distributions underlying for different agents. Therefore, training multi-agent systems with distinct LLMs should be the next step to solve. However, this approach introduces optimization challenges. For example, agents operate at different frequencies, rollouts involve varying sub-agent invocations, and agents are often deployed across separate servers, disrupting end-to-end gradient flow. To address these issues, we propose M-GRPO, a hierarchical extension of Group Relative Policy Optimization designed for vertical Multi-agent systems with a main agent (planner) and multiple sub-agents (multi-turn tool executors). M-GRPO computes group-relative advantages for both main and sub-agents, maintaining hierarchical credit assignment. It also introduces a trajectory-alignment scheme that generates fixed-size batches despite variable sub-agent invocations. We deploy a decoupled training pipeline in which agents run on separate servers and exchange minimal statistics via a shared store. This enables scalable training without cross-server backpropagation. In experiments on real-world benchmarks (e.g., GAIA, XBench-DeepSearch, and WebWalkerQA), M-GRPO consistently outperforms both single-agent GRPO and multi-agent GRPO with frozen sub-agents, demonstrating improved stability and sample efficiency. These results show that aligning heterogeneous trajectories and decoupling optimization across specialized agents enhances tool-augmented reasoning tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 66,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13290",
        "abs_url": "https://arxiv.org/abs/2511.13290",
        "pdf_url": "https://arxiv.org/pdf/2511.13290",
        "title": "Dropouts in Confidence: Moral Uncertainty in Human-LLM Alignment",
        "authors": [
            "Jea Kwon",
            "Luiz Felipe Vecchietti",
            "Sungwon Park",
            "Meeyoung Cha"
        ],
        "comments": "Accepted to AAAI 2026",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computers and Society (cs.CY)",
        "abstract": "Humans display significant uncertainty when confronted with moral dilemmas, yet the extent of such uncertainty in machines and AI agents remains underexplored. Recent studies have confirmed the overly confident tendencies of machine-generated responses, particularly in large language models (LLMs). As these systems are increasingly embedded in ethical decision-making scenarios, it is important to understand their moral reasoning and the inherent uncertainties in building reliable AI systems. This work examines how uncertainty influences moral decisions in the classical trolley problem, analyzing responses from 32 open-source models and 9 distinct moral dimensions. We first find that variance in model confidence is greater across models than within moral dimensions, suggesting that moral uncertainty is predominantly shaped by model architecture and training method. To quantify uncertainty, we measure binary entropy as a linear combination of total entropy, conditional entropy, and mutual information. To examine its effects, we introduce stochasticity into models via \"dropout\" at inference time. Our findings show that our mechanism increases total entropy, mainly through a rise in mutual information, while conditional entropy remains largely unchanged. Moreover, this mechanism significantly improves human-LLM moral alignment, with correlations in mutual information and alignment score shifts. Our results highlight the potential to better align model-generated decisions and human preferences by deliberately modulating uncertainty and reducing LLMs' confidence in morally complex scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 67,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13293",
        "abs_url": "https://arxiv.org/abs/2511.13293",
        "pdf_url": "https://arxiv.org/pdf/2511.13293",
        "title": "Grounded by Experience: Generative Healthcare Prediction Augmented with Hierarchical Agentic Retrieval",
        "authors": [
            "Chuang Zhao",
            "Hui Tang",
            "Hongke Zhao",
            "Xiaofang Zhou",
            "Xiaomeng Li"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Accurate healthcare prediction is critical for improving patient outcomes and reducing operational costs. Bolstered by growing reasoning capabilities, large language models (LLMs) offer a promising path to enhance healthcare predictions by drawing on their rich parametric knowledge. However, LLMs are prone to factual inaccuracies due to limitations in the reliability and coverage of their embedded knowledge. While retrieval-augmented generation (RAG) frameworks, such as GraphRAG and its variants, have been proposed to mitigate these issues by incorporating external knowledge, they face two key challenges in the healthcare scenario: (1) identifying the clinical necessity to activate the retrieval mechanism, and (2) achieving synergy between the retriever and the generator to craft contextually appropriate retrievals. To address these challenges, we propose GHAR, a \\underline{g}enerative \\underline{h}ierarchical \\underline{a}gentic \\underline{R}AG framework that simultaneously resolves when to retrieve and how to optimize the collaboration between submodules in healthcare. Specifically, for the first challenge, we design a dual-agent architecture comprising Agent-Top and Agent-Low. Agent-Top acts as the primary physician, iteratively deciding whether to rely on parametric knowledge or to initiate retrieval, while Agent-Low acts as the consulting service, summarising all task-relevant knowledge once retrieval was triggered. To tackle the second challenge, we innovatively unify the optimization of both agents within a formal Markov Decision Process, designing diverse rewards to align their shared goal of accurate prediction while preserving their distinct roles. Extensive experiments on three benchmark datasets across three popular tasks demonstrate our superiority over state-of-the-art baselines, highlighting the potential of hierarchical agentic RAG in advancing healthcare systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 68,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13306",
        "abs_url": "https://arxiv.org/abs/2511.13306",
        "pdf_url": "https://arxiv.org/pdf/2511.13306",
        "title": "DAP: A Discrete-token Autoregressive Planner for Autonomous Driving",
        "authors": [
            "Bowen Ye",
            "Bin Zhang",
            "Hang Zhao"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Gaining sustainable performance improvement with scaling data and model budget remains a pivotal yet unresolved challenge in autonomous driving. While autoregressive models exhibited promising data-scaling efficiency in planning tasks, predicting ego trajectories alone suffers sparse supervision and weakly constrains how scene evolution should shape ego motion. Therefore, we introduce DAP, a discrete-token autoregressive planner that jointly forecasts BEV semantics and ego trajectories, thereby enforcing comprehensive representation learning and allowing predicted dynamics to directly condition ego motion. In addition, we incorporate a reinforcement-learning-based fine-tuning, which preserves supervised behavior cloning priors while injecting reward-guided improvements. Despite a compact 160M parameter budget, DAP achieves state-of-the-art performance on open-loop metrics and delivers competitive closed-loop results on the NAVSIM benchmark. Overall, the fully discrete-token autoregressive formulation operating on both rasterized BEV and ego actions provides a compact yet scalable planning paradigm for autonomous driving.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 69,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13359",
        "abs_url": "https://arxiv.org/abs/2511.13359",
        "pdf_url": "https://arxiv.org/pdf/2511.13359",
        "title": "Reasoning Shapes Alignment: Investigating Cultural Alignment in Large Reasoning Models with Cultural Norms",
        "authors": [
            "Yuhang Wang",
            "Yanxu Zhu",
            "Jitao Sang"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "The advanced reasoning capabilities of Large Reasoning Models enable them to thoroughly understand and apply safety policies through deliberate thought processes, thereby improving the models' safety. Beyond safety, these models must also be able to reflect the diverse range of human values across various cultures. This paper presents the Cultural Norm-based Cultural Alignment (CNCA) framework, which enables models to leverage their powerful reasoning ability to align with cultural norms. Specifically, we propose three methods to automatically mine cultural norms from limited survey data and explore ways to effectively utilize these norms for improving cultural alignment. Two alignment paradigms are examined: an in-context alignment method, where cultural norms are explicitly integrated into the user context, and a fine-tuning-based method, which internalizes norms through enhanced Chain-of-Thought training data. Comprehensive experiments demonstrate the effectiveness of these methods, highlighting that models with stronger reasoning capabilities benefit more from cultural norm mining and utilization. Our findings emphasize the potential for reasoning models to better reflect diverse human values through culturally informed alignment strategies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 70,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13361",
        "abs_url": "https://arxiv.org/abs/2511.13361",
        "pdf_url": "https://arxiv.org/pdf/2511.13361",
        "title": "MedDCR: Learning to Design Agentic Workflows for Medical Coding",
        "authors": [
            "Jiyang Zheng",
            "Islam Nassar",
            "Thanh Vu",
            "Xu Zhong",
            "Yang Lin",
            "Tongliang Liu",
            "Long Duong",
            "Yuan-Fang Li"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)",
        "abstract": "Medical coding converts free-text clinical notes into standardized diagnostic and procedural codes, which are essential for billing, hospital operations, and medical research. Unlike ordinary text classification, it requires multi-step reasoning: extracting diagnostic concepts, applying guideline constraints, mapping to hierarchical codebooks, and ensuring cross-document consistency. Recent advances leverage agentic LLMs, but most rely on rigid, manually crafted workflows that fail to capture the nuance and variability of real-world documentation, leaving open the question of how to systematically learn effective workflows. We present MedDCR, a closed-loop framework that treats workflow design as a learning problem. A Designer proposes workflows, a Coder executes them, and a Reflector evaluates predictions and provides constructive feedback, while a memory archive preserves prior designs for reuse and iterative refinement. On benchmark datasets, MedDCR outperforms state-of-the-art baselines and produces interpretable, adaptable workflows that better reflect real coding practice, improving both the reliability and trustworthiness of automated systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 71,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13371",
        "abs_url": "https://arxiv.org/abs/2511.13371",
        "pdf_url": "https://arxiv.org/pdf/2511.13371",
        "title": "Cognitive Maps in Language Models: A Mechanistic Analysis of Spatial Planning",
        "authors": [
            "Caroline Baumgartner",
            "Eleanor Spens",
            "Neil Burgess",
            "Petru Manescu"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "How do large language models solve spatial navigation tasks? We investigate this by training GPT-2 models on three spatial learning paradigms in grid environments: passive exploration (Foraging Model- predicting steps in random walks), goal-directed planning (generating optimal shortest paths) on structured Hamiltonian paths (SP-Hamiltonian), and a hybrid model fine-tuned with exploratory data (SP-Random Walk). Using behavioural, representational and mechanistic analyses, we uncover two fundamentally different learned algorithms. The Foraging model develops a robust, map-like representation of space, akin to a 'cognitive map'. Causal interventions reveal that it learns to consolidate spatial information into a self-sufficient coordinate system, evidenced by a sharp phase transition where its reliance on historical direction tokens vanishes by the middle layers of the network. The model also adopts an adaptive, hierarchical reasoning system, switching between a low-level heuristic for short contexts and map-based inference for longer ones. In contrast, the goal-directed models learn a path-dependent algorithm, remaining reliant on explicit directional inputs throughout all layers. The hybrid model, despite demonstrating improved generalisation over its parent, retains the same path-dependent strategy. These findings suggest that the nature of spatial intelligence in transformers may lie on a spectrum, ranging from generalisable world models shaped by exploratory data to heuristics optimised for goal-directed tasks. We provide a mechanistic account of this generalisation-optimisation trade-off and highlight how the choice of training regime influences the strategies that emerge.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 72,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13411",
        "abs_url": "https://arxiv.org/abs/2511.13411",
        "pdf_url": "https://arxiv.org/pdf/2511.13411",
        "title": "An Operational Kardashev-Style Scale for Autonomous AI - Towards AGI and Superintelligence",
        "authors": [
            "Przemyslaw Chojecki"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "We propose a Kardashev-inspired yet operational Autonomous AI (AAI) Scale that measures the progression from fixed robotic process automation (AAI-0) to full artificial general intelligence (AAI-4) and beyond. Unlike narrative ladders, our scale is multi-axis and testable. We define ten capability axes (Autonomy, Generality, Planning, Memory/Persistence, Tool Economy, Self-Revision, Sociality/Coordination, Embodiment, World-Model Fidelity, Economic Throughput) aggregated by a composite AAI-Index (a weighted geometric mean). We introduce a measurable Self-Improvement Coefficient $\\kappa$ (capability growth per unit of agent-initiated resources) and two closure properties (maintenance and expansion) that convert ``self-improving AI'' into falsifiable criteria. We specify OWA-Bench, an open-world agency benchmark suite that evaluates long-horizon, tool-using, persistent agents. We define level gates for AAI-0\\ldots AAI-4 using thresholds on the axes, $\\kappa$, and closure proofs. Synthetic experiments illustrate how present-day systems map onto the scale and how the delegability frontier (quality vs.\\ autonomy) advances with self-improvement. We also prove a theorem that AAI-3 agent becomes AAI-5 over time with sufficient conditions, formalizing \"baby AGI\" becomes Superintelligence intuition.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 73,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13476",
        "abs_url": "https://arxiv.org/abs/2511.13476",
        "pdf_url": "https://arxiv.org/pdf/2511.13476",
        "title": "Multi-Agent Multimodal Large Language Model Framework for Automated Interpretation of Fuel Efficiency Analytics in Public Transportation",
        "authors": [
            "Zhipeng Ma",
            "Ali Rida Bahja",
            "Andreas Burgdorf",
            "André Pomp",
            "Tobias Meisen",
            "Bo Nørregaard Jørgensen",
            "Zheng Grace Ma"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Enhancing fuel efficiency in public transportation requires the integration of complex multimodal data into interpretable, decision-relevant insights. However, traditional analytics and visualization methods often yield fragmented outputs that demand extensive human interpretation, limiting scalability and consistency. This study presents a multi-agent framework that leverages multimodal large language models (LLMs) to automate data narration and energy insight generation. The framework coordinates three specialized agents, including a data narration agent, an LLM-as-a-judge agent, and an optional human-in-the-loop evaluator, to iteratively transform analytical artifacts into coherent, stakeholder-oriented reports. The system is validated through a real-world case study on public bus transportation in Northern Jutland, Denmark, where fuel efficiency data from 4006 trips are analyzed using Gaussian Mixture Model clustering. Comparative experiments across five state-of-the-art LLMs and three prompting paradigms identify GPT-4.1 mini with Chain-of-Thought prompting as the optimal configuration, achieving 97.3% narrative accuracy while balancing interpretability and computational cost. The findings demonstrate that multi-agent orchestration significantly enhances factual precision, coherence, and scalability in LLM-based reporting. The proposed framework establishes a replicable and domain-adaptive methodology for AI-driven narrative generation and decision support in energy informatics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 74,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2406.05348",
        "abs_url": "https://arxiv.org/abs/2406.05348",
        "pdf_url": "https://arxiv.org/pdf/2406.05348",
        "title": "Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets",
        "authors": [
            "Satanu Ghosh",
            "Neal R. Brodnik",
            "Carolina Frey",
            "Collin Holgate",
            "Tresa M. Pollock",
            "Samantha Daly",
            "Samuel Carton"
        ],
        "comments": "LLM for information extraction. Update on 12/11/2024: We added some relevant literature that we missed in the previous version of the paper. Update on 05/25/2025: We changed the metadata",
        "subjects": "Computation and Language (cs.CL); Materials Science (cond-mat.mtrl-sci); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)",
        "abstract": "We explore the ability of GPT-4 to perform ad-hoc schema based information extraction from scientific literature. We assess specifically whether it can, with a basic prompting approach, replicate two existing material science datasets, given the manuscripts from which they were originally manually extracted. We employ materials scientists to perform a detailed manual error analysis to assess where the model struggles to faithfully extract the desired information, and draw on their insights to suggest research directions to address this broadly important task.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 75,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2505.11225",
        "abs_url": "https://arxiv.org/abs/2505.11225",
        "pdf_url": "https://arxiv.org/pdf/2505.11225",
        "title": "HAPO: Training Language Models to Reason Concisely via History-Aware Policy Optimization",
        "authors": [
            "Chengyu Huang",
            "Zhengxin Zhang",
            "Claire Cardie"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "While scaling the length of responses at test-time has been shown to markedly improve the reasoning abilities and performance of large language models (LLMs), it often results in verbose outputs and increases inference cost. Prior approaches for efficient test-time scaling, typically using universal budget constraints or query-level length optimization, do not leverage historical information from previous encounters with the same problem during training. We hypothesize that this limits their ability to progressively make solutions more concise over time. To address this, we present History-Aware Policy Optimization (HAPO), which keeps track of a history state (e.g., the minimum length over previously generated correct responses) for each problem. HAPO employs a novel length reward function based on this history state to incentivize the discovery of correct solutions that are more concise than those previously found. Crucially, this reward structure avoids overly penalizing shorter incorrect responses with the goal of facilitating exploration towards more efficient solutions. By combining this length reward with a correctness reward, HAPO jointly optimizes for correctness and efficiency. We use HAPO to train DeepSeek-R1-Distill-Qwen-1.5B, DeepScaleR-1.5B-Preview, and Qwen-2.5-1.5B-Instruct, and evaluate HAPO on several math benchmarks that span various difficulty levels. Experiment results demonstrate that HAPO effectively induces LLMs' concise reasoning abilities, producing length reductions of 33-59% with accuracy drops of only 2-5%.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 76,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2506.14157",
        "abs_url": "https://arxiv.org/abs/2506.14157",
        "pdf_url": "https://arxiv.org/pdf/2506.14157",
        "title": "DCRM: A Heuristic to Measure Response Pair Quality in Preference Optimization",
        "authors": [
            "Chengyu Huang",
            "Tanya Goyal"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Recent research has attempted to associate preference optimization (PO) performance with the underlying preference datasets. In this work, our observation is that the differences between the preferred response $y^+$ and dispreferred response $y^-$ influence what LLMs can learn, which may not match the desirable differences to learn. Therefore, we use distance and reward margin to quantify these differences, and combine them to get Distance Calibrated Reward Margin (DCRM), a metric that measures the quality of a response pair for PO. Intuitively, DCRM encourages minimal noisy differences and maximal desired differences. With this, we study 3 types of commonly used preference datasets, classified along two axes: the source of the responses and the preference labeling function. We establish a general correlation between higher DCRM of the training set and better learning outcome. Inspired by this, we propose a best-of-$N^2$ pairing method that selects response pairs with the highest DCRM. Empirically, in various settings, our method produces training datasets that can further improve models' performance on AlpacaEval, MT-Bench, and Arena-Hard over the existing training sets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 77,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11576",
        "abs_url": "https://arxiv.org/abs/2511.11576",
        "pdf_url": "https://arxiv.org/pdf/2511.11576",
        "title": "DAOpt: Modeling and Evaluation of Data-Driven Optimization under Uncertainty with LLMs",
        "authors": [
            "WenZhuo Zhu",
            "Zheng Cui",
            "Wenhan Lu",
            "Sheng Liu",
            "Yue Zhao"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Recent advances in large language models (LLMs) have accelerated research on automated optimization modeling. While real-world decision-making is inherently uncertain, most existing work has focused on deterministic optimization with known parameters, leaving the application of LLMs in uncertain settings largely unexplored. To that end, we propose the DAOpt framework including a new dataset OptU, a multi-agent decision-making module, and a simulation environment for evaluating LLMs with a focus on out-of-sample feasibility and robustness. Additionally, we enhance LLMs' modeling capabilities by incorporating few-shot learning with domain knowledge from stochastic and robust optimization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 78,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11579",
        "abs_url": "https://arxiv.org/abs/2511.11579",
        "pdf_url": "https://arxiv.org/pdf/2511.11579",
        "title": "Decoupling Positional and Symbolic Attention Behavior in Transformers",
        "authors": [
            "Felipe Urrutia",
            "Jorge Salas",
            "Alexander Kozachinskiy",
            "Cristian Buc Calderon",
            "Hector Pasten",
            "Cristobal Rojas"
        ],
        "comments": "32 pages, 12 figures, repository available",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "An important aspect subtending language understanding and production is the ability to independently encode positional and symbolic information of the words within a sentence. In Transformers, positional information is typically encoded using Positional Encodings (PEs). One such popular PE, namely Rotary PE (RoPE), has been widely used due to its empirical success. Recently, it has been argued that part of RoPE's success emerges from its ability to encode robust positional and semantic information using large and small frequencies, respectively. In this work, we perform a deeper dive into the positional versus symbolic dichotomy of attention heads behavior, both at the theoretical and empirical level. We provide general definitions of what it means for a head to behave positionally or symbolically, prove that these are two mutually exclusive behaviors and develop a metric to quantify them. We apply our framework to analyze Transformer-based LLMs using RoPE and find that all heads exhibit a strong correspondence between behavior and frequency use. Finally, we introduce canonical tasks designed to be either purely positional or symbolic, and demonstrate that the Transformer performance causally relates to the ability of attention heads to leverage the appropriate frequencies. In particular, we show that we can control the Transformer performance by controlling which frequencies the attention heads can access. Altogether, our work provides a detailed understanding of RoPE, and how its properties relate to model behavior.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 79,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11581",
        "abs_url": "https://arxiv.org/abs/2511.11581",
        "pdf_url": "https://arxiv.org/pdf/2511.11581",
        "title": "The Anatomy of a Triton Attention Kernel",
        "authors": [
            "Burkhard Ringlein",
            "Jan van Lunteren",
            "Radu Stoica",
            "Thomas Parnell"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Distributed, Parallel, and Cluster Computing (cs.DC); Programming Languages (cs.PL)",
        "abstract": "A long-standing goal in both industry and academia is to develop an LLM inference platform that is portable across hardware architectures, eliminates the need for low-level hand-tuning, and still delivers best-in-class efficiency. In this work, we demonstrate that portable, efficient cross-platform LLM inference is indeed possible and share our experience. We develop a state-of-the-art paged attention kernel, the core performance-critical component of many LLM deployments, that builds exclusively on the domain-specific just-in-time compiled language Triton to achieve state-of-the-art performance on both NVIDIA and AMD GPUs. We describe our high-level approach, the key algorithmic and system-level improvements, the parameter auto-tuning required to unlock efficiency, and the integrations into a popular inference server that are necessary to bring the performance of a generic Triton attention kernel from 19.7% of the state-of-the-art to 105.9%. Our results highlight how open-source domain-specific languages can be leveraged to unlock model portability across different GPU vendors.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 80,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11583",
        "abs_url": "https://arxiv.org/abs/2511.11583",
        "pdf_url": "https://arxiv.org/pdf/2511.11583",
        "title": "Parallel and Multi-Stage Knowledge Graph Retrieval for Behaviorally Aligned Financial Asset Recommendations",
        "authors": [
            "Fernando Spadea",
            "Oshani Seneviratne"
        ],
        "comments": "10 pages, 3 figures, RAGE-KG 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)",
        "abstract": "Large language models (LLMs) show promise for personalized financial recommendations but are hampered by context limits, hallucinations, and a lack of behavioral grounding. Our prior work, FLARKO, embedded structured knowledge graphs (KGs) in LLM prompts to align advice with user behavior and market data. This paper introduces RAG-FLARKO, a retrieval-augmented extension to FLARKO, that overcomes scalability and relevance challenges using multi-stage and parallel KG retrieval processes. Our method first retrieves behaviorally relevant entities from a user's transaction KG and then uses this context to filter temporally consistent signals from a market KG, constructing a compact, grounded subgraph for the LLM. This pipeline reduces context overhead and sharpens the model's focus on relevant information. Empirical evaluation on a real-world financial transaction dataset demonstrates that RAG-FLARKO significantly enhances recommendation quality. Notably, our framework enables smaller, more efficient models to achieve high performance in both profitability and behavioral alignment, presenting a viable path for deploying grounded financial AI in resource-constrained environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 81,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11584",
        "abs_url": "https://arxiv.org/abs/2511.11584",
        "pdf_url": "https://arxiv.org/pdf/2511.11584",
        "title": "Output Supervision Can Obfuscate the Chain of Thought",
        "authors": [
            "Jacob Drori",
            "Luke Marks",
            "Bryce Woodworth",
            "Alex Cloud",
            "Alexander Matt Turner"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)",
        "abstract": "OpenAI (2025) showed that training against a chain of thought (CoT) monitor can cause obfuscated CoTs, which contain bad behavior the monitor cannot detect. They proposed to keep CoTs monitorable by training only against output monitors that do not have access to CoT. We show that such training can still cause obfuscated CoTs via two mechanisms. First, when a model is trained to produce a safe-looking output, that model may generalize to making its CoTs look safe. Second, since later tokens are conditioned on earlier ones, safe-looking CoTs may increase the likelihood of safe outputs, causing safe-looking CoTs to be reinforced. We introduce two mitigations to address these two issues, which achieve a Pareto improvement in terms of monitorability and task performance compared to regular training.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 82,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11587",
        "abs_url": "https://arxiv.org/abs/2511.11587",
        "pdf_url": "https://arxiv.org/pdf/2511.11587",
        "title": "MedBuild AI: An Agent-Based Hybrid Intelligence Framework for Reshaping Agency in Healthcare Infrastructure Planning through Generative Design for Medical Architecture",
        "authors": [
            "Yiming Zhang",
            "Yuejia Xu",
            "Ziyao Wang",
            "Xin Yan",
            "Xiaosai Hao"
        ],
        "comments": "25 pages, 16 figures. Submitted to the IJAC Special Issue \"Rebalance and Reciprocity\"",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Graphics (cs.GR); Multiagent Systems (cs.MA)",
        "abstract": "Globally, disparities in healthcare infrastructure remain stark, leaving countless communities without access to even basic services. Traditional infrastructure planning is often slow and inaccessible, and although many architects are actively delivering humanitarian and aid-driven hospital projects worldwide, these vital efforts still fall far short of the sheer scale and urgency of demand. This paper introduces MedBuild AI, a hybrid-intelligence framework that integrates large language models (LLMs) with deterministic expert systems to rebalance the early design and conceptual planning stages. As a web-based platform, it enables any region with satellite internet access to obtain guidance on modular, low-tech, low-cost medical building designs. The system operates through three agents: the first gathers local health intelligence via conversational interaction; the second translates this input into an architectural functional program through rule-based computation; and the third generates layouts and 3D models. By embedding computational negotiation into the design process, MedBuild AI fosters a reciprocal, inclusive, and equitable approach to healthcare planning, empowering communities and redefining agency in global healthcare architecture.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 83,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11590",
        "abs_url": "https://arxiv.org/abs/2511.11590",
        "pdf_url": "https://arxiv.org/pdf/2511.11590",
        "title": "Embedding Explainable AI in NHS Clinical Safety: The Explainability-Enabled Clinical Safety Framework (ECSF)",
        "authors": [
            "Robert Gigiu"
        ],
        "comments": "33 pages, 5 figures",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)",
        "abstract": "Artificial intelligence (AI) is increasingly embedded in NHS workflows, but its probabilistic and adaptive behaviour conflicts with the deterministic assumptions underpinning existing clinical-safety standards. DCB0129 and DCB0160 provide strong governance for conventional software yet do not define how AI-specific transparency, interpretability, or model drift should be evidenced within Safety Cases, Hazard Logs, or post-market monitoring. This paper proposes an Explainability-Enabled Clinical Safety Framework (ECSF) that integrates explainability into the DCB0129/0160 lifecycle, enabling Clinical Safety Officers to use interpretability outputs as structured safety evidence without altering compliance pathways. A cross-regulatory synthesis mapped DCB clauses to principles from Good Machine Learning Practice, the NHS AI Assurance and T.E.S.T. frameworks, and the EU AI Act. The resulting matrix links regulatory clauses, principles, ECSF checkpoints, and suitable explainability outputs. ECSF introduces five checkpoints: global transparency for hazard identification, case-level interpretability for verification, clinician usability for evaluation, traceable decision pathways for risk control, and longitudinal interpretability monitoring for post-market surveillance. Techniques such as SHAP, LIME, Integrated Gradients, saliency mapping, and attention visualisation are mapped to corresponding DCB artefacts. ECSF reframes explainability as a core element of clinical-safety assurance, bridging deterministic risk governance with the probabilistic behaviour of AI and supporting alignment with GMLP, the EU AI Act, and NHS AI Assurance principles.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 84,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11592",
        "abs_url": "https://arxiv.org/abs/2511.11592",
        "pdf_url": "https://arxiv.org/pdf/2511.11592",
        "title": "Mind Your Entropy: From Maximum Entropy to Trajectory Entropy-Constrained RL",
        "authors": [
            "Guojian Zhan",
            "Likun Wang",
            "Pengcheng Wang",
            "Feihong Zhang",
            "Jingliang Duan",
            "Masayoshi Tomizuka",
            "Shengbo Eben Li"
        ],
        "comments": "17 pages",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "Maximum entropy has become a mainstream off-policy reinforcement learning (RL) framework for balancing exploitation and exploration. However, two bottlenecks still limit further performance improvement: (1) non-stationary Q-value estimation caused by jointly injecting entropy and updating its weighting parameter, i.e., temperature; and (2) short-sighted local entropy tuning that adjusts temperature only according to the current single-step entropy, without considering the effect of cumulative entropy over time. In this paper, we extends maximum entropy framework by proposing a trajectory entropy-constrained reinforcement learning (TECRL) framework to address these two challenges. Within this framework, we first separately learn two Q-functions, one associated with reward and the other with entropy, ensuring clean and stable value targets unaffected by temperature updates. Then, the dedicated entropy Q-function, explicitly quantifying the expected cumulative entropy, enables us to enforce a trajectory entropy constraint and consequently control the policy long-term stochasticity. Building on this TECRL framework, we develop a practical off-policy algorithm, DSAC-E, by extending the state-of-the-art distributional soft actor-critic with three refinements (DSAC-T). Empirical results on the OpenAI Gym benchmark demonstrate that our DSAC-E can achieve higher returns and better stability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 85,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11593",
        "abs_url": "https://arxiv.org/abs/2511.11593",
        "pdf_url": "https://arxiv.org/pdf/2511.11593",
        "title": "Sound Logical Explanations for Mean Aggregation Graph Neural Networks",
        "authors": [
            "Matthew Morris",
            "Ian Horrocks"
        ],
        "comments": "Full version (with appendices) of paper accepted to NeurIPS 2025 (The Thirty-Ninth Annual Conference on Neural Information Processing Systems)",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Logic in Computer Science (cs.LO)",
        "abstract": "Graph neural networks (GNNs) are frequently used for knowledge graph completion. Their black-box nature has motivated work that uses sound logical rules to explain predictions and characterise their expressivity. However, despite the prevalence of GNNs that use mean as an aggregation function, explainability and expressivity results are lacking for them. We consider GNNs with mean aggregation and non-negative weights (MAGNNs), proving the precise class of monotonic rules that can be sound for them, as well as providing a restricted fragment of first-order logic to explain any MAGNN prediction. Our experiments show that restricting mean-aggregation GNNs to have non-negative weights yields comparable or improved performance on standard inductive benchmarks, that sound rules are obtained in practice, that insightful explanations can be generated in practice, and that the sound rules can expose issues in the trained models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 86,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11594",
        "abs_url": "https://arxiv.org/abs/2511.11594",
        "pdf_url": "https://arxiv.org/pdf/2511.11594",
        "title": "TimeStampEval: A Simple LLM Eval and a Little Fuzzy Matching Trick to Improve Search Accuracy",
        "authors": [
            "James McCammon"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Traditional fuzzy matching often fails when searching for quotes that are semantically identical but syntactically different across documents-a common issue when aligning official written records with speech-to-text transcripts. We introduce TimeStampEval, a benchmark for retrieving precise millisecond timestamps from long transcripts given non-verbatim quotes. Our simple two-stage method dramatically improves retrieval accuracy while cutting inference costs by over 90%. The motivating use case is an automated long-form podcast that assembles Congressional Record clips into AI-hosted narration. The technical challenge: given a sentence-timestamped transcript and a target quote that may differ due to transcription or editorial drift, return exact start and end boundaries. Standard algorithms handle verbatim text but break under fuzzier variants. Evaluating six modern LLMs on a 2,800-sentence (120k-token) transcript revealed four key findings. (1) Prompt design matters more than model choice: placing the query before the transcript and using compact formatting improved accuracy by 3-20 points while reducing token count by 30-40%. (2) Off-by-one errors form a distinct category, showing models understand the task but misplace boundaries. (3) A modest reasoning budget (600-850 tokens) raises accuracy from 37% to 77% for weak setups and to above 90% for strong ones. (4) Our \"Assisted Fuzzy\" approach-RapidFuzz pre-filtering followed by LLM verification on short snippets-improves fuzzy match accuracy by up to 50 points while halving latency and reducing cost per correct result by up to 96%. Extended tests on ten transcripts (50k-900k tokens, 1989-2025) confirm robustness to transcript length, vocabulary drift, and domain change, maintaining 95-100% rejection accuracy for absent targets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 87,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11595",
        "abs_url": "https://arxiv.org/abs/2511.11595",
        "pdf_url": "https://arxiv.org/pdf/2511.11595",
        "title": "Decision-Making Amid Information-Based Threats in Sociotechnical Systems: A Review",
        "authors": [
            "Aaron R. Allred",
            "Erin E. Richardson",
            "Sarah R. Bostrom",
            "James Crum",
            "Cara Spencer",
            "Chad Tossell",
            "Richard E. Niemeyer",
            "Leanne Hirshfield",
            "Allison P.A. Hayman"
        ],
        "comments": "",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)",
        "abstract": "Technological systems increasingly mediate human information exchange, spanning interactions among humans as well as between humans and artificial agents. The unprecedented scale and reliance on information disseminated through these systems substantially expand the scope of information-based influence that can both enable and undermine sound decision-making. Consequently, understanding and protecting decision-making today faces growing challenges, as individuals and organizations must navigate evolving opportunities and information-based threats across varied domains and information environments. While these risks are widely recognized, research remains fragmented: work evaluating information-based threat phenomena has progressed largely in isolation from foundational studies of human information processing. In this review, we synthesize insights from both domains to identify shared cognitive mechanisms that mediate vulnerability to information-based threats and shape behavioral outcomes. Finally, we outline directions for future research aimed at integrating these perspectives, emphasizing the importance of such integration for mitigating human vulnerabilities and aligning human-machine representations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 88,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11596",
        "abs_url": "https://arxiv.org/abs/2511.11596",
        "pdf_url": "https://arxiv.org/pdf/2511.11596",
        "title": "Loss Given Default Prediction Under Measurement-Induced Mixture Distributions: An Information-Theoretic Approach",
        "authors": [
            "Javier Marín"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Loss Given Default (LGD) modeling faces a fundamental data quality constraint: 90% of available training data consists of proxy estimates based on pre-distress balance sheets rather than actual recovery outcomes from completed bankruptcy proceedings. We demonstrate that this mixture-contaminated training structure causes systematic failure of recursive partitioning methods, with Random Forest achieving negative r-squared (-0.664, worse than predicting the mean) on held-out test data. Information-theoretic approaches based on Shannon entropy and mutual information provide superior generalization, achieving r-squared of 0.191 and RMSE of 0.284 on 1,218 corporate bankruptcies (1980-2023). Analysis reveals that leverage-based features contain 1.510 bits of mutual information while size effects contribute only 0.086 bits, contradicting regulatory assumptions about scale-dependent recovery. These results establish practical guidance for financial institutions deploying LGD models under Basel III requirements when representative outcome data is unavailable at sufficient scale. The findings generalize to medical outcomes research, climate forecasting, and technology reliability-domains where extended observation periods create unavoidable mixture structure in training data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 89,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11601",
        "abs_url": "https://arxiv.org/abs/2511.11601",
        "pdf_url": "https://arxiv.org/pdf/2511.11601",
        "title": "Mind the Gap: Revealing Inconsistencies Across Heterogeneous AI Accelerators",
        "authors": [
            "Elliott Wen",
            "Sean Ma",
            "Ewan Tempero",
            "Jens Dietrich",
            "Daniel Luo",
            "Jiaxing Shen",
            "Kaiqi Zhao",
            "Bruce Sham",
            "Yousong Song",
            "Jiayi Hua",
            "Jia Hong"
        ],
        "comments": "",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "While NVIDIA remains the dominant provider of AI accelerators within cloud data center, emerging vendors such as AMD, Intel, Mac, and Huawei offer cost-effective alternatives with claims of compatibility and performance. This paper presents the first empirical study investigating divergence in machine learning model across heterogeneous AI accelerators. Utilizing an automated pipeline, we synthesize over 100,000 variant models derived from 4,000 real-world models and execute them across five different enterprise-grade accelerators. Our findings suggest that newer AI platforms from Mac and Huawei support at least 17\\% fewer operators than NVIDIA. These platforms also exhibit a higher rate of output discrepancies (exceeding 5\\%), which stem from differences in operator implementations, handling of exceptional numerical values, and instruction scheduling. They are also more susceptible to failures during model compilation-based acceleration, and in some cases, the compiled models produce outputs that differ noticeably from those generated using the standard execution mode. In addition, we identify 7 implementation flaws in PyTorch and 40 platform-specific issues across vendors. These results underscore the challenges of achieving consistent machine learning behavior in an increasingly diverse hardware ecosystem.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 90,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11603",
        "abs_url": "https://arxiv.org/abs/2511.11603",
        "pdf_url": "https://arxiv.org/pdf/2511.11603",
        "title": "Machine learning-based cloud resource allocation algorithms: a comprehensive comparative review",
        "authors": [
            "Deep Bodra",
            "Sushil Khairnar"
        ],
        "comments": "",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Artificial Intelligence (cs.AI)",
        "abstract": "Cloud resource allocation has emerged as a major challenge in modern computing environments, with organizations struggling to manage complex, dynamic workloads while optimizing performance and cost efficiency. Traditional heuristic approaches prove inadequate for handling the multi-objective optimization demands of existing cloud infrastructures. This paper presents a comparative analysis of state-of-the-art artificial intelligence and machine learning algorithms for resource allocation. We systematically evaluate 10 algorithms across four categories: Deep Reinforcement Learning approaches, Neural Network architectures, Traditional Machine Learning enhanced methods, and Multi-Agent systems. Analysis of published results demonstrates significant performance improvements across multiple metrics including makespan reduction, cost optimization, and energy efficiency gains compared to traditional methods. The findings reveal that hybrid architectures combining multiple artificial intelligence and machine learning techniques consistently outperform single-method approaches, with edge computing environments showing the highest deployment readiness. Our analysis provides critical insights for both academic researchers and industry practitioners seeking to implement next-generation cloud resource allocation strategies in increasingly complex and dynamic computing environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 91,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11607",
        "abs_url": "https://arxiv.org/abs/2511.11607",
        "pdf_url": "https://arxiv.org/pdf/2511.11607",
        "title": "Clustering-Based Weight Orthogonalization for Stabilizing Deep Reinforcement Learning",
        "authors": [
            "Guoqing Ma",
            "Yuhan Zhang",
            "Yuming Dai",
            "Guangfu Hao",
            "Yang Chen",
            "Shan Yu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Reinforcement learning (RL) has made significant advancements, achieving superhuman performance in various tasks. However, RL agents often operate under the assumption of environmental stationarity, which poses a great challenge to learning efficiency since many environments are inherently non-stationary. This non-stationarity results in the requirement of millions of iterations, leading to low sample efficiency. To address this issue, we introduce the Clustering Orthogonal Weight Modified (COWM) layer, which can be integrated into the policy network of any RL algorithm and mitigate non-stationarity effectively. The COWM layer stabilizes the learning process by employing clustering techniques and a projection matrix. Our approach not only improves learning speed but also reduces gradient interference, thereby enhancing the overall learning efficiency. Empirically, the COWM outperforms state-of-the-art methods and achieves improvements of 9% and 12.6% in vision based and state-based DMControl benchmark. It also shows robustness and generality across various algorithms and tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 92,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11608",
        "abs_url": "https://arxiv.org/abs/2511.11608",
        "pdf_url": "https://arxiv.org/pdf/2511.11608",
        "title": "Why Should the Server Do It All?: A Scalable, Versatile, and Model-Agnostic Framework for Server-Light DNN Inference over Massively Distributed Clients via Training-Free Intermediate Feature Compression",
        "authors": [
            "Mingyu Sung",
            "Suhwan Im",
            "Daeho Bang",
            "Il-Min Kim",
            "Sangseok Yun",
            "Jae-Mo Kang"
        ],
        "comments": "",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Artificial Intelligence (cs.AI)",
        "abstract": "Modern DNNs often rely on edge-cloud model partitioning (MP), but widely used schemes fix shallow, static split points that underutilize edge compute and concentrate latency and energy on the server. The problem is exacerbated in autoregressive (AR) LLM inference, where per-token forward passes repeatedly generate bulky intermediate features (IFs). We introduce SLICER, a retraining-free, architecture-agnostic framework that compresses IFs to reduce both communication and server load in split computing. SLICER combines (i) asymmetric top-K filtering (ATKF) to sparsify low-magnitude activations, (ii) magnitude-splitting (MS) to group the remaining non-zeros into equal-cardinality blocks, and (iii) adaptive bit quantization (ABQ) that selects per-block bitwidths under a distortion budget. Across standard vision and LLM workloads (e.g., ImageNet/COCO; HellaSwag, PIQA, ARC-E/C, GSM8K, HumanEval), SLICER reduces uplink volume by up to 10x and server GPU time by up to 4.4x, while keeping task quality within ~0-3 pp of baseline. In multi-device settings and AR LLMs, SLICER scales by shifting meaningful compute to the edge and lowering bits-per-token and server time per token, stabilizing per-step traffic. The codec attaches to off-the-shelf models without retraining or architectural changes, offering a plug-and-play path to scalable, low-latency distributed inference. Code is provided in the supplementary material.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 93,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11612",
        "abs_url": "https://arxiv.org/abs/2511.11612",
        "pdf_url": "https://arxiv.org/pdf/2511.11612",
        "title": "Evaluating Large Language Models for Workload Mapping and Scheduling in Heterogeneous HPC Systems",
        "authors": [
            "Aasish Kumar Sharma",
            "Julian Kunkel"
        ],
        "comments": "14 pages, 4 figures, 2 tables. Evaluation study on LLM-based reasoning for HPC scheduling. Published in Research in Academic Engineering Journal (RAEJ), 2025",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Artificial Intelligence (cs.AI)",
        "abstract": "Large language models (LLMs) are increasingly explored for their reasoning capabilities, yet their ability to perform structured, constraint-based optimization from natural language remains insufficiently understood. This study evaluates twenty-one publicly available LLMs on a representative heterogeneous high-performance computing (HPC) workload mapping and scheduling problem. Each model received the same textual description of system nodes, task requirements, and scheduling constraints, and was required to assign tasks to nodes, compute the total makespan, and explain its reasoning. A manually derived analytical optimum of nine hours and twenty seconds served as the ground truth reference. Three models exactly reproduced the analytical optimum while satisfying all constraints, twelve achieved near-optimal results within two minutes of the reference, and six produced suboptimal schedules with arithmetic or dependency errors. All models generated feasible task-to-node mappings, though only about half maintained strict constraint adherence. Nineteen models produced partially executable verification code, and eighteen provided coherent step-by-step reasoning, demonstrating strong interpretability even when logical errors occurred. Overall, the results define the current capability boundary of LLM reasoning in combinatorial optimization: leading models can reconstruct optimal schedules directly from natural language, but most still struggle with precise timing, data transfer arithmetic, and dependency enforcement. These findings highlight the potential of LLMs as explainable co-pilots for optimization and decision-support tasks rather than autonomous solvers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 94,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11614",
        "abs_url": "https://arxiv.org/abs/2511.11614",
        "pdf_url": "https://arxiv.org/pdf/2511.11614",
        "title": "Beyond the GPU: The Strategic Role of FPGAs in the Next Wave of AI",
        "authors": [
            "Arturo Urías Jiménez"
        ],
        "comments": "",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Artificial Intelligence (cs.AI)",
        "abstract": "AI acceleration has been dominated by GPUs, but the growing need for lower latency, energy efficiency, and fine-grained hardware control exposes the limits of fixed architectures. In this context, Field-Programmable Gate Arrays (FPGAs) emerge as a reconfigurable platform that allows mapping AI algorithms directly into device logic. Their ability to implement parallel pipelines for convolutions, attention mechanisms, and post-processing with deterministic timing and reduced power consumption makes them a strategic option for workloads that demand predictable performance and deep customization. Unlike CPUs and GPUs, whose architecture is immutable, an FPGA can be reconfigured in the field to adapt its physical structure to a specific model, integrate as a SoC with embedded processors, and run inference near the sensor without sending raw data to the cloud. This reduces latency and required bandwidth, improves privacy, and frees GPUs from specialized tasks in data centers. Partial reconfiguration and compilation flows from AI frameworks are shortening the path from prototype to deployment, enabling hardware--algorithm co-design.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 95,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11615",
        "abs_url": "https://arxiv.org/abs/2511.11615",
        "pdf_url": "https://arxiv.org/pdf/2511.11615",
        "title": "Lightweight Hopfield Neural Networks for Bioacoustic Detection and Call Monitoring of Captive Primates",
        "authors": [
            "Wendy Lomas",
            "Andrew Gascoyne",
            "Colin Dubreuil",
            "Stefano Vaglio",
            "Liam Naughton"
        ],
        "comments": "16 pages, 3 figures, Proceedings of the Future Technologies Conference (FTC) 2025, Volume 1",
        "subjects": "Sound (cs.SD); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)",
        "abstract": "Passive acoustic monitoring is a sustainable method of monitoring wildlife and environments that leads to the generation of large datasets and, currently, a processing backlog. Academic research into automating this process is focused on the application of resource intensive convolutional neural networks which require large pre-labelled datasets for training and lack flexibility in application. We present a viable alternative relevant in both wild and captive settings; a transparent, lightweight and fast-to-train associative memory AI model with Hopfield neural network (HNN) architecture. Adapted from a model developed to detect bat echolocation calls, this model monitors captive endangered black-and-white ruffed lemur Varecia variegata vocalisations. Lemur social calls of interest when monitoring welfare are stored in the HNN in order to detect other call instances across the larger acoustic dataset. We make significant model improvements by storing an additional signal caused by movement and achieve an overall accuracy of 0.94. The model can perform $340$ classifications per second, processing over 5.5 hours of audio data per minute, on a standard laptop running other applications. It has broad applicability and trains in milliseconds. Our lightweight solution reduces data-to-insight turnaround times and can accelerate decision making in both captive and wild settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 96,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11616",
        "abs_url": "https://arxiv.org/abs/2511.11616",
        "pdf_url": "https://arxiv.org/pdf/2511.11616",
        "title": "Hierarchical Federated Graph Attention Networks for Scalable and Resilient UAV Collision Avoidance",
        "authors": [
            "Rathin Chandra Shit",
            "Sharmila Subudhi"
        ],
        "comments": "Accepted and scheduled for conference presentation",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multiagent Systems (cs.MA)",
        "abstract": "The real-time performance, adversarial resiliency, and privacy preservation are the most important metrics that need to be balanced to practice collision avoidance in large-scale multi-UAV (Unmanned Aerial Vehicle) systems. Current frameworks tend to prescribe monolithic solutions that are not only prohibitively computationally complex with a scaling cost of $O(n^2)$ but simply do not offer Byzantine fault tolerance. The proposed hierarchical framework presented in this paper tries to eliminate such trade-offs by stratifying a three-layered architecture. We spread the intelligence into three layers: an immediate collision avoiding local layer running on dense graph attention with latency of $<10 ms$, a regional layer using sparse attention with $O(nk)$ computational complexity and asynchronous federated learning with coordinate-wise trimmed mean aggregation, and lastly, a global layer using a lightweight Hashgraph-inspired protocol. We have proposed an adaptive differential privacy mechanism, wherein the noise level $(\\epsilon \\in [0.1, 1.0])$ is dynamically reduced based on an evaluation of the measured real-time threat that in turn maximized the privacy-utility tradeoff. Through the use of Distributed Hash Table (DHT)-based lightweight audit logging instead of heavyweight blockchain consensus, the median cost of getting a $95^{th}$ percentile decision within 50ms is observed across all tested swarm sizes. This architecture provides a scalable scenario of 500 UAVs with a collision rate of $< 2.0\\%$ and the Byzantine fault tolerance of $f < n/3$.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 97,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11619",
        "abs_url": "https://arxiv.org/abs/2511.11619",
        "pdf_url": "https://arxiv.org/pdf/2511.11619",
        "title": "DIAP: A Decentralized Agent Identity Protocol with Zero-Knowledge Proofs and a Hybrid P2P Stack",
        "authors": [
            "Yuanjie Liu",
            "Wenpeng Xing",
            "Ye Zhou",
            "Gaowei Chang",
            "Changting Lin",
            "Meng Han"
        ],
        "comments": "",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Artificial Intelligence (cs.AI)",
        "abstract": "The absence of a fully decentralized, verifiable, and privacy-preserving communication protocol for autonomous agents remains a core challenge in decentralized computing. Existing systems often rely on centralized intermediaries, which reintroduce trust bottlenecks, or lack decentralized identity-resolution mechanisms, limiting persistence and cross-network interoperability. We propose the Decentralized Interstellar Agent Protocol (DIAP), a novel framework for agent identity and communication that enables persistent, verifiable, and trustless interoperability in fully decentralized environments. DIAP binds an agent's identity to an immutable IPFS or IPNS content identifier and uses zero-knowledge proofs (ZKP) to dynamically and statelessly prove ownership, removing the need for record updates. We present a Rust SDK that integrates Noir (for zero-knowledge proofs), DID-Key, IPFS, and a hybrid peer-to-peer stack combining Libp2p GossipSub for discovery and Iroh for high-performance, QUIC based data exchange. DIAP introduces a zero-dependency ZKP deployment model through a universal proof manager and compile-time build script that embeds a precompiled Noir circuit, eliminating the need for external ZKP toolchains. This enables instant, verifiable, and privacy-preserving identity proofs. This work establishes a practical, high-performance foundation for next-generation autonomous agent ecosystems and agent-to-agent (A to A) economies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 98,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11621",
        "abs_url": "https://arxiv.org/abs/2511.11621",
        "pdf_url": "https://arxiv.org/pdf/2511.11621",
        "title": "AIvailable: A Software-Defined Architecture for LLM-as-a-Service on Heterogeneous and Legacy GPUs",
        "authors": [
            "Pedro Antunes",
            "Ana Rita Ortigoso",
            "Gabriel Vieira",
            "Daniel Fuentes",
            "Luís Frazão",
            "Nuno Costa",
            "António Pereira"
        ],
        "comments": "",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Artificial Intelligence (cs.AI); Networking and Internet Architecture (cs.NI)",
        "abstract": "The rise of Large Language Models (LLM) has increased the need for scalable, high-performance inference systems, yet most existing frameworks assume homogeneous, resource-rich hardware, often unrealistic in academic, or resource-constrained settings. We introduce AIvailable, a low-cost, highly available LLM-as-a-Service (LLMaaS) platform, that uses a software-defined approach for running LLMs across heterogeneous and legacy GPU nodes, including NVIDIA and AMD devices, with a focus on fully utilizing each node's VRAM. AIvailable operates as a fully GPU-accelerated inference without CPU fallbacks, featuring a unified client interface that allows seamless interaction with all deployed LLMs through a single logical unit. The architecture comprises four main components: the Client Interface for user access, the Service Frontend for secure request routing and load balancing, the SDAI Controller for orchestration, deployment, and monitoring, and the Service Backend of heterogeneous GPU nodes executing workloads. By abstracting GPU-specific details and providing dynamic, VRAM-aware allocation and reallocation of models, AIvailable ensures efficient use of resources and resilience against failures or workload fluctuations. Targeting academic labs, private companies, and other constrained organizations, it supports diverse open LLMs helping democratize generative AI through the repurposing of legacy GPUs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 99,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11622",
        "abs_url": "https://arxiv.org/abs/2511.11622",
        "pdf_url": "https://arxiv.org/pdf/2511.11622",
        "title": "Small Vocabularies, Big Gains: Pretraining and Tokenization in Time Series Models",
        "authors": [
            "Alexis Roger",
            "Gwen Legate",
            "Kashif Rasul",
            "Yuriy Nevmyvaka",
            "Irina Rish"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Tokenization and transfer learning are two critical components in building state of the art time series foundation models for forecasting. In this work, we systematically study the effect of tokenizer design, specifically scaling and quantization strategies, on model performance, alongside the impact of pretraining versus random initialization. We show that tokenizer configuration primarily governs the representational capacity and stability of the model, while transfer learning influences optimization efficiency and alignment. Using a combination of empirical training experiments and theoretical analyses, we demonstrate that pretrained models consistently leverage well-designed tokenizers more effectively, particularly at smaller vocabulary sizes. Conversely, misaligned tokenization can diminish or even invert the benefits of pretraining. These findings highlight the importance of careful tokenization in time series modeling and suggest that combining small, efficient vocabularies with pretrained weights is especially advantageous in multi-modal forecasting settings, where the overall vocabulary must be shared across modalities. Our results provide concrete guidance for designing tokenizers and leveraging transfer learning in discrete representation learning for continuous signals.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 100,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11623",
        "abs_url": "https://arxiv.org/abs/2511.11623",
        "pdf_url": "https://arxiv.org/pdf/2511.11623",
        "title": "Early GVHD Prediction in Liver Transplantation via Multi-Modal Deep Learning on Imbalanced EHR Data",
        "authors": [
            "Yushan Jiang",
            "Shuteng Niu",
            "Dongjin Song",
            "Yichen Wang",
            "Jingna Feng",
            "Xinyue Hu",
            "Liu Yang",
            "Cui Tao"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Graft-versus-host disease (GVHD) is a rare but often fatal complication in liver transplantation, with a very high mortality rate. By harnessing multi-modal deep learning methods to integrate heterogeneous and imbalanced electronic health records (EHR), we aim to advance early prediction of GVHD, paving the way for timely intervention and improved patient outcomes. In this study, we analyzed pre-transplant electronic health records (EHR) spanning the period before surgery for 2,100 liver transplantation patients, including 42 cases of graft-versus-host disease (GVHD), from a cohort treated at Mayo Clinic between 1992 and 2025. The dataset comprised four major modalities: patient demographics, laboratory tests, diagnoses, and medications. We developed a multi-modal deep learning framework that dynamically fuses these modalities, handles irregular records with missing values, and addresses extreme class imbalance through AUC-based optimization. The developed framework outperforms all single-modal and multi-modal machine learning baselines, achieving an AUC of 0.836, an AUPRC of 0.157, a recall of 0.768, and a specificity of 0.803. It also demonstrates the effectiveness of our approach in capturing complementary information from different modalities, leading to improved performance. Our multi-modal deep learning framework substantially improves existing approaches for early GVHD prediction. By effectively addressing the challenges of heterogeneity and extreme class imbalance in real-world EHR, it achieves accurate early prediction. Our proposed multi-modal deep learning method demonstrates promising results for early prediction of a GVHD in liver transplantation, despite the challenge of extremely imbalanced EHR data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 101,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11624",
        "abs_url": "https://arxiv.org/abs/2511.11624",
        "pdf_url": "https://arxiv.org/pdf/2511.11624",
        "title": "Characterizing and Understanding Energy Footprint and Efficiency of Small Language Model on Edges",
        "authors": [
            "Md Romyull Islam",
            "Bobin Deng",
            "Nobel Dhar",
            "Tu N. Nguyen",
            "Selena He",
            "Yong Shi",
            "Kun Suo"
        ],
        "comments": "Submitted version; 9 pages, 5 figures; presented at IEEE MASS 2025 (online publication pending)",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Cloud-based large language models (LLMs) and their variants have significantly influenced real-world applications. Deploying smaller models (i.e., small language models (SLMs)) on edge devices offers additional advantages, such as reduced latency and independence from network connectivity. However, edge devices' limited computing resources and constrained energy budgets challenge efficient deployment. This study evaluates the power efficiency of five representative SLMs - Llama 3.2, Phi-3 Mini, TinyLlama, and Gemma 2 on Raspberry Pi 5, Jetson Nano, and Jetson Orin Nano (CPU and GPU configurations). Results show that Jetson Orin Nano with GPU acceleration achieves the highest energy-to-performance ratio, significantly outperforming CPU-based setups. Llama 3.2 provides the best balance of accuracy and power efficiency, while TinyLlama is well-suited for low-power environments at the cost of reduced accuracy. In contrast, Phi-3 Mini consumes the most energy despite its high accuracy. In addition, GPU acceleration, memory bandwidth, and model architecture are key in optimizing inference energy efficiency. Our empirical analysis offers practical insights for AI, smart systems, and mobile ad-hoc platforms to leverage tradeoffs from accuracy, inference latency, and power efficiency in energy-constrained environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 102,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11625",
        "abs_url": "https://arxiv.org/abs/2511.11625",
        "pdf_url": "https://arxiv.org/pdf/2511.11625",
        "title": "MedFedPure: A Medical Federated Framework with MAE-based Detection and Diffusion Purification for Inference-Time Attacks",
        "authors": [
            "Mohammad Karami",
            "Mohammad Reza Nemati",
            "Aidin Kazemi",
            "Ali Mikaeili Barzili",
            "Hamid Azadegan",
            "Behzad Moshiri"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)",
        "abstract": "Artificial intelligence (AI) has shown great potential in medical imaging, particularly for brain tumor detection using Magnetic Resonance Imaging (MRI). However, the models remain vulnerable at inference time when they are trained collaboratively through Federated Learning (FL), an approach adopted to protect patient privacy. Adversarial attacks can subtly alter medical scans in ways invisible to the human eye yet powerful enough to mislead AI models, potentially causing serious misdiagnoses. Existing defenses often assume centralized data and struggle to cope with the decentralized and diverse nature of federated medical settings. In this work, we present MedFedPure, a personalized federated learning defense framework designed to protect diagnostic AI models at inference time without compromising privacy or accuracy. MedFedPure combines three key elements: (1) a personalized FL model that adapts to the unique data distribution of each institution; (2) a Masked Autoencoder (MAE) that detects suspicious inputs by exposing hidden perturbations; and (3) an adaptive diffusion-based purification module that selectively cleans only the flagged scans before classification. Together, these steps offer robust protection while preserving the integrity of normal, benign images. We evaluated MedFedPure on the Br35H brain MRI dataset. The results show a significant gain in adversarial robustness, improving performance from 49.50% to 87.33% under strong attacks, while maintaining a high clean accuracy of 97.67%. By operating locally and in real time during diagnosis, our framework provides a practical path to deploying secure, trustworthy, and privacy-preserving AI tools in clinical workflows. Index Terms: cancer, tumor detection, federated learning, masked autoencoder, diffusion, privacy",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 103,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11627",
        "abs_url": "https://arxiv.org/abs/2511.11627",
        "pdf_url": "https://arxiv.org/pdf/2511.11627",
        "title": "SA-EMO: Structure-Aligned Encoder Mixture of Operators for Generalizable Full-waveform Inversion",
        "authors": [
            "Wang Zhenyu",
            "Li Peiyuan",
            "Shi Yongxiang",
            "Wu Ruoyu",
            "Zhang Lei"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Full-waveform inversion (FWI) can produce high-resolution subsurface models, yet it remains inherently ill-posed, highly nonlinear, and computationally intensive. Although recent deep learning and numerical acceleration methods have improved speed and scalability, they often rely on single CNN architectures or single neural operators, which struggle to generalize in unknown or complex geological settings and are ineffective at distinguishing diverse geological types. To address these issues, we propose a Structure-Aligned Encoder-Mixture-of-Operators (SA-EMO) architecture for velocity-field inversion under unknown subsurface structures. First, a structure-aligned encoder maps high-dimensional seismic wavefields into a physically consistent latent space, thereby eliminating spatio-temporal mismatch between the waveform and velocity domains, recovering high-frequency components, and enhancing feature generalization. Then, an adaptive routing mechanism selects and fuses multiple neural-operator experts, including spectral, wavelet, multiscale, and local operators, to predict the velocity model. We systematically evaluate our approach on the OpenFWI benchmark and the Marmousi2 dataset. Results show that SA-EMO significantly outperforms traditional CNN or single-operator methods, achieving an average MAE reduction of approximately 58.443% and an improvement in boundary resolution of about 10.308%. Ablation studies further reveal that the structure-aligned encoder, the expert-fusion mechanism, and the routing module each contribute markedly to the performance gains. This work introduces a new paradigm for efficient, scalable, and physically interpretable full-waveform inversion.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 104,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11628",
        "abs_url": "https://arxiv.org/abs/2511.11628",
        "pdf_url": "https://arxiv.org/pdf/2511.11628",
        "title": "Mixture-of-Schedulers: An Adaptive Scheduling Agent as a Learned Router for Expert Policies",
        "authors": [
            "Xinbo Wang",
            "Shian Jia",
            "Ziyang Huang",
            "Jing Cao",
            "Mingli Song"
        ],
        "comments": "",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Artificial Intelligence (cs.AI)",
        "abstract": "Modern operating system schedulers employ a single, static policy, which struggles to deliver optimal performance across the diverse and dynamic workloads of contemporary systems. This \"one-policy-fits-all\" approach leads to significant compromises in fairness, throughput, and latency, particularly with the rise of heterogeneous hardware and varied application architectures. This paper proposes a new paradigm: dynamically selecting the optimal policy from a portfolio of specialized schedulers rather than designing a single, monolithic one. We present the Adaptive Scheduling Agent (ASA), a lightweight framework that intelligently matches workloads to the most suitable \"expert\" scheduling policy at runtime. ASA's core is a novel, low-overhead offline/online approach. First, an offline process trains a universal, hardware-agnostic machine learning model to recognize abstract workload patterns from system behaviors. Second, at runtime, ASA continually processes the model's predictions using a time-weighted probability voting algorithm to identify the workload, then makes a scheduling decision by consulting a pre-configured, machine-specific mapping table to switch to the optimal scheduler via Linux's sched_ext framework. This decoupled architecture allows ASA to adapt to new hardware platforms rapidly without expensive retraining of the core recognition model. Our evaluation, based on a novel benchmark focused on user-experience metrics, demonstrates that ASA consistently outperforms the default Linux scheduler (EEVDF), achieving superior results in 86.4% of test scenarios. Furthermore, ASA's selections are near-optimal, ranking among the top three schedulers in 78.6% of all scenarios. This validates our approach as a practical path toward more intelligent, adaptive, and responsive operating system schedulers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 105,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11629",
        "abs_url": "https://arxiv.org/abs/2511.11629",
        "pdf_url": "https://arxiv.org/pdf/2511.11629",
        "title": "Global Feature Enhancing and Fusion Framework for Strain Gauge Time Series Classification",
        "authors": [
            "Xu Zhang",
            "Peng Wang",
            "Chen Wang",
            "Zhe Xu",
            "Xiaohua Nie",
            "Wei Wang"
        ],
        "comments": "Global Feature Enhancing and Fusion Framework for Time Series Classification",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Strain Gauge Status (SGS) recognition is crucial in the field of intelligent manufacturing based on the Internet of Things, as accurate identification helps timely detection of failed mechanical components, avoiding accidents. The loading and unloading sequences generated by strain gauges can be identified through time series classification (TSC) algorithms. Recently, deep learning models, e.g., convolutional neural networks (CNNs) have shown remarkable success in the TSC task, as they can extract discriminative local features from the subsequences to identify the time series. However, we observe that only the local features may not be sufficient for expressing the time series, especially when the local sub-sequences between different time series are very similar, e.g., SGS data of aircraft wings in static strength experiments. Nevertheless, CNNs suffer from the limitation in extracting global features due to the nature of convolution operations. For extracting global features to more comprehensively represent the SGS time series, we propose two insights: (i) Constructing global features through feature engineering. (ii) Learning high-order relationships between local features to capture global features. To realize and utilize them, we propose a hypergraph-based global feature learning and fusion framework, which learns and fuses global features for semantic consistency to enhance the representation of SGS time series, thereby improving recognition accuracy. Our method designs are validated on industrial SGS and public UCR datasets, showing better generalization for unseen data in SGS recognition.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 106,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11630",
        "abs_url": "https://arxiv.org/abs/2511.11630",
        "pdf_url": "https://arxiv.org/pdf/2511.11630",
        "title": "Predicting Grain Growth in Polycrystalline Materials Using Deep Learning Time Series Models",
        "authors": [
            "Eliane Younes",
            "Elie Hachem",
            "Marc Bernacki"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Materials Science (cond-mat.mtrl-sci); Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE)",
        "abstract": "Grain Growth strongly influences the mechanical behavior of materials, making its prediction a key objective in microstructural engineering. In this study, several deep learning approaches were evaluated, including recurrent neural networks (RNN), long short-term memory (LSTM), temporal convolutional networks (TCN), and transformers, to forecast grain size distributions during grain growth. Unlike full-field simulations, which are computationally demanding, the present work relies on mean-field statistical descriptors extracted from high-fidelity simulations. A dataset of 120 grain growth sequences was processed into normalized grain size distributions as a function of time. The models were trained to predict future distributions from a short temporal history using a recursive forecasting strategy. Among the tested models, the LSTM network achieved the highest accuracy (above 90\\%) and the most stable performance, maintaining physically consistent predictions over extended horizons while reducing computation time from about 20 minutes per sequence to only a few seconds, whereas the other architectures tended to diverge when forecasting further in time. These results highlight the potential of low-dimensional descriptors and LSTM-based forecasting for efficient and accurate microstructure prediction, with direct implications for digital twin development and process optimization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 107,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11632",
        "abs_url": "https://arxiv.org/abs/2511.11632",
        "pdf_url": "https://arxiv.org/pdf/2511.11632",
        "title": "Toward Better Generalization in Few-Shot Learning through the Meta-Component Combination",
        "authors": [
            "Qiuhao Zeng"
        ],
        "comments": "20 pages, 5 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "In few-shot learning, classifiers are expected to generalize to unseen classes given only a small number of instances of each new class. One of the popular solutions to few-shot learning is metric-based meta-learning. However, it highly depends on the deep metric learned on seen classes, which may overfit to seen classes and fail to generalize well on unseen classes. To improve the generalization, we explore the substructures of classifiers and propose a novel meta-learning algorithm to learn each classifier as a combination of meta-components. Meta-components are learned across meta-learning episodes on seen classes and disentangled by imposing an orthogonal regularizer to promote its diversity and capture various shared substructures among different classifiers. Extensive experiments on few-shot benchmark tasks show superior performances of the proposed method.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 108,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11635",
        "abs_url": "https://arxiv.org/abs/2511.11635",
        "pdf_url": "https://arxiv.org/pdf/2511.11635",
        "title": "EduAgentQG: A Multi-Agent Workflow Framework for Personalized Question Generation",
        "authors": [
            "Rui Jia",
            "Min Zhang",
            "Fengrui Liu",
            "Bo Jiang",
            "Kun Kuang",
            "Zhongxiang Dai"
        ],
        "comments": "",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "High-quality personalized question banks are crucial for supporting adaptive learning and individualized assessment. Manually designing questions is time-consuming and often fails to meet diverse learning needs, making automated question generation a crucial approach to reduce teachers' workload and improve the scalability of educational resources. However, most existing question generation methods rely on single-agent or rule-based pipelines, which still produce questions with unstable quality, limited diversity, and insufficient alignment with educational goals. To address these challenges, we propose EduAgentQG, a multi-agent collaborative framework for generating high-quality and diverse personalized questions. The framework consists of five specialized agents and operates through an iterative feedback loop: the Planner generates structured design plans and multiple question directions to enhance diversity; the Writer produces candidate questions based on the plan and optimizes their quality and diversity using feedback from the Solver and Educator; the Solver and Educator perform binary scoring across multiple evaluation dimensions and feed the evaluation results back to the Writer; the Checker conducts final verification, including answer correctness and clarity, ensuring alignment with educational goals. Through this multi-agent collaboration and iterative feedback loop, EduAgentQG generates questions that are both high-quality and diverse, while maintaining consistency with educational objectives. Experiments on two mathematics question datasets demonstrate that EduAgentQG outperforms existing single-agent and multi-agent methods in terms of question diversity, goal consistency, and overall quality.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 109,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11641",
        "abs_url": "https://arxiv.org/abs/2511.11641",
        "pdf_url": "https://arxiv.org/pdf/2511.11641",
        "title": "EcoSpa: Efficient Transformer Training with Coupled Sparsity",
        "authors": [
            "Jinqi Xiao",
            "Cheng Luo",
            "Lingyi Huang",
            "Cheng Yang",
            "Yang Sui",
            "Huy Phan",
            "Xiao Zang",
            "Yibiao Ying",
            "Zhexiang Tang",
            "Anima Anandkumar",
            "Bo Yuan"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Performance (cs.PF)",
        "abstract": "Transformers have become the backbone of modern AI, yet their high computational demands pose critical system challenges. While sparse training offers efficiency gains, existing methods fail to preserve critical structural relationships between weight matrices that interact multiplicatively in attention and feed-forward layers. This oversight leads to performance degradation at high sparsity levels. We introduce EcoSpa, an efficient structured sparse training method that jointly evaluates and sparsifies coupled weight matrix pairs, preserving their interaction patterns through aligned row/column removal. EcoSpa introduces a new granularity for calibrating structural component importance and performs coupled estimation and sparsification across both pre-training and fine-tuning scenarios. Evaluations demonstrate substantial improvements: EcoSpa enables efficient training of LLaMA-1B with 50\\% memory reduction and 21\\% faster training, achieves $2.2\\times$ model compression on GPT-2-Medium with $2.4$ lower perplexity, and delivers $1.6\\times$ inference speedup. The approach uses standard PyTorch operations, requiring no custom hardware or kernels, making efficient transformer training accessible on commodity hardware.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 110,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11646",
        "abs_url": "https://arxiv.org/abs/2511.11646",
        "pdf_url": "https://arxiv.org/pdf/2511.11646",
        "title": "A Deep Learning Model to Predicting Changes in Consumer Attributes for New Line-extended Products",
        "authors": [
            "Li Yinxing",
            "Tsukasa Ishigaki"
        ],
        "comments": "23 pages",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)",
        "abstract": "Product line extension is a marketing strategy that enhances a company's sphere of influence. Because excessive line extensions disrupt brand image, only appropriate line extensions based on consumer needs are desirable. Marketers should know the key consumer attributes of the primary customers for new line-extended products before companies enter the market. This paper describes a method for predicting changes in consumer attributes for new line-extended products using a novel deep learning model. The proposed model, Conditional Tabular Variational Auto-Encoder (CTVAE), generates synthetic data from large-scale tabular data of consumers and products. It can provide various implications about effective product line marketing for marketers. The experimental results demonstrate that the CTVAE offers superior prediction performance than existing models. We indicate implications for new products that change containers or flavors for effective product line marketing. The proposed approach has the potential to contribute to avoiding cannibalization and to designing product images and marketing strategies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 111,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11647",
        "abs_url": "https://arxiv.org/abs/2511.11647",
        "pdf_url": "https://arxiv.org/pdf/2511.11647",
        "title": "Environment-Aware Transfer Reinforcement Learning for Sustainable Beam Selection",
        "authors": [
            "Dariush Salami",
            "Ramin Hashemi",
            "Parham Kazemi",
            "Mikko A. Uusitalo"
        ],
        "comments": "Accepted to be published in a workshop in IEEE GLOBECOM 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Information Theory (cs.IT)",
        "abstract": "This paper presents a novel and sustainable approach for improving beam selection in 5G and beyond networks using transfer learning and Reinforcement Learning (RL). Traditional RL-based beam selection models require extensive training time and computational resources, particularly when deployed in diverse environments with varying propagation characteristics posing a major challenge for scalability and energy efficiency. To address this, we propose modeling the environment as a point cloud, where each point represents the locations of gNodeBs (gNBs) and surrounding scatterers. By computing the Chamfer distance between point clouds, structurally similar environments can be efficiently identified, enabling the reuse of pre-trained models through transfer learning. This methodology leads to a 16x reduction in training time and computational overhead, directly contributing to energy efficiency. By minimizing the need for retraining in each new deployment, our approach significantly lowers power consumption and supports the development of green and sustainable Artificial Intelligence (AI) in wireless systems. Furthermore, it accelerates time-to-deployment, reduces carbon emissions associated with training, and enhances the viability of deploying AI-driven communication systems at the edge. Simulation results confirm that our approach maintains high performance while drastically cutting energy costs, demonstrating the potential of transfer learning to enable scalable, adaptive, and environmentally conscious RL-based beam selection strategies in dynamic and diverse propagation environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 112,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11648",
        "abs_url": "https://arxiv.org/abs/2511.11648",
        "pdf_url": "https://arxiv.org/pdf/2511.11648",
        "title": "Lightweight Time Series Data Valuation on Time Series Foundation Models via In-Context Finetuning",
        "authors": [
            "Shunyu Wu",
            "Tianyue Li",
            "Yixuan Leng",
            "Jingyi Suo",
            "Jian Lou",
            "Dan Li",
            "See-Kiong Ng"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Time series foundation models (TSFMs) have demonstrated increasing capabilities due to their extensive pretraining on large volumes of diverse time series data. Consequently, the quality of time series data is crucial to TSFM performance, rendering an accurate and efficient data valuation of time series for TSFMs indispensable. However, traditional data valuation methods, such as influence functions, face severe computational bottlenecks due to their poor scalability with growing TSFM model sizes and often fail to preserve temporal dependencies. In this paper, we propose LTSV, a Lightweight Time Series Valuation on TSFMS via in-context finetuning. Grounded in the theoretical evidence that in-context finetuning approximates the influence function, LTSV estimates a sample's contribution by measuring the change in context loss after in-context finetuning, leveraging the strong generalization capabilities of TSFMs to produce robust and transferable data valuations. To capture temporal dependencies, we introduce temporal block aggregation, which integrates per-block influence scores across overlapping time windows. Experiments across multiple time series datasets and models demonstrate that LTSV consistently provides reliable and strong valuation performance, while maintaining manageable computational requirements. Our results suggest that in-context finetuning on time series foundation models provides a practical and effective bridge between data attribution and model generalization in time series learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 113,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11650",
        "abs_url": "https://arxiv.org/abs/2511.11650",
        "pdf_url": "https://arxiv.org/pdf/2511.11650",
        "title": "Enhanced Water Leak Detection with Convolutional Neural Networks and One-Class Support Vector Machine",
        "authors": [
            "Daniele Ugo Leonzio",
            "Paolo Bestagini",
            "Marco Marcon",
            "Stefano Tubaro"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Water is a critical resource that must be managed efficiently. However, a substantial amount of water is lost each year due to leaks in Water Distribution Networks (WDNs). This underscores the need for reliable and effective leak detection and localization systems. In recent years, various solutions have been proposed, with data-driven approaches gaining increasing attention due to their superior performance. In this paper, we propose a new method for leak detection. The method is based on water pressure measurements acquired at a series of nodes of a WDN. Our technique is a fully data-driven solution that makes only use of the knowledge of the WDN topology, and a series of pressure data acquisitions obtained in absence of leaks. The proposed solution is based on an feature extractor and a one-class Support Vector Machines (SVM) trained on no-leak data, so that leaks are detected as anomalies. The results achieved on a simulate dataset using the Modena WDN demonstrate that the proposed solution outperforms recent methods for leak detection.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 114,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11651",
        "abs_url": "https://arxiv.org/abs/2511.11651",
        "pdf_url": "https://arxiv.org/pdf/2511.11651",
        "title": "Incomplete Depression Feature Selection with Missing EEG Channels",
        "authors": [
            "Zhijian Gong",
            "Wenjia Dong",
            "Xueyuan Xu",
            "Fulin Wei",
            "Chunyu Liu",
            "Li Zhuo"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "As a critical mental health disorder, depression has severe effects on both human physical and mental well-being. Recent developments in EEG-based depression analysis have shown promise in improving depression detection accuracies. However, EEG features often contain redundant, irrelevant, and noisy information. Additionally, real-world EEG data acquisition frequently faces challenges, such as data loss from electrode detachment and heavy noise interference. To tackle the challenges, we propose a novel feature selection approach for robust depression analysis, called Incomplete Depression Feature Selection with Missing EEG Channels (IDFS-MEC). IDFS-MEC integrates missing-channel indicator information and adaptive channel weighting learning into orthogonal regression to lessen the effects of incomplete channels on model construction, and then utilizes global redundancy minimization learning to reduce redundant information among selected feature subsets. Extensive experiments conducted on MODMA and PRED-d003 datasets reveal that the EEG feature subsets chosen by IDFS-MEC have superior performance than 10 popular feature selection methods among 3-, 64-, and 128-channel settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 115,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11653",
        "abs_url": "https://arxiv.org/abs/2511.11653",
        "pdf_url": "https://arxiv.org/pdf/2511.11653",
        "title": "GroupRank: A Groupwise Reranking Paradigm Driven by Reinforcement Learning",
        "authors": [
            "Duolin Sun",
            "Meixiu Long",
            "Dan Yang",
            "Yihan Jiao",
            "Zhehao Tan",
            "Jie Feng",
            "Junjie Wang",
            "Yue Shen",
            "Peng Wei",
            "Jian Wang",
            "Jinjie Gu"
        ],
        "comments": "",
        "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Large Language Models have shown strong potential as rerankers to enhance the overall performance of RAG systems. However, existing reranking paradigms are constrained by a core theoretical and practical dilemma: Pointwise methods, while simple and highly flexible, evaluate documents independently, making them prone to the Ranking Myopia Trap, overlooking the relative importance between documents. In contrast, Listwise methods can perceive the global ranking context, but suffer from inherent List Rigidity, leading to severe scalability and flexibility issues when handling large candidate sets. To address these challenges, we propose Groupwise, a novel reranking paradigm. In this approach, the query and a group of candidate documents are jointly fed into the model, which performs within-group comparisons to assign individual relevance scores to each document. This design retains the flexibility of Pointwise methods while enabling the comparative capability of Listwise methods. We further adopt GRPO for model training, equipped with a heterogeneous reward function that integrates ranking metrics with a distributional reward aimed at aligning score distributions across groups. To overcome the bottleneck caused by the scarcity of high quality labeled data, we further propose an innovative pipeline for synthesizing high quality retrieval and ranking data. The resulting data can be leveraged not only for training the reranker but also for training the retriever. Extensive experiments validate the effectiveness of our approach. On two reasoning intensive retrieval benchmarks, BRIGHT and R2MED.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 116,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11654",
        "abs_url": "https://arxiv.org/abs/2511.11654",
        "pdf_url": "https://arxiv.org/pdf/2511.11654",
        "title": "Convergence of Multiagent Learning Systems for Traffic control",
        "authors": [
            "Sayambhu Sen",
            "Shalabh Bhatnagar"
        ],
        "comments": "14 pages 2 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)",
        "abstract": "Rapid urbanization in cities like Bangalore has led to severe traffic congestion, making efficient Traffic Signal Control (TSC) essential. Multi-Agent Reinforcement Learning (MARL), often modeling each traffic signal as an independent agent using Q-learning, has emerged as a promising strategy to reduce average commuter delays. While prior work Prashant L A et. al has empirically demonstrated the effectiveness of this approach, a rigorous theoretical analysis of its stability and convergence properties in the context of traffic control has not been explored. This paper bridges that gap by focusing squarely on the theoretical basis of this multi-agent algorithm. We investigate the convergence problem inherent in using independent learners for the cooperative TSC task. Utilizing stochastic approximation methods, we formally analyze the learning dynamics. The primary contribution of this work is the proof that the specific multi-agent reinforcement learning algorithm for traffic control is proven to converge under the given conditions extending it from single agent convergence proofs for asynchronous value iteration.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 117,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11656",
        "abs_url": "https://arxiv.org/abs/2511.11656",
        "pdf_url": "https://arxiv.org/pdf/2511.11656",
        "title": "On the Probabilistic Learnability of Compact Neural Network Preimage Bounds",
        "authors": [
            "Luca Marzari",
            "Manuele Bicego",
            "Ferdinando Cicalese",
            "Alessandro Farinelli"
        ],
        "comments": "Accepted at the 40th Annual AAAI Conference on Artificial Intelligence 2026",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Although recent provable methods have been developed to compute preimage bounds for neural networks, their scalability is fundamentally limited by the #P-hardness of the problem. In this work, we adopt a novel probabilistic perspective, aiming to deliver solutions with high-confidence guarantees and bounded error. To this end, we investigate the potential of bootstrap-based and randomized approaches that are capable of capturing complex patterns in high-dimensional spaces, including input regions where a given output property holds. In detail, we introduce $\\textbf{R}$andom $\\textbf{F}$orest $\\textbf{Pro}$perty $\\textbf{Ve}$rifier ($\\texttt{RF-ProVe}$), a method that exploits an ensemble of randomized decision trees to generate candidate input regions satisfying a desired output property and refines them through active resampling. Our theoretical derivations offer formal statistical guarantees on region purity and global coverage, providing a practical, scalable solution for computing compact preimage approximations in cases where exact solvers fail to scale.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 118,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11663",
        "abs_url": "https://arxiv.org/abs/2511.11663",
        "pdf_url": "https://arxiv.org/pdf/2511.11663",
        "title": "SpecQuant: Spectral Decomposition and Adaptive Truncation for Ultra-Low-Bit LLMs Quantization",
        "authors": [
            "Zhixiong Zhao",
            "Fangxin Liu",
            "Junjie Wang",
            "Chenyang Guan",
            "Zongwu Wang",
            "Li Jiang",
            "Haibing Guan"
        ],
        "comments": "Accepted at AAAI 2026",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "The emergence of accurate open large language models (LLMs) has sparked a push for advanced quantization techniques to enable efficient deployment on end-user devices. In this paper, we revisit the challenge of extreme LLM compression -- targeting ultra-low-bit quantization for both activations and weights -- from a Fourier frequency domain perspective. We propose SpecQuant, a two-stage framework that tackles activation outliers and cross-channel variance. In the first stage, activation outliers are smoothed and transferred into the weight matrix to simplify downstream quantization. In the second stage, we apply channel-wise low-frequency Fourier truncation to suppress high-frequency components while preserving essential signal energy, improving quantization robustness. Our method builds on the principle that most of the weight energy is concentrated in low-frequency components, which can be retained with minimal impact on model accuracy. To enable runtime adaptability, we introduce a lightweight truncation module during inference that adjusts truncation thresholds based on channel characteristics. On LLaMA-3 8B, SpecQuant achieves 4-bit quantization for both weights and activations, narrowing the zero-shot accuracy gap to only 1.5% compared to full precision, while delivering 2 times faster inference and 3times lower memory usage.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 119,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11665",
        "abs_url": "https://arxiv.org/abs/2511.11665",
        "pdf_url": "https://arxiv.org/pdf/2511.11665",
        "title": "Clifford Algebraic Rotor Embeddings : Maybe embeddings should start to CARE",
        "authors": [
            "Sameeksha Sriram",
            "Ayush Paliwal",
            "Alexander S. Ecker",
            "Chase van de Geijn"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Rotary Positional Embeddings (RoPE) have demonstrated exceptional performance as a positional encoding method, consistently outperforming their baselines. While recent work has sought to extend RoPE to higher-dimensional inputs, many such extensions are non-commutative, thereby forfeiting RoPE's shift-equivariance property. Spherical RoPE is one such non-commutative variant, motivated by the idea of rotating embedding vectors on spheres rather than circles. However, spherical rotations are inherently non-commutative, making the choice of rotation sequence ambiguous. In this work, we explore a quaternion-based approach -- Quaternion Rotary Embeddings (QuatRo) -- in place of Euler angles, leveraging quaternions' ability to represent 3D rotations to parameterize the axes of rotation. We show Mixed RoPE and Spherical RoPE to be special cases of QuatRo. Further, we propose a generalization of QuatRo to Clifford Algebraic Rotary Embeddings (CARE) using geometric algebra. Viewing quaternions as the even subalgebra of Cl(3,0,0), we extend the notion of rotary embeddings from quaternions to Clifford rotors acting on multivectors. This formulation enables two key generalizations: (1) extending rotary embeddings to arbitrary dimensions, and (2) encoding positional information in multivectors of multiple grades, not just vectors. We present preliminary experiments comparing spherical, quaternion, and Clifford-based rotary embeddings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 120,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11667",
        "abs_url": "https://arxiv.org/abs/2511.11667",
        "pdf_url": "https://arxiv.org/pdf/2511.11667",
        "title": "Beyond Superficial Forgetting: Thorough Unlearning through Knowledge Density Estimation and Block Re-insertion",
        "authors": [
            "Feng Guo",
            "Yuntao Wen",
            "Shen Gao",
            "Junshuo Zhang",
            "Shuo Shang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Machine unlearning, which selectively removes harmful knowledge from a pre-trained model without retraining from scratch, is crucial for addressing privacy, regulatory compliance, and ethical concerns in Large Language Models (LLMs). However, existing unlearning methods often struggle to thoroughly remove harmful knowledge, leaving residual harmful knowledge that can be easily recovered. To address these limitations, we propose Knowledge Density-Guided Unlearning via Blocks Reinsertion (KUnBR), a novel approach that first identifies layers with rich harmful knowledge and then thoroughly eliminates the harmful knowledge via re-insertion strategy. Our method introduces knowledge density estimation to quantify and locate layers containing the most harmful knowledge, enabling precise unlearning. Additionally, we design a layer re-insertion strategy that extracts and re-inserts harmful knowledge-rich layers into the original LLM, bypassing gradient obstruction caused by cover layers and ensuring effective gradient propagation during unlearning. Extensive experiments conducted on several unlearning and general capability benchmarks demonstrate that KUnBR achieves state-of-the-art forgetting performance while maintaining model utility.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 121,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11668",
        "abs_url": "https://arxiv.org/abs/2511.11668",
        "pdf_url": "https://arxiv.org/pdf/2511.11668",
        "title": "Do traveling waves make good positional encodings?",
        "authors": [
            "Chase van de Geijn",
            "Ayush Paliwal",
            "Timo Lüddecke",
            "Alexander S. Ecker"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Transformers rely on positional encoding to compensate for the inherent permutation invariance of self-attention. Traditional approaches use absolute sinusoidal embeddings or learned positional vectors, while more recent methods emphasize relative encodings to better capture translation equivariances. In this work, we propose RollPE, a novel positional encoding mechanism based on traveling waves, implemented by applying a circular roll operation to the query and key tensors in self-attention. This operation induces a relative shift in phase across positions, allowing the model to compute attention as a function of positional differences rather than absolute indices. We show this simple method significantly outperforms traditional absolute positional embeddings and is comparable to RoPE. We derive a continuous case of RollPE which implicitly imposes a topographic structure on the query and key space. We further derive a mathematical equivalence of RollPE to a particular configuration of RoPE. Viewing RollPE through the lens of traveling waves may allow us to simplify RoPE and relate it to processes of information flow in the brain.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 122,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11674",
        "abs_url": "https://arxiv.org/abs/2511.11674",
        "pdf_url": "https://arxiv.org/pdf/2511.11674",
        "title": "The Singularity Warfare: The metatheoretical Framework",
        "authors": [
            "Ridvan Bari Urcosta"
        ],
        "comments": "",
        "subjects": "Physics and Society (physics.soc-ph); Artificial Intelligence (cs.AI)",
        "abstract": "This paper introduces the \"Singularity Warfare\" concept, arguing that the accelerating pace of technological revolution, driven by artificial intelligence and quantum mechanics, is fundamentally reshaping the nature of conflict. Moving beyond traditional \"Newtonian\" warfare and current military doctrines, this framework posits that future battlefields will be defined by a merger of physical and abstract domains, where human imagination and algorithmic logic become a unified, actionable reality. Victory will hinge on a unit's ability to maintain cognitive and technological \"coherence\" while creating \"decoherence\" in the adversary. The paper synthesizes theories from physics, philosophy, and futurology to provide a metatheoretical framework for understanding this paradigm shift.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 123,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11675",
        "abs_url": "https://arxiv.org/abs/2511.11675",
        "pdf_url": "https://arxiv.org/pdf/2511.11675",
        "title": "Beyond One-Way Pruning: Bidirectional Pruning-Regrowth for Extreme Accuracy-Sparsity Tradeoff",
        "authors": [
            "Junchen Liu",
            "Yi Sheng"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "As a widely adopted model compression technique, model pruning has demonstrated strong effectiveness across various architectures. However, we observe that when sparsity exceeds a certain threshold, both iterative and one-shot pruning methods lead to a steep decline in model performance. This rapid degradation limits the achievable compression ratio and prevents models from meeting the stringent size constraints required by certain hardware platforms, rendering them inoperable. To overcome this limitation, we propose a bidirectional pruning-regrowth strategy. Starting from an extremely compressed network that satisfies hardware constraints, the method selectively regenerates critical connections to recover lost performance, effectively mitigating the sharp accuracy drop commonly observed under high sparsity conditions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 124,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11678",
        "abs_url": "https://arxiv.org/abs/2511.11678",
        "pdf_url": "https://arxiv.org/pdf/2511.11678",
        "title": "A Structure-Agnostic Co-Tuning Framework for LLMs and SLMs in Cloud-Edge Systems",
        "authors": [
            "Yuze Liu",
            "Yunhan Wang",
            "Tiehua Zhang",
            "Zhishu Shen",
            "Cheng Peng",
            "Libing Wu",
            "Feng Xia",
            "Jiong Jin"
        ],
        "comments": "",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "The surge in intelligent applications driven by large language models (LLMs) has made it increasingly difficult for bandwidth-limited cloud servers to process extensive LLM workloads in real time without compromising user data privacy. To solve these problems, recent research has focused on constructing cloud-edge consortia that integrate server-based LLM with small language models (SLMs) on mobile edge devices. Furthermore, designing collaborative training mechanisms within such consortia to enhance inference performance has emerged as a promising research direction. However, the cross-domain deployment of SLMs, coupled with structural heterogeneity in SLMs architectures, poses significant challenges to enhancing model performance. To this end, we propose Co-PLMs, a novel co-tuning framework for collaborative training of large and small language models, which integrates the process of structure-agnostic mutual learning to realize knowledge exchange between the heterogeneous language models. This framework employs distilled proxy models (DPMs) as bridges to enable collaborative training between the heterogeneous server-based LLM and on-device SLMs, while preserving the domain-specific insights of each device. The experimental results show that Co-PLMs outperform state-of-the-art methods, achieving average increases of 5.38% in Rouge-L and 4.88% in EM.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 125,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11683",
        "abs_url": "https://arxiv.org/abs/2511.11683",
        "pdf_url": "https://arxiv.org/pdf/2511.11683",
        "title": "Stratified Knowledge-Density Super-Network for Scalable Vision Transformers",
        "authors": [
            "Longhua Li",
            "Lei Qi",
            "Xin Geng"
        ],
        "comments": "Accepted by AAAI 2026",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Training and deploying multiple vision transformer (ViT) models for different resource constraints is costly and inefficient. To address this, we propose transforming a pre-trained ViT into a stratified knowledge-density super-network, where knowledge is hierarchically organized across weights. This enables flexible extraction of sub-networks that retain maximal knowledge for varying model sizes. We introduce \\textbf{W}eighted \\textbf{P}CA for \\textbf{A}ttention \\textbf{C}ontraction (WPAC), which concentrates knowledge into a compact set of critical weights. WPAC applies token-wise weighted principal component analysis to intermediate features and injects the resulting transformation and inverse matrices into adjacent layers, preserving the original network function while enhancing knowledge compactness. To further promote stratified knowledge organization, we propose \\textbf{P}rogressive \\textbf{I}mportance-\\textbf{A}ware \\textbf{D}ropout (PIAD). PIAD progressively evaluates the importance of weight groups, updates an importance-aware dropout list, and trains the super-network under this dropout regime to promote knowledge stratification. Experiments demonstrate that WPAC outperforms existing pruning criteria in knowledge concentration, and the combination with PIAD offers a strong alternative to state-of-the-art model compression and model expansion methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 126,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11691",
        "abs_url": "https://arxiv.org/abs/2511.11691",
        "pdf_url": "https://arxiv.org/pdf/2511.11691",
        "title": "Beyond saliency: enhancing explanation of speech emotion recognition with expert-referenced acoustic cues",
        "authors": [
            "Seham Nasr",
            "Zhao Ren",
            "David Johnson"
        ],
        "comments": "5 pages, 2 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Sound (cs.SD)",
        "abstract": "Explainable AI (XAI) for Speech Emotion Recognition (SER) is critical for building transparent, trustworthy models. Current saliency-based methods, adapted from vision, highlight spectrogram regions but fail to show whether these regions correspond to meaningful acoustic markers of emotion, limiting faithfulness and interpretability. We propose a framework that overcomes these limitations by quantifying the magnitudes of cues within salient regions. This clarifies \"what\" is highlighted and connects it to \"why\" it matters, linking saliency to expert-referenced acoustic cues of speech emotions. Experiments on benchmark SER datasets show that our approach improves explanation quality by explicitly linking salient regions to theory-driven speech emotions expert-referenced acoustics. Compared to standard saliency methods, it provides more understandable and plausible explanations of SER models, offering a foundational step towards trustworthy speech-based affective computing.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 127,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11702",
        "abs_url": "https://arxiv.org/abs/2511.11702",
        "pdf_url": "https://arxiv.org/pdf/2511.11702",
        "title": "Task-Aware 3D Affordance Segmentation via 2D Guidance and Geometric Refinement",
        "authors": [
            "Lian He",
            "Meng Liu",
            "Qilang Ye",
            "Yu Zhou",
            "Xiang Deng",
            "Gangyi Ding"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Image and Video Processing (eess.IV)",
        "abstract": "Understanding 3D scene-level affordances from natural language instructions is essential for enabling embodied agents to interact meaningfully in complex environments. However, this task remains challenging due to the need for semantic reasoning and spatial grounding. Existing methods mainly focus on object-level affordances or merely lift 2D predictions to 3D, neglecting rich geometric structure information in point clouds and incurring high computational costs. To address these limitations, we introduce Task-Aware 3D Scene-level Affordance segmentation (TASA), a novel geometry-optimized framework that jointly leverages 2D semantic cues and 3D geometric reasoning in a coarse-to-fine manner. To improve the affordance detection efficiency, TASA features a task-aware 2D affordance detection module to identify manipulable points from language and visual inputs, guiding the selection of task-relevant views. To fully exploit 3D geometric information, a 3D affordance refinement module is proposed to integrate 2D semantic priors with local 3D geometry, resulting in accurate and spatially coherent 3D affordance masks. Experiments on SceneFun3D demonstrate that TASA significantly outperforms the baselines in both accuracy and efficiency in scene-level affordance segmentation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 128,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11703",
        "abs_url": "https://arxiv.org/abs/2511.11703",
        "pdf_url": "https://arxiv.org/pdf/2511.11703",
        "title": "Enhancing Reinforcement Learning in 3D Environments through Semantic Segmentation: A Case Study in ViZDoom",
        "authors": [
            "Hugo Huang"
        ],
        "comments": "Master's Thesis at the University of Edinburgh (2024)",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Robotics (cs.RO)",
        "abstract": "Reinforcement learning (RL) in 3D environments with high-dimensional sensory input poses two major challenges: (1) the high memory consumption induced by memory buffers required to stabilise learning, and (2) the complexity of learning in partially observable Markov Decision Processes (POMDPs). This project addresses these challenges by proposing two novel input representations: SS-only and RGB+SS, both employing semantic segmentation on RGB colour images. Experiments were conducted in deathmatches of ViZDoom, utilizing perfect segmentation results for controlled evaluation. Our results showed that SS-only was able to reduce the memory consumption of memory buffers by at least 66.6%, and up to 98.6% when a vectorisable lossless compression technique with minimal overhead such as run-length encoding is applied. Meanwhile, RGB+SS significantly enhances RL agents' performance with the additional semantic information provided. Furthermore, we explored density-based heatmapping as a tool to visualise RL agents' movement patterns and evaluate their suitability for data collection. A brief comparison with a previous approach highlights how our method overcame common pitfalls in applying semantic segmentation in 3D environments like ViZDoom.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 129,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11712",
        "abs_url": "https://arxiv.org/abs/2511.11712",
        "pdf_url": "https://arxiv.org/pdf/2511.11712",
        "title": "Reasoning: From Reflection to Solution",
        "authors": [
            "Zixi Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "What is reasoning? This question has driven centuries of philosophical inquiry, from Aristotle's syllogisms to modern computational complexity theory. In the age of large language models achieving superhuman performance on benchmarks like GSM8K (95\\% accuracy) and HumanEval (90\\% pass@1), we must ask: have these systems learned to \\emph{reason}, or have they learned to \\emph{pattern-match over reasoning traces}? This paper argues for a specific answer: \\textbf{reasoning is iterative operator application in state spaces, converging to fixed points}. This definition is not merely philosophical -- it has concrete architectural implications that explain both the failures of current systems and the path to genuine reasoning capabilities. Our investigation begins with a puzzle (OpenXOR), progresses through theory (OpenOperator), and culminates in a working solution (OpenLM) that achieves 76\\% accuracy where state-of-the-art LLMs achieve 0\\%. This is not about criticizing existing systems, but about \\emph{understanding what reasoning requires} and \\emph{building architectures that provide it}.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 130,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11719",
        "abs_url": "https://arxiv.org/abs/2511.11719",
        "pdf_url": "https://arxiv.org/pdf/2511.11719",
        "title": "ECCENTRIC: Edge-Cloud Collaboration Framework for Distributed Inference Using Knowledge Adaptation",
        "authors": [
            "Mohammad Mahdi Kamani",
            "Zhongwei Cheng",
            "Lin Chen"
        ],
        "comments": "",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Artificial Intelligence (cs.AI)",
        "abstract": "The massive growth in the utilization of edge AI has made the applications of machine learning models ubiquitous in different domains. Despite the computation and communication efficiency of these systems, due to limited computation resources on edge devices, relying on more computationally rich systems on the cloud side is inevitable in most cases. Cloud inference systems can achieve the best performance while the computation and communication cost is dramatically increasing by the expansion of a number of edge devices relying on these systems. Hence, there is a trade-off between the computation, communication, and performance of these systems. In this paper, we propose a novel framework, dubbed as Eccentric that learns models with different levels of trade-offs between these conflicting objectives. This framework, based on an adaptation of knowledge from the edge model to the cloud one, reduces the computation and communication costs of the system during inference while achieving the best performance possible. The Eccentric framework can be considered as a new form of compression method suited for edge-cloud inference systems to reduce both computation and communication costs. Empirical studies on classification and object detection tasks corroborate the efficacy of this framework.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 131,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11721",
        "abs_url": "https://arxiv.org/abs/2511.11721",
        "pdf_url": "https://arxiv.org/pdf/2511.11721",
        "title": "A Meta-Heuristic Load Balancer for Cloud Computing Systems",
        "authors": [
            "Leszek Sliwko",
            "Vladimir Getov"
        ],
        "comments": "",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Artificial Intelligence (cs.AI)",
        "abstract": "This paper presents a strategy to allocate services on a Cloud system without overloading nodes and maintaining the system stability with minimum cost. We specify an abstract model of cloud resources utilization, including multiple types of resources as well as considerations for the service migration costs. A prototype meta-heuristic load balancer is demonstrated and experimental results are presented and discussed. We also propose a novel genetic algorithm, where population is seeded with the outputs of other meta-heuristic algorithms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 132,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11725",
        "abs_url": "https://arxiv.org/abs/2511.11725",
        "pdf_url": "https://arxiv.org/pdf/2511.11725",
        "title": "Do Blind Spots Matter for Word-Referent Mapping? A Computational Study with Infant Egocentric Video",
        "authors": [
            "Zekai Shi",
            "Zhixi Cai",
            "Kalin Stefanov"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Typically, children start to learn their first words between 6 and 9 months, linking spoken utterances to their visual referents. Without prior knowledge, a word encountered for the first time can be interpreted in countless ways; it might refer to any of the objects in the environment, their components, or attributes. Using longitudinal, egocentric, and ecologically valid data from the experience of one child, in this work, we propose a self-supervised and biologically plausible strategy to learn strong visual representations. Our masked autoencoder-based visual backbone incorporates knowledge about the blind spot in human eyes to define a novel masking strategy. This mask and reconstruct approach attempts to mimic the way the human brain fills the gaps in the eyes' field of view. This represents a significant shift from standard random masking strategies, which are difficult to justify from a biological perspective. The pretrained encoder is utilized in a contrastive learning-based video-text model capable of acquiring word-referent mappings. Extensive evaluation suggests that the proposed biologically plausible masking strategy is at least as effective as random masking for learning word-referent mappings from cross-situational and temporally extended episodes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 133,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11733",
        "abs_url": "https://arxiv.org/abs/2511.11733",
        "pdf_url": "https://arxiv.org/pdf/2511.11733",
        "title": "Speculative Decoding in Decentralized LLM Inference: Turning Communication Latency into Computation Throughput",
        "authors": [
            "Jingwei Song",
            "Wanyi Chen",
            "Xinyuan Song",
            "Chris Tong",
            "Gufeng Chen",
            "Tianyi Zhao",
            "Eric Yang",
            "Bill Shi",
            "Lynn Ai"
        ],
        "comments": "6 pages, 2 figures, 2 tables. Uses ICML 2025 style",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Artificial Intelligence (cs.AI)",
        "abstract": "Speculative decoding accelerates large language model (LLM) inference by using a lightweight draft model to propose tokens that are later verified by a stronger target model. While effective in centralized systems, its behavior in decentralized settings, where network latency often dominates compute, remains under-characterized. We present Decentralized Speculative Decoding (DSD), a plug-and-play framework for decentralized inference that turns communication delay into useful computation by verifying multiple candidate tokens in parallel across distributed nodes. We further introduce an adaptive speculative verification strategy that adjusts acceptance thresholds by token-level semantic importance, delivering an additional 15% to 20% end-to-end speedup without retraining. In theory, DSD reduces cross-node communication cost by approximately (N-1)t1(k-1)/k, where t1 is per-link latency and k is the average number of tokens accepted per round. In practice, DSD achieves up to 2.56x speedup on HumanEval and 2.59x on GSM8K, surpassing the Eagle3 baseline while preserving accuracy. These results show that adapting speculative decoding for decentralized execution provides a system-level optimization that converts network stalls into throughput, enabling faster distributed LLM inference with no model retraining or architectural changes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 134,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11734",
        "abs_url": "https://arxiv.org/abs/2511.11734",
        "pdf_url": "https://arxiv.org/pdf/2511.11734",
        "title": "Physics-Informed Neural ODEs with Scale-Aware Residuals for Learning Stiff Biophysical Dynamics",
        "authors": [
            "Kamalpreet Singh Kainth",
            "Prathamesh Dinesh Joshi",
            "Raj Abhijit Dandekar",
            "Rajat Dandekar",
            "Sreedat Panat"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Neural differential equations offer a powerful framework for modeling continuous-time dynamics, but forecasting stiff biophysical systems remains unreliable. Standard Neural ODEs and physics informed variants often require orders of magnitude more iterations, and even then may converge to suboptimal solutions that fail to preserve oscillatory frequency or amplitude. We introduce PhysicsInformed Neural ODEs with with Scale-Aware Residuals (PI-NODE-SR), a framework that combines a low-order explicit solver (Heun method) residual normalisation to balance contributions between state variables evolving on disparate timescales. This combination stabilises training under realistic iteration budgets and avoids reliance on computationally expensive implicit solvers. On the Hodgkin-Huxley equations, PI-NODE-SR learns from a single oscillation simulated with a stiff solver (Rodas5P) and extrapolates beyond 100 ms, capturing both oscillation frequency and near-correct amplitudes. Remarkably, end-to-end learning of the vector field enables PI-NODE-SR to recover morphological features such as sharp subthreshold curvature in gating variables that are typically reserved for higher-order solvers, suggesting that neural correction can offset numerical diffusion. While performance remains sensitive to initialisation, PI-NODE-SR consistently reduces long-horizon errors relative to baseline Neural-ODEs and PINNs, offering a principled route to stable and efficient learning of stiff biological dynamics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 135,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11737",
        "abs_url": "https://arxiv.org/abs/2511.11737",
        "pdf_url": "https://arxiv.org/pdf/2511.11737",
        "title": "DK-Root: A Joint Data-and-Knowledge-Driven Framework for Root Cause Analysis of QoE Degradations in Mobile Networks",
        "authors": [
            "Qizhe Li",
            "Haolong Chen",
            "Jiansheng Li",
            "Shuqi Chai",
            "Xuan Li",
            "Yuzhou Hou",
            "Xinhua Shao",
            "Fangfang Li",
            "Kaifeng Han",
            "Guangxu Zhu"
        ],
        "comments": "13 pages, submitted for possible publication",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE)",
        "abstract": "Diagnosing the root causes of Quality of Experience (QoE) degradations in operational mobile networks is challenging due to complex cross-layer interactions among kernel performance indicators (KPIs) and the scarcity of reliable expert annotations. Although rule-based heuristics can generate labels at scale, they are noisy and coarse-grained, limiting the accuracy of purely data-driven approaches. To address this, we propose DK-Root, a joint data-and-knowledge-driven framework that unifies scalable weak supervision with precise expert guidance for robust root-cause analysis. DK-Root first pretrains an encoder via contrastive representation learning using abundant rule-based labels while explicitly denoising their noise through a supervised contrastive objective. To supply task-faithful data augmentation, we introduce a class-conditional diffusion model that generates KPIs sequences preserving root-cause semantics, and by controlling reverse diffusion steps, it produces weak and strong augmentations that improve intra-class compactness and inter-class separability. Finally, the encoder and the lightweight classifier are jointly fine-tuned with scarce expert-verified labels to sharpen decision boundaries. Extensive experiments on a real-world, operator-grade dataset demonstrate state-of-the-art accuracy, with DK-Root surpassing traditional ML and recent semi-supervised time-series methods. Ablations confirm the necessity of the conditional diffusion augmentation and the pretrain-finetune design, validating both representation quality and classification gains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 136,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11740",
        "abs_url": "https://arxiv.org/abs/2511.11740",
        "pdf_url": "https://arxiv.org/pdf/2511.11740",
        "title": "ExpertAD: Enhancing Autonomous Driving Systems with Mixture of Experts",
        "authors": [
            "Haowen Jiang",
            "Xinyu Huang",
            "You Lu",
            "Dingji Wang",
            "Yuheng Cao",
            "Chaofeng Sha",
            "Bihuan Chen",
            "Keyu Chen",
            "Xin Peng"
        ],
        "comments": "The paper has been accepted by the Fortieth AAAI Conference on Artificial Intelligence. AAAI 2026",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
        "abstract": "Recent advancements in end-to-end autonomous driving systems (ADSs) underscore their potential for perception and planning capabilities. However, challenges remain. Complex driving scenarios contain rich semantic information, yet ambiguous or noisy semantics can compromise decision reliability, while interference between multiple driving tasks may hinder optimal planning. Furthermore, prolonged inference latency slows decision-making, increasing the risk of unsafe driving behaviors. To address these challenges, we propose ExpertAD, a novel framework that enhances the performance of ADS with Mixture of Experts (MoE) architecture. We introduce a Perception Adapter (PA) to amplify task-critical features, ensuring contextually relevant scene understanding, and a Mixture of Sparse Experts (MoSE) to minimize task interference during prediction, allowing for effective and efficient planning. Our experiments show that ExpertAD reduces average collision rates by up to 20% and inference latency by 25% compared to prior methods. We further evaluate its multi-skill planning capabilities in rare scenarios (e.g., accidents, yielding to emergency vehicles) and demonstrate strong generalization to unseen urban environments. Additionally, we present a case study that illustrates its decision-making process in complex driving scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 137,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11743",
        "abs_url": "https://arxiv.org/abs/2511.11743",
        "pdf_url": "https://arxiv.org/pdf/2511.11743",
        "title": "Uncertainty Makes It Stable: Curiosity-Driven Quantized Mixture-of-Experts",
        "authors": [
            "Sebastián Andrés Cajas Ordóñez",
            "Luis Fernando Torres Torres",
            "Mackenzie J. Meni",
            "Carlos Andrés Duran Paredes",
            "Eric Arazo",
            "Cristian Bosch",
            "Ricardo Simon Carbajo",
            "Yuan Lai",
            "Leo Anthony Celi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Deploying deep neural networks on resource-constrained devices faces two critical challenges: maintaining accuracy under aggressive quantization while ensuring predictable inference latency. We present a curiosity-driven quantized Mixture-of-Experts framework that addresses both through Bayesian epistemic uncertainty-based routing across heterogeneous experts (BitNet ternary, 1-16 bit BitLinear, post-training quantization). Evaluated on audio classification benchmarks (ESC-50, Quinn, UrbanSound8K), our 4-bit quantization maintains 99.9 percent of 16-bit accuracy (0.858 vs 0.859 F1) with 4x compression and 41 percent energy savings versus 8-bit. Crucially, curiosity-driven routing reduces MoE latency variance by 82 percent (p = 0.008, Levene's test) from 230 ms to 29 ms standard deviation, enabling stable inference for battery-constrained devices. Statistical analysis confirms 4-bit/8-bit achieve practical equivalence with full precision (p > 0.05), while MoE architectures introduce 11 percent latency overhead (p < 0.001) without accuracy gains. At scale, deployment emissions dominate training by 10000x for models serving more than 1,000 inferences, making inference efficiency critical. Our information-theoretic routing demonstrates that adaptive quantization yields accurate (0.858 F1, 1.2M params), energy-efficient (3.87 F1/mJ), and predictable edge models, with simple 4-bit quantized architectures outperforming complex MoE for most deployments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 138,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11746",
        "abs_url": "https://arxiv.org/abs/2511.11746",
        "pdf_url": "https://arxiv.org/pdf/2511.11746",
        "title": "Diffusion Models: A Mathematical Introduction",
        "authors": [
            "Sepehr Maleki",
            "Negar Pourmoazemi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "We present a concise, self-contained derivation of diffusion-based generative models. Starting from basic properties of Gaussian distributions (densities, quadratic expectations, re-parameterisation, products, and KL divergences), we construct denoising diffusion probabilistic models from first principles. This includes the forward noising process, its closed-form marginals, the exact discrete reverse posterior, and the related variational bound. This bound simplifies to the standard noise-prediction goal used in practice. We then discuss likelihood estimation and accelerated sampling, covering DDIM, adversarially learned reverse dynamics (DDGAN), and multi-scale variants such as nested and latent diffusion, with Stable Diffusion as a canonical example. A continuous-time formulation follows, in which we derive the probability-flow ODE from the diffusion SDE via the continuity and Fokker-Planck equations, introduce flow matching, and show how rectified flows recover DDIM up to a time re-parameterisation. Finally, we treat guided diffusion, interpreting classifier guidance as a posterior score correction and classifier-free guidance as a principled interpolation between conditional and unconditional scores. Throughout, the focus is on transparent algebra, explicit intermediate steps, and consistent notation, so that readers can both follow the theory and implement the corresponding algorithms in practice.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 139,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11750",
        "abs_url": "https://arxiv.org/abs/2511.11750",
        "pdf_url": "https://arxiv.org/pdf/2511.11750",
        "title": "IDOL: Meeting Diverse Distribution Shifts with Prior Physics for Tropical Cyclone Multi-Task Estimation",
        "authors": [
            "Hanting Yan",
            "Pan Mu",
            "Shiqi Zhang",
            "Yuchao Zhu",
            "Jinglin Zhang",
            "Cong Bai"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Tropical Cyclone (TC) estimation aims to accurately estimate various TC attributes in real time. However, distribution shifts arising from the complex and dynamic nature of TC environmental fields, such as varying geographical conditions and seasonal changes, present significant challenges to reliable estimation. Most existing methods rely on multi-modal fusion for feature extraction but overlook the intrinsic distribution of feature representations, leading to poor generalization under out-of-distribution (OOD) scenarios. To address this, we propose an effective Identity Distribution-Oriented Physical Invariant Learning framework (IDOL), which imposes identity-oriented constraints to regulate the feature space under the guidance of prior physical knowledge, thereby dealing distribution variability with physical invariance. Specifically, the proposed IDOL employs the wind field model and dark correlation knowledge of TC to model task-shared and task-specific identity tokens. These tokens capture task dependencies and intrinsic physical invariances of TC, enabling robust estimation of TC wind speed, pressure, inner-core, and outer-core size under distribution shifts. Extensive experiments conducted on multiple datasets and tasks demonstrate the outperformance of the proposed IDOL, verifying that imposing identity-oriented constraints based on prior physical knowledge can effectively mitigates diverse distribution shifts in TC this http URL is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 140,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11757",
        "abs_url": "https://arxiv.org/abs/2511.11757",
        "pdf_url": "https://arxiv.org/pdf/2511.11757",
        "title": "Bridging the Skills Gap: A Course Model for Modern Generative AI Education",
        "authors": [
            "Anya Bardach",
            "Hamilton Murrah"
        ],
        "comments": "10 pages, 2 figures, in the 40th Annual AAAI Conference on Artificial Intelligence (AAAI-26) EAAI Symposium",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI); Software Engineering (cs.SE)",
        "abstract": "Research on how the popularization of generative Artificial Intelligence (AI) tools impacts learning environments has led to hesitancy among educators to teach these tools in classrooms, creating two observed disconnects. Generative AI competency is increasingly valued in industry but not in higher education, and students are experimenting with generative AI without formal guidance. The authors argue students across fields must be taught to responsibly and expertly harness the potential of AI tools to ensure job market readiness and positive outcomes. Computer Science trajectories are particularly impacted, and while consistently top ranked U.S. Computer Science departments teach the mechanisms and frameworks underlying AI, few appear to offer courses on applications for existing generative AI tools. A course was developed at a private research university to teach undergraduate and graduate Computer Science students applications for generative AI tools in software development. Two mixed method surveys indicated students overwhelmingly found the course valuable and effective. Co-authored by the instructor and one of the graduate students, this paper explores the context, implementation, and impact of the course through data analysis and reflections from both perspectives. It additionally offers recommendations for replication in and beyond Computer Science departments. This is the extended version of this paper to include technical appendices.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 141,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11758",
        "abs_url": "https://arxiv.org/abs/2511.11758",
        "pdf_url": "https://arxiv.org/pdf/2511.11758",
        "title": "Protein Structure Tokenization via Geometric Byte Pair Encoding",
        "authors": [
            "Michael Sun",
            "Weize Yuan",
            "Gang Liu",
            "Wojciech Matusik",
            "Marinka Zitnik"
        ],
        "comments": "",
        "subjects": "Quantitative Methods (q-bio.QM); Artificial Intelligence (cs.AI)",
        "abstract": "Protein structure is central to biological function, and enabling multimodal protein models requires joint reasoning over sequence, structure, and function. A key barrier is the lack of principled protein structure tokenizers (PSTs): existing approaches fix token size or rely on continuous vector codebooks, limiting interpretability, multi-scale control, and transfer across architectures. We introduce GeoBPE, a geometry-grounded PST that transforms continuous, noisy, multi-scale backbone conformations into discrete ``sentences'' of geometry while enforcing global constraints. Analogous to byte-pair encoding, GeoBPE generates a hierarchical vocabulary of geometric primitives by iteratively (i) clustering Geo-Pair occurrences with k-medoids to yield a resolution-controllable vocabulary; (ii) quantizing each Geo-Pair to its closest medoid prototype; and (iii) reducing drift through differentiable inverse kinematics that optimizes boundary glue angles under an $\\mathrm{SE}(3)$ end-frame loss. GeoBPE offers compression ($>$10x reduction in bits-per-residue at similar distortion rate), data efficiency ($>$10x less training data), and generalization (maintains test/train distortion ratio of $1.0-1.1$). It is architecture-agnostic: (a) its hierarchical vocabulary provides a strong inductive bias for coarsening residue-level embeddings from large PLMs into motif- and protein-level representations, consistently outperforming leading PSTs across $12$ tasks and $24$ test splits; (b) paired with a transformer, GeoBPE supports unconditional backbone generation via language modeling; and (c) tokens align with CATH functional families and support expert-interpretable case studies, offering functional meaning absent in prior PSTs. Code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 142,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11759",
        "abs_url": "https://arxiv.org/abs/2511.11759",
        "pdf_url": "https://arxiv.org/pdf/2511.11759",
        "title": "Can AI Models be Jailbroken to Phish Elderly Victims? An End-to-End Evaluation",
        "authors": [
            "Fred Heiding",
            "Simon Lermen"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Computers and Society (cs.CY)",
        "abstract": "We present an end-to-end demonstration of how attackers can exploit AI safety failures to harm vulnerable populations: from jailbreaking LLMs to generate phishing content, to deploying those messages against real targets, to successfully compromising elderly victims. We systematically evaluated safety guardrails across six frontier LLMs spanning four attack categories, revealing critical failures where several models exhibited near-complete susceptibility to certain attack vectors. In a human validation study with 108 senior volunteers, AI-generated phishing emails successfully compromised 11\\% of participants. Our work uniquely demonstrates the complete attack pipeline targeting elderly populations, highlighting that current AI safety measures fail to protect those most vulnerable to fraud. Beyond generating phishing content, LLMs enable attackers to overcome language barriers and conduct multi-turn trust-building conversations at scale, fundamentally transforming fraud economics. While some providers report voluntary counter-abuse efforts, we argue these remain insufficient.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 143,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11764",
        "abs_url": "https://arxiv.org/abs/2511.11764",
        "pdf_url": "https://arxiv.org/pdf/2511.11764",
        "title": "Demystify, Use, Reflect: Preparing students to be informed LLM-users",
        "authors": [
            "Nikitha Donekal Chandrashekar",
            "Sehrish Basir Nizamani",
            "Margaret Ellis",
            "Naren Ramakrishnan"
        ],
        "comments": "2 pages 1 table Submitted to SIGCSE 2026",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI)",
        "abstract": "We transitioned our post-CS1 course that introduces various subfields of computer science so that it integrates Large Language Models (LLMs) in a structured, critical, and practical manner. It aims to help students develop the skills needed to engage meaningfully and responsibly with AI. The course now includes explicit instruction on how LLMs work, exposure to current tools, ethical issues, and activities that encourage student reflection on personal use of LLMs as well as the larger evolving landscape of AI-assisted programming. In class, we demonstrate the use and verification of LLM outputs, guide students in the use of LLMs as an ingredient in a larger problem-solving loop, and require students to disclose and acknowledge the nature and extent of LLM assistance. Throughout the course, we discuss risks and benefits of LLMs across CS subfields. In our first iteration of the course, we collected and analyzed data from students pre and post surveys. Student understanding of how LLMs work became more technical, and their verification and use of LLMs shifted to be more discerning and collaborative. These strategies can be used in other courses to prepare students for the AI-integrated future.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 144,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11772",
        "abs_url": "https://arxiv.org/abs/2511.11772",
        "pdf_url": "https://arxiv.org/pdf/2511.11772",
        "title": "Scaling Equitable Reflection Assessment in Education via Large Language Models and Role-Based Feedback Agents",
        "authors": [
            "Chenyu Zhang",
            "Xiaohang Luo"
        ],
        "comments": "Accepted to AAAI-26 AISI Track",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI)",
        "abstract": "Formative feedback is widely recognized as one of the most effective drivers of student learning, yet it remains difficult to implement equitably at scale. In large or low-resource courses, instructors often lack the time, staffing, and bandwidth required to review and respond to every student reflection, creating gaps in support precisely where learners would benefit most. This paper presents a theory-grounded system that uses five coordinated role-based LLM agents (Evaluator, Equity Monitor, Metacognitive Coach, Aggregator, and Reflexion Reviewer) to score learner reflections with a shared rubric and to generate short, bias-aware, learner-facing comments. The agents first produce structured rubric scores, then check for potentially biased or exclusionary language, add metacognitive prompts that invite students to think about their own thinking, and finally compose a concise feedback message of at most 120 words. The system includes simple fairness checks that compare scoring error across lower and higher scoring learners, enabling instructors to monitor and bound disparities in accuracy. We evaluate the pipeline in a 12-session AI literacy program with adult learners. In this setting, the system produces rubric scores that approach expert-level agreement, and trained graders rate the AI-generated comments as helpful, empathetic, and well aligned with instructional goals. Taken together, these results show that multi-agent LLM systems can deliver equitable, high-quality formative feedback at a scale and speed that would be impossible for human graders alone. More broadly, the work points toward a future where feedback-rich learning becomes feasible for any course size or context, advancing long-standing goals of equity, access, and instructional capacity in education.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 145,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11784",
        "abs_url": "https://arxiv.org/abs/2511.11784",
        "pdf_url": "https://arxiv.org/pdf/2511.11784",
        "title": "NegBLEURT Forest: Leveraging Inconsistencies for Detecting Jailbreak Attacks",
        "authors": [
            "Lama Sleem",
            "Jerome Francois",
            "Lujun Li",
            "Nathan Foucher",
            "Niccolo Gentile",
            "Radu State"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
        "abstract": "Jailbreak attacks designed to bypass safety mechanisms pose a serious threat by prompting LLMs to generate harmful or inappropriate content, despite alignment with ethical guidelines. Crafting universal filtering rules remains difficult due to their inherent dependence on specific contexts. To address these challenges without relying on threshold calibration or model fine-tuning, this work introduces a semantic consistency analysis between successful and unsuccessful responses, demonstrating that a negation-aware scoring approach captures meaningful patterns. Building on this insight, a novel detection framework called NegBLEURT Forest is proposed to evaluate the degree of alignment between outputs elicited by adversarial prompts and expected safe behaviors. It identifies anomalous responses using the Isolation Forest algorithm, enabling reliable jailbreak detection. Experimental results show that the proposed method consistently achieves top-tier performance, ranking first or second in accuracy across diverse models using the crafted dataset, while competing approaches exhibit notable sensitivity to model and data variations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 146,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11788",
        "abs_url": "https://arxiv.org/abs/2511.11788",
        "pdf_url": "https://arxiv.org/pdf/2511.11788",
        "title": "MALBO: Optimizing LLM-Based Multi-Agent Teams via Multi-Objective Bayesian Optimization",
        "authors": [
            "Antonio Sabbatella"
        ],
        "comments": "Master's Thesis, University of Milano-Bicocca, 2025",
        "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI)",
        "abstract": "The optimal assignment of Large Language Models (LLMs) to specialized roles in multi-agent systems is a significant challenge, defined by a vast combinatorial search space, expensive black-box evaluations, and an inherent trade-off between performance and cost. Current optimization methods focus on single-agent settings and lack a principled framework for this multi-agent, multi-objective problem. This thesis introduces MALBO (Multi-Agent LLM Bayesian Optimization), a systematic framework designed to automate the efficient composition of LLM-based agent teams. We formalize the assignment challenge as a multi-objective optimization problem, aiming to identify the Pareto front of configurations between task accuracy and inference cost. The methodology employs multi-objective Bayesian Optimization (MOBO) with independent Gaussian Process surrogate models. By searching over a continuous feature-space representation of the LLMs, this approach performs a sample-efficient exploration guided by the expected hypervolume improvement. The primary contribution is a principled and automated methodology that yields a Pareto front of optimal team configurations. Our results demonstrate that the Bayesian optimization phase, compared to an initial random search, maintained a comparable average performance while reducing the average configuration cost by over 45%. Furthermore, MALBO identified specialized, heterogeneous teams that achieve cost reductions of up to 65.8% compared to homogeneous baselines, all while maintaining maximum performance. The framework thus provides a data-driven tool for deploying cost-effective and highly specialized multi-agent AI systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 147,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11789",
        "abs_url": "https://arxiv.org/abs/2511.11789",
        "pdf_url": "https://arxiv.org/pdf/2511.11789",
        "title": "From Single to Societal: Analyzing Persona-Induced Bias in Multi-Agent Interactions",
        "authors": [
            "Jiayi Li",
            "Xiao Liu",
            "Yansong Feng"
        ],
        "comments": "AAAI-2026",
        "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI)",
        "abstract": "Large Language Model (LLM)-based multi-agent systems are increasingly used to simulate human interactions and solve collaborative tasks. A common practice is to assign agents with personas to encourage behavioral diversity. However, this raises a critical yet underexplored question: do personas introduce biases into multi-agent interactions? This paper presents a systematic investigation into persona-induced biases in multi-agent interactions, with a focus on social traits like trustworthiness (how an agent's opinion is received by others) and insistence (how strongly an agent advocates for its opinion). Through a series of controlled experiments in collaborative problem-solving and persuasion tasks, we reveal that (1) LLM-based agents exhibit biases in both trustworthiness and insistence, with personas from historically advantaged groups (e.g., men and White individuals) perceived as less trustworthy and demonstrating less insistence; and (2) agents exhibit significant in-group favoritism, showing a higher tendency to conform to others who share the same persona. These biases persist across various LLMs, group sizes, and numbers of interaction rounds, highlighting an urgent need for awareness and mitigation to ensure the fairness and reliability of multi-agent systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 148,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11790",
        "abs_url": "https://arxiv.org/abs/2511.11790",
        "pdf_url": "https://arxiv.org/pdf/2511.11790",
        "title": "Differences in the Moral Foundations of Large Language Models",
        "authors": [
            "Peter Kirgis"
        ],
        "comments": "",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI)",
        "abstract": "Large language models are increasingly being used in critical domains of politics, business, and education, but the nature of their normative ethical judgment remains opaque. Alignment research has, to date, not sufficiently utilized perspectives and insights from the field of moral psychology to inform training and evaluation of frontier models. I perform a synthetic experiment on a wide range of models from most major model providers using Jonathan Haidt's influential moral foundations theory (MFT) to elicit diverse value judgments from LLMs. Using multiple descriptive statistical approaches, I document the bias and variance of large language model responses relative to a human baseline in the original survey. My results suggest that models rely on different moral foundations from one another and from a nationally representative human baseline, and these differences increase as model capabilities increase. This work seeks to spur further analysis of LLMs using MFT, including finetuning of open-source models, and greater deliberation by policymakers on the importance of moral foundations for LLM alignment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 149,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11810",
        "abs_url": "https://arxiv.org/abs/2511.11810",
        "pdf_url": "https://arxiv.org/pdf/2511.11810",
        "title": "On the Notion that Language Models Reason",
        "authors": [
            "Bertram Højer"
        ],
        "comments": "Accepted at the 1st Workshop on Epistemic Intelligence in Machine Learning, EurIPS 2025",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Language models (LMs) are said to be exhibiting reasoning, but what does this entail? We assess definitions of reasoning and how key papers in the field of natural language processing (NLP) use the notion and argue that the definitions provided are not consistent with how LMs are trained, process information, and generate new tokens. To illustrate this incommensurability we assume the view that transformer-based LMs implement an \\textit{implicit} finite-order Markov kernel mapping contexts to conditional token distributions. In this view, reasoning-like outputs correspond to statistical regularities and approximate statistical invariances in the learned kernel rather than the implementation of explicit logical mechanisms. This view is illustrative of the claim that LMs are \"statistical pattern matchers\"\" and not genuine reasoners and provides a perspective that clarifies why reasoning-like outputs arise in LMs without any guarantees of logical consistency. This distinction is fundamental to how epistemic uncertainty is evaluated in LMs. We invite a discussion on the importance of how the computational processes of the systems we build and analyze in NLP research are described.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 150,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11821",
        "abs_url": "https://arxiv.org/abs/2511.11821",
        "pdf_url": "https://arxiv.org/pdf/2511.11821",
        "title": "Scaling Open-Weight Large Language Models for Hydropower Regulatory Information Extraction: A Systematic Analysis",
        "authors": [
            "Hong-Jun Yoon",
            "Faisal Ashraf",
            "Thomas A. Ruggles",
            "Debjani Singh"
        ],
        "comments": "18 pages, zero figures, Preprint submitted to Environmental Modeling and Software",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Information extraction from regulatory documents using large language models presents critical trade-offs between performance and computational resources. We evaluated seven open-weight models (0.6B-70B parameters) on hydropower licensing documentation to provide empirical deployment guidance. Our analysis identified a pronounced 14B parameter threshold where validation methods transition from ineffective (F1 $<$ 0.15) to viable (F1 = 0.64). Consumer-deployable models achieve 64\\% F1 through appropriate validation, while smaller models plateau at 51\\%. Large-scale models approach 77\\% F1 but require enterprise infrastructure. We identified systematic hallucination patterns where perfect recall indicates extraction failure rather than success in smaller models. Our findings establish the first comprehensive resource-performance mapping for open-weight information extraction in regulatory contexts, enabling evidence-based model selection. These results provide immediate value for hydropower compliance while contributing insights into parameter scaling effects that generalize across information extraction tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 151,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11825",
        "abs_url": "https://arxiv.org/abs/2511.11825",
        "pdf_url": "https://arxiv.org/pdf/2511.11825",
        "title": "Real-Time Speech Enhancement via a Hybrid ViT: A Dual-Input Acoustic-Image Feature Fusion",
        "authors": [
            "Behnaz Bahmei",
            "Siamak Arzanpour",
            "Elina Birmingham"
        ],
        "comments": "",
        "subjects": "Sound (cs.SD); Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)",
        "abstract": "Speech quality and intelligibility are significantly degraded in noisy environments. This paper presents a novel transformer-based learning framework to address the single-channel noise suppression problem for real-time applications. Although existing deep learning networks have shown remarkable improvements in handling stationary noise, their performance often diminishes in real-world environments characterized by non-stationary noise (e.g., dog barking, baby crying). The proposed dual-input acoustic-image feature fusion using a hybrid ViT framework effectively models both temporal and spectral dependencies in noisy signals. Designed for real-world audio environments, the proposed framework is computationally lightweight and suitable for implementation on embedded devices. To evaluate its effectiveness, four standard and commonly used quality measurements, namely PESQ, STOI, Seg SNR, and LLR, are utilized. Experimental results obtained using the Librispeech dataset as the clean speech source and the UrbanSound8K and Google Audioset datasets as the noise sources, demonstrate that the proposed method significantly improves noise reduction, speech intelligibility, and perceptual quality compared to the noisy input signal, achieving performance close to the clean reference.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 152,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11828",
        "abs_url": "https://arxiv.org/abs/2511.11828",
        "pdf_url": "https://arxiv.org/pdf/2511.11828",
        "title": "Conformal Constrained Policy Optimization for Cost-Effective LLM Agents",
        "authors": [
            "Wenwen Si",
            "Sooyong Jang",
            "Insup Lee",
            "Osbert Bastani"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "While large language models (LLMs) have recently made tremendous progress towards solving challenging AI problems, they have done so at increasingly steep computational and API costs. We propose a novel strategy where we combine multiple LLM models with varying cost/accuracy tradeoffs in an agentic manner, where models and tools are run in sequence as determined by an orchestration model to minimize cost subject to a user-specified level of reliability; this constraint is formalized using conformal prediction to provide guarantees. To solve this problem, we propose Conformal Constrained Policy Optimization (CCPO), a training paradigm that integrates constrained policy optimization with off-policy reinforcement learning and recent advances in online conformal prediction. CCPO jointly optimizes a cost-aware policy (score function) and an adaptive threshold. Across two multi-hop question answering benchmarks, CCPO achieves up to a 30% cost reduction compared to other cost-aware baselines and LLM-guided methods without compromising reliability. Our approach provides a principled and practical framework for deploying LLM agents that are significantly more cost-effective while maintaining reliability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 153,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11829",
        "abs_url": "https://arxiv.org/abs/2511.11829",
        "pdf_url": "https://arxiv.org/pdf/2511.11829",
        "title": "Towards Autoformalization of LLM-generated Outputs for Requirement Verification",
        "authors": [
            "Mihir Gupte",
            "Ramesh S"
        ],
        "comments": "To be submitted for publication",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Formal Languages and Automata Theory (cs.FL); Logic in Computer Science (cs.LO)",
        "abstract": "Autoformalization, the process of translating informal statements into formal logic, has gained renewed interest with the emergence of powerful Large Language Models (LLMs). While LLMs show promise in generating structured outputs from natural language (NL), such as Gherkin Scenarios from NL feature requirements, there's currently no formal method to verify if these outputs are accurate. This paper takes a preliminary step toward addressing this gap by exploring the use of a simple LLM-based autoformalizer to verify LLM-generated outputs against a small set of natural language requirements. We conducted two distinct experiments. In the first one, the autoformalizer successfully identified that two differently-worded NL requirements were logically equivalent, demonstrating the pipeline's potential for consistency checks. In the second, the autoformalizer was used to identify a logical inconsistency between a given NL requirement and an LLM-generated output, highlighting its utility as a formal verification tool. Our findings, while limited, suggest that autoformalization holds significant potential for ensuring the fidelity and logical consistency of LLM-generated outputs, laying a crucial foundation for future, more extensive studies into this novel application.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 154,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11834",
        "abs_url": "https://arxiv.org/abs/2511.11834",
        "pdf_url": "https://arxiv.org/pdf/2511.11834",
        "title": "Volatility in Certainty (VC): A Metric for Detecting Adversarial Perturbations During Inference in Neural Network Classifiers",
        "authors": [
            "Vahid Hemmati",
            "Ahmad Mohammadi",
            "Abdul-Rauf Nuhu",
            "Reza Ahmari",
            "Parham Kebria",
            "Abdollah Homaifar"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Adversarial robustness remains a critical challenge in deploying neural network classifiers, particularly in real-time systems where ground-truth labels are unavailable during inference. This paper investigates \\textit{Volatility in Certainty} (VC), a recently proposed, label-free metric that quantifies irregularities in model confidence by measuring the dispersion of sorted softmax outputs. Specifically, VC is defined as the average squared log-ratio of adjacent certainty values, capturing local fluctuations in model output smoothness. We evaluate VC as a proxy for classification accuracy and as an indicator of adversarial drift. Experiments are conducted on artificial neural networks (ANNs) and convolutional neural networks (CNNs) trained on MNIST, as well as a regularized VGG-like model trained on CIFAR-10. Adversarial examples are generated using the Fast Gradient Sign Method (FGSM) across varying perturbation magnitudes. In addition, mixed test sets are created by gradually introducing adversarial contamination to assess VC's sensitivity under incremental distribution shifts. Our results reveal a strong negative correlation between classification accuracy and log(VC) (correlation rho < -0.90 in most cases), suggesting that VC effectively reflects performance degradation without requiring labeled data. These findings position VC as a scalable, architecture-agnostic, and real-time performance metric suitable for early-warning systems in safety-critical applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 155,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11836",
        "abs_url": "https://arxiv.org/abs/2511.11836",
        "pdf_url": "https://arxiv.org/pdf/2511.11836",
        "title": "Securing Generative AI in Healthcare: A Zero-Trust Architecture Powered by Confidential Computing on Google Cloud",
        "authors": [
            "Adaobi Amanna",
            "Ishana Shinde"
        ],
        "comments": "19 Pages, 1 Figure, 1 Table",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
        "abstract": "The integration of Generative Artificial Intelligence (GenAI) in healthcare is impeded by significant security challenges unaddressed by traditional frameworks, precisely the data-in-use gap where sensitive patient data and proprietary AI models are exposed during active processing. To address this, the paper proposes the Confidential Zero-Trust Framework (CZF), a novel security paradigm that synergistically combines Zero-Trust Architecture for granular access control with the hardware-enforced data isolation of Confidential Computing. We detailed a multi-tiered architectural blueprint for implementing the CZF on Google Cloud and analyzed its efficacy against real-world threats. The CZF provides a defense-in-depth architecture where data remains encrypted while in-use within a hardware-based Trusted Execution Environment (TEE). The framework's use of remote attestation offers cryptographic proof of workload integrity, transforming compliance from a procedural exercise into a verifiable technical fact and enabling secure, multi-party collaborations previously blocked by security and intellectual property concerns. By closing the data-in-use gap and enforcing Zero-Trust principles, the CZF provides a robust and verifiable framework that establishes the necessary foundation of trust to enable the responsible adoption of transformative AI technologies in healthcare.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 156,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11845",
        "abs_url": "https://arxiv.org/abs/2511.11845",
        "pdf_url": "https://arxiv.org/pdf/2511.11845",
        "title": "Autonomous Underwater Cognitive System for Adaptive Navigation: A SLAM-Integrated Cognitive Architecture",
        "authors": [
            "K. A. I. N Jayarathne",
            "R. M. N. M. Rathnayaka",
            "D. P. S. S. Peiris"
        ],
        "comments": "6 pages, 2 figures",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Hardware Architecture (cs.AR)",
        "abstract": "Deep-sea exploration poses significant challenges, including disorientation, communication loss, and navigational failures in dynamic underwater environments. This paper presents an Autonomous Underwater Cognitive System (AUCS) that integrates Simultaneous Localization and Mapping (SLAM) with a Soar-based cognitive architecture to enable adaptive navigation in complex oceanic conditions. The system fuses multi-sensor data from SONAR, LiDAR, IMU, and DVL with cognitive reasoning modules for perception, attention, planning, and learning. Unlike conventional SLAM systems, AUCS incorporates semantic understanding, adaptive sensor management, and memory-based learning to differentiate between dynamic and static objects, reducing false loop closures and enhancing long-term map consistency. The proposed architecture demonstrates a complete perception-cognition-action-learning loop, allowing autonomous underwater vehicles to sense, reason, and adapt intelligently. This work lays a foundation for next-generation cognitive submersible systems, improving safety, reliability, and autonomy in deep-sea exploration.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 157,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11847",
        "abs_url": "https://arxiv.org/abs/2511.11847",
        "pdf_url": "https://arxiv.org/pdf/2511.11847",
        "title": "A Multimodal Manufacturing Safety Chatbot: Knowledge Base Design, Benchmark Development, and Evaluation of Multiple RAG Approaches",
        "authors": [
            "Ryan Singh",
            "Austin Hamilton",
            "Amanda White",
            "Michael Wise",
            "Ibrahim Yousif",
            "Arthur Carvalho",
            "Zhe Shan",
            "Reza Abrisham Baf",
            "Mohammad Mayyas",
            "Lora A. Cavuoto",
            "Fadel M. Megahed"
        ],
        "comments": "25 pages, 5 figures",
        "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Computers and Society (cs.CY)",
        "abstract": "Ensuring worker safety remains a critical challenge in modern manufacturing environments. Industry 5.0 reorients the prevailing manufacturing paradigm toward more human-centric operations. Using a design science research methodology, we identify three essential requirements for next-generation safety training systems: high accuracy, low latency, and low cost. We introduce a multimodal chatbot powered by large language models that meets these design requirements. The chatbot uses retrieval-augmented generation to ground its responses in curated regulatory and technical documentation. To evaluate our solution, we developed a domain-specific benchmark of expert-validated question and answer pairs for three representative machines: a Bridgeport manual mill, a Haas TL-1 CNC lathe, and a Universal Robots UR5e collaborative robot. We tested 24 RAG configurations using a full-factorial design and assessed them with automated evaluations of correctness, latency, and cost. Our top 2 configurations were then evaluated by ten industry experts and academic researchers. Our results show that retrieval strategy and model configuration have a significant impact on performance. The top configuration (selected for chatbot deployment) achieved an accuracy of 86.66%, an average latency of 10.04 seconds, and an average cost of $0.005 per query. Overall, our work provides three contributions: an open-source, domain-grounded safety training chatbot; a validated benchmark for evaluating AI-assisted safety instruction; and a systematic methodology for designing and assessing AI-enabled instructional and immersive safety training systems for Industry 5.0 environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 158,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11857",
        "abs_url": "https://arxiv.org/abs/2511.11857",
        "pdf_url": "https://arxiv.org/pdf/2511.11857",
        "title": "Three Stage Narrative Analysis; Plot-Sentiment Breakdown, Structure Learning and Concept Detection",
        "authors": [
            "Taimur Khan",
            "Ramoza Ahsan",
            "Mohib Hameed"
        ],
        "comments": "18 pages",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Story understanding and analysis have long been challenging areas within Natural Language Understanding. Automated narrative analysis requires deep computational semantic representations along with syntactic processing. Moreover, the large volume of narrative data demands automated semantic analysis and computational learning rather than manual analytical approaches. In this paper, we propose a framework that analyzes the sentiment arcs of movie scripts and performs extended analysis related to the context of the characters involved. The framework enables the extraction of high-level and low-level concepts conveyed through the narrative. Using dictionary-based sentiment analysis, our approach applies a custom lexicon built with the LabMTsimple storylab module. The custom lexicon is based on the Valence, Arousal, and Dominance scores from the NRC-VAD dataset. Furthermore, the framework advances the analysis by clustering similar sentiment plots using Wards hierarchical clustering technique. Experimental evaluation on a movie dataset shows that the resulting analysis is helpful to consumers and readers when selecting a narrative or story.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 159,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11881",
        "abs_url": "https://arxiv.org/abs/2511.11881",
        "pdf_url": "https://arxiv.org/pdf/2511.11881",
        "title": "Better LLM Reasoning via Dual-Play",
        "authors": [
            "Zhengxin Zhang",
            "Chengyu Huang",
            "Aochong Oliver Li",
            "Claire Cardie"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Large Language Models (LLMs) have achieved remarkable progress through Reinforcement Learning with Verifiable Rewards (RLVR), yet still rely heavily on external supervision (e.g., curated labels). Adversarial learning, particularly through self-play, offers a promising alternative that enables models to iteratively learn from themselves - thus reducing reliance on external supervision. Dual-play extends adversarial learning by assigning specialized roles to two models and training them against each other, fostering sustained competition and mutual evolution. Despite its promise, adapting dual-play training to LLMs remains limited, largely due to their susceptibility to reward hacking and training instability. In this paper, we introduce PasoDoble, a novel LLM dual-play framework. PasoDoble adversarially trains two models initialized from the same base model: a Proposer, which generates challenging questions with ground-truth answers, and a Solver, which attempts to solve them. We enrich the Proposer with knowledge from a pre-training dataset to ensure the questions' quality and diversity. To avoid reward hacking, the Proposer is rewarded for producing only valid questions that push the Solver's limit, while the Solver is rewarded for solving them correctly, and both are updated jointly. To further enhance training stability, we introduce an optional offline paradigm that decouples Proposer and Solver updates, alternately updating each for several steps while holding the other fixed. Notably, PasoDoble operates without supervision during training. Experimental results show that PasoDoble can improve the reasoning performance of LLMs. Our project page is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 160,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11885",
        "abs_url": "https://arxiv.org/abs/2511.11885",
        "pdf_url": "https://arxiv.org/pdf/2511.11885",
        "title": "Flash-Fusion: Enabling Expressive, Low-Latency Queries on IoT Sensor Streams with LLMs",
        "authors": [
            "Kausar Patherya",
            "Ashutosh Dhekne",
            "Francisco Romero"
        ],
        "comments": "12 pages, 5 figures. Under review",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Artificial Intelligence (cs.AI); Databases (cs.DB)",
        "abstract": "Smart cities and pervasive IoT deployments have generated interest in IoT data analysis across transportation and urban planning. At the same time, Large Language Models offer a new interface for exploring IoT data - particularly through natural language. Users today face two key challenges when working with IoT data using LLMs: (1) data collection infrastructure is expensive, producing terabytes of low-level sensor readings that are too granular for direct use, and (2) data analysis is slow, requiring iterative effort and technical expertise. Directly feeding all IoT telemetry to LLMs is impractical due to finite context windows, prohibitive token costs at scale, and non-interactive latencies. What is missing is a system that first parses a user's query to identify the analytical task, then selects the relevant data slices, and finally chooses the right representation before invoking an LLM. We present Flash-Fusion, an end-to-end edge-cloud system that reduces the IoT data collection and analysis burden on users. Two principles guide its design: (1) edge-based statistical summarization (achieving 73.5% data reduction) to address data volume, and (2) cloud-based query planning that clusters behavioral data and assembles context-rich prompts to address data interpretation. We deploy Flash-Fusion on a university bus fleet and evaluate it against a baseline that feeds raw data to a state-of-the-art LLM. Flash-Fusion achieves a 95% latency reduction and 98% decrease in token usage and cost while maintaining high-quality responses. It enables personas across disciplines - safety officers, urban planners, fleet managers, and data scientists - to efficiently iterate over IoT data without the burden of manual query authoring or preprocessing.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 161,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11891",
        "abs_url": "https://arxiv.org/abs/2511.11891",
        "pdf_url": "https://arxiv.org/pdf/2511.11891",
        "title": "FLEX: Feature Importance from Layered Counterfactual Explanations",
        "authors": [
            "Nawid Keshtmand",
            "Roussel Desmond Nzoyem",
            "Jeffrey Nicholas Clark"
        ],
        "comments": "12 pages, 6 figures, 3 tables, 2 algorithms. Preprint under review",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Machine learning models achieve state-of-the-art performance across domains, yet their lack of interpretability limits safe deployment in high-stakes settings. Counterfactual explanations are widely used to provide actionable \"what-if\" recourse, but they typically remain instance-specific and do not quantify which features systematically drive outcome changes within coherent regions of the feature space or across an entire dataset. We introduce FLEX (Feature importance from Layered counterfactual EXplanations), a model- and domain-agnostic framework that converts sets of counterfactuals into feature change frequency scores at local, regional, and global levels. FLEX generalises local change-frequency measures by aggregating across instances and neighbourhoods, offering interpretable rankings that reflect how often each feature must change to flip predictions. The framework is compatible with different counterfactual generation methods, allowing users to emphasise characteristics such as sparsity, feasibility, or actionability, thereby tailoring the derived feature importances to practical constraints. We evaluate FLEX on two contrasting tabular tasks: traffic accident severity prediction and loan approval, and compare FLEX to SHAP- and LIME-derived feature importance values. Results show that (i) FLEX's global rankings correlate with SHAP while surfacing additional drivers, and (ii) regional analyses reveal context-specific factors that global summaries miss. FLEX thus bridges the gap between local recourse and global attribution, supporting transparent and intervention-oriented decision-making in risk-sensitive applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 162,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11894",
        "abs_url": "https://arxiv.org/abs/2511.11894",
        "pdf_url": "https://arxiv.org/pdf/2511.11894",
        "title": "Chain-of-Generation: Progressive Latent Diffusion for Text-Guided Molecular Design",
        "authors": [
            "Lingxiao Li",
            "Haobo Zhang",
            "Bin Chen",
            "Jiayu Zhou"
        ],
        "comments": "22 pages, 7 figures, 10 tables",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Text-conditioned molecular generation aims to translate natural-language descriptions into chemical structures, enabling scientists to specify functional groups, scaffolds, and physicochemical constraints without handcrafted rules. Diffusion-based models, particularly latent diffusion models (LDMs), have recently shown promise by performing stochastic search in a continuous latent space that compactly captures molecular semantics. Yet existing methods rely on one-shot conditioning, where the entire prompt is encoded once and applied throughout diffusion, making it hard to satisfy all the requirements in the prompt. We discuss three outstanding challenges of one-shot conditioning generation, including the poor interpretability of the generated components, the failure to generate all substructures, and the overambition in considering all requirements simultaneously. We then propose three principles to address those challenges, motivated by which we propose Chain-of-Generation (CoG), a training-free multi-stage latent diffusion framework. CoG decomposes each prompt into curriculum-ordered semantic segments and progressively incorporates them as intermediate goals, guiding the denoising trajectory toward molecules that satisfy increasingly rich linguistic constraints. To reinforce semantic guidance, we further introduce a post-alignment learning phase that strengthens the correspondence between textual and molecular latent spaces. Extensive experiments on benchmark and real-world tasks demonstrate that CoG yields higher semantic alignment, diversity, and controllability than one-shot baselines, producing molecules that more faithfully reflect complex, compositional prompts while offering transparent insight into the generation process.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 163,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11896",
        "abs_url": "https://arxiv.org/abs/2511.11896",
        "pdf_url": "https://arxiv.org/pdf/2511.11896",
        "title": "VULPO: Context-Aware Vulnerability Detection via On-Policy LLM Optimization",
        "authors": [
            "Youpeng Li",
            "Fuxun Yu",
            "Xinda Wang"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Software Engineering (cs.SE)",
        "abstract": "The widespread reliance on open-source software dramatically increases the risk of vulnerability exploitation, underscoring the need for effective and scalable vulnerability detection (VD). Existing VD techniques, whether traditional machine learning-based or LLM-based approaches like prompt engineering, supervised fine-tuning, or off-policy preference optimization, remain fundamentally limited in their ability to perform context-aware analysis: They depend on fixed inputs or static preference datasets, cannot adaptively explore repository-level dependencies, and are constrained by function-level benchmarks that overlook critical vulnerability context. This paper introduces Vulnerability-Adaptive Policy Optimization (VULPO), an on-policy LLM reinforcement learning framework for context-aware VD. To support training and evaluation, we first construct ContextVul, a new dataset that augments high-quality function-level samples with lightweight method to extract repository-level context information. We then design multi-dimensional reward structuring that jointly captures prediction correctness, vulnerability localization accuracy, and the semantic relevance of vulnerability analysis, thereby guiding the model toward comprehensive contextual reasoning. To address the asymmetric difficulty of different vulnerability cases and mitigate reward hacking, VULPO incorporates label-level and sample-level difficulty-adaptive reward scaling, encouraging the model to explore challenging cases while maintaining balanced reward distribution. Extensive experiments demonstrate the superiority of our VULPO framework in context-aware VD: Our VULPO-4B substantially outperforms existing VD baselines based on prompt engineering and off-policy optimization, improving F1 by 85% over Qwen3-4B and achieving performance comparable to a 150x larger-scale model, DeepSeek-R1-0528.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 164,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11902",
        "abs_url": "https://arxiv.org/abs/2511.11902",
        "pdf_url": "https://arxiv.org/pdf/2511.11902",
        "title": "Robust Bidirectional Associative Memory via Regularization Inspired by the Subspace Rotation Algorithm",
        "authors": [
            "Ci Lin",
            "Tet Yeap",
            "Iluju Kiringa",
            "Biwei Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Bidirectional Associative Memory (BAM) trained with Bidirectional Backpropagation (B-BP) often suffers from poor robustness and high sensitivity to noise and adversarial attacks. To address these issues, we propose a novel gradient-free training algorithm, the Bidirectional Subspace Rotation Algorithm (B-SRA), which significantly improves the robustness and convergence behavior of BAM. Through comprehensive experiments, we identify two key principles -- orthogonal weight matrices (OWM) and gradient-pattern alignment (GPA) -- as central to enhancing the robustness of BAM. Motivated by these findings, we introduce new regularization strategies into B-BP, resulting in models with greatly improved resistance to corruption and adversarial perturbations. We further conduct an ablation study across different training strategies to determine the most robust configuration and evaluate BAM's performance under a variety of attack scenarios and memory capacities, including 50, 100, and 200 associative pairs. Among all methods, the SAME configuration, which integrates both OWM and GPA, achieves the strongest resilience. Overall, our results demonstrate that B-SRA and the proposed regularization strategies lead to substantially more robust associative memories and open new directions for building resilient neural architectures.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 165,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11907",
        "abs_url": "https://arxiv.org/abs/2511.11907",
        "pdf_url": "https://arxiv.org/pdf/2511.11907",
        "title": "KVSwap: Disk-aware KV Cache Offloading for Long-Context On-device Inference",
        "authors": [
            "Huawei Zhang",
            "Chunwei Xia",
            "Zheng Wang"
        ],
        "comments": "",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Artificial Intelligence (cs.AI)",
        "abstract": "Language models (LMs) underpin emerging mobile and embedded AI applications like meeting and video summarization and document analysis, which often require processing multiple long-context inputs. Running an LM locally on-device improves privacy, enables offline use, and reduces cost, but long-context inference quickly hits a \\emph{memory capacity wall} as the key-value (KV) cache grows linearly with context length and batch size. We present KVSwap, a software framework to break this memory wall by offloading the KV cache to non-volatile secondary storage (disk). KVSwap leverages the observation that only a small, dynamically changing subset of KV entries is critical for generation. It stores the full cache on disk, uses a compact in-memory metadata to predict which entries to preload, overlaps computation with hardware-aware disk access, and orchestrates read patterns to match storage device characteristics. Our evaluation shows that across representative LMs and storage types, KVSwap delivers higher throughput under tight memory budgets while maintaining the generation quality when compared with existing KV cache offloading schemes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 166,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11918",
        "abs_url": "https://arxiv.org/abs/2511.11918",
        "pdf_url": "https://arxiv.org/pdf/2511.11918",
        "title": "Batch Matrix-form Equations and Implementation of Multilayer Perceptrons",
        "authors": [
            "Wieger Wesselink",
            "Bram Grooten",
            "Huub van de Wetering",
            "Qiao Xiao",
            "Decebal Constantin Mocanu"
        ],
        "comments": "32 pages; submitted to JMLR",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Multilayer perceptrons (MLPs) remain fundamental to modern deep learning, yet their algorithmic details are rarely presented in complete, explicit \\emph{batch matrix-form}. Rather, most references express gradients per sample or rely on automatic differentiation. Although automatic differentiation can achieve equally high computational efficiency, the usage of batch matrix-form makes the computational structure explicit, which is essential for transparent, systematic analysis, and optimization in settings such as sparse neural networks. This paper fills that gap by providing a mathematically rigorous and implementation-ready specification of MLPs in batch matrix-form. We derive forward and backward equations for all standard and advanced layers, including batch normalization and softmax, and validate all equations using the symbolic mathematics library SymPy. From these specifications, we construct uniform reference implementations in NumPy, PyTorch, JAX, TensorFlow, and a high-performance C++ backend optimized for sparse operations. Our main contributions are: (1) a complete derivation of batch matrix-form backpropagation for MLPs, (2) symbolic validation of all gradient equations, (3) uniform Python and C++ reference implementations grounded in a small set of matrix primitives, and (4) demonstration of how explicit formulations enable efficient sparse computation. Together, these results establish a validated, extensible foundation for understanding, teaching, and researching neural network algorithms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 167,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11938",
        "abs_url": "https://arxiv.org/abs/2511.11938",
        "pdf_url": "https://arxiv.org/pdf/2511.11938",
        "title": "Improving Neutrino Oscillation Measurements through Event Classification",
        "authors": [
            "Sebastian A. R. Ellis",
            "Daniel C. Hackett",
            "Shirley Weishi Li",
            "Pedro A. N. Machado",
            "Karla Tame-Narvaez"
        ],
        "comments": "11 pages, 7 figures",
        "subjects": "High Energy Physics - Phenomenology (hep-ph); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); High Energy Physics - Experiment (hep-ex)",
        "abstract": "Precise neutrino energy reconstruction is essential for next-generation long-baseline oscillation experiments, yet current methods remain limited by large uncertainties in neutrino-nucleus interaction modeling. Even so, it is well established that different interaction channels produce systematically varying amounts of missing energy and therefore yield different reconstruction performance--information that standard calorimetric approaches do not exploit. We introduce a strategy that incorporates this structure by classifying events according to their underlying interaction type prior to energy reconstruction. Using supervised machine-learning techniques trained on labeled generator events, we leverage intrinsic kinematic differences among quasi-elastic scattering, meson-exchange current, resonance production, and deep-inelastic scattering processes. A cross-generator testing framework demonstrates that this classification approach is robust to microphysics mismodeling and, when applied to a simulated DUNE $\\nu_\\mu$ disappearance analysis, yields improved accuracy and sensitivity. These results highlight a practical path toward reducing reconstruction-driven systematics in future oscillation measurements.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 168,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11947",
        "abs_url": "https://arxiv.org/abs/2511.11947",
        "pdf_url": "https://arxiv.org/pdf/2511.11947",
        "title": "AI-Open-RAN for Non-Terrestrial Networks",
        "authors": [
            "Tri Nhu Do"
        ],
        "comments": "",
        "subjects": "Signal Processing (eess.SP); Artificial Intelligence (cs.AI)",
        "abstract": "In this paper, we propose the concept of AIO-RAN-NTN, a unified all-in-one Radio Access Network (RAN) for Non-Terrestrial Networks (NTNs), built on an open architecture that leverages open interfaces and artificial intelligence (AI)-based functionalities. This approach advances interoperability, flexibility, and intelligence in next-generation telecommunications. First, we provide a concise overview of the state-of-the-art architectures for Open-RAN and AI-RAN, highlighting key network functions and infrastructure elements. Next, we introduce our integrated AIO-RAN-NTN blueprint, emphasizing how internal and air interfaces from AIO-RAN and the 3rd Generation Partnership Project (3GPP) can be applied to emerging environments such as NTNs. To examine the impact of mobility on AIO-RAN, we implement a testbed transmission using the OpenAirInterface platform for a standalone (SA) New Radio (NR) 5G system. We then train an AI model on realistic data to forecast key performance indicators (KPIs). Our experiments demonstrate that the AIO-based SA architecture is sensitive to mobility, even at low speeds, but this limitation can be mitigated through AI-driven KPI forecasting.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 169,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11951",
        "abs_url": "https://arxiv.org/abs/2511.11951",
        "pdf_url": "https://arxiv.org/pdf/2511.11951",
        "title": "Temporal Micro-Doppler Spectrogram-based ViT Multiclass Target Classification",
        "authors": [
            "Nghia Thinh Nguyen",
            "Tri Nhu Do"
        ],
        "comments": "",
        "subjects": "Signal Processing (eess.SP); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "In this paper, we propose a new Temporal MDS-Vision Transformer (T-MDS-ViT) for multiclass target classification using millimeter-wave FMCW radar micro-Doppler spectrograms. Specifically, we design a transformer-based architecture that processes stacked range-velocity-angle (RVA) spatiotemporal tensors via patch embeddings and cross-axis attention mechanisms to explicitly model the sequential nature of MDS data across multiple frames. The T-MDS-ViT exploits mobility-aware constraints in its attention layer correspondences to maintain separability under target overlaps and partial occlusions. Next, we apply an explainable mechanism to examine how the attention layers focus on characteristic high-energy regions of the MDS representations and their effect on class-specific kinematic features. We also demonstrate that our proposed framework is superior to existing CNN-based methods in terms of classification accuracy while achieving better data efficiency and real-time deployability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 170,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11966",
        "abs_url": "https://arxiv.org/abs/2511.11966",
        "pdf_url": "https://arxiv.org/pdf/2511.11966",
        "title": "On the Entropy Calibration of Language Models",
        "authors": [
            "Steven Cao",
            "Gregory Valiant",
            "Percy Liang"
        ],
        "comments": "Neurips 2025",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "We study the problem of entropy calibration, which asks whether a language model's entropy over generations matches its log loss on human text. Past work found that models are miscalibrated, with entropy per step increasing (and text quality decreasing) as generations grow longer. This error accumulation is a fundamental problem in autoregressive models, and the standard solution is to truncate the distribution, which improves text quality at the cost of diversity. In this paper, we ask: is miscalibration likely to improve with scale, and is it theoretically possible to calibrate without tradeoffs? To build intuition, we first study a simplified theoretical setting to characterize the scaling behavior of miscalibration with respect to dataset size. We find that the scaling behavior depends on the power law exponent of the data distribution -- in particular, for a power law exponent close to 1, the scaling exponent is close to 0, meaning that miscalibration improves very slowly with scale. Next, we measure miscalibration empirically in language models ranging from 0.5B to 70B parameters. We find that the observed scaling behavior is similar to what is predicted by the simplified setting: our fitted scaling exponents for text are close to 0, meaning that larger models accumulate error at a similar rate as smaller ones. This scaling (or, lack thereof) provides one explanation for why we sample from larger models with similar amounts of truncation as smaller models, even though the larger models are of higher quality. However, truncation is not a satisfying solution because it comes at the cost of increased log loss. In theory, is it even possible to reduce entropy while preserving log loss? We prove that it is possible, if we assume access to a black box which can fit models to predict the future entropy of text.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 171,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.11992",
        "abs_url": "https://arxiv.org/abs/2511.11992",
        "pdf_url": "https://arxiv.org/pdf/2511.11992",
        "title": "Goal-Oriented Multi-Agent Reinforcement Learning for Decentralized Agent Teams",
        "authors": [
            "Hung Du",
            "Hy Nguyen",
            "Srikanth Thudumu",
            "Rajesh Vasa",
            "Kon Mouzakis"
        ],
        "comments": "Accepted poster at the IEEE Consumer Communications & Networking Conference (CCNC) 2026",
        "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Connected and autonomous vehicles across land, water, and air must often operate in dynamic, unpredictable environments with limited communication, no centralized control, and partial observability. These real-world constraints pose significant challenges for coordination, particularly when vehicles pursue individual objectives. To address this, we propose a decentralized Multi-Agent Reinforcement Learning (MARL) framework that enables vehicles, acting as agents, to communicate selectively based on local goals and observations. This goal-aware communication strategy allows agents to share only relevant information, enhancing collaboration while respecting visibility limitations. We validate our approach in complex multi-agent navigation tasks featuring obstacles and dynamic agent populations. Results show that our method significantly improves task success rates and reduces time-to-goal compared to non-cooperative baselines. Moreover, task performance remains stable as the number of agents increases, demonstrating scalability. These findings highlight the potential of decentralized, goal-driven MARL to support effective coordination in realistic multi-vehicle systems operating across diverse domains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 172,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12027",
        "abs_url": "https://arxiv.org/abs/2511.12027",
        "pdf_url": "https://arxiv.org/pdf/2511.12027",
        "title": "GCAgent: Long-Video Understanding via Schematic and Narrative Episodic Memory",
        "authors": [
            "Jeong Hun Yeo",
            "Sangyun Chung",
            "Sungjune Park",
            "Dae Hoe Kim",
            "Jinyoung Moon",
            "Yong Man Ro"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Long-video understanding remains a significant challenge for Multimodal Large Language Models (MLLMs) due to inherent token limitations and the complexity of capturing long-term temporal dependencies. Existing methods often fail to capture the global context and complex event relationships necessary for deep video reasoning. To address this, we introduce GCAgent, a novel Global-Context-Aware Agent framework that achieves comprehensive long-video understanding. Our core innovation is the Schematic and Narrative Episodic Memory. This memory structurally models events and their causal and temporal relations into a concise, organized context, fundamentally resolving the long-term dependency problem. Operating in a multi-stage Perception-Action-Reflection cycle, our GCAgent utilizes a Memory Manager to retrieve relevant episodic context for robust, context-aware inference. Extensive experiments confirm that GCAgent significantly enhances long-video understanding, achieving up to 23.5\\% accuracy improvement on the Video-MME Long split over a strong MLLM baseline. Furthermore, our framework establishes state-of-the-art performance among comparable 7B-scale MLLMs, achieving 73.4\\% accuracy on the Long split and the highest overall average (71.9\\%) on the Video-MME benchmark, validating our agent-based reasoning paradigm and structured memory for cognitively-inspired long-video understanding.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 173,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12031",
        "abs_url": "https://arxiv.org/abs/2511.12031",
        "pdf_url": "https://arxiv.org/pdf/2511.12031",
        "title": "Striking the Right Balance between Compute and Copy: Improving LLM Inferencing Under Speculative Decoding",
        "authors": [
            "Arun Ramachandran",
            "Ramaswamy Govindarajan",
            "Murali Annavaram",
            "Prakash Raghavendra",
            "Hossein Entezari Zarch",
            "Lei Gao",
            "Chaoyi Jiang"
        ],
        "comments": "",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Artificial Intelligence (cs.AI)",
        "abstract": "With the skyrocketing costs of GPUs and their virtual instances in the cloud, there is a significant desire to use CPUs for large language model (LLM) inference. KV cache update, often implemented as allocation, copying, and in-place strided update for each generated token, incurs significant overhead. As the sequence length increases, the allocation and copy overheads dominate the performance. Alternate approaches may allocate large KV tensors upfront to enable in-place updates, but these matrices (with zero-padded rows) cause redundant computations. In this work, we propose a new KV cache allocation mechanism called Balancing Memory and Compute (BMC). BMC allocates, once every r iterations, KV tensors with r redundant rows, allowing in-place update without copy overhead for those iterations, but at the expense of a small amount of redundant computation. Second, we make an interesting observation that the extra rows allocated in the KV tensors and the resulting redundant computation can be repurposed for Speculative Decoding (SD) that improves token generation efficiency. Last, BMC represents a spectrum of design points with different values of r. To identify the best-performing design point(s), we derive a simple analytical model for BMC. The proposed BMC method achieves an average throughput acceleration of up to 3.2x over baseline HuggingFace (without SD). Importantly when we apply BMC with SD, it results in an additional speedup of up to 1.39x, over and above the speedup offered by SD. Further, BMC achieves a throughput acceleration of up to 1.36x and 2.29x over state-of-the-art inference servers vLLM and DeepSpeed, respectively. Although the BMC technique is evaluated extensively across different classes of CPUs (desktop and server class), we also evaluate the scheme with GPUs and demonstrate that it works well for GPUs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 174,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12033",
        "abs_url": "https://arxiv.org/abs/2511.12033",
        "pdf_url": "https://arxiv.org/pdf/2511.12033",
        "title": "EARL: Entropy-Aware RL Alignment of LLMs for Reliable RTL Code Generation",
        "authors": [
            "Jiahe Shi",
            "Zhengqi Gao",
            "Ching-Yun Ko",
            "Duane Boning"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Recent advances in large language models (LLMs) have demonstrated significant potential in hardware design automation, particularly in using natural language to synthesize Register-Transfer Level (RTL) code. Despite this progress, a gap remains between model capability and the demands of real-world RTL design, including syntax errors, functional hallucinations, and weak alignment to designer intent. Reinforcement Learning with Verifiable Rewards (RLVR) offers a promising approach to bridge this gap, as hardware provides executable and formally checkable signals that can be used to further align model outputs with design intent. However, in long, structured RTL code sequences, not all tokens contribute equally to functional correctness, and naïvely spreading gradients across all tokens dilutes learning signals. A key insight from our entropy analysis in RTL generation is that only a small fraction of tokens (e.g., always, if, assign, posedge) exhibit high uncertainty and largely influence control flow and module structure. To address these challenges, we present EARL, an Entropy-Aware Reinforcement Learning framework for Verilog generation. EARL performs policy optimization using verifiable reward signals and introduces entropy-guided selective updates that gate policy gradients to high-entropy tokens. This approach preserves training stability and concentrates gradient updates on functionally important regions of code. Our experiments on VerilogEval and RTLLM show that EARL improves functional pass rates over prior LLM baselines by up to 14.7%, while reducing unnecessary updates and improving training stability. These results indicate that focusing RL on critical, high-uncertainty tokens enables more reliable and targeted policy improvement for structured RTL code generation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 175,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12036",
        "abs_url": "https://arxiv.org/abs/2511.12036",
        "pdf_url": "https://arxiv.org/pdf/2511.12036",
        "title": "Preference Learning from Physics-Based Feedback: Tuning Language Models to Design BCC/B2 Superalloys",
        "authors": [
            "Satanu Ghosh",
            "Collin Holgate",
            "Neal R. Brodnik",
            "Doug Downey",
            "Samantha Daly",
            "Tresa M. Pollock",
            "Samuel Carton"
        ],
        "comments": "",
        "subjects": "Computational Engineering, Finance, and Science (cs.CE); Materials Science (cond-mat.mtrl-sci); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "We apply preference learning to the task of language model-guided design of novel structural alloys. In contrast to prior work that focuses on generating stable inorganic crystals, our approach targets the synthesizeability of a specific structural class: BCC/B2 superalloys, an underexplored family of materials with potential applications in extreme environments. Using three open-weight models (LLaMA-3.1, Gemma-2, and OLMo-2), we demonstrate that language models can be optimized for multiple design objectives using a single, unified reward signal through Direct Preference Optimization (DPO). Unlike prior approaches that rely on heuristic or human-in-the-loop feedback (costly), our reward signal is derived from thermodynamic phase calculations, offering a scientifically grounded criterion for model tuning. To our knowledge, this is the first demonstration of preference-tuning a language model using physics-grounded feedback for structural alloy design. The resulting framework is general and extensible, providing a path forward for intelligent design-space exploration across a range of physical science domains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 176,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12041",
        "abs_url": "https://arxiv.org/abs/2511.12041",
        "pdf_url": "https://arxiv.org/pdf/2511.12041",
        "title": "Mesh-based Super-resolution of Detonation Flows with Multiscale Graph Transformers",
        "authors": [
            "Shivam Barwey",
            "Pinaki Pal"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Super-resolution flow reconstruction using state-of-the-art data-driven techniques is valuable for a variety of applications, such as subgrid/subfilter closure modeling, accelerating spatiotemporal forecasting, data compression, and serving as an upscaling tool for sparse experimental measurements. In the present work, a first-of-its-kind multiscale graph transformer approach is developed for mesh-based super-resolution (SR-GT) of reacting flows. The novel data-driven modeling paradigm leverages a graph-based flow-field representation compatible with complex geometries and non-uniform/unstructured grids. Further, the transformer backbone captures long-range dependencies between different parts of the low-resolution flow-field, identifies important features, and then generates the super-resolved flow-field that preserves those features at a higher resolution. The performance of SR-GT is demonstrated in the context of spectral-element-discretized meshes for a challenging test problem of 2D detonation propagation within a premixed hydrogen-air mixture exhibiting highly complex multiscale reacting flow behavior. The SR-GT framework utilizes a unique element-local (+ neighborhood) graph representation for the coarse input, which is then tokenized before being processed by the transformer component to produce the fine output. It is demonstrated that SR-GT provides high super-resolution accuracy for reacting flow-field features and superior performance compared to traditional interpolation-based SR schemes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 177,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12047",
        "abs_url": "https://arxiv.org/abs/2511.12047",
        "pdf_url": "https://arxiv.org/pdf/2511.12047",
        "title": "DCMM-Transformer: Degree-Corrected Mixed-Membership Attention for Medical Imaging",
        "authors": [
            "Huimin Cheng",
            "Xiaowei Yu",
            "Shushan Wu",
            "Luyang Fang",
            "Chao Cao",
            "Jing Zhang",
            "Tianming Liu",
            "Dajiang Zhu",
            "Wenxuan Zhong",
            "Ping Ma"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Medical images exhibit latent anatomical groupings, such as organs, tissues, and pathological regions, that standard Vision Transformers (ViTs) fail to exploit. While recent work like SBM-Transformer attempts to incorporate such structures through stochastic binary masking, they suffer from non-differentiability, training instability, and the inability to model complex community structure. We present DCMM-Transformer, a novel ViT architecture for medical image analysis that incorporates a Degree-Corrected Mixed-Membership (DCMM) model as an additive bias in self-attention. Unlike prior approaches that rely on multiplicative masking and binary sampling, our method introduces community structure and degree heterogeneity in a fully differentiable and interpretable manner. Comprehensive experiments across diverse medical imaging datasets, including brain, chest, breast, and ocular modalities, demonstrate the superior performance and generalizability of the proposed approach. Furthermore, the learned group structure and structured attention modulation substantially enhance interpretability by yielding attention maps that are anatomically meaningful and semantically coherent.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 178,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12052",
        "abs_url": "https://arxiv.org/abs/2511.12052",
        "pdf_url": "https://arxiv.org/pdf/2511.12052",
        "title": "Exploring AI in Steganography and Steganalysis: Trends, Clusters, and Sustainable Development Potential",
        "authors": [
            "Aditya Kumar Sahu",
            "Chandan Kumar",
            "Saksham Kumar",
            "Serdar Solak"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
        "abstract": "Steganography and steganalysis are strongly related subjects of information security. Over the past decade, many powerful and efficient artificial intelligence (AI) - driven techniques have been designed and presented during research into steganography as well as steganalysis. This study presents a scientometric analysis of AI-driven steganography-based data hiding techniques using a thematic modelling approach. A total of 654 articles within the time span of 2017 to 2023 have been considered. Experimental evaluation of the study reveals that 69% of published articles are from Asian countries. The China is on top (TP:312), followed by India (TP-114). The study mainly identifies seven thematic clusters: steganographic image data hiding, deep image steganalysis, neural watermark robustness, linguistic steganography models, speech steganalysis algorithms, covert communication networks, and video steganography techniques. The proposed study also assesses the scope of AI-steganography under the purview of sustainable development goals (SDGs) to present the interdisciplinary reciprocity between them. It has been observed that only 18 of the 654 articles are aligned with one of the SDGs, which shows that limited studies conducted in alignment with SDG goals. SDG9 which is Industry, Innovation, and Infrastructure is leading among 18 SDGs mapped articles. To the top of our insight, this study is the unique one to present a scientometric study on AI-driven steganography-based data hiding techniques. In the context of descriptive statistics, the study breaks down the underlying causes of observed trends, including the influence of DL developments, trends in East Asia and maturity of foundational methods. The work also stresses upon the critical gaps in societal alignment, particularly the SDGs, ultimately working on unveiling the field's global impact on AI security challenges.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 179,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12061",
        "abs_url": "https://arxiv.org/abs/2511.12061",
        "pdf_url": "https://arxiv.org/pdf/2511.12061",
        "title": "MovSemCL: Movement-Semantics Contrastive Learning for Trajectory Similarity",
        "authors": [
            "Zhichen Lai",
            "Hua Lu",
            "Huan Li",
            "Jialiang Li",
            "Christian S. Jensen"
        ],
        "comments": "8 pages, 6 figures; accepted by AAAI 2026 as an Oral paper",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Databases (cs.DB)",
        "abstract": "Trajectory similarity computation is fundamental functionality that is used for, e.g., clustering, prediction, and anomaly detection. However, existing learning-based methods exhibit three key limitations: (1) insufficient modeling of trajectory semantics and hierarchy, lacking both movement dynamics extraction and multi-scale structural representation; (2) high computational costs due to point-wise encoding; and (3) use of physically implausible augmentations that distort trajectory semantics. To address these issues, we propose MovSemCL, a movement-semantics contrastive learning framework for trajectory similarity computation. MovSemCL first transforms raw GPS trajectories into movement-semantics features and then segments them into patches. Next, MovSemCL employs intra- and inter-patch attentions to encode local as well as global trajectory patterns, enabling efficient hierarchical representation and reducing computational costs. Moreover, MovSemCL includes a curvature-guided augmentation strategy that preserves informative segments (e.g., turns and intersections) and masks redundant ones, generating physically plausible augmented views. Experiments on real-world datasets show that MovSemCL is capable of outperforming state-of-the-art methods, achieving mean ranks close to the ideal value of 1 at similarity search tasks and improvements by up to 20.3% at heuristic approximation, while reducing inference latency by up to 43.4%.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 180,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12071",
        "abs_url": "https://arxiv.org/abs/2511.12071",
        "pdf_url": "https://arxiv.org/pdf/2511.12071",
        "title": "Improving Graph Embeddings in Machine Learning Using Knowledge Completion with Validation in a Case Study on COVID-19 Spread",
        "authors": [
            "Rosario Napoli",
            "Gabriele Morabito",
            "Antonio Celesti",
            "Massimo Villari",
            "Maria Fazio"
        ],
        "comments": "Accepted at the 16th IEEE International Conference on Knowledge Graphs (ICKG) 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "The rise of graph-structured data has driven major advances in Graph Machine Learning (GML), where graph embeddings (GEs) map features from Knowledge Graphs (KGs) into vector spaces, enabling tasks like node classification and link prediction. However, since GEs are derived from explicit topology and features, they may miss crucial implicit knowledge hidden in seemingly sparse datasets, affecting graph structure and their representation. We propose a GML pipeline that integrates a Knowledge Completion (KC) phase to uncover latent dataset semantics before embedding generation. Focusing on transitive relations, we model hidden connections with decay-based inference functions, reshaping graph topology, with consequences on embedding dynamics and aggregation processes in GraphSAGE and Node2Vec. Experiments show that our GML pipeline significantly alters the embedding space geometry, demonstrating that its introduction is not just a simple enrichment but a transformative step that redefines graph representation quality.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 181,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12072",
        "abs_url": "https://arxiv.org/abs/2511.12072",
        "pdf_url": "https://arxiv.org/pdf/2511.12072",
        "title": "ProAV-DiT: A Projected Latent Diffusion Transformer for Efficient Synchronized Audio-Video Generation",
        "authors": [
            "Jiahui Sun",
            "Weining Wang",
            "Mingzhen Sun",
            "Yirong Yang",
            "Xinxin Zhu",
            "Jing Liu"
        ],
        "comments": "",
        "subjects": "Multimedia (cs.MM); Artificial Intelligence (cs.AI); Sound (cs.SD)",
        "abstract": "Sounding Video Generation (SVG) remains a challenging task due to the inherent structural misalignment between audio and video, as well as the high computational cost of multimodal data processing. In this paper, we introduce ProAV-DiT, a Projected Latent Diffusion Transformer designed for efficient and synchronized audio-video generation. To address structural inconsistencies, we preprocess raw audio into video-like representations, aligning both the temporal and spatial dimensions between audio and video. At its core, ProAV-DiT adopts a Multi-scale Dual-stream Spatio-Temporal Autoencoder (MDSA), which projects both modalities into a unified latent space using orthogonal decomposition, enabling fine-grained spatiotemporal modeling and semantic alignment. To further enhance temporal coherence and modality-specific fusion, we introduce a multi-scale attention mechanism, which consists of multi-scale temporal self-attention and group cross-modal attention. Furthermore, we stack the 2D latents from MDSA into a unified 3D latent space, which is processed by a spatio-temporal diffusion Transformer. This design efficiently models spatiotemporal dependencies, enabling the generation of high-fidelity synchronized audio-video content while reducing computational overhead. Extensive experiments conducted on standard benchmarks demonstrate that ProAV-DiT outperforms existing methods in both generation quality and computational efficiency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 182,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12074",
        "abs_url": "https://arxiv.org/abs/2511.12074",
        "pdf_url": "https://arxiv.org/pdf/2511.12074",
        "title": "MF-Speech: Achieving Fine-Grained and Compositional Control in Speech Generation via Factor Disentanglement",
        "authors": [
            "Xinyue Yu",
            "Youqing Fang",
            "Pingyu Wu",
            "Guoyang Ye",
            "Wenbo Zhou",
            "Weiming Zhang",
            "Song Xiao"
        ],
        "comments": "",
        "subjects": "Sound (cs.SD); Artificial Intelligence (cs.AI)",
        "abstract": "Generating expressive and controllable human speech is one of the core goals of generative artificial intelligence, but its progress has long been constrained by two fundamental challenges: the deep entanglement of speech factors and the coarse granularity of existing control mechanisms. To overcome these challenges, we have proposed a novel framework called MF-Speech, which consists of two core components: MF-SpeechEncoder and MF-SpeechGenerator. MF-SpeechEncoder acts as a factor purifier, adopting a multi-objective optimization strategy to decompose the original speech signal into highly pure and independent representations of content, timbre, and emotion. Subsequently, MF-SpeechGenerator functions as a conductor, achieving precise, composable and fine-grained control over these factors through dynamic fusion and Hierarchical Style Adaptive Normalization (HSAN). Experiments demonstrate that in the highly challenging multi-factor compositional speech generation task, MF-Speech significantly outperforms current state-of-the-art methods, achieving a lower word error rate (WER=4.67%), superior style control (SECS=0.5685, Corr=0.68), and the highest subjective evaluation scores(nMOS=3.96, sMOS_emotion=3.86, sMOS_style=3.78). Furthermore, the learned discrete factors exhibit strong transferability, demonstrating their significant potential as a general-purpose speech representation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 183,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12075",
        "abs_url": "https://arxiv.org/abs/2511.12075",
        "pdf_url": "https://arxiv.org/pdf/2511.12075",
        "title": "Treatment Stitching with Schrödinger Bridge for Enhancing Offline Reinforcement Learning in Adaptive Treatment Strategies",
        "authors": [
            "Dong-Hee Shin",
            "Deok-Joong Lee",
            "Young-Han Son",
            "Tae-Eui Kam"
        ],
        "comments": "19 pages, 5 figures, AAAI conference",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Adaptive treatment strategies (ATS) are sequential decision-making processes that enable personalized care by dynamically adjusting treatment decisions in response to evolving patient symptoms. While reinforcement learning (RL) offers a promising approach for optimizing ATS, its conventional online trial-and-error learning mechanism is not permissible in clinical settings due to risks of harm to patients. Offline RL tackles this limitation by learning policies exclusively from historical treatment data, but its performance is often constrained by data scarcity-a pervasive challenge in clinical domains. To overcome this, we propose Treatment Stitching (TreatStitch), a novel data augmentation framework that generates clinically valid treatment trajectories by intelligently stitching segments from existing treatment data. Specifically, TreatStitch identifies similar intermediate patient states across different trajectories and stitches their respective segments. Even when intermediate states are too dissimilar to stitch directly, TreatStitch leverages the Schrödinger bridge method to generate smooth and energy-efficient bridging trajectories that connect dissimilar states. By augmenting these synthetic trajectories into the original dataset, offline RL can learn from a more diverse dataset, thereby improving its ability to optimize ATS. Extensive experiments across multiple treatment datasets demonstrate the effectiveness of TreatStitch in enhancing offline RL performance. Furthermore, we provide a theoretical justification showing that TreatStitch maintains clinical validity by avoiding out-of-distribution transitions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 184,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12085",
        "abs_url": "https://arxiv.org/abs/2511.12085",
        "pdf_url": "https://arxiv.org/pdf/2511.12085",
        "title": "Explainable Transformer-Based Email Phishing Classification with Adversarial Robustness",
        "authors": [
            "Sajad U P"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Phishing and related cyber threats are becoming more varied and technologically advanced. Among these, email-based phishing remains the most dominant and persistent threat. These attacks exploit human vulnerabilities to disseminate malware or gain unauthorized access to sensitive information. Deep learning (DL) models, particularly transformer-based models, have significantly enhanced phishing mitigation through their contextual understanding of language. However, some recent threats, specifically Artificial Intelligence (AI)-generated phishing attacks, are reducing the overall system resilience of phishing detectors. In response, adversarial training has shown promise against AI-generated phishing threats. This study presents a hybrid approach that uses DistilBERT, a smaller, faster, and lighter version of the BERT transformer model for email classification. Robustness against text-based adversarial perturbations is reinforced using Fast Gradient Method (FGM) adversarial training. Furthermore, the framework integrates the LIME Explainable AI (XAI) technique to enhance the transparency of the DistilBERT architecture. The framework also uses the Flan-T5-small language model from Hugging Face to generate plain-language security narrative explanations for end-users. This combined approach ensures precise phishing classification while providing easily understandable justifications for the model's decisions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 185,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12101",
        "abs_url": "https://arxiv.org/abs/2511.12101",
        "pdf_url": "https://arxiv.org/pdf/2511.12101",
        "title": "Decoupled Action Head: Confining Task Knowledge to Conditioning Layers",
        "authors": [
            "Jian Zhou",
            "Sihao Lin",
            "Shuai Fu",
            "Qi WU"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Behavior Cloning (BC) is a data-driven supervised learning approach that has gained increasing attention with the success of scaling laws in language and vision domains. Among its implementations in robotic manipulation, Diffusion Policy (DP), with its two variants DP-CNN (DP-C) and DP-Transformer (DP-T), is one of the most effective and widely adopted models, demonstrating the advantages of predicting continuous action sequences. However, both DP and other BC methods remain constrained by the scarcity of paired training data, and the internal mechanisms underlying DP's effectiveness remain insufficiently understood, leading to limited generalization and a lack of principled design in model development. In this work, we propose a decoupled training recipe that leverages nearly cost-free kinematics-generated trajectories as observation-free data to pretrain a general action head (action generator). The pretrained action head is then frozen and adapted to novel tasks through feature modulation. Our experiments demonstrate the feasibility of this approach in both in-distribution and out-of-distribution scenarios. As an additional benefit, decoupling improves training efficiency; for instance, DP-C achieves up to a 41% speedup. Furthermore, the confinement of task-specific knowledge to the conditioning components under decoupling, combined with the near-identical performance of DP-C in both normal and decoupled training, indicates that the action generation backbone plays a limited role in robotic manipulation. Motivated by this observation, we introduce DP-MLP, which replaces the 244M-parameter U-Net backbone of DP-C with only 4M parameters of simple MLP blocks, achieving a 83.9% faster training speed under normal training and 89.1% under decoupling.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 186,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12116",
        "abs_url": "https://arxiv.org/abs/2511.12116",
        "pdf_url": "https://arxiv.org/pdf/2511.12116",
        "title": "LLMLagBench: Identifying Temporal Training Boundaries in Large Language Models",
        "authors": [
            "Piotr Pęzik",
            "Konrad Kaczyński",
            "Maria Szymańska",
            "Filip Żarnecki",
            "Zuzanna Deckert",
            "Jakub Kwiatkowski",
            "Wojciech Janowski"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Large Language Models (LLMs) are pretrained on textual data up to a specific temporal cutoff. This creates a strict knowledge boundary beyond which models cannot provide accurate information without querying external sources. More subtly, when this limitation is unknown or ignored, LLMs may inadvertently blend outdated time-sensitive information with general knowledge during reasoning tasks, potentially compromising response accuracy. We introduce LLMLagBench, an LLM freshness benchmark, as a systematic approach for identifying the earliest probable temporal boundaries of an LLM's training data by evaluating its knowledge of recent events. We then apply this benchmark to evaluate a large set of LLMs, including models with both explicitly declared and undeclared training cutoffs. The reliability of the benchmark is assessed by manual validation and comparison with publicly released information about LLM pretraining.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 187,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12149",
        "abs_url": "https://arxiv.org/abs/2511.12149",
        "pdf_url": "https://arxiv.org/pdf/2511.12149",
        "title": "AttackVLA: Benchmarking Adversarial and Backdoor Attacks on Vision-Language-Action Models",
        "authors": [
            "Jiayu Li",
            "Yunhan Zhao",
            "Xiang Zheng",
            "Zonghuan Xu",
            "Yige Li",
            "Xingjun Ma",
            "Yu-Gang Jiang"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Vision-Language-Action (VLA) models enable robots to interpret natural-language instructions and perform diverse tasks, yet their integration of perception, language, and control introduces new safety vulnerabilities. Despite growing interest in attacking such models, the effectiveness of existing techniques remains unclear due to the absence of a unified evaluation framework. One major issue is that differences in action tokenizers across VLA architectures hinder reproducibility and fair comparison. More importantly, most existing attacks have not been validated in real-world scenarios. To address these challenges, we propose AttackVLA, a unified framework that aligns with the VLA development lifecycle, covering data construction, model training, and inference. Within this framework, we implement a broad suite of attacks, including all existing attacks targeting VLAs and multiple adapted attacks originally developed for vision-language models, and evaluate them in both simulation and real-world settings. Our analysis of existing attacks reveals a critical gap: current methods tend to induce untargeted failures or static action states, leaving targeted attacks that drive VLAs to perform precise long-horizon action sequences largely unexplored. To fill this gap, we introduce BackdoorVLA, a targeted backdoor attack that compels a VLA to execute an attacker-specified long-horizon action sequence whenever a trigger is present. We evaluate BackdoorVLA in both simulated benchmarks and real-world robotic settings, achieving an average targeted success rate of 58.4% and reaching 100% on selected tasks. Our work provides a standardized framework for evaluating VLA vulnerabilities and demonstrates the potential for precise adversarial manipulation, motivating further research on securing VLA-based embodied systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 188,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12154",
        "abs_url": "https://arxiv.org/abs/2511.12154",
        "pdf_url": "https://arxiv.org/pdf/2511.12154",
        "title": "Open Banking Foundational Model: Learning Language Representations from Few Financial Transactions",
        "authors": [
            "Gustavo Polleti",
            "Marlesson Santana",
            "Eduardo Fontes"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "We introduced a multimodal foundational model for financial transactions that integrates both structured attributes and unstructured textual descriptions into a unified representation. By adapting masked language modeling to transaction sequences, we demonstrated that our approach not only outperforms classical feature engineering and discrete event sequence methods but is also particularly effective in data-scarce Open Banking scenarios. To our knowledge, this is the first large-scale study across thousands of financial institutions in North America, providing evidence that multimodal representations can generalize across geographies and institutions. These results highlight the potential of self-supervised models to advance financial applications ranging from fraud prevention and credit risk to customer insights",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 189,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12175",
        "abs_url": "https://arxiv.org/abs/2511.12175",
        "pdf_url": "https://arxiv.org/pdf/2511.12175",
        "title": "AI-Enhanced IoT Systems for Predictive Maintenance and Affordability Optimization in Smart Microgrids: A Digital Twin Approach",
        "authors": [
            "Koushik Ahmed Kushal",
            "Florimond Gueniat"
        ],
        "comments": "12 pages, 6 figures, includes simulation and evaluation results",
        "subjects": "Systems and Control (eess.SY); Artificial Intelligence (cs.AI)",
        "abstract": "This study presents an AI enhanced IoT framework for predictive maintenance and affordability optimization in smart microgrids using a Digital Twin modeling approach. The proposed system integrates real time sensor data, machine learning based fault prediction, and cost aware operational analytics to improve reliability and energy efficiency in distributed microgrid environments. By synchronizing physical microgrid components with a virtual Digital Twin, the framework enables early detection of component degradation, dynamic load management, and optimized maintenance scheduling. Experimental evaluations demonstrate improved predictive accuracy, reduced operational downtime, and measurable cost savings compared to baseline microgrid management methods. The findings highlight the potential of Digital Twin driven IoT architectures as a scalable solution for next generation intelligent and affordable energy systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 190,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12176",
        "abs_url": "https://arxiv.org/abs/2511.12176",
        "pdf_url": "https://arxiv.org/pdf/2511.12176",
        "title": "Reinforcement Learning for Charging Optimization of Inhomogeneous Dicke Quantum Batteries",
        "authors": [
            "Xiaobin Song",
            "Siyuan Bai",
            "Da-Wei Wang",
            "Hanxiao Tao",
            "Xizhe Wang",
            "Rebing Wu",
            "Benben Jiang"
        ],
        "comments": "",
        "subjects": "Quantum Physics (quant-ph); Artificial Intelligence (cs.AI)",
        "abstract": "Charging optimization is a key challenge to the implementation of quantum batteries, particularly under inhomogeneity and partial observability. This paper employs reinforcement learning to optimize piecewise-constant charging policies for an inhomogeneous Dicke battery. We systematically compare policies across four observability regimes, from full-state access to experimentally accessible observables (energies of individual two-level systems (TLSs), first-order averages, and second-order correlations). Simulation results demonstrate that full observability yields near-optimal ergotropy with low variability, while under partial observability, access to only single-TLS energies or energies plus first-order averages lags behind the fully observed baseline. However, augmenting partial observations with second-order correlations recovers most of the gap, reaching 94%-98% of the full-state baseline. The learned schedules are nonmyopic, trading temporary plateaus or declines for superior terminal outcomes. These findings highlight a practical route to effective fast-charging protocols under realistic information constraints.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 191,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12203",
        "abs_url": "https://arxiv.org/abs/2511.12203",
        "pdf_url": "https://arxiv.org/pdf/2511.12203",
        "title": "Locally Optimal Solutions to Constraint Displacement Problems via Path-Obstacle Overlaps",
        "authors": [
            "Antony Thomas",
            "Fulvio Mastrogiovanni",
            "Marco Baglietto"
        ],
        "comments": "Robotics and Autonomous Systems",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
        "abstract": "We present a unified approach for constraint displacement problems in which a robot finds a feasible path by displacing constraints or obstacles. To this end, we propose a two stage process that returns locally optimal obstacle displacements to enable a feasible path for the robot. The first stage proceeds by computing a trajectory through the obstacles while minimizing an appropriate objective function. In the second stage, these obstacles are displaced to make the computed robot trajectory feasible, that is, collision-free. Several examples are provided that successfully demonstrate our approach on two distinct classes of constraint displacement problems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 192,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12212",
        "abs_url": "https://arxiv.org/abs/2511.12212",
        "pdf_url": "https://arxiv.org/pdf/2511.12212",
        "title": "Recursive Threshold Median Filter and Autoencoder for Salt-and-Pepper Denoising: SSIM analysis of Images and Entropy Maps",
        "authors": [
            "Petr Boriskov",
            "Kirill Rudkovskii",
            "Andrei Velichko"
        ],
        "comments": "14 pages, 13 figures, 4 tables",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "This paper studies the removal of salt-and-pepper noise from images using median filter (MF) and simple three-layer autoencoder (AE) within recursive threshold algorithm. The performance of denoising is assessed with two metrics: the standard Structural Similarity Index SSIMImg of restored and clean images and a newly applied metric SSIMMap - the SSIM of entropy maps of these images computed via 2D Sample Entropy in sliding windows. We shown that SSIMMap is more sensitive to blur and local intensity transitions and complements SSIMImg. Experiments on low- and high-resolution grayscales images demonstrate that recursive threshold MF robustly restores images even under strong noise (50-60 %), whereas simple AE is only capable of restoring images with low levels of noise (<30 %). We propose two scalable schemes: (i) 2MF, which uses two MFs with different window sizes and a final thresholding step, effective for highlighting sharp local details at low resolution; and (ii) MFs-AE, which aggregates features from multiple MFs via an AE and is beneficial for restoring the overall scene structure at higher resolution. Owing to its simplicity and computational efficiency, MF remains preferable for deployment on resource-constrained platforms (edge/IoT), whereas AE underperforms without prior denoising. The results also validate the practical value of SSIMMap for objective blur assessment and denoising parameter tuning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 193,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12213",
        "abs_url": "https://arxiv.org/abs/2511.12213",
        "pdf_url": "https://arxiv.org/pdf/2511.12213",
        "title": "MME-RAG: Multi-Manager-Expert Retrieval-Augmented Generation for Fine-Grained Entity Recognition in Task-Oriented Dialogues",
        "authors": [
            "Liang Xue",
            "Haoyu Liu",
            "Yajun Tian",
            "Xinyu Zhong",
            "Yang Liu"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Fine-grained entity recognition is crucial for reasoning and decision-making in task-oriented dialogues, yet current large language models (LLMs) continue to face challenges in domain adaptation and retrieval controllability. We introduce MME-RAG, a Multi-Manager-Expert Retrieval-Augmented Generation framework that decomposes entity recognition into two coordinated stages: type-level judgment by lightweight managers and span-level extraction by specialized experts. Each expert is supported by a KeyInfo retriever that injects semantically aligned, few-shot exemplars during inference, enabling precise and domain-adaptive extraction without additional training. Experiments on CrossNER, MIT-Movie, MIT-Restaurant, and our newly constructed multi-domain customer-service dataset demonstrate that MME-RAG performs better than recent baselines in most domains. Ablation studies further show that both the hierarchical decomposition and KeyInfo-guided retrieval are key drivers of robustness and cross-domain generalization, establishing MME-RAG as a scalable and interpretable solution for adaptive dialogue understanding.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 194,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12236",
        "abs_url": "https://arxiv.org/abs/2511.12236",
        "pdf_url": "https://arxiv.org/pdf/2511.12236",
        "title": "Consistency Is the Key: Detecting Hallucinations in LLM Generated Text By Checking Inconsistencies About Key Facts",
        "authors": [
            "Raavi Gupta",
            "Pranav Hari Panicker",
            "Sumit Bhatia",
            "Ganesh Ramakrishnan"
        ],
        "comments": "To appear at International Joint Conference on Natural Language Processing & Asia-Pacific Chapter of the Association for Computational Linguistics (IJCNLP-AACL), 2025",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Large language models (LLMs), despite their remarkable text generation capabilities, often hallucinate and generate text that is factually incorrect and not grounded in real-world knowledge. This poses serious risks in domains like healthcare, finance, and customer support. A typical way to use LLMs is via the APIs provided by LLM vendors where there is no access to model weights or options to fine-tune the model. Existing methods to detect hallucinations in such settings where the model access is restricted or constrained by resources typically require making multiple LLM API calls, increasing latency and API cost. We introduce CONFACTCHECK, an efficient hallucination detection approach that does not leverage any external knowledge base and works on the simple intuition that responses to factual probes within the generated text should be consistent within a single LLM and across different LLMs. Rigorous empirical evaluation on multiple datasets that cover both the generation of factual texts and the open generation shows that CONFACTCHECK can detect hallucinated facts efficiently using fewer resources and achieves higher accuracy scores compared to existing baselines that operate under similar conditions. Our code is available here.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 195,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12240",
        "abs_url": "https://arxiv.org/abs/2511.12240",
        "pdf_url": "https://arxiv.org/pdf/2511.12240",
        "title": "SCI: An Equilibrium for Signal Intelligence",
        "authors": [
            "Vishal Joshua Meesala"
        ],
        "comments": "34 pages, 7 figures. Preprint",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Systems and Control (eess.SY)",
        "abstract": "We present SCI, a closed-loop, control-theoretic framework that models interpretability as a regulated state. SCI formalizes the interpretive error Delta SP and actively drives SP(t) in [0, 1] (\"Surgical Precision\") toward a target via a projected update on the parameters Theta under a human-gain budget. The framework operates through three coordinated components: (1) reliability-weighted, multiscale features P(t, s); (2) a knowledge-guided interpreter psi_Theta that emits traceable markers and rationales; and (3) a Lyapunov-guided controller equipped with rollback, trust-region safeguards, and a descent condition. Across biomedical (EEG/ECG/ICU), industrial (bearings/tool wear), and environmental (climate/seismic) domains, SCI reduces interpretive error by 25-42% (mean 38%, 95% confidence interval 22-43%) relative to static explainers while maintaining AUC/F1 within approximately 1-2 percentage points of baseline. SCI also reduces SP variance from 0.030 to 0.011, indicating substantially more stable explanations. Modeling interpretability as a control objective yields steadier, faster-recovering, and more trustworthy interpretive behavior across diverse signal regimes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 196,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12256",
        "abs_url": "https://arxiv.org/abs/2511.12256",
        "pdf_url": "https://arxiv.org/pdf/2511.12256",
        "title": "Prompt-Conditioned FiLM and Multi-Scale Fusion on MedSigLIP for Low-Dose CT Quality Assessment",
        "authors": [
            "Tolga Demiroglu",
            "Mehmet Ozan Unal",
            "Metin Ertas",
            "Isa Yildirim"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Image and Video Processing (eess.IV)",
        "abstract": "We propose a prompt-conditioned framework built on MedSigLIP that injects textual priors via Feature-wise Linear Modulation (FiLM) and multi-scale pooling. Text prompts condition patch-token features on clinical intent, enabling data-efficient learning and rapid adaptation. The architecture combines global, local, and texture-aware pooling through separate regression heads fused by a lightweight MLP, trained with pairwise ranking loss. Evaluated on the LDCTIQA2023 (a public LDCT quality assessment challenge) with 1,000 training images, we achieve PLCC = 0.9575, SROCC = 0.9561, and KROCC = 0.8301, surpassing the top-ranked published challenge submissions and demonstrating the effectiveness of our prompt-guided approach.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 197,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12265",
        "abs_url": "https://arxiv.org/abs/2511.12265",
        "pdf_url": "https://arxiv.org/pdf/2511.12265",
        "title": "Calibrated Adversarial Sampling: Multi-Armed Bandit-Guided Generalization Against Unforeseen Attacks",
        "authors": [
            "Rui Wang",
            "Zeming Wei",
            "Xiyue Zhang",
            "Meng Sun"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV); Optimization and Control (math.OC)",
        "abstract": "Deep Neural Networks (DNNs) are known to be vulnerable to various adversarial perturbations. To address the safety concerns arising from these vulnerabilities, adversarial training (AT) has emerged as one of the most effective paradigms for enhancing the robustness of DNNs. However, existing AT frameworks primarily focus on a single or a limited set of attack types, leaving DNNs still exposed to attack types that may be encountered in practice but not addressed during training. In this paper, we propose an efficient fine-tuning method called Calibrated Adversarial Sampling (CAS) to address these issues. From the optimization perspective within the multi-armed bandit framework, it dynamically designs rewards and balances exploration and exploitation by considering the dynamic and interdependent characteristics of multiple robustness dimensions. Experiments on benchmark datasets show that CAS achieves superior overall robustness while maintaining high clean accuracy, providing a new paradigm for robust generalization of DNNs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 198,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12286",
        "abs_url": "https://arxiv.org/abs/2511.12286",
        "pdf_url": "https://arxiv.org/pdf/2511.12286",
        "title": "Sangam: Chiplet-Based DRAM-PIM Accelerator with CXL Integration for LLM Inferencing",
        "authors": [
            "Khyati Kiyawat",
            "Zhenxing Fan",
            "Yasas Seneviratne",
            "Morteza Baradaran",
            "Akhil Shekar",
            "Zihan Xia",
            "Mingu Kang",
            "Kevin Skadron"
        ],
        "comments": "",
        "subjects": "Hardware Architecture (cs.AR); Artificial Intelligence (cs.AI)",
        "abstract": "Large Language Models (LLMs) are becoming increasingly data-intensive due to growing model sizes, and they are becoming memory-bound as the context length and, consequently, the key-value (KV) cache size increase. Inference, particularly the decoding phase, is dominated by memory-bound GEMV or flat GEMM operations with low operational intensity (OI), making it well-suited for processing-in-memory (PIM) approaches. However, existing in/near-memory solutions face critical limitations such as reduced memory capacity due to the high area cost of integrating processing elements (PEs) within DRAM chips, and limited PE capability due to the constraints of DRAM fabrication technology. This work presents a chiplet-based memory module that addresses these limitations by decoupling logic and memory into chiplets fabricated in heterogeneous technology nodes and connected via an interposer. The logic chiplets sustain high bandwidth access to the DRAM chiplets, which house the memory banks, and enable the integration of advanced processing components such as systolic arrays and SRAM-based buffers to accelerate memory-bound GEMM kernels, capabilities that were not feasible in prior PIM architectures. We propose Sangam, a CXL-attached PIM-chiplet based memory module that can either act as a drop-in replacement for GPUs or co-executes along side the GPUs. Sangam achieves speedup of 3.93, 4.22, 2.82x speedup in end-to-end query latency, 10.3, 9.5, 6.36x greater decoding throughput, and order of magnitude energy savings compared to an H100 GPU for varying input size, output length, and batch size on LLaMA 2-7B, Mistral-7B, and LLaMA 3-70B, respectively.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 199,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12309",
        "abs_url": "https://arxiv.org/abs/2511.12309",
        "pdf_url": "https://arxiv.org/pdf/2511.12309",
        "title": "Optimal Self-Consistency for Efficient Reasoning with Large Language Models",
        "authors": [
            "Austin Feng",
            "Marius Alonso",
            "Ambroise Odonnat"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "Self-consistency (SC) is a widely used test-time inference technique for improving performance in chain-of-thought reasoning. It involves generating multiple responses, or samples from a large language model (LLM) and selecting the most frequent answer. This procedure can naturally be viewed as a majority vote or empirical mode estimation. Despite its effectiveness, SC is prohibitively expensive at scale when naively applied to datasets, and it lacks a unified theoretical treatment of sample efficiency and scaling behavior. In this paper, we provide the first comprehensive analysis of SC's scaling behavior and its variants, drawing on mode estimation and voting theory. We derive and empirically validate power law scaling for self-consistency across datasets, and analyze the sample efficiency for fixed-allocation and dynamic-allocation sampling schemes. From these insights, we introduce Blend-ASC, a novel variant of self-consistency that dynamically allocates samples to questions during inference, achieving state-of-the-art sample efficiency. Our approach uses 6.8x fewer samples than vanilla SC on average, outperforming both fixed- and dynamic-allocation SC baselines, thereby demonstrating the superiority of our approach in terms of efficiency. In contrast to existing variants, Blend-ASC is hyperparameter-free and can fit an arbitrary sample budget, ensuring it can be easily applied to any self-consistency application.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 200,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12319",
        "abs_url": "https://arxiv.org/abs/2511.12319",
        "pdf_url": "https://arxiv.org/pdf/2511.12319",
        "title": "Decision and Gender Biases in Large Language Models: A Behavioral Economic Perspective",
        "authors": [
            "Luca Corazzini",
            "Elisa Deriu",
            "Marco Guerzoni"
        ],
        "comments": "",
        "subjects": "General Economics (econ.GN); Artificial Intelligence (cs.AI)",
        "abstract": "Large language models (LLMs) increasingly mediate economic and organisational processes, from automated customer support and recruitment to investment advice and policy analysis. These systems are often assumed to embody rational decision making free from human error; yet they are trained on human language corpora that may embed cognitive and social biases. This study investigates whether advanced LLMs behave as rational agents or whether they reproduce human behavioural tendencies when faced with classic decision problems. Using two canonical experiments in behavioural economics, the ultimatum game and a gambling game, we elicit decisions from two state of the art models, Google Gemma7B and Qwen, under neutral and gender conditioned prompts. We estimate parameters of inequity aversion and loss-aversion and compare them with human benchmarks. The models display attenuated but persistent deviations from rationality, including moderate fairness concerns, mild loss aversion, and subtle gender conditioned differences.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 201,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12342",
        "abs_url": "https://arxiv.org/abs/2511.12342",
        "pdf_url": "https://arxiv.org/pdf/2511.12342",
        "title": "Ground Plane Projection for Improved Traffic Analytics at Intersections",
        "authors": [
            "Sajjad Pakdamansavoji",
            "Kumar Vaibhav Jha",
            "Baher Abdulhai",
            "James H Elder"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Accurate turning movement counts at intersections are important for signal control, traffic management and urban planning. Computer vision systems for automatic turning movement counts typically rely on visual analysis in the image plane of an infrastructure camera. Here we explore potential advantages of back-projecting vehicles detected in one or more infrastructure cameras to the ground plane for analysis in real-world 3D coordinates. For single-camera systems we find that back-projection yields more accurate trajectory classification and turning movement counts. We further show that even higher accuracy can be achieved through weak fusion of back-projected detections from multiple cameras. These results suggeest that traffic should be analyzed on the ground plane, not the image plane",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 202,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12351",
        "abs_url": "https://arxiv.org/abs/2511.12351",
        "pdf_url": "https://arxiv.org/pdf/2511.12351",
        "title": "Dynamic Reward Scaling for Multivariate Time Series Anomaly Detection: A VAE-Enhanced Reinforcement Learning Approach",
        "authors": [
            "Bahareh Golchin",
            "Banafsheh Rekabdar"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Detecting anomalies in multivariate time series is essential for monitoring complex industrial systems, where high dimensionality, limited labeled data, and subtle dependencies between sensors cause significant challenges. This paper presents a deep reinforcement learning framework that combines a Variational Autoencoder (VAE), an LSTM-based Deep Q-Network (DQN), dynamic reward shaping, and an active learning module to address these issues in a unified learning framework. The main contribution is the implementation of Dynamic Reward Scaling for Multivariate Time Series Anomaly Detection (DRSMT), which demonstrates how each component enhances the detection process. The VAE captures compact latent representations and reduces noise. The DQN enables adaptive, sequential anomaly classification, and the dynamic reward shaping balances exploration and exploitation during training by adjusting the importance of reconstruction and classification signals. In addition, active learning identifies the most uncertain samples for labeling, reducing the need for extensive manual supervision. Experiments on two multivariate benchmarks, namely Server Machine Dataset (SMD) and Water Distribution Testbed (WADI), show that the proposed method outperforms existing baselines in F1-score and AU-PR. These results highlight the effectiveness of combining generative modeling, reinforcement learning, and selective supervision for accurate and scalable anomaly detection in real-world multivariate systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 203,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12379",
        "abs_url": "https://arxiv.org/abs/2511.12379",
        "pdf_url": "https://arxiv.org/pdf/2511.12379",
        "title": "Quantum Optimization Algorithms",
        "authors": [
            "Jonas Stein",
            "Maximilian Zorn",
            "Leo Sünkel",
            "Thomas Gabor"
        ],
        "comments": "Preprint submitted to appear in a Springer Nature Book on Combinatorial Optimization using Quantum Computing",
        "subjects": "Quantum Physics (quant-ph); Artificial Intelligence (cs.AI)",
        "abstract": "Quantum optimization allows for up to exponential quantum speedups for specific, possibly industrially relevant problems. As the key algorithm in this field, we motivate and discuss the Quantum Approximate Optimization Algorithm (QAOA), which can be understood as a slightly generalized version of Quantum Annealing for gate-based quantum computers. We delve into the quantum circuit implementation of the QAOA, including Hamiltonian simulation techniques for higher-order Ising models, and discuss parameter training using the parameter shift rule. An example implementation with Pennylane source code demonstrates practical application for the Maximum Cut problem. Further, we show how constraints can be incorporated into the QAOA using Grover mixers, allowing to restrict the search space to strictly valid solutions for specific problems. Finally, we outline the Variational Quantum Eigensolver (VQE) as a generalization of the QAOA, highlighting its potential in the NISQ era and addressing challenges such as barren plateaus and ansatz design.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 204,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12381",
        "abs_url": "https://arxiv.org/abs/2511.12381",
        "pdf_url": "https://arxiv.org/pdf/2511.12381",
        "title": "Don't Think of the White Bear: Ironic Negation in Transformer Models Under Cognitive Load",
        "authors": [
            "Logan Mann",
            "Nayan Saxena",
            "Sarah Tandon",
            "Chenhao Sun",
            "Savar Toteja",
            "Kevin Zhu"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Negation instructions such as 'do not mention $X$' can paradoxically increase the accessibility of $X$ in human thought, a phenomenon known as ironic rebound. Large language models (LLMs) face the same challenge: suppressing a concept requires internally activating it, which may prime rebound instead of avoidance. We investigated this tension with two experiments. \\textbf{(1) Load \\& content}: after a negation instruction, we vary distractor text (semantic, syntactic, repetition) and measure rebound strength. \\textbf{(2) Polarity separation}: We test whether models distinguish neutral from negative framings of the same concept and whether this separation predicts rebound persistence. Results show that rebound consistently arises immediately after negation and intensifies with longer or semantic distractors, while repetition supports suppression. Stronger polarity separation correlates with more persistent rebound. Together, these findings, complemented by a circuit tracing analysis that identifies sparse middle-layer attention heads amplifying forbidden tokens while early layers suppress, link cognitive predictions of ironic rebound with mechanistic insights into long-context interference. To support future work, we release ReboundBench, a dataset of $5,000$ systematically varied negation prompts designed to probe rebound in LLMs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 205,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12387",
        "abs_url": "https://arxiv.org/abs/2511.12387",
        "pdf_url": "https://arxiv.org/pdf/2511.12387",
        "title": "From Phonemes to Meaning: Evaluating Large Language Models on Tamil",
        "authors": [
            "Jeyarajalingam Varsha",
            "Menan Velayuthan",
            "Sumirtha Karunakaran",
            "Rasan Nivethiga",
            "Kengatharaiyer Sarveswaran"
        ],
        "comments": "11 pages",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Large Language Models (LLMs) have shown strong generalization across tasks in high-resource languages; however, their linguistic competence in low-resource and morphologically rich languages such as Tamil remains largely unexplored. Existing multilingual benchmarks often rely on translated English datasets, failing to capture the linguistic and cultural nuances of the target language. To address this gap, we introduce ILAKKANAM, the first Tamil-specific linguistic evaluation benchmark manually curated using 820 questions from Sri Lankan school-level Tamil subject examination papers. Each question is annotated by trained linguists under five linguistic categories and a factual knowledge category, spanning Grades 1--13 to ensure broad linguistic coverage. We evaluate both closed-source and open-source LLMs using a standardized evaluation framework. Our results show that Gemini 2.5 achieves the highest overall performance, while open-source models lag behind, highlighting the gap in linguistic grounding. Category- and grade-wise analyses reveal that all models perform well on lower-grade questions but show a clear decline as linguistic complexity increases. Further, no strong correlation is observed between a model's overall performance and its ability to identify linguistic categories, suggesting that performance may be driven by exposure rather than genuine understanding.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 206,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12404",
        "abs_url": "https://arxiv.org/abs/2511.12404",
        "pdf_url": "https://arxiv.org/pdf/2511.12404",
        "title": "SynthGuard: An Open Platform for Detecting AI-Generated Multimedia with Multimodal LLMs",
        "authors": [
            "Shail Desai",
            "Aditya Pawar",
            "Li Lin",
            "Xin Wang",
            "Shu Hu"
        ],
        "comments": "",
        "subjects": "Multimedia (cs.MM); Artificial Intelligence (cs.AI); Sound (cs.SD)",
        "abstract": "Artificial Intelligence (AI) has made it possible for anyone to create images, audio, and video with unprecedented ease, enriching education, communication, and creative expression. At the same time, the rapid rise of AI-generated media has introduced serious risks, including misinformation, identity misuse, and the erosion of public trust as synthetic content becomes increasingly indistinguishable from real media. Although deepfake detection has advanced, many existing tools remain closed-source, limited in modality, or lacking transparency and educational value, making it difficult for users to understand how detection decisions are made. To address these gaps, we introduce SynthGuard, an open, user-friendly platform for detecting and analyzing AI-generated multimedia using both traditional detectors and multimodal large language models (MLLMs). SynthGuard provides explainable inference, unified image and audio support, and an interactive interface designed to make forensic analysis accessible to researchers, educators, and the public. The SynthGuard platform is available at: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 207,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12438",
        "abs_url": "https://arxiv.org/abs/2511.12438",
        "pdf_url": "https://arxiv.org/pdf/2511.12438",
        "title": "Real-Time Drivers' Drowsiness Detection and Analysis through Deep Learning",
        "authors": [
            "ANK Zaman",
            "Prosenjit Chatterjee",
            "Rajat Sharma"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)",
        "abstract": "A long road trip is fun for drivers. However, a long drive for days can be tedious for a driver to accommodate stringent deadlines to reach distant destinations. Such a scenario forces drivers to drive extra miles, utilizing extra hours daily without sufficient rest and breaks. Once a driver undergoes such a scenario, it occasionally triggers drowsiness during driving. Drowsiness in driving can be life-threatening to any individual and can affect other drivers' safety; therefore, a real-time detection system is needed. To identify fatigued facial characteristics in drivers and trigger the alarm immediately, this research develops a real-time driver drowsiness detection system utilizing deep convolutional neural networks (DCNNs) and this http URL proposed and implemented model takes real- time facial images of a driver using a live camera and utilizes a Python-based library named OpenCV to examine the facial images for facial landmarks like sufficient eye openings and yawn-like mouth movements. The DCNNs framework then gathers the data and utilizes a per-trained model to detect the drowsiness of a driver using facial landmarks. If the driver is identified as drowsy, the system issues a continuous alert in real time, embedded in the Smart Car this http URL potentially saving innocent lives on the roadways, the proposed technique offers a non-invasive, inexpensive, and cost-effective way to identify drowsiness. Our proposed and implemented DCNNs embedded drowsiness detection model successfully react with NTHU-DDD dataset and Yawn-Eye-Dataset with drowsiness detection classification accuracy of 99.6% and 97% respectively.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 208,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12442",
        "abs_url": "https://arxiv.org/abs/2511.12442",
        "pdf_url": "https://arxiv.org/pdf/2511.12442",
        "title": "Global-Lens Transformers: Adaptive Token Mixing for Dynamic Link Prediction",
        "authors": [
            "Tao Zou",
            "Chengfeng Wu",
            "Tianxi Liao",
            "Junchen Ye",
            "Bowen Du"
        ],
        "comments": "Accepted by AAAI 2026",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Dynamic graph learning plays a pivotal role in modeling evolving relationships over time, especially for temporal link prediction tasks in domains such as traffic systems, social networks, and recommendation platforms. While Transformer-based models have demonstrated strong performance by capturing long-range temporal dependencies, their reliance on self-attention results in quadratic complexity with respect to sequence length, limiting scalability on high-frequency or large-scale graphs. In this work, we revisit the necessity of self-attention in dynamic graph modeling. Inspired by recent findings that attribute the success of Transformers more to their architectural design than attention itself, we propose GLFormer, a novel attention-free Transformer-style framework for dynamic graphs. GLFormer introduces an adaptive token mixer that performs context-aware local aggregation based on interaction order and time intervals. To capture long-term dependencies, we further design a hierarchical aggregation module that expands the temporal receptive field by stacking local token mixers across layers. Experiments on six widely-used dynamic graph benchmarks show that GLFormer achieves SOTA performance, which reveals that attention-free architectures can match or surpass Transformer baselines in dynamic graph settings with significantly improved efficiency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 209,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12448",
        "abs_url": "https://arxiv.org/abs/2511.12448",
        "pdf_url": "https://arxiv.org/pdf/2511.12448",
        "title": "SeedAIchemy: LLM-Driven Seed Corpus Generation for Fuzzing",
        "authors": [
            "Aidan Wen",
            "Norah A. Alzahrani",
            "Jingzhi Jiang",
            "Andrew Joe",
            "Karen Shieh",
            "Andy Zhang",
            "Basel Alomair",
            "David Wagner"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
        "abstract": "We introduce SeedAIchemy, an automated LLM-driven corpus generation tool that makes it easier for developers to implement fuzzing effectively. SeedAIchemy consists of five modules which implement different approaches at collecting publicly available files from the internet. Four of the five modules use large language model (LLM) workflows to construct search terms designed to maximize corpus quality. Corpora generated by SeedAIchemy perform significantly better than a naive corpus and similarly to a manually-curated corpus on a diverse range of target programs and libraries.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 210,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12460",
        "abs_url": "https://arxiv.org/abs/2511.12460",
        "pdf_url": "https://arxiv.org/pdf/2511.12460",
        "title": "Personality-guided Public-Private Domain Disentangled Hypergraph-Former Network for Multimodal Depression Detection",
        "authors": [
            "Changzeng Fu",
            "Shiwen Zhao",
            "Yunze Zhang",
            "Zhongquan Jian",
            "Shiqi Zhao",
            "Chaoran Liu"
        ],
        "comments": "AAAI 2026 accepted",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Depression represents a global mental health challenge requiring efficient and reliable automated detection methods. Current Transformer- or Graph Neural Networks (GNNs)-based multimodal depression detection methods face significant challenges in modeling individual differences and cross-modal temporal dependencies across diverse behavioral contexts. Therefore, we propose P$^3$HF (Personality-guided Public-Private Domain Disentangled Hypergraph-Former Network) with three key innovations: (1) personality-guided representation learning using LLMs to transform discrete individual features into contextual descriptions for personalized encoding; (2) Hypergraph-Former architecture modeling high-order cross-modal temporal relationships; (3) event-level domain disentanglement with contrastive learning for improved generalization across behavioral contexts. Experiments on MPDD-Young dataset show P$^3$HF achieves around 10\\% improvement on accuracy and weighted F1 for binary and ternary depression classification task over existing methods. Extensive ablation studies validate the independent contribution of each architectural component, confirming that personality-guided representation learning and high-order hypergraph reasoning are both essential for generating robust, individual-aware depression-related representations. The code is released at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 211,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12472",
        "abs_url": "https://arxiv.org/abs/2511.12472",
        "pdf_url": "https://arxiv.org/pdf/2511.12472",
        "title": "Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing",
        "authors": [
            "Mengying Wang",
            "Chenhui Ma",
            "Ao Jiao",
            "Tuo Liang",
            "Pengjun Lu",
            "Shrinidhi Hegde",
            "Yu Yin",
            "Evren Gurkan-Cavusoglu",
            "Yinghui Wu"
        ],
        "comments": "The 40th AAAI Conference on Artificial Intelligence (AAAI-26)",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Large Language Models (LLMs) have greatly advanced knowledge graph question answering (KGQA), yet existing systems are typically optimized for returning highly relevant but predictable answers. A missing yet desired capacity is to exploit LLMs to suggest surprise and novel (\"serendipitious\") answers. In this paper, we formally define the serendipity-aware KGQA task and propose the SerenQA framework to evaluate LLMs' ability to uncover unexpected insights in scientific KGQA tasks. SerenQA includes a rigorous serendipity metric based on relevance, novelty, and surprise, along with an expert-annotated benchmark derived from the Clinical Knowledge Graph, focused on drug repurposing. Additionally, it features a structured evaluation pipeline encompassing three subtasks: knowledge retrieval, subgraph reasoning, and serendipity exploration. Our experiments reveal that while state-of-the-art LLMs perform well on retrieval, they still struggle to identify genuinely surprising and valuable discoveries, underscoring a significant room for future improvements. Our curated resources and extended version are released at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 212,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12484",
        "abs_url": "https://arxiv.org/abs/2511.12484",
        "pdf_url": "https://arxiv.org/pdf/2511.12484",
        "title": "One Request, Multiple Experts: LLM Orchestrates Domain Specific Models via Adaptive Task Routing",
        "authors": [
            "Xu Yang",
            "Chenhui Lin",
            "Haotian Liu",
            "Qi Wang",
            "Yue Yang",
            "Wenchuan Wu"
        ],
        "comments": "",
        "subjects": "Systems and Control (eess.SY); Artificial Intelligence (cs.AI)",
        "abstract": "With the integration of massive distributed energy resources and the widespread participation of novel market entities, the operation of active distribution networks (ADNs) is progressively evolving into a complex multi-scenario, multi-objective problem. Although expert engineers have developed numerous domain specific models (DSMs) to address distinct technical problems, mastering, integrating, and orchestrating these heterogeneous DSMs still entail considerable overhead for ADN operators. Therefore, an intelligent approach is urgently required to unify these DSMs and enable efficient coordination. To address this challenge, this paper proposes the ADN-Agent architecture, which leverages a general large language model (LLM) to coordinate multiple DSMs, enabling adaptive intent recognition, task decomposition, and DSM invocation. Within the ADN-Agent, we design a novel communication mechanism that provides a unified and flexible interface for diverse heterogeneous DSMs. Finally, for some language-intensive subtasks, we propose an automated training pipeline for fine-tuning small language models, thereby effectively enhancing the overall problem-solving capability of the system. Comprehensive comparisons and ablation experiments validate the efficacy of the proposed method and demonstrate that the ADN-Agent architecture outperforms existing LLM application paradigms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 213,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12487",
        "abs_url": "https://arxiv.org/abs/2511.12487",
        "pdf_url": "https://arxiv.org/pdf/2511.12487",
        "title": "Evolving Prompts for Toxicity Search in Large Language Models",
        "authors": [
            "Onkar Shelar",
            "Travis Desell"
        ],
        "comments": "pre-print",
        "subjects": "Neural and Evolutionary Computing (cs.NE); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Large Language Models remain vulnerable to adversarial prompts that elicit toxic content even after safety alignment. We present ToxSearch, a black-box evolutionary framework that tests model safety by evolving prompts in a synchronous steady-state loop. The system employs a diverse set of operators, including lexical substitutions, negation, back-translation, paraphrasing, and two semantic crossover operators, while a moderation oracle provides fitness guidance. Operator-level analysis shows heterogeneous behavior: lexical substitutions offer the best yield-variance trade-off, semantic-similarity crossover acts as a precise low-throughput inserter, and global rewrites exhibit high variance with elevated refusal costs. Using elite prompts evolved on LLaMA 3.1 8B, we observe practically meaningful but attenuated cross-model transfer, with toxicity roughly halving on most targets, smaller LLaMA 3.2 variants showing the strongest resistance, and some cross-architecture models retaining higher toxicity. These results suggest that small, controllable perturbations are effective vehicles for systematic red-teaming and that defenses should anticipate cross-model reuse of adversarial prompts rather than focusing only on single-model hardening.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 214,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12491",
        "abs_url": "https://arxiv.org/abs/2511.12491",
        "pdf_url": "https://arxiv.org/pdf/2511.12491",
        "title": "Uncover and Unlearn Nuisances: Agnostic Fully Test-Time Adaptation",
        "authors": [
            "Ponhvoan Srey",
            "Yaxin Shi",
            "Hangwei Qian",
            "Jing Li",
            "Ivor W. Tsang"
        ],
        "comments": "26 pages, 4 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Fully Test-Time Adaptation (FTTA) addresses domain shifts without access to source data and training protocols of the pre-trained models. Traditional strategies that align source and target feature distributions are infeasible in FTTA due to the absence of training data and unpredictable target domains. In this work, we exploit a dual perspective on FTTA, and propose Agnostic FTTA (AFTTA) as a novel formulation that enables the usage of off-the-shelf domain transformations during test-time to enable direct generalization to unforeseeable target data. To address this, we develop an uncover-and-unlearn approach. First, we uncover potential unwanted shifts between source and target domains by simulating them through predefined mappings and consider them as nuisances. Then, during test-time prediction, the model is enforced to unlearn these nuisances by regularizing the consequent shifts in latent representations and label predictions. Specifically, a mutual information-based criterion is devised and applied to guide nuisances unlearning in the feature space and encourage confident and consistent prediction in label space. Our proposed approach explicitly addresses agnostic domain shifts, enabling superior model generalization under FTTA constraints. Extensive experiments on various tasks, involving corruption and style shifts, demonstrate that our method consistently outperforms existing approaches.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 215,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12494",
        "abs_url": "https://arxiv.org/abs/2511.12494",
        "pdf_url": "https://arxiv.org/pdf/2511.12494",
        "title": "Towards Better IncomLDL: We Are Unaware of Hidden Labels in Advance",
        "authors": [
            "Jiecheng Jiang",
            "Jiawei Tang",
            "Jiahao Jiang",
            "Hui Liu",
            "Junhui Hou",
            "Yuheng Jia"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Label distribution learning (LDL) is a novel paradigm that describe the samples by label distribution of a sample. However, acquiring LDL dataset is costly and time-consuming, which leads to the birth of incomplete label distribution learning (IncomLDL). All the previous IncomLDL methods set the description degrees of \"missing\" labels in an instance to 0, but remains those of other labels unchanged. This setting is unrealistic because when certain labels are missing, the degrees of the remaining labels will increase accordingly. We fix this unrealistic setting in IncomLDL and raise a new problem: LDL with hidden labels (HidLDL), which aims to recover a complete label distribution from a real-world incomplete label distribution where certain labels in an instance are omitted during annotation. To solve this challenging problem, we discover the significance of proportional information of the observed labels and capture it by an innovative constraint to utilize it during the optimization process. We simultaneously use local feature similarity and the global low-rank structure to reveal the mysterious veil of hidden labels. Moreover, we theoretically give the recovery bound of our method, proving the feasibility of our method in learning from hidden labels. Extensive recovery and predictive experiments on various datasets prove the superiority of our method to state-of-the-art LDL and IncomLDL methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 216,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12497",
        "abs_url": "https://arxiv.org/abs/2511.12497",
        "pdf_url": "https://arxiv.org/pdf/2511.12497",
        "title": "SGuard-v1: Safety Guardrail for Large Language Models",
        "authors": [
            "JoonHo Lee",
            "HyeonMin Cho",
            "Jaewoong Yun",
            "Hyunjae Lee",
            "JunKyu Lee",
            "Juree Seok"
        ],
        "comments": "Technical Report",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)",
        "abstract": "We present SGuard-v1, a lightweight safety guardrail for Large Language Models (LLMs), which comprises two specialized models to detect harmful content and screen adversarial prompts in human-AI conversational settings. The first component, ContentFilter, is trained to identify safety risks in LLM prompts and responses in accordance with the MLCommons hazard taxonomy, a comprehensive framework for trust and safety assessment of AI. The second component, JailbreakFilter, is trained with a carefully designed curriculum over integrated datasets and findings from prior work on adversarial prompting, covering 60 major attack types while mitigating false-unsafe classification. SGuard-v1 is built on the 2B-parameter Granite-3.3-2B-Instruct model that supports 12 languages. We curate approximately 1.4 million training instances from both collected and synthesized data and perform instruction tuning on the base model, distributing the curated data across the two component according to their designated functions. Through extensive evaluation on public and proprietary safety benchmarks, SGuard-v1 achieves state-of-the-art safety performance while remaining lightweight, thereby reducing deployment overhead. SGuard-v1 also improves interpretability for downstream use by providing multi-class safety predictions and their binary confidence scores. We release the SGuard-v1 under the Apache-2.0 License to enable further research and practical deployment in AI safety.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 217,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12523",
        "abs_url": "https://arxiv.org/abs/2511.12523",
        "pdf_url": "https://arxiv.org/pdf/2511.12523",
        "title": "Perturbing Best Responses in Zero-Sum Games",
        "authors": [
            "Adam Dziwoki",
            "Rostislav Horcik"
        ],
        "comments": "Accepted to AAAI 2026",
        "subjects": "Computer Science and Game Theory (cs.GT); Artificial Intelligence (cs.AI)",
        "abstract": "This paper investigates the impact of perturbations on the best-response-based algorithms approximating Nash equilibria in zero-sum games, namely Double Oracle and Fictitious Play. More precisely, we assume that the oracle computing the best responses perturbs the utilities before selecting the best response. We show that using such an oracle reduces the number of iterations for both algorithms. For some cases, suitable perturbations ensure the expected number of iterations is logarithmic. Although the utility perturbation is computationally demanding as it requires iterating through all pure strategies, we demonstrate that one can efficiently perturb the utilities in games where pure strategies have further inner structure.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 218,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12529",
        "abs_url": "https://arxiv.org/abs/2511.12529",
        "pdf_url": "https://arxiv.org/pdf/2511.12529",
        "title": "Accepted with Minor Revisions: Value of AI-Assisted Scientific Writing",
        "authors": [
            "Sanchaita Hazra",
            "Doeun Lee",
            "Bodhisattwa Prasad Majumder",
            "Sachin Kumar"
        ],
        "comments": "",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Large Language Models have seen expanding application across domains, yet their effectiveness as assistive tools for scientific writing -- an endeavor requiring precision, multimodal synthesis, and domain expertise -- remains insufficiently understood. We examine the potential of LLMs to support domain experts in scientific writing, with a focus on abstract composition. We design an incentivized randomized controlled trial with a hypothetical conference setup where participants with relevant expertise are split into an author and reviewer pool. Inspired by methods in behavioral science, our novel incentive structure encourages authors to edit the provided abstracts to an acceptable quality for a peer-reviewed submission. Our 2x2 between-subject design expands into two dimensions: the implicit source of the provided abstract and the disclosure of it. We find authors make most edits when editing human-written abstracts compared to AI-generated abstracts without source attribution, often guided by higher perceived readability in AI generation. Upon disclosure of source information, the volume of edits converges in both source treatments. Reviewer decisions remain unaffected by the source of the abstract, but bear a significant correlation with the number of edits made. Careful stylistic edits, especially in the case of AI-generated abstracts, in the presence of source information, improve the chance of acceptance. We find that AI-generated abstracts hold potential to reach comparable levels of acceptability to human-written ones with minimal revision, and that perceptions of AI authorship, rather than objective quality, drive much of the observed editing behavior. Our findings reverberate the significance of source disclosure in collaborative scientific writing.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 219,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12568",
        "abs_url": "https://arxiv.org/abs/2511.12568",
        "pdf_url": "https://arxiv.org/pdf/2511.12568",
        "title": "Enhancing Machine Learning Model Efficiency through Quantization and Bit Depth Optimization: A Performance Analysis on Healthcare Data",
        "authors": [
            "Mitul Goswami",
            "Romit Chatterjee"
        ],
        "comments": "Published as Chapter 2 in Intelligent and Smart Computing: Applications to Engineering Problems, Cambridge Scholars Publishing (2025). ISBN: 978-1-0364-5886-7",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "This research aims to optimize intricate learning models by implementing quantization and bit-depth optimization techniques. The objective is to significantly cut time complexity while preserving model efficiency, thus addressing the challenge of extended execution times in intricate models. Two medical datasets were utilized as case studies to apply a Logistic Regression (LR) machine learning model. Using efficient quantization and bit depth optimization strategies the input data is downscaled from float64 to float32 and int32. The results demonstrated a significant reduction in time complexity, with only a minimal decrease in model accuracy post-optimization, showcasing the state-of-the-art optimization approach. This comprehensive study concludes that the impact of these optimization techniques varies depending on a set of parameters.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 220,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12573",
        "abs_url": "https://arxiv.org/abs/2511.12573",
        "pdf_url": "https://arxiv.org/pdf/2511.12573",
        "title": "Mitigating Length Bias in RLHF through a Causal Lens",
        "authors": [
            "Hyeonji Kim",
            "Sujeong Oh",
            "Sanghack Lee"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Reinforcement learning from human feedback (RLHF) is widely used to align large language models (LLMs) with human preferences. However, RLHF-trained reward models often exhibit length bias -- a systematic tendency to favor longer responses by conflating verbosity with quality. We propose a causal framework for analyzing and mitigating length bias in RLHF reward modeling. Central to our approach is a counterfactual data augmentation method that generates response pairs designed to isolate content quality from verbosity. These counterfactual examples are then used to train the reward model, enabling it to assess responses based on content quality independently of verbosity. Specifically, we construct (1) length-divergent pairs with similar content and (2) content-divergent pairs of similar length. Empirical evaluations show that our method reduces length bias in reward assignment and leads to more concise, content-focused outputs from the policy model. These findings demonstrate that the proposed approach effectively reduces length bias and improves the robustness and content sensitivity of reward modeling in RLHF pipelines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 221,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12592",
        "abs_url": "https://arxiv.org/abs/2511.12592",
        "pdf_url": "https://arxiv.org/pdf/2511.12592",
        "title": "Knowledge is Overrated: A zero-knowledge machine learning and cryptographic hashing-based framework for verifiable, low latency inference at the LHC",
        "authors": [
            "Pratik Jawahar",
            "Caterina Doglioni",
            "Maurizio Pierini"
        ],
        "comments": "ML4PS NeurIPS 2025",
        "subjects": "High Energy Physics - Experiment (hep-ex); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "Low latency event-selection (trigger) algorithms are essential components of Large Hadron Collider (LHC) operation. Modern machine learning (ML) models have shown great offline performance as classifiers and could improve trigger performance, thereby improving downstream physics analyses. However, inference on such large models does not satisfy the $40\\text{MHz}$ online latency constraint at the LHC. In this work, we propose \\texttt{PHAZE}, a novel framework built on cryptographic techniques like hashing and zero-knowledge machine learning (zkML) to achieve low latency inference, via a certifiable, early-exit mechanism from an arbitrarily large baseline model. We lay the foundations for such a framework to achieve nanosecond-order latency and discuss its inherent advantages, such as built-in anomaly detection, within the scope of LHC triggers, as well as its potential to enable a dynamic low-level trigger in the future.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 222,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12596",
        "abs_url": "https://arxiv.org/abs/2511.12596",
        "pdf_url": "https://arxiv.org/pdf/2511.12596",
        "title": "Group-Aware Reinforcement Learning for Output Diversity in Large Language Models",
        "authors": [
            "Oron Anschel",
            "Alon Shoshan",
            "Adam Botach",
            "Shunit Haviv Hakimi",
            "Asaf Gendler",
            "Emanuel Ben Baruch",
            "Nadav Bhonker",
            "Igor Kviatkovsky",
            "Manoj Aggarwal",
            "Gerard Medioni"
        ],
        "comments": "EMNLP Main 2025",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Large Language Models (LLMs) often suffer from mode collapse, repeatedly generating the same few completions even when many valid answers exist, limiting their diversity across a wide range of tasks. We introduce Group-Aware Policy Optimization (GAPO), a simple extension of the recent and popular Group Relative Policy Optimization (GRPO) that computes rewards over the group as a whole. GAPO enables learning from the group-level properties such as diversity and coverage. We demonstrate GAPO using a frequency-aware reward function that encourages uniform sampling over valid LLM completions, and show that GAPO-trained models produce valid and more diverse model responses. Beyond this setup, GAPO generalizes to open-ended prompts and improves response diversity without compromising accuracy on standard LLM benchmarks (GSM8K, MATH, HumanEval, MMLU-Pro). Our code will be made publicly available.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 223,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12601",
        "abs_url": "https://arxiv.org/abs/2511.12601",
        "pdf_url": "https://arxiv.org/pdf/2511.12601",
        "title": "Symmetry-Aware Graph Metanetwork Autoencoders: Model Merging through Parameter Canonicalization",
        "authors": [
            "Odysseas Boufalis",
            "Jorge Carrasco-Pollo",
            "Joshua Rosenthal",
            "Eduardo Terres-Caballero",
            "Alejandro García-Castellanos"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Neural network parameterizations exhibit inherent symmetries that yield multiple equivalent minima within the loss landscape. Scale Graph Metanetworks (ScaleGMNs) explicitly leverage these symmetries by proposing an architecture equivariant to both permutation and parameter scaling transformations. Previous work by Ainsworth et al. (2023) addressed permutation symmetries through a computationally intensive combinatorial assignment problem, demonstrating that leveraging permutation symmetries alone can map networks into a shared loss basin. In this work, we extend their approach by also incorporating scaling symmetries, presenting an autoencoder framework utilizing ScaleGMNs as invariant encoders. Experimental results demonstrate that our method aligns Implicit Neural Representations (INRs) and Convolutional Neural Networks (CNNs) under both permutation and scaling symmetries without explicitly solving the assignment problem. This approach ensures that similar networks naturally converge within the same basin, facilitating model merging, i.e., smooth linear interpolation while avoiding regions of high loss. The code is publicly available on our GitHub repository.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 224,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12603",
        "abs_url": "https://arxiv.org/abs/2511.12603",
        "pdf_url": "https://arxiv.org/pdf/2511.12603",
        "title": "PID-controlled Langevin Dynamics for Faster Sampling of Generative Models",
        "authors": [
            "Hongyi Chen",
            "Jianhai Shu",
            "Jingtao Ding",
            "Yong Li",
            "Xiao-Ping Zhang"
        ],
        "comments": "NeurIPS 2025 poster paper",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Systems and Control (eess.SY)",
        "abstract": "Langevin dynamics sampling suffers from extremely low generation speed, fundamentally limited by numerous fine-grained iterations to converge to the target distribution. We introduce PID-controlled Langevin Dynamics (PIDLD), a novel sampling acceleration algorithm that reinterprets the sampling process using control-theoretic principles. By treating energy gradients as feedback signals, PIDLD combines historical gradients (the integral term) and gradient trends (the derivative term) to efficiently traverse energy landscapes and adaptively stabilize, thereby significantly reducing the number of iterations required to produce high-quality samples. Our approach requires no additional training, datasets, or prior information, making it immediately integrable with any Langevin-based method. Extensive experiments across image generation and reasoning tasks demonstrate that PIDLD achieves higher quality with fewer steps, making Langevin-based generative models more practical for efficiency-critical applications. The implementation can be found at \\href{this https URL}{this https URL}.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 225,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12609",
        "abs_url": "https://arxiv.org/abs/2511.12609",
        "pdf_url": "https://arxiv.org/pdf/2511.12609",
        "title": "Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data",
        "authors": [
            "Yunxin Li",
            "Xinyu Chen",
            "Shenyuan Jiang",
            "Haoyuan Shi",
            "Zhenyu Liu",
            "Xuanyu Zhang",
            "Nanhao Deng",
            "Zhenran Xu",
            "Yicheng Ma",
            "Meishan Zhang",
            "Baotian Hu",
            "Min Zhang"
        ],
        "comments": "47 pages,10 Figures, Project Website: this https URL Codes: this https URL",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee's Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the Qwen2.5-7B dense architecture, we build Uni-MoE-2.0-Omni from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech. Architecturally, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer. For training, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning. Data-wise, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues. Extensive evaluation across 85 benchmarks demonstrates that our model achieves SOTA or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (reducing WER by 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 226,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12614",
        "abs_url": "https://arxiv.org/abs/2511.12614",
        "pdf_url": "https://arxiv.org/pdf/2511.12614",
        "title": "OPFormer: Object Pose Estimation leveraging foundation model with geometric encoding",
        "authors": [
            "Artem Moroz",
            "Vít Zeman",
            "Martin Mikšík",
            "Elizaveta Isianova",
            "Miroslav David",
            "Pavel Burget",
            "Varun Burde"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)",
        "abstract": "We introduce a unified, end-to-end framework that seamlessly integrates object detection and pose estimation with a versatile onboarding process. Our pipeline begins with an onboarding stage that generates object representations from either traditional 3D CAD models or, in their absence, by rapidly reconstructing a high-fidelity neural representation (NeRF) from multi-view images. Given a test image, our system first employs the CNOS detector to localize target objects. For each detection, our novel pose estimation module, OPFormer, infers the precise 6D pose. The core of OPFormer is a transformer-based architecture that leverages a foundation model for robust feature extraction. It uniquely learns a comprehensive object representation by jointly encoding multiple template views and enriches these features with explicit 3D geometric priors using Normalized Object Coordinate Space (NOCS). A decoder then establishes robust 2D-3D correspondences to determine the final pose. Evaluated on the challenging BOP benchmarks, our integrated system demonstrates a strong balance between accuracy and efficiency, showcasing its practical applicability in both model-based and model-free scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 227,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12627",
        "abs_url": "https://arxiv.org/abs/2511.12627",
        "pdf_url": "https://arxiv.org/pdf/2511.12627",
        "title": "C3Net: Context-Contrast Network for Camouflaged Object Detection",
        "authors": [
            "Baber Jan",
            "Aiman H. El-Maleh",
            "Abdul Jabbar Siddiqui",
            "Abdul Bais",
            "Saeed Anwar"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Image and Video Processing (eess.IV)",
        "abstract": "Camouflaged object detection identifies objects that blend seamlessly with their surroundings through similar colors, textures, and patterns. This task challenges both traditional segmentation methods and modern foundation models, which fail dramatically on camouflaged objects. We identify six fundamental challenges in COD: Intrinsic Similarity, Edge Disruption, Extreme Scale Variation, Environmental Complexities, Contextual Dependencies, and Salient-Camouflaged Object Disambiguation. These challenges frequently co-occur and compound the difficulty of detection, requiring comprehensive architectural solutions. We propose C3Net, which addresses all challenges through a specialized dual-pathway decoder architecture. The Edge Refinement Pathway employs gradient-initialized Edge Enhancement Modules to recover precise boundaries from early features. The Contextual Localization Pathway utilizes our novel Image-based Context Guidance mechanism to achieve intrinsic saliency suppression without external models. An Attentive Fusion Module synergistically combines the two pathways via spatial gating. C3Net achieves state-of-the-art performance with S-measures of 0.898 on COD10K, 0.904 on CAMO, and 0.913 on NC4K, while maintaining efficient processing. C3Net demonstrates that complex, multifaceted detection challenges require architectural innovation, with specialized components working synergistically to achieve comprehensive coverage beyond isolated improvements. Code, model weights, and results are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 228,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12630",
        "abs_url": "https://arxiv.org/abs/2511.12630",
        "pdf_url": "https://arxiv.org/pdf/2511.12630",
        "title": "Knots: A Large-Scale Multi-Agent Enhanced Expert-Annotated Dataset and LLM Prompt Optimization for NOTAM Semantic Parsing",
        "authors": [
            "Maoqi Liu",
            "Quan Fang",
            "Yang Yang",
            "Can Zhao",
            "Kaiquan Cai"
        ],
        "comments": "Accepted to Advanced Engineering Informatics",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Notice to Air Missions (NOTAMs) serve as a critical channel for disseminating key flight safety information, yet their complex linguistic structures and implicit reasoning pose significant challenges for automated parsing. Existing research mainly focuses on surface-level tasks such as classification and named entity recognition, lacking deep semantic understanding. To address this gap, we propose NOTAM semantic parsing, a task emphasizing semantic inference and the integration of aviation domain knowledge to produce structured, inference-rich outputs. To support this task, we construct Knots (Knowledge and NOTAM Semantics), a high-quality dataset of 12,347 expert-annotated NOTAMs covering 194 Flight Information Regions, enhanced through a multi-agent collaborative framework for comprehensive field discovery. We systematically evaluate a wide range of prompt-engineering strategies and model-adaptation techniques, achieving substantial improvements in aviation text understanding and processing. Our experimental results demonstrate the effectiveness of the proposed approach and offer valuable insights for automated NOTAM analysis systems. Our code is available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 229,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12631",
        "abs_url": "https://arxiv.org/abs/2511.12631",
        "pdf_url": "https://arxiv.org/pdf/2511.12631",
        "title": "Multivariate Diffusion Transformer with Decoupled Attention for High-Fidelity Mask-Text Collaborative Facial Generation",
        "authors": [
            "Yushe Cao",
            "Dianxi Shi",
            "Xing Fu",
            "Xuechao Zou",
            "Haikuo Peng",
            "Xueqi Li",
            "Chun Yu",
            "Junliang Xing"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "While significant progress has been achieved in multimodal facial generation using semantic masks and textual descriptions, conventional feature fusion approaches often fail to enable effective cross-modal interactions, thereby leading to suboptimal generation outcomes. To address this challenge, we introduce MDiTFace--a customized diffusion transformer framework that employs a unified tokenization strategy to process semantic mask and text inputs, eliminating discrepancies between heterogeneous modality representations. The framework facilitates comprehensive multimodal feature interaction through stacked, newly designed multivariate transformer blocks that process all conditions synchronously. Additionally, we design a novel decoupled attention mechanism by dissociating implicit dependencies between mask tokens and temporal embeddings. This mechanism segregates internal computations into dynamic and static pathways, enabling caching and reuse of features computed in static pathways after initial calculation, thereby reducing additional computational overhead introduced by mask condition by over 94% while maintaining performance. Extensive experiments demonstrate that MDiTFace significantly outperforms other competing methods in terms of both facial fidelity and conditional consistency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 230,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12635",
        "abs_url": "https://arxiv.org/abs/2511.12635",
        "pdf_url": "https://arxiv.org/pdf/2511.12635",
        "title": "LLM4SCREENLIT: Recommendations on Assessing the Performance of Large Language Models for Screening Literature in Systematic Reviews",
        "authors": [
            "Lech Madeyski",
            "Barbara Kitchenham",
            "Martin Shepperd"
        ],
        "comments": "19 pages, 4 figures",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Context: Large language models (LLMs) are released faster than users' ability to evaluate them rigorously. When LLMs underpin research, such as identifying relevant literature for systematic reviews (SRs), robust empirical assessment is essential. Objective: We identify and discuss key challenges in assessing LLM performance for selecting relevant literature, identify good (evaluation) practices, and propose recommendations. Method: Using a recent large-scale study as an example, we identify problems with the use of traditional metrics for assessing the performance of Gen-AI tools for identifying relevant literature in SRs. We analyzed 27 additional papers investigating this issue, extracted the performance metrics, and found both good practices and widespread problems, especially with the use and reporting of performance metrics for SR screening. Results: Major weaknesses included: i) a failure to use metrics that are robust to imbalanced data and do not directly indicate whether results are better than chance, e.g., the use of Accuracy, ii) a failure to consider the impact of lost evidence when making claims concerning workload savings, and iii) pervasive failure to report the full confusion matrix (or performance metrics from which it can be reconstructed) which is essential for future meta-analyses. On the positive side, we extract good (evaluation) practices on which our recommendations for researchers and practitioners, as well as policymakers, are built. Conclusions: SR screening evaluations should prioritize lost evidence/recall alongside chance-anchored and cost-sensitive Weighted MCC (WMCC) metric, report complete confusion matrices, treat unclassifiable outputs as referred-back positives for assessment, adopt leakage-aware designs with non-LLM baselines and open artifacts, and ground conclusions in cost-benefit analysis where FNs carry higher penalties than FPs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 231,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12648",
        "abs_url": "https://arxiv.org/abs/2511.12648",
        "pdf_url": "https://arxiv.org/pdf/2511.12648",
        "title": "Scalable Hierarchical AI-Blockchain Framework for Real-Time Anomaly Detection in Large-Scale Autonomous Vehicle Networks",
        "authors": [
            "Rathin Chandra Shit",
            "Sharmila Subudhi"
        ],
        "comments": "Submitted to the Journal",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "The security of autonomous vehicle networks is facing major challenges, owing to the complexity of sensor integration, real-time performance demands, and distributed communication protocols that expose vast attack surfaces around both individual and network-wide safety. Existing security schemes are unable to provide sub-10 ms (milliseconds) anomaly detection and distributed coordination of large-scale networks of vehicles within an acceptable safety/privacy framework. This paper introduces a three-tier hybrid security architecture HAVEN (Hierarchical Autonomous Vehicle Enhanced Network), which decouples real-time local threat detection and distributed coordination operations. It incorporates a light ensemble anomaly detection model on the edge (first layer), Byzantine-fault-tolerant federated learning to aggregate threat intelligence at a regional scale (middle layer), and selected blockchain mechanisms (top layer) to ensure critical security coordination. Extensive experimentation is done on a real-world autonomous driving dataset. Large-scale simulations with the number of vehicles ranging between 100 and 1000 and different attack types, such as sensor spoofing, jamming, and adversarial model poisoning, are conducted to test the scalability and resiliency of HAVEN. Experimental findings show sub-10 ms detection latency with an accuracy of 94% and F1-score of 92% across multimodal sensor data, Byzantine fault tolerance validated with 20\\% compromised nodes, and a reduced blockchain storage overhead, guaranteeing sufficient differential privacy. The proposed framework overcomes the important trade-off between real-time safety obligation and distributed security coordination with novel three-tiered processing. The scalable architecture of HAVEN is shown to provide great improvement in detection accuracy as well as network resilience over other methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 232,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12663",
        "abs_url": "https://arxiv.org/abs/2511.12663",
        "pdf_url": "https://arxiv.org/pdf/2511.12663",
        "title": "FLClear: Visually Verifiable Multi-Client Watermarking for Federated Learning",
        "authors": [
            "Chen Gu",
            "Yingying Sun",
            "Yifan She",
            "Donghui Hu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)",
        "abstract": "Federated learning (FL) enables multiple clients to collaboratively train a shared global model while preserving the privacy of their local data. Within this paradigm, the intellectual property rights (IPR) of client models are critical assets that must be protected. In practice, the central server responsible for maintaining the global model may maliciously manipulate the global model to erase client contributions or falsely claim sole ownership, thereby infringing on clients' IPR. Watermarking has emerged as a promising technique for asserting model ownership and protecting intellectual property. However, existing FL watermarking approaches remain limited, suffering from potential watermark collisions among clients, insufficient watermark security, and non-intuitive verification mechanisms. In this paper, we propose FLClear, a novel framework that simultaneously achieves collision-free watermark aggregation, enhanced watermark security, and visually interpretable ownership verification. Specifically, FLClear introduces a transposed model jointly optimized with contrastive learning to integrate the watermarking and main task objectives. During verification, the watermark is reconstructed from the transposed model and evaluated through both visual inspection and structural similarity metrics, enabling intuitive and quantitative ownership verification. Comprehensive experiments conducted over various datasets, aggregation schemes, and attack scenarios demonstrate the effectiveness of FLClear and confirm that it consistently outperforms state-of-the-art FL watermarking methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 233,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12668",
        "abs_url": "https://arxiv.org/abs/2511.12668",
        "pdf_url": "https://arxiv.org/pdf/2511.12668",
        "title": "AI Bill of Materials and Beyond: Systematizing Security Assurance through the AI Risk Scanning (AIRS) Framework",
        "authors": [
            "Samuel Nathanson",
            "Alexander Lee",
            "Catherine Chen Kieffer",
            "Jared Junkin",
            "Jessica Ye",
            "Amir Saeed",
            "Melanie Lockhart",
            "Russ Fink",
            "Elisha Peterson",
            "Lanier Watkins"
        ],
        "comments": "13 pages, 4 figures, 6 tables",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Assurance for artificial intelligence (AI) systems remains fragmented across software supply-chain security, adversarial machine learning, and governance documentation. Existing transparency mechanisms - including Model Cards, Datasheets, and Software Bills of Materials (SBOMs) - advance provenance reporting but rarely provide verifiable, machine-readable evidence of model security. This paper introduces the AI Risk Scanning (AIRS) Framework, a threat-model-based, evidence-generating framework designed to operationalize AI assurance. The AIRS Framework evolved through three progressive pilot studies - Smurf (AIBOM schema design), OPAL (operational validation), and Pilot C (AIRS) - that reframed AI documentation from descriptive disclosure toward measurable, evidence-bound verification. The framework aligns its assurance fields to the MITRE ATLAS adversarial ML taxonomy and automatically produces structured artifacts capturing model integrity, packaging and serialization safety, structural adapters, and runtime behaviors. Currently, the AIRS Framework is scoped to provide model-level assurances for LLMs, but it could be expanded to include other modalities and cover system-level threats (e.g. application-layer abuses, tool-calling). A proof-of-concept on a quantized GPT-OSS-20B model demonstrates enforcement of safe loader policies, per-shard hash verification, and contamination and backdoor probes executed under controlled runtime conditions. Comparative analysis with SBOM standards of SPDX 3.0 and CycloneDX 1.6 reveals alignment on identity and evaluation metadata, but identifies critical gaps in representing AI-specific assurance fields. The AIRS Framework thus extends SBOM practice to the AI domain by coupling threat modeling with automated, auditable evidence generation, providing a principled foundation for standardized, trustworthy, and machine-verifiable AI risk documentation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 234,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12676",
        "abs_url": "https://arxiv.org/abs/2511.12676",
        "pdf_url": "https://arxiv.org/pdf/2511.12676",
        "title": "BridgeEQA: Virtual Embodied Agents for Real Bridge Inspections",
        "authors": [
            "Subin Varghese",
            "Joshua Gao",
            "Asad Ur Rahman",
            "Vedhus Hoskere"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Deploying embodied agents that can answer questions about their surroundings in realistic real-world settings remains difficult, partly due to the scarcity of benchmarks that faithfully capture practical operating conditions. We propose infrastructure inspection as a compelling domain for open-vocabulary Embodied Question Answering (EQA): it naturally demands multi-scale reasoning, long-range spatial understanding, and complex semantic relationships, while offering unique evaluation advantages via standardized National Bridge Inventory (NBI) condition ratings (0-9), professional inspection reports, and egocentric imagery. We introduce BridgeEQA, a benchmark of 2,200 open-vocabulary question-answer pairs (in the style of OpenEQA) grounded in professional inspection reports across 200 real-world bridge scenes with 47.93 images on average per scene. Questions require synthesizing visual evidence across multiple images and aligning responses with NBI condition ratings. We further propose a new EQA metric Image Citation Relevance to evaluate the ability of a model to cite relevant images. Evaluations of state-of-the-art vision-language models reveal substantial performance gaps under episodic memory EQA settings. To address this, we propose Embodied Memory Visual Reasoning (EMVR), which formulates inspection as sequential navigation over an image-based scene graph: images are nodes, and an agent takes actions to traverse views, compare evidence, and reason within a Markov decision process. EMVR shows strong performance over the baselines. We publicly release both the dataset and code.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 235,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12690",
        "abs_url": "https://arxiv.org/abs/2511.12690",
        "pdf_url": "https://arxiv.org/pdf/2511.12690",
        "title": "Improving Direct Persian-English Speech-to-Speech Translation with Discrete Units and Synthetic Parallel Data",
        "authors": [
            "Sina Rashidi",
            "Hossein Sameti"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Direct speech-to-speech translation (S2ST), in which all components are trained jointly, is an attractive alternative to cascaded systems because it offers a simpler pipeline and lower inference latency. However, direct S2ST models require large amounts of parallel speech data in the source and target languages, which are rarely available for low-resource languages such as Persian. This paper presents a direct S2ST system for translating Persian speech into English speech, as well as a pipeline for synthetic parallel Persian-English speech generation. The model comprises three components: (1) a conformer-based encoder, initialized from self-supervised pre-training, maps source speech to high-level acoustic representations; (2) a causal transformer decoder with relative position multi-head attention translates these representations into discrete target speech units; (3) a unit-based neural vocoder generates waveforms from the predicted discrete units. To mitigate the data scarcity problem, we construct a new Persian-English parallel speech corpus by translating Persian speech transcriptions into English using a large language model and then synthesizing the corresponding English speech with a state-of-the-art zero-shot text-to-speech system. The resulting corpus increases the amount of available parallel speech by roughly a factor of six. On the Persian-English portion of the CVSS corpus, the proposed model achieves improvement of 4.6 ASR BLEU with the synthetic data over direct baselines. These results indicate that combining self-supervised pre-training, discrete speech units, and synthetic parallel data is effective for improving direct S2ST in low-resource language pairs such as Persian-English",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 236,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12691",
        "abs_url": "https://arxiv.org/abs/2511.12691",
        "pdf_url": "https://arxiv.org/pdf/2511.12691",
        "title": "R$^{2}$Seg: Training-Free OOD Medical Tumor Segmentation via Anatomical Reasoning and Statistical Rejection",
        "authors": [
            "Shuaike Shen",
            "Ke Liu",
            "Jiaqing Xie",
            "Shangde Gao",
            "Chunhua Shen",
            "Ge Liu",
            "Mireia Crispin-Ortuzar",
            "Shangqi Gao"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Foundation models for medical image segmentation struggle under out-of-distribution (OOD) shifts, often producing fragmented false positives on OOD tumors. We introduce R$^{2}$Seg, a training-free framework for robust OOD tumor segmentation that operates via a two-stage Reason-and-Reject process. First, the Reason step employs an LLM-guided anatomical reasoning planner to localize organ anchors and generate multi-scale ROIs. Second, the Reject step applies two-sample statistical testing to candidates generated by a frozen foundation model (BiomedParse) within these ROIs. This statistical rejection filter retains only candidates significantly different from normal tissue, effectively suppressing false positives. Our framework requires no parameter updates, making it compatible with zero-update test-time augmentation and avoiding catastrophic forgetting. On multi-center and multi-modal tumor segmentation benchmarks, R$^{2}$Seg substantially improves Dice, specificity, and sensitivity over strong baselines and the original foundation models. Code are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 237,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12693",
        "abs_url": "https://arxiv.org/abs/2511.12693",
        "pdf_url": "https://arxiv.org/pdf/2511.12693",
        "title": "HEDGE: Hallucination Estimation via Dense Geometric Entropy for VQA with Vision-Language Models",
        "authors": [
            "Sushant Gautam",
            "Michael A. Riegler",
            "Pål Halvorsen"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Vision-language models (VLMs) enable open-ended visual question answering but remain prone to hallucinations. We present HEDGE, a unified framework for hallucination detection that combines controlled visual perturbations, semantic clustering, and robust uncertainty metrics. HEDGE integrates sampling, distortion synthesis, clustering (entailment- and embedding-based), and metric computation into a reproducible pipeline applicable across multimodal architectures. Evaluations on VQA-RAD and KvasirVQA-x1 with three representative VLMs (LLaVA-Med, Med-Gemma, Qwen2.5-VL) reveal clear architecture- and prompt-dependent trends. Hallucination detectability is highest for unified-fusion models with dense visual tokenization (Qwen2.5-VL) and lowest for architectures with restricted tokenization (Med-Gemma). Embedding-based clustering often yields stronger separation when applied directly to the generated answers, whereas NLI-based clustering remains advantageous for LLaVA-Med and for longer, sentence-level responses. Across configurations, the VASE metric consistently provides the most robust hallucination signal, especially when paired with embedding clustering and a moderate sampling budget (n ~ 10-15). Prompt design also matters: concise, label-style outputs offer clearer semantic structure than syntactically constrained one-sentence responses. By framing hallucination detection as a geometric robustness problem shaped jointly by sampling scale, prompt structure, model architecture, and clustering strategy, HEDGE provides a principled, compute-aware foundation for evaluating multimodal reliability. The hedge-bench PyPI library enables reproducible and extensible benchmarking, with full code and experimental resources available at this https URL .",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 238,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12695",
        "abs_url": "https://arxiv.org/abs/2511.12695",
        "pdf_url": "https://arxiv.org/pdf/2511.12695",
        "title": "A Closer Look at Personalized Fine-Tuning in Heterogeneous Federated Learning",
        "authors": [
            "Minghui Chen",
            "Hrad Ghoukasian",
            "Ruinan Jin",
            "Zehua Wang",
            "Sai Praneeth Karimireddy",
            "Xiaoxiao Li"
        ],
        "comments": "33 pages, 6 figures, 8 tables",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (stat.ML)",
        "abstract": "Federated Learning (FL) enables decentralized, privacy-preserving model training but struggles to balance global generalization and local personalization due to non-identical data distributions across clients. Personalized Fine-Tuning (PFT), a popular post-hoc solution, fine-tunes the final global model locally but often overfits to skewed client distributions or fails under domain shifts. We propose adapting Linear Probing followed by full Fine-Tuning (LP-FT), a principled centralized strategy for alleviating feature distortion (Kumar et al., 2022), to the FL setting. Through systematic evaluation across seven datasets and six PFT variants, we demonstrate LP-FT's superiority in balancing personalization and generalization. Our analysis uncovers federated feature distortion, a phenomenon where local fine-tuning destabilizes globally learned features, and theoretically characterizes how LP-FT mitigates this via phased parameter updates. We further establish conditions (e.g., partial feature overlap, covariate-concept shift) under which LP-FT outperforms standard fine-tuning, offering actionable guidelines for deploying robust personalization in FL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 239,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12706",
        "abs_url": "https://arxiv.org/abs/2511.12706",
        "pdf_url": "https://arxiv.org/pdf/2511.12706",
        "title": "Beyond Fixed Tasks: Unsupervised Environment Design for Task-Level Pairs",
        "authors": [
            "Daniel Furelos-Blanco",
            "Charles Pert",
            "Frederik Kelbel",
            "Alex F. Spies",
            "Alessandra Russo",
            "Michael Dennis"
        ],
        "comments": "Extended version of paper accepted for publication at the 40th AAAI Conference on Artificial Intelligence (AAAI)",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Training general agents to follow complex instructions (tasks) in intricate environments (levels) remains a core challenge in reinforcement learning. Random sampling of task-level pairs often produces unsolvable combinations, highlighting the need to co-design tasks and levels. While unsupervised environment design (UED) has proven effective at automatically designing level curricula, prior work has only considered a fixed task. We present ATLAS (Aligning Tasks and Levels for Autocurricula of Specifications), a novel method that generates joint autocurricula over tasks and levels. Our approach builds upon UED to automatically produce solvable yet challenging task-level pairs for policy training. To evaluate ATLAS and drive progress in the field, we introduce an evaluation suite that models tasks as reward machines in Minigrid levels. Experiments demonstrate that ATLAS vastly outperforms random sampling approaches, particularly when sampling solvable pairs is unlikely. We further show that mutations leveraging the structure of both tasks and levels accelerate convergence to performant policies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 240,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12709",
        "abs_url": "https://arxiv.org/abs/2511.12709",
        "pdf_url": "https://arxiv.org/pdf/2511.12709",
        "title": "Adaptive Graph Rewiring to Mitigate Over-Squashing in Mesh-Based GNNs for Fluid Dynamics Simulations",
        "authors": [
            "Sangwoo Seo",
            "Hyunsung Kim",
            "Jiwan Kim",
            "Chanyoung Park"
        ],
        "comments": "Preprint",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Mesh-based simulation using Graph Neural Networks (GNNs) has been recognized as a promising approach for modeling fluid dynamics. However, the mesh refinement techniques which allocate finer resolution to regions with steep gradients can induce the over-squashing problem in mesh-based GNNs, which prevents the capture of long-range physical interactions. Conventional graph rewiring methods attempt to alleviate this issue by adding new edges, but they typically complete all rewiring operations before applying them to the GNN. These approaches are physically unrealistic, as they assume instantaneous interactions between distant nodes and disregard the distance information between particles. To address these limitations, we propose a novel framework, called Adaptive Graph Rewiring in Mesh-Based Graph Neural Networks (AdaMeshNet), that introduces an adaptive rewiring process into the message-passing procedure to model the gradual propagation of physical interactions. Our method computes a rewiring delay score for bottleneck nodes in the mesh graph, based on the shortest-path distance and the velocity difference. Using this score, it dynamically selects the message-passing layer at which new edges are rewired, which can lead to adaptive rewiring in a mesh graph. Extensive experiments on mesh-based fluid simulations demonstrate that AdaMeshNet outperforms conventional rewiring methods, effectively modeling the sequential nature of physical interactions and enabling more accurate predictions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 241,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12712",
        "abs_url": "https://arxiv.org/abs/2511.12712",
        "pdf_url": "https://arxiv.org/pdf/2511.12712",
        "title": "Adaptive Focus Memory for Language Models",
        "authors": [
            "Christopher Cruz"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Large language models (LLMs) are increasingly deployed in multi-turn dialogue settings, but their behavior is still bottlenecked by fixed context windows and naive memory strategies. Replaying the full conversation at every turn is simple but expensive, while static summarization or recency-only heuristics often erase safety-critical user details. We present Adaptive Focus Memory (AFM), a dynamic context manager that assigns each past message one of three fidelity levels -- FULL, COMPRESSED, or PLACEHOLDER -- based on semantic similarity to the current query, half-life recency weighting, and importance classification. AFM packs messages chronologically under a strict token budget, preferring high fidelity for the most relevant turns while aiming to preserve a cheap trace of the dialogue. In a safety-oriented benchmark involving a user with a severe peanut allergy planning a trip to Thailand, AFM retains the allergy across both short and medium-length conversations, matches the safety performance of naive replay, and cuts average token usage by 66% relative to a replay baseline. We release a modular Python implementation of AFM designed for OpenAI-compatible APIs and offline operation, enabling practitioners to reduce inference cost without sacrificing safety or factual continuity in the evaluated scenario.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 242,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12751",
        "abs_url": "https://arxiv.org/abs/2511.12751",
        "pdf_url": "https://arxiv.org/pdf/2511.12751",
        "title": "Are LLMs The Way Forward? A Case Study on LLM-Guided Reinforcement Learning for Decentralized Autonomous Driving",
        "authors": [
            "Timur Anvar",
            "Jeffrey Chen",
            "Yuyan Wang",
            "Rohan Chandra"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Robotics (cs.RO)",
        "abstract": "Autonomous vehicle navigation in complex environments such as dense and fast-moving highways and merging scenarios remains an active area of research. A key limitation of RL is its reliance on well-specified reward functions, which often fail to capture the full semantic and social complexity of diverse, out-of-distribution situations. As a result, a rapidly growing line of research explores using Large Language Models (LLMs) to replace or supplement RL for direct planning and control, on account of their ability to reason about rich semantic context. However, LLMs present significant drawbacks: they can be unstable in zero-shot safety-critical settings, produce inconsistent outputs, and often depend on expensive API calls with network latency. This motivates our investigation into whether small, locally deployed LLMs (< 14B parameters) can meaningfully support autonomous highway driving through reward shaping rather than direct control. We present a case study comparing RL-only, LLM-only, and hybrid approaches, where LLMs augment RL rewards by scoring state-action transitions during training, while standard RL policies execute at test time. Our findings reveal that RL-only agents achieve moderate success rates (73-89%) with reasonable efficiency, LLM-only agents can reach higher success rates (up to 94%) but with severely degraded speed performance, and hybrid approaches consistently fall between these extremes. Critically, despite explicit efficiency instructions, LLM-influenced approaches exhibit systematic conservative bias with substantial model-dependent variability, highlighting important limitations of current small LLMs for safety-critical control tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 243,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12752",
        "abs_url": "https://arxiv.org/abs/2511.12752",
        "pdf_url": "https://arxiv.org/pdf/2511.12752",
        "title": "Whose Narrative is it Anyway? A KV Cache Manipulation Attack",
        "authors": [
            "Mukkesh Ganesh",
            "Kaushik Iyer",
            "Arun Baalaaji Sankar Ananthan"
        ],
        "comments": "7 pages, 10 figures",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
        "abstract": "The Key Value(KV) cache is an important component for efficient inference in autoregressive Large Language Models (LLMs), but its role as a representation of the model's internal state makes it a potential target for integrity attacks. This paper introduces \"History Swapping,\" a novel block-level attack that manipulates the KV cache to steer model generation without altering the user-facing prompt. The attack involves overwriting a contiguous segment of the active generation's cache with a precomputed cache from a different topic. We empirically evaluate this method across 324 configurations on the Qwen 3 family of models, analyzing the impact of timing, magnitude, and layer depth of the cache overwrite. Our findings reveal that only full-layer overwrites can successfully hijack the conversation's topic, leading to three distinct behaviors: immediate and persistent topic shift, partial recovery, or a delayed hijack. Furthermore, we observe that high-level structural plans are encoded early in the generation process and local discourse structure is maintained by the final layers of the model. This work demonstrates that the KV cache is a significant vector for security analysis, as it encodes not just context but also topic trajectory and structural planning, making it a powerful interface for manipulating model behavior.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 244,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12757",
        "abs_url": "https://arxiv.org/abs/2511.12757",
        "pdf_url": "https://arxiv.org/pdf/2511.12757",
        "title": "Which Way from B to A: The role of embedding geometry in image interpolation for Stable Diffusion",
        "authors": [
            "Nicholas Karris",
            "Luke Durell",
            "Javier Flores",
            "Tegan Emerson"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "It can be shown that Stable Diffusion has a permutation-invariance property with respect to the rows of Contrastive Language-Image Pretraining (CLIP) embedding matrices. This inspired the novel observation that these embeddings can naturally be interpreted as point clouds in a Wasserstein space rather than as matrices in a Euclidean space. This perspective opens up new possibilities for understanding the geometry of embedding space. For example, when interpolating between embeddings of two distinct prompts, we propose reframing the interpolation problem as an optimal transport problem. By solving this optimal transport problem, we compute a shortest path (or geodesic) between embeddings that captures a more natural and geometrically smooth transition through the embedding space. This results in smoother and more coherent intermediate (interpolated) images when rendered by the Stable Diffusion generative model. We conduct experiments to investigate this effect, comparing the quality of interpolated images produced using optimal transport to those generated by other standard interpolation methods. The novel optimal transport--based approach presented indeed gives smoother image interpolations, suggesting that viewing the embeddings as point clouds (rather than as matrices) better reflects and leverages the geometry of the embedding space.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 245,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12768",
        "abs_url": "https://arxiv.org/abs/2511.12768",
        "pdf_url": "https://arxiv.org/pdf/2511.12768",
        "title": "Evidence of Phase Transitions in Small Transformer-Based Language Models",
        "authors": [
            "Noah Hong",
            "Tao Hong"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Phase transitions have been proposed as the origin of emergent abilities in large language models (LLMs), where new capabilities appear abruptly once models surpass critical thresholds of scale. Prior work, such as that of Wei et al., demonstrated these phenomena under model and data scaling, with transitions revealed after applying a log scale to training compute. In this work, we ask three complementary questions: (1) Are phase transitions unique to large models, or can they also be observed in small transformer-based language models? (2) Can such transitions be detected directly in linear training space, rather than only after log rescaling? and (3) Can these transitions emerge at early stages of training? To investigate, we train a small GPT-style transformer on a character-level corpus and analyze the evolution of vocabulary usage throughout training. We track the average word length, the number of correct versus incorrect words, and shifts in vocabulary diversity. Building on these measures, we apply Poisson and sub-Poisson statistics to quantify how words connect and reorganize. This combined analysis reveals a distinct transition point during training. Notably, these transitions are not apparent in standard loss or validation curves, but become visible through our vocabulary- and statistics-based probes. Our findings suggest that phase-transition reorganizations are a general feature of language model training, observable even in modest models, detectable directly in linear training space, and occurring surprisingly early as coherence emerges. This perspective provides new insight into the nonlinear dynamics of language model training and underscores the importance of tailored metrics for uncovering phase transition behaviors",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 246,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12779",
        "abs_url": "https://arxiv.org/abs/2511.12779",
        "pdf_url": "https://arxiv.org/pdf/2511.12779",
        "title": "Scalable Multi-Objective and Meta Reinforcement Learning via Gradient Estimation",
        "authors": [
            "Zhenshuo Zhang",
            "Minxuan Duan",
            "Youran Ye",
            "Hongyang R. Zhang"
        ],
        "comments": "17 pages. To appear in AAAI'26",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "We study the problem of efficiently estimating policies that simultaneously optimize multiple objectives in reinforcement learning (RL). Given $n$ objectives (or tasks), we seek the optimal partition of these objectives into $k \\ll n$ groups, where each group comprises related objectives that can be trained together. This problem arises in applications such as robotics, control, and preference optimization in language models, where learning a single policy for all $n$ objectives is suboptimal as $n$ grows. We introduce a two-stage procedure -- meta-training followed by fine-tuning -- to address this problem. We first learn a meta-policy for all objectives using multitask learning. Then, we adapt the meta-policy to multiple randomly sampled subsets of objectives. The adaptation step leverages a first-order approximation property of well-trained policy networks, which is empirically verified to be accurate within a $2\\%$ error margin across various RL environments. The resulting algorithm, PolicyGradEx, efficiently estimates an aggregate task-affinity score matrix given a policy evaluation algorithm. Based on the estimated affinity score matrix, we cluster the $n$ objectives into $k$ groups by maximizing the intra-cluster affinity scores. Experiments on three robotic control and the Meta-World benchmarks demonstrate that our approach outperforms state-of-the-art baselines by $16\\%$ on average, while delivering up to $26\\times$ faster speedup relative to performing full training to obtain the clusters. Ablation studies validate each component of our approach. For instance, compared with random grouping and gradient-similarity-based grouping, our loss-based clustering yields an improvement of $19\\%$. Finally, we analyze the generalization error of policy networks by measuring the Hessian trace of the loss surface, which gives non-vacuous measures relative to the observed generalization errors.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 247,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12785",
        "abs_url": "https://arxiv.org/abs/2511.12785",
        "pdf_url": "https://arxiv.org/pdf/2511.12785",
        "title": "Lightweight Optimal-Transport Harmonization on Edge Devices",
        "authors": [
            "Maria Larchenko",
            "Dmitry Guskov",
            "Alexander Lobashev",
            "Georgy Derevyanko"
        ],
        "comments": "AAAI 2026, Oral",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Graphics (cs.GR)",
        "abstract": "Color harmonization adjusts the colors of an inserted object so that it perceptually matches the surrounding image, resulting in a seamless composite. The harmonization problem naturally arises in augmented reality (AR), yet harmonization algorithms are not currently integrated into AR pipelines because real-time solutions are scarce. In this work, we address color harmonization for AR by proposing a lightweight approach that supports on-device inference. For this, we leverage classical optimal transport theory by training a compact encoder to predict the Monge-Kantorovich transport map. We benchmark our MKL-Harmonizer algorithm against state-of-the-art methods and demonstrate that for real composite AR images our method achieves the best aggregated score. We release our dedicated AR dataset of composite images with pixel-accurate masks and data-gathering toolkit to support further data acquisition by researchers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 248,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12791",
        "abs_url": "https://arxiv.org/abs/2511.12791",
        "pdf_url": "https://arxiv.org/pdf/2511.12791",
        "title": "Optimal Look-back Horizon for Time Series Forecasting in Federated Learning",
        "authors": [
            "Dahao Tang",
            "Nan Yang",
            "Yanli Li",
            "Zhiyu Zhu",
            "Zhibo Jin",
            "Dong Yuan"
        ],
        "comments": "Accepted by AAAI-26 as Oral Presentation",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Selecting an appropriate look-back horizon remains a fundamental challenge in time series forecasting (TSF), particularly in the federated learning scenarios where data is decentralized, heterogeneous, and often non-independent. While recent work has explored horizon selection by preserving forecasting-relevant information in an intrinsic space, these approaches are primarily restricted to centralized and independently distributed settings. This paper presents a principled framework for adaptive horizon selection in federated time series forecasting through an intrinsic space formulation. We introduce a synthetic data generator (SDG) that captures essential temporal structures in client data, including autoregressive dependencies, seasonality, and trend, while incorporating client-specific heterogeneity. Building on this model, we define a transformation that maps time series windows into an intrinsic representation space with well-defined geometric and statistical properties. We then derive a decomposition of the forecasting loss into a Bayesian term, which reflects irreducible uncertainty, and an approximation term, which accounts for finite-sample effects and limited model capacity. Our analysis shows that while increasing the look-back horizon improves the identifiability of deterministic patterns, it also increases approximation error due to higher model complexity and reduced sample efficiency. We prove that the total forecasting loss is minimized at the smallest horizon where the irreducible loss starts to saturate, while the approximation loss continues to rise. This work provides a rigorous theoretical foundation for adaptive horizon selection for time series forecasting in federated learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 249,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12796",
        "abs_url": "https://arxiv.org/abs/2511.12796",
        "pdf_url": "https://arxiv.org/pdf/2511.12796",
        "title": "Maximizing the efficiency of human feedback in AI alignment: a comparative analysis",
        "authors": [
            "Andreas Chouliaras",
            "Dimitris Chatzopoulos"
        ],
        "comments": "16 pages, 6 figures, 6 algorithms. AICS2025",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)",
        "abstract": "Reinforcement Learning from Human Feedback (RLHF) relies on preference modeling to align machine learning systems with human values, yet the popular approach of random pair sampling with Bradley-Terry modeling is statistically limited and inefficient under constrained annotation budgets. In this work, we explore alternative sampling and evaluation strategies for preference inference in RLHF, drawing inspiration from areas such as game theory, statistics, and social choice theory. Our best-performing method, Swiss InfoGain, employs a Swiss tournament system with a proxy mutual-information-gain pairing rule, which significantly outperforms all other methods in constrained annotation budgets while also being more sample-efficient. Even in high-resource settings, we can identify superior alternatives to the Bradley-Terry baseline. Our experiments demonstrate that adaptive, resource-aware strategies reduce redundancy, enhance robustness, and yield statistically significant improvements in preference learning, highlighting the importance of balancing alignment quality with human workload in RLHF pipelines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 250,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12797",
        "abs_url": "https://arxiv.org/abs/2511.12797",
        "pdf_url": "https://arxiv.org/pdf/2511.12797",
        "title": "Genomic Next-Token Predictors are In-Context Learners",
        "authors": [
            "Nathan Breslow",
            "Aayush Mishra",
            "Mahler Revsine",
            "Michael C. Schatz",
            "Anqi Liu",
            "Daniel Khashabi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Genomics (q-bio.GN)",
        "abstract": "In-context learning (ICL) -- the capacity of a model to infer and apply abstract patterns from examples provided within its input -- has been extensively studied in large language models trained for next-token prediction on human text. In fact, prior work often attributes this emergent behavior to distinctive statistical properties in human language. This raises a fundamental question: can ICL arise organically in other sequence domains purely through large-scale predictive training? To explore this, we turn to genomic sequences, an alternative symbolic domain rich in statistical structure. Specifically, we study the Evo2 genomic model, trained predominantly on next-nucleotide (A/T/C/G) prediction, at a scale comparable to mid-sized LLMs. We develop a controlled experimental framework comprising symbolic reasoning tasks instantiated in both linguistic and genomic forms, enabling direct comparison of ICL across genomic and linguistic models. Our results show that genomic models, like their linguistic counterparts, exhibit log-linear gains in pattern induction as the number of in-context demonstrations increases. To the best of our knowledge, this is the first evidence of organically emergent ICL in genomic sequences, supporting the hypothesis that ICL arises as a consequence of large-scale predictive modeling over rich data. These findings extend emergent meta-learning beyond language, pointing toward a unified, modality-agnostic view of in-context learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 251,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12804",
        "abs_url": "https://arxiv.org/abs/2511.12804",
        "pdf_url": "https://arxiv.org/pdf/2511.12804",
        "title": "The Alignment Game: A Theory of Long-Horizon Alignment Through Recursive Curation",
        "authors": [
            "Ali Falahati",
            "Mohammad Mohammadi Amiri",
            "Kate Larson",
            "Lukasz Golab"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "In self-consuming generative models that train on their own outputs, alignment with user preferences becomes a recursive rather than one-time process. We provide the first formal foundation for analyzing the long-term effects of such recursive retraining on alignment. Under a two-stage curation mechanism based on the Bradley-Terry (BT) model, we model alignment as an interaction between two factions: the Model Owner, who filters which outputs should be learned by the model, and the Public User, who determines which outputs are ultimately shared and retained through interactions with the model. Our analysis reveals three structural convergence regimes depending on the degree of preference alignment: consensus collapse, compromise on shared optima, and asymmetric refinement. We prove a fundamental impossibility theorem: no recursive BT-based curation mechanism can simultaneously preserve diversity, ensure symmetric influence, and eliminate dependence on initialization. Framing the process as dynamic social choice, we show that alignment is not a static goal but an evolving equilibrium, shaped both by power asymmetries and path dependence.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 252,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12808",
        "abs_url": "https://arxiv.org/abs/2511.12808",
        "pdf_url": "https://arxiv.org/pdf/2511.12808",
        "title": "Expressive Temporal Specifications for Reward Monitoring",
        "authors": [
            "Omar Adalat",
            "Francesco Belardinelli"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Logic in Computer Science (cs.LO)",
        "abstract": "Specifying informative and dense reward functions remains a pivotal challenge in Reinforcement Learning, as it directly affects the efficiency of agent training. In this work, we harness the expressive power of quantitative Linear Temporal Logic on finite traces (($\\text{LTL}_f[\\mathcal{F}]$)) to synthesize reward monitors that generate a dense stream of rewards for runtime-observable state trajectories. By providing nuanced feedback during training, these monitors guide agents toward optimal behaviour and help mitigate the well-known issue of sparse rewards under long-horizon decision making, which arises under the Boolean semantics dominating the current literature. Our framework is algorithm-agnostic and only relies on a state labelling function, and naturally accommodates specifying non-Markovian properties. Empirical results show that our quantitative monitors consistently subsume and, depending on the environment, outperform Boolean monitors in maximizing a quantitative measure of task completion and in reducing convergence time.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 253,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12810",
        "abs_url": "https://arxiv.org/abs/2511.12810",
        "pdf_url": "https://arxiv.org/pdf/2511.12810",
        "title": "MSRNet: A Multi-Scale Recursive Network for Camouflaged Object Detection",
        "authors": [
            "Leena Alghamdi",
            "Muhammad Usman",
            "Hafeez Anwar",
            "Abdul Bais",
            "Saeed Anwar"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Image and Video Processing (eess.IV)",
        "abstract": "Camouflaged object detection is an emerging and challenging computer vision task that requires identifying and segmenting objects that blend seamlessly into their environments due to high similarity in color, texture, and size. This task is further complicated by low-light conditions, partial occlusion, small object size, intricate background patterns, and multiple objects. While many sophisticated methods have been proposed for this task, current methods still struggle to precisely detect camouflaged objects in complex scenarios, especially with small and multiple objects, indicating room for improvement. We propose a Multi-Scale Recursive Network that extracts multi-scale features via a Pyramid Vision Transformer backbone and combines them via specialized Attention-Based Scale Integration Units, enabling selective feature merging. For more precise object detection, our decoder recursively refines features by incorporating Multi-Granularity Fusion Units. A novel recursive-feedback decoding strategy is developed to enhance global context understanding, helping the model overcome the challenges in this task. By jointly leveraging multi-scale learning and recursive feature optimization, our proposed method achieves performance gains, successfully detecting small and multiple camouflaged objects. Our model achieves state-of-the-art results on two benchmark datasets for camouflaged object detection and ranks second on the remaining two. Our codes, model weights, and results are available at \\href{this https URL}{this https URL}.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 254,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12828",
        "abs_url": "https://arxiv.org/abs/2511.12828",
        "pdf_url": "https://arxiv.org/pdf/2511.12828",
        "title": "Catastrophic Forgetting in Kolmogorov-Arnold Networks",
        "authors": [
            "Mohammad Marufur Rahman",
            "Guanchu Wang",
            "Kaixiong Zhou",
            "Minghan Chen",
            "Fan Yang"
        ],
        "comments": "14 pages, 5 figures, accepted in the main technical track of AAAI 2026",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Catastrophic forgetting is a longstanding challenge in continual learning, where models lose knowledge from earlier tasks when learning new ones. While various mitigation strategies have been proposed for Multi-Layer Perceptrons (MLPs), recent architectural advances like Kolmogorov-Arnold Networks (KANs) have been suggested to offer intrinsic resistance to forgetting by leveraging localized spline-based activations. However, the practical behavior of KANs under continual learning remains unclear, and their limitations are not well understood. To address this, we present a comprehensive study of catastrophic forgetting in KANs and develop a theoretical framework that links forgetting to activation support overlap and intrinsic data dimension. We validate these analyses through systematic experiments on synthetic and vision tasks, measuring forgetting dynamics under varying model configurations and data complexity. Further, we introduce KAN-LoRA, a novel adapter design for parameter-efficient continual fine-tuning of language models, and evaluate its effectiveness in knowledge editing tasks. Our findings reveal that while KANs exhibit promising retention in low-dimensional algorithmic settings, they remain vulnerable to forgetting in high-dimensional domains such as image classification and language modeling. These results advance the understanding of KANs' strengths and limitations, offering practical insights for continual learning system design.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 255,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12832",
        "abs_url": "https://arxiv.org/abs/2511.12832",
        "pdf_url": "https://arxiv.org/pdf/2511.12832",
        "title": "From Passive to Persuasive: Steering Emotional Nuance in Human-AI Negotiation",
        "authors": [
            "Niranjan Chebrolu",
            "Gerard Christopher Yeo",
            "Kokil Jaidka"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Large Language Models (LLMs) demonstrate increasing conversational fluency, yet instilling them with nuanced, human-like emotional expression remains a significant challenge. Current alignment techniques often address surface-level output or require extensive fine-tuning. This paper demonstrates that targeted activation engineering can steer LLaMA 3.1-8B to exhibit more human-like emotional nuances. We first employ attribution patching to identify causally influential components, to find a key intervention locus by observing activation patterns during diagnostic conversational tasks. We then derive emotional expression vectors from the difference in the activations generated by contrastive text pairs (positive vs. negative examples of target emotions). Applying these vectors to new conversational prompts significantly enhances emotional characteristics: steered responses show increased positive sentiment (e.g., joy, trust) and more frequent first-person pronoun usage, indicative of greater personal engagement. Our findings offer a precise and interpretable framework and new directions for the study of conversational AI.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 256,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12834",
        "abs_url": "https://arxiv.org/abs/2511.12834",
        "pdf_url": "https://arxiv.org/pdf/2511.12834",
        "title": "SAGA: Source Attribution of Generative AI Videos",
        "authors": [
            "Rohit Kundu",
            "Vishal Mohanty",
            "Hao Xiong",
            "Shan Jia",
            "Athula Balachandran",
            "Amit K. Roy-Chowdhury"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "The proliferation of generative AI has led to hyper-realistic synthetic videos, escalating misuse risks and outstripping binary real/fake detectors. We introduce SAGA (Source Attribution of Generative AI videos), the first comprehensive framework to address the urgent need for AI-generated video source attribution at a large scale. Unlike traditional detection, SAGA identifies the specific generative model used. It uniquely provides multi-granular attribution across five levels: authenticity, generation task (e.g., T2V/I2V), model version, development team, and the precise generator, offering far richer forensic insights. Our novel video transformer architecture, leveraging features from a robust vision foundation model, effectively captures spatio-temporal artifacts. Critically, we introduce a data-efficient pretrain-and-attribute strategy, enabling SAGA to achieve state-of-the-art attribution using only 0.5\\% of source-labeled data per class, matching fully supervised performance. Furthermore, we propose Temporal Attention Signatures (T-Sigs), a novel interpretability method that visualizes learned temporal differences, offering the first explanation for why different video generators are distinguishable. Extensive experiments on public datasets, including cross-domain scenarios, demonstrate that SAGA sets a new benchmark for synthetic video provenance, providing crucial, interpretable insights for forensic and regulatory applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 257,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12838",
        "abs_url": "https://arxiv.org/abs/2511.12838",
        "pdf_url": "https://arxiv.org/pdf/2511.12838",
        "title": "Connectivity-Guided Sparsification of 2-FWL GNNs: Preserving Full Expressivity with Improved Efficiency",
        "authors": [
            "Rongqin Chen",
            "Fan Mo",
            "Pak Lon Ip",
            "Shenghui Zhang",
            "Dan Wu",
            "Ye Li",
            "Leong Hou U"
        ],
        "comments": "Accepted by AAAI 2026",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Higher-order Graph Neural Networks (HOGNNs) based on the 2-FWL test achieve superior expressivity by modeling 2- and 3-node interactions, but at $\\mathcal{O}(n^3)$ computational cost. However, this computational burden is typically mitigated by existing efficiency methods at the cost of reduced expressivity. We propose \\textbf{Co-Sparsify}, a connectivity-aware sparsification framework that eliminates \\emph{provably redundant} computations while preserving full 2-FWL expressive power. Our key insight is that 3-node interactions are expressively necessary only within \\emph{biconnected components} -- maximal subgraphs where every pair of nodes lies on a cycle. Outside these components, structural relationships can be fully captured via 2-node message passing or global readout, rendering higher-order modeling unnecessary. Co-Sparsify restricts 2-node message passing to connected components and 3-node interactions to biconnected ones, removing computation without approximation or sampling. We prove that Co-Sparsified GNNs are as expressive as the 2-FWL test. Empirically, on PPGN, Co-Sparsify matches or exceeds accuracy on synthetic substructure counting tasks and achieves state-of-the-art performance on real-world benchmarks (ZINC, QM9). This study demonstrates that high expressivity and scalability are not mutually exclusive: principled, topology-guided sparsification enables powerful, efficient GNNs with theoretical guarantees.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 258,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12846",
        "abs_url": "https://arxiv.org/abs/2511.12846",
        "pdf_url": "https://arxiv.org/pdf/2511.12846",
        "title": "RoS-Guard: Robust and Scalable Online Change Detection with Delay-Optimal Guarantees",
        "authors": [
            "Zelin Zhu",
            "Yancheng Huang",
            "Kai Yang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Online change detection (OCD) aims to rapidly identify change points in streaming data and is critical in applications such as power system monitoring, wireless network sensing, and financial anomaly detection. Existing OCD methods typically assume precise system knowledge, which is unrealistic due to estimation errors and environmental variations. Moreover, existing OCD methods often struggle with efficiency in large-scale systems. To overcome these challenges, we propose RoS-Guard, a robust and optimal OCD algorithm tailored for linear systems with uncertainty. Through a tight relaxation and reformulation of the OCD optimization problem, RoS-Guard employs neural unrolling to enable efficient parallel computation via GPU acceleration. The algorithm provides theoretical guarantees on performance, including expected false alarm rate and worst-case average detection delay. Extensive experiments validate the effectiveness of RoS-Guard and demonstrate significant computational speedup in large-scale system scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 259,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12851",
        "abs_url": "https://arxiv.org/abs/2511.12851",
        "pdf_url": "https://arxiv.org/pdf/2511.12851",
        "title": "NeuroLex: A Lightweight Domain Language Model for EEG Report Understanding and Generation",
        "authors": [
            "Kang Yin",
            "Hye-Bin Shin"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Clinical electroencephalogram (EEG) reports encode domain-specific linguistic conventions that general-purpose language models (LMs) fail to capture. We introduce NeuroLex, a lightweight domain-adaptive language model trained purely on EEG report text from the Harvard Electroencephalography Database. Unlike existing biomedical LMs, NeuroLex is tailored to the linguistic and diagnostic characteristics of EEG reporting, enabling it to serve as both an independent textual model and a decoder backbone for multimodal EEG-language systems. Using span-corruption pretraining and instruction-style fine-tuning on report polishing, paragraph summarization, and terminology question answering, NeuroLex learns the syntax and reasoning patterns characteristic of EEG interpretation. Comprehensive evaluations show that it achieves lower perplexity, higher extraction and summarization accuracy, better label efficiency, and improved robustness to negation and factual hallucination compared with general models of the same scale. With an EEG-aware linguistic backbone, NeuroLex bridges biomedical text modeling and brain-computer interface applications, offering a foundation for interpretable and language-driven neural decoding.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 260,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12852",
        "abs_url": "https://arxiv.org/abs/2511.12852",
        "pdf_url": "https://arxiv.org/pdf/2511.12852",
        "title": "From Black-Box to White-Box: Control-Theoretic Neural Network Interpretability",
        "authors": [
            "Jihoon Moon"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Systems and Control (eess.SY)",
        "abstract": "Deep neural networks achieve state of the art performance but remain difficult to interpret mechanistically. In this work, we propose a control theoretic framework that treats a trained neural network as a nonlinear state space system and uses local linearization, controllability and observability Gramians, and Hankel singular values to analyze its internal computation. For a given input, we linearize the network around the corresponding hidden activation pattern and construct a state space model whose state consists of hidden neuron activations. The input state and state output Jacobians define local controllability and observability Gramians, from which we compute Hankel singular values and associated modes. These quantities provide a principled notion of neuron and pathway importance: controllability measures how easily each neuron can be excited by input perturbations, observability measures how strongly each neuron influences the output, and Hankel singular values rank internal modes that carry input output energy. We illustrate the framework on simple feedforward networks, including a 1 2 2 1 SwiGLU network and a 2 3 3 2 GELU network. By comparing different operating points, we show how activation saturation reduces controllability, shrinks the dominant Hankel singular value, and shifts the dominant internal mode to a different subset of neurons. The proposed method turns a neural network into a collection of local white box dynamical models and suggests which internal directions are natural candidates for pruning or constraints to improve interpretability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 261,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12865",
        "abs_url": "https://arxiv.org/abs/2511.12865",
        "pdf_url": "https://arxiv.org/pdf/2511.12865",
        "title": "An approach of deep reinforcement learning for maximizing the net present value of stochastic projects",
        "authors": [
            "Wei Xu",
            "Fan Yang",
            "Qinyuan Cui",
            "Zhi Chen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "This paper investigates a project with stochastic activity durations and cash flows under discrete scenarios, where activities must satisfy precedence constraints generating cash inflows and outflows. The objective is to maximize expected net present value (NPV) by accelerating inflows and deferring outflows. We formulate the problem as a discrete-time Markov Decision Process (MDP) and propose a Double Deep Q-Network (DDQN) approach. Comparative experiments demonstrate that DDQN outperforms traditional rigid and dynamic strategies, particularly in large-scale or highly uncertain environments, exhibiting superior computational capability, policy reliability, and adaptability. Ablation studies further reveal that the dual-network architecture mitigates overestimation of action values, while the target network substantially improves training convergence and robustness. These results indicate that DDQN not only achieves higher expected NPV in complex project optimization but also provides a reliable framework for stable and effective policy implementation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 262,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12868",
        "abs_url": "https://arxiv.org/abs/2511.12868",
        "pdf_url": "https://arxiv.org/pdf/2511.12868",
        "title": "Video Finetuning Improves Reasoning Between Frames",
        "authors": [
            "Ruiqi Yang",
            "Tian Yun",
            "Zihan Wang",
            "Ellie Pavlick"
        ],
        "comments": "Accepted at CogInterp @ NeurIPS 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Multimodal large language models (LLMs) have made rapid progress in visual understanding, yet their extension from images to videos often reduces to a naive concatenation of frame tokens. In this work, we investigate what video finetuning brings to multimodal LLMs. We propose Visual Chain-of-Thought (vCoT), an explicit reasoning process that generates transitional event descriptions between consecutive frames. Using vCoT, we systematically compare image-only LVLMs with their video-finetuned counterparts, both with and without access to these transitional cues. Our experiments show that vCoT significantly improves the performance of image-only models on long-form video question answering, while yielding only marginal gains for video-finetuned models. This suggests that the latter already capture frame-to-frame transitions implicitly. Moreover, we find that video models transfer this temporal reasoning ability to purely static settings, outperforming image models' baselines on relational visual reasoning tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 263,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12869",
        "abs_url": "https://arxiv.org/abs/2511.12869",
        "pdf_url": "https://arxiv.org/pdf/2511.12869",
        "title": "On the Fundamental Limits of LLMs at Scale",
        "authors": [
            "Muhammad Ahmed Mohsin",
            "Muhammad Umer",
            "Ahsan Bilal",
            "Zeeshan Memon",
            "Muhammad Ibtsaam Qadir",
            "Sagnik Bhattacharya",
            "Hassan Rizwan",
            "Abhiram R. Gorle",
            "Maahe Zehra Kazmi",
            "Ayesha Mohsin",
            "Muhammad Usman Rafique",
            "Zihao He",
            "Pulkit Mehta",
            "Muhammad Ali Jamshed",
            "John M. Cioffi"
        ],
        "comments": "Submitted to TMLR 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC); Information Theory (cs.IT); Multiagent Systems (cs.MA)",
        "abstract": "Large Language Models (LLMs) have benefited enormously from scaling, yet these gains are bounded by five fundamental limitations: (1) hallucination, (2) context compression, (3) reasoning degradation, (4) retrieval fragility, and (5) multimodal misalignment. While existing surveys describe these phenomena empirically, they lack a rigorous theoretical synthesis connecting them to the foundational limits of computation, information, and learning. This work closes that gap by presenting a unified, proof-informed framework that formalizes the innate theoretical ceilings of LLM scaling. First, computability and uncomputability imply an irreducible residue of error: for any computably enumerable model family, diagonalization guarantees inputs on which some model must fail, and undecidable queries (e.g., halting-style tasks) induce infinite failure sets for all computable predictors. Second, information-theoretic and statistical constraints bound attainable accuracy even on decidable tasks, finite description length enforces compression error, and long-tail factual knowledge requires prohibitive sample complexity. Third, geometric and computational effects compress long contexts far below their nominal size due to positional under-training, encoding attenuation, and softmax crowding. We further show how likelihood-based training favors pattern completion over inference, how retrieval under token limits suffers from semantic drift and coupling noise, and how multimodal scaling inherits shallow cross-modal alignment. Across sections, we pair theorems and empirical evidence to outline where scaling helps, where it saturates, and where it cannot progress, providing both theoretical foundations and practical mitigation paths like bounded-oracle retrieval, positional curricula, and sparse or hierarchical attention.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 264,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12874",
        "abs_url": "https://arxiv.org/abs/2511.12874",
        "pdf_url": "https://arxiv.org/pdf/2511.12874",
        "title": "Classification of Hope in Textual Data using Transformer-Based Models",
        "authors": [
            "Chukwuebuka Fortunate Ijezue",
            "Tania-Amanda Fredrick Eneye",
            "Maaz Amjad"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "This paper presents a transformer-based approach for classifying hope expressions in text. We developed and compared three architectures (BERT, GPT-2, and DeBERTa) for both binary classification (Hope vs. Not Hope) and multiclass categorization (five hope-related categories). Our initial BERT implementation achieved 83.65% binary and 74.87% multiclass accuracy. In the extended comparison, BERT demonstrated superior performance (84.49% binary, 72.03% multiclass accuracy) while requiring significantly fewer computational resources (443s vs. 704s training time) than newer architectures. GPT-2 showed lowest overall accuracy (79.34% binary, 71.29% multiclass), while DeBERTa achieved moderate results (80.70% binary, 71.56% multiclass) but at substantially higher computational cost (947s for multiclass training). Error analysis revealed architecture-specific strengths in detecting nuanced hope expressions, with GPT-2 excelling at sarcasm detection (92.46% recall). This study provides a framework for computational analysis of hope, with applications in mental health and social media analysis, while demonstrating that architectural suitability may outweigh model size for specialized emotion detection tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 265,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12882",
        "abs_url": "https://arxiv.org/abs/2511.12882",
        "pdf_url": "https://arxiv.org/pdf/2511.12882",
        "title": "Towards High-Consistency Embodied World Model with Multi-View Trajectory Videos",
        "authors": [
            "Taiyi Su",
            "Jian Zhu",
            "Yaxuan Li",
            "Chong Ma",
            "Zitai Huang",
            "Yichen Zhu",
            "Hanli Wang",
            "Yi Xu"
        ],
        "comments": "11 pages, 5 figures",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
        "abstract": "Embodied world models aim to predict and interact with the physical world through visual observations and actions. However, existing models struggle to accurately translate low-level actions (e.g., joint positions) into precise robotic movements in predicted frames, leading to inconsistencies with real-world physical interactions. To address these limitations, we propose MTV-World, an embodied world model that introduces Multi-view Trajectory-Video control for precise visuomotor prediction. Specifically, instead of directly using low-level actions for control, we employ trajectory videos obtained through camera intrinsic and extrinsic parameters and Cartesian-space transformation as control signals. However, projecting 3D raw actions onto 2D images inevitably causes a loss of spatial information, making a single view insufficient for accurate interaction modeling. To overcome this, we introduce a multi-view framework that compensates for spatial information loss and ensures high-consistency with physical world. MTV-World forecasts future frames based on multi-view trajectory videos as input and conditioning on an initial frame per view. Furthermore, to systematically evaluate both robotic motion precision and object interaction accuracy, we develop an auto-evaluation pipeline leveraging multimodal large models and referring video object segmentation models. To measure spatial consistency, we formulate it as an object location matching problem and adopt the Jaccard Index as the evaluation metric. Extensive experiments demonstrate that MTV-World achieves precise control execution and accurate physical interaction modeling in complex dual-arm scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 266,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12903",
        "abs_url": "https://arxiv.org/abs/2511.12903",
        "pdf_url": "https://arxiv.org/pdf/2511.12903",
        "title": "Contrastive Entropy Bounds for Density and Conditional Density Decomposition",
        "authors": [
            "Bo Hu",
            "Jose C. Principe"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "This paper studies the interpretability of neural network features from a Bayesian Gaussian view, where optimizing a cost is reaching a probabilistic bound; learning a model approximates a density that makes the bound tight and the cost optimal, often with a Gaussian mixture density. The two examples are Mixture Density Networks (MDNs) using the bound for the marginal and autoencoders using the conditional bound. It is a known result, not only for autoencoders, that minimizing the error between inputs and outputs maximizes the dependence between inputs and the middle. We use Hilbert space and decomposition to address cases where a multiple-output network produces multiple centers defining a Gaussian mixture. Our first finding is that an autoencoder's objective is equivalent to maximizing the trace of a Gaussian operator, the sum of eigenvalues under bases orthonormal w.r.t. the data and model distributions. This suggests that, when a one-to-one correspondence as needed in autoencoders is unnecessary, we can instead maximize the nuclear norm of this operator, the sum of singular values, to maximize overall rank rather than trace. Thus the trace of a Gaussian operator can be used to train autoencoders, and its nuclear norm can be used as divergence to train MDNs. Our second test uses inner products and norms in a Hilbert space to define bounds and costs. Such bounds often have an extra norm compared to KL-based bounds, which increases sample diversity and prevents the trivial solution where a multiple-output network produces the same constant, at the cost of requiring a sample batch to estimate and optimize. We propose an encoder-mixture-decoder architecture whose decoder is multiple-output, producing multiple centers per sample, potentially tightening the bound. Assuming the data are small-variance Gaussian mixtures, this upper bound can be tracked and analyzed quantitatively.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 267,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12905",
        "abs_url": "https://arxiv.org/abs/2511.12905",
        "pdf_url": "https://arxiv.org/pdf/2511.12905",
        "title": "LinkedIn Profile Characteristics and Professional Success Indicators",
        "authors": [
            "Tania-Amanda Fredrick Eneye",
            "Ashlesha Malla",
            "Pawan Paudel"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "This study explores the relationship between LinkedIn profile characteristics and professional success, focusing on the indicators of promotions, follower count, and career progression rate. By leveraging a dataset of over 62,000 anonymized LinkedIn profiles, we developed predictive models using machine learning techniques to identify the most influential factors driving professional success. Results indicate that while promotions are highly predictable, follower growth exhibits greater complexity. This research provides actionable insights for professionals seeking to optimize their LinkedIn presence and career strategies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 268,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12908",
        "abs_url": "https://arxiv.org/abs/2511.12908",
        "pdf_url": "https://arxiv.org/pdf/2511.12908",
        "title": "DeepSport: A Multimodal Large Language Model for Comprehensive Sports Video Reasoning via Agentic Reinforcement Learning",
        "authors": [
            "Junbo Zou",
            "Haotian Xia",
            "Zhen Ye",
            "Shengjie Zhang",
            "Christopher Lai",
            "Vicente Ordonez",
            "Weining Shen",
            "Hanjie Chen"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Sports video understanding presents unique challenges, requiring models to perceive high-speed dynamics, comprehend complex rules, and reason over long temporal contexts. While Multimodal Large Language Models (MLLMs) have shown promise in genral domains, the current state of research in sports remains narrowly focused: existing approaches are either single-sport centric, limited to specific tasks, or rely on training-free paradigms that lack robust, learned reasoning process. To address this gap, we introduce DeepSport, the first end-to-end trained MLLM framework designed for multi-task, multi-sport video understanding. DeepSport shifts the paradigm from passive frame processing to active, iterative reasoning, empowering the model to ``think with videos'' by dynamically interrogating content via a specialized frame-extraction tool. To enable this, we propose a data distillation pipeline that synthesizes high-quality Chain-of-Thought (CoT) trajectories from 10 diverse data source, creating a unified resource of 78k training data. We then employ a two-stage training strategy, Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) with a novel gated tool-use reward, to optimize the model's reasoning process. Extensive experiments on the testing benchmark of 6.7k questions demonstrate that DeepSport achieves state-of-the-art performance, significantly outperforming baselines of both proprietary model and open-source models. Our work establishes a new foundation for domain-specific video reasoning to address the complexities of diverse sports.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 269,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12920",
        "abs_url": "https://arxiv.org/abs/2511.12920",
        "pdf_url": "https://arxiv.org/pdf/2511.12920",
        "title": "Auditing Google's AI Overviews and Featured Snippets: A Case Study on Baby Care and Pregnancy",
        "authors": [
            "Desheng Hu",
            "Joachim Baumann",
            "Aleksandra Urman",
            "Elsa Lichtenegger",
            "Robin Forsberg",
            "Aniko Hannak",
            "Christo Wilson"
        ],
        "comments": "18 pages, 10 figures; to appear in AAAI ICWSM 2026",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Human-Computer Interaction (cs.HC); Information Retrieval (cs.IR)",
        "abstract": "Google Search increasingly surfaces AI-generated content through features like AI Overviews (AIO) and Featured Snippets (FS), which users frequently rely on despite having no control over their presentation. Through a systematic algorithm audit of 1,508 real baby care and pregnancy-related queries, we evaluate the quality and consistency of these information displays. Our robust evaluation framework assesses multiple quality dimensions, including answer consistency, relevance, presence of medical safeguards, source categories, and sentiment alignment. Our results reveal concerning gaps in information consistency, with information in AIO and FS displayed on the same search result page being inconsistent with each other in 33% of cases. Despite high relevance scores, both features critically lack medical safeguards (present in just 11% of AIO and 7% of FS responses). While health and wellness websites dominate source categories for both, AIO and FS, FS also often link to commercial sources. These findings have important implications for public health information access and demonstrate the need for stronger quality controls in AI-mediated health information. Our methodology provides a transferable framework for auditing AI systems across high-stakes domains where information quality directly impacts user well-being.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 270,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12922",
        "abs_url": "https://arxiv.org/abs/2511.12922",
        "pdf_url": "https://arxiv.org/pdf/2511.12922",
        "title": "Tokenize Once, Recommend Anywhere: Unified Item Tokenization for Multi-domain LLM-based Recommendation",
        "authors": [
            "Yu Hou",
            "Won-Yong Shin"
        ],
        "comments": "20 pages, 8 figures, 9 tables; Annual AAAI Conference on Artificial Intelligence (AAAI-26) (to appear) (Please cite our conference version.)",
        "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Social and Information Networks (cs.SI)",
        "abstract": "Large language model (LLM)-based recommender systems have achieved high-quality performance by bridging the discrepancy between the item space and the language space through item tokenization. However, existing item tokenization methods typically require training separate models for each item domain, limiting generalization. Moreover, the diverse distributions and semantics across item domains make it difficult to construct a unified tokenization that preserves domain-specific information. To address these challenges, we propose UniTok, a Unified item Tokenization framework that integrates our own mixture-of-experts (MoE) architecture with a series of codebooks to convert items into discrete tokens, enabling scalable tokenization while preserving semantic information across multiple item domains. Specifically, items from different domains are first projected into a unified latent space through a shared encoder. They are then routed to domain-specific experts to capture the unique semantics, while a shared expert, which is always active, encodes common knowledge transferable across domains. Additionally, to mitigate semantic imbalance across domains, we present a mutual information calibration mechanism, which guides the model towards retaining similar levels of semantic information for each domain. Comprehensive experiments on wide-ranging real-world datasets demonstrate that the proposed UniTok framework is (a) highly effective: achieving up to 51.89% improvements over strong benchmarks, (b) theoretically sound: showing the analytical validity of our architectural design and optimization; and (c) highly generalizable: demonstrating robust performance across diverse domains without requiring per-domain retraining, a capability not supported by existing baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 271,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12935",
        "abs_url": "https://arxiv.org/abs/2511.12935",
        "pdf_url": "https://arxiv.org/pdf/2511.12935",
        "title": "PFAvatar: Pose-Fusion 3D Personalized Avatar Reconstruction from Real-World Outfit-of-the-Day Photos",
        "authors": [
            "Dianbing Xi",
            "Guoyuan An",
            "Jingsen Zhu",
            "Zhijian Liu",
            "Yuan Liu",
            "Ruiyuan Zhang",
            "Jiayuan Lu",
            "Rui Wang",
            "Yuchi Huo"
        ],
        "comments": "Accepted by AAAI 2026",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Graphics (cs.GR)",
        "abstract": "We propose PFAvatar (Pose-Fusion Avatar), a new method that reconstructs high-quality 3D avatars from ``Outfit of the Day'' (OOTD) photos, which exhibit diverse poses, occlusions, and complex backgrounds. Our method consists of two stages: (1) fine-tuning a pose-aware diffusion model from few-shot OOTD examples and (2) distilling a 3D avatar represented by a neural radiance field (NeRF). In the first stage, unlike previous methods that segment images into assets (e.g., garments, accessories) for 3D assembly, which is prone to inconsistency, we avoid decomposition and directly model the full-body appearance. By integrating a pre-trained ControlNet for pose estimation and a novel Condition Prior Preservation Loss (CPPL), our method enables end-to-end learning of fine details while mitigating language drift in few-shot training. Our method completes personalization in just 5 minutes, achieving a 48$\\times$ speed-up compared to previous approaches. In the second stage, we introduce a NeRF-based avatar representation optimized by canonical SMPL-X space sampling and Multi-Resolution 3D-SDS. Compared to mesh-based representations that suffer from resolution-dependent discretization and erroneous occluded geometry, our continuous radiance field can preserve high-frequency textures (e.g., hair) and handle occlusions correctly through transmittance. Experiments demonstrate that PFAvatar outperforms state-of-the-art methods in terms of reconstruction fidelity, detail preservation, and robustness to occlusions/truncations, advancing practical 3D avatar generation from real-world OOTD albums. In addition, the reconstructed 3D avatar supports downstream applications such as virtual try-on, animation, and human video reenactment, further demonstrating the versatility and practical value of our approach.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 272,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12936",
        "abs_url": "https://arxiv.org/abs/2511.12936",
        "pdf_url": "https://arxiv.org/pdf/2511.12936",
        "title": "Privacy-Preserving Federated Learning from Partial Decryption Verifiable Threshold Multi-Client Functional Encryption",
        "authors": [
            "Minjie Wang",
            "Jinguang Han",
            "Weizhi Meng"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
        "abstract": "In federated learning, multiple parties can cooperate to train the model without directly exchanging their own private data, but the gradient leakage problem still threatens the privacy security and model integrity. Although the existing scheme uses threshold cryptography to mitigate the inference attack, it can not guarantee the verifiability of the aggregation results, making the system vulnerable to the threat of poisoning attack. We construct a partial decryption verifiable threshold multi client function encryption scheme, and apply it to Federated learning to implement the federated learning verifiable threshold security aggregation protocol (VTSAFL). VTSAFL empowers clients to verify aggregation results, concurrently minimizing both computational and communication overhead. The size of the functional key and partial decryption results of the scheme are constant, which provides efficiency guarantee for large-scale deployment. The experimental results on MNIST dataset show that vtsafl can achieve the same accuracy as the existing scheme, while reducing the total training time by more than 40%, and reducing the communication overhead by up to 50%. This efficiency is critical for overcoming the resource constraints inherent in Internet of Things (IoT) devices.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 273,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12955",
        "abs_url": "https://arxiv.org/abs/2511.12955",
        "pdf_url": "https://arxiv.org/pdf/2511.12955",
        "title": "Global Cross-Time Attention Fusion for Enhanced Solar Flare Prediction from Multivariate Time Series",
        "authors": [
            "Onur Vural",
            "Shah Muhammad Hamdi",
            "Soukaina Filali Boubrahimi"
        ],
        "comments": "This work has been accepted at the 2025 IEEE International Conference on Big Data (IEEE BigData 2025) on October 23, 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Multivariate time series classification is increasingly investigated in space weather research as a means to predict intense solar flare events, which can cause widespread disruptions across modern technological systems. Magnetic field measurements of solar active regions are converted into structured multivariate time series, enabling predictive modeling across segmented observation windows. However, the inherently imbalanced nature of solar flare occurrences, where intense flares are rare compared to minor flare events, presents a significant barrier to effective learning. To address this challenge, we propose a novel Global Cross-Time Attention Fusion (GCTAF) architecture, a transformer-based model to enhance long-range temporal modeling. Unlike traditional self-attention mechanisms that rely solely on local interactions within time series, GCTAF injects a set of learnable cross-attentive global tokens that summarize salient temporal patterns across the entire sequence. These tokens are refined through cross-attention with the input sequence and fused back into the temporal representation, enabling the model to identify globally significant, non-contiguous time points that are critical for flare prediction. This mechanism functions as a dynamic attention-driven temporal summarizer that augments the model's capacity to capture discriminative flare-related dynamics. We evaluate our approach on the benchmark solar flare dataset and show that GCTAF effectively detects intense flares and improves predictive performance, demonstrating that refining transformer-based architectures presents a high-potential alternative for solar flare prediction tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 274,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12962",
        "abs_url": "https://arxiv.org/abs/2511.12962",
        "pdf_url": "https://arxiv.org/pdf/2511.12962",
        "title": "EndoSight AI: Deep Learning-Driven Real-Time Gastrointestinal Polyp Detection and Segmentation for Enhanced Endoscopic Diagnostics",
        "authors": [
            "Daniel Cavadia"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Precise and real-time detection of gastrointestinal polyps during endoscopic procedures is crucial for early diagnosis and prevention of colorectal cancer. This work presents EndoSight AI, a deep learning architecture developed and evaluated independently to enable accurate polyp localization and detailed boundary delineation. Leveraging the publicly available Hyper-Kvasir dataset, the system achieves a mean Average Precision (mAP) of 88.3% for polyp detection and a Dice coefficient of up to 69% for segmentation, alongside real-time inference speeds exceeding 35 frames per second on GPU hardware. The training incorporates clinically relevant performance metrics and a novel thermal-aware procedure to ensure model robustness and efficiency. This integrated AI solution is designed for seamless deployment in endoscopy workflows, promising to advance diagnostic accuracy and clinical decision-making in gastrointestinal healthcare.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 275,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12964",
        "abs_url": "https://arxiv.org/abs/2511.12964",
        "pdf_url": "https://arxiv.org/pdf/2511.12964",
        "title": "CalibrateMix: Guided-Mixup Calibration of Image Semi-Supervised Models",
        "authors": [
            "Mehrab Mustafy Rahman",
            "Jayanth Mohan",
            "Tiberiu Sosea",
            "Cornelia Caragea"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Semi-supervised learning (SSL) has demonstrated high performance in image classification tasks by effectively utilizing both labeled and unlabeled data. However, existing SSL methods often suffer from poor calibration, with models yielding overconfident predictions that misrepresent actual prediction likelihoods. Recently, neural networks trained with {\\tt mixup} that linearly interpolates random examples from the training set have shown better calibration in supervised settings. However, calibration of neural models remains under-explored in semi-supervised settings. Although effective in supervised model calibration, random mixup of pseudolabels in SSL presents challenges due to the overconfidence and unreliability of pseudolabels. In this work, we introduce CalibrateMix, a targeted mixup-based approach that aims to improve the calibration of SSL models while maintaining or even improving their classification accuracy. Our method leverages training dynamics of labeled and unlabeled samples to identify ``easy-to-learn'' and ``hard-to-learn'' samples, which in turn are utilized in a targeted mixup of easy and hard samples. Experimental results across several benchmark image datasets show that our method achieves lower expected calibration error (ECE) and superior accuracy compared to existing SSL approaches.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 276,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12971",
        "abs_url": "https://arxiv.org/abs/2511.12971",
        "pdf_url": "https://arxiv.org/pdf/2511.12971",
        "title": "Esim: EVM Bytecode Similarity Detection Based on Stable-Semantic Graph",
        "authors": [
            "Zhuo Chen",
            "Gaoqiang Ji",
            "Yiling He",
            "Lei Wu",
            "Yajin Zhou"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
        "abstract": "Decentralized finance (DeFi) is experiencing rapid expansion. However, prevalent code reuse and limited open-source contributions have introduced significant challenges to the blockchain ecosystem, including plagiarism and the propagation of vulnerable code. Consequently, an effective and accurate similarity detection method for EVM bytecode is urgently needed to identify similar contracts. Traditional binary similarity detection methods are typically based on instruction stream or control flow graph (CFG), which have limitations on EVM bytecode due to specific features like low-level EVM bytecode and heavily-reused basic blocks. Moreover, the highly-diverse Solidity Compiler (Solc) versions further complicate accurate similarity detection. Motivated by these challenges, we propose a novel EVM bytecode representation called Stable-Semantic Graph (SSG), which captures relationships between 'stable instructions' (special instructions identified by our study). Moreover, we implement a prototype, Esim, which embeds SSG into matrices for similarity detection using a heterogeneous graph neural network. Esim demonstrates high accuracy in SSG construction, achieving F1-scores of 100% for control flow and 95.16% for data flow, and its similarity detection performance reaches 96.3% AUC, surpassing traditional approaches. Our large-scale study, analyzing 2,675,573 smart contracts on six EVM-compatible chains over a one-year period, also demonstrates that Esim outperforms the SOTA tool Etherscan in vulnerability search.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 277,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12986",
        "abs_url": "https://arxiv.org/abs/2511.12986",
        "pdf_url": "https://arxiv.org/pdf/2511.12986",
        "title": "Learning Branching Policies for MILPs with Proximal Policy Optimization",
        "authors": [
            "Abdelouahed Ben Mhamed",
            "Assia Kamal-Idrissi",
            "Amal El Fallah Seghrouchni"
        ],
        "comments": "11 pages, 3 figures, AAAI conference",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Optimization and Control (math.OC)",
        "abstract": "Branch-and-Bound (B\\&B) is the dominant exact solution method for Mixed Integer Linear Programs (MILP), yet its exponential time complexity poses significant challenges for large-scale instances. The growing capabilities of machine learning have spurred efforts to improve B\\&B by learning data-driven branching policies. However, most existing approaches rely on Imitation Learning (IL), which tends to overfit to expert demonstrations and struggles to generalize to structurally diverse or unseen instances. In this work, we propose Tree-Gate Proximal Policy Optimization (TGPPO), a novel framework that employs Proximal Policy Optimization (PPO), a Reinforcement Learning (RL) algorithm, to train a branching policy aimed at improving generalization across heterogeneous MILP instances. Our approach builds on a parameterized state space representation that dynamically captures the evolving context of the search tree. Empirical evaluations show that TGPPO often outperforms existing learning-based policies in terms of reducing the number of nodes explored and improving p-Primal-Dual Integrals (PDI), particularly in out-of-distribution instances. These results highlight the potential of RL to develop robust and adaptable branching strategies for MILP solvers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 278,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.12988",
        "abs_url": "https://arxiv.org/abs/2511.12988",
        "pdf_url": "https://arxiv.org/pdf/2511.12988",
        "title": "UNSEEN: Enhancing Dataset Pruning from a Generalization Perspective",
        "authors": [
            "Furui Xu",
            "Shaobo Wang",
            "Jiajun Zhang",
            "Chenghao Sun",
            "Haixiang Tang",
            "Linfeng Zhang"
        ],
        "comments": "AAAI 2026, 13 pages, 9 figures, 5 tables",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "The growing scale of datasets in deep learning has introduced significant computational challenges. Dataset pruning addresses this challenge by constructing a compact but informative coreset from the full dataset with comparable performance. Previous approaches typically establish scoring metrics based on specific criteria to identify representative samples. However, these methods predominantly rely on sample scores obtained from the model's performance during the training (i.e., fitting) phase. As scoring models achieve near-optimal performance on training data, such fitting-centric approaches induce a dense distribution of sample scores within a narrow numerical range. This concentration reduces the distinction between samples and hinders effective selection. To address this challenge, we conduct dataset pruning from the perspective of generalization, i.e., scoring samples based on models not exposed to them during training. We propose a plug-and-play framework, UNSEEN, which can be integrated into existing dataset pruning methods. Additionally, conventional score-based methods are single-step and rely on models trained solely on the complete dataset, providing limited perspective on the importance of samples. To address this limitation, we scale UNSEEN to multi-step scenarios and propose an incremental selection technique through scoring models trained on varying coresets, and optimize the quality of the coreset dynamically. Extensive experiments demonstrate that our method significantly outperforms existing state-of-the-art (SOTA) methods on CIFAR-10, CIFAR-100, and ImageNet-1K. Notably, on ImageNet-1K, UNSEEN achieves lossless performance while reducing training data by 30\\%.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 279,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13005",
        "abs_url": "https://arxiv.org/abs/2511.13005",
        "pdf_url": "https://arxiv.org/pdf/2511.13005",
        "title": "SAGE: Spuriousness-Aware Guided Prompt Exploration for Mitigating Multimodal Bias",
        "authors": [
            "Wenqian Ye",
            "Di Wang",
            "Guangtao Zheng",
            "Bohan Liu",
            "Aidong Zhang"
        ],
        "comments": "Accepted at AAAI 2026",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Large vision-language models, such as CLIP, have shown strong zero-shot classification performance by aligning images and text in a shared embedding space. However, CLIP models often develop multimodal spurious biases, which is the undesirable tendency to rely on spurious features. For example, CLIP may infer object types in images based on frequently co-occurring backgrounds rather than the object's core features. This bias significantly impairs the robustness of pre-trained CLIP models on out-of-distribution data, where such cross-modal associations no longer hold. Existing methods for mitigating multimodal spurious bias typically require fine-tuning on downstream data or prior knowledge of the bias, which undermines the out-of-the-box usability of CLIP. In this paper, we first theoretically analyze the impact of multimodal spurious bias in zero-shot classification. Based on this insight, we propose Spuriousness-Aware Guided Exploration (SAGE), a simple and effective method that mitigates spurious bias through guided prompt selection. SAGE requires no training, fine-tuning, or external annotations. It explores a space of prompt templates and selects the prompts that induce the largest semantic separation between classes, thereby improving worst-group robustness. Extensive experiments on four real-world benchmark datasets and five popular backbone models demonstrate that SAGE consistently improves zero-shot performance and generalization, outperforming previous zero-shot approaches without any external knowledge or model updates.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 280,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13010",
        "abs_url": "https://arxiv.org/abs/2511.13010",
        "pdf_url": "https://arxiv.org/pdf/2511.13010",
        "title": "Are Graph Transformers Necessary? Efficient Long-Range Message Passing with Fractal Nodes in MPNNs",
        "authors": [
            "Jeongwhan Choi",
            "Seungjun Park",
            "Sumin Park",
            "Sung-Bae Cho",
            "Noseong Park"
        ],
        "comments": "Accepted in AAAI 2026 for Oral Representation. This is the extended version including the appendix",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Graph Neural Networks (GNNs) have emerged as powerful tools for learning on graph-structured data, but often struggle to balance local and global information. While graph Transformers aim to address this by enabling long-range interactions, they often overlook the inherent locality and efficiency of Message Passing Neural Networks (MPNNs). We propose a new concept called fractal nodes, inspired by the fractal structure observed in real-world networks. Our approach is based on the intuition that graph partitioning naturally induces fractal structure, where subgraphs often reflect the connectivity patterns of the full graph. Fractal nodes are designed to coexist with the original nodes and adaptively aggregate subgraph-level feature representations, thereby enforcing feature similarity within each subgraph. We show that fractal nodes alleviate the over-squashing problem by providing direct shortcut connections that enable long-range propagation of subgraph-level representations. Experiment results show that our method improves the expressive power of MPNNs and achieves comparable or better performance to graph Transformers while maintaining the computational efficiency of MPNN by improving the long-range dependencies of MPNN.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 281,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13019",
        "abs_url": "https://arxiv.org/abs/2511.13019",
        "pdf_url": "https://arxiv.org/pdf/2511.13019",
        "title": "MeanFlow Transformers with Representation Autoencoders",
        "authors": [
            "Zheyuan Hu",
            "Chieh-Hsin Lai",
            "Ge Wu",
            "Yuki Mitsufuji",
            "Stefano Ermon"
        ],
        "comments": "Code is available at this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "MeanFlow (MF) is a diffusion-motivated generative model that enables efficient few-step generation by learning long jumps directly from noise to data. In practice, it is often used as a latent MF by leveraging the pre-trained Stable Diffusion variational autoencoder (SD-VAE) for high-dimensional data modeling. However, MF training remains computationally demanding and is often unstable. During inference, the SD-VAE decoder dominates the generation cost, and MF depends on complex guidance hyperparameters for class-conditional generation. In this work, we develop an efficient training and sampling scheme for MF in the latent space of a Representation Autoencoder (RAE), where a pre-trained vision encoder (e.g., DINO) provides semantically rich latents paired with a lightweight decoder. We observe that naive MF training in the RAE latent space suffers from severe gradient explosion. To stabilize and accelerate training, we adopt Consistency Mid-Training for trajectory-aware initialization and use a two-stage scheme: distillation from a pre-trained flow matching teacher to speed convergence and reduce variance, followed by an optional bootstrapping stage with a one-point velocity estimator to further reduce deviation from the oracle mean flow. This design removes the need for guidance, simplifies training configurations, and reduces computation in both training and sampling. Empirically, our method achieves a 1-step FID of 2.03, outperforming vanilla MF's 3.43, while reducing sampling GFLOPS by 38% and total training cost by 83% on ImageNet 256. We further scale our approach to ImageNet 512, achieving a competitive 1-step FID of 3.23 with the lowest GFLOPS among all baselines. Code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 282,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13020",
        "abs_url": "https://arxiv.org/abs/2511.13020",
        "pdf_url": "https://arxiv.org/pdf/2511.13020",
        "title": "SpectralAdapt: Semi-Supervised Domain Adaptation with Spectral Priors for Human-Centered Hyperspectral Image Reconstruction",
        "authors": [
            "Yufei Wen",
            "Yuting Zhang",
            "Jingdan Kang",
            "Hao Ren",
            "Weibin Cheng",
            "Jintai Chen",
            "Kaishun Wu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Hyperspectral imaging (HSI) holds great potential for healthcare due to its rich spectral information. However, acquiring HSI data remains costly and technically demanding. Hyperspectral image reconstruction offers a practical solution by recovering HSI data from accessible modalities, such as RGB. While general domain datasets are abundant, the scarcity of human HSI data limits progress in medical applications. To tackle this, we propose SpectralAdapt, a semi-supervised domain adaptation (SSDA) framework that bridges the domain gap between general and human-centered HSI datasets. To fully exploit limited labels and abundant unlabeled data, we enhance spectral reasoning by introducing Spectral Density Masking (SDM), which adaptively masks RGB channels based on their spectral complexity, encouraging recovery of informative regions from complementary cues during consistency training. Furthermore, we introduce Spectral Endmember Representation Alignment (SERA), which derives physically interpretable endmembers from valuable labeled pixels and employs them as domain-invariant anchors to guide unlabeled predictions, with momentum updates ensuring adaptability and stability. These components are seamlessly integrated into SpectralAdapt, a spectral prior-guided framework that effectively mitigates domain shift, spectral degradation, and data scarcity in HSI reconstruction. Experiments on benchmark datasets demonstrate consistent improvements in spectral fidelity, cross-domain generalization, and training stability, highlighting the promise of SSDA as an efficient solution for hyperspectral imaging in healthcare.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 283,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13023",
        "abs_url": "https://arxiv.org/abs/2511.13023",
        "pdf_url": "https://arxiv.org/pdf/2511.13023",
        "title": "SLMQuant:Benchmarking Small Language Model Quantization for Practical Deployment",
        "authors": [
            "Jiacheng Wang",
            "Yejun Zeng",
            "Jinyang Guo",
            "Yuqing Ma",
            "Aishan Liu",
            "Xianglong Liu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Despite the growing interest in Small Language Models (SLMs) as resource-efficient alternatives to Large Language Models (LLMs), their deployment on edge devices remains challenging due to unresolved efficiency gaps in model compression. While quantization has proven effective for LLMs, its applicability to SLMs is significantly underexplored, with critical questions about differing quantization bottlenecks and efficiency profiles. This paper introduces SLMQuant, the first systematic benchmark for evaluating LLM compression techniques when applied to SLMs. Through comprehensive multi-track evaluations across diverse architectures and tasks, we analyze how state-of-the-art quantization methods perform on SLMs. Our findings reveal fundamental disparities between SLMs and LLMs in quantization sensitivity, demonstrating that direct transfer of LLM-optimized techniques leads to suboptimal results due to SLMs' unique architectural characteristics and training dynamics. We identify key factors governing effective SLM quantization and propose actionable design principles for SLM-tailored compression. SLMQuant establishes a foundational framework for advancing efficient SLM deployment on low-end devices in edge applications, and provides critical insights for deploying lightweight language models in resource-constrained scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 284,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13029",
        "abs_url": "https://arxiv.org/abs/2511.13029",
        "pdf_url": "https://arxiv.org/pdf/2511.13029",
        "title": "AA-Omniscience: Evaluating Cross-Domain Knowledge Reliability in Large Language Models",
        "authors": [
            "Declan Jackson",
            "William Keating",
            "George Cameron",
            "Micah Hill-Smith"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Existing language model evaluations primarily measure general capabilities, yet reliable use of these models across a range of domains demands factual accuracy and recognition of knowledge gaps. We introduce AA-Omniscience, a benchmark designed to measure both factual recall and knowledge calibration across 6,000 questions. Questions are derived from authoritative academic and industry sources, and cover 42 economically relevant topics within six different domains. The evaluation measures a model's Omniscience Index, a bounded metric (-100 to 100) measuring factual recall that jointly penalizes hallucinations and rewards abstention when uncertain, with 0 equating to a model that answers questions correctly as much as it does incorrectly. Among evaluated models, Claude 4.1 Opus attains the highest score (4.8), making it one of only three models to score above zero. These results reveal persistent factuality and calibration weaknesses across frontier models. Performance also varies by domain, with the models from three different research labs leading across the six domains. This performance variability suggests models should be chosen according to the demands of the use case rather than general performance for tasks where knowledge is important.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 285,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13035",
        "abs_url": "https://arxiv.org/abs/2511.13035",
        "pdf_url": "https://arxiv.org/pdf/2511.13035",
        "title": "One-Step Generative Policies with Q-Learning: A Reformulation of MeanFlow",
        "authors": [
            "Zeyuan Wang",
            "Da Li",
            "Yulin Chen",
            "Ye Shi",
            "Liang Bai",
            "Tianyuan Yu",
            "Yanwei Fu"
        ],
        "comments": "Accepted in AAAI 2026 Poster",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "We introduce a one-step generative policy for offline reinforcement learning that maps noise directly to actions via a residual reformulation of MeanFlow, making it compatible with Q-learning. While one-step Gaussian policies enable fast inference, they struggle to capture complex, multimodal action distributions. Existing flow-based methods improve expressivity but typically rely on distillation and two-stage training when trained with Q-learning. To overcome these limitations, we propose to reformulate MeanFlow to enable direct noise-to-action generation by integrating the velocity field and noise-to-action transformation into a single policy network-eliminating the need for separate velocity estimation. We explore several reformulation variants and identify an effective residual formulation that supports expressive and stable policy learning. Our method offers three key advantages: 1) efficient one-step noise-to-action generation, 2) expressive modelling of multimodal action distributions, and 3) efficient and stable policy learning via Q-learning in a single-stage training setup. Extensive experiments on 73 tasks across the OGBench and D4RL benchmarks demonstrate that our method achieves strong performance in both offline and offline-to-online reinforcement learning settings. Code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 286,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13052",
        "abs_url": "https://arxiv.org/abs/2511.13052",
        "pdf_url": "https://arxiv.org/pdf/2511.13052",
        "title": "Learning from the Undesirable: Robust Adaptation of Language Models without Forgetting",
        "authors": [
            "Yunhun Nam",
            "Jaehyung Kim",
            "Jongheon Jeong"
        ],
        "comments": "17 pages; AAAI 2026; Code is available at this https URL",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Language models (LMs) are often adapted through supervised fine-tuning (SFT) to specialize their capabilities for downstream tasks. However, in typical scenarios where the fine-tuning data is limited, e.g., compared to pre-training, SFT can lead LMs to overfit, causing them to rely on spurious patterns within the target task or to compromise other broadly useful capabilities as a side effect of narrow specialization. In this paper, we propose Learning-from-the-Undesirable (LfU), a simple yet effective regularization scheme for SFT to mitigate overfitting issues when fine-tuning LMs with limited data. Specifically, we aim to regularize the fine-tuning process to favor solutions that are resilient to \"undesirable\" model updates, e.g., gradient ascent steps that steer the model toward undesirable behaviors. To this end, we propose a novel form of consistency regularization that directly aligns internal representations of the model with those after an undesirable update. By leveraging representation-level data augmentation through undesirable updates, LfU effectively promotes generalization under limited data. Our experiments on diverse LM downstream tasks show that LfU serves as an effective prior that enhances adaptability while preserving pretrained knowledge. For example, our LM from LfU achieves a 16.8% average improvement on math tasks compared to vanilla SFT on the same dataset, where the latter even leads to degraded performance on those tasks. Furthermore, LfU exhibits improved robustness to prompt variations, e.g., yielding a 92.1% lower standard deviation in output performances compared to SFT, highlighting its versatile effects.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 287,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13057",
        "abs_url": "https://arxiv.org/abs/2511.13057",
        "pdf_url": "https://arxiv.org/pdf/2511.13057",
        "title": "Dimension vs. Precision: A Comparative Analysis of Autoencoders and Quantization for Efficient Vector Retrieval on BEIR SciFact",
        "authors": [
            "Satyanarayan Pati"
        ],
        "comments": "16 pages, 9 figures, 1 table",
        "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI)",
        "abstract": "Dense retrieval models have become a standard for state-of-the-art information retrieval. However, their high-dimensional, high-precision (float32) vector embeddings create significant storage and memory challenges for real-world deployment. To address this, we conduct a rigorous empirical study on the BEIR SciFact benchmark, evaluating the trade-offs between two primary compression strategies: (1) Dimensionality Reduction via deep Autoencoders (AE), reducing original 384-dim vectors to latent spaces from 384 down to 12, and (2) Precision Reduction via Quantization (float16, int8, and binary). We systematically compare each method by measuring the \"performance loss\" (or gain) relative to a float32 baseline across a full suite of retrieval metrics (NDCG, MAP, MRR, Recall, Precision) at various k cutoffs. Our results show that int8 scalar quantization provides the most effective \"sweet spot,\" achieving a 4x compression with a negligible [~1-2%] drop in nDCG@10. In contrast, Autoencoders show a graceful degradation but suffer a more significant performance loss at equivalent 4x compression ratios (AE-96). binary quantization was found to be unsuitable for this task due to catastrophic performance drops. This work provides a practical guide for deploying efficient, high-performance retrieval systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 288,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13060",
        "abs_url": "https://arxiv.org/abs/2511.13060",
        "pdf_url": "https://arxiv.org/pdf/2511.13060",
        "title": "Latency and Ordering Effects in Online Decisions",
        "authors": [
            "Duo Yi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Online decision systems routinely operate under delayed feedback and order-sensitive (noncommutative) dynamics: actions affect which observations arrive, and in what sequence. Taking a Bregman divergence $D_\\Phi$ as the loss benchmark, we prove that the excess benchmark loss admits a structured lower bound $L \\ge L_{\\mathrm{ideal}} + g_1(\\lambda) + g_2(\\varepsilon_\\star) + g_{12}(\\lambda,\\varepsilon_\\star) - D_{\\mathrm{ncx}}$, where $g_1$ and $g_2$ are calibrated penalties for latency and order-sensitivity, $g_{12}$ captures their geometric interaction, and $D_{\\mathrm{ncx}}\\ge 0$ is a nonconvexity/approximation penalty that vanishes under convex Legendre assumptions. We extend this inequality to prox-regular and weakly convex settings, obtaining robust guarantees beyond the convex case. We also give an operational recipe for estimating and monitoring the four terms via simple $2\\times 2$ randomized experiments and streaming diagnostics (effective sample size, clipping rate, interaction heatmaps). The framework packages heterogeneous latency, noncommutativity, and implementation-gap effects into a single interpretable lower-bound statement that can be stress-tested and tuned in real-world systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 289,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13061",
        "abs_url": "https://arxiv.org/abs/2511.13061",
        "pdf_url": "https://arxiv.org/pdf/2511.13061",
        "title": "MACKO: Sparse Matrix-Vector Multiplication for Low Sparsity",
        "authors": [
            "Vladimír Macko",
            "Vladimír Boža"
        ],
        "comments": "8 pages + 7 pages appendix, 11 figures, Code available at this https URL",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC); Data Structures and Algorithms (cs.DS)",
        "abstract": "Sparse Matrix-Vector Multiplication (SpMV) is a fundamental operation in the inference of sparse Large Language Models (LLMs). Because existing SpMV methods perform poorly under the low and unstructured sparsity (30-90%) commonly observed in pruned LLMs, unstructured pruning provided only limited memory reduction and speedup. We propose MACKO-SpMV, a GPU-optimized format and kernel co-designed to reduce storage overhead while preserving compatibility with the GPU's execution model. This enables efficient SpMV for unstructured sparsity without specialized hardware units (e.g., tensor cores) or format-specific precomputation. Empirical results show that at sparsity 50%, MACKO is the first approach with significant 1.5x memory reduction and 1.2-1.5x speedup over dense representation. Speedups over other SpMV baselines: 2.8-13.0x over cuSPARSE, 1.9-2.6x over Sputnik, and 2.2-2.5x over DASP. Applied to Llama2-7B pruned with Wanda to sparsity 50%, it delivers 1.5x memory reduction and 1.5x faster inference at fp16 precision. Thanks to MACKO, unstructured pruning at 50% sparsity is now justified in real-world LLM workloads.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 290,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13062",
        "abs_url": "https://arxiv.org/abs/2511.13062",
        "pdf_url": "https://arxiv.org/pdf/2511.13062",
        "title": "Self-Adaptive Graph Mixture of Models",
        "authors": [
            "Mohit Meena",
            "Yash Punjabi",
            "Abhishek A",
            "Vishal Sharma",
            "Mahesh Chandran"
        ],
        "comments": "17 pages, 5 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Graph Neural Networks (GNNs) have emerged as powerful tools for learning over graph-structured data, yet recent studies have shown that their performance gains are beginning to plateau. In many cases, well-established models such as GCN and GAT, when appropriately tuned, can match or even exceed the performance of more complex, state-of-the-art architectures. This trend highlights a key limitation in the current landscape: the difficulty of selecting the most suitable model for a given graph task or dataset. To address this, we propose Self-Adaptive Graph Mixture of Models (SAGMM), a modular and practical framework that learns to automatically select and combine the most appropriate GNN models from a diverse pool of architectures. Unlike prior mixture-of-experts approaches that rely on variations of a single base model, SAGMM leverages architectural diversity and a topology-aware attention gating mechanism to adaptively assign experts to each node based on the structure of the input graph. To improve efficiency, SAGMM includes a pruning mechanism that reduces the number of active experts during training and inference without compromising performance. We also explore a training-efficient variant in which expert models are pretrained and frozen, and only the gating and task-specific layers are trained. We evaluate SAGMM on 16 benchmark datasets covering node classification, graph classification, regression, and link prediction tasks, and demonstrate that it consistently outperforms or matches leading GNN baselines and prior mixture-based methods, offering a robust and adaptive solution for real-world graph learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 291,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13081",
        "abs_url": "https://arxiv.org/abs/2511.13081",
        "pdf_url": "https://arxiv.org/pdf/2511.13081",
        "title": "Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations",
        "authors": [
            "Yehonatan Elisha",
            "Seffi Cohen",
            "Oren Barkan",
            "Noam Koenigstein"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Saliency maps are widely used for visual explanations in deep learning, but a fundamental lack of consensus persists regarding their intended purpose and alignment with diverse user queries. This ambiguity hinders the effective evaluation and practical utility of explanation this http URL address this gap by introducing the Reference-Frame $\\times$ Granularity (RFxG) taxonomy, a principled conceptual framework that organizes saliency explanations along two essential axes:Reference-Frame: Distinguishing between pointwise (\"Why this prediction?\") and contrastive (\"Why this and not an alternative?\") this http URL: Ranging from fine-grained class-level (e.g., \"Why Husky?\") to coarse-grained group-level (e.g., \"Why Dog?\") this http URL the RFxG lens, we demonstrate critical limitations in existing evaluation metrics, which overwhelmingly prioritize pointwise faithfulness while neglecting contrastive reasoning and semantic granularity. To systematically assess explanation quality across both RFxG dimensions, we propose four novel faithfulness metrics. Our comprehensive evaluation framework applies these metrics to ten state-of-the-art saliency methods, four model architectures, and three this http URL advocating a shift toward user-intent-driven evaluation, our work provides both the conceptual foundation and the practical tools necessary to develop visual explanations that are not only faithful to the underlying model behavior but are also meaningfully aligned with the complexity of human understanding and inquiry.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 292,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13111",
        "abs_url": "https://arxiv.org/abs/2511.13111",
        "pdf_url": "https://arxiv.org/pdf/2511.13111",
        "title": "NuBench: An Open Benchmark for Deep Learning-Based Event Reconstruction in Neutrino Telescopes",
        "authors": [
            "Rasmus F. Orsoe",
            "Stephan Meighen-Berger",
            "Jeffrey Lazar",
            "Jorge Prado",
            "Ivan Mozun-Mateo",
            "Aske Rosted",
            "Philip Weigel",
            "Arturo Llorente Anaya"
        ],
        "comments": "Prepared for JINST",
        "subjects": "High Energy Physics - Experiment (hep-ex); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Data Analysis, Statistics and Probability (physics.data-an); Instrumentation and Detectors (physics.ins-det)",
        "abstract": "Neutrino telescopes are large-scale detectors designed to observe Cherenkov radiation produced from neutrino interactions in water or ice. They exist to identify extraterrestrial neutrino sources and to probe fundamental questions pertaining to the elusive neutrino itself. A central challenge common across neutrino telescopes is to solve a series of inverse problems known as event reconstruction, which seeks to resolve properties of the incident neutrino, based on the detected Cherenkov light. In recent times, significant efforts have been made in adapting advances from deep learning research to event reconstruction, as such techniques provide several benefits over traditional methods. While a large degree of similarity in reconstruction needs and low-level data exists, cross-experimental collaboration has been hindered by a lack of diverse open-source datasets for comparing methods. We present NuBench, an open benchmark for deep learning-based event reconstruction in neutrino telescopes. NuBench comprises seven large-scale simulated datasets containing nearly 130 million charged- and neutral-current muon-neutrino interactions spanning 10 GeV to 100 TeV, generated across six detector geometries inspired by existing and proposed experiments. These datasets provide pulse- and event-level information suitable for developing and comparing machine-learning reconstruction methods in both water and ice environments. Using NuBench, we evaluate four reconstruction algorithms - ParticleNeT and DynEdge, both actively used within the KM3NeT and IceCube collaborations, respectively, along with GRIT and DeepIce - on up to five core tasks: energy and direction reconstruction, topology classification, interaction vertex prediction, and inelasticity estimation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 293,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13116",
        "abs_url": "https://arxiv.org/abs/2511.13116",
        "pdf_url": "https://arxiv.org/pdf/2511.13116",
        "title": "Synthetic Forgetting without Access: A Few-shot Zero-glance Framework for Machine Unlearning",
        "authors": [
            "Qipeng Song",
            "Nan Yang",
            "Ziqi Xu",
            "Yue Li",
            "Wei Shao",
            "Feng Xia"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Machine unlearning aims to eliminate the influence of specific data from trained models to ensure privacy compliance. However, most existing methods assume full access to the original training dataset, which is often impractical. We address a more realistic yet challenging setting: few-shot zero-glance, where only a small subset of the retained data is available and the forget set is entirely inaccessible. We introduce GFOES, a novel framework comprising a Generative Feedback Network (GFN) and a two-phase fine-tuning procedure. GFN synthesises Optimal Erasure Samples (OES), which induce high loss on target classes, enabling the model to forget class-specific knowledge without access to the original forget data, while preserving performance on retained classes. The two-phase fine-tuning procedure enables aggressive forgetting in the first phase, followed by utility restoration in the second. Experiments on three image classification datasets demonstrate that GFOES achieves effective forgetting at both logit and representation levels, while maintaining strong performance using only 5% of the original data. Our framework offers a practical and scalable solution for privacy-preserving machine learning under data-constrained conditions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 294,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13118",
        "abs_url": "https://arxiv.org/abs/2511.13118",
        "pdf_url": "https://arxiv.org/pdf/2511.13118",
        "title": "Extracting Events Like Code: A Multi-Agent Programming Framework for Zero-Shot Event Extraction",
        "authors": [
            "Quanjiang Guo",
            "Sijie Wang",
            "Jinchuan Zhang",
            "Ben Zhang",
            "Zhao Kang",
            "Ling Tian",
            "Ke Yan"
        ],
        "comments": "11 pages, 5 figures, accepted by AAAI 2026 (Oral)",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Zero-shot event extraction (ZSEE) remains a significant challenge for large language models (LLMs) due to the need for complex reasoning and domain-specific understanding. Direct prompting often yields incomplete or structurally invalid outputs--such as misclassified triggers, missing arguments, and schema violations. To address these limitations, we present Agent-Event-Coder (AEC), a novel multi-agent framework that treats event extraction like software engineering: as a structured, iterative code-generation process. AEC decomposes ZSEE into specialized subtasks--retrieval, planning, coding, and verification--each handled by a dedicated LLM agent. Event schemas are represented as executable class definitions, enabling deterministic validation and precise feedback via a verification agent. This programming-inspired approach allows for systematic disambiguation and schema enforcement through iterative refinement. By leveraging collaborative agent workflows, AEC enables LLMs to produce precise, complete, and schema-consistent extractions in zero-shot settings. Experiments across five diverse domains and six LLMs demonstrate that AEC consistently outperforms prior zero-shot baselines, showcasing the power of treating event extraction like code generation. The code and data are released on this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 295,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13132",
        "abs_url": "https://arxiv.org/abs/2511.13132",
        "pdf_url": "https://arxiv.org/pdf/2511.13132",
        "title": "Shedding Light on VLN Robustness: A Black-box Framework for Indoor Lighting-based Adversarial Attack",
        "authors": [
            "Chenyang Li",
            "Wenbing Tang",
            "Yihao Huang",
            "Sinong Simon Zhan",
            "Ming Hu",
            "Xiaojun Jia",
            "Yang Liu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Vision-and-Language Navigation (VLN) agents have made remarkable progress, but their robustness remains insufficiently studied. Existing adversarial evaluations often rely on perturbations that manifest as unusual textures rarely encountered in everyday indoor environments. Errors under such contrived conditions have limited practical relevance, as real-world agents are unlikely to encounter such artificial patterns. In this work, we focus on indoor lighting, an intrinsic yet largely overlooked scene attribute that strongly influences navigation. We propose Indoor Lighting-based Adversarial Attack (ILA), a black-box framework that manipulates global illumination to disrupt VLN agents. Motivated by typical household lighting usage, we design two attack modes: Static Indoor Lighting-based Attack (SILA), where the lighting intensity remains constant throughout an episode, and Dynamic Indoor Lighting-based Attack (DILA), where lights are switched on or off at critical moments to induce abrupt illumination changes. We evaluate ILA on two state-of-the-art VLN models across three navigation tasks. Results show that ILA significantly increases failure rates while reducing trajectory efficiency, revealing previously unrecognized vulnerabilities of VLN agents to realistic indoor lighting variations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 296,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13133",
        "abs_url": "https://arxiv.org/abs/2511.13133",
        "pdf_url": "https://arxiv.org/pdf/2511.13133",
        "title": "Soft Conflict-Resolution Decision Transformer for Offline Multi-Task Reinforcement Learning",
        "authors": [
            "Shudong Wang",
            "Xinfei Wang",
            "Chenhao Zhang",
            "Shanchen Pang",
            "Haiyuan Gui",
            "Wenhao Ji",
            "Xiaojian Liao"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Multi-task reinforcement learning (MTRL) seeks to learn a unified policy for diverse tasks, but often suffers from gradient conflicts across tasks. Existing masking-based methods attempt to mitigate such conflicts by assigning task-specific parameter masks. However, our empirical study shows that coarse-grained binary masks have the problem of over-suppressing key conflicting parameters, hindering knowledge sharing across tasks. Moreover, different tasks exhibit varying conflict levels, yet existing methods use a one-size-fits-all fixed sparsity strategy to keep training stability and performance, which proves inadequate. These limitations hinder the model's generalization and learning efficiency. To address these issues, we propose SoCo-DT, a Soft Conflict-resolution method based by parameter importance. By leveraging Fisher information, mask values are dynamically adjusted to retain important parameters while suppressing conflicting ones. In addition, we introduce a dynamic sparsity adjustment strategy based on the Interquartile Range (IQR), which constructs task-specific thresholding schemes using the distribution of conflict and harmony scores during training. To enable adaptive sparsity evolution throughout training, we further incorporate an asymmetric cosine annealing schedule to continuously update the threshold. Experimental results on the Meta-World benchmark show that SoCo-DT outperforms the state-of-the-art method by 7.6% on MT50 and by 10.5% on the suboptimal dataset, demonstrating its effectiveness in mitigating gradient conflicts and improving overall multi-task performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 297,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13143",
        "abs_url": "https://arxiv.org/abs/2511.13143",
        "pdf_url": "https://arxiv.org/pdf/2511.13143",
        "title": "SoK: The Last Line of Defense: On Backdoor Defense Evaluation",
        "authors": [
            "Gorka Abad",
            "Marina Krček",
            "Stefanos Koffas",
            "Behrad Tajalli",
            "Marco Arazzi",
            "Roberto Riaño",
            "Xiaoyun Xu",
            "Zhuoran Liu",
            "Antonino Nocera",
            "Stjepan Picek"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
        "abstract": "Backdoor attacks pose a significant threat to deep learning models by implanting hidden vulnerabilities that can be activated by malicious inputs. While numerous defenses have been proposed to mitigate these attacks, the heterogeneous landscape of evaluation methodologies hinders fair comparison between defenses. This work presents a systematic (meta-)analysis of backdoor defenses through a comprehensive literature review and empirical evaluation. We analyzed 183 backdoor defense papers published between 2018 and 2025 across major AI and security venues, examining the properties and evaluation methodologies of these defenses. Our analysis reveals significant inconsistencies in experimental setups, evaluation metrics, and threat model assumptions in the literature. Through extensive experiments involving three datasets (MNIST, CIFAR-100, ImageNet-1K), four model architectures (ResNet-18, VGG-19, ViT-B/16, DenseNet-121), 16 representative defenses, and five commonly used attacks, totaling over 3\\,000 experiments, we demonstrate that defense effectiveness varies substantially across different evaluation setups. We identify critical gaps in current evaluation practices, including insufficient reporting of computational overhead and behavior under benign conditions, bias in hyperparameter selection, and incomplete experimentation. Based on our findings, we provide concrete challenges and well-motivated recommendations to standardize and improve future defense evaluations. Our work aims to equip researchers and industry practitioners with actionable insights for developing, assessing, and deploying defenses to different systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 298,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13145",
        "abs_url": "https://arxiv.org/abs/2511.13145",
        "pdf_url": "https://arxiv.org/pdf/2511.13145",
        "title": "Automated Road Distress Detection Using Vision Transformersand Generative Adversarial Networks",
        "authors": [
            "Cesar Portocarrero Rodriguez",
            "Laura Vandeweyen",
            "Yosuke Yamamoto"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "The American Society of Civil Engineers has graded Americas infrastructure condition as a C, with the road system receiving a dismal D. Roads are vital to regional economic viability, yet their management, maintenance, and repair processes remain inefficient, relying on outdated manual or laser-based inspection methods that are both costly and time-consuming. With the increasing availability of real-time visual data from autonomous vehicles, there is an opportunity to apply computer vision (CV) methods for advanced road monitoring, providing insights to guide infrastructure rehabilitation efforts. This project explores the use of state-of-the-art CV techniques for road distress segmentation. It begins by evaluating synthetic data generated with Generative Adversarial Networks (GANs) to assess its usefulness for model training. The study then applies Convolutional Neural Networks (CNNs) for road distress segmentation and subsequently examines the transformer-based model MaskFormer. Results show that GAN-generated data improves model performance and that MaskFormer outperforms the CNN model in two metrics: mAP50 and IoU.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 299,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13166",
        "abs_url": "https://arxiv.org/abs/2511.13166",
        "pdf_url": "https://arxiv.org/pdf/2511.13166",
        "title": "Local Collaborative Filtering: A Collaborative Filtering Method that Utilizes Local Similarities among Users",
        "authors": [
            "Zhaoxin Shen",
            "Dan Wu"
        ],
        "comments": "4 pages, 2 figures",
        "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)",
        "abstract": "To leverage user behavior data from the Internet more effectively in recommender systems, this paper proposes a novel collaborative filtering (CF) method called Local Collaborative Filtering (LCF). LCF utilizes local similarities among users and integrates their data using the law of large numbers (LLN), thereby improving the utilization of user behavior data. Experiments are conducted on the Steam game dataset, and the results of LCF align with real-world needs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 300,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13168",
        "abs_url": "https://arxiv.org/abs/2511.13168",
        "pdf_url": "https://arxiv.org/pdf/2511.13168",
        "title": "SOMA: Feature Gradient Enhanced Affine-Flow Matching for SAR-Optical Registration",
        "authors": [
            "Haodong Wang",
            "Tao Zhuo",
            "Xiuwei Zhang",
            "Hanlin Yin",
            "Wencong Wu",
            "Yanning Zhang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Achieving pixel-level registration between SAR and optical images remains a challenging task due to their fundamentally different imaging mechanisms and visual characteristics. Although deep learning has achieved great success in many cross-modal tasks, its performance on SAR-Optical registration tasks is still unsatisfactory. Gradient-based information has traditionally played a crucial role in handcrafted descriptors by highlighting structural differences. However, such gradient cues have not been effectively leveraged in deep learning frameworks for SAR-Optical image matching. To address this gap, we propose SOMA, a dense registration framework that integrates structural gradient priors into deep features and refines alignment through a hybrid matching strategy. Specifically, we introduce the Feature Gradient Enhancer (FGE), which embeds multi-scale, multi-directional gradient filters into the feature space using attention and reconstruction mechanisms to boost feature distinctiveness. Furthermore, we propose the Global-Local Affine-Flow Matcher (GLAM), which combines affine transformation and flow-based refinement within a coarse-to-fine architecture to ensure both structural consistency and local accuracy. Experimental results demonstrate that SOMA significantly improves registration precision, increasing the CMR@1px by 12.29% on the SEN1-2 dataset and 18.50% on the GFGE_SO dataset. In addition, SOMA exhibits strong robustness and generalizes well across diverse scenes and resolutions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 301,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13198",
        "abs_url": "https://arxiv.org/abs/2511.13198",
        "pdf_url": "https://arxiv.org/pdf/2511.13198",
        "title": "ParaDySe: A Parallel-Strategy Switching Framework for Dynamic Sequence Lengths in Transformer",
        "authors": [
            "Zhixin Ou",
            "Peng Liang",
            "Jianchen Han",
            "Baihui Liu",
            "Linbo Qiao"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Dynamic sequences with varying lengths have been widely used in the training of Transformer-based large language models (LLMs). However, current training frameworks adopt a pre-defined static parallel strategy for these sequences, causing neither communication-parallelization cancellation on short sequences nor out-of-memory on long sequences. To mitigate these issues, we propose ParaDySe, a novel adaptive Parallel strategy switching framework for Dynamic Sequences. ParaDySe enables on-the-fly optimal strategy adoption according to the immediate input sequence. It first implements the modular function libraries for parallel strategies with unified tensor layout specifications, and then builds sequence-aware memory and time cost models with hybrid methods. Guided by cost models, ParaDySe selects optimal layer-wise strategies for dynamic sequences via an efficient heuristic algorithm. By integrating these techniques together, ParaDySe achieves seamless hot-switching of optimal strategies through its well-designed function libraries. We compare ParaDySe with baselines on representative LLMs under datasets with sequence lengths up to 624K. Experimental results indicate that ParaDySe addresses OOM and CPC bottlenecks in LLM training by systematically integrating long-sequence optimizations with existing frameworks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 302,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13219",
        "abs_url": "https://arxiv.org/abs/2511.13219",
        "pdf_url": "https://arxiv.org/pdf/2511.13219",
        "title": "FoleyBench: A Benchmark For Video-to-Audio Models",
        "authors": [
            "Satvik Dixit",
            "Koichi Saito",
            "Zhi Zhong",
            "Yuki Mitsufuji",
            "Chris Donahue"
        ],
        "comments": "",
        "subjects": "Sound (cs.SD); Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)",
        "abstract": "Video-to-audio generation (V2A) is of increasing importance in domains such as film post-production, AR/VR, and sound design, particularly for the creation of Foley sound effects synchronized with on-screen actions. Foley requires generating audio that is both semantically aligned with visible events and temporally aligned with their timing. Yet, there is a mismatch between evaluation and downstream applications due to the absence of a benchmark tailored to Foley-style scenarios. We find that 74% of videos from past evaluation datasets have poor audio-visual correspondence. Moreover, they are dominated by speech and music, domains that lie outside the use case for Foley. To address this gap, we introduce FoleyBench, the first large-scale benchmark explicitly designed for Foley-style V2A evaluation. FoleyBench contains 5,000 (video, ground-truth audio, text caption) triplets, each featuring visible sound sources with audio causally tied to on-screen events. The dataset is built using an automated, scalable pipeline applied to in-the-wild internet videos from YouTube-based and Vimeo-based sources. Compared to past datasets, we show that videos from FoleyBench have stronger coverage of sound categories from a taxonomy specifically designed for Foley sound. Each clip is further labeled with metadata capturing source complexity, UCS/AudioSet category, and video length, enabling fine-grained analysis of model performance and failure modes. We benchmark several state-of-the-art V2A models, evaluating them on audio quality, audio-video alignment, temporal synchronization, and audio-text consistency. Samples are available at: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 303,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13223",
        "abs_url": "https://arxiv.org/abs/2511.13223",
        "pdf_url": "https://arxiv.org/pdf/2511.13223",
        "title": "TokenSqueeze: Performance-Preserving Compression for Reasoning LLMs",
        "authors": [
            "Yuxiang Zhang",
            "Zhengxu Yu",
            "Weihang Pan",
            "Zhongming Jin",
            "Qiang Fu",
            "Deng Cai",
            "Binbin Lin",
            "Jieping Ye"
        ],
        "comments": "Accepted to NeurIPS 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Emerging reasoning LLMs such as OpenAI-o1 and DeepSeek-R1 have achieved strong performance on complex reasoning tasks by generating long chain-of-thought (CoT) traces. However, these long CoTs result in increased token usage, leading to higher inference latency and memory consumption. As a result, balancing accuracy and reasoning efficiency has become essential for deploying reasoning LLMs in practical applications. Existing long-to-short (Long2Short) methods aim to reduce inference length but often sacrifice accuracy, revealing a need for an approach that maintains performance while lowering token costs. To address this efficiency-accuracy tradeoff, we propose TokenSqueeze, a novel Long2Short method that condenses reasoning paths while preserving performance and relying exclusively on self-generated data. First, to prevent performance degradation caused by excessive compression of reasoning depth, we propose to select self-generated samples whose reasoning depth is adaptively matched to the complexity of the problem. To further optimize the linguistic expression without altering the underlying reasoning paths, we introduce a distribution-aligned linguistic refinement method that enhances the clarity and conciseness of the reasoning path while preserving its logical integrity. Comprehensive experimental results demonstrate the effectiveness of TokenSqueeze in reducing token usage while maintaining accuracy. Notably, DeepSeek-R1-Distill-Qwen-7B fine-tuned using our proposed method achieved a 50\\% average token reduction while preserving accuracy on the MATH500 benchmark. TokenSqueeze exclusively utilizes the model's self-generated data, enabling efficient and high-fidelity reasoning without relying on manually curated short-answer datasets across diverse applications. Our code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 304,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13238",
        "abs_url": "https://arxiv.org/abs/2511.13238",
        "pdf_url": "https://arxiv.org/pdf/2511.13238",
        "title": "Computational Measurement of Political Positions: A Review of Text-Based Ideal Point Estimation Algorithms",
        "authors": [
            "Patrick Parschan",
            "Charlott Jakob"
        ],
        "comments": "46 pages, 8 figures, 2 tables, accepted for publication in Quality & Quantity",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computers and Society (cs.CY)",
        "abstract": "This article presents the first systematic review of unsupervised and semi-supervised computational text-based ideal point estimation (CT-IPE) algorithms, methods designed to infer latent political positions from textual data. These algorithms are widely used in political science, communication, computational social science, and computer science to estimate ideological preferences from parliamentary speeches, party manifestos, and social media. Over the past two decades, their development has closely followed broader NLP trends -- beginning with word-frequency models and most recently turning to large language models (LLMs). While this trajectory has greatly expanded the methodological toolkit, it has also produced a fragmented field that lacks systematic comparison and clear guidance for applied use. To address this gap, we identified 25 CT-IPE algorithms through a systematic literature review and conducted a manual content analysis of their modeling assumptions and development contexts. To compare them meaningfully, we introduce a conceptual framework that distinguishes how algorithms generate, capture, and aggregate textual variance. On this basis, we identify four methodological families -- word-frequency, topic modeling, word embedding, and LLM-based approaches -- and critically assess their assumptions, interpretability, scalability, and limitations. Our review offers three contributions. First, it provides a structured synthesis of two decades of algorithm development, clarifying how diverse methods relate to one another. Second, it translates these insights into practical guidance for applied researchers, highlighting trade-offs in transparency, technical requirements, and validation strategies that shape algorithm choice. Third, it emphasizes that differences in estimation outcomes across algorithms are themselves informative, underscoring the need for systematic benchmarking.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 305,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13243",
        "abs_url": "https://arxiv.org/abs/2511.13243",
        "pdf_url": "https://arxiv.org/pdf/2511.13243",
        "title": "Uncovering and Mitigating Transient Blindness in Multimodal Model Editing",
        "authors": [
            "Xiaoqi Han",
            "Ru Li",
            "Ran Yi",
            "Hongye Tan",
            "Zhuomin Liang",
            "Víctor Gutiérrez-Basulto",
            "Jeff Z. Pan"
        ],
        "comments": "Accepted at AAAI'26",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Multimodal Model Editing (MMED) aims to correct erroneous knowledge in multimodal models. Existing evaluation methods, adapted from textual model editing, overstate success by relying on low-similarity or random inputs, obscure overfitting. We propose a comprehensive locality evaluation framework, covering three key dimensions: random-image locality, no-image locality, and consistent-image locality, operationalized through seven distinct data types, enabling a detailed and structured analysis of multimodal edits. We introduce De-VQA, a dynamic evaluation for visual question answering, uncovering a phenomenon we term transient blindness, overfitting to edit-similar text while ignoring visuals. Token analysis shows edits disproportionately affect textual tokens. We propose locality-aware adversarial losses to balance cross-modal representations. Empirical results demonstrate that our approach consistently outperforms existing baselines, reducing transient blindness and improving locality by 17% on average.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 306,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13244",
        "abs_url": "https://arxiv.org/abs/2511.13244",
        "pdf_url": "https://arxiv.org/pdf/2511.13244",
        "title": "Seek and You Shall Fold",
        "authors": [
            "Nadav Bojan Sellam",
            "Meital Bojan",
            "Paul Schanda",
            "Alex Bronstein"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Accurate protein structures are essential for understanding biological function, yet incorporating experimental data into protein generative models remains a major challenge. Most predictors of experimental observables are non-differentiable, making them incompatible with gradient-based conditional sampling. This is especially limiting in nuclear magnetic resonance, where rich data such as chemical shifts are hard to directly integrate into generative modeling. We introduce a framework for non-differentiable guidance of protein generative models, coupling a continuous diffusion-based generator with any black-box objective via a tailored genetic algorithm. We demonstrate its effectiveness across three modalities: pairwise distance constraints, nuclear Overhauser effect restraints, and for the first time chemical shifts. These results establish chemical shift guided structure generation as feasible, expose key weaknesses in current predictors, and showcase a general strategy for incorporating diverse experimental signals. Our work points toward automated, data-conditioned protein modeling beyond the limits of differentiability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 307,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13245",
        "abs_url": "https://arxiv.org/abs/2511.13245",
        "pdf_url": "https://arxiv.org/pdf/2511.13245",
        "title": "Proceedings Seventh International Workshop on Formal Methods for Autonomous Systems",
        "authors": [
            "Matt Luckcuck",
            "Maike Schwammberger",
            "Mengwei Xu"
        ],
        "comments": "",
        "subjects": "Logic in Computer Science (cs.LO); Artificial Intelligence (cs.AI); Robotics (cs.RO)",
        "abstract": "This EPTCS volume contains the papers from the Seventh International Workshop on Formal Methods for Autonomous Systems (FMAS 2025), which was held between the 17th and 19th of November 2025. The goal of the FMAS workshop series is to bring together leading researchers who are using formal methods to tackle the unique challenges that autonomous systems present, so that they can publish and discuss their work with a growing community of researchers. FMAS 2025 was co-located with the 20th International Conference on integrated Formal Methods (iFM'25), hosted by Inria Paris, France at the Inria Paris Center.  In total, FMAS 2025 received 16 submissions from researchers at institutions in: Canada, China, France, Germany, Ireland, Italy, Japan, the Netherlands, Portugal, Sweden, the United States of America, and the United Kingdom. Though we received fewer submissions than last year, we are encouraged to see the submissions being sent from a wide range of countries. Submissions come from both past and new FMAS authors, which shows us that the existing community appreciates the network that FMAS has built over the past 7 years, while new authors also show the FMAS community's great potential of growth.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 308,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13259",
        "abs_url": "https://arxiv.org/abs/2511.13259",
        "pdf_url": "https://arxiv.org/pdf/2511.13259",
        "title": "GeoX-Bench: Benchmarking Cross-View Geo-Localization and Pose Estimation Capabilities of Large Multimodal Models",
        "authors": [
            "Yushuo Zheng",
            "Jiangyong Ying",
            "Huiyu Duan",
            "Chunyi Li",
            "Zicheng Zhang",
            "Jing Liu",
            "Xiaohong Liu",
            "Guangtao Zhai"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Large multimodal models (LMMs) have demonstrated remarkable capabilities across a wide range of tasks, however their knowledge and abilities in the cross-view geo-localization and pose estimation domains remain unexplored, despite potential benefits for navigation, autonomous driving, outdoor robotics, \\textit{etc}. To bridge this gap, we introduce \\textbf{GeoX-Bench}, a comprehensive \\underline{Bench}mark designed to explore and evaluate the capabilities of LMMs in \\underline{cross}-view \\underline{Geo}-localization and pose estimation. Specifically, GeoX-Bench contains 10,859 panoramic-satellite image pairs spanning 128 cities in 49 countries, along with corresponding 755,976 question-answering (QA) pairs. Among these, 42,900 QA pairs are designated for benchmarking, while the remaining are intended to enhance the capabilities of LMMs. Based on GeoX-Bench, we evaluate the capabilities of 25 state-of-the-art LMMs on cross-view geo-localization and pose estimation tasks, and further explore the empowered capabilities of instruction-tuning. Our benchmark demonstrate that while current LMMs achieve impressive performance in geo-localization tasks, their effectiveness declines significantly on the more complex pose estimation tasks, highlighting a critical area for future improvement, and instruction-tuning LMMs on the training data of GeoX-Bench can significantly improve the cross-view geo-sense abilities. The GeoX-Bench is available at \\textcolor{magenta}{this https URL}.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 309,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13271",
        "abs_url": "https://arxiv.org/abs/2511.13271",
        "pdf_url": "https://arxiv.org/pdf/2511.13271",
        "title": "Examining the Usage of Generative AI Models in Student Learning Activities for Software Programming",
        "authors": [
            "Rufeng Chen",
            "Shuaishuai Jiang",
            "Jiyun Shen",
            "AJung Moon",
            "Lili Wei"
        ],
        "comments": "9 pages, 4 figures, accepted at AIWARE 2025",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)",
        "abstract": "The rise of Generative AI (GenAI) tools like ChatGPT has created new opportunities and challenges for computing education. Existing research has primarily focused on GenAI's ability to complete educational tasks and its impact on student performance, often overlooking its effects on knowledge gains. In this study, we investigate how GenAI assistance compares to conventional online resources in supporting knowledge gains across different proficiency levels. We conducted a controlled user experiment with 24 undergraduate students of two different levels of programming experience (beginner, intermediate) to examine how students interact with ChatGPT while solving programming tasks. We analyzed task performance, conceptual understanding, and interaction behaviors. Our findings reveal that generating complete solutions with GenAI significantly improves task performance, especially for beginners, but does not consistently result in knowledge gains. Importantly, usage strategies differ by experience: beginners tend to rely heavily on GenAI toward task completion often without knowledge gain in the process, while intermediates adopt more selective approaches. We find that both over-reliance and minimal use result in weaker knowledge gains overall. Based on our results, we call on students and educators to adopt GenAI as a learning rather than a problem solving tool. Our study highlights the urgent need for guidance when integrating GenAI into programming education to foster deeper understanding.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 310,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13273",
        "abs_url": "https://arxiv.org/abs/2511.13273",
        "pdf_url": "https://arxiv.org/pdf/2511.13273",
        "title": "Spatial Blind Spot: Auditory Motion Perception Deficits in Audio LLMs",
        "authors": [
            "Zhe Sun",
            "Yujun Cai",
            "Jiayu Yao",
            "Yiwei Wang"
        ],
        "comments": "",
        "subjects": "Sound (cs.SD); Artificial Intelligence (cs.AI)",
        "abstract": "Large Audio-Language Models (LALMs) have recently shown impressive progress in speech recognition, audio captioning, and auditory question answering. Yet, whether these models can perceive spatial dynamics, particularly the motion of sound sources, remains unclear. In this work, we uncover a systematic motion perception deficit in current ALLMs. To investigate this issue, we introduce AMPBench, the first benchmark explicitly designed to evaluate auditory motion understanding. AMPBench introduces a controlled question-answering benchmark designed to evaluate whether Audio-Language Models (LALMs) can infer the direction and trajectory of moving sound sources from binaural audio. Comprehensive quantitative and qualitative analyses reveal that current models struggle to reliably recognize motion cues or distinguish directional patterns. The average accuracy remains below 50%, underscoring a fundamental limitation in auditory spatial reasoning. Our study highlights a fundamental gap between human and model auditory spatial reasoning, providing both a diagnostic tool and new insight for enhancing spatial cognition in future Audio-Language Models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 311,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13274",
        "abs_url": "https://arxiv.org/abs/2511.13274",
        "pdf_url": "https://arxiv.org/pdf/2511.13274",
        "title": "KForge: Program Synthesis for Diverse AI Hardware Accelerators",
        "authors": [
            "Taras Sereda",
            "Tom St. John",
            "Burak Bartan",
            "Natalie Serrino",
            "Sachin Katti",
            "Zain Asgar"
        ],
        "comments": "Under review at MLSys 2026",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA); Performance (cs.PF); Software Engineering (cs.SE)",
        "abstract": "GPU kernels are critical for ML performance but difficult to optimize across diverse accelerators. We present KForge, a platform-agnostic framework built on two collaborative LLM-based agents: a generation agent that produces and iteratively refines programs through compilation and correctness feedback, and a performance analysis agent that interprets profiling data to guide optimization. This agent-based architecture requires only a single-shot example to target new platforms. We make three key contributions: (1) introducing an iterative refinement system where the generation agent and performance analysis agent collaborate through functional and optimization passes, interpreting diverse profiling data (from programmatic APIs to GUI-based tools) to generate actionable recommendations that guide program synthesis for arbitrary accelerators; (2) demonstrating that the generation agent effectively leverages cross-platform knowledge transfer, where a reference implementation from one architecture substantially improves generation quality for different hardware targets; and (3) validating the platform-agnostic nature of our approach by demonstrating effective program synthesis across fundamentally different parallel computing platforms: NVIDIA CUDA and Apple Metal.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 312,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13312",
        "abs_url": "https://arxiv.org/abs/2511.13312",
        "pdf_url": "https://arxiv.org/pdf/2511.13312",
        "title": "EL3DD: Extended Latent 3D Diffusion for Language Conditioned Multitask Manipulation",
        "authors": [
            "Jonas Bode",
            "Raphael Memmesheimer",
            "Sven Behnke"
        ],
        "comments": "10 pages; 2 figures; 1 table. Prprint submitted to the European Robotics Forum 2026",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Acting in human environments is a crucial capability for general-purpose robots, necessitating a robust understanding of natural language and its application to physical tasks. This paper seeks to harness the capabilities of diffusion models within a visuomotor policy framework that merges visual and textual inputs to generate precise robotic trajectories. By employing reference demonstrations during training, the model learns to execute manipulation tasks specified through textual commands within the robot's immediate environment. The proposed research aims to extend an existing model by leveraging improved embeddings, and adapting techniques from diffusion models for image generation. We evaluate our methods on the CALVIN dataset, proving enhanced performance on various manipulation tasks and an increased long-horizon success rate when multiple tasks are executed in sequence. Our approach reinforces the usefulness of diffusion models and contributes towards general multitask manipulation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 313,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13315",
        "abs_url": "https://arxiv.org/abs/2511.13315",
        "pdf_url": "https://arxiv.org/pdf/2511.13315",
        "title": "Computer Vision based group activity detection and action spotting",
        "authors": [
            "Narthana Sivalingam",
            "Santhirarajah Sivasthigan",
            "Thamayanthi Mahendranathan",
            "G.M.R.I. Godaliyadda",
            "M.P.B. Ekanayake",
            "H.M.V.R. Herath"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Group activity detection in multi-person scenes is challenging due to complex human interactions, occlusions, and variations in appearance over time. This work presents a computer vision based framework for group activity recognition and action spotting using a combination of deep learning models and graph based relational reasoning. The system first applies Mask R-CNN to obtain accurate actor localization through bounding boxes and instance masks. Multiple backbone networks, including Inception V3, MobileNet, and VGG16, are used to extract feature maps, and RoIAlign is applied to preserve spatial alignment when generating actor specific features. The mask information is then fused with the feature maps to obtain refined masked feature representations for each actor. To model interactions between individuals, we construct Actor Relation Graphs that encode appearance similarity and positional relations using methods such as normalized cross correlation, sum of absolute differences, and dot product. Graph Convolutional Networks operate on these graphs to reason about relationships and predict both individual actions and group level activities. Experiments on the Collective Activity dataset demonstrate that the combination of mask based feature refinement, robust similarity search, and graph neural network reasoning leads to improved recognition performance across both crowded and non crowded scenarios. This approach highlights the potential of integrating segmentation, feature extraction, and relational graph reasoning for complex video understanding tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 314,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13319",
        "abs_url": "https://arxiv.org/abs/2511.13319",
        "pdf_url": "https://arxiv.org/pdf/2511.13319",
        "title": "Whistledown: Combining User-Level Privacy with Conversational Coherence in LLMs",
        "authors": [
            "Chelsea McMurray",
            "Hayder Tirmazi"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
        "abstract": "Users increasingly rely on large language models (LLMs) for personal, emotionally charged, and socially sensitive conversations. However, prompts sent to cloud-hosted models can contain personally identifiable information (PII) that users do not want logged, retained, or leaked. We observe this to be especially acute when users discuss friends, coworkers, or adversaries, i.e., when they spill the tea. Enterprises face the same challenge when they want to use LLMs for internal communication and decision-making. In this whitepaper, we present Whistledown, a best-effort privacy layer that modifies prompts before they are sent to the LLM. Whistledown combines pseudonymization and $\\epsilon$-local differential privacy ($\\epsilon$-LDP) with transformation caching to provide best-effort privacy protection without sacrificing conversational utility. Whistledown is designed to have low compute and memory overhead, allowing it to be deployed directly on a client's device in the case of individual users. For enterprise users, Whistledown is deployed centrally within a zero-trust gateway that runs on an enterprise's trusted infrastructure. Whistledown requires no changes to the existing APIs of popular LLM providers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 315,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13322",
        "abs_url": "https://arxiv.org/abs/2511.13322",
        "pdf_url": "https://arxiv.org/pdf/2511.13322",
        "title": "Explainable RL Policies by Distilling to Locally-Specialized Linear Policies with Voronoi State Partitioning",
        "authors": [
            "Senne Deproost",
            "Dennis Steckelmacher",
            "Ann Nowé"
        ],
        "comments": "Accepted for BNAIC/BeNeLearn 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Deep Reinforcement Learning is one of the state-of-the-art methods for producing near-optimal system controllers. However, deep RL algorithms train a deep neural network, that lacks transparency, which poses challenges when the controller has to meet regulations, or foster trust. To alleviate this, one could transfer the learned behaviour into a model that is human-readable by design using knowledge distilla- tion. Often this is done with a single model which mimics the original model on average but could struggle in more dynamic situations. A key challenge is that this simpler model should have the right balance be- tween flexibility and complexity or right balance between balance bias and accuracy. We propose a new model-agnostic method to divide the state space into regions where a simplified, human-understandable model can operate in. In this paper, we use Voronoi partitioning to find regions where linear models can achieve similar performance to the original con- troller. We evaluate our approach on a gridworld environment and a classic control task. We observe that our proposed distillation to locally- specialized linear models produces policies that are explainable and show that the distillation matches or even slightly outperforms the black-box policy they are distilled from.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 316,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13333",
        "abs_url": "https://arxiv.org/abs/2511.13333",
        "pdf_url": "https://arxiv.org/pdf/2511.13333",
        "title": "AutoMalDesc: Large-Scale Script Analysis for Cyber Threat Research",
        "authors": [
            "Alexandru-Mihai Apostu",
            "Andrei Preda",
            "Alexandra Daniela Damir",
            "Diana Bolocan",
            "Radu Tudor Ionescu",
            "Ioana Croitoru",
            "Mihaela Gaman"
        ],
        "comments": "Accepted at AAAI 2026 (oral)",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Generating thorough natural language explanations for threat detections remains an open problem in cybersecurity research, despite significant advances in automated malware detection systems. In this work, we present AutoMalDesc, an automated static analysis summarization framework that, following initial training on a small set of expert-curated examples, operates independently at scale. This approach leverages an iterative self-paced learning pipeline to progressively enhance output quality through synthetic data generation and validation cycles, eliminating the need for extensive manual data annotation. Evaluation across 3,600 diverse samples in five scripting languages demonstrates statistically significant improvements between iterations, showing consistent gains in both summary quality and classification accuracy. Our comprehensive validation approach combines quantitative metrics based on established malware labels with qualitative assessment from both human experts and LLM-based judges, confirming both technical precision and linguistic coherence of generated summaries. To facilitate reproducibility and advance research in this domain, we publish our complete dataset of more than 100K script samples, including annotated seed (0.9K) and test (3.6K) datasets, along with our methodology and evaluation framework.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 317,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13335",
        "abs_url": "https://arxiv.org/abs/2511.13335",
        "pdf_url": "https://arxiv.org/pdf/2511.13335",
        "title": "AHaSIS: Shared Task on Sentiment Analysis for Arabic Dialects",
        "authors": [
            "Maram Alharbi",
            "Salmane Chafik",
            "Saad Ezzini",
            "Ruslan Mitkov",
            "Tharindu Ranasinghe",
            "Hansi Hettiarachchi"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "The hospitality industry in the Arab world increasingly relies on customer feedback to shape services, driving the need for advanced Arabic sentiment analysis tools. To address this challenge, the Sentiment Analysis on Arabic Dialects in the Hospitality Domain shared task focuses on Sentiment Detection in Arabic Dialects. This task leverages a multi-dialect, manually curated dataset derived from hotel reviews originally written in Modern Standard Arabic (MSA) and translated into Saudi and Moroccan (Darija) dialects. The dataset consists of 538 sentiment-balanced reviews spanning positive, neutral, and negative categories. Translations were validated by native speakers to ensure dialectal accuracy and sentiment preservation. This resource supports the development of dialect-aware NLP systems for real-world applications in customer experience analysis. More than 40 teams have registered for the shared task, with 12 submitting systems during the evaluation phase. The top-performing system achieved an F1 score of 0.81, demonstrating the feasibility and ongoing challenges of sentiment analysis across Arabic dialects.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 318,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13341",
        "abs_url": "https://arxiv.org/abs/2511.13341",
        "pdf_url": "https://arxiv.org/pdf/2511.13341",
        "title": "An LLM-based Quantitative Framework for Evaluating High-Stealthy Backdoor Risks in OSS Supply Chains",
        "authors": [
            "Zihe Yan",
            "Kai Luo",
            "Haoyu Yang",
            "Yang Yu",
            "Zhuosheng Zhang",
            "Guancheng Li"
        ],
        "comments": "7 figures, 4 tables, conference",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)",
        "abstract": "In modern software development workflows, the open-source software supply chain contributes significantly to efficient and convenient engineering practices. With increasing system complexity, using open-source software as third-party dependencies has become a common practice. However, the lack of maintenance for underlying dependencies and insufficient community auditing create challenges in ensuring source code security and the legitimacy of repository maintainers, especially under high-stealthy backdoor attacks exemplified by the XZ-Util incident. To address these problems, we propose a fine-grained project evaluation framework for backdoor risk assessment in open-source software. The framework models stealthy backdoor attacks from the viewpoint of the attacker and defines targeted metrics for each attack stage. In addition, to overcome the limitations of static analysis in assessing the reliability of repository maintenance activities such as irregular committer privilege escalation and limited participation in reviews, the framework uses large language models (LLMs) to conduct semantic evaluation of code repositories without relying on manually crafted patterns. The framework is evaluated on sixty six high-priority packages in the Debian ecosystem. The experimental results indicate that the current open-source software supply chain is exposed to various security risks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 319,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13351",
        "abs_url": "https://arxiv.org/abs/2511.13351",
        "pdf_url": "https://arxiv.org/pdf/2511.13351",
        "title": "Dual-LoRA and Quality-Enhanced Pseudo Replay for Multimodal Continual Food Learning",
        "authors": [
            "Xinlan Wu",
            "Bin Zhu",
            "Feng Han",
            "Pengkun Jiao",
            "Jingjing Chen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Food analysis has become increasingly critical for health-related tasks such as personalized nutrition and chronic disease prevention. However, existing large multimodal models (LMMs) in food analysis suffer from catastrophic forgetting when learning new tasks, requiring costly retraining from scratch. To address this, we propose a novel continual learning framework for multimodal food learning, integrating a Dual-LoRA architecture with Quality-Enhanced Pseudo Replay. We introduce two complementary low-rank adapters for each task: a specialized LoRA that learns task-specific knowledge with orthogonal constraints to previous tasks' subspaces, and a cooperative LoRA that consolidates shared knowledge across tasks via pseudo replay. To improve the reliability of replay data, our Quality-Enhanced Pseudo Replay strategy leverages self-consistency and semantic similarity to reduce hallucinations in generated samples. Experiments on the comprehensive Uni-Food dataset show superior performance in mitigating forgetting, representing the first effective continual learning approach for complex food tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 320,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13353",
        "abs_url": "https://arxiv.org/abs/2511.13353",
        "pdf_url": "https://arxiv.org/pdf/2511.13353",
        "title": "Semi-Supervised Multi-Task Learning for Interpretable Quality As- sessment of Fundus Images",
        "authors": [
            "Lucas Gabriel Telesco",
            "Danila Nejamkin",
            "Estefanía Mata",
            "Francisco Filizzola",
            "Kevin Wignall",
            "Lucía Franco Troilo",
            "María de los Angeles Cenoz",
            "Melissa Thompson",
            "Mercedes Leguía",
            "Ignacio Larrabide",
            "José Ignacio Orlando"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Retinal image quality assessment (RIQA) supports computer-aided diagnosis of eye diseases. However, most tools classify only overall image quality, without indicating acquisition defects to guide recapture. This gap is mainly due to the high cost of detailed annotations. In this paper, we aim to mitigate this limitation by introducing a hybrid semi-supervised learning approach that combines manual labels for overall quality with pseudo-labels of quality details within a multi-task framework. Our objective is to obtain more interpretable RIQA models without requiring extensive manual labeling. Pseudo-labels are generated by a Teacher model trained on a small dataset and then used to fine-tune a pre-trained model in a multi-task setting. Using a ResNet-18 backbone, we show that these weak annotations improve quality assessment over single-task baselines (F1: 0.875 vs. 0.863 on EyeQ, and 0.778 vs. 0.763 on DeepDRiD), matching or surpassing existing methods. The multi-task model achieved performance statistically comparable to the Teacher for most detail prediction tasks (p > 0.05). In a newly annotated EyeQ subset released with this paper, our model performed similarly to experts, suggesting that pseudo-label noise aligns with expert variability. Our main finding is that the proposed semi-supervised approach not only improves overall quality assessment but also provides interpretable feedback on capture conditions (illumination, clarity, contrast). This enhances interpretability at no extra manual labeling cost and offers clinically actionable outputs to guide image recapture.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 321,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13356",
        "abs_url": "https://arxiv.org/abs/2511.13356",
        "pdf_url": "https://arxiv.org/pdf/2511.13356",
        "title": "Enhancing All-to-X Backdoor Attacks with Optimized Target Class Mapping",
        "authors": [
            "Lei Wang",
            "Yulong Tian",
            "Hao Han",
            "Fengyuan Xu"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
        "abstract": "Backdoor attacks pose severe threats to machine learning systems, prompting extensive research in this area. However, most existing work focuses on single-target All-to-One (A2O) attacks, overlooking the more complex All-to-X (A2X) attacks with multiple target classes, which are often assumed to have low attack success rates. In this paper, we first demonstrate that A2X attacks are robust against state-of-the-art defenses. We then propose a novel attack strategy that enhances the success rate of A2X attacks while maintaining robustness by optimizing grouping and target class assignment mechanisms. Our method improves the attack success rate by up to 28%, with average improvements of 6.7%, 16.4%, 14.1% on CIFAR10, CIFAR100, and Tiny-ImageNet, respectively. We anticipate that this study will raise awareness of A2X attacks and stimulate further research in this under-explored area. Our code is available at this https URL .",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 322,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13365",
        "abs_url": "https://arxiv.org/abs/2511.13365",
        "pdf_url": "https://arxiv.org/pdf/2511.13365",
        "title": "InfoDecom: Decomposing Information for Defending against Privacy Leakage in Split Inference",
        "authors": [
            "Ruijun Deng",
            "Zhihui Lu",
            "Qiang Duan"
        ],
        "comments": "Accepted by AAAI 2026",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)",
        "abstract": "Split inference (SI) enables users to access deep learning (DL) services without directly transmitting raw data. However, recent studies reveal that data reconstruction attacks (DRAs) can recover the original inputs from the smashed data sent from the client to the server, leading to significant privacy leakage. While various defenses have been proposed, they often result in substantial utility degradation, particularly when the client-side model is shallow. We identify a key cause of this trade-off: existing defenses apply excessive perturbation to redundant information in the smashed data. To address this issue in computer vision tasks, we propose InfoDecom, a defense framework that first decomposes and removes redundant information and then injects noise calibrated to provide theoretically guaranteed privacy. Experiments demonstrate that InfoDecom achieves a superior utility-privacy trade-off compared to existing baselines. The code and the appendix are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 323,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13368",
        "abs_url": "https://arxiv.org/abs/2511.13368",
        "pdf_url": "https://arxiv.org/pdf/2511.13368",
        "title": "Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning",
        "authors": [
            "Kajetan Dymkiewicz",
            "Ivan Vulic",
            "Helen Yannakoudakis",
            "Eilam Shapira",
            "Roi Reichart",
            "Anna Korhonen"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Large language models (LLMs) perform strongly across tasks and languages, yet how improvements in one task or language affect other tasks and languages and their combinations remains poorly understood. We conduct a controlled PEFT/LoRA study across multiple open-weight LLM families and sizes, treating task and language as transfer axes while conditioning on model family and size; we fine-tune each model on a single task-language source and measure transfer as the percentage-point change versus its baseline score when evaluated on all other task-language target pairs. We decompose transfer into (i) Matched-Task (Cross-Language), (ii) Matched-Language (Cross-Task), and (iii) Cross-Task (Cross-Language) regimes. We uncover two consistent general patterns. First, a pronounced on-task vs. off-task asymmetry: Matched-Task (Cross-Language) transfer is reliably positive, whereas off-task transfer often incurs collateral degradation. Second, a stable donor-recipient structure across languages and tasks (hub donors vs. brittle recipients). We outline implications for risk-aware fine-tuning and model specialisation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 324,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13373",
        "abs_url": "https://arxiv.org/abs/2511.13373",
        "pdf_url": "https://arxiv.org/pdf/2511.13373",
        "title": "A Novel Hierarchical Integration Method for Efficient Model Merging in Medical LLMs",
        "authors": [
            "Prakrit Timilsina",
            "Anuj Nepal",
            "Rajan Kadel",
            "Robin Doss"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Large Language Models (LLMs) face significant challenges in distributed healthcare, including consolidating specialized domain knowledge across institutions while maintaining privacy, reducing computational overhead, and preventing catastrophic forgetting during model this http URL paper presents a systematic evaluation of six parameter-space merging techniques applied to two architecturally compatible medical LLMs derived from the Mistral-7B base model. We introduce a novel hierarchical method that combines selective Optimal Transport (OT) alignment for attention layers with cosine similarity-weighted interpolation, designed to address permutation variance while minimizing computational overhead for edge deployment scenarios. Our study evaluates Task Arithmetic, Linear Averaging, DARE-TIES, DELLA, Breadcrumbs, and our Hierarchical approach across five medical benchmarks. Results demonstrate that architecturally compatible models benefit significantly from simple averaging methods, with Task Arithmetic achieving 45.80% accuracy on MedQA, outperforming complex pruning-based approaches. These findings offer critical insights for the deployment of distributed medical AI in resource-constrained IoT environments, where computational efficiency and model compatibility are paramount. Our work establishes that for architecturally compatible models, simple averaging provides a robust and computationally efficient baseline for knowledge consolidation, offering a pragmatic path forward for scalable medical AI systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 325,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13378",
        "abs_url": "https://arxiv.org/abs/2511.13378",
        "pdf_url": "https://arxiv.org/pdf/2511.13378",
        "title": "Moving Pictures of Thought: Extracting Visual Knowledge in Charles S. Peirce's Manuscripts with Vision-Language Models",
        "authors": [
            "Carlo Teo Pedretti",
            "Davide Picca",
            "Dario Rodighiero"
        ],
        "comments": "",
        "subjects": "Digital Libraries (cs.DL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multimedia (cs.MM)",
        "abstract": "Diagrams are crucial yet underexplored tools in many disciplines, demonstrating the close connection between visual representation and scholarly reasoning. However, their iconic form poses obstacles to visual studies, intermedial analysis, and text-based digital workflows. In particular, Charles S. Peirce consistently advocated the use of diagrams as essential for reasoning and explanation. His manuscripts, often combining textual content with complex visual artifacts, provide a challenging case for studying documents involving heterogeneous materials. In this preliminary study, we investigate whether Visual Language Models (VLMs) can effectively help us identify and interpret such hybrid pages in context. First, we propose a workflow that (i) segments manuscript page layouts, (ii) reconnects each segment to IIIF-compliant annotations, and (iii) submits fragments containing diagrams to a VLM. In addition, by adopting Peirce's semiotic framework, we designed prompts to extract key knowledge about diagrams and produce concise captions. Finally, we integrated these captions into knowledge graphs, enabling structured representations of diagrammatic content within composite sources.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 326,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13387",
        "abs_url": "https://arxiv.org/abs/2511.13387",
        "pdf_url": "https://arxiv.org/pdf/2511.13387",
        "title": "Generalized Denoising Diffusion Codebook Models (gDDCM): Tokenizing images using a pre-trained diffusion model",
        "authors": [
            "Fei Kong"
        ],
        "comments": "in Chinese language",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Recently, the Denoising Diffusion Codebook Models (DDCM) was proposed. DDCM leverages the Denoising Diffusion Probabilistic Model (DDPM) and replaces the random noise in the backward process with noise sampled from specific sets according to a predefined rule, thereby enabling image compression. However, DDCM cannot be applied to methods other than DDPM. In this paper, we propose the generalized Denoising Diffusion Compression Model (gDDCM), which extends DDCM to mainstream diffusion models and their variants, including DDPM, Score-Based Models, Consistency Models, and Rectified Flow. We evaluate our method on CIFAR-10 and LSUN Bedroom datasets. Experimental results demonstrate that our approach successfully generalizes DDCM to the aforementioned models and achieves improved performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 327,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13391",
        "abs_url": "https://arxiv.org/abs/2511.13391",
        "pdf_url": "https://arxiv.org/pdf/2511.13391",
        "title": "Finding Kissing Numbers with Game-theoretic Reinforcement Learning",
        "authors": [
            "Chengdong Ma",
            "Théo Tao Zhaowei",
            "Pengyu Li",
            "Minghao Liu",
            "Haojun Chen",
            "Zihao Mao",
            "Yuan Cheng",
            "Yuan Qi",
            "Yaodong Yang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Since Isaac Newton first studied the Kissing Number Problem in 1694, determining the maximal number of non-overlapping spheres around a central sphere has remained a fundamental challenge. This problem represents the local analogue of Hilbert's 18th problem on sphere packing, bridging geometry, number theory, and information theory. Although significant progress has been made through lattices and codes, the irregularities of high-dimensional geometry and exponentially growing combinatorial complexity beyond 8 dimensions, which exceeds the complexity of Go game, limit the scalability of existing methods. Here we model this problem as a two-player matrix completion game and train the game-theoretic reinforcement learning system, PackingStar, to efficiently explore high-dimensional spaces. The matrix entries represent pairwise cosines of sphere center vectors; one player fills entries while another corrects suboptimal ones, jointly maximizing the matrix size, corresponding to the kissing number. This cooperative dynamics substantially improves sample quality, making the extremely large spaces tractable. PackingStar reproduces previous configurations and surpasses all human-known records from dimensions 25 to 31, with the configuration in 25 dimensions geometrically corresponding to the Leech lattice and suggesting possible optimality. It achieves the first breakthrough beyond rational structures from 1971 in 13 dimensions and discovers over 6000 new structures in 14 and other dimensions. These results demonstrate AI's power to explore high-dimensional spaces beyond human intuition and open new pathways for the Kissing Number Problem and broader geometry problems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 328,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13397",
        "abs_url": "https://arxiv.org/abs/2511.13397",
        "pdf_url": "https://arxiv.org/pdf/2511.13397",
        "title": "Descriptor: Distance-Annotated Traffic Perception Question Answering (DTPQA)",
        "authors": [
            "Nikos Theodoridis",
            "Tim Brophy",
            "Reenu Mohandas",
            "Ganesh Sistu",
            "Fiachra Collins",
            "Anthony Scanlan",
            "Ciaran Eising"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "The remarkable progress of Vision-Language Models (VLMs) on a variety of tasks has raised interest in their application to automated driving. However, for these models to be trusted in such a safety-critical domain, they must first possess robust perception capabilities, i.e., they must be capable of understanding a traffic scene, which can often be highly complex, with many things happening simultaneously. Moreover, since critical objects and agents in traffic scenes are often at long distances, we require systems with not only strong perception capabilities at close distances (up to 20 meters), but also at long (30+ meters) range. Therefore, it is important to evaluate the perception capabilities of these models in isolation from other skills like reasoning or advanced world knowledge. Distance-Annotated Traffic Perception Question Answering (DTPQA) is a Visual Question Answering (VQA) benchmark designed specifically for this purpose: it can be used to evaluate the perception systems of VLMs in traffic scenarios using trivial yet crucial questions relevant to driving decisions. It consists of two parts: a synthetic benchmark (DTP-Synthetic) created using a simulator, and a real-world benchmark (DTP-Real) built on top of existing images of real traffic scenes. Additionally, DTPQA includes distance annotations, i.e., how far the object in question is from the camera. More specifically, each DTPQA sample consists of (at least): (a) an image, (b) a question, (c) the ground truth answer, and (d) the distance of the object in question, enabling analysis of how VLM performance degrades with increasing object distance. In this article, we provide the dataset itself along with the Python scripts used to create it, which can be used to generate additional data of the same kind.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 329,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13399",
        "abs_url": "https://arxiv.org/abs/2511.13399",
        "pdf_url": "https://arxiv.org/pdf/2511.13399",
        "title": "TripleFDS: Triple Feature Disentanglement and Synthesis for Scene Text Editing",
        "authors": [
            "Yuchen Bao",
            "Yiting Wang",
            "Wenjian Huang",
            "Haowei Wang",
            "Shen Chen",
            "Taiping Yao",
            "Shouhong Ding",
            "Jianguo Zhang"
        ],
        "comments": "Accepted by AAAI2026",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Scene Text Editing (STE) aims to naturally modify text in images while preserving visual consistency, the decisive factors of which can be divided into three parts, i.e., text style, text content, and background. Previous methods have struggled with incomplete disentanglement of editable attributes, typically addressing only one aspect - such as editing text content - thus limiting controllability and visual consistency. To overcome these limitations, we propose TripleFDS, a novel framework for STE with disentangled modular attributes, and an accompanying dataset called SCB Synthesis. SCB Synthesis provides robust training data for triple feature disentanglement by utilizing the \"SCB Group\", a novel construct that combines three attributes per image to generate diverse, disentangled training groups. Leveraging this construct as a basic training unit, TripleFDS first disentangles triple features, ensuring semantic accuracy through inter-group contrastive regularization and reducing redundancy through intra-sample multi-feature orthogonality. In the synthesis phase, TripleFDS performs feature remapping to prevent \"shortcut\" phenomena during reconstruction and mitigate potential feature leakage. Trained on 125,000 SCB Groups, TripleFDS achieves state-of-the-art image fidelity (SSIM of 44.54) and text accuracy (ACC of 93.58%) on the mainstream STE benchmarks. Besides superior performance, the more flexible editing of TripleFDS supports new operations such as style replacement and background transfer. Code: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 330,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13414",
        "abs_url": "https://arxiv.org/abs/2511.13414",
        "pdf_url": "https://arxiv.org/pdf/2511.13414",
        "title": "PAST: A Primary-Auxiliary Spatio-Temporal Network for Traffic Time Series Imputation",
        "authors": [
            "Hanwen Hu",
            "Zimo Wen",
            "Shiyou Qian",
            "Jian Co"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Traffic time series imputation is crucial for the safety and reliability of intelligent transportation systems, while diverse types of missing data, including random, fiber, and block missing make the imputation task challenging. Existing models often focus on disentangling and separately modeling spatial and temporal patterns based on relationships between data points. However, these approaches struggle to adapt to the random missing positions, and fail to learn long-term and large-scale dependencies, which are essential in extensive missing conditions. In this paper, patterns are categorized into two types to handle various missing data conditions: primary patterns, which originate from internal relationships between data points, and auxiliary patterns, influenced by external factors like timestamps and node attributes. Accordingly, we propose the Primary-Auxiliary Spatio-Temporal network (PAST). It comprises a graph-integrated module (GIM) and a cross-gated module (CGM). GIM captures primary patterns via dynamic graphs with interval-aware dropout and multi-order convolutions, and CGM extracts auxiliary patterns through bidirectional gating on embedded external features. The two modules interact via shared hidden vectors and are trained under an ensemble self-supervised framework. Experiments on three datasets under 27 missing data conditions demonstrate that the imputation accuracy of PAST outperforms seven state-of-the-art baselines by up to 26.2% in RMSE and 31.6% in MAE.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 331,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13418",
        "abs_url": "https://arxiv.org/abs/2511.13418",
        "pdf_url": "https://arxiv.org/pdf/2511.13418",
        "title": "Exploring Multi-Table Retrieval Through Iterative Search",
        "authors": [
            "Allaa Boutaleb",
            "Bernd Amann",
            "Rafael Angarita",
            "Hubert Naacke"
        ],
        "comments": "Accepted @ the AI for Tabular Data Workshop, EurIPS 2025",
        "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Databases (cs.DB); Machine Learning (cs.LG)",
        "abstract": "Open-domain question answering over datalakes requires retrieving and composing information from multiple tables, a challenging subtask that demands semantic relevance and structural coherence (e.g., joinability). While exact optimization methods like Mixed-Integer Programming (MIP) can ensure coherence, their computational complexity is often prohibitive. Conversely, simpler greedy heuristics that optimize for query coverage alone often fail to find these coherent, joinable sets. This paper frames multi-table retrieval as an iterative search process, arguing this approach offers advantages in scalability, interpretability, and flexibility. We propose a general framework and a concrete instantiation: a fast, effective Greedy Join-Aware Retrieval algorithm that holistically balances relevance, coverage, and joinability. Experiments across 5 NL2SQL benchmarks demonstrate that our iterative method achieves competitive retrieval performance compared to the MIP-based approach while being 4-400x faster depending on the benchmark and search space settings. This work highlights the potential of iterative heuristics for practical, scalable, and composition-aware retrieval.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 332,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13442",
        "abs_url": "https://arxiv.org/abs/2511.13442",
        "pdf_url": "https://arxiv.org/pdf/2511.13442",
        "title": "Unlocking the Forgery Detection Potential of Vanilla MLLMs: A Novel Training-Free Pipeline",
        "authors": [
            "Rui Zuo",
            "Qinyue Tong",
            "Zhe-Ming Lu",
            "Ziqian Lu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "With the rapid advancement of artificial intelligence-generated content (AIGC) technologies, including multimodal large language models (MLLMs) and diffusion models, image generation and manipulation have become remarkably effortless. Existing image forgery detection and localization (IFDL) methods often struggle to generalize across diverse datasets and offer limited interpretability. Nowadays, MLLMs demonstrate strong generalization potential across diverse vision-language tasks, and some studies introduce this capability to IFDL via large-scale training. However, such approaches cost considerable computational resources, while failing to reveal the inherent generalization potential of vanilla MLLMs to address this problem. Inspired by this observation, we propose Foresee, a training-free MLLM-based pipeline tailored for image forgery analysis. It eliminates the need for additional training and enables a lightweight inference process, while surpassing existing MLLM-based methods in both tamper localization accuracy and the richness of textual explanations. Foresee employs a type-prior-driven strategy and utilizes a Flexible Feature Detector (FFD) module to specifically handle copy-move manipulations, thereby effectively unleashing the potential of vanilla MLLMs in the forensic domain. Extensive experiments demonstrate that our approach simultaneously achieves superior localization accuracy and provides more comprehensive textual explanations. Moreover, Foresee exhibits stronger generalization capability, outperforming existing IFDL methods across various tampering types, including copy-move, splicing, removal, local enhancement, deepfake, and AIGC-based editing. The code will be released in the final version.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 333,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13444",
        "abs_url": "https://arxiv.org/abs/2511.13444",
        "pdf_url": "https://arxiv.org/pdf/2511.13444",
        "title": "Discovering Operational Patterns Using Image-Based Convolutional Clustering and Composite Evaluation: A Case Study in Foundry Melting Processes",
        "authors": [
            "Zhipeng Ma",
            "Bo Nørregaard Jørgensen",
            "Zheng Grace Ma"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Industrial process monitoring increasingly relies on sensor-generated time-series data, yet the lack of labels, high variability, and operational noise make it difficult to extract meaningful patterns using conventional methods. Existing clustering techniques either rely on fixed distance metrics or deep models designed for static data, limiting their ability to handle dynamic, unstructured industrial sequences. Addressing this gap, this paper proposes a novel framework for unsupervised discovery of operational modes in univariate time-series data using image-based convolutional clustering with composite internal evaluation. The proposed framework improves upon existing approaches in three ways: (1) raw time-series sequences are transformed into grayscale matrix representations via overlapping sliding windows, allowing effective feature extraction using a deep convolutional autoencoder; (2) the framework integrates both soft and hard clustering outputs and refines the selection through a two-stage strategy; and (3) clustering performance is objectively evaluated by a newly developed composite score, S_eva, which combines normalized Silhouette, Calinski-Harabasz, and Davies-Bouldin indices. Applied to over 3900 furnace melting operations from a Nordic foundry, the method identifies seven explainable operational patterns, revealing significant differences in energy consumption, thermal dynamics, and production duration. Compared to classical and deep clustering baselines, the proposed approach achieves superior overall performance, greater robustness, and domain-aligned explainability. The framework addresses key challenges in unsupervised time-series analysis, such as sequence irregularity, overlapping modes, and metric inconsistency, and provides a generalizable solution for data-driven diagnostics and energy optimization in industrial systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 334,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13457",
        "abs_url": "https://arxiv.org/abs/2511.13457",
        "pdf_url": "https://arxiv.org/pdf/2511.13457",
        "title": "Artificial Intelligence-Enabled Spirometry for Early Detection of Right Heart Failure",
        "authors": [
            "Bin Liu",
            "Qinghao Zhao",
            "Yuxi Zhou",
            "Zhejun Sun",
            "Kaijie Lei",
            "Deyun Zhang",
            "Shijia Geng",
            "Shenda Hong"
        ],
        "comments": "19 pages, 5 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Right heart failure (RHF) is a disease characterized by abnormalities in the structure or function of the right ventricle (RV), which is associated with high morbidity and mortality. Lung disease often causes increased right ventricular load, leading to RHF. Therefore, it is very important to screen out patients with cor pulmonale who develop RHF from people with underlying lung diseases. In this work, we propose a self-supervised representation learning method to early detecting RHF from patients with cor pulmonale, which uses spirogram time series to predict patients with RHF at an early stage. The proposed model is divided into two stages. The first stage is the self-supervised representation learning-based spirogram embedding (SLSE) network training process, where the encoder of the Variational autoencoder (VAE-encoder) learns a robust low-dimensional representation of the spirogram time series from the data-augmented unlabeled data. Second, this low-dimensional representation is fused with demographic information and fed into a CatBoost classifier for the downstream RHF prediction task. Trained and tested on a carefully selected subset of 26,617 individuals from the UK Biobank, our model achieved an AUROC of 0.7501 in detecting RHF, demonstrating strong population-level distinction ability. We further evaluated the model on high-risk clinical subgroups, achieving AUROC values of 0.8194 on a test set of 74 patients with chronic kidney disease (CKD) and 0.8413 on a set of 64 patients with valvular heart disease (VHD). These results highlight the model's potential utility in predicting RHF among clinically elevated-risk populations. In conclusion, this study presents a self-supervised representation learning approach combining spirogram time series and demographic data, demonstrating promising potential for early RHF detection in clinical practice.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 335,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13458",
        "abs_url": "https://arxiv.org/abs/2511.13458",
        "pdf_url": "https://arxiv.org/pdf/2511.13458",
        "title": "Trust in Vision-Language Models: Insights from a Participatory User Workshop",
        "authors": [
            "Agnese Chiatti",
            "Lara Piccolo",
            "Sara Bernardini",
            "Matteo Matteucci",
            "Viola Schiaffonati"
        ],
        "comments": "",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "With the growing deployment of Vision-Language Models (VLMs), pre-trained on large image-text and video-text datasets, it is critical to equip users with the tools to discern when to trust these systems. However, examining how user trust in VLMs builds and evolves remains an open problem. This problem is exacerbated by the increasing reliance on AI models as judges for experimental validation, to bypass the cost and implications of running participatory design studies directly with users. Following a user-centred approach, this paper presents preliminary results from a workshop with prospective VLM users. Insights from this pilot workshop inform future studies aimed at contextualising trust metrics and strategies for participants' engagement to fit the case of user-VLM interaction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 336,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13463",
        "abs_url": "https://arxiv.org/abs/2511.13463",
        "pdf_url": "https://arxiv.org/pdf/2511.13463",
        "title": "Multi-task GINN-LP for Multi-target Symbolic Regression",
        "authors": [
            "Hussein Rajabu",
            "Lijun Qian",
            "Xishuang Dong"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "In the area of explainable artificial intelligence, Symbolic Regression (SR) has emerged as a promising approach by discovering interpretable mathematical expressions that fit data. However, SR faces two main challenges: most methods are evaluated on scientific datasets with well-understood relationships, limiting generalization, and SR primarily targets single-output regression, whereas many real-world problems involve multi-target outputs with interdependent variables. To address these issues, we propose multi-task regression GINN-LP (MTRGINN-LP), an interpretable neural network for multi-target symbolic regression. By integrating GINN-LP with a multi-task deep learning, the model combines a shared backbone including multiple power-term approximator blocks with task-specific output layers, capturing inter-target dependencies while preserving interpretability. We validate multi-task GINN-LP on practical multi-target applications, including energy efficiency prediction and sustainable agriculture. Experimental results demonstrate competitive predictive performance alongside high interpretability, effectively extending symbolic regression to broader real-world multi-output tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 337,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13466",
        "abs_url": "https://arxiv.org/abs/2511.13466",
        "pdf_url": "https://arxiv.org/pdf/2511.13466",
        "title": "The Quick Red Fox gets the best Data Driven Classroom Interviews: A manual for an interview app and its associated methodology",
        "authors": [
            "Jaclyn Ocumpaugh",
            "Luc Paquette",
            "Ryan S. Baker",
            "Amanda Barany",
            "Jeff Ginger",
            "Nathan Casano",
            "Andres F. Zambrano",
            "Xiner Liu",
            "Zhanlan Wei",
            "Yiqui Zhou",
            "Qianhui Liu",
            "Stephen Hutt",
            "Alexandra M.A. Andres",
            "Nidhi Nasiar",
            "Camille Giordano",
            "Martin van Velsen",
            "Micheal Mogessi"
        ],
        "comments": "",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI); Emerging Technologies (cs.ET)",
        "abstract": "Data Driven Classroom Interviews (DDCIs) are an interviewing technique that is facilitated by recent technological developments in the learning analytics community. DDCIs are short, targeted interviews that allow researchers to contextualize students' interactions with a digital learning environment (e.g., intelligent tutoring systems or educational games) while minimizing the amount of time that the researcher interrupts that learning experience, and focusing researcher time on the events they most want to focus on DDCIs are facilitated by a research tool called the Quick Red Fox (QRF)--an open-source server-client Android app that optimizes researcher time by directing interviewers to users that have just displayed an interesting behavior (previously defined by the research team). QRF integrates with existing student modeling technologies (e.g., behavior-sensing, affect-sensing, detection of self-regulated learning) to alert researchers to key moments in a learner's experience. This manual documents the tech while providing training on the processes involved in developing triggers and interview techniques; it also suggests methods of analyses.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 338,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13478",
        "abs_url": "https://arxiv.org/abs/2511.13478",
        "pdf_url": "https://arxiv.org/pdf/2511.13478",
        "title": "Semantic Document Derendering: SVG Reconstruction via Vision-Language Modeling",
        "authors": [
            "Adam Hazimeh",
            "Ke Wang",
            "Mark Collier",
            "Gilles Baechler",
            "Efi Kokiopoulou",
            "Pascal Frossard"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Multimedia documents such as slide presentations and posters are designed to be interactive and easy to modify. Yet, they are often distributed in a static raster format, which limits editing and customization. Restoring their editability requires converting these raster images back into structured vector formats. However, existing geometric raster-vectorization methods, which rely on low-level primitives like curves and polygons, fall short at this task. Specifically, when applied to complex documents like slides, they fail to preserve the high-level structure, resulting in a flat collection of shapes where the semantic distinction between image and text elements is lost. To overcome this limitation, we address the problem of semantic document derendering by introducing SliDer, a novel framework that uses Vision-Language Models (VLMs) to derender slide images as compact and editable Scalable Vector Graphic (SVG) representations. SliDer detects and extracts attributes from individual image and text elements in a raster input and organizes them into a coherent SVG format. Crucially, the model iteratively refines its predictions during inference in a process analogous to human design, generating SVG code that more faithfully reconstructs the original raster upon rendering. Furthermore, we introduce Slide2SVG, a novel dataset comprising raster-SVG pairs of slide documents curated from real-world scientific presentations, to facilitate future research in this domain. Our results demonstrate that SliDer achieves a reconstruction LPIPS of 0.069 and is favored by human evaluators in 82.9% of cases compared to the strongest zero-shot VLM baseline.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 339,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True",
        "arxiv_id": "2511.13480",
        "abs_url": "https://arxiv.org/abs/2511.13480",
        "pdf_url": "https://arxiv.org/pdf/2511.13480",
        "title": "A Lexical Analysis of online Reviews on Human-AI Interactions",
        "authors": [
            "Parisa Arbab",
            "Xiaowen Fang"
        ],
        "comments": "10 pages, 1 table",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)",
        "abstract": "This study focuses on understanding the complex dynamics between humans and AI systems by analyzing user reviews. While previous research has explored various aspects of human-AI interaction, such as user perceptions and ethical considerations, there remains a gap in understanding the specific concerns and challenges users face. By using a lexical approach to analyze 55,968 online reviews from this http URL, this http URL, and this http URL, this preliminary research aims to analyze human-AI interaction. Initial results from factor analysis reveal key factors influencing these interactions. The study aims to provide deeper insights into these factors through content analysis, contributing to the development of more user-centric AI systems. The findings are expected to enhance our understanding of human-AI interaction and inform future AI technology and user experience improvements.",
        "gemini2.5flash": "",
        "overall_idea": ""
    }
]