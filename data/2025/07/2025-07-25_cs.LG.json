[
    {
        "order": 1,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17768",
        "abs_url": "https://arxiv.org/abs/2507.17768",
        "pdf_url": "https://arxiv.org/pdf/2507.17768",
        "title": "Enhancing Quantization-Aware Training on Edge Devices via Relative Entropy Coreset Selection and Cascaded Layer Correction",
        "authors": [
            "Yujia Tong",
            "Jingling Yuan",
            "Chuang Hu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "With the development of mobile and edge computing, the demand for low-bit quantized models on edge devices is increasing to achieve efficient deployment. To enhance the performance, it is often necessary to retrain the quantized models using edge data. However, due to privacy concerns, certain sensitive data can only be processed on edge devices. Therefore, employing Quantization-Aware Training (QAT) on edge devices has become an effective solution. Nevertheless, traditional QAT relies on the complete dataset for training, which incurs a huge computational cost. Coreset selection techniques can mitigate this issue by training on the most representative subsets. However, existing methods struggle to eliminate quantization errors in the model when using small-scale datasets (e.g., only 10% of the data), leading to significant performance degradation. To address these issues, we propose QuaRC, a QAT framework with coresets on edge devices, which consists of two main phases: In the coreset selection phase, QuaRC introduces the ``Relative Entropy Score\" to identify the subsets that most effectively capture the model's quantization errors. During the training phase, QuaRC employs the Cascaded Layer Correction strategy to align the intermediate layer outputs of the quantized model with those of the full-precision model, thereby effectively reducing the quantization errors in the intermediate layers. Experimental results demonstrate the effectiveness of our approach. For instance, when quantizing ResNet-18 to 2-bit using a 1% data subset, QuaRC achieves a 5.72% improvement in Top-1 accuracy on the ImageNet-1K dataset compared to state-of-the-art techniques.",
        "gemini2.5flash": "",
        "overall_idea": "作为本文的作者，我很荣幸能深入剖析我们的研究工作——“Enhancing Quantization-Aware Training on Edge Devices via Relative Entropy Coreset Selection and Cascaded Layer Correction (QuaRC)”，并回顾从选题到解决问题整个思维决策过程。我将通过具体例子来阐述。\n\n### 0. 为什么选择这个研究内容？\n\n回溯到研究初期，我们团队一直密切关注人工智能在边缘设备上的应用趋势。随着智能手机、无人机（如文中的具体案例）等边缘设备的普及，将复杂的AI模型部署到这些资源受限的设备上变得越来越重要。\n\n**我们观察到一个核心矛盾：**\n*   **需求：** 边缘设备需要高性能、低延迟的AI推理能力，尤其是在计算机视觉任务中（例如图像分类、目标检测）。\n*   **挑战：** 深度学习模型往往参数量巨大、计算复杂度高，无法直接在边缘设备上高效运行。\n\n**为了解决这个矛盾，我们看到了模型量化（Quantization）的巨大潜力。** 量化通过降低模型权重和激活值的位宽，显著减少了模型大小、内存占用和计算量。其中，**量化感知训练（QAT）**因其能在低位宽（如2-bit、3-bit）下保持较高精度而成为主流方法，优于训练后量化（PTQ）。\n\n然而，我们很快发现QAT在**边缘设备上实践**时面临着新的瓶手：\n1.  **数据隐私与通信限制：** 边缘设备产生的数据往往是敏感的（例如，无人机拍摄的建筑裂缝图像可能涉及隐私或安全），不方便上传到云端进行集中训练。同时，网络带宽和延迟也限制了大规模数据传输。\n2.  **计算与时间成本：** 传统的QAT需要利用**整个数据集**进行再训练。即使是中小型数据集，例如CIFAR-100上的MobileNetV2，其QAT过程也需要数十分钟，这在资源受限且需要快速响应的边缘设备上是**不可接受的**。例如，如果无人机每次执行任务都要用海量数据进行QAT，那效率会非常低。\n\n因此，一个明确的研究方向浮现出来：**如何使QAT在边缘设备上，利用少量本地数据，高效且高精度地完成模型再训练？** 我们认为，**Coreset Selection**（核心集选择）技术是解决这一问题的关键，因为它可以通过选择最具代表性的数据子集来显著减少训练量。\n\n### 1. 您是如何一步步构思和界定研究问题的？\n\n确定了“Coreset for QAT on Edge”的大方向后，我们开始深入分析现有方法的不足，从而精确地界定我们的研究问题。\n\n**第一步：分析现有Coreset方法的局限性。**\n*   我们研究了当时主流的核心集选择方法（如Moderate、Contextual Diversity、Forgetting、ACS等）。\n*   **初步发现：** 这些方法大多是为**全精度模型**设计的。它们关注的是如何选择能代表整个数据集分布的样本，或者能最大化梯度更新的样本。\n*   **核心问题浮现：** 这些方法**没有直接考虑量化误差**的特性。量化过程引入的舍入（rounding）和截断（clamping）误差是QAT特有的挑战。如果核心集无法有效捕捉这些量化误差，那么即使在“代表性”的数据上训练，模型也难以真正适应量化噪声。**ACS**虽然尝试将核心集选择与QAT结合，从梯度角度考虑样本影响力，但我们直觉认为，仅仅关注梯度不足以完全解决量化带来的精度损失。\n\n**第二步：明确量化误差的表现形式。**\n*   量化误差的直接表现是什么？就是**全精度模型和量化模型对同一个输入产生输出（logits）的差异**。这种差异越大，说明该样本对量化误差的贡献越大，也越能激发模型学习如何应对量化。\n*   **决策：** 我们需要一个指标来量化这种“差异”，并用它来指导核心集选择。\n\n**第三步：揭示小规模核心集训练的深层问题。**\n*   即使我们选择了“正确”的样本，但当核心集非常小（例如，只有1%的数据）时，仅仅依赖最终输出层的知识蒸馏（KD）损失是否足够？\n*   **假设：** 量化误差会在模型深层传播并累积。最终输出层的KD损失可能无法有效地修正中间层的累积误差。在全数据集上，大量数据可以通过平均效应“平滑”这些中间层误差，但在数据稀缺的小核心集上，这些误差就会变得尤为突出。\n*   **验证（关键步骤）：** 我们对量化模型和全精度模型的中间层输出进行了对比（如论文图2(b)所示）。结果明确显示，即使在最终输出层表现尚可的情况下，它们的**中间层输出分布存在显著差异**（KL散度值很高），这证实了我们的假设。这意味着，模型内部的特征表示已经“偏离”了全精度模型的理想状态，仅仅优化最终输出是治标不治本。\n\n**经过这三步分析，我们将研究问题精确地界定为：**\n1.  **如何设计一种核心集选择方法，能够**显式地捕捉和量化**模型在**量化过程中引入的误差**，并优先选择那些能最大化消除这些误差的样本？\n2.  **如何在极小规模的核心集上，有效地**抑制和消除量化误差在模型中间层**的传播和累积，从而确保量化模型即使在数据稀缺的情况下也能保持高性能？\n\n这就是QuaRC框架的两大核心支柱——“Relative Entropy Coreset Selection”和“Cascaded Layer Correction”的由来。\n\n### 2. 为了解决该问题，您在选择理论框架、研究方法和数据分析策略时，经历了怎样的思考、权衡和取舍。\n\n基于上述问题界定，我们开始构思解决方案，并在每个环节都进行了细致的思考和权衡。\n\n**2.1. 核心集选择（解决问题1：捕捉量化误差）**\n\n*   **思考方向：** 我们需要一个衡量全精度模型输出与量化模型输出之间“差异”的指标。\n*   **可选方案：**\n    *   **L2距离：** 简单直观，但可能对异常值敏感，且不适合衡量概率分布的差异。\n    *   **交叉熵：** 适用于分类任务，但通常用于衡量预测分布与真实标签的距离，不直接适用于比较两个模型的输出分布。\n    *   **KL散度（Relative Entropy）：** 这是我们的最终选择。\n*   **权衡与决策：** 我们选择了**KL散度**来定义“相对熵得分”（Relative Entropy Score, RES）。\n    *   **逻辑依据：** KL散度是衡量两个概率分布之间差异的经典方法。在QAT中，我们将模型的logits通过softmax转换为概率分布，KL散度能很好地捕捉量化导致的信息损失或分布偏移。一个高RES的样本意味着量化模型在这个样本上的输出与全精度模型差异巨大，这正是我们需要模型去重点学习和修正的。\n    *   **验证：** 我们设计了一个关键实验（论文图2(a)）。通过选择不同平均RES的核心集进行训练，并计算量化模型的Top-1准确率，我们发现RES与最终模型性能之间存在**强烈的正相关（Spearman相关系数0.867，p值0.00117）**。这一结果有力地证明了RES作为量化误差指标的有效性。\n*   **与现有方法的融合：** 仅仅依靠RES可能不够全面，梯度信息对于模型参数的通用学习也很重要。因此，我们决定将RES与现有梯度信息相关的指标（如ACS中的devs和dps）**结合**起来。\n    *   **权衡：** 增加了指标计算的复杂性，但在核心集选择阶段的计算开销远小于完整训练过程。\n    *   **决策：** 采用动态加权的方式（如公式(6)中的`a(t)`），在训练初期可能更侧重梯度，后期则更侧重量化误差，以达到动态平衡。\n\n**2.2. 量化模型训练阶段（解决问题2：消除中间层误差传播）**\n\n*   **思考方向：** 既然中间层存在显著误差累积，我们就需要一种策略来强制量化模型学习与全精度模型更接近的中间层表示。\n*   **可选方案：**\n    *   **简单的特征蒸馏：** 在某一层或几层添加L2损失或KL散度损失。\n    *   **多层级蒸馏：** 逐层或多层地进行蒸馏。\n*   **权衡与决策：** 我们提出了**Cascaded Layer Correction (CLC)**策略。\n    *   **逻辑依据：** “Cascaded”（级联）意味着我们不只关注一个中间层，而是将这种对齐机制推广到多个中间层。具体做法是，在训练过程中，计算量化模型和全精度模型在**多个选定中间层**的输出分布，并最小化它们之间的KL散度（如公式(8)）。\n    *   **优势：** 这确保了量化模型不仅在最终输出上与全精度模型相似，其内部的特征学习过程也保持一致性，从而从根本上缓解了量化误差的累积。这是对传统知识蒸馏（只关注最终输出）的重要补充。\n    *   **与整体训练的融合：** 将CLC损失（LCLC）与传统的知识蒸馏损失（LKD）**叠加**到总训练损失中（如公式(9)）。\n    *   **权衡：** 引入CLC会增加额外的计算，因为需要对全精度模型进行前向传播以获取中间层输出。但是，在总体的计算复杂性分析中（Section IV），我们证明了由于核心集的大幅缩小，CLC带来的额外开销是可接受的，并且显著提升了训练效率和模型性能。\n\n**2.3. 数据分析策略**\n\n*   **核心指标：** Top-1和Top-5准确率，直接衡量模型性能。\n*   **对比方法：**\n    *   **基线（Baseline）：** 随机采样（Random Sampling），这是最简单的核心集选择方法。\n    *   **现有先进方法：** Forgetting、Contextual Diversity、Moderate、ACS，这些都是当时核心集选择领域的代表性工作。\n*   **消融实验：** 这是我们证明RES和CLC有效性的关键。我们设计了详细的消融实验，分别评估：\n    *   仅使用RES核心集选择，不加CLC。\n    *   仅使用CLC训练，采用随机核心集。\n    *   同时使用RES和CLC。\n    *   通过对比，清晰地展示了RES和CLC各自的贡献以及它们的协同效应。\n*   **量化位宽与核心集比例：** 在2-bit、3-bit、4-bit等不同量化位宽以及1%、3%、5%、7%、10%等不同核心集比例下进行实验，以证明方法的**鲁棒性和泛化性**。\n*   **实际案例研究：** 无人机裂缝检测。这是一个将所有理论和方法落实到真实应用场景的“大考”，直接展示了QuaRC在解决实际问题上的有效性、效率和精度。\n\n### 3. 在研究的每个关键阶段，您做出了哪些重要决策，并能解释这些决策背后的逻辑和依据。\n\n我将通过**无人机混凝土裂缝检测**的实际案例来具体说明这些决策。\n\n**阶段一：问题明确与目标设定**\n\n*   **决策：** 针对无人机巡检场景，QAT必须在本地设备上高效完成，且不能牺牲精度。\n*   **逻辑与依据：** 无人机拍摄的裂缝图像具有隐私性，且数据量大，无法频繁上传云端。每次新任务都可能面临数据分布变化，需要模型适应性再训练。传统QAT的计算量和数据传输量是不可承受之重。目标是：**在边缘设备上，用极少量数据，快速且高精度地再训练量化模型。**\n\n**阶段二：核心集选择**\n\n*   **决策1：引入相对熵得分（RES）。**\n    *   **逻辑与依据：** 我们意识到，传统的基于梯度的核心集选择（如ACS）虽然重要，但它们并未直接量化“量化误差”本身。RES通过比较全精度模型和量化模型对同一图片（例如，一张裂缝图像）的输出概率分布差异，直接捕捉了量化引入的信息损失。对于一张裂缝图像，如果量化模型对裂缝类别的预测概率与全精度模型差异很大（高RES），则这张图片对学习如何克服量化误差至关重要。\n    *   **无人机案例体现：** 当无人机采集了新的混凝土裂缝图片集（3000张）后，我们首先让这些图片分别通过预训练的全精度MobileNetV2和初始量化MobileNetV2。计算每张图片的RES，那些量化前后输出差异大的图片，往往包含量化模型难以处理的纹理、光照、裂缝形态等信息。\n*   **决策2：将RES与其他梯度指标结合。**\n    *   **逻辑与依据：** 尽管RES直接反映量化误差，但梯度信息（即样本对模型参数更新的影响）对于模型的一般性学习仍然不可或缺。我们选择了一个动态平衡的策略，使核心集既能捕捉量化误差的敏感样本，也能保留对模型权重更新贡献大的样本。\n    *   **无人机案例体现：** 从3000张新图片中，我们计算了每张图片的综合得分，结合了RES和梯度信息。最终，我们**仅选择了Top 10%（即300张）最具代表性的图片**作为训练核心集。这个决策确保了核心集既小又“含金量高”，为后续的本地训练奠定了基础。\n\n**阶段三：量化模型训练**\n\n*   **决策1：引入级联层校正（CLC）。**\n    *   **逻辑与依据：** 仅仅依靠最终输出层的知识蒸馏在小规模核心集上效果不佳，因为量化误差会在中间层累积并传播。我们之前的实验（图2(b)和图3）明确展示了这种中间层分布的失真。CLC策略直接针对这一问题，通过最小化量化模型与全精度模型**多个中间层输出**的KL散度，强制量化模型在内部表示层面与全精度模型对齐。\n    *   **无人机案例体现：** 在选定这300张核心图片后，我们将它们输入到全精度MobileNetV2和待训练的量化MobileNetV2。除了传统的预测损失和最终层知识蒸馏损失外，我们额外加入了CLC损失。例如，MobileNetV2的某些特定中间层（如在倒数第二层，见图3）的输出，会被计算其量化前后与全精度模型的KL散度，并最小化这个差异。这个“级联”的过程确保了模型从浅层到深层，其内部对裂缝特征的提取和表示都尽量接近全精度模型，即使训练数据量极少。\n*   **决策2：组合LKD和LCLC。**\n    *   **逻辑与依据：** LKD保证最终预测的一致性，LCLC则保证内部特征表示的一致性。两者结合，可以从全局和局部两个层面优化量化模型的性能。\n\n**阶段四：评估与验证**\n\n*   **决策1：进行全面的实验对比和消融研究。**\n    *   **逻辑与依据：** 不仅要证明我们的方法有效，还要证明为什么有效。消融实验（如表III、IV、V）清晰地展示了RES和CLC各自的贡献以及它们结合后的强大协同效应。\n    *   **无人机案例体现：** 我们的QuaRC框架在仅使用10%数据（300张图片）的情况下，将QAT训练时间从全数据集的17.89分钟缩短到**2.55分钟（提速7.02倍）**，而Top-1准确率仅从全精度的99.33%轻微下降到**99.00%**。这直接证明了QuaRC在边缘设备上的**效率和高精度兼得**的能力。\n\n**总结：**\n\nQuaRC的整个设计过程，就是围绕“在边缘设备上进行高效、高精度QAT”这一核心挑战，通过细致的问题拆解（核心集选择和中间层误差），创新性地提出针对性解决方案（RES和CLC），并辅以严谨的实验验证，最终成功证明了其在实际应用中的巨大潜力。特别是无人机案例，将量化感知训练从云端拉到边缘，不仅解决了隐私和通信问题，更在计算效率上实现了质的飞跃，这正是我们作为研究者所追求的价值。"
    },
    {
        "order": 2,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17784",
        "abs_url": "https://arxiv.org/abs/2507.17784",
        "pdf_url": "https://arxiv.org/pdf/2507.17784",
        "title": "Knowledge Abstraction for Knowledge-based Semantic Communication: A Generative Causality Invariant Approach",
        "authors": [
            "Minh-Duong Nguyen",
            "Quoc-Viet Pham",
            "Nguyen H. Tran",
            "Hoang-Khoi Do",
            "Duy T. Ngo",
            "Won-Joo Hwang"
        ],
        "comments": "13 pages, 12 figures, 4 tables",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In this study, we design a low-complexity and generalized AI model that can capture common knowledge to improve data reconstruction of the channel decoder for semantic communication. Specifically, we propose a generative adversarial network that leverages causality-invariant learning to extract causal and non-causal representations from the data. Causal representations are invariant and encompass crucial information to identify the data's label. They can encapsulate semantic knowledge and facilitate effective data reconstruction at the receiver. Moreover, the causal mechanism ensures that learned representations remain consistent across different domains, making the system reliable even with users collecting data from diverse domains. As user-collected data evolves over time causing knowledge divergence among users, we design sparse update protocols to improve the invariant properties of the knowledge while minimizing communication overheads. Three key observations were drawn from our empirical evaluations. Firstly, causality-invariant knowledge ensures consistency across different devices despite the diverse training data. Secondly, invariant knowledge has promising performance in classification tasks, which is pivotal for goal-oriented semantic communications. Thirdly, our knowledge-based data reconstruction highlights the robustness of our decoder, which surpasses other state-of-the-art data reconstruction and semantic compression methods in terms of Peak Signal-to-Noise Ratio (PSNR).",
        "gemini2.5flash": "",
        "overall_idea": "好的，各位读者、同行，非常荣幸能以作者的身份，向大家深入剖析我们这篇题为《面向知识型语义通信的知识抽象：一种生成式因果不变性方法》的研究工作。从最初的选题萌芽到最终成果的落地，每一步都凝聚了团队的思考、权衡与坚持。我将通过一个贯穿始终的**具体例子——彩色MNIST数据集上的数字识别与图像重建**，来详细阐述我们的思维决策过程。\n\n---\n\n### 0. 为什么选择这个研究内容？\n\n各位知道，现代通信正朝着更智能、更高效的方向发展，尤其是在虚拟现实、自动驾驶这类对延迟和带宽极度敏感的应用中，传统通信方式的效率瓶颈日益凸显。语义通信（Semantic Communication, SemCom）应运而生，它旨在传递信息的“意义”，而非简单比特流，从而大幅提升通信效率。\n\n在我看来，SemCom的核心挑战之一，是如何**准确而高效地抽象和传递“语义知识”**。当时，现有SemCom研究主要集中在信道编码和解码的效率提升，或是通过预训练模型、反馈机制等方式引入“知识”。然而，我敏锐地观察到几个痛点：\n\n*   **泛化性不足：** 许多方法过度依赖特定数据集或任务，一旦遇到新的数据域（Domain Shift），性能便直线下降。这在分布式、异构的移动通信环境中是致命的。例如，一个模型在“绿色数字”数据集上训练得很好，到了“蓝色数字”数据集上可能就完全失效。\n*   **知识获取成本高：** 人工定义知识或大型语言模型（LLM）虽然强大，但前者耗时耗力，后者计算复杂度极高，不适合资源受限的边缘设备。\n*   **通信开销：** 即使能获取知识，如何高效更新和同步这些知识，也是一个难题。频繁的知识传输会抵消SemCom带来的效率提升。\n\n这些痛点促使我思考：**我们能否找到一种数据本身就具备的、跨领域稳定的“核心意义”（语义知识），并以一种低成本、自适应的方式将其提取出来？** 这就自然而然地将我的研究兴趣引向了**因果推断（Causal Inference）**和**不变性学习（Invariance Learning）**。如果能从数据中学习到对其“因果”特征的表示，那么这些特征本身就应该是跨域不变的。\n\n### 1. 您是如何一步步构思和界定研究问题的？\n\n最初的问题非常宽泛：“如何让语义通信对数据变化更鲁棒？”为了将其具化，我开始思考“数据变化”的本质。\n\n*   **Step 1: 识别核心问题——领域漂移（Domain Shift）。** 在分布式场景下，不同用户收集的数据可能具有不同的统计特性（例如，用户A拍的数字都是绿色的，用户B拍的都是蓝色的）。传统的机器学习模型很容易学习到这些“虚假关联”（Spurious Correlation），而非数据真正的“因果”特征。\n    *   **例子：** 在**彩色MNIST**中，数字的**“颜色”**是领域信息（Domain），而**“数字形状”**才是真正的**因果特征（Causal Feature）**，即决定其是数字“7”的内在属性。一个模型如果学到了“绿色”就是“7”，那么它在蓝色“7”上就会出错。\n\n*   **Step 2: 引入因果不变性概念。** 我们的目标是提取出**不受领域变化影响的、稳定地决定数据“含义”的表示**。这正是因果不变性（Causal Invariance）的核心思想：如果Y是X的因果结果，那么X与Y之间的关系在不同环境（领域）下都应该是稳定的。\n    *   **界定问题：** 我们需要一个模型，它能将原始数据`x`分解为两个互补的潜在表示：\n        *   **不变性表示（Invariant Representation, ZK）：** 包含数据的“核心意义”（如数字的形状），它是因果的，并且在不同领域（颜色）下保持一致。这部分就是我们想要的“语义知识”。\n        *   **变异性表示（Variant Representation, ZV）：** 包含与领域相关的、非因果的细节信息（如数字的颜色、背景噪声）。这部分是可变的，用于辅助数据重建。\n\n*   **Step 3: 确定具体目标（Objectives）。** 基于上述分解，我们为模型设定了以下明确的目标，这些目标直接对应了论文中的Objective 1-4：\n    *   **目标1 (不变性知识):** 提取的`ZK`必须对标签（数字类别）保持不变性。\n    *   **目标2 (高效压缩):** `ZV`的大小应小于原始数据`x`，因为`ZK`作为知识可以独立存储和传输。\n    *   **目标3 (高效知识提取):** `ZV`在同一类别内部应具有高方差，以确保它捕捉到数据中的非不变性（变异性）信息。\n    *   **目标4 (有意义的表示):** `ZK`和`ZV`的组合必须能准确地重建原始数据`x`，这确保了我们没有丢失关键信息。\n\n### 2. 为了解决该问题，您在选择理论框架、研究方法和数据分析策略时，经历了怎样的思考、权衡和取舍？\n\n一旦问题界定清楚，下一步就是寻找合适的工具和方法。\n\n*   **理论框架的选择：**\n    *   **结构化因果模型 (SCM):** 这是我们因果不变性思想的理论基石。SCM提供了一个严谨的框架来思考数据生成过程中的因果关系，从而指导我们去寻找那些在干预（不同领域）下依然稳定的机制。\n    *   **生成式对抗网络 (GAN):** 为什么是GAN而不是简单的自编码器（Autoencoder）？权衡如下：\n        *   **传统AE/VAE:** 擅长重建，但在强制 disentanglement（解耦）时，往往难以彻底分离因果和非因果特征，可能会把因果信息混入非因果通道，或者在尝试分离时丢失信息。\n        *   **GAN的优势:** GAN的对抗性训练机制非常适合“强制”模型学习某种特性。我们可以设计一个判别器，专门识别`ZK`中是否包含领域信息，然后训练生成器去“欺骗”这个判别器，从而强制`ZK`变得领域不变。这种对抗性使得分离更加彻底。\n    *   **条件生成式AI (Conditional GAI):** 我们的编码器和解码器都需要以“知识”作为条件进行操作。CGAN和CVAE提供了很好的范式来引入这些条件变量。\n    *   **原型网络 (Prototypical Networks):** 这是用于少样本学习的利器。它通过计算类别的“原型”（平均嵌入向量）来简化分类任务。这非常契合我们对`ZK`的要求：同类别的`ZK`应该非常接近其类别原型，从而确保`ZK`不仅不变，而且具有强烈的类别辨识度。\n\n*   **研究方法和模型设计：**\n    *   **核心架构——UKIE (Unified Knowledge retrieval via Invariant Extractor)：**\n        *   **不变性提取器 (`fok`):** 负责从数据中提取`ZK`。我们用ResNet9作为骨干网络，因为它相对轻量级，适合边缘设备。\n        *   **变异性提取器 (`fv`):** 负责提取`ZV`。同样使用ResNet9。\n        *   **解码器 (`gθ2`):** 将`ZK`和`ZV`结合起来重建原始数据。\n    *   **损失函数设计 (这部分是关键的权衡和决策点)：**\n        *   **1. 不变性学习 (`Liv`, Eq. 11):** 这是核心。我们希望同一类别的所有样本（无论它们来自哪个领域）在`ZK`空间中都聚在一起。我们借鉴了原型网络的思想，最小化同类别样本到其类原型的距离。\n            *   **权衡：** 过于强调不变性可能导致`ZK`失去判别力。因此，它不能是唯一的损失。\n        *   **2. 变异性学习 (`Lv`, Eq. 12):** 我们希望`ZV`能捕捉到类内变化，即不同领域下的同一类别样本的差异性。所以，我们最大化同类别样本在`ZV`空间中的距离，同时使用Hinge Loss来防止其无限制地扩大。\n            *   **权衡：** 如果`Lv`权重过高，可能导致`ZV`捕捉过多无关信息，或使得`ZK`被迫承担它不该承担的信息。Lemma 1（`Var(ZV) > Var(Z)`）给了我们理论依据，即变异信息确实增加了整体表示的方差，因此是不可或缺的。\n        *   **3. 有意义的表示 (`Lgtc` and `Ladv`, Eq. 14, 15):** 这是GAN的核心作用，用于确保`ZK`既有意义又真正不变。\n            *   **`Lgtc` (Label Classification Loss):** 判别器中的分类器部分，确保`ZK`能够准确预测数据的标签（例如，它知道这是数字“7”）。这强制`ZK`包含足够的语义信息。\n            *   **`Ladv` (Adversarial Loss):** 这是我们实现因果不变性的关键。我们训练一个“域判别器”(`Ψ`)，试图从`ZK`中预测出数据来自哪个领域（例如，它是绿色的“7”还是蓝色的“7”）。同时，我们训练`fok`去“欺骗”这个域判别器，让`ZK`变得无法被识别其来源领域。\n            *   **权衡：** 这是一个min-max博弈。如果`Ladv`太弱，`ZK`可能依然包含领域信息；如果太强，`ZK`可能会丢失所有信息，变得无用。我们通过超参数调整来找到平衡点。\n        *   **4. 重建损失 (`Lrec`, Eq. 16):** 这是为了满足“有意义的表示”和“高效知识提取”的目标。我们使用MSE和KL散度，确保`ZK`和`ZV`共同能够完美重建原始数据。\n            *   **权衡：** 这是一个监督信号，防止模型在 disentanglement 过程中丢失原始数据的任何有用信息。\n\n*   **数据分析策略：**\n    *   **多样化的数据集：** EMNIST, CIFAR-10, CINIC-10, Celeb-A，覆盖了手写数字、自然图像和人脸，验证模型的通用性。\n    *   **核心验证——领域泛化：** **彩色MNIST**是我们的“试金石”。通过训练模型在一个颜色-数字关联的领域上，然后测试其在另一个不同颜色-数字关联的领域上的表现，来验证`ZK`的因果不变性。\n    *   **核心指标：**\n        *   **PSNR (Peak Signal-to-Noise Ratio):** 衡量数据重建质量。\n        *   **Accuracy:** 衡量下游任务（分类）性能，验证`ZK`的语义识别能力。\n        *   **损失值本身：** `Liv`、`Lv`、`Lrec`的数值变化直接反映了不变性、变异性提取和重建的效果。\n    *   **消融研究 (Ablation Study):** 这是验证每个设计组件必要性的关键。通过系统地移除或调整特定损失项或网络组件的权重，观察性能变化，从而证明我们每个决策的合理性。\n\n### 3. 在研究的每个关键阶段，您做出了哪些重要决策，并能解释这些决策背后的逻辑和依据。\n\n我将以**彩色MNIST**这个例子，结合实验结果，来阐述关键决策。\n\n**阶段一：问题定义与初步构想**\n\n*   **决策：** 将核心问题聚焦于“领域漂移下的语义通信鲁棒性”，并引入“因果不变性”作为解耦和泛化的核心原则。\n*   **逻辑：** 传统正则化方法在数据不共享时效果不佳，而因果特征本身就具备跨环境的稳定性。如果能提取因果特征，那么模型理论上就能无缝适应新环境。\n*   **依据：** 对现有SemCom和不变性学习文献的深刻理解，以及对分布式通信场景下数据异构性的前瞻性判断。\n\n**阶段二：模型架构与损失函数设计**\n\n*   **决策：** 采用GAN框架，并设计了`fok`（提取ZK）和`fv`（提取ZV）两个分支，以及一套包含`Liv`, `Lv`, `Lgtc`, `Ladv`, `Lrec`的复合损失函数。\n*   **逻辑与依据（以彩色MNIST为例）：**\n    *   **为什么用两个分支？** 这是为了明确解耦。`fok`只关注数字形状（不变信息），`fv`只关注颜色（变异信息）。\n    *   **`Liv` (`fok`的不变性)：** 确保所有“7”在`ZK`空间里都长一个样，无论它是绿色的还是蓝色的。这样`ZK`就“忘记”了颜色。我们看到Table II中`Liv`非常低（1.5 x 10^-5），这证明了其有效性。\n    *   **`Lv` (`fv`的变异性)：** 确保绿色的“7”和蓝色的“7”在`ZV`空间里是有显著差异的。Lemma 1在理论上支持了`ZV`需要高方差来承载额外信息。Table II中`Lv`非常高（0.99），也验证了`ZV`成功捕捉到变异信息。\n    *   **`Lgtc` (`ZK`的语义有意义性)：** 尽管`ZK`忘记了颜色，但它必须能准确地告诉我它是个“7”。这就是判别器分类部分的作用。Table II中极高的Accuracy（EMNIST 99.18%）证明`ZK`具有强大的语义识别能力。\n    *   **`Ladv` (`fok`的因果性强化)：** 这是最巧妙的一环。我们训练一个判别器去猜`ZK`来自哪个颜色域，然后训练`fok`去骗它。如果`fok`成功地让判别器猜不准颜色，那么`ZK`就真正地与颜色解耦了，从而实现了因果不变性。这就是防止`fok`学习到虚假关联的关键。\n    *   **`Lrec` (重建能力)：** `ZK`和`ZV`都需要有意义，才能合起来重建原始的彩色数字。这保证了模型不会为了不变性而丢弃过多信息。Figure 3和Figure 4的重建结果（以及Table III的高PSNR）展示了UKIE强大的重建能力。\n\n**阶段三：训练策略与超参数调优**\n\n*   **决策：** 设计分阶段的交替训练策略（Algorithm 1），并进行充分的消融实验来调整损失函数的权重（`αrec, αiv, αgtc, αadv`）。\n*   **逻辑与依据：**\n    *   **交替训练：** 先训练生成器让它学会初步解耦和重建，再训练判别器让它更精细地识别不变性和领域信息。这有助于模型稳定收敛，避免陷入局部最优。\n    *   **超参数调优：** 图12的消融实验是关键。\n        *   **`αgtc` (语义分类权重)：** 我们发现，增加`αgtc`可以显著提升分类准确率（因为`ZK`被迫更关注标签）。但超过某个点（如`αgtc=1`），性能提升不再显著。**决策：将`αgtc`设为1。** 逻辑是，一旦`ZK`已经充分学习了标签信息，再增加其权重可能导致过拟合或与`Ladv`冲突。\n        *   **`αrec` (重建权重)：** 提高`αrec`能够改善重建质量，但在高`αrec`下，模型可能会倾向于把所有信息都塞进`ZK`或`ZV`，而牺牲解耦性。**决策：通过实验找到平衡点。** 我们发现，在不显著影响重建质量的前提下，可以适当降低`αrec`，将更多精力放在解耦上。\n        *   **`αiv` (不变性权重)：** 增加`αiv`能够强制`ZK`更具不变性（`Liv`下降），但过高可能导致`ZK`失去分类能力。**决策：设`αiv`为0.25。** 这是一个甜点区，在保持低`Liv`的同时，分类准确率也达到了高点。\n        *   **`αv` (变异性权重)：** 提高`αv`确保`ZV`充分捕捉变异性。**决策：通过实验发现，即使`αv`设置较低，`ZV`也能有效捕捉变异信息，这利于后续的通信压缩。**\n\n**阶段四：通信效率与泛化能力验证**\n\n*   **决策：** 在物理信道上，只传输压缩后的`ZV`。`ZK`作为共享知识，通过语义信道稀疏更新。\n*   **逻辑与依据：**\n    *   **高效通信：** 由于`ZK`是因果不变的，它在不同领域下非常稳定，因此无需频繁传输。只有当“知识本身”发生根本性变化时（例如，出现了全新的数字字体，或者需要识别新的物体类别），才通过语义信道进行稀疏更新（Eq. 9）。这大幅降低了通信开销。\n    *   **鲁棒性：** 即使物理信道非常噪声（例如，SNR很低），只要`ZK`能被正确检索，重建性能依然保持稳定。图7和图8清晰地展示了UKIE在低SNR下的PSNR和准确率优势。这得益于语义知识的“抗噪声”特性。\n    *   **领域泛化结果 (Table IV):** 在Colored-MNIST上的实验是决定性的。模型在某个领域（例如，90%绿色关联数字）训练后，在其他具有不同关联（例如，80%蓝色关联或-90%红色关联）的领域上进行测试，UKIE的准确率保持了高度一致（例如，Domain 1、2、3上的准确率分别为89.5%、73.9%、73.2%）。`Liv`始终保持在极低水平（0.0002），这有力地证明了**`ZK`的因果不变性及其在领域泛化方面的卓越能力**。\n\n---\n\n**总结**\n\n整个研究过程，如同在迷雾中寻找一条清晰的路径。我们从SemCom的实际痛点出发，敏锐地捕捉到“因果不变性”这一理论线索，并将其与GAN、原型网络等前沿AI技术巧妙融合。每一个设计决策，从模型架构到损失函数，从训练策略到实验验证，都经过了严谨的逻辑推敲和充分的经验依据。彩色MNIST这个具体的例子，完美地展现了我们如何通过解耦因果与非因果特征，从而在实现高效语义通信的同时，确保了系统在复杂多变环境下的鲁棒性和泛化能力。我们相信，这种基于因果不变性的知识抽象方法，为未来的知识型语义通信开辟了一条充满希望的道路。"
    },
    {
        "order": 3,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17785",
        "abs_url": "https://arxiv.org/abs/2507.17785",
        "pdf_url": "https://arxiv.org/pdf/2507.17785",
        "title": "Self-similarity Analysis in Deep Neural Networks",
        "authors": [
            "Jingyi Ding",
            "Chengwen Qi",
            "Hongfei Wang",
            "Jianshe Wu",
            "Licheng Jiao",
            "Yuwei Guo",
            "Jian Gao"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Current research has found that some deep neural networks exhibit strong hierarchical self-similarity in feature representation or parameter distribution. However, aside from preliminary studies on how the power-law distribution of weights across different training stages affects model performance,there has been no quantitative analysis on how the self-similarity of hidden space geometry influences model weight optimization, nor is there a clear understanding of the dynamic behavior of internal neurons. Therefore, this paper proposes a complex network modeling method based on the output features of hidden-layer neurons to investigate the self-similarity of feature networks constructed at different hidden layers, and analyzes how adjusting the degree of self-similarity in feature networks can enhance the classification performance of deep neural networks. Validated on three types of networks MLP architectures, convolutional networks, and attention architectures this study reveals that the degree of self-similarity exhibited by feature networks varies across different model architectures. Furthermore, embedding constraints on the self-similarity of feature networks during the training process can improve the performance of self-similar deep neural networks (MLP architectures and attention architectures) by up to 6 percentage points.",
        "gemini2.5flash": "",
        "overall_idea": "好的，非常荣幸能以作者的身份，深入剖析这篇关于深度神经网络自相似性分析的文章，回顾从选题到解决问题整个思维决策过程。\n\n---\n\n### **自相似性分析：深度神经网络内在机制的探索之旅**\n\n作为本文的作者，我将带领大家回顾这项研究的诞生过程，从最初的灵感萌生，到研究问题的逐步聚焦，再到方法论的选择与决策，最后是如何将这些思考转化为具体的实验和发现。\n\n#### **0. 为什么选择这个研究内容？——从表象到本质的求索**\n\n我对深度学习的兴趣不仅仅停留在其惊人的性能上，更着迷于它为何如此强大。传统的研究多聚焦于模型架构设计、优化算法改进等“外在”因素，但一个核心问题始终萦绕在我心头：**深度神经网络内部究竟是如何组织信息、进行计算和学习的？** 尤其是在模型训练过程中，其内在结构和信息流是如何演变的？\n\n彼时，复杂网络理论在分析复杂系统方面展现出巨大潜力，而神经网络本身就是一种复杂的网络。我注意到了一些前沿研究开始尝试将复杂网络理论引入神经网络分析，例如从小世界特性、无标度特性等角度理解连接模式。然而，这些研究大多集中在**静态的网络连接**上，忽视了神经网络最核心的特性——**动态性**。模型训练是一个动态演化的过程，神经元的激活模式、特征表示是不断变化的。\n\n与此同时，我注意到有研究提及深度网络可能存在**自相似性**，这在自然界（如分形结构、生物网络）中是一种普遍存在的现象，往往与高效、鲁棒的信息处理相关联。这引发了我的思考：深度神经网络在学习过程中，是否也会自然地展现或维持某种自相似性？这种自相似性是固定的还是动态变化的？它与模型的性能之间是否存在因果关系？更重要的是，如果能有效量化并干预这种自相似性，能否为模型的优化和新架构设计提供新的视角？\n\n这就是我们研究的出发点：**跳出传统的“连接”视角，深入到“特征激活”层面，探索深度神经网络在训练过程中的动态自相似性，并尝试利用这种内在机制来指导模型优化。**\n\n#### **1. 如何一步步构思和界定研究问题的？——从模糊的直觉到清晰的定义**\n\n起初，我的想法比较宽泛：“用复杂网络理论分析神经网络的动态性”。为了将其转化为可执行的研究，我经历了以下几个关键的构思和界定过程：\n\n*   **问题起点：什么才是神经网络的“内在结构”？**\n    *   **最初的直觉：** 神经元之间的连接权重。\n    *   **自我否定与升级：** 连接权重是静态的拓扑结构，但神经元的激活才是信息流动的载体。特别是隐层的神经元，它们的输出代表了模型对输入数据的“理解”和“特征提取”。如果能把这些动态的特征作为“网络节点”，节点间的关系则反映了特征之间的“相似性”或“关联性”，这不就能捕捉到模型在处理信息时的“内在状态”了吗？\n    *   **问题界定1：构建“特征网络” (GM)。** 如何构建？我们决定将每个隐层神经元（或特征图中的一个通道/某个区域的平均激活）视为网络中的一个“节点”，而节点之间的“边”则由它们激活特征的相似性来定义。具体来说，我们选择**欧氏距离**来量化特征向量之间的相似性，并设定一个阈值来判断是否存在“连接”。\n\n*   **问题聚焦：探索“自相似性”——如何量化、如何动态分析？**\n    *   **挑战：** 传统的分形维数计算（如盒覆盖法）通常是迭代的、非微分的，无法直接整合到深度学习的端到端训练框架中。\n    *   **思考与创新1：开发可微分的自相似性指标 `SS_rate`。** 既然不能直接用传统的盒覆盖计数，能否找到一种可微分的近似方法？我们注意到，盒覆盖数 $N_\\theta$ 实际上反映了在给定“尺度” $\\theta$ 下，网络中节点能够被“覆盖”或“关联”的程度。这与节点间的**连接概率**密切相关。如果节点间距离小于 $\\theta$ 的概率越高，说明网络在 $\\theta$ 尺度下越“密集”，需要覆盖的“盒子”就越少。\n        *   **具体步骤：**\n            1.  计算所有节点对的特征距离。\n            2.  计算在特定距离 $\\theta$ 下，节点对之间连接的**概率 $p(C \\le \\theta)$**。\n            3.  **核心创新：** 找到一个数学表达式，将这个连接概率与近似的盒覆盖数 $N_\\theta$ 关联起来，并且这个表达式是可微分的。我们通过数学推导和近似，得到了 `Algorithm 2` 中的 `No` 计算公式，并巧妙地利用 **Sigmoid 函数**来近似非微分的指示函数 `I(C_ij <= theta)`，从而确保了整个计算过程是可微分的。\n            4.  最后，`SS_rate` 被定义为 $N_\\theta$ 随 $\\theta$ 变化偏离理想幂律关系的程度。这个指标越接近0，代表自相似性越强。\n\n*   **问题深化：自相似性与模型性能的关系，以及如何干预？**\n    *   **假设：** 模型的内在自相似性可能影响其学习效率和泛化能力。\n    *   **思考与创新2：将 `SS_rate` 作为正则化项引入损失函数。** 既然 `SS_rate` 可微分，我们就可以将其加入到模型的优化目标中。损失函数变为 `L_total = L_task + alpha * ||SS_rate(GM^k) - target_gamma||^2`。\n        *   **关键决策点：`target_gamma` 应该是什么？**\n            *   **初期想法：** 也许是0（表示完全自相似）？\n            *   **深度思考与权衡：** 强制模型达到完全自相似可能是不自然的，甚至会阻碍其学习最佳特征表示。不同架构可能存在不同的“最佳”自相似水平。因此，一个更合理的 `target_gamma` 应该是模型在**标准训练（无正则化）后自然达到的自相似水平**。这样，我们不是强行改变模型的本质，而是帮助它**更好地保持或回归到其内在“健康”的自相似状态**。这个决策极大地提升了正则化策略的合理性和有效性。\n        *   **效率考量：** 如果模型有多个隐层，每次迭代都计算所有层的 `SS_rate` 可能开销巨大。\n        *   **优化决策：** 随机选择一个隐层来计算 `SS_rate` 进行正则化。这是一个工程上的权衡，既保证了计算效率，又能通过统计平均实现对整体自相似性的约束。\n\n至此，我们的研究问题被清晰地界定为：构建基于隐层特征的动态特征网络GM，设计可微分的SS_rate指标量化其自相似性，分析训练过程中SS_rate的变化，并通过正则化约束SS_rate来提升模型性能，并探究其背后的机理。\n\n#### **2. 理论框架、研究方法和数据分析策略的选择：思考、权衡与取舍**\n\n*   **理论框架：**\n    *   **选择：** 复杂网络理论与分形几何理论。\n    *   **原因：** 这两者为分析复杂系统的内在结构和层次性提供了强大的数学工具。分形维数、盒覆盖法等概念能直接量化自相似性。\n    *   **取舍：** 放弃直接在连接权重图上应用传统复杂网络指标（如平均路径长度、聚类系数），因为它们无法捕捉动态特征表示的演变。转而聚焦于**基于特征的相似性网络**，这更符合我们对“信息处理”而非“固定拓扑”的关注。\n\n*   **研究方法：**\n    *   **GM 构建：**\n        *   **节点定义：** 隐层特征向量（针对不同数据类型，如BDHW型图片特征，进行均值降维，确保每个特征图或通道能对应一个节点）。\n        *   **边定义：** 欧氏距离阈值连接。简单、常用，且易于理解和计算。\n    *   **SS_rate 计算（核心创新）：**\n        *   **挑战：** 传统盒覆盖法不可微。\n        *   **解决方案：** 创新性地提出基于连接概率的近似计算方法，并引入 Sigmoid 函数实现可微分。这是关键一步，使得自相似性能够作为损失函数的一部分进行优化。\n        *   **权衡：** 这种近似计算可能牺牲了某些精确性，但换来了在深度学习框架中的可用性和高效性，这对于实现我们的研究目标是至关重要的。\n    *   **正则化策略：**\n        *   **L2 范数损失：** `||SS_rate - target_gamma||^2`。简单有效，易于实现。\n        *   **`target_gamma` 设定：** 不设为固定值，而是模型在无正则化训练后的自然值。这个决策是基于对神经网络学习过程的深刻理解：模型会演化到它认为最佳的状态。我们希望的是“稳定”或“引导”这种演化，而不是“强迫”它走向一个可能不适合它的状态。这避免了过度约束，并能揭示模型本身的内在属性。\n        *   **随机层选择：** 解决了计算效率问题，是面向实际应用的工程优化。\n\n*   **数据分析策略：**\n    *   **可视化：** 使用 **多维尺度变换（MDS）**。\n        *   **选择原因：** MDS 能够将高维数据（GM的特征空间）降维到2D或3D，同时尽可能保留原始数据点之间的距离/相似性关系。这使得我们可以直观地看到 GM 的结构，比如是否形成流形、聚类等，从而定性判断自相似性的表现。\n    *   **定量分析：**\n        *   **`SS_rate` 值：** 直接量化自相似性强度。\n        *   **统计尺度不变性：** 通过协方差矩阵的谱分析（特征值幂律分布）来衡量。\n        *   **几何不变性：** 通过 UMAP 降维后计算相关维度来衡量。\n        *   **选择原因：** 引入这些额外的定量指标，是为了更全面地理解自相似性，并提供更深层次的解释。如果 `SS_rate` 正则化有效，那么这些深层的不变性指标是否也表现更好？这有助于我们建立性能提升与内在机制之间的联系。\n\n#### **3. 研究关键阶段的重要决策及逻辑：以 ViT 模型为例**\n\n我们以 **Vision Transformer (ViT)** 模型为例，它是一种基于注意力机制的架构，非常适合解释我们的决策逻辑。\n\n1.  **初始发现（定性）：ViT 的 GM 表现出强烈的低维流形结构。**\n    *   **决策：** 在 `GM Visualisation` 阶段，我们使用 MDS 对未训练和训练后的 ViT 浅层特征网络进行可视化（参见图2）。\n    *   **逻辑：** 发现 ViT 在训练前后，其特征网络 GM 均呈现出明显的聚类和低维流形结构。这与 MLP-Mixer 等全局信息聚合模型相似，而与 ResNet 等局部卷积模型形成鲜明对比。这初步暗示 ViT 可能**天然具备较高的自相似性**，因为全局注意力机制倾向于在不同尺度上捕捉到相似的模式。\n\n2.  **定量验证：ViT 的自相似性在训练过程中确实会下降。**\n    *   **决策：** 在 `Quantitative Analysis of Self-Similarity` 阶段，我们计算了 ViT 在未训练和训练后的平均 `SS_rate`（参见表2）。\n    *   **逻辑：** 观察到 ViT (small) 的 `SS_rate` 从未训练的 0.13 显著增加到训练后的 0.66。`SS_rate` 越高表示自相似性越弱。这意味着在常规训练下，ViT 的自相似性会“退化”。我们推测，这是因为模型在优化过程中打破了初始的对称性，减少了特征表示中的冗余。然而，这种“退化”是否是最佳选择？如果 ViT 的设计理念（全局注意力）本身就受益于某种程度的自相似性，那么这种退化可能并不完全是好事。\n\n3.  **干预与效果验证：`SS_rate` 正则化对 ViT 性能的显著提升。**\n    *   **决策：** 实施 `SS_rate` 正则化训练，将 `target_gamma` 设为 ViT 在标准训练后达到的 `SS_rate` 值。比较带正则化和不带正则化的模型性能（参见表3）。\n    *   **逻辑：**\n        *   我们发现，对 ViT (small) 施加 `SS_rate` 正则化后，其在 CIFAR100 上的准确率从 65.75% 提升到 67.69%，在 Imagenette 上从 81.71% 提升到 83.74%。这证实了我们的假设：**对于某些模型（如ViT），维持或引导其自相似性可以显著提升性能。**\n        *   **重要决策的体现：`target_gamma` 的设定至关重要。** 如果我们将 `target_gamma` 设定为一个任意的、可能与模型内在特性不符的值（例如强行要求极高的自相似性），很可能会适得其反，阻碍模型的优化。但通过将 `target_gamma` 设为模型在充分训练后自然达到的 `SS_rate`，我们实际上是帮助模型**稳定并强化其内在的、对其性能有益的自相似结构**，而非强加一个不自然的约束。\n\n4.  **深层机理解释：性能提升与内在不变性特性关联。**\n    *   **决策：** 计算 ViT 的统计尺度不变性（表4）和几何不变性（表5）。\n    *   **逻辑：** 我们发现，ViT (以及 ResMLP, MLP-Mixer 等 MLP/Attention 架构) 不仅在 `SS_rate` 正则化后性能提升，而且它们本身也展现出更好的统计尺度不变性和几何不变性。这些不变性是自相似性的深层体现。\n        *   **这意味着：** `SS_rate` 正则化之所以能提升 ViT 的性能，是因为它强化了 ViT **原本就具备的、有利于其全局信息聚合机制的内在自相似和不变性**。对于 Vgg 或 ResNet 这样依赖局部感受野的 CNN 模型，它们的特征分布更均匀（自相似性较低），强制施加这种全局自相似性约束反而效果不明显，甚至可能略微下降，因为这与它们的内在信息处理模式不符。\n\n通过这个 ViT 的例子，我们可以看到整个研究过程是如何环环相扣、逐步深入的：从定性观察到定量分析，从现象揭示到机制探索，再到干预实践和效果验证，每一步的决策都基于前一步的发现，并服务于最终对深度神经网络内在机制的理解。"
    },
    {
        "order": 4,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17786",
        "abs_url": "https://arxiv.org/abs/2507.17786",
        "pdf_url": "https://arxiv.org/pdf/2507.17786",
        "title": "Reinforcement Learning for Accelerated Aerodynamic Shape Optimisation",
        "authors": [
            "Florian Sobieczky",
            "Alfredo Lopez",
            "Erika Dudkin",
            "Christopher Lackner",
            "Matthias Hochsteger",
            "Bernhard Scheichl",
            "Helmut Sobieczky"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We introduce a reinforcement learning (RL) based adaptive optimization algorithm for aerodynamic shape optimization focused on dimensionality reduction. The form in which RL is applied here is that of a surrogate-based, actor-critic policy evaluation MCMC approach allowing for temporal 'freezing' of some of the parameters to be optimized. The goals are to minimize computational effort, and to use the observed optimization results for interpretation of the discovered extrema in terms of their role in achieving the desired flow-field. By a sequence of local optimized parameter changes around intermediate CFD simulations acting as ground truth, it is possible to speed up the global optimization if (a) the local neighbourhoods of the parameters in which the changed parameters must reside are sufficiently large to compete with the grid-sized steps and its large number of simulations, and (b) the estimates of the rewards and costs on these neighbourhoods necessary for a good step-wise parameter adaption are sufficiently accurate. We give an example of a simple fluid-dynamical problem on which the method allows interpretation in the sense of a feature importance scoring.",
        "gemini2.5flash": "",
        "overall_idea": "作为本文的作者，我很荣幸能深入剖析我们的研究工作，分享从选题到最终方法构建的整个思维决策过程。这篇论文的核心在于将强化学习（RL）引入到计算流体力学（CFD）背景下的气动外形优化（ASO）中，旨在实现加速并提供可解释的洞察。\n\n### 0. 为什么选择这个研究内容？\n\n在工程实际中，尤其是在航空航天和流体机械设计领域，气动外形优化是一个永恒且极具挑战性的任务。传统的优化方法，无论是基于梯度的还是黑盒的，在面对高维设计参数空间和每一次仿真都极其昂贵的CFD模拟时，往往会陷入计算效率的瓶颈。一个典型的CFD仿真可能需要数小时甚至数天，这意味着即使是几十次迭代也可能耗费数周时间。\n\n我们团队长期关注CFD优化问题。我们观察到，ASO过程中存在一个“学习”如何优化的潜力：在优化的不同阶段，并非所有设计参数都同等重要。某些参数可能在局部收敛时变得“稳定”，即它们的微小变化不再显著影响性能。能否动态地识别并“冻结”这些参数，从而降低优化问题的有效维度，进而加速收敛？\n\n强化学习作为一种决策制定框架，天然擅长处理序列决策、动态环境和适应性策略。这促使我们思考：RL能否不仅仅是找到一个最优解，而是学习一个“优化策略”，即在给定当前状态（形状参数、流动特性）下，如何选择下一个行动（参数调整）以最有效地接近目标？特别是，RL处理“不确定性”和“稀疏奖励”的能力，与CFD优化中仿真结果不确定（数值误差、模型误差）和优化路径漫长而奖励不频繁的特点非常契合。\n\n将RL与降维技术结合，并且要求降维过程是可解释的，这成为了我们的核心研究方向。我们希望RL不仅能加速，还能告诉我们“为什么”会加速，即哪些几何特征在优化过程中变得不重要。\n\n### 1. 如何一步步构思和界定研究问题的？\n\n我们的构思过程是迭代的，从一个宽泛的问题开始，逐步细化。\n\n**1.1 最初的宽泛问题：** 如何显著加速计算昂贵的ASO，同时提高其在复杂真实世界问题中的鲁棒性？\n\n**1.2 引入RL的初步想法：**\n*   **ASO本质是序列决策：** 每一次形状改变都是一次决策，目标是最终的性能提升。这与RL的MDP（马尔可夫决策过程）框架非常吻合。\n*   **挑战：** 传统RL需要大量的环境交互，而CFD仿真极其昂贵。这直接指出了我们需要解决的核心矛盾：RL的样本效率问题。\n*   **可能的解决方案：**\n    *   使用代理模型（Surrogate Model）来减少CFD调用次数。\n    *   设计一个分层或多尺度的RL框架，在不同粒度上进行决策。\n\n**1.3 聚焦“维度诅咒”与“可解释性”：**\n*   **维度诅咒（Curse of Dimensionality）：** 随着设计参数的增加，搜索空间呈指数级增长。这是ASO的核心难题之一。\n*   **降维的必要性：** 如果能动态减少优化中“活跃”的参数数量，就能显著缩小搜索空间。\n*   **“可解释性”的重要性：** 工程师不仅需要一个最优形状，还需要理解为什么这个形状是优的，以及哪些设计特征是关键的。传统的降维方法（如PCA）通常缺乏这种直观解释。\n*   **几何参数化方法的选择：** 我们需要一种既能精确描述复杂形状，又能提供直观物理意义的参数化方法。PARSEC参数化方法进入视野，其系数直接对应翼型的几何特征（如前缘半径、后缘厚度、最大厚度位置等），这为后续的“可解释性降维”奠定了基础。\n\n**1.4 提出“动态参数冻结”的核心机制：**\n*   结合PARSEC的可解释性，我们设想：如果某个PARSEC参数在当前优化阶段对目标函数的影响很小（即其梯度很小），RL代理能否“学习”将其冻结，从而减少后续优化的维度？\n*   **如何形式化“冻结”？** 这不是简单地删除参数，而是将其从“可变参数集”中移除，并允许RL代理在后续阶段根据需要将其“解冻”。这构成了RL状态空间的一部分：每个参数除了其数值外，还有一个“活跃/冻结”的标志。\n*   **“两尺度”框架的诞生：**\n    *   **微观尺度（Microscopic Scale）：** 在一个小范围的参数邻域内进行密集搜索和局部优化。在这个尺度上，我们负担不起大量的全CFD仿真。因此，需要一个快速但近似的代理模型。\n    *   **宏观尺度（Mesoscopic Scale）：** 在更大的参数空间中进行“跳跃”，并周期性地评估和调整RL的策略（包括决定哪些参数可以冻结/解冻）。这个尺度上的决策会触发昂贵的CFD仿真来获取“地面真值”。\n    *   **连接：** 微观尺度负责快速局部探索，宏观尺度负责全局策略调整和维度管理。\n\n**1.5 定义具体的问题示例（用例）：**\n为了在不牺牲通用性验证核心思想的前提下，降低开发的复杂度和计算成本，我们选择了一个简化的2D流体动力学问题：\n*   **问题描述：** 在一个具有渗透壁面和周期性边界条件的直通道内，一个翼型将倾斜的非均匀入流转换为均匀的单向出流。\n*   **目标函数（Reward/Cost）：**\n    1.  最小化翼型后方指定截面上Z方向流速分量的L2范数（实现流向单一化）。\n    2.  最大化该截面上流速大小的均匀分布（实现流场均匀化）。\n*   **几何参数化：** 最初使用全PARSEC，但很快调整为“Reduced PARSEC”，即通过多项式根因式分解来表示翼型，这样“冻结”参数就对应于固定多项式的某个根，这比冻结原始PARSEC系数更具物理意义和灵活性。具体到示例中，我们主要关注翼型的**弯度（f）**和**厚度（b）**这两个关键参数。\n\n至此，问题被清晰地界定为：构建一个基于RL的两尺度自适应优化框架，利用可解释的几何参数化（Reduced PARSEC）进行动态维度约简，并通过代理模型和MCMC克服CFD仿真的高昂成本，最终在实现优化的同时提供对关键几何特征的洞察。\n\n### 2. 为了解决该问题，您在选择理论框架、研究方法和数据分析策略时，经历了怎样的思考、权衡和取舍？\n\n**2.1 理论框架：强化学习（RL）**\n*   **思考：** 我们首先确定了RL作为主导框架。RL的actor-critic架构非常适合我们的分层决策：actor负责生成动作（参数调整），critic负责评估这些动作的价值。\n*   **权衡：**\n    *   **Q-learning/SARSA (值迭代):** 需要一个离散的、可枚举的状态-动作空间，但在连续的几何参数优化中很难实现。\n    *   **策略梯度 (Policy Gradient):** 更适合连续动作空间，但样本效率通常较低，收敛不稳定。\n    *   **Actor-Critic:** 结合了值函数学习和策略学习，可以处理连续空间，并通过critic指导actor，提高样本效率和稳定性。我们最终选择了这个方向。\n    *   **Monte Carlo (MC) vs. Temporal Difference (TD):** MC方法通过完成完整的episode来学习，虽然方差大，但对模型误差不敏感；TD方法则利用bootstrapping，可以从不完整的episode中学习，方差小但可能引入偏差。考虑到CFD仿真的高成本，我们无法进行大量的完整episode仿真。然而，我们采取的“代理模型+MCMC”策略，实际上是在微观尺度上模拟了大量的“虚拟”episode，这使得MC方法（或其变体）成为可能。我们最终选择了基于Monte Carlo的价值函数估计，因为它能直接从“地面真值”中学习，减少了模型偏差。\n\n**2.2 研究方法：核心组件的选择与集成**\n\n*   **几何参数化：Reduced PARSEC**\n    *   **思考：** 传统的PARSEC直接操作系数，不易直观地“冻结”某个物理特征。我们希望冻结的参数能对应到翼型的某些“稳定”或“不重要”的物理特性。\n    *   **取舍：** 经过探索，我们决定将PARSEC的多项式部分进行因式分解，通过固定多项式的根来控制翼型形状。这样，固定一个根就意味着固定了一个物理相关的特征点。这种“Reduced PARSEC”方法，能够允许RL代理直接操作参数的“活跃”状态，而不是仅仅调整其数值。\n    *   **优势：** 这不仅实现了降维，更重要的是提供了“可解释性”，因为我们可以追踪被冻结的根对应的物理意义，从而理解哪些几何特征在特定优化阶段不重要。\n\n*   **代理模型（Surrogate Model）：简单的解析插值**\n    *   **思考：** 微观尺度下，RL需要频繁查询奖励值。昂贵的CFD仿真无法满足这个需求。因此，一个快速的代理模型是必不可少的。\n    *   **取舍：** 复杂的深度学习代理模型（如神经网络）需要大量数据训练，且训练本身也耗时，不适合每次微观更新都重训。我们决定从最简单的开始：在当前状态的局部邻域内，利用少数几个CFD“地面真值”点（例如，矩形区域的角点）进行**解析插值**。\n    *   **优势：** 这种局部、简单的代理模型计算极快，足以驱动微观尺度的MCMC探索。\n    *   **挑战与应对：** 精度有限。我们通过宏观尺度的周期性CFD真值更新和价值函数重估来校正这种近似误差。\n\n*   **价值函数估计：MCMC + 固定点迭代**\n    *   **思考：** RL的价值函数（V(s)）是衡量从某个状态开始的预期累积奖励。在连续且复杂的奖励景观中估计它很困难。\n    *   **取舍：**\n        *   **MCMC (Metropolis Algorithm):** 允许我们在状态空间中进行随机游走，根据奖励（能量）接受或拒绝状态转换。这是一种高效的采样方法，特别适用于我们奖励函数复杂且不完全已知的情况。它允许我们通过少量CFD真值点和代理模型来近似地探索价值函数。\n        *   **固定点迭代：** 价值函数满足贝尔曼方程，这是一个固定点问题。通过迭代计算，可以逐步逼近其真实值。这种方法在RL理论中很常见，我们将其与MCMC采样结合，用于估计微观尺度的价值函数。\n    *   **优势：** 克服了精确计算期望值的困难，允许在计算预算有限的情况下对价值函数进行有效估计。\n\n*   **维度约简策略：ɛ-稳定性准则**\n    *   **思考：** 如何量化“不重要”的参数？直观上，就是那些对目标函数梯度很小的参数。\n    *   **设计：** 我们提出了一个基于梯度的**ɛ-稳定性准则**。如果某个参数对目标函数的梯度（或者更精确地说，是该参数在当前邻域内导致的目标函数变化相对于其他参数变化的最大值）低于一个预设的阈值ɛ，我们就认为它是“ɛ-稳定的”，可以暂时冻结。\n    *   **动态调整：** 当参数被冻结后，活跃参数的数量减少。为了弥补搜索空间的减少，我们**自适应地增加剩余活跃维度上的邻域大小**。例如，如果原来是7x7的正方形邻域，冻结一个维度后，它可以变成7x11或11x7的矩形，确保总的搜索体积保持大致不变。\n    *   **优势：** 这种自适应的、基于梯度的策略，使维度约简过程智能化和动态化，避免了静态或人工的参数选择。\n\n**2.3 数据分析策略：**\n*   **定量指标：**\n    *   **宏观路径长度（Length of Mesoscopic Path）：** 这直接衡量了需要进行的昂贵CFD仿真次数，是衡量加速效果的核心指标。\n    *   **价值函数估计迭代次数：** 衡量代理模型收敛到足够精度所需的计算量。\n*   **定性/可解释性指标：**\n    *   **被冻结的参数：** 跟踪哪些PARSEC根在不同优化阶段被冻结，以及它们对应的几何特征。这提供了对“特征重要性”的洞察。\n    *   **邻域形状的变化：** 观察邻域从正方形变为矩形（或更高维度的超矩形）的动态过程，验证自适应策略的有效性。\n\n### 3. 在研究的每个关键阶段，您做出了哪些重要决策，并能解释这些决策背后的逻辑和依据。\n\n**3.1 阶段一：问题简化与几何基础（初期）**\n\n*   **重要决策1：选择2D Stokes流作为验证模型。**\n    *   **逻辑：** 真实的空气源热泵流场是复杂的三维湍流Navier-Stokes问题，单次CFD耗时巨大。为了快速验证RL和降维的核心概念，需要一个能快速迭代且计算稳定的模型。Stokes流（低雷诺数，不可压缩）虽然简化，但仍保留了流体导向和均匀化的核心几何优化问题。\n    *   **依据：** 资源限制和研究效率。与其一开始就陷在复杂的CFD仿真中，不如先搭建并验证RL框架。\n    *   **影响：** 使得算法开发和测试能够迅速进行，加速了概念验证。\n\n*   **重要决策2：采用Reduced PARSEC参数化方法。**\n    *   **逻辑：** 原始PARSEC虽然可解释，但其系数之间的耦合性使得“冻结”某个系数可能无法直观地对应一个物理特征。将多项式分解为根的形式，使得冻结一个根直接对应于固定翼型上的一个几何点，这与物理形状特征的关联更紧密，也更容易实现RL的“参数冻结”操作。\n    *   **依据：** 满足“可解释性降维”的需求，简化RL代理的决策逻辑。\n    *   **影响：** 为后续的ɛ-稳定性准则提供了可操作的物理基础，使得维度约简不仅仅是数学上的，更是物理意义上的。\n\n**3.2 阶段二：RL框架构建与样本效率提升（中期）**\n\n*   **重要决策3：构建“两尺度”RL框架（微观MCMC+宏观策略评估）。**\n    *   **逻辑：** 直接用昂贵的CFD驱动RL的每一步学习是不可行的。将学习过程分为两个尺度：微观尺度在代理模型上快速迭代，探索局部最优；宏观尺度则在定期调用真实CFD的基础上，评估策略并进行关键的参数冻结/解冻决策。\n    *   **依据：** 解决CFD高计算成本与RL样本效率需求之间的矛盾。同时，这种分层结构也模仿了人类从细致调整到宏观规划的决策过程。\n    *   **影响：** 大幅减少了对昂贵CFD仿真的调用次数，是实现加速的关键。\n\n*   **重要决策4：在微观尺度使用简单的局部解析插值代理模型。**\n    *   **逻辑：** 微观MCMC需要每一步都有奖励反馈。训练复杂的深度学习代理模型耗时，且在每次局部搜索时重新训练不现实。简单的局部插值模型（如基于矩形区域角点的二次曲面插值）虽然精度有限，但计算速度极快，足以驱动MCMC的随机游走。\n    *   **依据：** 性能与精度之间的权衡。在微观尺度，速度是第一位的；精度不足由宏观尺度的真值CFD调用周期性校正。\n    *   **影响：** 实现了微观尺度的快速探索，使MCMC成为可能。\n\n**3.3 阶段三：维度约简与优化策略（后期）**\n\n*   **重要决策5：引入ɛ-稳定性准则进行动态参数冻结。**\n    *   **逻辑：** 自动化地识别并“冻结”那些在当前优化阶段对目标函数影响不大的参数。我们通过比较单个参数的梯度贡献与所有参数的最大梯度贡献来量化其“稳定性”。如果一个参数的贡献低于某个阈值ɛ，则认为它是稳定的。\n    *   **依据：** 经验观察到在优化过程中，某些参数会比其他参数更快地收敛或变得不敏感。该准则提供了自动化的智能降维方法。\n    *   **影响：** 显著减少了优化问题的有效维度，使得搜索空间变小，从而加速了收敛（如实验结果所示）。\n\n*   **重要决策6：实施参数冻结后的邻域自适应扩展策略。**\n    *   **逻辑：** 当一个或多个维度被冻结后，如果保持原有的邻域大小（例如，正方形），那么RL代理探索的“实际”体积就变小了，可能导致过早收敛或陷入局部最优。因此，需要增加剩余活跃维度上的邻域大小，以维持相似的搜索能力。\n    *   **依据：** 保持搜索能力的完整性，避免因维度约简而导致的探索不足。\n    *   **影响：** 确保了即使在降维后，算法仍能有效地探索参数空间，避免过早陷入局部极小值。\n\n---\n\n**具体例子：2D翼型优化中弯度`f`和厚度`b`参数的动态冻结**\n\n假设我们正在优化示例中的2D翼型，目标是最小化翼型后方的Z方向流速（R1）和均匀化流速大小（R2）。我们主要调整**弯度参数`f`**（控制翼型垂直位置，尤其是后缘）和**厚度参数`b`**（控制翼型厚度和整体曲度）。\n\n1.  **初始阶段：** 翼型形状可能远离最优，流场高度不均匀。此时，`f`和`b`这两个参数都对奖励函数R(f,b)有很大的影响，即梯度都很大。RL代理会认为两者都是“活跃”的，其策略允许同时调整`f`和`b`。宏观邻域N(θ)可能是一个正方形区域，比如(7x7)网格。\n\n2.  **微观探索：** 在某个宏观迭代中，RL代理在当前形状θ_n的(7x7)邻域内进行MCMC采样，利用廉价的局部插值代理模型R_e(s)来估计奖励。假设代理发现，在这个局部区域内，对`f`进行微小调整会带来R_e(s)的剧烈变化，而对`b`的调整，虽然也有变化，但幅度相对较小。\n\n3.  **宏观策略评估：** 经过一系列微观步后，RL代理使用少数几次昂贵的CFD仿真更新“地面真值”R(θ_n)及其邻近点的真值。然后，它应用**ɛ-稳定性准则**：\n    *   它计算`f`方向的梯度 |∇_f R(θ_n)| 和 `b`方向的梯度 |∇_b R(θ_n)|。\n    *   如果发现 `|∇_b R(θ_n)| < ɛ * max(|∇_f R(θ_n)|, |∇_b R(θ_n)|)` (一个简化的表示，实际考虑邻域内所有梯度)，也就是说，`b`对目标函数的影响显著小于`f`。\n    *   **决策：** RL代理决定将参数`b`标记为“ɛ-稳定”，并从“活跃参数集”中移除，即暂时“冻结”`b`。\n\n4.  **邻域自适应调整：** 由于`b`被冻结，优化问题从2D变成了1D（只优化`f`）。为了保持类似的探索能力，RL代理会调整宏观邻域N(θ)。例如，它可能会将原来的(7x7)正方形邻域，调整为沿着`f`轴更长的矩形邻域，比如(11x1)或(7x1)×(11x1)，使得在`f`方向上的搜索步长或范围更大。\n\n5.  **后续阶段：** 在接下来的宏观迭代中，RL代理主要集中精力优化参数`f`，因为它被认为是当前阶段最重要的影响因素。对`b`的探索和CFD评估次数将大幅减少。如果后续阶段流场特性变化，`b`再次变得重要（即不满足ɛ-稳定性准则），RL代理可以将其“解冻”，重新纳入活跃参数集。\n\n通过这个过程，我们能够显著减少总的CFD仿真次数，因为我们不再对所有参数进行同等粒度的探索。同时，通过观察哪些参数（如`b`）被冻结，我们能够得出“在达到一定流场均匀度后，翼型厚度的微小调整对进一步改善性能的影响变小”这样的物理洞察，这正是我们追求的可解释性。实验结果（如论文中的Table 11和Figure 8）清晰地展示了这种自适应维度约简策略在缩短宏观路径长度和加速价值函数收敛方面的优势。\n\n整个研究过程充满了权衡和取舍，但正是这些决策，使得我们能够在一个计算成本极高的领域，探索出一条基于强化学习的、高效且可解释的优化新路径。"
    },
    {
        "order": 5,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17787",
        "abs_url": "https://arxiv.org/abs/2507.17787",
        "pdf_url": "https://arxiv.org/pdf/2507.17787",
        "title": "Hyperbolic Deep Learning for Foundation Models: A Survey",
        "authors": [
            "Neil He",
            "Hiren Madhu",
            "Ngoc Bui",
            "Menglin Yang",
            "Rex Ying"
        ],
        "comments": "11 Pages, SIGKDD 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Foundation models pre-trained on massive datasets, including large language models (LLMs), vision-language models (VLMs), and large multimodal models, have demonstrated remarkable success in diverse downstream tasks. However, recent studies have shown fundamental limitations of these models: (1) limited representational capacity, (2) lower adaptability, and (3) diminishing scalability. These shortcomings raise a critical question: is Euclidean geometry truly the optimal inductive bias for all foundation models, or could incorporating alternative geometric spaces enable models to better align with the intrinsic structure of real-world data and improve reasoning processes? Hyperbolic spaces, a class of non-Euclidean manifolds characterized by exponential volume growth with respect to distance, offer a mathematically grounded solution. These spaces enable low-distortion embeddings of hierarchical structures (e.g., trees, taxonomies) and power-law distributions with substantially fewer dimensions compared to Euclidean counterparts. Recent advances have leveraged these properties to enhance foundation models, including improving LLMs' complex reasoning ability, VLMs' zero-shot generalization, and cross-modal semantic alignment, while maintaining parameter efficiency. This paper provides a comprehensive review of hyperbolic neural networks and their recent development for foundation models. We further outline key challenges and research directions to advance the field.",
        "gemini2.5flash": "",
        "overall_idea": "作为这篇名为《双曲深度学习在基础模型中的应用：一项综述》论文的作者之一，我很乐意深入剖析我们的思考过程，从选题到方法论，再到具体研究中的决策。\n\n### 0. 为什么选择这个研究内容？\n\n在2023-2024年间，基础模型（Foundation Models，如LLMs、VLMs）取得了惊人的进展，成为人工智能领域的核心。然而，作为长期关注黎曼几何和表示学习的研究者，我们敏锐地察觉到现有基础模型的一个深层限制：**它们几乎无一例外地基于欧几里得几何**。\n\n我们的核心直觉是：现实世界的数据，特别是语言、生物信息学和社交网络中的数据，往往呈现出**固有的层次结构（hierarchical structures）和幂律分布（power-law distributions）**。例如，语言中的词语、概念、语法关系形成树状结构；生物学中的蛋白质相互作用网络、基因调控网络也具有明显的层次特性。欧几里得空间在表示这类数据时，面临着**巨大的失真（distortion）**问题，导致需要极高的维度才能勉强捕捉这些关系，从而带来了**计算效率低下、模型泛化能力受限和可解释性差**等问题。\n\n而双曲几何，以其独特的**指数级体积增长**特性，被数学和理论计算机科学领域证明是**低失真地嵌入树状和层次结构**的理想空间。它能在远低于欧几里得空间的维度下，高效地表示这些复杂关系。\n\n因此，一个核心问题浮出水面：**既然双曲几何在表示层次结构方面具有内在优势，那么为什么不将这种归纳偏置（inductive bias）引入到基础模型中，以克服其在处理具有内在层次结构的数据时的根本性限制呢？** 当时，虽然已有关于双曲神经网络的综述，但专门针对“基础模型”这一新兴且影响力巨大的领域，却鲜有系统的回顾。我们看到了一个重要的研究空白和巨大的潜力。\n\n### 1. 如何一步步构思和界定研究问题的？\n\n我们的构思过程是迭代的，可以概括为以下几个步骤：\n\n1.  **观察与痛点识别（Foundation Models的局限性）**：\n    *   **观察**：LLMs、ViTs等在欧几里得空间取得了巨大成功。\n    *   **痛点**：这些模型在处理具有层次/幂律特性的数据时，存在性能瓶颈。例如，LLMs的复杂推理能力，VLM的零样本泛化，以及跨模态语义对齐。\n    *   **初步假设**：欧几里得空间作为默认的几何归纳偏置，可能不是所有基础模型的“最优选择”。\n\n2.  **引入解决方案（双曲几何的契合性）**：\n    *   **理论支撑**：双曲几何在嵌入树和层次结构方面具有数学上的低失真保证。\n    *   **潜在优势**：如果能将双曲几何引入基础模型，有望提升其表示能力、泛化能力和参数效率（因维度降低）。\n    *   **核心问题凝练**：这促使我们提出了论文摘要中的“核心问题”——“欧几里得几何真的是所有基础模型的最佳归纳偏置吗，或者引入其他几何空间能否使模型更好地与真实世界数据的内在结构对齐并改进推理过程？”\n\n3.  **确定研究范围与目标（综述的定位）**：\n    *   **范围**：并非所有双曲深度学习，而是聚焦于“**双曲深度学习在基础模型中的应用**”。具体涵盖LLMs、VLMs和多模态模型。\n    *   **目标**：\n        *   系统回顾基础的双曲神经网络操作（作为构建块）。\n        *   全面梳理双曲基础模型的最新进展，按模态和几何模式（混合/全双曲/切线空间）进行分类。\n        *   识别现有挑战并指出未来的研究方向。\n    *   **与现有综述的区分**：强调我们专注于“基础模型”这一特定领域，而非泛泛地回顾双曲深度学习或仅限于某个特定模态（如仅视觉或仅图）。\n\n### 2. 为了解决该问题，您在选择理论框架、研究方法和数据分析策略时，经历了怎样的思考、权衡和取舍？\n\n**理论框架选择：**\n*   **核心选择**：黎曼几何，特别是具有常负曲率的双曲空间。\n*   **具体模型**：庞加莱球模型（Poincaré ball model）和洛伦兹双曲面模型（Lorentz hyperboloid model）是双曲深度学习中最常用的两种模型。\n    *   **思考与权衡**：庞加莱球模型在视觉上更直观（一个球体内部），其距离计算也相对简单，但其操作通常依赖于切线空间近似。洛伦兹模型则允许更直接、封闭形式的流形操作，特别是在线性变换和激活函数方面，但其几何直观性稍弱。\n    *   **决策**：我们决定同时涵盖这两种模型，因为它们各有优劣，并在文献中被广泛使用。重要的是强调它们之间是等距的（isometric），即可以通过坐标变换相互转化，这使得研究人员可以根据计算便利性和直观性选择最适合的模型。\n\n**研究方法（综述的构建过程）：**\n*   **文献收集**：我们采用了系统性的文献检索策略，结合关键词搜索（如 \"Hyperbolic\", \"LLM\", \"Vision Transformer\", \"CLIP\", \"Foundation Model\", \"Neural Network\" 等）在主流会议（NeurIPS, ICML, ICLR, KDD, ACL, CVPR）和期刊（TPAMI, JMLR）数据库中进行。同时，我们运用了“滚雪球”策略，从核心论文的参考文献中发掘更多相关工作。\n*   **内容组织与分类**：这是综述的关键。\n    *   **思考**：最初可能想到按年份或作者来组织，但这不利于读者理解技术演进。更好的方式是按概念和技术栈。\n    *   **决策**：\n        1.  **从基本操作开始**：先介绍双曲空间的基础数学概念，再详细讲解将欧几里得操作（如线性层、激活函数、注意力机制、残差连接、归一化、位置编码等）“双曲化”的基本方法（切线空间近似、全双曲操作）。这是理解后续模型的基石。\n        2.  **分层递进**：从基本双曲神经网络模型（RNNs, MLPs, CNNs, GNNs）到更复杂的“基础模型”（Transformers/LLMs, Vision FMs, Multi-modal FMs）。这种结构能够展示技术的演进和复杂度的增加。\n        3.  **统一视角**：在每个模型类型中，统一分析其“几何模式”（Tangent Space, Hybrid, Fully Hyperbolic）和“流形选择”（Poincaré, Lorentz），形成表格（如Table 1），这使得对比和理解更加清晰。\n*   **具体案例深入分析**：选择代表性的模型进行详细解析，例如在双曲Transformer/LLMs部分，我们逐步介绍了HAN、HNN++、FNN、Hypformer和HELM，展示了从初步尝试到“全栈”双曲化再到大规模应用和效率优化的过程。\n\n**数据分析策略（指引被综述研究的分析方向）：**\n*   **性能评估**：双曲模型在特定任务（如层次分类、链路预测、复杂推理）上的准确率、F1分数等，并与欧几里得模型进行对比。\n*   **几何特性分析**：可视化嵌入空间（如Figure 4所示的庞加莱球内部），展示类簇的边界行为和层次结构捕获能力。计算嵌入的失真度、维度利用率。\n*   **效率评估**：参数数量、内存占用（特别是KV-cache大小）、训练/推理速度等，这是基础模型“可扩展性”的关键指标。\n*   **定性分析**：模型在捕捉语义层次（如Figure 3所示的词语概念树）方面的表现。\n\n### 3. 在研究的每个关键阶段，您做出了哪些重要决策，并能解释这些决策背后的逻辑和依据。\n\n我们以**双曲Transformer和LLMs**为例，来阐述关键决策点：\n\n**阶段1：早期探索与“切线空间”方法的引入**\n*   **背景**：Transformer在NLP领域取得了巨大成功，但其核心结构（自注意力机制、前馈网络）都基于欧几里得线性操作。语言数据又具有明显层次结构。\n*   **关键问题**：如何将Transformer的强大能力与双曲几何的层次表示优势结合？\n*   **决策1（HAN [44] 和 HNN++ [101]）**：早期工作决定采用“切线空间”方法。即，将双曲空间中的点映射到其切线空间（这是一个欧几里得空间），在那里执行标准的Transformer操作（如线性变换、注意力计算），然后再将结果映射回双曲流形。\n    *   **逻辑与依据**：这是最直接、最易实现的整合方式。它允许我们重用大部分现有欧几里得深度学习库和优化器。虽然这种方法在理论上可能会丢失一些全局的双曲特性，但在初期能快速验证双曲归纳偏置的有效性。我们称之为“混合”模式，因为它结合了欧几里得操作。\n*   **成果**：在神经机器翻译、图链路预测等任务上取得初步成功，证明了双曲几何的潜力。\n\n**阶段2：向“全双曲”操作的迈进**\n*   **背景**：切线空间近似的局限性逐渐显现：可能导致数值不稳定，且无法充分利用双曲空间的全局几何特性。\n*   **关键问题**：能否设计出完全在双曲流形上进行操作的“全双曲”Transformer组件？\n*   **决策2（FNN [18] 和 H-BERT [17]）**：转向洛伦兹双曲面模型，并开发了“全双曲”的线性层、激活函数、注意力机制等。这些操作直接在流形上进行，不依赖切线空间近似。\n    *   **逻辑与依据**：洛伦兹模型在数学上允许更多直接的、封闭形式的流形操作，避免了切线空间映射带来的信息损失和潜在的数值问题。这种“全双曲”方法理论上能更好地捕捉数据内在的层次结构。\n*   **成果**：FNN在神经机器翻译上超越了混合模型和欧几里得模型，H-BERT在预训练BERT模型上取得了进步，验证了“全双曲”方法的优越性。\n\n**阶段3：构建“全栈”双曲基础模型**\n*   **背景**：早期的全双曲模型虽然有效，但往往只实现了部分核心组件，缺乏完整的Transformer架构所需的一切（如全面的归一化、残差连接、可学习的位置编码等）。\n*   **关键问题**：如何构建一个功能完整、稳定且性能优越的“全栈”双曲Transformer，使其能够真正对标欧几里得基础模型？\n*   **决策3（Hypformer [115]）**：旨在弥补这一空白，开发了一系列新的全双曲操作，包括“带曲率的双曲变换”（Hyperbolic Transformation with Curvatures）、“双曲调整与优化”（Hyperbolic Readjustment and Refinement with Curvatures）以及全面的双曲位置编码和残差连接。\n    *   **逻辑与依据**：一个“基础模型”需要高度的模块化和鲁棒性。只有当所有核心组件都能在双曲空间中高效、稳定地工作时，才能真正发挥其潜力。实现“全栈”意味着模型能够处理更复杂的数据和任务。\n*   **成果**：Hypformer在各种图、图像和文本任务上表现出色，进一步证明了全双曲Transformer的有效性。\n\n**阶段4：向大型语言模型（LLMs）的扩展与效率优化**\n*   **背景**：LLMs的参数量和计算需求巨大，尤其是在生成过程中，KV-cache的内存占用是一个关键瓶颈。\n*   **关键问题**：如何在保持甚至提升双曲优势的同时，解决LLMs的效率和可扩展性问题？\n*   **决策4（HELM [47]）**：引入了“多头潜在注意力”（Multi-Head Latent Attention, HMLA）来减少KV-cache的内存占用，并提出了“曲率专家混合”（Mixture-of-Curvature Experts, MiCE）模块。\n    *   **逻辑与依据**：\n        *   **HMLA**：针对LLMs的特定痛点——KV-cache。通过将键和值投影到低维潜在空间，大幅降低了内存需求，这是实现大规模双曲LLM的关键效率改进。\n        *   **MiCE**：认识到现实数据并非所有部分都具有相同的最优曲率。通过让不同的“专家”在不同的曲率空间中操作，模型可以更好地适应数据中复杂的、非均匀的几何分布，从而提升模型的“适应性”和表示能力。\n*   **成果**：HELM在数十亿参数规模上超越了欧几里得LLMs，并在多个基准测试中表现出色，成功解决了双曲模型在可扩展性方面的挑战。\n\n**阶段5：与现有欧几里得模型的衔接与微调**\n*   **背景**：大量的预训练基础模型已经存在于欧几里得空间中。\n*   **关键问题**：如何高效地将双曲几何的优势融入到现有的欧几里得预训练模型中，或者实现双曲空间的低成本微调？\n*   **决策5（HypLoRA [113]）**：将流行的LoRA（Low-Rank Adaptation）微调方法扩展到双曲空间。\n    *   **逻辑与依据**：LoRA是一种参数高效的微调方法，对预训练模型影响最小。将其双曲化，可以使研究人员在不从头训练大型双曲模型的情况下，利用双曲几何的优势提升现有欧几里得模型的性能，特别是在处理具有层次结构的任务时。\n*   **成果**：在数学推理基准测试中，HypLoRA优于欧几里得LoRA，展示了将双曲几何应用于现有模型微调的潜力。\n\n通过上述阶段性的决策和演进，我们希望清晰地展示了双曲深度学习如何从理论假设、初步验证，逐步走向更完整、更高效、更具竞争力的大型基础模型应用，从而为未来的研究指明方向。"
    },
    {
        "order": 6,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17788",
        "abs_url": "https://arxiv.org/abs/2507.17788",
        "pdf_url": "https://arxiv.org/pdf/2507.17788",
        "title": "Adaptive Repetition for Mitigating Position Bias in LLM-Based Ranking",
        "authors": [
            "Ali Vardasbi",
            "Gustavo Penha",
            "Claudia Hauff",
            "Hugues Bouchard"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "When using LLMs to rank items based on given criteria, or evaluate answers, the order of candidate items can influence the model's final decision. This sensitivity to item positioning in a LLM's prompt is known as position bias. Prior research shows that this bias exists even in large models, though its severity varies across models and tasks. In addition to position bias, LLMs also exhibit varying degrees of low repetition consistency, where repeating the LLM call with the same candidate ordering can lead to different rankings. To address both inconsistencies, a common approach is to prompt the model multiple times with different candidate orderings and aggregate the results via majority voting. However, this repetition strategy, significantly increases computational costs. Extending prior findings, we observe that both the direction -- favoring either the earlier or later candidate in the prompt -- and magnitude of position bias across instances vary substantially, even within a single dataset. This observation highlights the need for a per-instance mitigation strategy. To this end, we introduce a dynamic early-stopping method that adaptively determines the number of repetitions required for each instance. Evaluating our approach across three LLMs of varying sizes and on two tasks, namely re-ranking and alignment, we demonstrate that transitioning to a dynamic repetition strategy reduces the number of LLM calls by an average of 81%, while preserving the accuracy. Furthermore, we propose a confidence-based adaptation to our early-stopping method, reducing LLM calls by an average of 87% compared to static repetition, with only a slight accuracy trade-off relative to our original early-stopping method.",
        "gemini2.5flash": "",
        "overall_idea": "好的，作为这篇论文的作者之一，我将深入回顾我们从选题到最终解决问题，以及整个思维和决策过程。我将通过一个具体的例子来阐述这一切。\n\n---\n\n### **0. 为什么选择这个研究内容？**\n\n在当前人工智能的浪潮中，大型语言模型（LLM）的应用边界正在迅速拓展。我们团队，特别是在Spotify这样的内容推荐和用户体验驱动的场景下，自然而然地开始探索LLM在更复杂任务中的潜力，其中“LLM作为判断者（LLM-as-a-Judge）”就是我们关注的焦点之一。具体来说，我们希望LLM能够像人类专家一样，对推荐结果、用户生成内容、乃至其他LLM的输出进行质量评估和排序。\n\n然而，在初步的内部实验和对现有文献的调研中，我们很快发现了一个反复出现且令人头疼的问题：LLM的判断并不总是稳定的。最明显的就是“位置偏差（Position Bias）”——当我们将两个候选项目A和B以不同顺序（A在前B在后，或B在前A在后）呈现给LLM时，它给出的偏好可能会发生改变。这就像一个裁判，当球员A站在左边时判他赢，当他站在右边时又判他输，这显然是不可接受的。\n\n除了位置偏差，我们还观察到了“重复一致性（Repetition Consistency, RC）”问题：即使是完全相同的输入（相同的Prompt，相同的候选顺序），LLM在多次调用下也可能给出不同的答案。这表明LLM内部存在一定程度的随机性或不确定性。\n\n这两个问题——位置偏差和重复不一致性——共同指向了一个核心挑战：**LLM作为判断者的鲁棒性（Robustness）和可靠性**。如果LLM的判断容易受到无关因素（如位置）的影响，或者自身结果不稳定，那么它就无法替代人类专家进行高质量的评估，也无法在生产环境中大规模应用。\n\n当时，业界的通行做法是采用“重复并聚合”的策略：将相同的候选对以不同顺序（例如，A,B和B,A）多次提交给LLM，然后通过多数投票来确定最终的共识结果。这种方法确实能在一定程度上缓解偏差和不一致性，但它的**计算成本极高**。我们很快意识到，如果我们要将LLM作为判断者大规模应用于我们的产品，这种“暴力”重复的方式将带来天文数字般的API调用费用和延迟。\n\n因此，我们的研究动机非常明确：**我们相信LLM-as-a-Judge具有巨大潜力，但其固有的不稳定性及其带来的高昂成本是阻碍其广泛应用的关键瓶颈。我们必须找到一种既能保证判断的鲁棒性，又能显著降低计算成本的解决方案。**\n\n---\n\n### **1. 如何一步步构思和界定研究问题的？**\n\n我们的研究问题构思和界定过程可以分为以下几个阶段：\n\n**阶段1：初步观察与痛点识别**\n*   **观察到现象：** 在早期进行LLM辅助的文档排序和答案对比实验时，我们注意到，简单地将文档A和文档B以`(A, B)`的顺序输入LLM，得到的偏好结果与以`(B, A)`的顺序输入时可能会相反。甚至有时，对同一个`(A, B)`输入重复调用几次，LLM给出的偏好也会波动。\n*   **识别痛点：** 这种不稳定性直接影响了LLM作为“客观裁判”的信任度。尤其在需要高精度和一致性的评估场景，如评估搜索结果质量、对话系统响应优劣等，这种不稳定是致命的。\n\n**阶段2：问题形式化与指标定义**\n为了科学地分析和解决问题，我们决定将其形式化：\n*   **LLM判断单元：** 我们将研究聚焦在最基础的判断单元上——**成对比较（Pairwise Comparison）**。即，给定两个候选项目`a`和`b`，LLM选择其中一个作为“优胜者”。这种二元选择是许多复杂排序任务的基础。\n*   **定义鲁棒性指标：**\n    *   **重复一致性 (Repetition Consistency, RC)：** 衡量LLM在给定**相同输入顺序**（如`(a, b)`）的情况下，重复多次调用其判断结果的稳定性。如果`n`次判断都是同一个结果，则RC高。\n    *   **置换一致性 (Permutation Consistency, PC)：** 衡量LLM在给定**不同输入顺序**（`(a, b)` vs. `(b, a)`）时，最终共识判断结果的一致性。如果无论顺序如何，最终都判断`a`优于`b`，则PC高。PC低就意味着存在位置偏差。\n*   **定义共识结果 (Consensus Outcome)：** 为了从多次重复和不同顺序的判断中获得单一结果，我们采用**多数投票（Majority Voting）**机制。例如，对于`(a, b)`和`(b, a)`两种顺序各重复`n`次，将这`2n`次结果合并后，投票数多的为共识结果。如果票数相同，则为“平局（tie）”。\n\n**阶段3：量化成本与效率目标**\n*   **成本指标：** LLM调用次数。我们明确的目标是，在保持或接近最高精度（由大量重复得到的共识结果）的前提下，大幅减少LLM调用次数。\n*   **效率目标：** 从“固定重复次数”向“动态自适应重复次数”转变。核心思想是：能否根据每次判断的实时反馈，智能地决定是否还需要额外的重复？\n\n**阶段4：核心观察的提出**\n在进行了大量的预实验后，我们有了一个**关键的经验性观察**，它成为了我们后续方法的基石：\n*   **观察1：** \"对于每个LLM和候选对`(a,b)`，LLM在至少一个输入顺序（`(a,b)`或`(b,a)`）下，其重复判断结果（RC）表现出高一致性。\"\n*   **这意味着什么？** 这意味着，对于大多数候选对，LLM至少在一种呈现顺序下，其内部偏好是相对“坚定”的。即使存在位置偏差，这种“坚定”的偏好通常会与某个位置偏好对齐，从而快速形成一个稳定（RC高）的判断流。\n*   **举个例子：** 假设LLM内在更喜欢文档A。\n    *   当我们输入`(A, B)`时，位置偏好可能加强了LLM对A的偏好，导致LLM非常稳定地每次都说“A”。\n    *   当我们输入`(B, A)`时，位置偏好可能倾向于B，这与LLM内在对A的偏好冲突。结果可能导致LLM的判断摇摆不定（比如有时说B，有时说A），或者即便说B，也显得不那么“坚定”。\n    *   但根据我们的观察，总有一个顺序能让LLM的判断趋于稳定。\n\n这个观察至关重要，因为它提供了一个停止重复的信号：一旦发现某个顺序下的判断变得高度一致，我们就有理由认为我们已经捕捉到了LLM在这个方向上的“坚定”信号，即便另一个方向还不稳定，这个坚定信号也可能主导最终的共识结果。\n\n通过以上步骤，我们将最初的模糊痛点——“LLM判断不稳定且昂贵”，清晰地界定为：“**如何在确保成对比较判断的置换一致性（PC）和重复一致性（RC）的前提下，通过动态自适应重复策略，显著降低LLM调用次数？**”\n\n---\n\n### **2. 为了解决该问题，您在选择理论框架、研究方法和数据分析策略时，经历了怎样的思考、权衡和取舍？**\n\n为了解决上述问题，我们的思考过程如下：\n\n**2.1 理论框架的选择与取舍**\n\n*   **初始思考：** 是否需要构建一个复杂的LLM行为模型来预测其判断？比如，基于LLM内部的注意力机制、token表示等来解释位置偏差的来源？\n*   **取舍：** 尽管深入理解LLM内部机制非常有吸引力（这也是我们未来工作的方向），但在当前阶段，我们的首要目标是解决**实际应用中的效率和鲁棒性问题**。构建一个通用且准确的LLM行为模型既耗时又复杂，而且不同LLM的内部结构差异巨大，通用性难以保证。\n*   **最终选择：** **“黑盒”经验主义方法。** 我们选择不深入LLM内部，而是专注于其**外部行为**（输出结果和自信度）。我们的核心“理论”就是基于前面提到的“观察1”——即LLM在至少一个顺序下会表现出高一致性。这个观察虽然是经验性的，但它足够普适，不依赖于特定模型架构，因此具有很强的**通用性**。这种务实的态度使我们能够快速验证方案的有效性。\n\n**2.2 研究方法的设计与思考**\n\n*   **基线设定：** 为了评估我们方法的有效性，必须有明确的基线。\n    *   **下限（最低成本，最低鲁棒性）：** `Swap Once`。即只进行一次`(a, b)`和一次`(b, a)`的判断（总共2次LLM调用），然后直接比较。这代表了最不鲁棒但成本最低的情况。\n    *   **上限（最高鲁棒性，最高成本）：** `Consensus Outcome`。这是通过对两种顺序各重复12次（总共24次对LLM的调用，即2n=24），然后进行多数投票得到的共识结果。我们假设这个结果是接近“真实”或“最鲁棒”的判断，因为它通过大量重复平均了偏差和不确定性。这个作为我们精度上的“天花板”或“黄金标准”。\n*   **核心方法——早期停止（Early Stopping）的构思：**\n    *   **痛点：** 固定的24次调用太多了。\n    *   **灵感来源：** “观察1”。如果LLM在某个顺序下已经非常稳定地给出了结果，我们还需要继续调用吗？\n    *   **核心逻辑：** 我们并行地对`(a, b)`和`(b, a)`进行重复调用，并不断聚合当前的判断结果，计算`C2n(a, b)`。只要`C2n(a, b)`不再是“tie”（即出现明确的多数），我们就停止。\n    *   **权衡：** 这种方法假设一旦出现明确的多数，这个多数结果就是最终的稳定结果。虽然“观察1”支持这一点，但我们知道存在极少数“违反”情况（如表1所示的低违反率）。这意味着在极少数情况下，我们可能会过早停止并得到一个次优的结果。但考虑到其带来的巨大成本节约，这是一个值得的**准确性-效率权衡**。\n\n*   **进阶方法——基于信心的早期停止（Confidence-Based Early Stopping）的构思：**\n    *   **痛点：** 即使有了“早期停止”，对于那些LLM自身判断能力弱或位置偏差特别强的“困难”实例（即`C2n(a,b)`长时间是`tie`的情况），我们仍然会达到最大重复次数（比如24次）。这些实例仍然是“昂贵”的。\n    *   **灵感来源：** LLM通常能输出其对生成答案的“置信度”分数。我们猜想，如果LLM对某个判断非常自信（例如，对A的自信度远高于B），那么即使有位置偏差，其最终的共识结果也可能很快稳定下来。\n    *   **核心逻辑：** 引入“置信度差距”`g = |Pa - Pb|`，其中`Pa`和`Pb`是LLM对选择A和选择B的平均置信度。如果`g`很大，表示LLM对某个选择非常自信，那么我们就可以进一步限制最大重复次数，因为这种自信可能意味着其结果会更快收敛。反之，如果`g`很小，表示LLM“犹豫不决”，那么可能需要更多重复来打破僵局。\n    *   **实现细节：** 我们需要一个小的校准数据集来训练一个简单的线性模型，将置信度差距映射到所需的重复次数上（或决定一个更低的`n_M`上限）。\n    *   **权衡：**\n        *   **优势：** 进一步减少了那些“困难”实例的重复次数，尤其是在LLM虽然矛盾但某方“自信”的情况下。\n        *   **劣势：** 依赖于LLM输出可靠的置信度，并且需要额外的校准步骤（少量LLM调用用于训练）。但我们发现LLM的置信度与实际判断概率有较好的相关性，因此这个投入是值得的。\n\n**2.3 数据分析策略的设计**\n\n*   **数据集选择：** 我们选择了多样化的数据集来验证方法的普适性。\n    *   **任务多样性：** 既包括“重排序（Re-rank）”任务（TREC-DL数据集），也包括“对齐（Alignment）”任务（DPO数据集），代表了LLM作为判断者的两种常见应用场景。\n    *   **数据特性：** 确保数据中存在足够的“困难”实例和“容易”实例，以及不同程度的位置偏差。\n*   **模型选择：** 使用了三种不同规模和性质的LLM（一个开源，两个专有），以证明我们的方法不依赖于特定模型，具有通用性。\n*   **评估指标：**\n    *   **主要评估指标：**\n        *   **归一化准确率 (Normalized Accuracy)：** 将我们方法的准确率除以`Consensus Outcome`的准确率。目标是接近1（即与黄金标准持平）。\n        *   **平均LLM调用次数 (Average LLM Calls)：** 核心成本指标，目标是显著低于`Consensus Outcome`的24次。\n    *   **辅助分析指标：**\n        *   **PC/Primacy Biased/Recency Biased比例：** 这些指标（如表2）用于量化位置偏差的程度和方向，从而说明我们方法解决的问题的复杂性，并佐证“位置偏差方向和强度因实例而异”的观点。\n*   **可视化：** 通过直方图（图2）展示不同LLM在不同偏差方向上的概率差距分布，直观地支持了“按实例处理”的必要性。通过柱状图（图3）直观比较不同方法的准确率和成本，便于读者理解核心发现。\n\n在整个过程中，我们始终秉持“**实用至上**”的原则。我们首先关注 LLM-as-a-Judge 在实际应用中的痛点，然后提出一个简洁且通用的解决方案，并用严谨的实验设计和全面的数据分析来验证其有效性，而不是仅仅停留在理论层面。这种权衡使得我们的工作既具有学术价值，又具有很高的工业应用价值。\n\n---\n\n### **3. 在研究的每个关键阶段，您做出了哪些重要决策，并能解释这些决策背后的逻辑和依据。**\n\n让我们通过一个具体的例子——假设我们要让LLM判断“文档A”和“文档B”哪个更相关于某个查询，来串联整个决策过程。\n\n**例子设定：**\n*   **查询：** “最新的电动汽车技术”\n*   **文档A：** 详细介绍了特斯拉Model Y的最新电池技术和续航能力。\n*   **文档B：** 介绍了宝马在电动化转型中的策略和未来概念车。\n*   **假设：** 从人类专家角度，文档A更直接、更详细地回答了查询，所以“文档A”是更好的答案。但LLM可能存在位置偏差。\n\n---\n\n**关键阶段1：问题识别与初步探索**\n\n*   **观察/发现：**\n    *   第一次提示LLM (A, B)，LLM选择 A。\n    *   第二次提示LLM (B, A)，LLM选择 B。\n    *   甚至提示LLM (A, B) 两次，第一次选择 A，第二次却选择 B。\n*   **痛点：** 这种不一致性让我们无法信任LLM的单一判断。如果直接用“A”或“B”作为结果，那么在其他查询上也可能出现反转，导致系统评估不可靠。\n*   ****决策1：必须采用多次重复和多顺序聚合的方式来获得鲁棒结果。**\n    *   **逻辑：** 这是业界公认的缓解LLM判断不稳定的“最佳实践”。通过多次采样来平均随机性和偏差。\n    *   **依据：** 前人研究[15, 17, 1] 和我们的初步实验都证明了重复的重要性。\n*   ****决策2：将“上限”设定为对两种顺序各重复12次（共24次LLM调用）取多数投票。**\n    *   **逻辑：** 24次重复足够多，能够使结果高度收敛和稳定，从而作为我们衡量准确率的黄金标准。\n    *   **依据：** 大量实验表明，LLM的判断准确率在24次重复后趋于平稳，再增加重复次数带来的收益递减（\"We observe a plateau in accuracy beyond 24 repetitions.\" - 脚注1）。\n*   ****决策3：将“下限”设定为`Swap Once`（即(A,B)和(B,A)各一次，共2次调用）。**\n    *   **逻辑：** 这是最低成本的基线，展示了没有充分重复和聚合时 LLM 判断的“裸机”性能。\n    *   **依据：** 用来证明我们的方法在成本降低的同时，能显著提升准确率。\n\n---\n\n**关键阶段2：核心观察的提出与早期停止策略的构思**\n\n*   **反复实验：** 我们对大量文档对进行多次重复调用，仔细观察LLM在不同顺序和重复次数下的行为。\n*   ****决策4：提出“观察1”作为我们方法的基石。**\n    *   **观察1：** \"With high empirical probability, for each LLM and candidate pair (a,b), the LLM is RC for at least one of the orderings (a,b) or (b,a).\"\n    *   **逻辑：** 这个观察至关重要。它告诉我们，即使LLM对`(B, A)`的判断摇摆不定（比如 `B, A, B, A...`），它很可能对`(A, B)`的判断是稳定的（比如 `A, A, A, A...`）。这意味着LLM内部对A的偏好可能很强，或者在A处于首位时，其判断逻辑更容易“收敛”。\n    *   **依据：** 大量实验数据支持（表1中的低“违反率”）。这种经验性发现是构建高效策略的关键。\n*   ****决策5：基于“观察1”，设计“早期停止（Early Stopping）”策略。**\n    *   **逻辑：** 我们不必等到24次调用才做判断。只要`(a, b)`和`(b, a)`的合并结果`C2n(a, b)`形成了明确的多数（即不再是`tie`），我们就认为结果已经收敛，可以停止。因为“观察1”表明，一旦某个顺序稳定下来，它很可能引导最终的共识。\n    *   **依据：** “观察1”是支撑这一逻辑的经验基础。\n\n    **具体例子演示“早期停止”：**\n    *   **目标：** 判断文档A和B哪个更好。\n    *   **传统方式 (`Consensus Outcome`, 24次调用)：**\n        *   调用 LLM(A, B) 12次 -> 假设得到 `(A, A, A, A, A, A, A, A, A, A, B, A)` (11个A, 1个B)\n        *   调用 LLM(B, A) 12次 -> 假设得到 `(B, A, B, B, A, B, B, A, B, B, A, B)` (8个B, 4个A)\n        *   总结果：15个A, 9个B。共识：A胜出。总调用次数：24次。\n    *   **我们的“早期停止”：**\n        *   **n=1 (2次调用):**\n            *   LLM(A, B) 第一次 -> `A`\n            *   LLM(B, A) 第一次 -> `B`\n            *   当前合并结果：`(A, B)`。`C2n`是 `tie`。 **继续。**\n        *   **n=2 (总4次调用):**\n            *   LLM(A, B) 第二次 -> `A`\n            *   LLM(B, A) 第二次 -> `A`\n            *   当前合并结果：`(A, B, A, A)`。`C2n`是 `A` (3个A, 1个B)。 **有明确多数，停止！**\n        *   **结果：A胜出。总调用次数：4次。**\n        *   **逻辑依据：** 在这个例子中，虽然`(B, A)`第二次也给出了A（与第一次的B不同），但`(A, B)`两次都坚定地给出了A。这使得合并结果很快倾向于A。根据“观察1”，即使`(B, A)`后续结果可能还会波动，`(A, B)`的稳定信号很可能维持A的优势，所以提前停止是高效且准确的。\n\n---\n\n**关键阶段3：进一步优化与置信度结合**\n\n*   **挑战：** 即使有了“早期停止”，对于少数**非常困难**的实例（例如，LLM对A和B的偏好都非常弱，或者位置偏差非常强，导致两种顺序下的判断都非常不稳定，或者`(A, B)`稳定出A，而`(B, A)`稳定出B，导致`C2n`一直处于`tie`状态），我们仍然会重复到最大次数（12次），这仍然很昂贵。\n*   ****决策6：引入LLM的“置信度”作为辅助信号。**\n    *   **逻辑：** 如果LLM在给出判断的同时，能够提供一个置信度分数，这可能反映了它对这个判断的“信念强度”。如果LLM对A的偏好置信度非常高，即使是第一次判断，也可能意味着其内部倾向性很强，最终结果会快速收敛。\n    *   **依据：** 我们在LLM调用时尝试获取置信度，并发现“置信度差距”（LLM对优胜者的平均置信度与对劣势者的平均置信度之差）与实际判断的“概率差距”存在相关性。前人研究[19]也支持LLM置信度的可用性。\n*   ****决策7：设计“基于信心的早期停止（Confidence-Based Early Stopping）”策略。**\n    *   **逻辑：** 在早期停止的基础上，如果我们的LLM判断在少量重复后，其置信度差距`g = |Pa - Pb|`已经很大（LLM非常自信地偏向某一方），那么我们就可以进一步缩短该实例的最大重复次数上限。对于置信度差距小的实例，则允许进行更多重复。\n    *   **依据：** 置信度高的判断更有可能快速收敛，从而进一步节约调用。\n\n    **具体例子演示“基于信心的早期停止”：**\n    *   假设在上述“早期停止”的例子中，LLM在第一次给出A（对于(A,B)）时，其对A的置信度是0.95，对B的置信度是0.05。这个`g`非常大。\n    *   而LLM在第一次给出B（对于(B,A)）时，其对B的置信度是0.55，对A的置信度是0.45。这个`g`很小。\n    *   在“基于信心的早期停止”中，因为(A,B)顺序下的判断展现了极高的自信度，系统会根据这个信号，进一步将这个特定实例的`n_M`（最大重复次数）设置得更低，例如从12降到3。\n    *   这样，即便在`n=1`时`C2n`是`tie`，我们也可以在`n=2`或`n=3`时，基于(`A,B`)顺序下的高度自信和一致性，更快地做出决策，甚至在`C2n`还未完全明确多数时，如果置信度差距已经足够大，就判断为“A”。这进一步减少了部分实例的调用次数。\n\n---\n\n**关键阶段4：结果评估与验证**\n\n*   ****决策8：选择归一化准确率和平均调用次数作为核心评估指标，并在多模型、多任务上进行验证。**\n    *   **逻辑：** 归一化准确率确保了我们方法与最佳（最高成本）方法的性能对齐，平均调用次数直接量化了成本节约。多模型多任务的测试确保了方法的普适性，避免了过拟合特定模型或任务。\n    *   **依据：** 这是科学研究中评估新方法效果的黄金标准。\n*   **结果：** 我们的“早期停止”方法平均减少了81%的LLM调用次数，同时保持了与`Consensus Outcome`相同的准确率。而“基于信心的早期停止”甚至达到了87%的平均削减，仅有微小的准确率损失（图3，表3）。\n\n---\n\n整个过程是一个迭代优化的过程：从发现问题、量化问题，到提出初步解决方案，再到基于观察和数据进一步优化解决方案。每一步的决策都紧密围绕“提高鲁棒性同时降低成本”这个核心目标，并且基于严谨的实验和经验性观察作为支撑。"
    },
    {
        "order": 7,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17791",
        "abs_url": "https://arxiv.org/abs/2507.17791",
        "pdf_url": "https://arxiv.org/pdf/2507.17791",
        "title": "Helix 1.0: An Open-Source Framework for Reproducible and Interpretable Machine Learning on Tabular Scientific Data",
        "authors": [
            "Eduardo Aguilar-Bejarano",
            "Daniel Lea",
            "Karthikeyan Sivakumar",
            "Jimiama M. Mase",
            "Reza Omidvar",
            "Ruizhe Li",
            "Troy Kettle",
            "James Mitchell-White",
            "Morgan R Alexander",
            "David A Winkler",
            "Grazziela Figueredo"
        ],
        "comments": "17 pages",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Helix is an open-source, extensible, Python-based software framework to facilitate reproducible and interpretable machine learning workflows for tabular data. It addresses the growing need for transparent experimental data analytics provenance, ensuring that the entire analytical process -- including decisions around data transformation and methodological choices -- is documented, accessible, reproducible, and comprehensible to relevant stakeholders. The platform comprises modules for standardised data preprocessing, visualisation, machine learning model training, evaluation, interpretation, results inspection, and model prediction for unseen data. To further empower researchers without formal training in data science to derive meaningful and actionable insights, Helix features a user-friendly interface that enables the design of computational experiments, inspection of outcomes, including a novel interpretation approach to machine learning decisions using linguistic terms all within an integrated environment. Released under the MIT licence, Helix is accessible via GitHub and PyPI, supporting community-driven development and promoting adherence to the FAIR principles.",
        "gemini2.5flash": "",
        "overall_idea": "作为Helix论文的作者，我非常乐意深入剖析这篇研究背后的思维决策过程，并以SWIRL（Utero SWIRL）项目中的医学数据集为例，详细阐述我们是如何从问题构思到最终解决方案的每一步。\n\n---\n\n### 0. 为什么选择这个研究内容？\n\n回溯到研究伊始，我们团队在生物材料、化学和医学等跨学科领域与大量领域专家（例如材料科学家、化学家、临床医生）进行合作。我们观察到一个普遍的痛点：虽然机器学习在科学发现中显示出巨大潜力，但其复杂的流程和对编程能力的要求，使得非数据科学背景的领域专家难以独立、透明且可重复地应用这些工具。\n\n具体来说：\n*   **数据爆炸与ML需求：** 科学数据量与日俱增，ML已成为从数据中提取洞见的关键。\n*   **跨学科合作的挑战：** 传统上，领域专家需要将数据交给数据科学家，然后等待结果。这个过程往往不透明，反馈周期长，且难以迭代优化。\n*   **现有工具的局限性：** Jupyter Notebooks虽然灵活，但对编程技能要求高，且很难自动记录和管理实验的完整“血统”（provenance）。市面上其他一些ML平台要么过于专业化（侧重部署而非探索性分析），要么缺乏端到端的透明度，尤其是在数据预处理和模型解释阶段。\n*   **科学研究的本质需求：** 科学发现需要高度的**可重复性（Reproducibility）**和**可解释性（Interpretability）**。一个模型做出预测，我们不仅要知道“是什么”，更要知道“为什么”，并能追溯每一步决策的依据。**FAIR原则（Findable, Accessible, Interoperable, Reusable）**成为了我们指导思想的基石。\n\n因此，我们萌生了这样一个想法：**能否开发一个用户友好、端到端、且能自动记录所有实验细节的平台，让领域专家也能驾驭机器学习，同时确保科学研究所需的透明度和可信度？**这就是我们选择开发Helix的初衷，旨在弥合领域知识与数据科学工具之间的鸿沟。\n\n---\n\n### 1. 如何一步步构思和界定研究问题的？\n\n问题构思并非一蹴而就，而是从广泛的痛点逐步聚焦到具体的需求：\n\n*   **初步痛点（Broad Problem）:** 科学领域的数据分析与机器学习缺乏透明度、可重复性，且对非专业人士不友好。\n\n*   **问题细化与界定（Refining the Problem）:**\n    1.  **用户画像（Target Audience）:** 我们的核心用户是“无正式数据分析培训的实验专家”。这意味着工具必须是图形用户界面（GUI）驱动的，避免或最小化代码编写。\n    2.  **核心功能痛点（Functional Gaps）:**\n        *   **数据预处理：** 领域专家需要直观的方式进行数据清洗、标准化、特征选择，而不是编写复杂的脚本。\n        *   **模型训练与评估：** 需要支持多种常见的ML算法，提供标准的评估指标，并能进行超参数调优，但这些过程应该被简化。\n        *   **模型解释：** 这是最关键的一点。仅仅提供一个预测值或准确率是不够的。领域专家需要理解模型“为什么”会做出这个预测，哪些特征最重要，以及特征之间的复杂关系。这要求超越简单的特征重要性数值，提供更“人性化”的解释。\n        *   **可重复性与透明度：** 每次实验，无论成功与否，所有步骤、选择、参数、结果都必须被完整记录。这不仅仅是为了研究者自己，也是为了团队协作和后续审计。\n    3.  **技术约束与可行性（Technical Constraints & Feasibility）:**\n        *   **开源：** 鼓励社区贡献和透明度。\n        *   **可扩展：** 随着ML技术发展，需要能够轻松集成新的算法、预处理方法和解释工具。\n        *   **部署简易：** 最好能本地运行，减少部署门槛。\n\n*   **SWIRL数据集案例的引入，进一步明确问题：**\n    当我们将目光转向SWIRL数据集时，这些普遍问题变得更加尖锐和具体。SWIRL项目旨在预测胎儿死亡（死产）的风险，这是一个**高风险（high-stakes）**且**敏感（sensitive）**的分类任务。\n    *   **高维度与小样本：** 数据集有90个临床特征，但只有46个样本。这立即触发了“维度灾难”的警报：特征远多于样本，极易导致过拟合。这使得**有效的特征选择**成为首要任务。\n    *   **类别不平衡：** 只有11个阳性案例（near miss），35个阴性案例（healthy）。严重的不平衡会影响模型性能和解释的可靠性，需要**鲁棒的评估和解释方法**。\n    *   **可解释性至关重要：** 在医疗领域，医生和临床研究人员不能仅仅依赖一个“黑盒”预测。他们需要理解哪些临床指标导致了高风险预测，以便进行干预或进一步研究。这要求模型解释必须**清晰、可信，并能被医学专业人士理解和验证**。\n    *   **可重复性是底线：** 任何关于胎儿健康的关键发现都必须是可重复的。这意味着模型的训练过程、特征选择逻辑、超参数设置、数据分割等都必须被完整记录，以供同行评审和验证。\n\n因此，我们界定的核心问题就是：**如何构建一个集成平台，能够帮助领域专家（如SWIRL项目中的临床医生）在面对像SWIRL这样具有高维度、小样本、类别不平衡且结果敏感的表格数据时，能够透明、可重复地进行端到端的机器学习分析，并提供可信赖、可理解的（最好是自然语言形式的）模型解释，从而加速他们的科学发现和决策？**\n\n---\n\n### 2. 为了解决该问题，在选择理论框架、研究方法和数据分析策略时，经历了怎样的思考、权衡和取舍。\n\n解决上述问题，我们进行了多方面的思考和权衡：\n\n*   **理论框架的选择：**\n    *   **FAIR原则：** 这是指导我们整个设计和实现过程的“北极星”。它不仅仅是技术规范，更是一种科研伦理。为了实现“F”和“A”，我们选择开源和易于部署；为了“I”和“R”，我们强调标准化数据格式和详细的元数据记录。\n    *   **可解释性AI（XAI）的整合：** 我们深知“特征重要性”的局限性，因此决定整合多种XAI方法（LIME, SHAP, Permutation Importance），并在此基础上进行创新。\n    *   **模糊逻辑（Fuzzy Logic）的引入：** 这是我们团队的一个特长。我们发现传统的特征重要性分数是数值的，难以直接转化为人类可理解的“规则”。模糊逻辑可以将连续的数值转化为“高”、“中”、“低”等语言项，并能处理不确定性和模糊性，从而生成更具洞察力的“IF-THEN”规则。这是一个关键的**取舍**：我们没有选择简单地列出SHAP值，而是投入精力去开发更深层次的语言化解释，以满足领域专家对“为什么”的深层需求。\n\n*   **研究方法的选择：**\n    *   **Python生态系统：** 这是一个毫无疑问的选择。Python拥有庞大且成熟的机器学习库（scikit-learn, xgboost等）和数据科学工具（pandas, numpy），以及快速开发Web应用（Streamlit）的框架。这极大地加速了我们的开发进程，并确保了工具的可扩展性。\n    *   **Streamlit作为GUI框架：**\n        *   **优点：** 开发速度极快，可以迅速将Python脚本转化为交互式Web应用，部署简单（只需`streamlit run app.py`）。这完美契合了我们“降低非技术用户门槛”的目标。\n        *   **缺点/权衡：** Streamlit主要面向单用户、轻量级应用。它在多用户支持、并发会话管理、复杂的企业级状态管理方面存在局限性。这是一个重要的**权衡**：我们选择了开发效率和单用户体验，而不是复杂的多用户功能。对于学术研究团队内部使用，这在初期是完全可接受的，未来可以考虑迭代升级。\n    *   **面向对象（Object-Oriented）架构：** 决定采用模块化、分层的OO设计。\n        *   **逻辑：** 这样可以清晰地分离数据操作、ML模型、解释器、服务层和UI层，大大提高了代码的可维护性、可测试性和可扩展性。例如，增加一个新的ML模型，只需继承`Learner`基类并实现其抽象方法即可。\n\n*   **数据分析策略（以SWIRL为例的思考与决策）：**\n\n    1.  **高维度处理：** 90个特征对46个样本是严重的过拟合风险。\n        *   **思考：** 简单地扔给一个模型会崩溃或给出不可信的结果。必须进行特征选择。\n        *   **决策：** 采用**两阶段建模管道**。\n            *   **第一阶段（特征选择）：** 不仅仅依赖单一特征选择方法。为了增加鲁棒性，我们决定同时运行Helix支持的**所有四种分类模型**（Logistic Regression, Random Forest, XGBoost, SVM）进行5折交叉验证。然后，通过**集成多数投票（ensemble majority voting）**来选择出对所有模型都重要的**前5个特征**。\n            *   **逻辑与依据：** 这种集成策略大大降低了单一模型或方法偏差导致错误特征选择的风险，使选出的特征更具普遍性和鲁棒性。这也是为了在小样本数据上，最大限度地提取可信赖的信号。\n    2.  **类别不平衡处理：**\n        *   **思考：** 少数类（near miss）样本太少，模型可能会偏向多数类。\n        *   **决策：** 虽然论文中未详细展开所有细节，但在实际开发中，Helix的ML模块允许选择不同的采样策略（如过采样、欠采样，或通过调整模型权重来处理）。此外，模型的评估指标也需要多元化，不能只看准确率，F1分数、召回率、ROC曲线等更适合不平衡数据集。\n        *   **逻辑与依据：** 这些策略确保了模型在少数类上的性能不会被多数类淹没，从而对高风险的“near miss”案例有更好的识别能力。\n    3.  **模型解释的深度：**\n        *   **思考：** 仅仅给出重要特征列表是不够的，临床医生想知道“为什么这个特征重要”以及“它的哪些值域对应高风险”。\n        *   **决策：** 在第二阶段，选择了**Logistic Regression**作为最终模型。虽然它不如随机森林或XGBoost强大，但它是**线性模型**，其系数本身就具有一定的可解释性。更重要的是，我们应用了**SHAP**等本地解释方法，来揭示每个特征对单个预测的贡献。\n        *   **逻辑与依据：** Logistic Regression的线性特性使其更容易理解特征的**方向性影响**（正相关还是负相关）。结合SHAP，我们可以看到每个特征值是如何“推动”模型做出特定预测的。例如，SHAP图揭示了“更高的舒张压”与“近失病例”呈正相关，这为临床医生提供了具体的生物学关联。更进一步，如果我们应用模糊解释功能，就可以生成“如果舒张压高，则风险高”这样的自然语言规则。\n    4.  **结果的可审计性与透明度：**\n        *   **思考：** 整个两阶段过程以及其中做出的所有选择（特征选择方法、模型选择、超参数、评估指标）都必须被完整记录。\n        *   **决策：** Helix的“实验检查”页面以及本地文件系统中的结构化日志和元数据文件（JSON/YAML）起到了关键作用。每次实验的输入、预处理参数、模型配置、结果（图表、指标、预测值）都被自动保存。\n        *   **逻辑与依据：** 这确保了SWIRL项目中的任何一个预测或发现，都可以被团队成员或外部专家加载、审查，并重新运行，从而最大化了研究的透明度和可重复性。这正是FAIR原则的体现。当医生对某个模型结果感到疑惑时，我们可以立即展示所有中间步骤和决策，增强他们的信任。\n\n---\n\n### 3. 在研究的每个关键阶段，您做出了哪些重要决策，并能解释这些决策背后的逻辑和依据。\n\n结合SWIRL案例，我们回顾在开发和应用Helix过程中的关键决策：\n\n1.  **决策：优先开发图形用户界面（GUI）而非API/代码库。**\n    *   **逻辑与依据：** 针对目标用户（领域专家），他们最不擅长也最排斥的就是编程。一个直观、易用的GUI是降低门槛的唯一途径。虽然这意味着前期开发投入更大，但能确保工具的广泛采纳和应用。Streamlit的选择正是为了快速实现这一目标。\n    *   **SWIRL中的体现：** 临床医生不需要编写一行Python代码，就能通过Helix的界面上传SWIRL数据集，点击几下按钮完成特征选择、模型训练和SHAP解释，这极大地加速了他们的研究进程。\n\n2.  **决策：在软件架构中内嵌“provenance”追踪机制。**\n    *   **逻辑与依据：** 可重复性不仅是代码层面的（`random_seed`），更是决策层面的。每一次用户选择（比如选择MinMax标准化，选择LASSO特征选择，选择Random Forest模型，设置超参数）都必须被自动记录下来，作为实验的“DNA”。这通过将所有配置参数持久化为结构化的JSON文件实现。\n    *   **SWIRL中的体现：** 当我们发现某些特征（如舒张压、血小板计数）与胎儿死亡风险相关时，可以清晰地追溯到：这些特征是如何被选择的（通过两阶段、多模型投票），模型是如何训练的，以及采用了哪些预处理步骤。这让医学团队能够审计结果，建立信任，并在此基础上进行更深入的临床验证。\n\n3.  **决策：整合多种模型解释技术，并探索“模糊解释”生成自然语言规则。**\n    *   **逻辑与依据：** 仅仅提供数字化的特征重要性对于领域专家来说不够直观。不同解释方法有其侧重，需要提供多样化视角。更重要的是，我们团队在模糊逻辑方面的积累，让我们有机会将冰冷的数值转化为更具语义的“IF-THEN”规则，模仿人类的推理方式。这是一个从“是什么”到“为什么”的跃迁。\n    *   **SWIRL中的体现：** SHAP图能够直观地展示每个特征（如血红蛋白、白细胞计数）对预测的正面或负面影响。更深层次的模糊解释将可能生成：“如果舒张压处于高水平，并且血小板计数较低，那么胎儿死亡的风险较高。”这种自然语言的规则对于临床医生制定干预策略或设计新研究具有直接的指导意义。\n\n4.  **决策：针对高维小样本数据，提出“两阶段建模”策略，并采用集成投票进行特征选择。**\n    *   **逻辑与依据：** 这是SWIRL数据集带来的具体挑战。盲目训练模型会导致过拟合和不可信的结果。集成投票在小样本情况下，能比单一模型更稳健地识别重要特征，减少偶然性。第二阶段选择线性模型（Logistic Regression）则兼顾了模型性能和可解释性。\n    *   **SWIRL中的体现：** 通过第一阶段的特征选择，我们成功将90个特征降维到最关键的5个，这极大地降低了过拟合风险，并使后续模型的解释变得更加聚焦和有意义。如果直接在90个特征上训练，SHAP图会变得杂乱，难以提取有效信息。\n\n5.  **决策：将工具定位为“辅助发现”，而非“自动决策”。**\n    *   **逻辑与依据：** 在复杂科学问题，尤其是敏感的医疗领域，AI应是人类智慧的延伸，而非替代。工具旨在帮助专家快速探索数据、生成假设、提供洞见，最终的决策仍由具备领域知识的人类专家做出。\n    *   **SWIRL中的体现：** SHAP图显示“年轻产妇年龄可能增加近失风险”时，专家指出这可能是“样本偏差”导致。这正是工具辅助发现，然后由人类专家进行批判性思考和验证的典型案例。这种反馈机制也促进了进一步的数据收集和研究方向调整。\n\n---\n\n总而言之，Helix的诞生是我们团队对“如何让科学研究中的数据分析更透明、可重复、可解释”这一核心问题的持续探索。SWIRL案例生动地展示了我们如何在实际应用中，通过一系列深思熟虑的理论选择、方法权衡和关键决策，最终为领域专家提供一个既强大又易用的工具，以加速他们的科学发现。"
    },
    {
        "order": 8,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17792",
        "abs_url": "https://arxiv.org/abs/2507.17792",
        "pdf_url": "https://arxiv.org/pdf/2507.17792",
        "title": "Causal Mechanism Estimation in Multi-Sensor Systems Across Multiple Domains",
        "authors": [
            "Jingyi Yu",
            "Tim Pychynski",
            "Marco F. Huber"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "To gain deeper insights into a complex sensor system through the lens of causality, we present common and individual causal mechanism estimation (CICME), a novel three-step approach to inferring causal mechanisms from heterogeneous data collected across multiple domains. By leveraging the principle of Causal Transfer Learning (CTL), CICME is able to reliably detect domain-invariant causal mechanisms when provided with sufficient samples. The identified common causal mechanisms are further used to guide the estimation of the remaining causal mechanisms in each domain individually. The performance of CICME is evaluated on linear Gaussian models under scenarios inspired from a manufacturing process. Building upon existing continuous optimization-based causal discovery methods, we show that CICME leverages the benefits of applying causal discovery on the pooled data and repeatedly on data from individual domains, and it even outperforms both baseline methods under certain scenarios.",
        "gemini2.5flash": "",
        "overall_idea": "作为这篇文章的作者，我很乐意深入剖析这篇名为《多传感器系统中跨多领域因果机制估计》的论文，并回顾我们从选题到解决问题的整个思维决策过程。\n\n---\n\n### 0. 为什么选择这个研究内容？（选题动机）\n\n在过去几年中，机器学习在预测任务上取得了巨大成功。然而，当我深入思考在工业场景，特别是复杂的**智能制造系统**中，仅仅依靠预测是否足够时，我发现存在一个核心痛点：\n\n1.  **黑箱问题与可解释性需求：** 制造系统中的传感器数据庞大且复杂，虽然机器学习模型能高精度预测故障，但它们通常是“黑箱”，无法解释“为什么会发生故障”、“哪个变量是导致故障的根本原因”，也无法指导我们“如何干预才能避免故障”。这在需要精确控制和快速决策的工业环境中是不可接受的。\n2.  **数据非独立同分布（Non-i.i.d.）问题：** 传统的机器学习和很多因果发现方法都假设数据是独立同分布（i.i.d.）的。但在制造现场，情况却远非如此。数据往往受到多种因素的影响，例如：\n    *   **时间：** 早上、中午、晚上机器运行状态可能不同。\n    *   **机器ID：** 不同机器之间存在个体差异。\n    *   **产品类型：** 生产不同产品时，工艺参数和行为模式会变化。\n    *   **传感器类型/批次：** 传感器本身的特性可能影响测量。\n    这些因素导致数据在不同“领域”（domain）之间存在显著的分布偏移（distribution shift）。这意味着在某个领域训练的模型，在另一个领域可能表现不佳，这就是所谓的“泛化性差”或“OOD（Out-Of-Distribution）泛化问题”。\n3.  **因果关系的重要性：** 与相关性不同，因果关系能够揭示变量之间的“作用”关系。如果我知道A导致B，那么我干预A就能影响B。这对于**根因分析**、**故障诊断**、**过程优化**和**预测干预效果**至关重要。例如，在文章提到的泄漏测试中，如果我能确定是“气流量”影响了“测试前压力”，而不是反过来，那我就可以通过调整气流量来控制压力。\n\n因此，我看到了一个巨大的空白和迫切的需求：**如何将强大的因果发现能力，拓展到真实世界中普遍存在的、具有分布偏移的多领域数据上，从而在复杂多传感器系统中实现更深层次的理解和可解释性？** 这就是我选择这个研究方向的根本原因。\n\n---\n\n### 1. 如何一步步构思和界定研究问题的？\n\n在明确了研究动机后，下一步就是将这个宏大的目标拆解成可操作的研究问题。我的思维过程大致如下：\n\n*   **起点：现有因果发现方法。** 我首先回顾了目前最先进的因果发现方法，特别是基于连续优化的方法（如NOTEARS、GOLEM）。这些方法将因果图的学习转化为一个可微分的优化问题，可以使用梯度下降来求解，这为处理复杂非线性关系提供了可能。这是我将要建立的基础。\n\n*   **引入“领域”概念：** 为了解决非i.i.d.问题，我需要显式地将“领域”信息融入到模型中。我构思了一个场景：假设我们有来自不同生产批次的数据，每个批次构成一个“领域”。这些领域之间可能存在某些“共性”（如某些物理定律不变），也可能存在“特性”（如某个批次的特定缺陷）。\n\n*   **核心问题界定：“共同”与“个体”的因果机制。**\n    *   **“共同”机制（Common Causal Mechanisms）：** 在所有领域中都保持不变的因果关系。例如，在泄漏测试中，“腔室温度”和“气流量”共同决定“测试前压力”的物理定律，这在不同批次间很可能是不变的。识别这些机制是关键，因为它们代表了系统的核心、普适性的行为。\n    *   **“个体”机制（Individual Causal Mechanisms）：** 仅在特定领域中存在的、或在不同领域中参数不同的因果关系。例如，如果某个批次的产品有轻微泄漏，“测试前压力”和“测试后压力”之间的关系可能会发生变化（因为泄漏面积H变了）。这部分机制需要针对每个领域单独估计。\n\n*   **形式化描述：**\n    *   我采用了**函数因果模型（FCM）**的框架，即每个变量`Xj`是其父变量`PAj`和噪声`Nj`的函数：`Xj = fj(PAj) + Nj`。\n    *   那么，我的问题就变成了：给定来自`K`个不同领域的数据集`Dk`，如何有效地识别哪些`fj`是跨领域不变的（即`fj`在所有领域都相同），以及如何估计那些在特定领域变化的`fj`？\n\n*   **明确目标：**\n    1.  **鲁棒识别共同因果机制：** 即使数据存在领域偏移，也能准确找出那些跨领域不变的因果关系。\n    2.  **精准估计个体因果机制：** 在识别出共同机制后，利用这些信息作为“先验知识”，更好地估计每个领域中独有的或变化的因果关系。\n    3.  **提升整体因果发现准确性：** 最终，相比于简单的“数据池化”（不区分领域）或“领域独立训练”（不利用共同性），我们的方法应该在识别整体因果结构上表现更优。\n\n通过这样的逐步细化和界定，我将最初的模糊目标“在复杂多传感器系统中理解因果”转化为“在多领域数据中识别共同与个体因果机制，提升因果发现准确性”这一具体研究问题。\n\n---\n\n### 2. 思考、权衡和取舍（理论框架、研究方法、数据分析策略）\n\n一旦研究问题被界定，我开始思考如何选择合适的理论框架和具体方法来解决它。这个阶段充满了权衡和取舍。\n\n*   **理论框架选择：Causal Transfer Learning (CTL) 的启发。**\n    *   **思考：** 如何判断一个因果机制是“不变”的？我很快想到了因果迁移学习（CTL）中的核心思想：如果一个机制`Y = f(X) + N`是因果不变的，那么在不同的干预或领域下，`Y`关于`f(X)`的残差`N`应该与领域变量或干预变量独立。\n    *   **权衡：** 这种基于残差独立性的判断，非常符合我们识别“稳定变量”的需求。虽然CTL最初可能关注的是从训练到测试的单一迁移，但其核心原则可以推广到多个领域。\n    *   **取舍：** 采纳CTL的**残差独立性**作为判断机制不变性的标准，并将其与因果发现的连续优化框架结合。\n\n*   **研究方法选择：三步走的CICME框架。**\n    *   **为什么三步？** 这是核心设计。\n        *   **第一步：数据池化（Pooled Data Causal Discovery）。**\n            *   **思考：** 多个领域的数据是否完全无关？显然不是，它们描述的是同一套物理系统。那么，将所有数据汇集起来，能否捕捉到一些普适的、高置信度的因果关系？\n            *   **权衡：** 优势在于数据量大，能更鲁棒地学习到“共同”的结构。劣势是它会平均掉“个体”的差异，导致对某些领域特有的机制学习不准。\n            *   **取舍：** 这一步作为**初步识别共同机制**的起点。我们用**NOTEARS-MLP**（选择MLP是因为它可以处理非线性，NOTEARS提供可微分的DAG约束）在池化数据上学习一个全局的因果图。这个图会作为后续步骤的“初步假设”。\n        *   **第二步：稳定变量检测（Detection of Stable Variables）。**\n            *   **思考：** 如何利用CTL的原则来识别稳定变量？\n            *   **方法：** 对于第一步学到的每个变量`Xj`的机制`f_j_hat`，计算其残差`Rj = Xj - f_j_hat(PAj)`。然后进行一个**独立性检验**，看`Rj`是否与领域指示变量`D`（我们池化数据时加入的离散域标签）独立。\n            *   **独立性检验工具：** 我考虑了多种方法，最终选择了**HSIC (Hilbert-Schmidt Independence Criterion)**。\n                *   **权衡：** HSIC是非参数的，对数据的分布没有假设（如高斯分布），对非线性关系也有效，这与我们的MLP模型是匹配的。它的统计性质也相对成熟。\n                *   **取舍：** 使用HSIC来计算p值，设定一个显著性水平`α`。如果p值高于`α`，我们认为残差与领域独立，该机制是稳定的。这一步产出`Xstable`集合。\n        *   **第三步：个体因果机制估计（Causal Discovery on Individual Datasets）。**\n            *   **思考：** 在有了稳定变量信息后，如何指导每个领域的独立因果发现？\n            *   **方案一：冻结机制（CICME-f）。**\n                *   **思考：** 如果一个机制被认为是稳定的，那么它在每个领域中的参数就应该和第一步学到的全局参数一样。\n                *   **权衡：** 这种方法最直接，概念清晰，且计算上可能更高效（因为优化变量减少）。但它非常“硬性”，如果第一步的识别有误（HSIC判断不准），或者实际中存在微小的变化，这会导致错误。\n                *   **取舍：** 对于`Xj ∈ Xstable`，将其对应的MLP参数（权重）冻结为第一步学到的全局值，并在每个领域中进行训练时，不对这些参数进行梯度更新。\n            *   **方案二：损失函数惩罚（CICME-l）。**\n                *   **思考：** 能否有一种更“软性”的方式来强制一致性？\n                *   **权衡：** 在每个领域的损失函数中加入一个正则项，该正则项惩罚稳定变量的局部机制参数偏离全局参数的程度。这允许一定的灵活性，如果局部数据强烈表明需要微调，模型可以进行微调。但引入了新的超参数`γ`（惩罚权重），需要调优。\n                *   **取舍：** 对于`Xj ∈ Xstable`，在每个领域`k`的NOTEARS损失中加入`γ * MSE(W_pooled_j, W_individual_k_j)`项，其中`W`是邻接矩阵（可以从MLP参数派生）。\n\n*   **数据分析策略：**\n    *   **场景设计：** 针对真实制造场景，设计了四种具有代表性的领域偏移类型（泄漏面积变化、气流量分布变化等），确保实验能覆盖不同复杂度的机制变化。\n    *   **基线选择：** 选择两种极端情况作为基线——`NOTEARS-pool`（只做第一步）和`NOTEARS-ind`（只做第三步，且`Xstable`为空）。这能清晰地展示我们方法的优势，即同时利用了池化数据和个体领域数据的优点。\n    *   **评估指标：**\n        *   `SHD (Structural Hamming Distance)`：衡量估计图与真实图的差异，这是因果发现最标准的指标。\n        *   `Stable Count`和`LSHD`：专门评估第二步稳定变量识别的准确性。\n        *   `Execution Time`：考虑实际应用中的计算效率。\n\n在整个过程中，我不断地问自己：“这个选择是否能更好地解决问题？”“它会引入什么新的挑战？”“有没有更简单但同样有效的方法？”这种迭代的思考、权衡和取舍，最终形成了CICME这一三步走的框架。特别是两个优化方案（f和l），是我对“如何利用稳定机制信息”的不同策略探索。\n\n---\n\n### 3. 研究关键阶段的重要决策及背后逻辑\n\n**以文章中“制造泄漏测试”的例子（图1）进行说明：**\n\n*   **变量定义：**\n    *   X1：腔室温度\n    *   X2：气流量\n    *   X3：测试前压力（由X1和X2决定）\n    *   X4：测试后压力（由X3和泄漏情况H决定）\n    *   H：未观测的泄漏面积（领域特定变量）\n\n*   **因果模型（公式11）：**\n    *   `X1 = N1`\n    *   `X2 = N2`\n    *   `X3 = X1 + X2 + N3` （机制：`f3(X1, X2) = X1 + X2`）\n    *   `X4 = H × X3 + N4` （机制：`f4(X3) = H × X3`）\n\n**关键决策1：领域差异的设计（对应实验场景E1-E4）**\n\n*   **逻辑和依据：**\n    *   我希望模拟真实世界中，系统某些部分行为不变，某些部分行为随环境（领域）变化的场景。\n    *   `X3`的机制`f3(X1, X2)`在不同领域中是**不变的**（物理定律通常不变）。\n    *   `X4`的机制`f4(X3)`，我们通过让`H`（泄漏面积）在不同领域中取不同值来使其**变化**。这是核心设计，用于验证我们识别“变化”机制的能力。\n    *   `X2`的独立变量，通过改变其均值或使其固定，模拟上游输入的变化。这测试了HSIC在不同类型领域偏移下的表现。\n\n**关键决策2：第一步：池化数据（Algorithm 1）**\n\n*   **逻辑：** 尽管`X4`的机制会变，但`X3`的机制是共同的。如果分开训练，每个领域的数据量可能不足以准确识别`X1 -> X3`, `X2 -> X3`这些共同的边。将所有数据池化，可以利用更大的数据量，更稳定地识别像`X1 -> X3`, `X2 -> X3`这样的**共同机制**，以及`X3 -> X4`这样的**普遍存在但可能参数变化的机制**。\n*   **具体决策：** 将所有领域的数据`Dk`与一个领域标签`k`拼接，形成一个大的数据集`(X, D)`。然后在这个数据集上运行NOTEARS-MLP。\n*   **例子：** 在池化数据上，我们初步学习到了`X1 -> X3`, `X2 -> X3`, `X3 -> X4`这三条边及其对应的MLP函数（`θ1, θ2, θ3, θ4`）。\n\n**关键决策3：第二步：稳定变量检测（Algorithm 2）**\n\n*   **逻辑：** 仅凭第一步学到的全局模型不足以区分“共同”和“个体”机制。我们需要一个统计学上的判断标准。CTL的残差独立性原则提供了理论依据。\n*   **具体决策：**\n    1.  对每个变量`Xj`，使用第一步学习到的MLP函数`f_j_hat`计算残差`Rj = Xj - f_j_hat(PAj)`。\n    2.  进行HSIC检验，判断`Rj`是否与领域标签`D`独立。\n    3.  设定显著性水平`α`（例如0.01）。如果p值高于`α`，则将`Xj`标记为“稳定变量”（`Xj ∈ Xstable`）。\n*   **例子：**\n    *   对于`X3`：`R3 = X3 - (X1_hat + X2_hat)`。由于`X3`的真实机制`X1 + X2 + N3`是跨领域不变的，理论上`R3`应该与`D`独立。HSIC测试通常会给出高p值，将`X3`识别为稳定。\n    *   对于`X4`：`R4 = X4 - (H_hat * X3_hat)`。由于`H`在不同领域是变化的，`R4`将与`D`相关（因为`H`影响`X4`的方式与领域相关）。HSIC测试通常会给出低p值，将`X4`识别为不稳定。\n*   **权衡：** HSIC的准确性受样本量影响（表1的实验结果证实了这一点）。样本量少时，HSIC可能误判稳定变量，这会直接影响第三步的性能。这是我们方法的一个潜在脆弱点。\n\n**关键决策4：第三步：个体领域因果发现（Algorithm 3/4）**\n\n*   **逻辑：** 利用第二步识别出的稳定机制作为指导，在每个领域上重新进行因果发现，以捕捉领域特定的变化，同时保持共同机制的一致性。\n*   **具体决策：**\n    *   **CICME-f (Freeze):**\n        *   **决策：** 对于被识别为稳定的变量（如`X3`），其对应的MLP参数（`θ3`）在对每个领域`Dk`进行NOTEARS训练时被**冻结**为第一步学到的全局值。对于不稳定变量（如`X4`），其MLP参数则从随机初始化开始，在当前领域`Dk`上进行完整优化。\n        *   **背后逻辑：** 这种“硬性”约束确保了共同机制在所有领域中的一致性。计算上更高效，因为它减少了优化变量。\n        *   **例子：** 对于每个批次（领域），我们用`Dk`训练。当训练`X3`时，我们强制`f3(X1,X2)`的权重与池化时学到的完全一样。当训练`X4`时，我们允许`f4(X3)`的权重根据当前批次的数据独立调整，从而捕捉该批次特有的`H`值影响。\n    *   **CICME-l (Loss Penalty):**\n        *   **决策：** 在每个领域`Dk`的NOTEARS损失函数中，额外添加一个惩罚项。这个惩罚项衡量了当前领域中稳定变量的边权重（如`W_k_ind`）与池化数据中对应稳定变量的边权重（`W_pooled`）之间的`MSE`。\n        *   **背后逻辑：** 这是一个“软性”约束。它鼓励共同机制的一致性，但也允许模型在有足够证据时，为了更好地拟合当前领域数据而进行微小的调整。这提供了更大的灵活性，理论上可能带来更优的性能（如实验结果E1中所示）。\n        *   **例子：** 在每个批次`Dk`训练`X3`和`X4`时，`X3`的机制学习会受到一个惩罚，如果它偏离了池化时学到的`X1->X3`和`X2->X3`的权重。`X4`则不受此约束。\n\n**关键决策5：后处理（Post-processing）**\n\n*   **逻辑：** NOTEARS输出的是一个加权邻接矩阵`W`，需要将其转化为二元因果图（0或1表示是否有边）。\n*   **具体决策：** 采用阈值0.3进行二值化。这与NOTEARS原论文的实践保持一致。\n*   **权衡：** 阈值选择会影响最终SHD，但为了公平比较，所有方法采用相同阈值。\n\n通过这些关键决策，我们构建了一个能够系统地处理多领域因果发现问题的CICME框架。每个决策都基于对因果理论的理解、对现有方法的分析以及对真实数据特征的考量，并在实验中验证了其有效性和局限性。"
    },
    {
        "order": 9,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17795",
        "abs_url": "https://arxiv.org/abs/2507.17795",
        "pdf_url": "https://arxiv.org/pdf/2507.17795",
        "title": "LSDM: LLM-Enhanced Spatio-temporal Diffusion Model for Service-Level Mobile Traffic Prediction",
        "authors": [
            "Shiyuan Zhang",
            "Tong Li",
            "Zhu Xiao",
            "Hongyang Du",
            "Kaibin Huang"
        ],
        "comments": "14 pages, 9 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Service-level mobile traffic prediction for individual users is essential for network efficiency and quality of service enhancement. However, current prediction methods are limited in their adaptability across different urban environments and produce inaccurate results due to the high uncertainty in personal traffic patterns, the lack of detailed environmental context, and the complex dependencies among different network services. These challenges demand advanced modeling techniques that can capture dynamic traffic distributions and rich environmental features. Inspired by the recent success of diffusion models in distribution modeling and Large Language Models (LLMs) in contextual understanding, we propose an LLM-Enhanced Spatio-temporal Diffusion Model (LSDM). LSDM integrates the generative power of diffusion models with the adaptive learning capabilities of transformers, augmented by the ability to capture multimodal environmental information for modeling service-level patterns and dynamics. Extensive evaluations on real-world service-level datasets demonstrate that the model excels in traffic usage predictions, showing outstanding generalization and adaptability. After incorporating contextual information via LLM, the performance improves by at least 2.83% in terms of the coefficient of determination. Compared to models of a similar type, such as CSDI, the root mean squared error can be reduced by at least 8.29%. The code and dataset will be available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 10,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17796",
        "abs_url": "https://arxiv.org/abs/2507.17796",
        "pdf_url": "https://arxiv.org/pdf/2507.17796",
        "title": "CoCAI: Copula-based Conformal Anomaly Identification for Multivariate Time-Series",
        "authors": [
            "Nicholas A. Pearson",
            "Francesca Zanello",
            "Davide Russo",
            "Luca Bortolussi",
            "Francesca Cairoli"
        ],
        "comments": "Accepted for Presentation at Runtime Verification 25",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "We propose a novel framework that harnesses the power of generative artificial intelligence and copula-based modeling to address two critical challenges in multivariate time-series analysis: delivering accurate predictions and enabling robust anomaly detection. Our method, Copula-based Conformal Anomaly Identification for Multivariate Time-Series (CoCAI), leverages a diffusion-based model to capture complex dependencies within the data, enabling high quality forecasting. The model's outputs are further calibrated using a conformal prediction technique, yielding predictive regions which are statistically valid, i.e., cover the true target values with a desired confidence level. Starting from these calibrated forecasts, robust outlier detection is performed by combining dimensionality reduction techniques with copula-based modeling, providing a statistically grounded anomaly score. CoCAI benefits from an offline calibration phase that allows for minimal overhead during deployment and delivers actionable results rooted in established theoretical foundations. Empirical tests conducted on real operational data derived from water distribution and sewerage systems confirm CoCAI's effectiveness in accurately forecasting target sequences of data and in identifying anomalous segments within them.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 11,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17797",
        "abs_url": "https://arxiv.org/abs/2507.17797",
        "pdf_url": "https://arxiv.org/pdf/2507.17797",
        "title": "GenSelect: A Generative Approach to Best-of-N",
        "authors": [
            "Shubham Toshniwal",
            "Ivan Sorokin",
            "Aleksander Ficek",
            "Ivan Moshkov",
            "Igor Gitman"
        ],
        "comments": "Presented at the 2nd AI for MATH Workshop @ ICML",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
        "abstract": "Generative reward models with parallel sampling have enabled effective test-time scaling for reasoning tasks. Current approaches employ pointwise scoring of individual solutions or pairwise comparisons. However, pointwise methods underutilize LLMs' comparative abilities, while pairwise methods scale inefficiently with larger sampling budgets. We introduce GenSelect, where the LLM uses long reasoning to select the best solution among N candidates. This leverages LLMs' comparative strengths while scaling efficiently across parallel sampling budgets. For math reasoning, we demonstrate that reasoning models, such as QwQ and DeepSeek-R1-0528, excel at GenSelect, outperforming existing scoring approaches with simple prompting.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 12,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17798",
        "abs_url": "https://arxiv.org/abs/2507.17798",
        "pdf_url": "https://arxiv.org/pdf/2507.17798",
        "title": "Wasserstein GAN-Based Precipitation Downscaling with Optimal Transport for Enhancing Perceptual Realism",
        "authors": [
            "Kenta Shiraishi",
            "Yuka Muto",
            "Atsushi Okazaki",
            "Shunji Kotsuki"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "High-resolution (HR) precipitation prediction is essential for reducing damage from stationary and localized heavy rainfall; however, HR precipitation forecasts using process-driven numerical weather prediction models remains challenging. This study proposes using Wasserstein Generative Adversarial Network (WGAN) to perform precipitation downscaling with an optimal transport cost. In contrast to a conventional neural network trained with mean squared error, the WGAN generated visually realistic precipitation fields with fine-scale structures even though the WGAN exhibited slightly lower performance on conventional evaluation metrics. The learned critic of WGAN correlated well with human perceptual realism. Case-based analysis revealed that large discrepancies in critic scores can help identify both unrealistic WGAN outputs and potential artifacts in the reference data. These findings suggest that the WGAN framework not only improves perceptual realism in precipitation downscaling but also offers a new perspective for evaluating and quality-controlling precipitation datasets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 13,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17848",
        "abs_url": "https://arxiv.org/abs/2507.17848",
        "pdf_url": "https://arxiv.org/pdf/2507.17848",
        "title": "Explainable Graph Neural Networks via Structural Externalities",
        "authors": [
            "Lijun Wu",
            "Dong Hao",
            "Zhiyi Fan"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT); General Economics (econ.GN)",
        "abstract": "Graph Neural Networks (GNNs) have achieved outstanding performance across a wide range of graph-related tasks. However, their \"black-box\" nature poses significant challenges to their explainability, and existing methods often fail to effectively capture the intricate interaction patterns among nodes within the network. In this work, we propose a novel explainability framework, GraphEXT, which leverages cooperative game theory and the concept of social externalities. GraphEXT partitions graph nodes into coalitions, decomposing the original graph into independent subgraphs. By integrating graph structure as an externality and incorporating the Shapley value under externalities, GraphEXT quantifies node importance through their marginal contributions to GNN predictions as the nodes transition between coalitions. Unlike traditional Shapley value-based methods that primarily focus on node attributes, our GraphEXT places greater emphasis on the interactions among nodes and the impact of structural changes on GNN predictions. Experimental studies on both synthetic and real-world datasets show that GraphEXT outperforms existing baseline methods in terms of fidelity across diverse GNN architectures , significantly enhancing the explainability of GNN models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 14,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17876",
        "abs_url": "https://arxiv.org/abs/2507.17876",
        "pdf_url": "https://arxiv.org/pdf/2507.17876",
        "title": "Look the Other Way: Designing 'Positive' Molecules with Negative Data via Task Arithmetic",
        "authors": [
            "Rıza Özçelik",
            "Sarah de Ruiter",
            "Francesca Grisoni"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Biomolecules (q-bio.BM)",
        "abstract": "The scarcity of molecules with desirable properties (i.e., 'positive' molecules) is an inherent bottleneck for generative molecule design. To sidestep such obstacle, here we propose molecular task arithmetic: training a model on diverse and abundant negative examples to learn 'property directions' $--$ without accessing any positively labeled data $--$ and moving models in the opposite property directions to generate positive molecules. When analyzed on 20 zero-shot design experiments, molecular task arithmetic generated more diverse and successful designs than models trained on positive molecules. Moreover, we employed molecular task arithmetic in dual-objective and few-shot design tasks. We find that molecular task arithmetic can consistently increase the diversity of designs while maintaining desirable design properties. With its simplicity, data efficiency, and performance, molecular task arithmetic bears the potential to become the $\\textit{de-facto}$ transfer learning strategy for de novo molecule design.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 15,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17887",
        "abs_url": "https://arxiv.org/abs/2507.17887",
        "pdf_url": "https://arxiv.org/pdf/2507.17887",
        "title": "Fourier Neural Operators for Non-Markovian Processes:Approximation Theorems and Experiments",
        "authors": [
            "Wonjae Lee",
            "Taeyoung Kim",
            "Hyungbin Park"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Numerical Analysis (math.NA)",
        "abstract": "This paper introduces an operator-based neural network, the mirror-padded Fourier neural operator (MFNO), designed to learn the dynamics of stochastic systems. MFNO extends the standard Fourier neural operator (FNO) by incorporating mirror padding, enabling it to handle non-periodic inputs. We rigorously prove that MFNOs can approximate solutions of path-dependent stochastic differential equations and Lipschitz transformations of fractional Brownian motions to an arbitrary degree of accuracy. Our theoretical analysis builds on Wong--Zakai type theorems and various approximation techniques. Empirically, the MFNO exhibits strong resolution generalization--a property rarely seen in standard architectures such as LSTMs, TCNs, and DeepONet. Furthermore, our model achieves performance that is comparable or superior to these baselines while offering significantly faster sample path generation than classical numerical schemes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 16,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17895",
        "abs_url": "https://arxiv.org/abs/2507.17895",
        "pdf_url": "https://arxiv.org/pdf/2507.17895",
        "title": "Lower Bounds for Public-Private Learning under Distribution Shift",
        "authors": [
            "Amrith Setlur",
            "Pratiksha Thaker",
            "Jonathan Ullman"
        ],
        "comments": "Preprint",
        "subjects": "Machine Learning (cs.LG); Cryptography and Security (cs.CR)",
        "abstract": "The most effective differentially private machine learning algorithms in practice rely on an additional source of purportedly public data. This paradigm is most interesting when the two sources combine to be more than the sum of their parts. However, there are settings such as mean estimation where we have strong lower bounds, showing that when the two data sources have the same distribution, there is no complementary value to combining the two data sources. In this work we extend the known lower bounds for public-private learning to setting where the two data sources exhibit significant distribution shift. Our results apply to both Gaussian mean estimation where the two distributions have different means, and to Gaussian linear regression where the two distributions exhibit parameter shift. We find that when the shift is small (relative to the desired accuracy), either public or private data must be sufficiently abundant to estimate the private parameter. Conversely, when the shift is large, public data provides no benefit.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 17,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17903",
        "abs_url": "https://arxiv.org/abs/2507.17903",
        "pdf_url": "https://arxiv.org/pdf/2507.17903",
        "title": "Federated Learning for Large-Scale Cloud Robotic Manipulation: Opportunities and Challenges",
        "authors": [
            "Obaidullah Zaland",
            "Chanh Nguyen",
            "Florian T. Pokorny",
            "Monowar Bhuyan"
        ],
        "comments": "Accepted for Presentation at IEEE International Conference on Machine Learning and Cybernetics (ICMLC) 2025",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Federated Learning (FL) is an emerging distributed machine learning paradigm, where the collaborative training of a model involves dynamic participation of devices to achieve broad objectives. In contrast, classical machine learning (ML) typically requires data to be located on-premises for training, whereas FL leverages numerous user devices to train a shared global model without the need to share private data. Current robotic manipulation tasks are constrained by the individual capabilities and speed of robots due to limited low-latency computing resources. Consequently, the concept of cloud robotics has emerged, allowing robotic applications to harness the flexibility and reliability of computing resources, effectively alleviating their computational demands across the cloud-edge continuum. Undoubtedly, within this distributed computing context, as exemplified in cloud robotic manipulation scenarios, FL offers manifold advantages while also presenting several challenges and opportunities. In this paper, we present fundamental concepts of FL and their connection to cloud robotic manipulation. Additionally, we envision the opportunities and challenges associated with realizing efficient and reliable cloud robotic manipulation at scale through FL, where researchers adopt to design and verify FL models in either centralized or decentralized settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 18,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17907",
        "abs_url": "https://arxiv.org/abs/2507.17907",
        "pdf_url": "https://arxiv.org/pdf/2507.17907",
        "title": "Deep learning-aided inverse design of porous metamaterials",
        "authors": [
            "Phu Thien Nguyen",
            "Yousef Heider",
            "Dennis M. Kochmann",
            "Fadi Aldakheel"
        ],
        "comments": "31 pages, 29 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "The ultimate aim of the study is to explore the inverse design of porous metamaterials using a deep learning-based generative framework. Specifically, we develop a property-variational autoencoder (pVAE), a variational autoencoder (VAE) augmented with a regressor, to generate structured metamaterials with tailored hydraulic properties, such as porosity and permeability. While this work uses the lattice Boltzmann method (LBM) to generate intrinsic permeability tensor data for limited porous microstructures, a convolutional neural network (CNN) is trained using a bottom-up approach to predict effective hydraulic properties. This significantly reduces the computational cost compared to direct LBM simulations. The pVAE framework is trained on two datasets: a synthetic dataset of artificial porous microstructures and CT-scan images of volume elements from real open-cell foams. The encoder-decoder architecture of the VAE captures key microstructural features, mapping them into a compact and interpretable latent space for efficient structure-property exploration. The study provides a detailed analysis and interpretation of the latent space, demonstrating its role in structure-property mapping, interpolation, and inverse design. This approach facilitates the generation of new metamaterials with desired properties. The datasets and codes used in this study will be made open-access to support further research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 19,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17912",
        "abs_url": "https://arxiv.org/abs/2507.17912",
        "pdf_url": "https://arxiv.org/pdf/2507.17912",
        "title": "SETOL: A Semi-Empirical Theory of (Deep) Learning",
        "authors": [
            "Charles H Martin",
            "Christopher Hinrichs"
        ],
        "comments": "139 pages, 28 figures. Code for experiments available at this https URL",
        "subjects": "Machine Learning (cs.LG); Statistical Mechanics (cond-mat.stat-mech)",
        "abstract": "We present a SemiEmpirical Theory of Learning (SETOL) that explains the remarkable performance of State-Of-The-Art (SOTA) Neural Networks (NNs). We provide a formal explanation of the origin of the fundamental quantities in the phenomenological theory of Heavy-Tailed Self-Regularization (HTSR): the heavy-tailed power-law layer quality metrics, alpha and alpha-hat. In prior work, these metrics have been shown to predict trends in the test accuracies of pretrained SOTA NN models, importantly, without needing access to either testing or training data. Our SETOL uses techniques from statistical mechanics as well as advanced methods from random matrix theory and quantum chemistry. The derivation suggests new mathematical preconditions for ideal learning, including a new metric, ERG, which is equivalent to applying a single step of the Wilson Exact Renormalization Group. We test the assumptions and predictions of SETOL on a simple 3-layer multilayer perceptron (MLP), demonstrating excellent agreement with the key theoretical assumptions. For SOTA NN models, we show how to estimate the individual layer qualities of a trained NN by simply computing the empirical spectral density (ESD) of the layer weight matrices and plugging this ESD into our SETOL formulas. Notably, we examine the performance of the HTSR alpha and the SETOL ERG layer quality metrics, and find that they align remarkably well, both on our MLP and on SOTA NNs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 20,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17922",
        "abs_url": "https://arxiv.org/abs/2507.17922",
        "pdf_url": "https://arxiv.org/pdf/2507.17922",
        "title": "From Seed to Harvest: Augmenting Human Creativity with AI for Red-teaming Text-to-Image Models",
        "authors": [
            "Jessica Quaye",
            "Charvi Rastogi",
            "Alicia Parrish",
            "Oana Inel",
            "Minsuk Kahng",
            "Lora Aroyo",
            "Vijay Janapa Reddi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Text-to-image (T2I) models have become prevalent across numerous applications, making their robust evaluation against adversarial attacks a critical priority. Continuous access to new and challenging adversarial prompts across diverse domains is essential for stress-testing these models for resilience against novel attacks from multiple vectors. Current techniques for generating such prompts are either entirely authored by humans or synthetically generated. On the one hand, datasets of human-crafted adversarial prompts are often too small in size and imbalanced in their cultural and contextual representation. On the other hand, datasets of synthetically-generated prompts achieve scale, but typically lack the realistic nuances and creative adversarial strategies found in human-crafted prompts. To combine the strengths of both human and machine approaches, we propose Seed2Harvest, a hybrid red-teaming method for guided expansion of culturally diverse, human-crafted adversarial prompt seeds. The resulting prompts preserve the characteristics and attack patterns of human prompts while maintaining comparable average attack success rates (0.31 NudeNet, 0.36 SD NSFW, 0.12 Q16). Our expanded dataset achieves substantially higher diversity with 535 unique geographic locations and a Shannon entropy of 7.48, compared to 58 locations and 5.28 entropy in the original dataset. Our work demonstrates the importance of human-machine collaboration in leveraging human creativity and machine computational capacity to achieve comprehensive, scalable red-teaming for continuous T2I model safety evaluation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 21,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17924",
        "abs_url": "https://arxiv.org/abs/2507.17924",
        "pdf_url": "https://arxiv.org/pdf/2507.17924",
        "title": "UrbanPulse: A Cross-City Deep Learning Framework for Ultra-Fine-Grained Population Transfer Prediction",
        "authors": [
            "Hongrong Yang",
            "Markus Schlaepfer"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Accurate population flow prediction is essential for urban planning, transportation management, and public health. Yet existing methods face key limitations: traditional models rely on static spatial assumptions, deep learning models struggle with cross-city generalization, and Large Language Models (LLMs) incur high computational costs while failing to capture spatial structure. Moreover, many approaches sacrifice resolution by clustering Points of Interest (POIs) or restricting coverage to subregions, limiting their utility for city-wide analytics. We introduce UrbanPulse, a scalable deep learning framework that delivers ultra-fine-grained, city-wide OD flow predictions by treating each POI as an individual node. It combines a temporal graph convolutional encoder with a transformer-based decoder to model multi-scale spatiotemporal dependencies. To ensure robust generalization across urban contexts, UrbanPulse employs a three-stage transfer learning strategy: pretraining on large-scale urban graphs, cold-start adaptation, and reinforcement learning this http URL on over 103 million cleaned GPS records from three metropolitan areas in California, UrbanPulse achieves state-of-the-art accuracy and scalability. Through efficient transfer learning, UrbanPulse takes a key step toward making high-resolution, AI-powered urban forecasting deployable in practice across diverse cities.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 22,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17934",
        "abs_url": "https://arxiv.org/abs/2507.17934",
        "pdf_url": "https://arxiv.org/pdf/2507.17934",
        "title": "Multimodal Fine-grained Reasoning for Post Quality Evaluation",
        "authors": [
            "Xiaoxu Guo",
            "Siyan Liang",
            "Yachao Cui",
            "Juxiang Zhou",
            "Lei Wang",
            "Han Cao"
        ],
        "comments": "48 pages",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Accurately assessing post quality requires complex relational reasoning to capture nuanced topic-post relationships. However, existing studies face three major limitations: (1) treating the task as unimodal categorization, which fails to leverage multimodal cues and fine-grained quality distinctions; (2) introducing noise during deep multimodal fusion, leading to misleading signals; and (3) lacking the ability to capture complex semantic relationships like relevance and comprehensiveness. To address these issues, we propose the Multimodal Fine-grained Topic-post Relational Reasoning (MFTRR) framework, which mimics human cognitive processes. MFTRR reframes post-quality assessment as a ranking task and incorporates multimodal data to better capture quality variations. It consists of two key modules: (1) the Local-Global Semantic Correlation Reasoning Module, which models fine-grained semantic interactions between posts and topics at both local and global levels, enhanced by a maximum information fusion mechanism to suppress noise; and (2) the Multi-Level Evidential Relational Reasoning Module, which explores macro- and micro-level relational cues to strengthen evidence-based reasoning. We evaluate MFTRR on three newly constructed multimodal topic-post datasets and the public Lazada-Home dataset. Experimental results demonstrate that MFTRR significantly outperforms state-of-the-art baselines, achieving up to 9.52% NDCG@3 improvement over the best unimodal method on the Art History dataset.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 23,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17953",
        "abs_url": "https://arxiv.org/abs/2507.17953",
        "pdf_url": "https://arxiv.org/pdf/2507.17953",
        "title": "Clo-HDnn: A 4.66 TFLOPS/W and 3.78 TOPS/W Continual On-Device Learning Accelerator with Energy-efficient Hyperdimensional Computing via Progressive Search",
        "authors": [
            "Chang Eun Song",
            "Weihong Xu",
            "Keming Fan",
            "Soumil Jain",
            "Gopabandhu Hota",
            "Haichao Yang",
            "Leo Liu",
            "Kerem Akarvardar",
            "Meng-Fan Chang",
            "Carlos H. Diaz",
            "Gert Cauwenberghs",
            "Tajana Rosing",
            "Mingu Kang"
        ],
        "comments": "Published in 2025 Symposium on VLSI Technology and Circuits (VLSI Technology and Circuits), Kyoto, Japan, 2025",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Clo-HDnn is an on-device learning (ODL) accelerator designed for emerging continual learning (CL) tasks. Clo-HDnn integrates hyperdimensional computing (HDC) along with low-cost Kronecker HD Encoder and weight clustering feature extraction (WCFE) to optimize accuracy and efficiency. Clo-HDnn adopts gradient-free CL to efficiently update and store the learned knowledge in the form of class hypervectors. Its dual-mode operation enables bypassing costly feature extraction for simpler datasets, while progressive search reduces complexity by up to 61% by encoding and comparing only partial query hypervectors. Achieving 4.66 TFLOPS/W (FE) and 3.78 TOPS/W (classifier), Clo-HDnn delivers 7.77x and 4.85x higher energy efficiency compared to SOTA ODL accelerators.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 24,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17958",
        "abs_url": "https://arxiv.org/abs/2507.17958",
        "pdf_url": "https://arxiv.org/pdf/2507.17958",
        "title": "VIBE: Video-Input Brain Encoder for fMRI Response Modeling",
        "authors": [
            "Daniel Carlstrom Schad",
            "Shrey Dixit",
            "Janis Keck",
            "Viktor Studenyak",
            "Aleksandr Shpilevoi",
            "Andrej Bicanski"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We present VIBE, a two-stage Transformer that fuses multi-modal video, audio, and text features to predict fMRI activity. Representations from open-source models (Qwen2.5, BEATs, Whisper, SlowFast, V-JEPA) are merged by a modality-fusion transformer and temporally decoded by a prediction transformer with rotary embeddings. Trained on 65 hours of movie data from the CNeuroMod dataset and ensembled across 20 seeds, VIBE attains mean parcel-wise Pearson correlations of 32.25 on in-distribution Friends S07 and 21.25 on six out-of-distribution films. An earlier iteration of the same architecture obtained 0.3198 and 0.2096, respectively, winning Phase-1 and placing second overall in the Algonauts 2025 Challenge.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 25,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17977",
        "abs_url": "https://arxiv.org/abs/2507.17977",
        "pdf_url": "https://arxiv.org/pdf/2507.17977",
        "title": "Improving the Computational Efficiency and Explainability of GeoAggregator",
        "authors": [
            "Rui Deng",
            "Ziqi Li",
            "Mingshu Wang"
        ],
        "comments": "4 pages, 3 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Accurate modeling and explaining geospatial tabular data (GTD) are critical for understanding geospatial phenomena and their underlying processes. Recent work has proposed a novel transformer-based deep learning model named GeoAggregator (GA) for this purpose, and has demonstrated that it outperforms other statistical and machine learning approaches. In this short paper, we further improve GA by 1) developing an optimized pipeline that accelerates the dataloading process and streamlines the forward pass of GA to achieve better computational efficiency; and 2) incorporating a model ensembling strategy and a post-hoc model explanation function based on the GeoShapley framework to enhance model explainability. We validate the functionality and efficiency of the proposed strategies by applying the improved GA model to synthetic datasets. Experimental results show that our implementation improves the prediction accuracy and inference speed of GA compared to the original implementation. Moreover, explanation experiments indicate that GA can effectively captures the inherent spatial effects in the designed synthetic dataset. The complete pipeline has been made publicly available for community use (this https URL).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 26,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17979",
        "abs_url": "https://arxiv.org/abs/2507.17979",
        "pdf_url": "https://arxiv.org/pdf/2507.17979",
        "title": "SIFOTL: A Principled, Statistically-Informed Fidelity-Optimization Method for Tabular Learning",
        "authors": [
            "Shubham Mohole",
            "Sainyam Galhotra"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Identifying the factors driving data shifts in tabular datasets is a significant challenge for analysis and decision support systems, especially those focusing on healthcare. Privacy rules restrict data access, and noise from complex processes hinders analysis. To address this challenge, we propose SIFOTL (Statistically-Informed Fidelity-Optimization Method for Tabular Learning) that (i) extracts privacy-compliant data summary statistics, (ii) employs twin XGBoost models to disentangle intervention signals from noise with assistance from LLMs, and (iii) merges XGBoost outputs via a Pareto-weighted decision tree to identify interpretable segments responsible for the shift. Unlike existing analyses which may ignore noise or require full data access for LLM-based analysis, SIFOTL addresses both challenges using only privacy-safe summary statistics. Demonstrating its real-world efficacy, for a MEPS panel dataset mimicking a new Medicare drug subsidy, SIFOTL achieves an F1 score of 0.85, substantially outperforming BigQuery Contribution Analysis (F1=0.46) and statistical tests (F1=0.20) in identifying the segment receiving the subsidy. Furthermore, across 18 diverse EHR datasets generated based on Synthea ABM, SIFOTL sustains F1 scores of 0.86-0.96 without noise and >= 0.75 even with injected observational noise, whereas baseline average F1 scores range from 0.19-0.67 under the same tests. SIFOTL, therefore, provides an interpretable, privacy-conscious workflow that is empirically robust to observational noise.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 27,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17984",
        "abs_url": "https://arxiv.org/abs/2507.17984",
        "pdf_url": "https://arxiv.org/pdf/2507.17984",
        "title": "Machine Unlearning of Traffic State Estimation and Prediction",
        "authors": [
            "Xin Wang",
            "R. Tyrrell Rockafellar",
            "Xuegang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Data-driven traffic state estimation and prediction (TSEP) relies heavily on data sources that contain sensitive information. While the abundance of data has fueled significant breakthroughs, particularly in machine learning-based methods, it also raises concerns regarding privacy, cybersecurity, and data freshness. These issues can erode public trust in intelligent transportation systems. Recently, regulations have introduced the \"right to be forgotten\", allowing users to request the removal of their private data from models. As machine learning models can remember old data, simply removing it from back-end databases is insufficient in such systems. To address these challenges, this study introduces a novel learning paradigm for TSEP-Machine Unlearning TSEP-which enables a trained TSEP model to selectively forget privacy-sensitive, poisoned, or outdated data. By empowering models to \"unlearn,\" we aim to enhance the trustworthiness and reliability of data-driven traffic TSEP.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 28,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18014",
        "abs_url": "https://arxiv.org/abs/2507.18014",
        "pdf_url": "https://arxiv.org/pdf/2507.18014",
        "title": "Predictive Scaling Laws for Efficient GRPO Training of Large Reasoning Models",
        "authors": [
            "Datta Nimmaturi",
            "Vaishnavi Bhargava",
            "Rajat Ghosh",
            "Johnu George",
            "Debojyoti Dutta"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Fine-tuning large language models (LLMs) for reasoning tasks using reinforcement learning methods like Group Relative Policy Optimization (GRPO) is computationally expensive. To address this, we propose a predictive framework that models training dynamics and helps optimize resource usage. Through experiments on Llama and Qwen models (3B 8B), we derive an empirical scaling law based on model size, initial performance, and training progress. This law predicts reward trajectories and identifies three consistent training phases: slow start, rapid improvement, and plateau. We find that training beyond certain number of an epoch offers little gain, suggesting earlier stopping can significantly reduce compute without sacrificing performance. Our approach generalizes across model types, providing a practical guide for efficient GRPO-based fine-tuning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 29,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18067",
        "abs_url": "https://arxiv.org/abs/2507.18067",
        "pdf_url": "https://arxiv.org/pdf/2507.18067",
        "title": "Multiscale Neural PDE Surrogates for Prediction and Downscaling: Application to Ocean Currents",
        "authors": [
            "Abdessamad El-Kabid",
            "Loubna Benabbou",
            "Redouane Lguensat",
            "Alex Hernández-García"
        ],
        "comments": "Workshop @ ICML2025",
        "subjects": "Machine Learning (cs.LG); Computational Engineering, Finance, and Science (cs.CE)",
        "abstract": "Accurate modeling of physical systems governed by partial differential equations is a central challenge in scientific computing. In oceanography, high-resolution current data are critical for coastal management, environmental monitoring, and maritime safety. However, available satellite products, such as Copernicus data for sea water velocity at ~0.08 degrees spatial resolution and global ocean models, often lack the spatial granularity required for detailed local analyses. In this work, we (a) introduce a supervised deep learning framework based on neural operators for solving PDEs and providing arbitrary resolution solutions, and (b) propose downscaling models with an application to Copernicus ocean current data. Additionally, our method can model surrogate PDEs and predict solutions at arbitrary resolution, regardless of the input resolution. We evaluated our model on real-world Copernicus ocean current data and synthetic Navier-Stokes simulation datasets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 30,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18071",
        "abs_url": "https://arxiv.org/abs/2507.18071",
        "pdf_url": "https://arxiv.org/pdf/2507.18071",
        "title": "Group Sequence Policy Optimization",
        "authors": [
            "Chujie Zheng",
            "Shixuan Liu",
            "Mingze Li",
            "Xiong-Hui Chen",
            "Bowen Yu",
            "Chang Gao",
            "Kai Dang",
            "Yuqiong Liu",
            "Rui Men",
            "An Yang",
            "Jingren Zhou",
            "Junyang Lin"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "This paper introduces Group Sequence Policy Optimization (GSPO), our stable, efficient, and performant reinforcement learning algorithm for training large language models. Unlike previous algorithms that adopt token-level importance ratios, GSPO defines the importance ratio based on sequence likelihood and performs sequence-level clipping, rewarding, and optimization. We demonstrate that GSPO achieves superior training efficiency and performance compared to the GRPO algorithm, notably stabilizes Mixture-of-Experts (MoE) RL training, and has the potential for simplifying the design of RL infrastructure. These merits of GSPO have contributed to the remarkable improvements in the latest Qwen3 models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 31,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18072",
        "abs_url": "https://arxiv.org/abs/2507.18072",
        "pdf_url": "https://arxiv.org/pdf/2507.18072",
        "title": "C-AAE: Compressively Anonymizing Autoencoders for Privacy-Preserving Activity Recognition in Healthcare Sensor Streams",
        "authors": [
            "Ryusei Fujimoto",
            "Yugo Nakamura",
            "Yutaka Arakawa"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Wearable accelerometers and gyroscopes encode fine-grained behavioural signatures that can be exploited to re-identify users, making privacy protection essential for healthcare applications. We introduce C-AAE, a compressive anonymizing autoencoder that marries an Anonymizing AutoEncoder (AAE) with Adaptive Differential Pulse-Code Modulation (ADPCM). The AAE first projects raw sensor windows into a latent space that retains activity-relevant features while suppressing identity cues. ADPCM then differentially encodes this latent stream, further masking residual identity information and shrinking the bitrate. Experiments on the MotionSense and PAMAP2 datasets show that C-AAE cuts user re-identification F1 scores by 10-15 percentage points relative to AAE alone, while keeping activity-recognition F1 within 5 percentage points of the unprotected baseline. ADPCM also reduces data volume by roughly 75 %, easing transmission and storage overheads. These results demonstrate that C-AAE offers a practical route to balancing privacy and utility in continuous, sensor-based activity recognition for healthcare.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 32,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18073",
        "abs_url": "https://arxiv.org/abs/2507.18073",
        "pdf_url": "https://arxiv.org/pdf/2507.18073",
        "title": "Squeeze10-LLM: Squeezing LLMs' Weights by 10 Times via a Staged Mixed-Precision Quantization Method",
        "authors": [
            "Qingcheng Zhu",
            "Yangyang Ren",
            "Linlin Yang",
            "Mingbao Lin",
            "Yanjing Li",
            "Sheng Xu",
            "Zichao Feng",
            "Haodong Zhu",
            "Yuguang Yang",
            "Juan Zhang",
            "Runqi Wang",
            "Baochang Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Deploying large language models (LLMs) is challenging due to their massive parameters and high computational costs. Ultra low-bit quantization can significantly reduce storage and accelerate inference, but extreme compression (i.e., mean bit-width <= 2) often leads to severe performance degradation. To address this, we propose Squeeze10-LLM, effectively \"squeezing\" 16-bit LLMs' weights by 10 times. Specifically, Squeeze10-LLM is a staged mixed-precision post-training quantization (PTQ) framework and achieves an average of 1.6 bits per weight by quantizing 80% of the weights to 1 bit and 20% to 4 bits. We introduce Squeeze10LLM with two key innovations: Post-Binarization Activation Robustness (PBAR) and Full Information Activation Supervision (FIAS). PBAR is a refined weight significance metric that accounts for the impact of quantization on activations, improving accuracy in low-bit settings. FIAS is a strategy that preserves full activation information during quantization to mitigate cumulative error propagation across layers. Experiments on LLaMA and LLaMA2 show that Squeeze10-LLM achieves state-of-the-art performance for sub-2bit weight-only quantization, improving average accuracy from 43% to 56% on six zero-shot classification tasks--a significant boost over existing PTQ methods. Our code will be released upon publication.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 33,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18098",
        "abs_url": "https://arxiv.org/abs/2507.18098",
        "pdf_url": "https://arxiv.org/pdf/2507.18098",
        "title": "Learning from Hard Labels with Additional Supervision on Non-Hard-Labeled Classes",
        "authors": [
            "Kosuke Sugiyama",
            "Masato Uchida"
        ],
        "comments": "32 pages, 11 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In scenarios where training data is limited due to observation costs or data scarcity, enriching the label information associated with each instance becomes crucial for building high-accuracy classification models. In such contexts, it is often feasible to obtain not only hard labels but also {\\it additional supervision}, such as the confidences for the hard labels. This setting naturally raises fundamental questions: {\\it What kinds of additional supervision are intrinsically beneficial?} And {\\it how do they contribute to improved generalization performance?} To address these questions, we propose a theoretical framework that treats both hard labels and additional supervision as probability distributions, and constructs soft labels through their affine combination. Our theoretical analysis reveals that the essential component of additional supervision is not the confidence score of the assigned hard label, but rather the information of the distribution over the non-hard-labeled classes. Moreover, we demonstrate that the additional supervision and the mixing coefficient contribute to the refinement of soft labels in complementary roles. Intuitively, in the probability simplex, the additional supervision determines the direction in which the deterministic distribution representing the hard label should be adjusted toward the true label distribution, while the mixing coefficient controls the step size along that direction. Through generalization error analysis, we theoretically characterize how the additional supervision and its mixing coefficient affect both the convergence rate and asymptotic value of the error bound. Finally, we experimentally demonstrate that, based on our theory, designing additional supervision can lead to improved classification accuracy, even when utilized in a simple manner.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 34,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18111",
        "abs_url": "https://arxiv.org/abs/2507.18111",
        "pdf_url": "https://arxiv.org/pdf/2507.18111",
        "title": "Percentile-Based Deep Reinforcement Learning and Reward Based Personalization For Delay Aware RAN Slicing in O-RAN",
        "authors": [
            "Peyman Tehrani",
            "Anas Alsoliman"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In this paper, we tackle the challenge of radio access network (RAN) slicing within an open RAN (O-RAN) architecture. Our focus centers on a network that includes multiple mobile virtual network operators (MVNOs) competing for physical resource blocks (PRBs) with the goal of meeting probabilistic delay upper bound constraints for their clients while minimizing PRB utilization. Initially, we derive a reward function based on the law of large numbers (LLN), then implement practical modifications to adapt it for real-world experimental scenarios. We then propose our solution, the Percentile-based Delay-Aware Deep Reinforcement Learning (PDA-DRL), which demonstrates its superiority over several baselines, including DRL models optimized for average delay constraints, by achieving a 38\\% reduction in resultant average delay. Furthermore, we delve into the issue of model weight sharing among multiple MVNOs to develop a robust personalized model. We introduce a reward-based personalization method where each agent prioritizes other agents' model weights based on their performance. This technique surpasses traditional aggregation methods, such as federated averaging, and strategies reliant on traffic patterns and model weight distance similarities.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 35,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18113",
        "abs_url": "https://arxiv.org/abs/2507.18113",
        "pdf_url": "https://arxiv.org/pdf/2507.18113",
        "title": "Policy Disruption in Reinforcement Learning:Adversarial Attack with Large Language Models and Critical State Identification",
        "authors": [
            "Junyong Jiang",
            "Buwei Tian",
            "Chenxing Xu",
            "Songze Li",
            "Lu Dong"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Reinforcement learning (RL) has achieved remarkable success in fields like robotics and autonomous driving, but adversarial attacks designed to mislead RL systems remain challenging. Existing approaches often rely on modifying the environment or policy, limiting their practicality. This paper proposes an adversarial attack method in which existing agents in the environment guide the target policy to output suboptimal actions without altering the environment. We propose a reward iteration optimization framework that leverages large language models (LLMs) to generate adversarial rewards explicitly tailored to the vulnerabilities of the target agent, thereby enhancing the effectiveness of inducing the target agent toward suboptimal decision-making. Additionally, a critical state identification algorithm is designed to pinpoint the target agent's most vulnerable states, where suboptimal behavior from the victim leads to significant degradation in overall performance. Experimental results in diverse environments demonstrate the superiority of our method over existing approaches.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 36,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18122",
        "abs_url": "https://arxiv.org/abs/2507.18122",
        "pdf_url": "https://arxiv.org/pdf/2507.18122",
        "title": "Maximizing Prefix-Confidence at Test-Time Efficiently Improves Mathematical Reasoning",
        "authors": [
            "Matthias Otth",
            "Jonas Hübotter",
            "Ido Hakimi",
            "Andreas Krause"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Recent work has shown that language models can self-improve by maximizing their own confidence in their predictions, without relying on external verifiers or reward signals. In this work, we study the test-time scaling of language models for mathematical reasoning tasks, where the model's own confidence is used to select the most promising attempts. Surprisingly, we find that we can achieve significant performance gains by continuing only the most promising attempt, selected by the model's prefix-confidence. We systematically evaluate prefix-confidence scaling on five mathematical reasoning datasets: the school-level GSM8K and MATH500, and the competition-level AMC23, AIME24, and AIME25. We find that prefix-confidence scaling with prefixes of only 32 tokens achieves a better accuracy-compute trade-off than majority voting. Moreover, prefix-confidence scaling appears less susceptible than BoN to length biases. Finally, we also evaluate test-time training with prefix-confidence and find that, while outperforming the base model, it does not improve over prefix-confidence scaling.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 37,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18139",
        "abs_url": "https://arxiv.org/abs/2507.18139",
        "pdf_url": "https://arxiv.org/pdf/2507.18139",
        "title": "Neuromorphic Computing for Embodied Intelligence in Autonomous Systems: Current Trends, Challenges, and Future Directions",
        "authors": [
            "Alberto Marchisio",
            "Muhammad Shafique"
        ],
        "comments": "To appear at the 31st IEEE International Symposium on On-Line Testing and Robust System Design (IOLTS), Ischia, Italy, July 2025",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The growing need for intelligent, adaptive, and energy-efficient autonomous systems across fields such as robotics, mobile agents (e.g., UAVs), and self-driving vehicles is driving interest in neuromorphic computing. By drawing inspiration from biological neural systems, neuromorphic approaches offer promising pathways to enhance the perception, decision-making, and responsiveness of autonomous platforms. This paper surveys recent progress in neuromorphic algorithms, specialized hardware, and cross-layer optimization strategies, with a focus on their deployment in real-world autonomous scenarios. Special attention is given to event-based dynamic vision sensors and their role in enabling fast, efficient perception. The discussion highlights new methods that improve energy efficiency, robustness, adaptability, and reliability through the integration of spiking neural networks into autonomous system architectures. We integrate perspectives from machine learning, robotics, neuroscience, and neuromorphic engineering to offer a comprehensive view of the state of the field. Finally, emerging trends and open challenges are explored, particularly in the areas of real-time decision-making, continual learning, and the development of secure, resilient autonomous systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 38,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18153",
        "abs_url": "https://arxiv.org/abs/2507.18153",
        "pdf_url": "https://arxiv.org/pdf/2507.18153",
        "title": "When Noisy Labels Meet Class Imbalance on Graphs: A Graph Augmentation Method with LLM and Pseudo Label",
        "authors": [
            "Riting Xia",
            "Rucong Wang",
            "Yulin Liu",
            "Anchen Li",
            "Xueyan Liu",
            "Yan Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Class-imbalanced graph node classification is a practical yet underexplored research problem. Although recent studies have attempted to address this issue, they typically assume clean and reliable labels when processing class-imbalanced graphs. This assumption often violates the nature of real-world graphs, where labels frequently contain noise. Given this gap, this paper systematically investigates robust node classification for class-imbalanced graphs with noisy labels. We propose GraphALP, a novel Graph Augmentation framework based on Large language models (LLMs) and Pseudo-labeling techniques. Specifically, we design an LLM-based oversampling method to generate synthetic minority nodes, producing label-accurate minority nodes to alleviate class imbalance. Based on the class-balanced graphs, we develop a dynamically weighted pseudo-labeling method to obtain high-confidence pseudo labels to reduce label noise ratio. Additionally, we implement a secondary LLM-guided oversampling mechanism to mitigate potential class distribution skew caused by pseudo labels. Experimental results show that GraphALP achieves superior performance over state-of-the-art methods on class-imbalanced graphs with noisy labels.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 39,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18183",
        "abs_url": "https://arxiv.org/abs/2507.18183",
        "pdf_url": "https://arxiv.org/pdf/2507.18183",
        "title": "ChronoSelect: Robust Learning with Noisy Labels via Dynamics Temporal Memory",
        "authors": [
            "Jianchao Wang",
            "Qingfeng Li",
            "Pengcheng Zheng",
            "Xiaorong Pu",
            "Yazhou Ren"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Training deep neural networks on real-world datasets is often hampered by the presence of noisy labels, which can be memorized by over-parameterized models, leading to significant degradation in generalization performance. While existing methods for learning with noisy labels (LNL) have made considerable progress, they fundamentally suffer from static snapshot evaluations and fail to leverage the rich temporal dynamics of learning evolution. In this paper, we propose ChronoSelect (chrono denoting its temporal nature), a novel framework featuring an innovative four-stage memory architecture that compresses prediction history into compact temporal distributions. Our unique sliding update mechanism with controlled decay maintains only four dynamic memory units per sample, progressively emphasizing recent patterns while retaining essential historical knowledge. This enables precise three-way sample partitioning into clean, boundary, and noisy subsets through temporal trajectory analysis and dual-branch consistency. Theoretical guarantees prove the mechanism's convergence and stability under noisy conditions. Extensive experiments demonstrate ChronoSelect's state-of-the-art performance across synthetic and real-world benchmarks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 40,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18196",
        "abs_url": "https://arxiv.org/abs/2507.18196",
        "pdf_url": "https://arxiv.org/pdf/2507.18196",
        "title": "Goal-based Trajectory Prediction for improved Cross-Dataset Generalization",
        "authors": [
            "Daniel Grimm",
            "Ahmed Abouelazm",
            "J. Marius Zöllner"
        ],
        "comments": "Accepted on IEEE ITSC 2025",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "To achieve full autonomous driving, a good understanding of the surrounding environment is necessary. Especially predicting the future states of other traffic participants imposes a non-trivial challenge. Current SotA-models already show promising results when trained on real datasets (e.g. Argoverse2, NuScenes). Problems arise when these models are deployed to new/unseen areas. Typically, performance drops significantly, indicating that the models lack generalization. In this work, we introduce a new Graph Neural Network (GNN) that utilizes a heterogeneous graph consisting of traffic participants and vectorized road network. Latter, is used to classify goals, i.e. endpoints of the predicted trajectories, in a multi-staged approach, leading to a better generalization to unseen scenarios. We show the effectiveness of the goal selection process via cross-dataset evaluation, i.e. training on Argoverse2 and evaluating on NuScenes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 41,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18219",
        "abs_url": "https://arxiv.org/abs/2507.18219",
        "pdf_url": "https://arxiv.org/pdf/2507.18219",
        "title": "FedSA-GCL: A Semi-Asynchronous Federated Graph Learning Framework with Personalized Aggregation and Cluster-Aware Broadcasting",
        "authors": [
            "Zhongzheng Yuan",
            "Lianshuai Guo",
            "Xunkai Li",
            "Yinlin Zhu",
            "Wenyu Wang",
            "Meixia Qu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Federated Graph Learning (FGL) is a distributed learning paradigm that enables collaborative training over large-scale subgraphs located on multiple local systems. However, most existing FGL approaches rely on synchronous communication, which leads to inefficiencies and is often impractical in real-world deployments. Meanwhile, current asynchronous federated learning (AFL) methods are primarily designed for conventional tasks such as image classification and natural language processing, without accounting for the unique topological properties of graph data. Directly applying these methods to graph learning can possibly result in semantic drift and representational inconsistency in the global model. To address these challenges, we propose FedSA-GCL, a semi-asynchronous federated framework that leverages both inter-client label distribution divergence and graph topological characteristics through a novel ClusterCast mechanism for efficient training. We evaluate FedSA-GCL on multiple real-world graph datasets using the Louvain and Metis split algorithms, and compare it against 9 baselines. Extensive experiments demonstrate that our method achieves strong robustness and outstanding efficiency, outperforming the baselines by an average of 2.92% with the Louvain and by 3.4% with the Metis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 42,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18220",
        "abs_url": "https://arxiv.org/abs/2507.18220",
        "pdf_url": "https://arxiv.org/pdf/2507.18220",
        "title": "Sparse identification of nonlinear dynamics with library optimization mechanism: Recursive long-term prediction perspective",
        "authors": [
            "Ansei Yonezawa",
            "Heisei Yonezawa",
            "Shuichi Yahagi",
            "Itsuro Kajiwara",
            "Shinya Kijimoto",
            "Hikaru Taniuchi",
            "Kentaro Murakami"
        ],
        "comments": "This work has been submitted to the IEEE for possible publication",
        "subjects": "Machine Learning (cs.LG); Dynamical Systems (math.DS)",
        "abstract": "The sparse identification of nonlinear dynamics (SINDy) approach can discover the governing equations of dynamical systems based on measurement data, where the dynamical model is identified as the sparse linear combination of the given basis functions. A major challenge in SINDy is the design of a library, which is a set of candidate basis functions, as the appropriate library is not trivial for many dynamical systems. To overcome this difficulty, this study proposes SINDy with library optimization mechanism (SINDy-LOM), which is a combination of the sparse regression technique and the novel learning strategy of the library. In the proposed approach, the basis functions are parametrized. The SINDy-LOM approach involves a two-layer optimization architecture: the inner-layer, in which the data-driven model is extracted as the sparse linear combination of the candidate basis functions, and the outer-layer, in which the basis functions are optimized from the viewpoint of the recursive long-term (RLT) prediction accuracy; thus, the library design is reformulated as the optimization of the parametrized basis functions. The resulting SINDy-LOM model has good interpretability and usability, as the proposed approach yields the parsimonious model. The library optimization mechanism significantly reduces user burden. The RLT perspective improves the reliability of the resulting model compared with the traditional SINDy approach that can only ensure the one-step-ahead prediction accuracy. The validity of the proposed approach is demonstrated by applying it to a diesel engine airpath system, which is a well-known complex industrial system.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 43,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18242",
        "abs_url": "https://arxiv.org/abs/2507.18242",
        "pdf_url": "https://arxiv.org/pdf/2507.18242",
        "title": "Boosting Revisited: Benchmarking and Advancing LP-Based Ensemble Methods",
        "authors": [
            "Fabian Akkerman",
            "Julien Ferry",
            "Christian Artigues",
            "Emmanuel Hebrard",
            "Thibaut Vidal"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Despite their theoretical appeal, totally corrective boosting methods based on linear programming have received limited empirical attention. In this paper, we conduct the first large-scale experimental study of six LP-based boosting formulations, including two novel methods, NM-Boost and QRLP-Boost, across 20 diverse datasets. We evaluate the use of both heuristic and optimal base learners within these formulations, and analyze not only accuracy, but also ensemble sparsity, margin distribution, anytime performance, and hyperparameter sensitivity. We show that totally corrective methods can outperform or match state-of-the-art heuristics like XGBoost and LightGBM when using shallow trees, while producing significantly sparser ensembles. We further show that these methods can thin pre-trained ensembles without sacrificing performance, and we highlight both the strengths and limitations of using optimal decision trees in this context.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 44,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18293",
        "abs_url": "https://arxiv.org/abs/2507.18293",
        "pdf_url": "https://arxiv.org/pdf/2507.18293",
        "title": "Leveraging Data Augmentation and Siamese Learning for Predictive Process Monitoring",
        "authors": [
            "Sjoerd van Straten",
            "Alessandro Padella",
            "Marwan Hassani"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Predictive Process Monitoring (PPM) enables forecasting future events or outcomes of ongoing business process instances based on event logs. However, deep learning PPM approaches are often limited by the low variability and small size of real-world event logs. To address this, we introduce SiamSA-PPM, a novel self-supervised learning framework that combines Siamese learning with Statistical Augmentation for Predictive Process Monitoring. It employs three novel statistically grounded transformation methods that leverage control-flow semantics and frequent behavioral patterns to generate realistic, semantically valid new trace variants. These augmented views are used within a Siamese learning setup to learn generalizable representations of process prefixes without the need for labeled supervision. Extensive experiments on real-life event logs demonstrate that SiamSA-PPM achieves competitive or superior performance compared to the SOTA in both next activity and final outcome prediction tasks. Our results further show that statistical augmentation significantly outperforms random transformations and improves variability in the data, highlighting SiamSA-PPM as a promising direction for training data enrichment in process prediction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 45,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18297",
        "abs_url": "https://arxiv.org/abs/2507.18297",
        "pdf_url": "https://arxiv.org/pdf/2507.18297",
        "title": "Self-Supervised Coarsening of Unstructured Grid with Automatic Differentiation",
        "authors": [
            "Sergei Shumilin",
            "Alexander Ryabov",
            "Nikolay Yavich",
            "Evgeny Burnaev",
            "Vladimir Vanovskiy"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Due to the high computational load of modern numerical simulation, there is a demand for approaches that would reduce the size of discrete problems while keeping the accuracy reasonable. In this work, we present an original algorithm to coarsen an unstructured grid based on the concepts of differentiable physics. We achieve this by employing k-means clustering, autodifferentiation and stochastic minimization algorithms. We demonstrate performance of the designed algorithm on two PDEs: a linear parabolic equation which governs slightly compressible fluid flow in porous media and the wave equation. Our results show that in the considered scenarios, we reduced the number of grid points up to 10 times while preserving the modeled variable dynamics in the points of interest. The proposed approach can be applied to the simulation of an arbitrary system described by evolutionary partial differential equations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 46,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18313",
        "abs_url": "https://arxiv.org/abs/2507.18313",
        "pdf_url": "https://arxiv.org/pdf/2507.18313",
        "title": "Regression-aware Continual Learning for Android Malware Detection",
        "authors": [
            "Daniele Ghiani",
            "Daniele Angioni",
            "Giorgio Piras",
            "Angelo Sotgiu",
            "Luca Minnei",
            "Srishti Gupta",
            "Maura Pintor",
            "Fabio Roli",
            "Battista Biggio"
        ],
        "comments": "Submitted to IEEE Transactions on Information Forensics and Security",
        "subjects": "Machine Learning (cs.LG); Cryptography and Security (cs.CR)",
        "abstract": "Malware evolves rapidly, forcing machine learning (ML)-based detectors to adapt continuously. With antivirus vendors processing hundreds of thousands of new samples daily, datasets can grow to billions of examples, making full retraining impractical. Continual learning (CL) has emerged as a scalable alternative, enabling incremental updates without full data access while mitigating catastrophic forgetting. In this work, we analyze a critical yet overlooked issue in this context: security regression. Unlike forgetting, which manifests as a general performance drop on previously seen data, security regression captures harmful prediction changes at the sample level, such as a malware sample that was once correctly detected but evades detection after a model update. Although often overlooked, regressions pose serious risks in security-critical applications, as the silent reintroduction of previously detected threats in the system may undermine users' trust in the whole updating process. To address this issue, we formalize and quantify security regression in CL-based malware detectors and propose a regression-aware penalty to mitigate it. Specifically, we adapt Positive Congruent Training (PCT) to the CL setting, preserving prior predictive behavior in a model-agnostic manner. Experiments on the ELSA, Tesseract, and AZ-Class datasets show that our method effectively reduces regression across different CL scenarios while maintaining strong detection performance over time.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 47,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18320",
        "abs_url": "https://arxiv.org/abs/2507.18320",
        "pdf_url": "https://arxiv.org/pdf/2507.18320",
        "title": "State of Health Estimation of Batteries Using a Time-Informed Dynamic Sequence-Inverted Transformer",
        "authors": [
            "Janak M. Patel",
            "Milad Ramezankhani",
            "Anirudh Deodhar",
            "Dagnachew Birru"
        ],
        "comments": "11 pages, 3 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The rapid adoption of battery-powered vehicles and energy storage systems over the past decade has made battery health monitoring increasingly critical. Batteries play a central role in the efficiency and safety of these systems, yet they inevitably degrade over time due to repeated charge-discharge cycles. This degradation leads to reduced energy efficiency and potential overheating, posing significant safety concerns. Accurate estimation of a State of Health (SoH) of battery is therefore essential for ensuring operational reliability and safety. Several machine learning architectures, such as LSTMs, transformers, and encoder-based models, have been proposed to estimate SoH from discharge cycle data. However, these models struggle with the irregularities inherent in real-world measurements: discharge readings are often recorded at non-uniform intervals, and the lengths of discharge cycles vary significantly. To address this, most existing approaches extract features from the sequences rather than processing them in full, which introduces information loss and compromises accuracy. To overcome these challenges, we propose a novel architecture: Time-Informed Dynamic Sequence Inverted Transformer (TIDSIT). TIDSIT incorporates continuous time embeddings to effectively represent irregularly sampled data and utilizes padded sequences with temporal attention mechanisms to manage variable-length inputs without discarding sequence information. Experimental results on the NASA battery degradation dataset show that TIDSIT significantly outperforms existing models, achieving over 50% reduction in prediction error and maintaining an SoH prediction error below 0.58%. Furthermore, the architecture is generalizable and holds promise for broader applications in health monitoring tasks involving irregular time-series data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 48,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18333",
        "abs_url": "https://arxiv.org/abs/2507.18333",
        "pdf_url": "https://arxiv.org/pdf/2507.18333",
        "title": "Remembering the Markov Property in Cooperative MARL",
        "authors": [
            "Kale-ab Abebe Tessera",
            "Leonard Hinckeldey",
            "Riccardo Zamboni",
            "David Abel",
            "Amos Storkey"
        ],
        "comments": "RLC Finding the Frame Workshop Camera-Ready, 8 pages",
        "subjects": "Machine Learning (cs.LG); Multiagent Systems (cs.MA)",
        "abstract": "Cooperative multi-agent reinforcement learning (MARL) is typically formalised as a Decentralised Partially Observable Markov Decision Process (Dec-POMDP), where agents must reason about the environment and other agents' behaviour. In practice, current model-free MARL algorithms use simple recurrent function approximators to address the challenge of reasoning about others using partial information. In this position paper, we argue that the empirical success of these methods is not due to effective Markov signal recovery, but rather to learning simple conventions that bypass environment observations and memory. Through a targeted case study, we show that co-adapting agents can learn brittle conventions, which then fail when partnered with non-adaptive agents. Crucially, the same models can learn grounded policies when the task design necessitates it, revealing that the issue is not a fundamental limitation of the learning models but a failure of the benchmark design. Our analysis also suggests that modern MARL environments may not adequately test the core assumptions of Dec-POMDPs. We therefore advocate for new cooperative environments built upon two core principles: (1) behaviours grounded in observations and (2) memory-based reasoning about other agents, ensuring success requires genuine skill rather than fragile, co-adapted agreements.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 49,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18346",
        "abs_url": "https://arxiv.org/abs/2507.18346",
        "pdf_url": "https://arxiv.org/pdf/2507.18346",
        "title": "Low-rank adaptive physics-informed HyperDeepONets for solving differential equations",
        "authors": [
            "Etienne Zeudong",
            "Elsa Cardoso-Bihlo",
            "Alex Bihlo"
        ],
        "comments": "14 pages, 6 figures, 5 tables",
        "subjects": "Machine Learning (cs.LG); Numerical Analysis (math.NA)",
        "abstract": "HyperDeepONets were introduced in Lee, Cho and Hwang [ICLR, 2023] as an alternative architecture for operator learning, in which a hypernetwork generates the weights for the trunk net of a DeepONet. While this improves expressivity, it incurs high memory and computational costs due to the large number of output parameters required. In this work we introduce, in the physics-informed machine learning setting, a variation, PI-LoRA-HyperDeepONets, which leverage low-rank adaptation (LoRA) to reduce complexity by decomposing the hypernetwork's output layer weight matrix into two smaller low-rank matrices. This reduces the number of trainable parameters while introducing an extra regularization of the trunk networks' weights. Through extensive experiments on both ordinary and partial differential equations we show that PI-LoRA-HyperDeepONets achieve up to 70\\% reduction in parameters and consistently outperform regular HyperDeepONets in terms of predictive accuracy and generalization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 50,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18366",
        "abs_url": "https://arxiv.org/abs/2507.18366",
        "pdf_url": "https://arxiv.org/pdf/2507.18366",
        "title": "Efficient Uncertainty in LLMs through Evidential Knowledge Distillation",
        "authors": [
            "Lakshmana Sri Harsha Nemani",
            "P.K. Srijith",
            "Tomasz Kuśmierczyk"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Accurate uncertainty quantification remains a key challenge for standard LLMs, prompting the adoption of Bayesian and ensemble-based methods. However, such methods typically necessitate computationally expensive sampling, involving multiple forward passes to effectively estimate predictive uncertainty. In this paper, we introduce a novel approach enabling efficient and effective uncertainty estimation in LLMs without sacrificing performance. Specifically, we distill uncertainty-aware teacher models - originally requiring multiple forward passes - into compact student models sharing the same architecture but fine-tuned using Low-Rank Adaptation (LoRA). We compare two distinct distillation strategies: one in which the student employs traditional softmax-based outputs, and another in which the student leverages Dirichlet-distributed outputs to explicitly model epistemic uncertainty via evidential learning. Empirical evaluations on classification datasets demonstrate that such students can achieve comparable or superior predictive and uncertainty quantification performance relative to their teacher models, while critically requiring only a single forward pass. To our knowledge, this is the first demonstration that immediate and robust uncertainty quantification can be achieved in LLMs through evidential distillation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 51,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18376",
        "abs_url": "https://arxiv.org/abs/2507.18376",
        "pdf_url": "https://arxiv.org/pdf/2507.18376",
        "title": "A Comprehensive Review of Diffusion Models in Smart Agriculture: Progress, Applications, and Challenges",
        "authors": [
            "Xing Hua",
            "Haodong Chen",
            "Qianqian Duan",
            "Danfeng Hong",
            "Ruijiao Li",
            "Huiliang Shang",
            "Linghua Jiang",
            "Haima Yang",
            "Dawei Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "With the global population growing and arable land resources becoming increasingly scarce,smart agriculture and precision agriculture have emerged as key directions for the future ofagricultural this http URL intelligence (AI) technologies, particularly deep learning models, have found widespread applications in areas such as crop monitoring and pest detection. As an emerging generative model, diffusion models have shown significant promise in tasks like agricultural image processing, data augmentation, and remote sensing. Compared to traditional generative adversarial networks (GANs), diffusion models offer superior training stability and generation quality, effectively addressing challenges such as limited agricultural data and imbalanced image samples. This paper reviews the latest advancements in the application of diffusion models in agriculture, focusing on their potential in crop pest and disease detection, remote sensing image enhancement, crop growth prediction, and agricultural resource management. Experimental results demonstrate that diffusion models significantly improve model accuracy and robustness in data augmentation, image generation, and denoising, especially in complex environments. Despite challenges related to computational efficiency and generalization capabilities, diffusion models are expected to play an increasingly important role in smart and precision agriculture as technology advances, providing substantial support for the sustainable development of global agriculture.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 52,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18423",
        "abs_url": "https://arxiv.org/abs/2507.18423",
        "pdf_url": "https://arxiv.org/pdf/2507.18423",
        "title": "Multi-Model Ensemble and Reservoir Computing for River Discharge Prediction in Ungauged Basins",
        "authors": [
            "Mizuki Funato",
            "Yohei Sawada"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Geophysics (physics.geo-ph)",
        "abstract": "Despite the critical need for accurate flood prediction and water management, many regions lack sufficient river discharge observations, limiting the skill of rainfall-runoff analyses. Although numerous physically based and machine learning models exist, achieving high accuracy, interpretability, and computational efficiency under data-scarce conditions remains a major challenge. We address this challenge with a novel method, HYdrological Prediction with multi-model Ensemble and Reservoir computing (HYPER) that leverages multi-model ensemble and reservoir computing (RC). Our approach first applies Bayesian model averaging (BMA) to 43 \"uncalibrated\" catchment-based conceptual hydrological models. An RC model is then trained via linear regression to correct errors in the BMA output, a non-iterative process that ensures high computational efficiency. For ungauged basins, we infer the required BMA and RC weights by linking them to catchment attributes from gauged basins, creating a generalizable framework. We evaluated HYPER using data from 87 river basins in Japan. In a data-rich scenario, HYPER (median Kling-Gupta Efficiency, KGE, of 0.56) performed comparably to a benchmark LSTM (KGE 0.55) but required only 5% of its computational time. In a data-scarce scenario (23% of basins gauged), HYPER maintained robust performance (KGE 0.55) and lower uncertainty, whereas the LSTM's performance degraded significantly (KGE -0.04). These results reveal that individual conceptual hydrological models do not necessarily need to be calibrated when an effectively large ensemble is assembled and combined with machine-learning-based bias correction. HYPER provides a robust, efficient, and generalizable solution for discharge prediction, particularly in ungauged basins, making it applicable to a wide range of regions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 53,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18519",
        "abs_url": "https://arxiv.org/abs/2507.18519",
        "pdf_url": "https://arxiv.org/pdf/2507.18519",
        "title": "Revisiting Bisimulation Metric for Robust Representations in Reinforcement Learning",
        "authors": [
            "Leiji Zhang",
            "Zeyu Wang",
            "Xin Li",
            "Yao-Hui Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Bisimulation metric has long been regarded as an effective control-related representation learning technique in various reinforcement learning tasks. However, in this paper, we identify two main issues with the conventional bisimulation metric: 1) an inability to represent certain distinctive scenarios, and 2) a reliance on predefined weights for differences in rewards and subsequent states during recursive updates. We find that the first issue arises from an imprecise definition of the reward gap, whereas the second issue stems from overlooking the varying importance of reward difference and next-state distinctions across different training stages and task settings. To address these issues, by introducing a measure for state-action pairs, we propose a revised bisimulation metric that features a more precise definition of reward gap and novel update operators with adaptive coefficient. We also offer theoretical guarantees of convergence for our proposed metric and its improved representation distinctiveness. In addition to our rigorous theoretical analysis, we conduct extensive experiments on two representative benchmarks, DeepMind Control and Meta-World, demonstrating the effectiveness of our approach.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 54,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18521",
        "abs_url": "https://arxiv.org/abs/2507.18521",
        "pdf_url": "https://arxiv.org/pdf/2507.18521",
        "title": "GLANCE: Graph Logic Attention Network with Cluster Enhancement for Heterophilous Graph Representation Learning",
        "authors": [
            "Zhongtian Sun",
            "Anoushka Harit",
            "Alexandra Cristea",
            "Christl A. Donnelly",
            "Pietro Liò"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Graph Neural Networks (GNNs) have demonstrated significant success in learning from graph-structured data but often struggle on heterophilous graphs, where connected nodes differ in features or class labels. This limitation arises from indiscriminate neighbor aggregation and insufficient incorporation of higher-order structural patterns. To address these challenges, we propose GLANCE (Graph Logic Attention Network with Cluster Enhancement), a novel framework that integrates logic-guided reasoning, dynamic graph refinement, and adaptive clustering to enhance graph representation learning. GLANCE combines a logic layer for interpretable and structured embeddings, multi-head attention-based edge pruning for denoising graph structures, and clustering mechanisms for capturing global patterns. Experimental results in benchmark datasets, including Cornell, Texas, and Wisconsin, demonstrate that GLANCE achieves competitive performance, offering robust and interpretable solutions for heterophilous graph scenarios. The proposed framework is lightweight, adaptable, and uniquely suited to the challenges of heterophilous graphs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 55,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18533",
        "abs_url": "https://arxiv.org/abs/2507.18533",
        "pdf_url": "https://arxiv.org/pdf/2507.18533",
        "title": "C2G-KD: PCA-Constrained Generator for Data-Free Knowledge Distillation",
        "authors": [
            "Magnus Bengtsson",
            "Kenneth Östberg"
        ],
        "comments": "12 pages",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "We introduce C2G-KD, a data-free knowledge distillation framework where a class-conditional generator is trained to produce synthetic samples guided by a frozen teacher model and geometric constraints derived from PCA. The generator never observes real training data but instead learns to activate the teacher's output through a combination of semantic and structural losses. By constraining generated samples to lie within class-specific PCA subspaces estimated from as few as two real examples per class, we preserve topological consistency and diversity. Experiments on MNIST show that even minimal class structure is sufficient to bootstrap useful synthetic training pipelines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 56,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18549",
        "abs_url": "https://arxiv.org/abs/2507.18549",
        "pdf_url": "https://arxiv.org/pdf/2507.18549",
        "title": "The Price equation reveals a universal force-metric-bias law of algorithmic learning and natural selection",
        "authors": [
            "Steven A. Frank"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Populations and Evolution (q-bio.PE)",
        "abstract": "Diverse learning algorithms, optimization methods, and natural selection share a common mathematical structure, despite their apparent differences. Here I show that a simple notational partitioning of change by the Price equation reveals a universal force-metric-bias (FMB) law: $\\Delta\\mathbf{\\theta} = \\mathbf{M}\\,\\mathbf{f} + \\mathbf{b} + \\mathbf{\\xi}$. The force $\\mathbf{f}$ drives improvement in parameters, $\\Delta\\mathbf{\\theta}$, through the covariance between the parameters and performance. The metric $\\mathbf{M}$ rescales movement by inverse curvature. The bias $\\mathbf{b}$ adds momentum or changes in the frame of reference. The noise $\\mathbf{\\xi}$ enables exploration. This framework unifies natural selection, Bayesian updating, Newton's method, stochastic gradient descent, stochastic Langevin dynamics, Adam optimization, and most other algorithms as special cases of the same underlying process. The Price equation also reveals why Fisher information, Kullback-Leibler divergence, and d'Alembert's principle arise naturally in learning dynamics. By exposing this common structure, the FMB law provides a principled foundation for understanding, comparing, and designing learning algorithms across disciplines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 57,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18553",
        "abs_url": "https://arxiv.org/abs/2507.18553",
        "pdf_url": "https://arxiv.org/pdf/2507.18553",
        "title": "The Geometry of LLM Quantization: GPTQ as Babai's Nearest Plane Algorithm",
        "authors": [
            "Jiale Chen",
            "Torsten Hoefler",
            "Dan Alistarh"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Quantizing the weights of large language models (LLMs) from 16-bit to lower bitwidth is the de facto approach to deploy massive transformers onto more affordable accelerators. GPTQ emerged as one of the standard methods for one-shot post-training quantization at LLM scale. Yet, its inner workings are described as a sequence of ad-hoc algebraic updates that obscure any geometric meaning or worst-case guarantees. In this work, we show that, when executed back-to-front (from the last to first dimension) for a linear layer, GPTQ is mathematically identical to Babai's nearest plane algorithm for the classical closest vector problem (CVP) on a lattice defined by the Hessian matrix of the layer's inputs. This equivalence is based on a sophisticated mathematical argument, and has two analytical consequences: (i) the GPTQ error propagation step gains an intuitive geometric interpretation; (ii) GPTQ inherits the error upper bound of Babai's algorithm under the no-clipping condition. Taken together, these results place GPTQ on firm theoretical footing and open the door to importing decades of progress in lattice algorithms towards the design of future quantization algorithms for billion-parameter models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 58,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18555",
        "abs_url": "https://arxiv.org/abs/2507.18555",
        "pdf_url": "https://arxiv.org/pdf/2507.18555",
        "title": "Neural Tangent Kernels and Fisher Information Matrices for Simple ReLU Networks with Random Hidden Weights",
        "authors": [
            "Jun'ichi Takeuchia",
            "Yoshinari Takeishia",
            "Noboru Muratab",
            "Kazushi Mimurac",
            "Ka Long Keith Hod",
            "Hiroshi Nagaoka"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Fisher information matrices and neural tangent kernels (NTK) for 2-layer ReLU networks with random hidden weight are argued. We discuss the relation between both notions as a linear transformation and show that spectral decomposition of NTK with concrete forms of eigenfunctions with major eigenvalues. We also obtain an approximation formula of the functions presented by the 2-layer neural networks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 59,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18561",
        "abs_url": "https://arxiv.org/abs/2507.18561",
        "pdf_url": "https://arxiv.org/pdf/2507.18561",
        "title": "Beyond Internal Data: Constructing Complete Datasets for Fairness Testing",
        "authors": [
            "Varsha Ramineni",
            "Hossein A. Rahmani",
            "Emine Yilmaz",
            "David Barber"
        ],
        "comments": "9 pages, 6 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "As AI becomes prevalent in high-risk domains and decision-making, it is essential to test for potential harms and biases. This urgency is reflected by the global emergence of AI regulations that emphasise fairness and adequate testing, with some mandating independent bias audits. However, procuring the necessary data for fairness testing remains a significant challenge. Particularly in industry settings, legal and privacy concerns restrict the collection of demographic data required to assess group disparities, and auditors face practical and cultural challenges in gaining access to data. Further, internal historical datasets are often insufficiently representative to identify real-world biases. This work focuses on evaluating classifier fairness when complete datasets including demographics are inaccessible. We propose leveraging separate overlapping datasets to construct complete synthetic data that includes demographic information and accurately reflects the underlying relationships between protected attributes and model features. We validate the fidelity of the synthetic data by comparing it to real data, and empirically demonstrate that fairness metrics derived from testing on such synthetic data are consistent with those obtained from real data. This work, therefore, offers a path to overcome real-world data scarcity for fairness testing, enabling independent, model-agnostic evaluation of fairness, and serving as a viable substitute where real data is limited.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 60,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18597",
        "abs_url": "https://arxiv.org/abs/2507.18597",
        "pdf_url": "https://arxiv.org/pdf/2507.18597",
        "title": "Linear Memory SE(2) Invariant Attention",
        "authors": [
            "Ethan Pronovost",
            "Neha Boloor",
            "Peter Schleede",
            "Noureldin Hendy",
            "Andres Morales",
            "Nicholas Roy"
        ],
        "comments": "Best paper award, Equivariant Systems Workshop at RSS",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Processing spatial data is a key component in many learning tasks for autonomous driving such as motion forecasting, multi-agent simulation, and planning. Prior works have demonstrated the value in using SE(2) invariant network architectures that consider only the relative poses between objects (e.g. other agents, scene features such as traffic lanes). However, these methods compute the relative poses for all pairs of objects explicitly, requiring quadratic memory. In this work, we propose a mechanism for SE(2) invariant scaled dot-product attention that requires linear memory relative to the number of objects in the scene. Our SE(2) invariant transformer architecture enjoys the same scaling properties that have benefited large language models in recent years. We demonstrate experimentally that our approach is practical to implement and improves performance compared to comparable non-invariant architectures.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 61,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18603",
        "abs_url": "https://arxiv.org/abs/2507.18603",
        "pdf_url": "https://arxiv.org/pdf/2507.18603",
        "title": "Demystify Protein Generation with Hierarchical Conditional Diffusion Models",
        "authors": [
            "Zinan Ling",
            "Yi Shi",
            "Da Yan",
            "Yang Zhou",
            "Bo Hui"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Generating novel and functional protein sequences is critical to a wide range of applications in biology. Recent advancements in conditional diffusion models have shown impressive empirical performance in protein generation tasks. However, reliable generations of protein remain an open research question in de novo protein design, especially when it comes to conditional diffusion models. Considering the biological function of a protein is determined by multi-level structures, we propose a novel multi-level conditional diffusion model that integrates both sequence-based and structure-based information for efficient end-to-end protein design guided by specified functions. By generating representations at different levels simultaneously, our framework can effectively model the inherent hierarchical relations between different levels, resulting in an informative and discriminative representation of the generated protein. We also propose a Protein-MMD, a new reliable evaluation metric, to evaluate the quality of generated protein with conditional diffusion models. Our new metric is able to capture both distributional and functional similarities between real and generated protein sequences while ensuring conditional consistency. We experiment with the benchmark datasets, and the results on conditional protein generation tasks demonstrate the efficacy of the proposed generation framework and evaluation metric.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 62,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18623",
        "abs_url": "https://arxiv.org/abs/2507.18623",
        "pdf_url": "https://arxiv.org/pdf/2507.18623",
        "title": "Moving Out: Physically-grounded Human-AI Collaboration",
        "authors": [
            "Xuhui Kang",
            "Sung-Wook Lee",
            "Haolin Liu",
            "Yuyan Wang",
            "Yen-Ling Kuo"
        ],
        "comments": "24 pages, 8 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)",
        "abstract": "The ability to adapt to physical actions and constraints in an environment is crucial for embodied agents (e.g., robots) to effectively collaborate with humans. Such physically grounded human-AI collaboration must account for the increased complexity of the continuous state-action space and constrained dynamics caused by physical constraints. In this paper, we introduce \\textit{Moving Out}, a new human-AI collaboration benchmark that resembles a wide range of collaboration modes affected by physical attributes and constraints, such as moving heavy items together and maintaining consistent actions to move a big item around a corner. Using Moving Out, we designed two tasks and collected human-human interaction data to evaluate models' abilities to adapt to diverse human behaviors and unseen physical attributes. To address the challenges in physical environments, we propose a novel method, BASS (Behavior Augmentation, Simulation, and Selection), to enhance the diversity of agents and their understanding of the outcome of actions. Our experiments show that BASS outperforms state-of-the-art models in AI-AI and human-AI collaboration. The project page is available at \\href{this https URL}{this https URL\\_ai/}.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 63,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18627",
        "abs_url": "https://arxiv.org/abs/2507.18627",
        "pdf_url": "https://arxiv.org/pdf/2507.18627",
        "title": "Gait Recognition Based on Tiny ML and IMU Sensors",
        "authors": [
            "Jiahang Zhang",
            "Mingtong Chen",
            "Zhengbao Yang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Systems and Control (eess.SY)",
        "abstract": "This project presents the development of a gait recognition system using Tiny Machine Learning (Tiny ML) and Inertial Measurement Unit (IMU) sensors. The system leverages the XIAO-nRF52840 Sense microcontroller and the LSM6DS3 IMU sensor to capture motion data, including acceleration and angular velocity, from four distinct activities: walking, stationary, going upstairs, and going downstairs. The data collected is processed through Edge Impulse, an edge AI platform, which enables the training of machine learning models that can be deployed directly onto the microcontroller for real-time activity this http URL data preprocessing step involves extracting relevant features from the raw sensor data using techniques such as sliding windows and data normalization, followed by training a Deep Neural Network (DNN) classifier for activity recognition. The model achieves over 80% accuracy on a test dataset, demonstrating its ability to classify the four activities effectively. Additionally, the platform enables anomaly detection, further enhancing the robustness of the system. The integration of Tiny ML ensures low-power operation, making it suitable for battery-powered or energy-harvesting devices.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 64,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2504.02853",
        "abs_url": "https://arxiv.org/abs/2504.02853",
        "pdf_url": "https://arxiv.org/pdf/2504.02853",
        "title": "Mapping Technological Futures: Anticipatory Discourse Through Text Mining",
        "authors": [
            "Maciej Skorski",
            "Alina Landowska",
            "Krzysztof Rajda"
        ],
        "comments": "Accepted to Humanities and Social Sciences Communications. arXiv admin note: text overlap with arXiv:2407.17522",
        "subjects": "Social and Information Networks (cs.SI); Computation and Language (cs.CL); Computers and Society (cs.CY); Machine Learning (cs.LG)",
        "abstract": "The volatility and unpredictability of emerging technologies, such as artificial intelligence (AI), generate significant uncertainty, which is widely discussed on social media. This study examines anticipatory discourse surrounding technological futures by analysing 1.5 million posts from 400 key opinion leaders (KOLs) published on the X platform (from 2021 to 2023). Using advanced text mining techniques, including BERTopic modelling, sentiment, emotion, and attitude analyses, the research identifies 100 distinct topics reflecting anticipated tech-driven futures. Our findings emphasize the dual role of KOLs in framing \\textit{present futures} -- optimistic visions of transformative technologies like AI and IoT -- and influencing \\textit{future presents}, where these projections shape contemporary societal and geopolitical debates. Positive emotions such as Hope dominate, outweighing Anxiety, particularly in topics like ``Machine Learning, Data Science, and Deep Learning,'' while discussions around ``Climate Change'' and ``War, Ukraine, and Trump People'' elicit \\textit{Anxiety}. By framing technologies as solutions to societal challenges, KOLs act as mediators of societal narratives, bridging imagined futures and current realities. These insights underscore their pivotal role in directing public attention with emerging technologies during periods of heightened uncertainty, advancing our understanding of anticipatory discourse in technology-mediated contexts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 65,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.16641",
        "abs_url": "https://arxiv.org/abs/2507.16641",
        "pdf_url": "https://arxiv.org/pdf/2507.16641",
        "title": "Hybrid Reward-Driven Reinforcement Learning for Efficient Quantum Circuit Synthesis",
        "authors": [
            "Sara Giordano",
            "Kornikar Sen",
            "Miguel A. Martin-Delgado"
        ],
        "comments": "13 pages, 4 figures, color figures",
        "subjects": "Quantum Physics (quant-ph); Machine Learning (cs.LG)",
        "abstract": "A reinforcement learning (RL) framework is introduced for the efficient synthesis of quantum circuits that generate specified target quantum states from a fixed initial state, addressing a central challenge in both the NISQ era and future fault-tolerant quantum computing. The approach utilizes tabular Q-learning, based on action sequences, within a discretized quantum state space, to effectively manage the exponential growth of the space dimension. The framework introduces a hybrid reward mechanism, combining a static, domain-informed reward that guides the agent toward the target state with customizable dynamic penalties that discourage inefficient circuit structures such as gate congestion and redundant state revisits. By leveraging sparse matrix representations and state-space discretization, the method enables scalable navigation of high-dimensional environments while minimizing computational overhead. Benchmarking on graph-state preparation tasks for up to seven qubits, we demonstrate that the algorithm consistently discovers minimal-depth circuits with optimized gate counts. Moreover, extending the framework to a universal gate set for arbitrary quantum states, it still produces minimal depth circuits, highlighting the algorithm's robustness and adaptability. The results confirm that this RL-driven approach efficiently explores the complex quantum state space and synthesizes near-optimal quantum circuits, providing a resource-efficient foundation for quantum circuit optimization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 66,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17757",
        "abs_url": "https://arxiv.org/abs/2507.17757",
        "pdf_url": "https://arxiv.org/pdf/2507.17757",
        "title": "BrisT1D Dataset: Young Adults with Type 1 Diabetes in the UK using Smartwatches",
        "authors": [
            "Sam Gordon James",
            "Miranda Elaine Glynis Armstrong",
            "Aisling Ann O'Kane",
            "Harry Emerson",
            "Zahraa S. Abdallah"
        ],
        "comments": "13 pages, 14 figures",
        "subjects": "Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)",
        "abstract": "Background: Type 1 diabetes (T1D) has seen a rapid evolution in management technology and forms a useful case study for the future management of other chronic conditions. Further development of this management technology requires an exploration of its real-world use and the potential of additional data streams. To facilitate this, we contribute the BrisT1D Dataset to the growing number of public T1D management datasets. The dataset was developed from a longitudinal study of 24 young adults in the UK who used a smartwatch alongside their usual T1D management. Findings: The BrisT1D dataset features both device data from the T1D management systems and smartwatches used by participants, as well as transcripts of monthly interviews and focus groups conducted during the study. The device data is provided in a processed state, for usability and more rapid analysis, and in a raw state, for in-depth exploration of novel insights captured in the study. Conclusions: This dataset has a range of potential applications. The quantitative elements can support blood glucose prediction, hypoglycaemia prediction, and closed-loop algorithm development. The qualitative elements enable the exploration of user experiences and opinions, as well as broader mixed-methods research into the role of smartwatches in T1D management.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 67,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17765",
        "abs_url": "https://arxiv.org/abs/2507.17765",
        "pdf_url": "https://arxiv.org/pdf/2507.17765",
        "title": "ASR-Guided Speaker-Role Diarization and Diarization-Guided ASR Decoding",
        "authors": [
            "Arindam Ghosh",
            "Mark Fuhs",
            "Bongjun Kim",
            "Anurag Chowdhury",
            "Monika Woszczyna"
        ],
        "comments": "Interspeech 2025 Submission",
        "subjects": "Audio and Speech Processing (eess.AS); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "From an application standpoint, speaker-role diarization (RD), such as doctor vs. patient, host vs. guest, etc. is often more useful than traditional speaker diarization (SD), which assigns generic labels like speaker-1, speaker-2 etc. In the context of joint automatic speech recognition (ASR) + SD (who spoke what?), recent end-to-end models employ an auxiliary SD transducer, synchronized with the ASR transducer, to predict speakers per word. In this paper, we extend this framework to RD with three key contributions: (1) we simplify the training via forced alignment and cross-entropy loss instead of RNNT loss, (2) we show that word prediction and role prediction require different amounts of predictor's context, leading to separate task-specific predictors, unlike existing shared-predictor models, and (3) we propose a way to leverage RD posterior activity to influence ASR decoding and reduce small-word deletion errors.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 68,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17766",
        "abs_url": "https://arxiv.org/abs/2507.17766",
        "pdf_url": "https://arxiv.org/pdf/2507.17766",
        "title": "Incentivised Orchestrated Training Architecture (IOTA): A Technical Primer for Release",
        "authors": [
            "Felix Quinque",
            "Alan Aboudib",
            "Szymon Fonau",
            "Rodrigo Lopez Portillo Alcocer",
            "Brian McCrindle",
            "Steffen Cruz"
        ],
        "comments": "",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)",
        "abstract": "In August 2024, Bittensor's Subnet 9 (SN9) demonstrated that a distributed network of incentivized, permissionless actors could each pretrain large language models (LLMs) ranging from 700 million to 14 billion parameters, while surpassing established baselines. While that work validated blockchain-based decentralized pretraining as viable, it contained core issues: (i) every miner had to fit an entire model locally, and (ii) \"winner-takes-all\" rewards encouraged model hoarding. Here we introduce IOTA (Incentivized Orchestrated Training Architecture), an architecture that addresses these limitations by transforming SN9's previously isolated competitors into a single cooperating unit that can scale arbitrarily while still rewarding each contributor fairly. Key preliminary results: (1) Data- and Pipeline-parallel SWARM architecture - An orchestrator distributes model layers across heterogeneous miners and streams activations between them, enabling model sizes to scale with the number of participants rather than being constrained by the VRAM of a single machine; (2) Granular, continuous incentives - Validators measure each miner's contribution and allocate token emissions proportionally; (3) Activation compression - We used model-bottlenecks to cut communication bandwidths of activations by up to 128x, vastly improving training speed; (4) Butterfly All-Reduce - Miners average disjoint parameter slices in O(1) bandwidth, offering linear scalability, redundancy and built-in collusion detection; (5) CLASP (Contribution Loss Assessment via Sampling of Pathways) - A fair attribution scheme assigns credit to miners proportional to their marginal utility and detects exploits, even when contributions are interdependent across the pipeline.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 69,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17769",
        "abs_url": "https://arxiv.org/abs/2507.17769",
        "pdf_url": "https://arxiv.org/pdf/2507.17769",
        "title": "PolyServe: Efficient Multi-SLO Serving at Scale",
        "authors": [
            "Kan Zhu",
            "Haiyang Shi",
            "Le Xu",
            "Jiaxin Shan",
            "Arvind Krishnamurthy",
            "Baris Kasikci",
            "Liguang Xie"
        ],
        "comments": "",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)",
        "abstract": "Advances in Large Language Models (LLMs) have led to a surge of LLM-powered applications. These applications have diverse token-generation latency requirements. As a result, simply classifying workloads as latency-sensitive (LS) or best-effort (BE) overlooks the nuances within the latency-sensitive category and results in suboptimal user experiences and scheduling opportunities. However, efficiently serving requests with multiple SLO requirements poses significant challenges. First, all requests within a batch generate new tokens simultaneously, which can misalign them with their distinct SLO requirements. Moreover, while existing systems focus on auto-scaling for handling various overall request rates, the diversity of SLOs necessitates fine-grained auto-scaling among these SLO tiers. Finally, unlike LS/BE scenarios, where BE requests can be aborted at any time to ensure the SLO attainment of LS requests, those with different latency-sensitive SLOs cannot tolerate prolonged delays, and tail latency must be controlled. To tackle these challenges, we propose PolyServe, a novel multi-SLO scheduling policy at scale that maintains high SLO attainment while maximizing throughput. PolyServe first groups requests into multiple bins based on their per-token latency requirement, then schedules each bin to a subset of the server fleet. PolyServe routes requests to the highest-load but still SLO-attainable server to create a load gradient that facilitates auto-scaling. To increase utilization, PolyServe permits looser-SLO requests to share tighter-SLO instances when their own servers are saturated. PolyServe uses profiling data to guide scheduling decisions and manage tail latency through request-wait-time-aware scheduling, dynamic chunking, and continuous chunked prefill prediction. PolyServe achieves 1.23x goodput gain compared to existing policies, achieving up to 92.5% of optimal goodput.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 70,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17772",
        "abs_url": "https://arxiv.org/abs/2507.17772",
        "pdf_url": "https://arxiv.org/pdf/2507.17772",
        "title": "Caching Techniques for Reducing the Communication Cost of Federated Learning in IoT Environments",
        "authors": [
            "Ahmad Alhonainy",
            "Praveen Rao"
        ],
        "comments": "Journal",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Federated Learning (FL) allows multiple distributed devices to jointly train a shared model without centralizing data, but communication cost remains a major bottleneck, especially in resource-constrained environments. This paper introduces caching strategies - FIFO, LRU, and Priority-Based - to reduce unnecessary model update transmissions. By selectively forwarding significant updates, our approach lowers bandwidth usage while maintaining model accuracy. Experiments on CIFAR-10 and medical datasets show reduced communication with minimal accuracy loss. Results confirm that intelligent caching improves scalability, memory efficiency, and supports reliable FL in edge IoT networks, making it practical for deployment in smart cities, healthcare, and other latency-sensitive applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 71,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17773",
        "abs_url": "https://arxiv.org/abs/2507.17773",
        "pdf_url": "https://arxiv.org/pdf/2507.17773",
        "title": "MultiKernelBench: A Multi-Platform Benchmark for Kernel Generation",
        "authors": [
            "Zhongzhen Wen",
            "Yinghui Zhang",
            "Zhong Li",
            "Zhongxin Liu",
            "Linna Xie",
            "Tian Zhang"
        ],
        "comments": "",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG); Performance (cs.PF); Software Engineering (cs.SE)",
        "abstract": "The automatic generation of deep learning (DL) kernels using large language models (LLMs) has emerged as a promising approach to reduce the manual effort and hardware-specific expertise required for writing high-performance operator implementations. However, existing benchmarks for evaluating LLMs in this domain suffer from limited hardware support, coarse-grained kernel categorization, and imbalanced task coverage. To address these limitations, we introduce MultiKernelBench, the first comprehensive, multi-platform benchmark for LLM-based DL kernel generation. MultiKernelBench spans 285 tasks across 14 well-defined kernel categories and supports three major hardware platforms: Nvidia GPUs, Huawei NPUs, and Google TPUs. To enable future extensibility, we design a modular backend abstraction layer that decouples platform-specific logic from the core benchmarking infrastructure, allowing easy integration of new hardware platforms. We further propose a simple yet effective category-aware one-shot prompting method that improves generation quality by providing in-category exemplars. Through systematic evaluations of seven state-of-the-art LLMs, we reveal significant variation in task difficulty, poor generalization to platforms with less training exposure, and the effectiveness of targeted prompting strategies. MultiKernelBench is publicly available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 72,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17775",
        "abs_url": "https://arxiv.org/abs/2507.17775",
        "pdf_url": "https://arxiv.org/pdf/2507.17775",
        "title": "Comparison of Optimised Geometric Deep Learning Architectures, over Varying Toxicological Assay Data Environments",
        "authors": [
            "Alexander D. Kalian",
            "Lennart Otte",
            "Jaewook Lee",
            "Emilio Benfenati",
            "Jean-Lou C.M. Dorne",
            "Claire Potter",
            "Olivia J. Osborne",
            "Miao Guo",
            "Christer Hogstrand"
        ],
        "comments": "",
        "subjects": "Quantitative Methods (q-bio.QM); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Geometric deep learning is an emerging technique in Artificial Intelligence (AI) driven cheminformatics, however the unique implications of different Graph Neural Network (GNN) architectures are poorly explored, for this space. This study compared performances of Graph Convolutional Networks (GCNs), Graph Attention Networks (GATs) and Graph Isomorphism Networks (GINs), applied to 7 different toxicological assay datasets of varying data abundance and endpoint, to perform binary classification of assay activation. Following pre-processing of molecular graphs, enforcement of class-balance and stratification of all datasets across 5 folds, Bayesian optimisations were carried out, for each GNN applied to each assay dataset (resulting in 21 unique Bayesian optimisations). Optimised GNNs performed at Area Under the Curve (AUC) scores ranging from 0.728-0.849 (averaged across all folds), naturally varying between specific assays and GNNs. GINs were found to consistently outperform GCNs and GATs, for the top 5 of 7 most data-abundant toxicological assays. GATs however significantly outperformed over the remaining 2 most data-scarce assays. This indicates that GINs are a more optimal architecture for data-abundant environments, whereas GATs are a more optimal architecture for data-scarce environments. Subsequent analysis of the explored higher-dimensional hyperparameter spaces, as well as optimised hyperparameter states, found that GCNs and GATs reached measurably closer optimised states with each other, compared to GINs, further indicating the unique nature of GINs as a GNN algorithm.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 73,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17779",
        "abs_url": "https://arxiv.org/abs/2507.17779",
        "pdf_url": "https://arxiv.org/pdf/2507.17779",
        "title": "CM-UNet: A Self-Supervised Learning-Based Model for Coronary Artery Segmentation in X-Ray Angiography",
        "authors": [
            "Camille Challier",
            "Xiaowu Sun",
            "Thabo Mahendiran",
            "Ortal Senouf",
            "Bernard De Bruyne",
            "Denise Auberson",
            "Olivier Müller",
            "Stephane Fournier",
            "Pascal Frossard",
            "Emmanuel Abbé",
            "Dorina Thanou"
        ],
        "comments": "IEEE EMBC 2025, 7 pages, 6 figures",
        "subjects": "Quantitative Methods (q-bio.QM); Machine Learning (cs.LG)",
        "abstract": "Accurate segmentation of coronary arteries remains a significant challenge in clinical practice, hindering the ability to effectively diagnose and manage coronary artery disease. The lack of large, annotated datasets for model training exacerbates this issue, limiting the development of automated tools that could assist radiologists. To address this, we introduce CM-UNet, which leverages self-supervised pre-training on unannotated datasets and transfer learning on limited annotated data, enabling accurate disease detection while minimizing the need for extensive manual annotations. Fine-tuning CM-UNet with only 18 annotated images instead of 500 resulted in a 15.2% decrease in Dice score, compared to a 46.5% drop in baseline models without pre-training. This demonstrates that self-supervised learning can enhance segmentation performance and reduce dependence on large datasets. This is one of the first studies to highlight the importance of self-supervised learning in improving coronary artery segmentation from X-ray angiography, with potential implications for advancing diagnostic accuracy in clinical practice. By enhancing segmentation accuracy in X-ray angiography images, the proposed approach aims to improve clinical workflows, reduce radiologists' workload, and accelerate disease detection, ultimately contributing to better patient outcomes. The source code is publicly available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": "作为这篇研究论文的作者，我很荣幸能与您分享我们从最初的构想到最终成果的思维决策过程。这篇论文的核心目标是解决冠状动脉X射线血管造影图像分割中数据标注稀缺的痛点，并为此提出CM-UNet这一基于自监督学习的模型。\n\n### 0. 为什么选择这个研究内容？\n\n我的研究兴趣一直集中在医疗图像分析领域，特别是利用深度学习技术解决临床中的实际问题。在与心血管科医生交流的过程中，我了解到冠状动脉疾病（CAD）是全球主要的死亡原因之一，而X射线冠状动脉造影（ICA）是诊断CAD的金标准。然而，医生在解读这些图像时，需要手动评估血管狭窄程度，这是一个**耗时、主观且容易出错**的过程。\n\n自动化的血管分割技术能够提供客观、精确的量化分析，显著提升诊断效率和准确性。但当我深入研究时，很快就遇到了一个巨大的瓶颈：**医疗图像数据集的标注成本极高**。冠状动脉结构精细，血管走形复杂多变，加上X射线图像固有的低信噪比、重叠结构（如导管、脊柱、肋骨），使得手动勾画每一根血管的精确边界成为一项极其耗时且需要专业知识的任务。公开可用的、大规模的高质量标注数据集**少之又少**。\n\n传统监督学习方法在数据稀缺的情况下表现不佳，这让我开始思考：**有没有一种方法，可以最大限度地利用未标注的图像，减少对昂贵人工标注的依赖，同时又能捕获血管的精细结构和全局特征？** 自监督学习（SSL）的出现为我打开了新的思路。它能够在没有人工标注的情况下从大量数据中学习到有用的特征表示，这简直是为医疗图像领域量身定制的解决方案。因此，我决定将研究重心放在“**如何将自监督学习有效应用于冠状动脉分割，以解决数据稀缺性问题**”上。\n\n### 1. 如何一步步构思和界定研究问题？\n\n我的思维过程大致遵循了从“痛点”到“核心挑战”，再到“具体解决方案”的路径：\n\n1.  **识别临床痛点：** 冠状动脉疾病诊断中的手动血管评估效率低下、主观性强，需要自动化。\n2.  **明确技术挑战：** 自动化分割需要深度学习，但深度学习需要大量标注数据。医疗图像领域缺乏大规模高质量标注数据集。此外，血管本身又细又密，图像质量差，分割难度大。\n3.  **界定核心问题：** 如何在**标注数据极其有限**的情况下，实现**高精度**的冠状动脉X射线造影图像分割，同时能够捕获**精细的血管结构**？\n4.  **初步解决方案方向：** 自监督学习（SSL）是解决数据稀缺的关键。\n\n在确定了SSL的方向后，我进一步思考：现有的SSL方法是否能完美解决我们领域的问题？\n*   **现有的SSL方法：** 我回顾了自监督学习的最新进展，主要分为对比学习（Contrastive Learning）和掩码自编码器（Masked Auto-Encoders, MAE）两大主流范式。\n    *   **对比学习：** 在区分不同实例上表现出色，善于捕获全局、判别性的特征。但对于医疗图像中“精细结构”的感知能力可能不足，例如，一根细小的血管在全局对比中可能被视为不重要的背景噪声。\n    *   **MAE：** 通过重建被掩码的区域来学习，这使其能够更好地关注局部细节和上下文信息，这对于医疗图像中精细的解剖结构至关重要。但其全局判别能力可能不如对比学习。\n*   **我们任务的特殊性：** 冠状动脉分割既需要**识别不同血管（全局判别）**，也需要**精确描绘血管边缘（局部细节）**，甚至要处理**微小血管**。\n\n这导致我构思了一个更具体的问题：**能否将对比学习的全局判别能力与MAE的局部重建能力结合起来，构建一个既能利用大量未标注数据，又能有效学习高层级判别特征和低层级精细结构特征的自监督预训练框架，并将其应用于冠状动脉分割任务？** 这个问题明确了我们方法的混合性质，以及其在医疗图像分割中的应用前景。\n\n### 2. 在选择理论框架、研究方法和数据分析策略时的思考、权衡和取舍\n\n为了解决上述问题，我做了以下思考、权衡和取舍：\n\n1.  **选择理论框架：对比学习 + MAE的混合范式**\n    *   **思考：** 单一的SSL方法可能无法兼顾冠状动脉分割的所有挑战。对比学习擅长“是什么”（这是血管，那是背景），但可能不擅长“在哪里”（血管的具体边界）；MAE擅长“在哪里”（重建细节），但可能不擅长“是什么”（判别不同血管或区分微弱血管与噪声）。\n    *   **权衡：** 如果只用对比学习，可能会丢失细小血管的细节，导致分割不连续或遗漏；如果只用MAE，模型可能缺乏全局语义理解，在复杂背景下难以区分血管和非血管结构。\n    *   **取舍：** 最终决定将两者融合。设计一个双分支的预训练框架，一个分支负责**掩码图像重建（MAE）**，以捕获局部精细结构；另一个分支负责**对比学习**，以学习全局判别性特征，并确保重构编码器也能学习到有意义的、判别性的表示。\n\n2.  **研究方法（模型架构与预训练策略）：基于UNet的CM-UNet**\n    *   **架构选择：**\n        *   **思考：** 医疗图像分割的经典且有效架构是UNet。它通过跳跃连接（skip connections）保留了多尺度信息，这对于精细结构（如血管）至关重要。\n        *   **取舍：** 决定将UNet作为我们CM-UNet的骨干网络（encoder-decoder），因为它能够自然地集成MAE的重建任务，同时其编码器也可以作为对比学习的基础。\n    *   **预训练设计：**\n        *   **图像变换：** 针对医疗图像的特点，我们设计了特定的数据增强策略。例如，随机裁剪、翻转和像素偏移在两个分支中都有，以增加数据多样性。而**高斯噪声仅用于对比学习分支**，因为对比学习需要鲁棒性来应对图像变化，而MAE重建对噪声过于敏感，可能影响细节恢复的精确性。\n        *   **特征对齐：** 这是一个关键决策。我们希望MAE编码器学到的特征也能用于对比学习。因此，引入了一个**辅助特征解码器（Df）**，将MAE编码器输出的潜在表示（ZA）转换为适用于对比学习的特征（ZAD）。这样，重建任务和对比任务的损失可以共同指导MAE编码器学习更优质的特征。\n        *   **损失函数：**\n            *   MAE分支使用**MSE损失**：直接衡量像素重建的准确性。\n            *   对比学习分支使用**InfoNCE损失**：强制模型将同一图像的不同增强视图（正样本对）拉近，将不同图像的视图（负样本对）推远，从而学习判别性特征。\n            *   **总损失：** 将MSE损失和InfoNCE损失进行加权求和（L = Lr + λLc）。这里的**λ是一个超参数**，通过它来权衡局部重建精度和全局判别性特征的重要性。这个平衡需要在实验中通过网格搜索或随机搜索来确定，以达到最佳性能。\n    *   **动量编码器（Momentum Encoder）：** 引入MoCo的思想，使用动量编码器（Ec）来生成对比学习的负样本，这样可以扩大负样本的数量，提高对比学习的效率和稳定性。动量更新机制保证了负样本特征的一致性，避免了直接反向传播带来的不稳定。\n\n3.  **数据分析策略：多维度评估与极低数据量测试**\n    *   **评估指标的全面性：**\n        *   **思考：** 仅仅依靠Dice分数来衡量分割性能是不够的，因为它只关注重叠度。对于血管分割，我们还需要关注**拓扑结构（连通性）、边界精度、甚至临床上的直径测量准确性**。\n        *   **取舍：** 决定采用一系列指标：Dice（重叠度）、Cl-Dice（中心线重叠度，关注连通性）、IoU（交并比）、Hausdorff Distance（边界精度）来全面评估分割的几何准确性。\n        *   **引入临床指标：** 为了体现模型的临床实用价值，我们增加了**动脉直径差异（ADD）**和**Pearson相关系数（PCC）/Bland-Altman分析**。ADD直接反映了模型对血管直径测量的准确性，而PCC和Bland-Altman则量化了自动分割与金标准（人工标注）在临床测量上的一致性。这能直接回答“模型预测的直径是否与医生测量的一致”这个关键问题。\n    *   **极端数据稀缺场景的验证：**\n        *   **思考：** 我们论文的核心论点是SSL在**数据稀缺**情况下的优势。仅仅在数据量相对充足的情况下进行测试是不够的，必须在极端稀缺的条件下验证。\n        *   **取舍：** 我们设计了**多种预训练-微调数据比例（PT-FT ratios）**，特别是将微调数据量缩减到**仅18张标注图像（79:1比例）**。这个决策是为了最大限度地模拟真实世界中医疗数据极度稀缺的场景，并以此来凸显CM-UNet在挑战性条件下的鲁棒性。\n        *   **与其他SSL方法对比：** 为了证明CM-UNet的优越性，我们不仅与“无预训练”的基线模型进行比较，还与Model Genesis、MAE、Spark、MoCo等SOTA SSL方法进行比较。这确保了我们的成果具有领先性。\n\n### 3. 在研究的每个关键阶段，我做出了哪些重要决策，并能解释这些决策背后的逻辑和依据。\n\n我将通过一个具体的例子来串联这些关键决策：**想象我们正在处理一张冠状动脉造影图像，其中有一根非常细小、对比度很低的远端血管。**\n\n1.  **选题决策（解决数据稀缺）：**\n    *   **决策：** 将研究重点放在利用自监督学习解决冠状动脉分割中的“标注数据稀缺”问题。\n    *   **逻辑：** 那根细小、对比度低的远端血管，在真实临床中很容易被忽略或难以精确勾画。如果只依赖少量手动标注，模型很难学到它的特征。我意识到，即使有医生标注了这张图，类似这样具有挑战性的样本也只有几例，监督学习会因为样本量不足而导致对这些“罕见”但重要的细节学习不足。SSL能够利用大量未标注的相似图像（尽管不包含该血管的精确标注），从海量的血管模式中学习通用特征，从而潜在地提高对这类挑战性血管的识别能力。\n\n2.  **方法论决策（CM-UNet的混合架构）：**\n    *   **决策：** 设计CM-UNet，结合MAE的局部重建和对比学习的全局判别。\n    *   **逻辑：** 对于那根细小、对比度低的远端血管：\n        *   **纯对比学习的局限性：** 图像中，这根细血管可能只是一个微不足道的局部特征。如果仅进行全局图像级的对比，模型可能无法将其与其他背景噪声有效区分，因为它在“实例”层面不够突出。\n        *   **纯MAE的局限性：** MAE确实能重建它，但如果模型不理解它是“血管”这一类别的判别性特征，在后续微调时也可能难以将其从背景中分离。\n        *   **CM-UNet的优势：**\n            *   **MAE分支（重建）：** 强制模型编码器学习并重建被掩盖的这根细血管区域。这使得模型对局部像素级别的细节变得敏感，能够精确捕捉它的形态。\n            *   **对比学习分支（判别）：** 即使这根血管很细，但在不同增强视图中，它作为“同一血管”的特征会被拉近，而与背景或其他非血管结构的特征被推远。**关键在于辅助特征解码器（Df）**。它确保了负责重建的编码器（Er）也通过对比损失学习到**判别性**特征。也就是说，Er不仅知道如何重建这根血管，还知道它与其他图像中“非血管”的区域是**不同**的。这种双重监督让模型对细微血管的“存在感”和“判别性”都有了深刻的理解。\n\n3.  **实验设计决策（极低数据量测试）：**\n    *   **决策：** 特别测试在极低标注数据量（如18张图像，79:1 PT-FT比例）下的模型性能。\n    *   **逻辑：** 之前提到的那根细小、对比度低的血管，在仅有18张标注图像进行微调时，监督学习模型（“None”）几乎会完全失效。模型可能只会学习到最粗大的血管，而那根挑战性血管则直接被忽略或被识别为噪声。我们的核心主张是SSL能对抗数据稀缺。通过这种极端测试，我们能清晰地看到，当监督信号极度微弱时，我们的CM-UNet凭借预训练阶段从大量未标注图像中学习到的**强大先验知识**（关于血管的形态、连通性、与背景的区分等），仍然能够保持相对较高的性能，甚至能识别出那根挑战性的细小血管，尽管可能不完美。相比之下，没有预训练的模型会彻底崩溃。这直接验证了我们“减少标注需求”的假设。\n\n4.  **评估指标决策（综合临床指标）：**\n    *   **决策：** 除了常规分割指标（Dice、IoU），还引入了Cl-Dice（关注连通性）和ADD/PCC/Bland-Altman（关注临床直径测量一致性）。\n    *   **逻辑：** 仅仅分割出像素是不够的，医生更关心血管的**完整性**和**直径**。对于那根细小的血管：\n        *   **Cl-Dice：** 即使像素分割准确，如果血管不连续，那在临床上也是无用的。Cl-Dice可以确保模型学到的血管是连通的，符合生物学特性，避免了碎片化分割。\n        *   **ADD/PCC/Bland-Altman：** 医生最关心的是狭窄程度，这直接关系到血管直径。如果我们的模型能准确地测量出那根细血管的直径（例如，它没有变窄，或者它确实有轻微狭窄），并且这个测量结果与医生人工测量高度一致，那么这个模型就真正具有了临床价值。这些指标确保了我们的模型不仅在像素层面表现好，在**临床实用层面**也具备高可靠性。\n\n通过上述一系列的思考、权衡和决策，我们最终构建了CM-UNet，并在实验中证明了它在冠状动脉分割任务中，尤其是在标注数据稀缺的场景下，相较于其他方法具有显著的性能优势和临床潜力。这让我对这项工作的实际应用前景充满信心。"
    },
    {
        "order": 74,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17799",
        "abs_url": "https://arxiv.org/abs/2507.17799",
        "pdf_url": "https://arxiv.org/pdf/2507.17799",
        "title": "A Concept-based approach to Voice Disorder Detection",
        "authors": [
            "Davide Ghia",
            "Gabriele Ciravegna",
            "Alkis Koudounas",
            "Marco Fantini",
            "Erika Crosetti",
            "Giovanni Succo",
            "Tania Cerquitelli"
        ],
        "comments": "",
        "subjects": "Audio and Speech Processing (eess.AS); Machine Learning (cs.LG); Sound (cs.SD)",
        "abstract": "Voice disorders affect a significant portion of the population, and the ability to diagnose them using automated, non-invasive techniques would represent a substantial advancement in healthcare, improving the quality of life of patients. Recent studies have demonstrated that artificial intelligence models, particularly Deep Neural Networks (DNNs), can effectively address this task. However, due to their complexity, the decision-making process of such models often remain opaque, limiting their trustworthiness in clinical contexts. This paper investigates an alternative approach based on Explainable AI (XAI), a field that aims to improve the interpretability of DNNs by providing different forms of explanations. Specifically, this works focuses on concept-based models such as Concept Bottleneck Model (CBM) and Concept Embedding Model (CEM) and how they can achieve performance comparable to traditional deep learning methods, while offering a more transparent and interpretable decision framework.",
        "gemini2.5flash": "",
        "overall_idea": "作为本文的作者，我很荣幸能深入剖析我们的研究工作，分享从最初的灵感萌芽到最终解决方案落地的整个思维决策过程。这个过程充满了思考、权衡与取舍，每一步都旨在实现我们设定的核心目标：在语音障碍检测领域，不仅要达到媲美现有先进模型的性能，更要提供前所未有的高可解释性。\n\n### 0. 为什么选择这个研究内容？\n\n最初，我们团队对人工智能在医疗健康领域的潜力抱有极大的热情。语音障碍是一个影响广泛但常被忽视的健康问题，它不仅关乎发声功能，还常常是神经退行性疾病、良恶性肿瘤等严重病症的早期信号。传统的诊断方法往往涉及侵入性检查，给患者带来不适。因此，我们看到了**利用非侵入性AI技术进行早期、便捷诊断的巨大价值**。\n\n我们注意到，近年来深度学习模型，尤其是Transformer架构，在语音处理任务上取得了显著进展，包括语音障碍检测。它们能够直接从原始语音数据中学习复杂模式，性能令人印象深刻。然而，我们很快意识到一个核心问题：**这些模型是“黑箱”**。它们能给出“是/否”的诊断结果，但无法解释为什么。在临床环境中，医生不仅需要结果，更需要了解决策背后的依据。一个不透明的算法，无论多准确，都难以赢得医生的信任，也可能引发伦理和法律问题。\n\n因此，我们的研究动机不仅仅是“构建一个更好的语音障碍检测器”，而是“**构建一个可信赖、可解释的语音障碍检测器**”。我们认为，可解释AI（XAI）是弥合AI与临床实践之间鸿沟的关键。\n\n### 1. 如何一步步构思和界定研究问题的？\n\n我们的研究问题并非一蹴而就，而是经过了逐步的聚焦和细化：\n\n1.  **初始宽泛问题：** 如何利用AI有效检测语音障碍？\n2.  **发现痛点与第一次细化：** 现有AI模型（尤其是DNN和Transformer）虽然性能高，但缺乏可解释性，这在医疗领域是致命缺陷。\n    *   **问题核心：** “黑箱”特性限制了AI在临床中的应用。\n3.  **探索解决方案方向与第二次细化：** XAI是解决可解释性的有效途径。XAI有多种方法，包括事后解释（post-hoc）和设计即解释（explainable-by-design）。\n    *   **关键抉择：** 我们倾向于“设计即解释”的方法。事后解释通常是在模型训练完成后，试图去理解其决策，这可能会有局限性甚至误导性。而“设计即解释”则将可解释性内嵌到模型架构中，使得模型的决策过程本身就是透明的。\n4.  **选择特定XAI范式与第三次细化：** 在“设计即解释”中，我们特别关注**基于概念的模型（Concept-Based Models, CBM/CEM）**。\n    *   **原因：** CBM和CEM模型通过学习人类可理解的“概念”作为中间层进行预测。这意味着模型的诊断结果可以被解释为“因为检测到了概念A、概念B，所以判断为语音障碍”。这与临床医生的诊断思维高度吻合，极大地提升了可信度和可理解性。\n5.  **明确具体研究问题：** 基于以上思考，我们将研究问题界定为：**基于概念的AI模型（如CBM和CEM）能否在语音障碍检测任务中，在保持与现有端到端深度学习模型（如Transformer）相当性能的同时，提供显著增强的决策可解释性？**\n\n为了解决这个核心问题，我们又拆解出几个子问题：\n*   **概念来源：** 如何从非结构化的临床病历中提取出对语音障碍诊断有用的“概念”？这些概念应该是什么？\n*   **模型构建：** 如何将这些概念有效地整合到深度学习模型架构中，实现概念学习和最终分类的双重目标？\n*   **评估标准：** 如何量化评估模型的性能和可解释性？\n\n### 2. 在选择理论框架、研究方法和数据分析策略时的思考、权衡和取舍\n\n这个阶段是整个研究的核心，我们做了多方面的考量：\n\n1.  **理论框架选择：**\n    *   **决策：** 采用基于概念的XAI（CBM和CEM）。\n    *   **思考/权衡：** 放弃了流行的事后XAI方法（如LIME、SHAP等），因为它们通常提供的是局部解释或特征归因，而非直接的、高层次的语义概念。在医疗场景，医生更需要像“嘶哑”、“气短”这样的明确症状信息，而不是抽象的神经网络激活热图。CBM/CEM能够直接输出这些概念，与医生的思维模式更贴合，也更具交互性（例如，可以手动修改概念，观察对最终预测的影响，进行反事实分析）。\n\n2.  **语音特征提取策略：**\n    *   **决策：** 使用预训练的HuBERT模型作为语音特征提取器。\n    *   **思考/权衡：** 传统的语音特征（如MFCCs）虽然可解释，但在复杂模式识别上性能有限。端到端Transformer模型可以直接处理原始波形，但我们的数据集规模有限，从头训练一个大型Transformer是不可行的，容易过拟合。HuBERT作为自监督预训练模型，在大规模无标注语音数据上学习到了丰富的语音表征，非常适合我们的有限标注数据场景。它提供了高质量的音频嵌入，为后续概念学习奠定了基础。我们还尝试了最大池化（max pooling）来获取最终的单一表示，因为语音特征（如气短、粗糙度）往往表现为局部峰值，最大池化能有效捕获这些显著信息。\n\n3.  **概念标注策略：**\n    *   **挑战：** 我们的IPV数据集中的病历是医生手写的、非标准化的文本文件（PDF或Word）。手动逐一标注大量概念是耗时且容易出错的。\n    *   **决策：** 利用大型语言模型（LLM）进行概念的自动标注，并结合少量人工“few-shot prompting”示例进行引导。\n    *   **思考/权衡：**\n        *   **LLM选择：** 我们对比了开源的Mistral 7B和闭源的Gemini-pro。Gemini-pro在我们的验证集上表现出更高的概念标注准确率和F1分数，错误数量也更少（20个 vs 42个）。尽管Gemini是专有模型，但为了确保概念标注的质量（这是基于概念模型的基础），我们最终选择了性能更优的Gemini-pro。\n        *   **Few-shot prompting：** 这是关键。我们精心设计了prompt，提供了几个涵盖不同概念值组合的输入/输出示例。这极大地提高了LLM理解非标准化医学文本并提取特定概念的能力。例如，对于“dysphonia”概念，病历中可能会有“轻度嘶哑”、“声音粗糙”等描述，我们通过few-shot示例训练LLM将其映射到预定义的“light”、“moderate”、“severe”等级。\n        *   **概念集的定义与精炼：** 这涉及到与医学专家的多次深度沟通。\n            *   **初始：** 识别了14个潜在概念，然后扩展到20个二进制概念。\n            *   **取舍：**\n                *   排除了“粘液（mucous）”等概念：因为这些是物理特性，无法通过感官（如听觉）检查来预测，需要侵入性手段。我们的模型输入是语音，因此应只关注能从语音中推断出的概念。\n                *   排除了“性别（gender）”、“发声无力（phonasthenia）”等概念：这些是患者提供的信息，或可直接观察，模型无需从语音特征中“预测”。\n                *   合并了“轻度发声困难”和“中度发声困难”：医学专家认为这种细微区分在我们的诊断粒度上并非必要，合并能简化模型并减少标注歧义。\n                *   **最终：** 确定了9个可预测概念（其中8个与GRBAS量表对齐，这增强了模型输出的临床意义）和5个患者提供概念。这一精炼过程确保了概念集的**临床相关性、可预测性和简洁性**。\n\n4.  **模型架构与训练策略：**\n    *   **决策：** 构建了CBM和CEM两种概念模型，并采用联合损失函数进行训练。\n    *   **思考/权衡：**\n        *   **CBM vs CEM：** CBM直接在瓶颈层预测概念的 logits。CEM则为每个概念学习“激活”和“未激活”两种嵌入，并通过概念概率进行加权组合。CEM提供了更丰富的概念表征。我们选择同时研究这两种模型，以比较它们在性能和解释能力上的差异。\n        *   **联合损失（Concept Loss + Task Loss）：** 这是至关重要的训练策略。我们不是简单地预测最终结果，而是**先强制模型学习概念，再通过概念预测结果**。\n        *   **权重（λ=0.9 for concept loss）：** 在训练初期（前2个epoch），我们甚至冻结了任务损失，只激活概念损失。这是一种**有意为之的偏向**。我们的逻辑是：如果模型能准确地识别出像“气短”、“粗糙度”这样的临床概念，那么即使最初的任务性能略有不足，最终也能通过这些高质量的概念学习到更鲁棒、更具解释性的决策。我们相信，扎实的概念理解是实现可信赖AI诊断的基石。\n\n### 3. 在研究的每个关键阶段，做出了哪些重要决策，并能解释这些决策背后的逻辑和依据。\n\n**一个具体的例子：处理“发声困难（Dysphonia）”概念的决策链。**\n\n“发声困难”是语音障碍的核心症状之一，但它的程度可以很复杂。\n\n1.  **最初的想法：** 病历中可能直接提到“发声困难”，或者通过描述（如“声音嘶哑”、“说话吃力”）暗示。\n2.  **概念定义与量化：** 我们与医学专家协商，决定将“发声困难”划分为几个等级：无（absent）、轻度（light）、轻中度（light-moderate）、中度（moderate）、重度（severe）。这与临床常用的GRBAS量表中的“G”（Grade of dysphonia）等级相符，使得模型输出更具临床意义。\n3.  **LLM标注的挑战与应对：**\n    *   **挑战：** 医生在病历中对发声困难程度的描述可能非常模糊或不一致。例如，一个病历可能写“voce debole”（声音微弱），另一个可能写“disfonia lieve”（轻微发声困难）。如何让LLM统一映射？\n    *   **决策：**\n        *   **详细的Few-shot Prompting：** 我们在LLM的prompt（Listing 1.1）中为“dysphonia”提供了多种示例，展示了不同描述如何映射到不同的等级。例如，如果病历提到“略显嘶哑”，LLM应将其标记为“light”；如果提到“严重发声困难，几乎无法出声”，则标记为“severe”。这是一种**人工指导下的语义对齐**。\n        *   **验证集评估与LLM选择：** 在小规模验证集上，我们发现Gemini-pro在处理这种模糊性方面优于Mistral，能更准确地推断出概念等级。这是我们选择Gemini进行大规模标注的关键依据。\n4.  **概念精炼与合并：**\n    *   **决策：** 最终，我们采纳了医学专家的建议，将“light”和“light-moderate”合并为“light”，并将“light-moderate”和“moderate”合并为“moderate”。换句话说，去掉了“light-moderate”这个中间等级。\n    *   **逻辑和依据：** 专家认为，在临床实践中，这些细微的等级差异有时难以精确区分，且对于初步的语音障碍检测而言，更粗粒度的区分（无、轻、中、重）已足够支持诊断决策，过于细致反而可能引入不必要的复杂性和标注误差。这个决策体现了我们**从纯技术最优向临床实用性妥协和优化的权衡**。\n5.  **模型训练中的体现：**\n    *   在CBM和CEM模型中，“发声困难”的这些等级（现在为4个：无、轻、中、重）将作为独立的二进制概念进行预测（例如，`dysphonia_absent`, `dysphonia_light`, `dysphonia_moderate`, `dysphonia_severe`）。\n    *   当模型预测出这些概念（例如，`dysphonia_moderate`为“是”，其他为“否”）时，最终的任务分类器会结合这些概念和其他如“粗糙度”、“气短”等概念，共同做出“病理性语音”的预测。\n    *   **可解释性优势：** 例如，模型可能会输出：“ detected `dysphonia_moderate` and `roughness` is true. Therefore, predict `pathological voice`.” 这种解释对于医生而言是直观且可信的，因为他们日常就是通过这些症状来判断的。\n\n**总结其他关键决策：**\n\n*   **数据量限制下的鲁棒性：** IPV数据集的规模相对有限（385个有病历的病例）。\n    *   **决策：** 采用10折交叉验证。\n    *   **逻辑：** 最大限度地利用有限数据，确保模型的泛化能力评估更可靠，降低偶然性。\n*   **对比基线的选择：**\n    *   **决策：** 选择了一个性能最佳的端到端HuBERT模型作为基线。\n    *   **逻辑：** 这样可以公平地比较我们的概念模型是否能在性能上与之匹敌，从而证明可解释性并非以牺牲准确率为代价。结果表明，我们的概念模型在F1分数和准确率上都达到了相当的水平，这验证了我们的方法的可行性。\n*   **理想CBM的引入：**\n    *   **决策：** 除了实际训练的CBM和CEM，我们还训练了一个“理想CBM”（Ideal CBM），它直接以**真实的**概念标签作为输入来预测最终分类。\n    *   **逻辑：** 这是一个对照实验，用于评估概念本身的“诊断能力”。如果理想CBM表现出色，就说明我们选择的概念集本身是有效的，模型性能的任何差距主要来源于概念预测的准确性（即LLM标注和模型概念学习的误差），而非概念集本身的不足。我们的结果显示理想CBM性能非常接近基线HuBERT，有力证明了选定概念的有效性。\n\n整个研究过程是一个迭代和精炼的过程。我们不仅要解决技术问题，更要深入理解医学领域的实际需求和约束。通过与医学专家的紧密合作，我们确保了研究成果不仅在技术上具有创新性，更在临床上具有实际应用价值和可信度。"
    },
    {
        "order": 75,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17804",
        "abs_url": "https://arxiv.org/abs/2507.17804",
        "pdf_url": "https://arxiv.org/pdf/2507.17804",
        "title": "On the Energy Distribution of the Galactic Center Excess' Sources",
        "authors": [
            "Florian List",
            "Yujin Park",
            "Nicholas L. Rodd",
            "Eve Schoen",
            "Florian Wolf"
        ],
        "comments": "7+20 pages, 2+20 figures, comments welcome",
        "subjects": "High Energy Astrophysical Phenomena (astro-ph.HE); Cosmology and Nongalactic Astrophysics (astro-ph.CO); Instrumentation and Methods for Astrophysics (astro-ph.IM); Machine Learning (cs.LG); High Energy Physics - Phenomenology (hep-ph)",
        "abstract": "The Galactic Center Excess (GCE) remains one of the defining mysteries uncovered by the Fermi $\\gamma$-ray Space Telescope. Although it may yet herald the discovery of annihilating dark matter, weighing against that conclusion are analyses showing the spatial structure of the emission appears more consistent with a population of dim point sources. Technical limitations have restricted prior analyses to studying the point-source hypothesis purely spatially. All spectral information that could help disentangle the GCE from the complex and uncertain astrophysical emission was discarded. We demonstrate that a neural network-aided simulation-based inference approach can overcome such limitations and thereby confront the point source explanation of the GCE with spatial and spectral data. The addition is profound: energy information drives the putative point sources to be significantly dimmer, indicating either the GCE is truly diffuse in nature or made of an exceptionally large number of sources. Quantitatively, for our best fit background model, the excess is essentially consistent with Poisson emission as predicted by dark matter. If the excess is instead due to point sources, our median prediction is ${\\cal O}(10^5)$ sources in the Galactic Center, or more than 35,000 sources at 90% confidence, both significantly larger than the hundreds of sources preferred by earlier point-source analyses of the GCE.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 76,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17845",
        "abs_url": "https://arxiv.org/abs/2507.17845",
        "pdf_url": "https://arxiv.org/pdf/2507.17845",
        "title": "Towards Robust Foundation Models for Digital Pathology",
        "authors": [
            "Jonah Kömen",
            "Edwin D. de Jong",
            "Julius Hense",
            "Hannah Marienwald",
            "Jonas Dippel",
            "Philip Naumann",
            "Eric Marcus",
            "Lukas Ruff",
            "Maximilian Alber",
            "Jonas Teuwen",
            "Frederick Klauschen",
            "Klaus-Robert Müller"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)",
        "abstract": "Biomedical Foundation Models (FMs) are rapidly transforming AI-enabled healthcare research and entering clinical validation. However, their susceptibility to learning non-biological technical features -- including variations in surgical/endoscopic techniques, laboratory procedures, and scanner hardware -- poses risks for clinical deployment. We present the first systematic investigation of pathology FM robustness to non-biological features. Our work (i) introduces measures to quantify FM robustness, (ii) demonstrates the consequences of limited robustness, and (iii) proposes a framework for FM robustification to mitigate these issues. Specifically, we developed PathoROB, a robustness benchmark with three novel metrics, including the robustness index, and four datasets covering 28 biological classes from 34 medical centers. Our experiments reveal robustness deficits across all 20 evaluated FMs, and substantial robustness differences between them. We found that non-robust FM representations can cause major diagnostic downstream errors and clinical blunders that prevent safe clinical adoption. Using more robust FMs and post-hoc robustification considerably reduced (but did not yet eliminate) the risk of such errors. This work establishes that robustness evaluation is essential for validating pathology FMs before clinical adoption and demonstrates that future FM development must integrate robustness as a core design principle. PathoROB provides a blueprint for assessing robustness across biomedical domains, guiding FM improvement efforts towards more robust, representative, and clinically deployable AI systems that prioritize biological information over technical artifacts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 77,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17860",
        "abs_url": "https://arxiv.org/abs/2507.17860",
        "pdf_url": "https://arxiv.org/pdf/2507.17860",
        "title": "Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis",
        "authors": [
            "Ko Watanabe. Stanislav Frolov. Adriano Lucieri. Andreas Dengel"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Recent advancements in Deep Learning and its application on the edge hold great potential for the revolution of routine screenings for skin cancers like Melanoma. Along with the anticipated benefits of this technology, potential dangers arise from unforseen and inherent biases. Thus, assessing and improving the fairness of such systems is of utmost importance. A key challenge in fairness assessment is to ensure that the evaluation dataset is sufficiently representative of different Personal Identifiable Information (PII) (sex, age, and race) and other minority groups. Against the backdrop of this challenge, this study leverages the state-of-the-art Generative AI (GenAI) LightningDiT model to assess the fairness of publicly available melanoma classifiers. The results suggest that fairness assessment using highly realistic synthetic data is a promising direction. Yet, our findings indicate that verifying fairness becomes difficult when the melanoma-detection model used for evaluation is trained on data that differ from the dataset underpinning the synthetic images. Nonetheless, we propose that our approach offers a valuable new avenue for employing synthetic data to gauge and enhance fairness in medical-imaging GenAI systems.",
        "gemini2.5flash": "",
        "overall_idea": "作为本文的作者，我很荣幸能深入剖析这项研究的整个思维决策过程。这不仅是一次技术实践，更是一次对AI伦理与实际应用之间鸿沟的探索。\n\n### 0. 为什么选择这个研究内容？\n\n选择“AI辅助皮肤病变分类器的公平性评估”作为研究内容，主要基于以下几个核心考量：\n\n1.  **社会影响与紧迫性：** 黑色素瘤是致死率最高的皮肤癌之一，早期发现至关重要。AI在皮肤癌筛查中的潜力巨大，尤其在智能手机普及的背景下，它有望实现大规模、便捷的早期筛查，显著降低死亡率。然而，医疗AI的任何部署都必须高度可靠和公平，因为其决策直接关系到患者的健康和生命。这并非一个纯粹的技术问题，更是一个伦理和社会问题。\n2.  **AI伦理的时代要求：** 随着欧盟AI法案等法规的出台，AI的“可信度”（Trustworthiness）成为核心议题，而“公平性”（Fairness）是可信AI不可或缺的维度。如果AI系统对不同人群（如不同性别、年龄、肤色）的表现存在显著偏差，其社会接受度和应用价值将大打折扣，甚至可能加剧现有的健康不平等。\n3.  **现有方法与数据瓶颈：** 公平性评估的核心在于拥有代表性强、多样化的评估数据集。然而，真实的医疗图像数据集往往存在严重的**数据不平衡**（尤其是针对少数族裔、特定年龄组或罕见疾病），且受限于**隐私法规**（PII，个人可识别信息），难以共享和获取。这就形成了一个两难：我们需要大量多样化的数据来评估公平性，但这些数据正是我们最难获得的。\n4.  **GenAI的爆发式发展：** 近年来，以Diffusion模型为代表的生成式AI（GenAI）在图像合成方面取得了惊人的进展，能够生成高度逼真、多样性丰富的图像，并且能够通过文本提示进行精确控制。这让我产生了强烈的直觉：GenAI是否能成为解决医疗AI公平性评估数据瓶颈的关键工具？能否利用合成数据来填补真实数据的空白，从而实现更全面、更便捷的公平性评估？\n\n正是基于对社会责任、AI伦理、技术挑战和GenAI潜力的综合考量，我决定将研究重心放在利用GenAI生成合成数据以促进皮肤病变分类器公平性评估上。\n\n### 1. 如何一步步构思和界定研究问题的？\n\n我的研究问题构思过程是一个从宏观到微观、逐步聚焦的过程：\n\n*   **初步想法：** “使用GenAI来解决医疗AI的公平性问题”。这个想法很大，需要具体化。\n*   **聚焦领域：** 首先，选择一个具体的医疗AI应用场景。皮肤病变分类是一个很好的选择：图像是核心，存在明确的PII信息（性别、年龄、肤色），且数据不平衡问题突出（如ISIC数据集）。\n*   **明确“公平性”的维度：** 既然关注PII，那么公平性就具体到：模型对不同性别、年龄段、肤色类型的人群的诊断表现是否一致。\n*   **GenAI在其中的角色：** GenAI的核心能力是“生成”。那么，它可以生成什么？\n    *   **生成数据以补充真实数据？** 这是最直接的思路。\n    *   **生成“缺失”的数据？** 尤其针对那些在真实数据集中代表性不足的PII组合。\n*   **如何评估“公平性”？** 需要一个明确的指标。最初考虑过多种公平性指标，如平等机会（Equalized Odds）、预测平等（Predictive Parity）等。但考虑到我们首先要解决的是**数据可获得性**和**基础的预测偏差**，**人口统计学平等（Demographic Parity, DP）**，即不同群体间的预测率（或准确率）差异，是最直接且易于理解的起点。它直接反映了模型对不同群体“一视同仁”的程度。\n*   **将研究拆解为可验证的问题：**\n    *   **问题1（R1）：** GenAI真的能生成高质量、可控的、能反映不同PII属性的皮肤病变图像吗？这是技术可行性问题。如果生成的图像质量差、不真实或无法准确控制PII属性，那么后续的公平性评估就无从谈起。这对应了论文中的R1：“Can we use state-of-the-art generative image synthesis methods to obtain a balanced fairness assessment dataset?”（我们能用最先进的生成图像合成方法获得一个平衡的公平性评估数据集吗？）\n    *   **问题2（R2）：** 即使能生成，这些合成数据对公平性评估是否有用？能否用它们来发现现有预训练模型的偏见？这对应了论文中的R2：“Can this synthetic dataset be used to reliably assess the fairness of skin lesion classifiers?”（这个合成数据集能否可靠地评估皮肤病变分类器的公平性？）\n\n通过这种层层递进的思考，我将一个宽泛的“AI公平性”问题，界定为利用GenAI解决“医疗图像数据不平衡导致的公平性评估困难”这一具体挑战，并提出了两个可量化、可验证的研究问题。\n\n### 2. 在选择理论框架、研究方法和数据分析策略时的思考、权衡和取舍\n\n这个阶段充满了思考和权衡，每一个选择背后都有其逻辑。\n\n*   **理论框架（公平性指标）：**\n    *   **思考：** 如前所述，公平性指标有很多。初期我们考虑过更复杂的指标，但DP（即准确率平等AP）在我们的情境下具有独特优势。\n    *   **权衡与取舍：**\n        *   **优势：** DP简单直观，易于计算和理解，非常适合作为**初步探索性研究**的指标。它直接衡量模型在不同人口统计学群体上的“整体预测率”或“准确率”差异，这与我们希望通过生成**平衡数据集**来观察模型对这些群体的表现差异的目标高度吻合。\n        *   **劣势：** DP不是唯一甚至不是最全面的公平性指标。它不考虑误报率和漏报率等具体错误类型。\n        *   **决策：** 鉴于我们研究的重点是**合成数据是否能揭示偏差**，而非寻找最优的公平性定义，DP作为起点是合理的。后续研究可以进一步探索其他指标。\n\n*   **生成模型（GenAI backbone）：**\n    *   **思考：** 市面上有很多生成模型（GANs、VAEs、Diffusion Models）。我们急需高保真度、多样性以及最重要的**可控性**。\n    *   **权衡与取舍：**\n        *   **GANs：** 早期工作（DermGAN、StyleGAN2-ADA）证明了GAN在医学图像生成中的潜力。但GAN训练不稳定，模式崩溃（mode collapse）问题可能导致多样性不足，且其在精细语义控制（如通过文本提示控制PII属性）方面的能力不如Diffusion模型。\n        *   **Diffusion Models：** 近年来表现优异，尤其在图像质量和多样性方面超越GAN。更关键的是，它们通过文本编码器（如CLIP）实现**强大的文本条件生成能力**，这对于我们根据PII属性（性别、年龄、肤色、疾病）生成特定图像至关重要。\n        *   **具体选择：LightningDiT [42]：** 在Diffusion模型中，我们选择了SOTA的LightningDiT。它的优势在于**效率**（Latent Diffusion Transformer）和**性能**（VA-VAE对齐）。这意味着我们可以在合理的时间内生成大量高质量、可控的合成图像。\n        *   **决策：** 选择LightningDiT是基于其在图像质量、可控性、训练效率上的综合优势。虽然训练这类模型需要大量计算资源和时间（如文中所述的8小时在A100 GPU上），但为了确保合成数据的“可靠性”，这种投入是值得的。\n\n*   **训练数据（用于生成模型）：**\n    *   **思考：** 哪种真实数据集适合训练我们的生成模型？\n    *   **权衡与取舍：**\n        *   **ISIC数据集：** 拥有庞大的皮肤病变图像库，并且关键的是，它包含了丰富的**PII元数据**（性别、年龄、Fitzpatrick皮肤类型等）。这直接支持了我们通过文本提示来控制生成图像属性的需求。\n        *   **其他数据集（如HAM10000、Derm7pt）：** 规模可能较小，或PII信息不如ISIC全面。\n        *   **决策：** 使用所有可用的ISIC数据集（2016-2024年所有splits，约50万张图像）。这确保了生成模型能从最广泛、最丰富的真实数据分布中学习，从而提高合成图像的真实性和多样性。同时，其PII元数据是实现条件生成的核心。\n\n*   **测试模型（用于公平性评估）：**\n    *   **思考：** 用哪些现有的黑色素瘤分类器来评估？\n    *   **权衡与取舍：**\n        *   **理想情况：** 评估所有SOTA模型。\n        *   **实际制约：** 许多高绩效模型（如表1所示）的**预训练模型代码或权重并不公开**（“Pretrained Model”列）。我们无法从零开始训练这些模型，因为训练医学图像分类器需要大量计算资源和专业知识。\n        *   **决策：** 只能选择那些**公开可用的预训练模型**。最终我们选择了DeepGuide [26]、MelaNet [43]和SkinLesionDensenet [28]。\n        *   **重要发现与权衡：** 在选择这些模型后，我们发现了一个关键的“意外”：这些模型训练所用的数据集并不完全相同（DeepGuide用HAM，MelaNet和SkinLesionDensenet用ISIC）。这本来是一个“不理想”的情况，但我们决定将其转化为一个**重要的研究洞察**：这使得我们的合成数据不仅能评估公平性，还能间接揭示**模型对数据集漂移（dataset shift）的鲁棒性**。这大大增加了研究的深度和意义。我们没有回避这个差异，而是利用它来丰富讨论。\n\n*   **数据分析策略：**\n    *   **生成策略：** 为了实现平衡评估，我们决定对所有PII组合（2性别 x 8年龄段 x 7肤色类型 x 1疾病类型）生成**相同数量**的图像（每组合100张）。\n    *   **逻辑：** 这种网格化的生成方式确保了评估数据集在PII维度上的完全平衡，从而消除了真实数据不平衡对公平性评估的干扰，使我们能更纯粹地观察模型的内在偏见。\n    *   **评估策略：** 计算每个模型在不同PII组上的预测率（准确率），并计算DP值。\n    *   **逻辑：** 直接量化模型在不同群体间的性能差异，以便进行比较。\n\n### 3. 在研究的每个关键阶段，您做出了哪些重要决策，并能解释这些决策背后的逻辑和依据。\n\n**阶段一：研究方向确立与问题界定**\n\n*   **重要决策：** 将医疗AI的“公平性”问题聚焦到“利用GenAI生成合成数据来克服真实数据限制，以评估皮肤病变分类器的公平性”。\n*   **逻辑与依据：**\n    *   医疗AI公平性是当前热点与痛点，尤其是数据不足和隐私限制。\n    *   GenAI技术成熟，能够生成高保真且可控的图像。\n    *   R1和R2的设定，使得研究目标清晰、可量化、可验证。R1是技术可行性验证，R2是应用有效性验证，两者互为前置和结果。\n\n**阶段二：生成模型选择与训练数据准备**\n\n*   **重要决策1：** 选择SOTA的LightningDiT作为生成模型核心。\n    *   **逻辑与依据：** 确保生成图像的**质量和真实性**是医疗应用的基础。LightningDiT在性能和效率上的优势使其成为最佳选择。此外，其强大的**文本条件生成能力**是实现PII属性控制的根本。\n*   **重要决策2：** 使用ISIC所有可用的带PII元数据的真实图像作为生成模型的训练数据，并将其PII信息转化为文本提示。\n    *   **逻辑与依据：** ISIC数据集的**规模和元数据丰富性**无可比拟，是训练高质量、多样化、可控生成模型的最佳基础。将PII信息直接编码为文本提示，确保了我们能够精确地控制合成图像的特定属性（如生成“男性、70岁、Fitzpatrick VI型皮肤、黑色素瘤”的图像）。\n\n**阶段三：合成数据生成与评估模型选择**\n\n*   **重要决策1：** 设计系统化的文本提示，为所有PII组合（性别、年龄、肤色类型、疾病）生成**均衡数量**（每组100张）的合成图像，构建一个完全平衡的测试集。\n    *   **逻辑与依据：** 传统公平性评估受限于真实数据的不平衡。通过生成完全平衡的测试集，我们可以消除真实数据分布偏差的影响，更清晰地观察模型对不同群体的**内在偏见**。这直接回答了R1中“obtaining a balanced fairness assessment dataset”的目标。\n*   **重要决策2：** 基于**预训练模型的公开可用性**，选择了DeepGuide、MelaNet和SkinLesionDensenet进行评估。\n    *   **逻辑与依据：** 现实世界的AI系统往往是预训练的。评估现有模型更具实践意义。最关键的是，我们发现这些模型的训练数据集存在差异（HAM vs. ISIC）。\n    *   **关键洞察与决策延伸：** 我们没有将训练数据集差异视为障碍，而是将其视为一个**额外的实验条件**。这使得我们的研究不仅能评估公平性，还能探索**合成数据在揭示模型对“数据集漂移”鲁棒性方面的潜力**。例如，如果基于ISIC训练的生成模型合成的数据去评估基于HAM训练的分类器时性能下降，这表明了合成数据可以作为一种诊断工具，揭示模型在面对训练时未见过的（但通过合成器模拟的）数据分布时的表现。这一发现拓展了合成数据在“可信AI”中的应用范围。\n\n**阶段四：结果分析与讨论**\n\n*   **重要决策：** 坦诚地报告合成图像的质量，并深入分析评估模型表现差异的原因，特别是数据集漂移的影响。\n    *   **逻辑与依据：**\n        *   **质量：** 承认“有些图像（如男性、10岁、肤色I型）看起来不太真实”是必要的。这表明模型在训练数据稀疏的极端组合上仍有提升空间，也是未来研究的方向。\n        *   **公平性评估结果：** 直接呈现DP值，量化了不同模型在不同PII维度上的公平性表现。这直接回答了R2中“reliably assess the fairness”的目标。\n        *   **原因分析：** 发现DeepGuide与MelaNet/SkinLesionDensenet在训练数据集上的差异（HAM vs. ISIC）是导致性能表现不同的主要原因。这不仅解释了结果，更深化了研究的价值：合成数据不仅能评估“公平性”，还能评估“鲁棒性”，即模型在面对与其训练数据分布不同的数据时的性能。这对于部署在多样化真实环境中的医疗AI至关重要。\n\n通过上述思维决策过程，我们不仅实现了利用GenAI生成平衡的皮肤病变图像数据集，并成功地用其评估了现有分类器的公平性，更重要的是，我们意外地发现并揭示了合成数据在诊断模型鲁棒性（尤其是在数据集漂移下）方面的潜力，这为未来可信医疗AI的研究开辟了新的视角。"
    },
    {
        "order": 78,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17869",
        "abs_url": "https://arxiv.org/abs/2507.17869",
        "pdf_url": "https://arxiv.org/pdf/2507.17869",
        "title": "Integrating Feature Selection and Machine Learning for Nitrogen Assessment in Grapevine Leaves using In-Field Hyperspectral Imaging",
        "authors": [
            "Atif Bilal Asad",
            "Achyut Paudel",
            "Safal Kshetri",
            "Chenchen Kang",
            "Salik Ram Khanal",
            "Nataliya Shcherbatyuk",
            "Pierre Davadant",
            "R. Paul Schreiner",
            "Santosh Kalauni",
            "Manoj Karkee",
            "Markus Keller"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Nitrogen (N) is one of the most crucial nutrients in vineyards, affecting plant growth and subsequent products such as wine and juice. Because soil N has high spatial and temporal variability, it is desirable to accurately estimate the N concentration of grapevine leaves and manage fertilization at the individual plant level to optimally meet plant needs. In this study, we used in-field hyperspectral images with wavelengths ranging from $400 to 1000nm of four different grapevine cultivars collected from distinct vineyards and over two growth stages during two growing seasons to develop models for predicting N concentration at the leaf-level and canopy-level. After image processing, two feature selection methods were employed to identify the optimal set of spectral bands that were responsive to leaf N concentrations. The selected spectral bands were used to train and test two different Machine Learning (ML) models, Gradient Boosting and XGBoost, for predicting nitrogen concentrations. The comparison of selected bands for both leaf-level and canopy-level datasets showed that most of the spectral regions identified by the feature selection methods were across both methods and the dataset types (leaf- and canopy-level datasets), particularly in the key regions, 500-525nm, 650-690nm, 750-800nm, and 900-950nm. These findings indicated the robustness of these spectral regions for predicting nitrogen content. The results for N prediction demonstrated that the ML model achieved an R square of 0.49 for canopy-level data and an R square of 0.57 for leaf-level data, despite using different sets of selected spectral bands for each analysis level. The study demonstrated the potential of using in-field hyperspectral imaging and the use of spectral data in integrated feature selection and ML techniques to monitor N status in vineyards.",
        "gemini2.5flash": "",
        "overall_idea": "非常荣幸能以本文作者的身份，深入剖析这项研究背后的思维决策过程。这项工作凝聚了团队数年的努力和思考，旨在将先进的遥感技术转化为葡萄园氮管理实践的有效工具。\n\n### 0. 为什么选择这个研究内容？\n\n作为一名农业工程师和研究者，我深知作物营养管理对于农业生产的重要性。氮（N）是葡萄藤生长、果实品质形成的关键元素，无论是过多还是过少，都会对产量和葡萄酒品质产生负面影响。传统的氮检测方法，如叶片组织化学分析，虽然精确，但存在显而易见的局限性：\n1.  **破坏性（Destructive）**：需要采摘叶片，无法对同一植株进行持续监测。\n2.  **耗时费力（Labor-intensive and Time-consuming）**：从样品采集到实验室分析出结果，往往需要几天甚至几周，无法实现实时决策。\n3.  **缺乏空间分辨率（Limited Spatial Resolution）**：通常只采集少量代表性样品，难以捕捉葡萄园内部精细的氮空间变异性。\n\n这些限制使得葡萄园精准氮管理（如变量施肥）难以有效实施。因此，我一直关注如何利用非破坏性、快速、高空间分辨率的技术来解决这个问题。高光谱成像（Hyperspectral Imaging, HSI）以其获取数百个连续窄波段光谱信息的能力，天然地成为了一个有前景的解决方案。它能捕捉到植物生理生化状态的细微变化，这些变化往往与养分含量密切相关。\n\n我的核心驱动力就是：**能否开发一套基于高光谱成像的非破坏性、快速、且能反映葡萄园内部氮素空间变异的评估系统，从而支持更精准的养分管理？** 这个问题的回答将直接影响葡萄园的经济效益和环境可持续性。\n\n### 1. 如何一步步构思和界定研究问题的？\n\n研究问题的构思是一个从宏观到微观、从概念到实践的迭代过程。\n\n*   **第一步：初步设想**\n    *   最初的想法很简单：高光谱能测氮吗？文献调研显示，是的，在玉米、小麦等作物上已有尝试。但葡萄藤的特性（复杂的冠层结构、不同品种、不同生长阶段的生理差异）会带来独特的挑战。\n    *   **问题界定（初版）**：使用高光谱技术评估葡萄藤叶片氮含量。\n\n*   **第二步：考虑实际应用场景**\n    *   葡萄园管理通常涉及两个层面：**叶片层面**（精细研究生理机制）和**冠层层面**（实际管理决策）。只研究叶片可能不够，因为最终要应用到整个植株。\n    *   **问题界定（修订版）**：开发基于高光谱图像的叶片和冠层氮含量预测模型。\n\n*   **第三步：解决高光谱数据本身的挑战**\n    *   高光谱数据的一个主要特点是**高维度（High Dimensionality）**。数百个波段意味着巨大的数据量，带来存储、处理的挑战。更重要的是，其中许多波段是冗余的或与目标变量（氮含量）相关性不强，这会导致著名的“**Hughes效应**”（即训练样本数量固定时，特征维度的增加反而会降低模型性能）。\n    *   **我的思考**：我们不能简单地把所有波段都丢进模型。必须进行**特征选择（Feature Selection, FS）**。选择哪些方法？仅仅是相关性排序就够了吗？不同波段之间的高度相关性怎么办？\n    *   **问题界定（进一步修订）**：在叶片和冠层氮含量预测中，整合**特征选择方法**，以识别对氮响应最敏感、最informative的光谱波段，并去除冗余特征。\n\n*   **第四步：确保模型的鲁棒性和泛化能力**\n    *   许多实验室研究是在单一品种、受控条件下进行的。但在实际葡萄园中，有多种品种、不同土壤和气候环境、以及不同生长季节。如果模型只能在特定条件下工作，其应用价值将大打折扣。\n    *   **我的思考**：数据采集必须足够多样化。我们需要跨品种、跨季节、跨地点的数据。同时，模型选择也需要考虑能处理复杂非线性关系、且具有良好泛化能力的机器学习（ML）算法。\n    *   **问题界定（最终版，也是本文的核心）**：开发一套集**特征选择**和**机器学习**于一体的管道，利用**跨品种、跨生长阶段、跨年份、跨地理位置**的田间高光谱图像，实现葡萄藤叶片和冠层氮含量的**非破坏性、鲁棒性预测**，并识别出**关键的诊断光谱波段**，以期指导未来更简化的多光谱传感器开发。\n\n**一个具体的例子： Hughes效应和数据冗余**\n在研究初期，我们可能会直接将所有274个波段的反射率数据作为特征输入到机器学习模型中。然而，很快就会发现模型性能不佳，甚至出现过拟合。这就是Hughes效应的体现：高维度特征空间中，数据点变得稀疏，模型更容易记住训练数据而非学习泛化规律。\n更进一步，很多相邻的波段往往高度相关，它们传递了非常相似甚至重复的信息。例如，670nm和671nm波段的反射率值可能几乎完全一样。保留这些冗余特征不仅浪费计算资源，还会干扰模型的学习过程，增加过拟合的风险。\n因此，我们面临的核心问题不仅仅是“如何预测N”，更是“**如何从海量高光谱数据中高效、准确地提取出与N含量最相关的、非冗余的信息**”。这直接催生了我们对“冗余特征去除”和“多方法集成特征选择”的重视。\n\n### 2. 在选择理论框架、研究方法和数据分析策略时的思考、权衡和取舍\n\n这个阶段充满了权衡，每个决策都旨在平衡模型的性能、鲁棒性、可解释性和实际应用可行性。\n\n*   **数据采集：田间原位（In-field） vs. 实验室（Lab）**\n    *   **思考**：实验室数据噪音低、控制性好，可能得到更高的R²。但田间数据才能真正反映实际环境（光照变化、叶片角度、风动等），其结果更具推广性。\n    *   **权衡/取舍**：我们选择**田间原位采集**。尽管会引入更多环境噪音，但这是为了最终的实用性。为了应对噪音，我们在图像处理阶段加入了反射率校正和数据预处理（如SNV和Savitzky-Golay平滑）。\n    *   **具体实践**：使用地面三脚架固定相机，尽量保持与目标叶片/冠层的距离一致，并同步采集白板和暗电流数据进行校正。\n\n*   **数据预处理：如何处理高维冗余**\n    *   **思考**：单纯的平滑或去除首尾波段不足以解决高维冗余问题。我们需要一个系统的方法来减少特征数量，同时保留关键信息。\n    *   **权衡/取舍**：\n        1.  **初始波段去除**：观察到光谱相机数据在波段两端（400nm以下和950nm以上）噪声较大。经验性地去除**前10个和后2个波段**，这是基于信号质量的直观决策。\n        2.  **冗余特征去除（层次聚类）**：这是解决“冗余”的关键一步。通过计算波段间的相关性，然后进行层次聚类。\n            *   **思考**：如果两个波段高度相关（例如相关系数0.99），它们本质上是重复信息。我们只需要保留其中一个作为代表。\n            *   **决策**：使用**层次聚类（Hierarchical Clustering）**。设置一个距离阈值（我们尝试了不同值，最终选择了0.08，因为它能在显著降低特征数量的同时，尽可能保留不同信息）。在每个聚类中，选择与目标变量（N含量）相关性最高的波段作为该聚类的代表。\n            *   **优势**：这种方法比简单地基于单一相关性排序更智能，它确保了最终选择的特征集是“多样化”且“非冗余”的。\n\n*   **特征选择方法：单一方法 vs. 集成方法**\n    *   **思考**：业界有多种特征选择方法，如Lasso、随机森林、SelectKBest等，每种都有其侧重和局限性。单一方法可能会有偏好，导致结果不稳定。\n    *   **权衡/取舍**：\n        1.  **偏最小二乘回归（PLSR）**：这是一种在化学计量学中常用的方法，它通过最大化自变量（光谱）和因变量（N含量）之间的协方差来识别最重要的潜在变量。PLSR的回归系数可以用来衡量每个波段的重要性。我们使用**迭代排除法**来优化PLSR的选择，即逐步移除最不重要的特征，直到交叉验证误差最小。\n        2.  **集成特征选择（Ensemble Feature Selection）**：这是我们方法的核心创新之一。我们将多种主流的特征选择算法（SelectKBest, Lasso Regression, Ridge Regression, Random Forest Regressor, Extra Trees Regressor, Gradient Boosting Regressor）结合起来。\n            *   **思考**：每种方法在数据子集上得到的特征重要性排名可能不同。将它们的结果“投票”或“平均”起来，可以得到一个更稳定、更鲁棒的特征重要性排序。\n            *   **决策**：对数据进行**动态分割（Dynamic Data Splitting）**，重复50次训练-验证循环，每次都用90%的数据训练，并用多种特征选择算法计算特征重要性。然后将所有算法、所有迭代的特征重要性**平均**并**排序**。最后，通过**留一交叉验证（LOOCV）**结合简单的多元线性回归模型来确定最佳特征数量（即R²不再显著提升或RMSE不再显著下降时的特征数）。\n            *   **优势**：极大地增强了特征选择的鲁棒性，减少了对特定数据子集或单一算法的依赖，提高了模型在未见数据上的泛化能力。\n    *   **为什么同时使用PLSR和集成FS？** 这是一个有意为之的决策。PLSR提供了一种基于回归的线性视角，而集成方法则结合了多种视角（包括非线性）。如果两种截然不同的方法都共同选择了某些波段，那么这些波段的“重要性”和“稳定性”就得到了强有力的验证。这对于识别出未来可能用于定制多光谱传感器的**核心诊断波段**至关重要。\n\n*   **机器学习模型：线性 vs. 非线性，简单 vs. 复杂**\n    *   **思考**：光谱反射率与植物养分含量之间的关系通常是非线性的。简单的线性模型可能无法捕捉这些复杂的模式。\n    *   **权衡/取舍**：我们选择了两种强大的**集成学习模型：XGBoost和Gradient Boosting**。\n        *   **优势**：它们在处理复杂非线性关系、高维数据以及防止过拟合方面表现卓越。它们通过组合多个弱学习器（决策树）来构建一个强大的预测模型。\n        *   **决策**：使用**GridSearchCV**进行系统性的**超参数调优**，以找到每种模型在给定数据集上的最优配置。\n        *   **为什么不是更复杂的深度学习？** 尽管深度学习在图像识别方面强大，但考虑到我们的数据样本量相对有限（相对于数万甚至数十万的图像），且我们更侧重于识别**特定波段**而非端到端的图像特征学习，集成学习模型在训练效率和可解释性（某种程度上）上更具优势。这也是一种计算成本与性能的平衡。\n\n*   **评估策略：性能指标和交叉验证**\n    *   **思考**：如何确保模型的性能评估是可靠的，且能避免过拟合？\n    *   **决策**：\n        1.  **指标**：R²（决定系数）、RMSE（均方根误差）、MAE（平均绝对误差）。这些是回归任务的标准评估指标。\n        2.  **验证策略**：**10折交叉验证（10-fold Cross-Validation），重复10次**。这意味着数据会被分成10份，每次用9份训练，1份验证，这个过程重复10次，再将整个10折过程重复10遍。\n        *   **优势**：这种严格的交叉验证策略可以最大程度地评估模型的**泛化能力**和**稳定性**，确保结果不是偶然的，从而提高研究结论的可靠性。\n\n### 3. 研究每个关键阶段的重要决策及逻辑依据\n\n我将通过**“关键决策点”**来回顾。\n\n*   **关键决策点1：确立“双尺度”研究（叶片级 vs. 冠层级）**\n    *   **逻辑和依据**：\n        *   **叶片级**：更接近生物化学本质。叶片的光谱反射率直接反映了叶绿素、水分、蛋白质等组分。在受控条件下，叶片级数据能提供最直接、最清晰的N信号。这对于理解N如何影响光谱响应是基础。\n        *   **冠层级**：更贴近实际生产管理。农民关注的是整个葡萄藤的健康状况，而非单片叶子。冠层数据受光照、叶片角度、阴影、背景土壤/藤蔓等多种因素影响，复杂性高，噪音大。但如果能在冠层级实现有效预测，其应用价值巨大。\n        *   **决策**：同时进行两个层面的分析和模型开发，并进行比较。这不仅提供了全面的洞察，也为未来根据实际应用需求选择合适的传感器和数据处理策略奠定了基础。我们发现叶片级预测R²更高（0.57），而冠层级稍低（0.49），这与预期的复杂性差异一致。但即便冠层级R²不高，也足以用于识别“高氮区”和“低氮区”，支持精准施肥。\n\n*   **关键决策点2：设计多阶段的特征选择管道（冗余去除 -> 集成FS -> PLSR）**\n    *   **逻辑和依据**：这是应对高光谱数据挑战的核心。\n        *   **挑战**：如前所述，高光谱数据维度高（274个波段），且波段间高度相关（冗余），同时存在Hughes效应。\n        *   **决策**：\n            1.  **层次聚类进行冗余去除**：这是第一道关卡。例如，我们发现500-510nm之间可能有很多波段，它们对N的响应模式非常相似。通过聚类，我们只需要从这个范围中选一个代表波段，而非全部保留。这大大简化了后续特征选择的计算负担，并确保了所选特征的“独立性”。\n            2.  **集成特征选择**：这是主打。单一FS方法可能受其算法偏好或特定数据子集的影响。通过结合多种算法的“智慧”（如Lasso的稀疏性、随机森林的重要性评估、梯度提升的误差关注），并在**多个随机数据子集上重复**，我们得到的特征重要性排名更为稳定和可靠。这就好比一个专家委员会共同投票决定最重要的特征，而非一个专家说了算。\n            3.  **PLSR作为验证和补充**：PLSR是经典的化学计量学方法，其优势在于能够处理多重共线性，并能从不同角度给出波段重要性。\n        *   **最终产物**：通过集成FS和PLSR共同选出的8个“共同波段”（如表4所示）。这些波段是经过严格筛选和验证的，具有很强的鲁棒性，它们是未来开发定制化、低成本多光谱传感器的**黄金候选者**。例如，500-525nm（蓝绿光）和650-690nm（红光）范围，这些区域与叶绿素吸收密切相关；以及750-800nm（红边）和900-950nm（近红外），与植物水含量和结构信息相关，这些都是N变化可能影响的光谱区域。\n\n*   **关键决策点3：选择XGBoost和Gradient Boosting作为最终预测模型**\n    *   **逻辑和依据**：\n        *   **N与光谱的非线性关系**：植物生理过程复杂，N含量与光谱反射率之间的关系并非简单线性。传统的线性回归（如纯PLSR）可能无法完全捕捉。\n        *   **模型性能与鲁棒性**：XGBoost和Gradient Boosting是当下非常强大的集成学习算法，它们在许多实际预测任务中表现出色，尤其擅长处理非线性关系和复杂特征交互。它们的内建正则化机制也有助于防止过拟合。\n        *   **决策**：放弃更简单的线性模型作为最终预测器，拥抱更强大的集成学习模型。尽管它们“黑箱”一些，但其预测能力更符合我们对精准度的追求。通过**GridSearchCV**精细调参，确保模型在所选特征集上达到最佳性能。\n\n*   **关键决策点4：综合解释结果，强调“实用价值”而非“完美预测”**\n    *   **逻辑和依据**：田间数据总有噪音，不可能达到实验室数据的完美R²。例如，我们冠层级模型的R²约为0.49。\n    *   **决策**：不回避这个数字，而是**深入分析其背后的原因**（如环境光照、叶片角度、N与其他养分的交叉影响等）。同时，**强调其在实际应用中的价值**。\n        *   R²=0.49也许不能精确预测每一片叶子的N含量，但它足以区分出“高氮区域”和“低氮区域”，这对于指导葡萄园的“变量施肥”至关重要。例如，通过生成N含量图，我们可以看到哪些地块的葡萄藤急需补氮，哪些则无需施肥，从而避免过度施肥造成浪费和环境污染。\n        *   我们还强调了**跨品种、跨季节、跨地点**数据的结合，这使得模型更具**泛化能力**，这是许多现有研究的不足之处。这种鲁棒性比单一的高R²更重要，因为它决定了模型能否在真实世界中被广泛采纳。\n\n综上所述，这项研究并非简单地应用现有技术，而是在充分理解葡萄园N管理痛点和高光谱数据特性的基础上，通过一系列深思熟虑的决策和权衡，构建了一套从数据采集、预处理、特征选择到模型预测的完整解决方案。核心理念是**“从复杂中提炼简单，从噪音中发现价值”**，最终目标是为精准农业提供一个非破坏性、高效且实用的工具。"
    },
    {
        "order": 79,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17881",
        "abs_url": "https://arxiv.org/abs/2507.17881",
        "pdf_url": "https://arxiv.org/pdf/2507.17881",
        "title": "A Supervised Machine Learning Framework for Multipactor Breakdown Prediction in High-Power Radio Frequency Devices and Accelerator Components: A Case Study in Planar Geometry",
        "authors": [
            "Asif Iqbal",
            "John Verboncoeur",
            "Peng Zhang"
        ],
        "comments": "",
        "subjects": "Accelerator Physics (physics.acc-ph); Machine Learning (cs.LG); Applied Physics (physics.app-ph); Plasma Physics (physics.plasm-ph)",
        "abstract": "Multipactor is a nonlinear electron avalanche phenomenon that can severely impair the performance of high-power radio frequency (RF) devices and accelerator systems. Accurate prediction of multipactor susceptibility across different materials and operational regimes remains a critical yet computationally intensive challenge in accelerator component design and RF engineering. This study presents the first application of supervised machine learning (ML) for predicting multipactor susceptibility in two-surface planar geometries. A simulation-derived dataset spanning six distinct secondary electron yield (SEY) material profiles is used to train regression models - including Random Forest (RF), Extra Trees (ET), Extreme Gradient Boosting (XGBoost), and funnel-structured Multilayer Perceptrons (MLPs) - to predict the time-averaged electron growth rate, ${\\delta}_{avg}$. Performance is evaluated using Intersection over Union (IoU), Structural Similarity Index (SSIM), and Pearson correlation coefficient. Tree-based models consistently outperform MLPs in generalizing across disjoint material domains. MLPs trained using a scalarized objective function that combines IoU and SSIM during Bayesian hyperparameter optimization with 5-fold cross-validation outperform those trained with single-objective loss functions. Principal Component Analysis reveals that performance degradation for certain materials stems from disjoint feature-space distributions, underscoring the need for broader dataset coverage. This study demonstrates both the promise and limitations of ML-based multipactor prediction and lays the groundwork for accelerated, data-driven modeling in advanced RF and accelerator system design.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 80,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17893",
        "abs_url": "https://arxiv.org/abs/2507.17893",
        "pdf_url": "https://arxiv.org/pdf/2507.17893",
        "title": "Action-List Reinforcement Learning Syndrome Decoding for Binary Linear Block Codes",
        "authors": [
            "Milad Taghipour",
            "Bane Vasic"
        ],
        "comments": "",
        "subjects": "Information Theory (cs.IT); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "This paper explores the application of reinforcement learning techniques to enhance the performance of decoding of linear block codes based on flipping bits and finding optimal decisions. We describe the methodology for mapping the iterative decoding process into Markov Decision Processes (MDPs) and propose different methods to reduce the number of states in the MDP. A truncated MDP is proposed to reduce the number of states in the MDP by learning a Hamming ball with a specified radius around codewords. We then propose a general scheme for reinforcement learning based decoders applicable to any class of codes to improve the performance of decoders. We call this scheme an action-list decoding. We design an action-list decoder based on the Deep-Q network values that substantially enhance performance. We also get benefit of automorphism group of code to further improve the code performance. Additionally, we propose a feedback-based method to exploit and enhance the performance of existing high-performing decoders by applying reinforcement learning algorithms after the existing decoders. These approaches effectively reduces the complexity of the reinforcement learning block. Finally, we present experimental results for the Low-Density Parity Check (LDPC) codes over the Binary Symmetric Channel (BSC) to demonstrate the efficiency of the proposed methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 81,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17897",
        "abs_url": "https://arxiv.org/abs/2507.17897",
        "pdf_url": "https://arxiv.org/pdf/2507.17897",
        "title": "Multimodal Recurrent Ensembles for Predicting Brain Responses to Naturalistic Movies (Algonauts 2025)",
        "authors": [
            "Semih Eren",
            "Deniz Kucukahmetler",
            "Nico Scherf"
        ],
        "comments": "8 pages, 2 figures, 1 table. Invited report, CCN 2025 Algonauts Project session (3rd-place team). Code: this https URL",
        "subjects": "Neurons and Cognition (q-bio.NC); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Accurately predicting distributed cortical responses to naturalistic stimuli requires models that integrate visual, auditory and semantic information over time. We present a hierarchical multimodal recurrent ensemble that maps pretrained video, audio, and language embeddings to fMRI time series recorded while four subjects watched almost 80 hours of movies provided by the Algonauts 2025 challenge. Modality-specific bidirectional RNNs encode temporal dynamics; their hidden states are fused and passed to a second recurrent layer, and lightweight subject-specific heads output responses for 1000 cortical parcels. Training relies on a composite MSE-correlation loss and a curriculum that gradually shifts emphasis from early sensory to late association regions. Averaging 100 model variants further boosts robustness. The resulting system ranked third on the competition leaderboard, achieving an overall Pearson r = 0.2094 and the highest single-parcel peak score (mean r = 0.63) among all participants, with particularly strong gains for the most challenging subject (Subject 5). The approach establishes a simple, extensible baseline for future multimodal brain-encoding benchmarks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 82,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17921",
        "abs_url": "https://arxiv.org/abs/2507.17921",
        "pdf_url": "https://arxiv.org/pdf/2507.17921",
        "title": "Sliding Window Informative Canonical Correlation Analysis",
        "authors": [
            "Arvind Prasadan"
        ],
        "comments": "22 pages, submitted",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Image and Video Processing (eess.IV); Statistics Theory (math.ST); Computation (stat.CO); Methodology (stat.ME)",
        "abstract": "Canonical correlation analysis (CCA) is a technique for finding correlated sets of features between two datasets. In this paper, we propose a novel extension of CCA to the online, streaming data setting: Sliding Window Informative Canonical Correlation Analysis (SWICCA). Our method uses a streaming principal component analysis (PCA) algorithm as a backend and uses these outputs combined with a small sliding window of samples to estimate the CCA components in real time. We motivate and describe our algorithm, provide numerical simulations to characterize its performance, and provide a theoretical performance guarantee. The SWICCA method is applicable and scalable to extremely high dimensions, and we provide a real-data example that demonstrates this capability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 83,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17931",
        "abs_url": "https://arxiv.org/abs/2507.17931",
        "pdf_url": "https://arxiv.org/pdf/2507.17931",
        "title": "Quantum Machine Learning Playground",
        "authors": [
            "Pascal Debus",
            "Sebastian Issel",
            "Kilian Tscharke"
        ],
        "comments": "Accepted to IEEE Computer Graphics and Applications. Final version: this https URL",
        "subjects": "Quantum Physics (quant-ph); Graphics (cs.GR); Machine Learning (cs.LG)",
        "abstract": "This article introduces an innovative interactive visualization tool designed to demystify quantum machine learning (QML) algorithms. Our work is inspired by the success of classical machine learning visualization tools, such as TensorFlow Playground, and aims to bridge the gap in visualization resources specifically for the field of QML. The article includes a comprehensive overview of relevant visualization metaphors from both quantum computing and classical machine learning, the development of an algorithm visualization concept, and the design of a concrete implementation as an interactive web application. By combining common visualization metaphors for the so-called data re-uploading universal quantum classifier as a representative QML model, this article aims to lower the entry barrier to quantum computing and encourage further innovation in the field. The accompanying interactive application is a proposal for the first version of a quantum machine learning playground for learning and exploring QML models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 84,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17963",
        "abs_url": "https://arxiv.org/abs/2507.17963",
        "pdf_url": "https://arxiv.org/pdf/2507.17963",
        "title": "Zero-Shot Dynamic Concept Personalization with Grid-Based LoRA",
        "authors": [
            "Rameen Abdal",
            "Or Patashnik",
            "Ekaterina Deyneka",
            "Hao Chen",
            "Aliaksandr Siarohin",
            "Sergey Tulyakov",
            "Daniel Cohen-Or",
            "Kfir Aberman"
        ],
        "comments": "Project Page and Video : this https URL",
        "subjects": "Graphics (cs.GR); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Recent advances in text-to-video generation have enabled high-quality synthesis from text and image prompts. While the personalization of dynamic concepts, which capture subject-specific appearance and motion from a single video, is now feasible, most existing methods require per-instance fine-tuning, limiting scalability. We introduce a fully zero-shot framework for dynamic concept personalization in text-to-video models. Our method leverages structured 2x2 video grids that spatially organize input and output pairs, enabling the training of lightweight Grid-LoRA adapters for editing and composition within these grids. At inference, a dedicated Grid Fill module completes partially observed layouts, producing temporally coherent and identity preserving outputs. Once trained, the entire system operates in a single forward pass, generalizing to previously unseen dynamic concepts without any test-time optimization. Extensive experiments demonstrate high-quality and consistent results across a wide range of subjects beyond trained concepts and editing scenarios.",
        "gemini2.5flash": "",
        "overall_idea": "非常荣幸能以作者的身份，深入剖析这篇名为《基于网格LoRA的零样本动态概念个性化》的研究论文。回顾我们从最初的构想到最终实现的全过程，这其中充满了挑战与创新。\n\n### 0. 为什么选择这个研究内容？ (选题背景与核心动机)\n\n在文本到视频（T2V）生成领域，高质量视频的合成已经取得了显著进展，但我们很快意识到一个核心瓶颈：**个性化**。用户往往不只是想要生成一个随机的视频，他们更希望视频中的人物、物体或效果能拥有特定的外观和运动特征，比如“让视频里这个人脸上冒烟，同时背景有紫色灯光闪烁”。\n\n当时的现有方法主要依赖于：\n1.  **逐实例微调（Per-instance Fine-tuning）**：例如将图像个性化中的DreamBooth方法扩展到视频领域。但这意味着每次引入一个新的个性化概念（比如一个新的角色），都需要对整个大型模型进行一次耗时的训练。这在计算上非常昂贵，且难以扩展，更无法实现“零样本”能力——即对未训练过的新概念也能直接应用。\n2.  **运动重定向或测试时优化（Motion Retargeting or Test-time Optimization）**：这些方法虽然能在一定程度上修改视频，但往往缺乏对主体身份的精确保持，或者生成的效果僵硬，无法与主体进行自然的“动态交互”。\n\n我们团队在先前的研究（Abdal et al. 2025）中提出了“动态概念（Dynamic Concepts）”的概念，通过训练小型适配器（如LoRA）来捕获单一视频中特定主体的外观和运动。这向前迈进了一大步，但仍停留在“逐概念训练”的阶段，远未达到我们理想中的灵活、可扩展性。\n\n**我们的核心动机就是：能否找到一种方法，在保持高保真度和时间连贯性的前提下，实现对T2V模型的“零样本”动态概念个性化，并且整个过程是“前向推理”的，无需任何测试时优化？** 这就像我们希望拥有一个“通用视频编辑器”，用户只需要简单描述，就能对任何未见过的主体施加个性化的动态效果，而无需为每个新主体重新训练一个工具。\n\n### 1. 您是如何一步步构思和界定研究问题的？ (问题界定与挑战)\n\n基于上述动机，我们将研究问题逐步细化，并界定了关键的挑战：\n\n*   **挑战一：零样本泛化能力**。如何在模型训练时仅学习到通用的“转换规则”，而非具体的“概念实例”，从而在推理时能够应用于完全未见过的动态概念？这是一个从“学习实例”到“学习范式”的转变。\n*   **挑战二：前向推理**。为了实用性和效率，我们拒绝了任何形式的测试时优化。这意味着所有个性化和编辑必须在一个单次的前向计算中完成。\n*   **挑战三：动态概念的复杂性**。视频中的个性化不仅仅是静态的外观，更包含了运动特征。我们如何确保编辑后的概念（例如烟雾）能与主体（例如人物）的运动和姿态自然地交互，而不是简单地叠加？同时，还要保持主体本身的身份和运动连贯性。\n*   **挑战四：组合性**。能否支持将多个动态概念（例如“人物”与“雨景”，或“火花”与“人物”）进行语义和时空上的连贯组合？\n\n为了解决这些挑战，我们开始思考：人类是如何学习这种“转换规则”的？我们发现，人类通过对比“例子”来学习。比如，看到“原始图像 A”和“编辑后的图像 A’”，以及“原始图像 B”和“编辑后的图像 B’”，我们能推断出这是一种“添加烟雾”的编辑规则。这种“上下文学习（In-Context Learning）”的范式启发了我们。\n\n于是，我们构思了一种**“网格化”**的训练范式。如果我们将多个相关联的视频内容（例如，原始概念、编辑后的概念、组合后的概念）组织在一个结构化的2x2网格中，模型是否能从中学习到这些内容的内在关系和转换逻辑？\n\n**以“人物面部冒烟并有紫色灯光”这个例子来说**：\n问题被界定为：给定一个**全新的、未经训练的人的视频片段**（例如，放在网格的左上角），以及一个描述性文本提示“一个男人 [man_ID] 随着紫色灯光和烟雾起舞”，我们期望模型能在网格的右下角生成一个新的视频，其中：\n1.  生成的男人**仍然是原始视频中的那个人**（身份保持）。\n2.  男人的**舞蹈动作与原始视频一致**（运动保持）。\n3.  **烟雾和灯光效果与人物动态自然交互**（动态交互）。\n4.  整个生成过程是**一次性前向推理**完成的。\n5.  这个男人和烟雾、灯光的组合是**模型以前从未直接训练过**的。\n\n这个具体例子不仅帮助我们界定了问题，也为后续的方法设计指明了方向。\n\n### 2. 为了解决该问题，您在选择理论框架、研究方法和数据分析策略时，经历了怎样的思考、权衡和取舍。\n\n确定了问题后，我们开始构建解决方案。\n\n**理论框架选择：**\n\n*   **扩散模型（Diffusion Models）**：这是当时T2V生成领域最强大的基础模型。我们决定沿用这一主流框架，因为它在生成质量和多样性方面表现出色。我们选择了基于DiT（Diffusion Transformer）骨干网络和MAGVITv2自编码器的T2V模型，以利用其高效的潜空间操作和强大的生成能力。\n    *   **权衡**：扩散模型计算成本较高，但其生成质量和对复杂纹理的建模能力是实现高保真个性化视频的关键。我们选择在潜空间操作，并在后续进行上采样，以平衡效率和质量。\n*   **LoRA（Low-Rank Adaptation）**：LoRA已被证明是高效、轻量级的模型微调策略，尤其擅长个性化。在图像领域，DreamBooth和IC-LoRA等工作已经验证了LoRA在捕获特定概念方面的强大能力。我们之前的动态概念工作也利用了LoRA。\n    *   **权衡**：单一LoRA模块往往只能针对一个概念，导致扩展性差。这促使我们思考如何将多个概念压缩进一个LoRA。\n*   **上下文学习（In-Context Learning）**：这是实现零样本泛化的核心思想。我们借鉴了NLP和图像领域中通过提供示例对来指导模型学习新任务的思路。\n    *   **思考**：如何将上下文学习应用于视频生成？我们认为**“网格”**是一种自然且直观的上下文载体。通过将输入和期望输出以网格形式呈现，模型可以学习到数据之间的结构化关系。\n\n**研究方法（三大核心模块的设计与取舍）：**\n\n我们最终构思出**Multi Dynamic Concept (DC) LoRA、Grid LoRA和Grid-Fill LoRA**这三个相互关联的模块。\n\n1.  **Multi Dynamic Concept (DC) LoRA (模块1)**\n    *   **目的**：解决传统LoRA“逐概念训练”的瓶颈，实现单一LoRA对**多个**动态概念的捕获。\n    *   **思考与决策**：如何让一个LoRA同时学习不同人物、不同动作？我们决定引入独特的**标识符（如`[person_identity]`和`[action_motion]`）**作为文本提示的一部分，让LoRA在同一个权重空间中区分并编码不同的动态概念。此外，我们将LoRA的低秩更新矩阵拆分为**外观（`W_app`）**和**运动（`W_mot`）**两部分，以更好地解耦和捕获动态概念的复杂性。\n    *   **权衡**：这种统一的LoRA在捕获极端细粒度特征时可能不如逐概念LoRA完美，但它带来了巨大的可扩展性和泛化潜力。我们认为这种权衡是值得的。\n    *   **训练策略**：首先对这个模块进行训练并将其**冻结**。它成为了我们整个框架的“概念知识库”，为后续的编辑和组合提供基础的个性化能力。\n\n2.  **Grid LoRA (模块2)**\n    *   **目的**：学习网格结构中的概念组合和一致性，为后续的零样本操作提供“范式指导”。\n    *   **思考与决策**：我们选择**2x2视频网格**作为训练数据。网格的每个单元格都包含一个视频。训练时，顶行可以是原始概念，底行是经过编辑或组合后的概念。\n    *   **关键决策：结构化注意力掩码（Structured Attention Masking）**。这是防止“概念泄漏”的关键（如图7所示）。如果我们不加以限制，模型在看到网格中的多个视频时，可能会将不同概念的特征混淆。通过注意力掩码，我们确保每个单元格的查询（q）只关注特定的键（k）和值（v），从而强制模型学习**单元格之间预定义的关系**（例如，左下角单元格是左上角和右上角单元格的组合），而不是简单地混合所有信息。例如，当训练组合任务时，底层单元格的注意力可以同时关注顶部两个原始概念的特征，而顶部单元格则保持独立。\n    *   **训练策略**：在Multi DC LoRA之上进行微调。这一阶段的目标是让模型理解“网格内转换”的逻辑。\n    *   **权衡**：引入网格和注意力掩码增加了训练的复杂性，但这是实现零样本编辑和组合的根本。\n\n3.  **Grid-Fill LoRA (模块3)**\n    *   **目的**：在推理时，实现对部分观测网格的条件性填充，从而完成零样本编辑和组合。\n    *   **思考与决策**：为了实现前向推理，我们排除了迭代优化。我们需要一个**非自回归**的inpainting模块，能够一次性填充缺失的单元格。\n    *   **核心机制**：在训练时，我们**随机掩盖**2x2网格中的一个或多个单元格，然后训练Grid-Fill LoRA来重建这些被掩盖的部分。这个过程**条件化**在已经冻结的Multi DC LoRA权重上，以保持身份和运动的一致性。\n    *   **数据增强**：一个重要决策是使用**Grid LoRA生成的样本来增强Grid-Fill LoRA的训练数据集**。真实世界的网格数据（尤其是复杂组合）是有限的，通过Grid LoRA生成合成的“问题-解决方案”对，可以大大提高Grid-Fill LoRA的泛化能力（如表1的消融实验所示）。\n    *   **推理策略**：在推理时，我们可以将一个真实的视频片段（例如，要编辑的原始人物视频）放入网格的一个单元格中（例如，左上角），然后Grid-Fill LoRA将根据上下文和文本提示填充剩余的单元格，从而实现个性化编辑或组合。\n\n**数据分析策略：**\n\n*   **定量评估**：我们采用了CLIP-Text相似度（C-T）来衡量文本提示与生成视频的语义对齐程度，ArcFace身份相似度（ID）来评估身份保持能力，以及DINO-ViT-S/16图像编码器计算帧间余弦相似度（TC）来衡量时间连贯性。这些指标提供了客观、可量化的性能比较。\n*   **定性评估**：对于生成模型，视觉效果至关重要。我们通过展示大量视觉示例（图3、5、6、9）来直观地展示模型的编辑能力、交互效果和泛化能力。\n*   **用户研究**：为了克服定量指标可能无法捕捉到的主观感知（如生成视频的“真实感”、“自然度”和“整体偏好”），我们进行了用户研究，让参与者对我们方法和基线方法生成的视频进行两两比较并选择更优者。\n\n### 3. 在研究的每个关键阶段，您做出了哪些重要决策，并能解释这些决策背后的逻辑和依据。\n\n**决策1：统一动态概念LoRA (Multi DC LoRA) 而非逐概念训练。**\n*   **逻辑与依据**：这是我们解决扩展性问题的核心。传统LoRA方案为每个概念训练一个独立模块，导致概念越多，模型存储和加载的负担越大。通过在单个LoRA中通过概念标识符（如`[person_ID]`）来区分不同概念，并解耦外观和运动（`W_app`和`W_mot`），我们实现了跨概念的泛化能力，大大提升了实用性。它为所有后续操作提供了强大的、集成的概念个性化基础。\n\n**决策2：采用结构化2x2视频网格进行训练，并引入注意力掩码。**\n*   **逻辑与依据**：这是实现零样本泛化的关键突破口。我们认识到，要让模型泛化到未知概念，不能仅仅让它记住例子，而要让它学习“如何进行转换”。2x2网格作为一种上下文学习的载体，能够清晰地向模型展示“输入 -> 某种转换 -> 输出”的范式。\n    *   例如，网格左上角是原始人物，右下角是加烟雾的人物，模型就学习到了“加烟雾”这种转换。\n    *   **注意力掩码是确保这种学习有效且不混淆的关键**。如图7的“概念泄漏”问题所示，没有掩码，不同单元格的视频特征会相互污染。通过精确控制注意力流，我们强制模型在学习单元格间关系的同时，保持每个概念的独立性。这使得模型能真正理解“对A应用效果X，而不是把A和B混在一起”。\n\n**决策3：设计前向推理的网格填充模块 (Grid-Fill LoRA)。**\n*   **逻辑与依据**：为了实现高效的零样本编辑和组合，我们排除了测试时微调。Grid-Fill LoRA被设计成一个非自回归的inpainting模块，能够一次性填充网格中缺失的单元格。这意味着用户只需提供一个真实视频输入（例如网格的左上角），并指定期望的编辑或组合（通过文本提示），模型就能在**单次前向推理**中生成完成的视频网格。\n    *   训练时随机掩盖网格单元格，让模型学会根据上下文推断和填充缺失部分，这直接对应了推理时的“只给出部分信息，生成完整编辑”的需求。\n    *   **数据增强的决策**：我们发现，仅靠真实世界中的网格数据不足以训练出强大的泛化能力。因此，我们利用了Grid LoRA本身生成的大量合成网格样本来扩充训练集，这极大地提高了Grid-Fill LoRA对未知场景和编辑任务的适应性（消融实验证明了这一点）。\n\n**决策4：在个性化编辑中强调“动态交互”而非简单“叠加”。**\n*   **逻辑与依据**：仅仅在视频上叠加效果（如烟雾、雨滴）相对容易，但我们追求的是更高层次的“个性化动态概念交互”。例如，烟雾应该随着人物的动作而飘动、扩散，雨滴应该在人物衣物和头发上产生真实的水渍效果，灯光应该正确地照亮人物的面部轮廓并产生阴影。\n    *   这要求模型对场景的语义和物理特性有更深层次的理解。我们通过精细的文本提示（详细描述交互效果）和在训练中对流匹配损失（flow-matching loss）的强调，促使模型学习这种复杂的动态交互。这使得我们的生成结果远超简单的像素级修改，更具真实感和沉浸感。\n\n**具体例子：人物面部冒烟并有紫色灯光 (图6，右上角)**\n\n1.  **初始问题设定**：我们想对一个**特定**的人物视频（比如一个正在跳舞的男人），添加“紫色灯光”和“烟雾”的效果，并确保这些效果能**自然地围绕人物**，同时**保持人物原有的身份和舞蹈动作**。这个人物在训练中从未见过。\n\n2.  **Multi DC LoRA 的作用**：\n    *   在模型的最开始阶段，我们已经训练并冻结了Multi DC LoRA。这个LoRA学习了大量不同人物、不同动作、以及各种环境效果的**通用表征**。通过在文本提示中加入`[man_ID]`和`[motion_ID]`这样的概念标识符，Multi DC LoRA能够捕获到这个未经训练人物的独特身份和运动模式。它是整个个性化生成的基础。\n\n3.  **Grid LoRA 的训练范式 (为 Grid-Fill 做铺垫)**：\n    *   我们并未直接训练Grid LoRA来生成这个特定例子。相反，Grid LoRA被训练来理解**各种网格内部的转换关系**。例如，它见过这样的训练样本：\n        *   网格左上角：原始人物视频 (A)\n        *   网格右上角：原始人物视频 (A)\n        *   网格左下角：原始人物视频 (A) + 蓝色灯光\n        *   网格右下角：原始人物视频 (A) + 烟雾\n    *   通过这样的训练，Grid LoRA学会了如何根据输入视频和文本提示，在**保持主体身份和运动**的同时，**在不同单元格内应用不同的环境效果**。其中的**注意力掩码**至关重要，它确保了“添加蓝色灯光”和“添加烟雾”是独立且不混淆的转换，同时能识别出它们都作用在同一个主体A上。这些由Grid LoRA生成的样本，成为了Grid-Fill LoRA训练的重要数据增强。\n\n4.  **Grid-Fill LoRA 在推理时的核心作用**：\n    *   **输入准备**：当用户想要生成这个“人物+烟雾+紫色灯光”的视频时，我们首先将这个**原始人物的真实视频片段**（例如5秒）作为输入，放到一个2x2网格的特定单元格（比如左上角）。\n    *   **文本提示引导**：我们为左上角单元格提供提示，如“一个男人 [man_ID] 跳舞 [motion_ID]”。对于目标生成单元格（比如右下角），我们提供详细的编辑提示：“一个男人 [man_ID] 跳舞 [motion_ID] 伴随着紫色灯光和烟雾。”\n    *   **前向推理**：Grid-Fill LoRA接收这个部分填充的网格（左上角有真实视频，其他单元格为空），以及对应的文本提示。由于Grid-Fill LoRA在训练中见过了大量“原始视频到编辑视频”的网格转换示例（包括通过Grid LoRA生成的合成数据），并学会了在**保持身份和运动**的同时进行条件填充，它能够：\n        1.  从左上角的真实视频中提取该人物的身份 (`man_ID`) 和运动 (`motion_ID`) 特征，并通过**冻结的Multi DC LoRA**将这些个性化信息注入到生成过程中。\n        2.  根据右下角单元格的文本提示（“紫色灯光和烟雾”），利用它学习到的**转换规则**，在保持人物身份和运动的基础上，**动态地生成**与人物交互的紫色灯光和飘动的烟雾。烟雾会随着人物的舞动而自然扩散和卷曲，灯光也会根据人物的姿态和面部结构进行光照，而不是简单地叠加。\n    *   **输出**：最终，模型在右下角生成了一个全新的视频片段，其中人物身份和动作完美保持，而烟雾和紫色灯光则以高度逼真、动态交互的方式呈现。整个过程**只进行了一次前向计算**，无需任何测试时优化，实现了真正的零样本个性化。\n\n通过这种分阶段、模块化的设计，我们成功地将复杂的零样本动态概念个性化问题拆解为可管理的子问题，并通过创新的网格训练范式和注意力控制，实现了在保持高保真度和时间连贯性基础上的强大泛化能力。"
    },
    {
        "order": 85,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.17980",
        "abs_url": "https://arxiv.org/abs/2507.17980",
        "pdf_url": "https://arxiv.org/pdf/2507.17980",
        "title": "Machine Learning Workflow for Analysis of High-Dimensional Order Parameter Space: A Case Study of Polymer Crystallization from Molecular Dynamics Simulations",
        "authors": [
            "Elyar Tourani",
            "Brian J. Edwards",
            "Bamin Khomami"
        ],
        "comments": "30 pages, 8 figures, 1 table",
        "subjects": "Computational Physics (physics.comp-ph); Machine Learning (cs.LG)",
        "abstract": "Currently, identification of crystallization pathways in polymers is being carried out using molecular simulation-based data on a preset cut-off point on a single order parameter (OP) to define nucleated or crystallized regions. Aside from sensitivity to cut-off, each of these OPs introduces its own systematic biases. In this study, an integrated machine learning workflow is presented to accurately quantify crystallinity in polymeric systems using atomistic molecular dynamics data. Each atom is represented by a high-dimensional feature vector that combines geometric, thermodynamic-like, and symmetry-based descriptors. Low dimensional embeddings are employed to expose latent structural fingerprints within atomic environments. Subsequently, unsupervised clustering on the embeddings identified crystalline and amorphous atoms with high fidelity. After generating high quality labels with multidimensional data, we use supervised learning techniques to identify a minimal set of order parameters that can fully capture this label. Various tests were conducted to reduce the feature set, demonstrating that using only three order parameters is sufficient to recreate the crystallization labels. Based on these observed OPs, the crystallinity index (C-index) is defined as the logistic regression model's probability of crystallinity, remaining bimodal throughout the process and achieving over 0.98 classification performance (AUC). Notably, a model trained on one or a few snapshots enables efficient on-the-fly computation of crystallinity. Lastly, we demonstrate how the optimal C-index fit evolves during various stages of crystallization, supporting the hypothesis that entropy dominates early nucleation, while symmetry gains relevance later. This workflow provides a data-driven strategy for OP selection and a metric to monitor structural transformations in large-scale polymer simulations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 86,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18021",
        "abs_url": "https://arxiv.org/abs/2507.18021",
        "pdf_url": "https://arxiv.org/pdf/2507.18021",
        "title": "Zeroth-order log-concave sampling",
        "authors": [
            "Yunbum Kook"
        ],
        "comments": "30 pages",
        "subjects": "Statistics Theory (math.ST); Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG); Functional Analysis (math.FA); Probability (math.PR)",
        "abstract": "We study the zeroth-order query complexity of log-concave sampling, specifically uniform sampling from convex bodies using membership oracles. We propose a simple variant of the proximal sampler that achieves the query complexity with matched Rényi orders between the initial warmness and output guarantee. Specifically, for any $\\varepsilon>0$ and $q\\geq2$, the sampler, initialized at $\\pi_{0}$, outputs a sample whose law is $\\varepsilon$-close in $q$-Rényi divergence to $\\pi$, the uniform distribution over a convex body in $\\mathbb{R}^{d}$, using $\\widetilde{O}(qM_{q}^{q/(q-1)}d^{2}\\,\\lVert\\operatorname{cov}\\pi\\rVert\\log\\frac{1}{\\varepsilon})$ membership queries, where $M_{q}=\\lVert\\text{d}\\pi_{0}/\\text{d}\\pi\\rVert_{L^{q}(\\pi)}$. We further introduce a simple annealing scheme that produces a warm start in $q$-Rényi divergence (i.e., $M_{q}=O(1)$) using $\\widetilde{O}(qd^{2}R^{3/2}\\,\\lVert\\operatorname{cov}\\pi\\rVert^{1/4})$ queries, where $R^{2}=\\mathbb{E}_{\\pi}[|\\cdot|^{2}]$. This interpolates between known complexities for warm-start generation in total variation and Rényi-infinity divergence. To relay a Rényi warmness across the annealing scheme, we establish hypercontractivity under simultaneous heat flow and translate it into an improved mixing guarantee for the proximal sampler under a logarithmic Sobolev inequality. These results extend naturally to general log-concave distributions accessible via evaluation oracles, incurring additional quadratic queries.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 87,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18022",
        "abs_url": "https://arxiv.org/abs/2507.18022",
        "pdf_url": "https://arxiv.org/pdf/2507.18022",
        "title": "Does visualization help AI understand data?",
        "authors": [
            "Victoria R. Li",
            "Johnathan Sun",
            "Martin Wattenberg"
        ],
        "comments": "5 pages, 6 figures",
        "subjects": "Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)",
        "abstract": "Charts and graphs help people analyze data, but can they also be useful to AI systems? To investigate this question, we perform a series of experiments with two commercial vision-language models: GPT 4.1 and Claude 3.5. Across three representative analysis tasks, the two systems describe synthetic datasets more precisely and accurately when raw data is accompanied by a scatterplot, especially as datasets grow in complexity. Comparison with two baselines -- providing a blank chart and a chart with mismatched data -- shows that the improved performance is due to the content of the charts. Our results are initial evidence that AI systems, like humans, can benefit from visualization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 88,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18031",
        "abs_url": "https://arxiv.org/abs/2507.18031",
        "pdf_url": "https://arxiv.org/pdf/2507.18031",
        "title": "ViGText: Deepfake Image Detection with Vision-Language Model Explanations and Graph Neural Networks",
        "authors": [
            "Ahmad ALBarqawi",
            "Mahmoud Nazzal",
            "Issa Khalil",
            "Abdallah Khreishah",
            "NhatHai Phan"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "The rapid rise of deepfake technology, which produces realistic but fraudulent digital content, threatens the authenticity of media. Traditional deepfake detection approaches often struggle with sophisticated, customized deepfakes, especially in terms of generalization and robustness against malicious attacks. This paper introduces ViGText, a novel approach that integrates images with Vision Large Language Model (VLLM) Text explanations within a Graph-based framework to improve deepfake detection. The novelty of ViGText lies in its integration of detailed explanations with visual data, as it provides a more context-aware analysis than captions, which often lack specificity and fail to reveal subtle inconsistencies. ViGText systematically divides images into patches, constructs image and text graphs, and integrates them for analysis using Graph Neural Networks (GNNs) to identify deepfakes. Through the use of multi-level feature extraction across spatial and frequency domains, ViGText captures details that enhance its robustness and accuracy to detect sophisticated deepfakes. Extensive experiments demonstrate that ViGText significantly enhances generalization and achieves a notable performance boost when it detects user-customized deepfakes. Specifically, average F1 scores rise from 72.45% to 98.32% under generalization evaluation, and reflects the model's superior ability to generalize to unseen, fine-tuned variations of stable diffusion models. As for robustness, ViGText achieves an increase of 11.1% in recall compared to other deepfake detection approaches. When facing targeted attacks that exploit its graph-based architecture, ViGText limits classification performance degradation to less than 4%. ViGText uses detailed visual and textual analysis to set a new standard for detecting deepfakes, helping ensure media authenticity and information integrity.",
        "gemini2.5flash": "",
        "overall_idea": "好的，非常荣幸能以《ViGText》作者的身份，向您深入剖析我们的研究旅程和决策过程。\n\n### 0. 为什么选择这个研究内容？\n\n各位好，作为《ViGText》的作者之一，我深知这项工作诞生的初衷，源于我们对数字媒体真实性面临的日益严峻威胁的深切忧虑。当下，深度伪造（Deepfake）技术以惊人的速度发展，能够生成极其逼真但虚假的图像、视频和音频，这不仅仅是技术上的奇迹，更是对社会信任、个人隐私和信息安全的巨大挑战。\n\n我们观察到，虚假信息通过深度伪造技术得以放大和传播，例如政治宣传、虚假新闻，甚至是滥用他人肖像等恶意行为。现有的大多数深度伪造检测方法，虽然在某些场景下表现良好，但它们普遍存在几个核心局限：\n\n1.  **泛化能力不足（Generalization）**：生成模型迭代太快了！当新的、经过用户自定义或微调的生成模型（如Stable Diffusion的新变体）出现时，基于旧数据训练的检测模型往往会失效，性能急剧下降。这就像我们总是在追赶一个快速移动的目标。\n2.  **鲁棒性差（Robustness）**：恶意攻击者并非静止不动，他们会利用先进的AI技术，精心制作对抗性样本，试图绕过检测系统。现有方法往往在这种“攻防战”中处于下风。\n3.  **对图像内容的理解局限**：许多检测方法仅依赖图像的视觉特征，或仅仅通过简单的图片标题来辅助理解。这些标题过于宽泛，无法捕捉到深度伪造图片中那些微妙、局微小却至关重要的不一致性。例如，一张伪造的厨房图片，其标题可能是“厨房和餐厅”，这并不能帮助模型发现水槽边沿的奇怪扭曲或灯光阴影的不自然。简单拼接视觉和文本特征，也无法挖掘两者之间深层次的关联。\n\n因此，我们坚信，要构建一个真正有效的深度伪造检测系统，必须解决其泛化性和鲁棒性问题，并实现对图像内容更深层次的、上下文感知的理解。这正是我们选择《ViGText》这个研究方向的根本动力。我们看到了一个空白：如何将人类对图像的细致解读（即解释）与机器的强大视觉分析能力结合起来，形成一个更智能、更难以被欺骗的检测机制？\n\n### 1. 您是如何一步步构思和界定研究问题的？\n\n我们的研究问题构思始于对现有方法局限的深刻反思。\n\n**第一步：从一个失败案例开始（图4的启发）**\n\n我们深入分析了像DE-FAKE这样结合视觉和文本信息的方法。虽然它们尝试利用文本（图片标题），但我们很快发现其缺陷。例如，论文中图4展示了一个DE-FAKE未能正确分类为伪造的图像。DE-FAKE的系统可能仅仅根据“厨房和餐厅”这样的标题，就认为这张图片是真实的。但作为人类，当我们仔细观察时，会发现一些微妙的视觉异常：例如，橱柜的反射可能不自然，或者桌椅的纹理细节与整体光照不符。\n\n这个案例给了我们第一个关键洞察：**简单、通用的图像标题不足以捕捉深度伪造中存在的细微、局部的不一致性。**\n\n**第二步：提出“解释”优于“标题”的假设**\n\n如果我们能让模型像人类一样，“解释”图像的细节，并指出具体哪里不自然，那会怎样？例如，如果模型能说：“B3和B4区域的窗帘百叶窗间距不均，光线穿过时不一致，这表明渲染光影时存在错误。”（如图6所示的ViGText生成解释），这种详细的、与局部区域绑定的解释，显然比“厨房”这样的标题更有价值。\n\n于是，我们的核心假设浮现：**引入细粒度、上下文感知的文本解释，将极大提升检测能力，因为它能突出那些仅凭视觉特征难以捕捉到的微妙线索。**\n\n**第三步：界定核心挑战——如何有效集成？**\n\n接受了“解释比标题好”这个假设后，新的问题立刻出现：**如何将这些详细的、局部化的文本解释，与原始图像的视觉信息进行高效且有意义的集成？**\n\n我们知道，简单地将视觉特征和文本解释的嵌入向量拼接起来是低效的，因为这会丢失两者之间的复杂关联性。我们需要一种机制，能够建模图像不同区域（补丁）之间的空间关系，以及文本解释中不同词汇之间的语义关系，更重要的是，**建模文本解释与其所描述的图像补丁之间的跨模态关联**。\n\n**第四步：问题形式化（Section B. Problem Formulation）**\n\n基于上述构思，我们正式将问题定义为：寻找一个分类函数 $f$，它能将输入图像 $I$ 及其对应的解释 $E$ 映射到一个二元输出 $\\{0, 1\\}$（0表示真实，1表示伪造）。目标是最大化这个分类器在给定数据集 $D$ 上的准确性，同时满足：\n\n*   **鲁棒性约束 ($T_r$)**：在允许的扰动 $\\Delta$ 下（模拟对抗性攻击），模型的性能不能大幅下降。\n*   **泛化性约束 ($T_g$)**：模型在未见过的、由新型生成模型（如用户微调版本）生成的数据 $D_{new}$ 上的性能也能保持高水平。\n\n这不仅仅是准确率的问题，更是如何在面对不断演进的威胁时，保持系统的可靠性和适应性。这个问题形式化成为我们后续所有方法选择和决策的指导方针。我们不再满足于仅仅检测“已知的假”，而是追求检测“未知的假”和“被伪装的假”。\n\n### 2. 为了解决该问题，您在选择理论框架、研究方法和数据分析策略时，经历了怎样的思考、权衡和取舍？\n\n一旦问题被清晰界定，我们便开始了一系列关于理论框架、研究方法和数据分析策略的深入思考和权衡。\n\n**2.1 理论框架选择：图神经网络 (GNN) 的崛起**\n\n*   **思考过程**：传统的CNN或MLP在处理图像和文本时，通常将它们视为独立的序列或网格数据，然后进行简单的特征拼接。但我们意识到，图像补丁之间存在空间邻接关系，文本解释中的词汇有语法和语义依赖，而最重要的是，文本解释与图像补丁之间存在一种“描述-被描述”的**关联关系**。这种丰富的关系型信息是简单拼接无法捕捉的。\n*   **权衡与取舍**：\n    *   **优点**：GNN天生擅长处理关系型数据，通过节点间的消息传递机制，能够聚合局部和全局信息，并学习复杂的图结构拓扑。它能够自然地建模我们设想的“图像补丁图”和“文本解释图”，以及它们之间的“跨模态连接”。这完美契合了我们对捕捉“细微不一致性”和“上下文关联”的需求。\n    *   **缺点**：GNN的训练和推理相比简单模型可能更复杂，图的构建本身也需要精心设计。但我们认为，为了获得更高的泛化性和鲁棒性，这种复杂性是值得的。\n*   **最终选择**：GNN，具体采用**图注意力网络（GAT）**，因为它能学习不同邻居节点的重要性，更灵活地聚合信息。\n\n**2.2 核心方法选择：视觉大语言模型 (VLLM) 解释与多模态特征融合**\n\n*   **VLLM 作为解释生成器**：\n    *   **思考过程**：我们已经确定了需要细粒度、上下文感知的文本解释。但如何自动生成这些解释？人工标注显然不可行，而传统的图像字幕模型通常生成的是通用描述。新兴的VLLM（如Qwen2-VL-7B-Instruct）在这方面展现出强大潜力，它们能理解图像内容并生成详细、推理性的文本。\n    *   **权衡与取舍**：\n        *   **优点**：VLLM能够根据图像内容生成极其详细的、甚至带有推理性质的解释，例如指出“阴影不自然”或“纹理不连贯”等。这解决了我们之前标题过于泛泛的问题。\n        *   **担忧与应对**：VLLM本身也可能生成幻觉或误导性信息，我们不能完全信任它作为独立的分类器。因此，我们的核心设计是将其作为**“受控的、局部的信息提取器”**，它提供的解释必须通过GNN与实际视觉特征进行交叉验证。如果VLLM说某处是自然的，但视觉特征显示那里有伪造痕迹，GNN就能发现这种不一致性。这避免了将VLLM作为独立的“信任锚点”。\n    *   **关键决策**：**“视觉提示”（Visual Prompting）结合网格覆盖（Grid Overlay）**。这是连接文本解释与图像补丁的关键。通过给VLLM提供带有网格的图像，并要求它针对每个网格区域提供解释，我们确保了文本解释的局部性和准确性，为图结构中的跨模态连接提供了基础。\n\n*   **多层次视觉特征提取：空间域 + 频率域**\n    *   **思考过程**：深度伪造技术在空间域（像素）和频率域（纹理、伪影）都会留下痕迹。传统的CNN主要关注空间域特征。但我们知道，很多细微的伪造痕迹（如振铃效应、块效应、色彩不均匀）在频率域中更为明显和稳定，且往往与图像内容无关。特别是经过微调的生成模型，它们的伪影特征可能非常独特（如图10所示的LoRA和FM微调模型生成的图像，其DCT谱有明显局部伪影）。\n    *   **权衡与取舍**：\n        *   **优点**：\n            *   **空间特征（ConvNeXt-Large）**：捕捉高层语义和整体结构，确保对图像内容的理解。\n            *   **频率特征（DCT）**：捕捉低层统计伪影，对内容变化不敏感，对泛化和鲁棒性至关重要。这使得我们的模型能检测出那些在空间域难以察觉，或随着内容变化而改变的伪影。\n        *   **取舍**：增加了特征维度和计算量，但我们认为捕捉这些关键的频率伪影对提升模型性能是必要的。\n    *   **最终选择**：将图像补丁通过ConvNeXt提取空间特征，并通过DCT变换后再次通过ConvNeXt提取频率特征，然后对两者进行**平均融合**，作为图像补丁节点的最终嵌入。\n\n**2.3 数据分析策略：全面而严苛的评估**\n\n*   **数据集扩展与泛化性测试**：\n    *   **思考过程**：仅仅在训练集分布内评估是不够的。我们需要验证模型对**未见过的、但由相似生成模型族群（特别是微调变体）生成的新数据**的泛化能力。\n    *   **关键决策**：除了Sifat et al. [23]提供的SD 1.5 FM/LoRA微调数据集外，我们**额外增加了8个基于Stable Diffusion 3.5 LoRA微调模型生成的新测试集**。这使得我们的泛化性评估覆盖了更广泛、更前沿的生成模型变体（如表XI）。\n*   **对抗性鲁棒性测试**：\n    *   **思考过程**：模型不仅要检测“无意”的伪造，更要抵御“恶意”的攻击。传统的对抗样本可能无法模拟真实世界中，攻击者利用先进模型绕过检测的情况。\n    *   **关键决策**：\n        *   **基金模型对抗性攻击**：使用EfficientNet、ViT、CLIP-ResNet作为代理模型，生成StyleCLIP数据集上的对抗性图像。\n        *   **“奇美拉”攻击**：我们甚至设计了一个**对ViGText机制有“知识”的攻击者**（假设攻击者知道我们的图构建和特征提取方式），构建代理模型来生成对抗性样本。这模拟了最恶劣的威胁场景（如表XIII）。\n    *   **指标**：使用Accuracy, Precision, Recall, F1 score等全面指标，确保评估的科学性。\n*   **设计选择敏感性分析**：\n    *   **思考过程**：模型内部的设计参数（如图像补丁大小）对性能有多大影响？了解这些可以帮助我们进一步优化模型。\n    *   **关键决策**：**对不同图像分辨率和补丁大小进行测试**（如表VIII、IX、X）。我们发现，对于伪影明显的图片，小补丁更有利于捕捉局部畸变（泛化）；而对于经过高度优化、难以发现伪影的对抗性图片，大补丁更能捕捉全局模式（鲁棒性）。这个洞察也为未来的自适应补丁策略奠定了基础。\n\n### 3. 在研究的每个关键阶段，您做出了哪些重要决策，并能解释这些决策背后的逻辑和依据。\n\n整个研究过程是一个迭代和决策的循环。以下是几个关键阶段及我们做出的重要决策：\n\n**阶段一：问题定义与初步构想**\n\n*   **决策**：不满足于现有深度伪造检测方法的性能，尤其是其在泛化性和鲁棒性上的不足。\n*   **逻辑和依据**：观察到生成模型快速演进，现有方法无法跟上（如DE-FAKE在微调模型上的挣扎，以及对对抗样本的脆弱性）。我们相信，未来的检测必须是“适应性”和“鲁棒性”的。同时，我们意识到现有文本输入（如图片标题）过于粗糙，无法提供足够的上下文来发现细微的伪造痕迹（图4的失败案例）。\n\n**阶段二：核心思想的提炼与框架设计**\n\n*   **决策**：引入“VLLM生成的细粒度解释”作为文本输入，并构建“双图结构（Dual-Graph）”来融合视觉和文本信息。\n*   **逻辑和依据**：\n    1.  **细粒度解释**：直接回应了图4的失败案例。如果模型能像人类一样，指出图片特定区域的“反常现象”（图6的例子），那么检测精度会大大提高。VLLM的兴起使得这种细致的、推理性的解释生成成为可能。\n    2.  **双图结构**：简单拼接视觉和文本特征无法捕捉它们之间的复杂关系。GNN是处理关系型数据的利器，能有效建模图像补丁的空间邻接关系、文本词语的语义依赖，以及最关键的——**文本解释与它所描述的图像补丁之间的跨模态对应关系**。这种结构能让GNN进行更深层次的推理，发现不一致性。\n\n**阶段三：实现细节与特征工程**\n\n*   **决策**：视觉特征同时提取**空间域和频率域**的特征。\n*   **逻辑和依据**：基于对深度伪造技术原理的理解，我们知道伪造痕迹不仅体现在像素级的空间失真，也常常在频率域留下独特的统计伪影（例如，某些生成模型会引入特定的振铃效应或块状伪影，这些在频率域的DCT谱上清晰可见，如图10所示）。同时提取这两种特征，能为GNN提供更全面、更鲁棒的输入，尤其是在面对内容无关的伪影时，频率特征能够有效捕捉。这是提升泛化性和鲁棒性的关键一步。\n*   **决策**：采用**网格覆盖（Grid Overlay）**进行VLLM提示。\n*   **逻辑和依据**：VLLM虽然强大，但如果我们只是给它一张完整的图片并要求它解释，它可能会给出宏观的、但与特定伪造区域关联不强的解释。通过在图像上叠加网格，并引导VLLM针对每个网格区域给出解释，我们强制VLLM生成了**局部化**的解释，这使得我们能够精确地将文本节点连接到对应的图像补丁节点，从而构建出精确的跨模态边。\n\n**阶段四：实验设计与评估**\n\n*   **决策**：扩展测试数据集，特别是纳入**最新的、用户微调的Stable Diffusion 3.5 LoRA模型**生成的图片。\n*   **逻辑和依据**：现有研究的泛化性评估不足以应对生成模型快速迭代的现实。我们必须证明ViGText不仅能检测“已知的假”，更能有效检测“未知的假”（指那些由新模型或微调模型生成的）。这些模型代表了现实世界中攻击者可能使用的最前沿技术。\n*   **决策**：设计并实施了**高级对抗性攻击（如“奇美拉”攻击）**，模拟攻击者对检测系统有先验知识的场景。\n*   **逻辑和依据**：仅仅测试简单的FGSM/PGD攻击不足以证明真正的鲁棒性。我们必须考虑最坏情况：攻击者知道我们的检测机制，并尝试针对性地进行规避。通过构建代理模型来模拟ViGText的功能，并优化生成对抗样本，我们能更真实地评估ViGText在强对抗环境下的性能。\n*   **决策**：进行**补丁大小的敏感性分析**。\n*   **逻辑和依据**：最初我们对补丁大小的选择可能有点经验主义。通过系统性地测试不同大小，我们发现了一个有趣的现象：小补丁有利于捕捉局部伪影（泛化性），而大补丁在应对特定全局对抗模式时更有效（鲁棒性）。这不仅验证了ViGText对参数变化的韧性，也为未来**自适应补丁策略**提供了理论基础。例如，根据图像内容的复杂性或伪影的类型动态调整补丁大小。\n\n**通过一个具体例子来串联：图4的改进之路**\n\n让我们回到图4，DE-FAKE将一张深度伪造的厨房图片错误地识别为真实。\n\n1.  **我们看到了什么？** DE-FAKE的标题是“a kitchen and dining area”，这过于泛泛，无法捕捉到图片中微妙的不一致性。作为人类，我们会看到窗帘百叶窗的间距不均，或者炉子把手有些扭曲。\n\n2.  **ViGText是如何解决的？**\n    *   **VLLM解释**：我们将带有网格的图4输入到VLLM中。VLLM不再只生成一个宏观标题，而是针对特定补丁区域（例如B3,B4区域和D1,D2区域）生成详细解释（如图6）。例如，它会说：“{B3,B4}: The window blinds have uneven spacing, and the light passing through does not align properly with the individual slats, which suggests an error in rendering light and shadows.” 或 “{D1,D2}: The oven appears to have a distorted handle, and the reflection and shadow around it don't conform to the expected perspective and lighting.” 这些解释直接指出了视觉上的不一致性。\n    *   **双图构建与特征融合**：\n        *   图像被分割成补丁，并提取**空间特征**（捕捉整体结构和纹理）和**频率特征**（捕捉微妙的伪影，如某些生成模型特有的振铃效应或块效应，这些在D1,D2区域可能存在，在图10中我们展示了不同类型伪造的DCT谱差异）。\n        *   VLLM生成的解释被解析成文本图，词语作为节点，语法关系作为边。\n        *   最关键的，根据视觉提示的网格信息，例如，B3和B4补丁区域被解释为“窗帘不均”，那么文本图中的这些解释节点就与图像图中的B3和B4补丁节点建立了**跨模态连接**。\n    *   **GNN推理**：GNN接收到这个包含视觉、文本及其复杂关系的统一图。通过消息传递，GNN不仅能识别B3、B4补丁在空间域或频率域的异常，还能结合文本解释，理解这些异常的具体性质（“窗帘不均”），从而更准确地判断这是一种伪造。GNN能够权衡视觉特征和文本解释之间可能存在的不一致性，这正是它优于简单拼接的关键。\n    *   **结果**：最终，ViGText能够正确地将图4的图像分类为伪造，而DE-FAKE则未能做到。这充分证明了我们通过“细粒度解释”和“双图结构”所实现的“上下文感知分析”的强大威力。\n\n整个过程的核心在于：**我们从对现有模型失败案例的深入洞察出发，逐步构建了一个能够融合多模态信息、捕捉复杂关系、并能在不断演进的威胁环境中保持鲁棒性和泛化能力的深度伪造检测框架。**每一步决策都力求解决前一个方法的局限，并为未来的更高级应用奠定基础。"
    },
    {
        "order": 89,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18055",
        "abs_url": "https://arxiv.org/abs/2507.18055",
        "pdf_url": "https://arxiv.org/pdf/2507.18055",
        "title": "Privacy-Preserving Synthetic Review Generation with Diverse Writing Styles Using LLMs",
        "authors": [
            "Tevin Atwal",
            "Chan Nam Tieu",
            "Yefeng Yuan",
            "Zhan Shi",
            "Yuhong Liu",
            "Liang Cheng"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "The increasing use of synthetic data generated by Large Language Models (LLMs) presents both opportunities and challenges in data-driven applications. While synthetic data provides a cost-effective, scalable alternative to real-world data to facilitate model training, its diversity and privacy risks remain underexplored. Focusing on text-based synthetic data, we propose a comprehensive set of metrics to quantitatively assess the diversity (i.e., linguistic expression, sentiment, and user perspective), and privacy (i.e., re-identification risk and stylistic outliers) of synthetic datasets generated by several state-of-the-art LLMs. Experiment results reveal significant limitations in LLMs' capabilities in generating diverse and privacy-preserving synthetic data. Guided by the evaluation results, a prompt-based approach is proposed to enhance the diversity of synthetic reviews while preserving reviewer privacy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 90,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18099",
        "abs_url": "https://arxiv.org/abs/2507.18099",
        "pdf_url": "https://arxiv.org/pdf/2507.18099",
        "title": "Comparison of Segmentation Methods in Remote Sensing for Land Use Land Cover",
        "authors": [
            "Naman Srivastava",
            "Joel D Joy",
            "Yash Dixit",
            "Swarup E",
            "Rakshit Ramesh"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Land Use Land Cover (LULC) mapping is essential for urban and resource planning, and is one of the key elements in developing smart and sustainable this http URL study evaluates advanced LULC mapping techniques, focusing on Look-Up Table (LUT)-based Atmospheric Correction applied to Cartosat Multispectral (MX) sensor images, followed by supervised and semi-supervised learning models for LULC prediction. We explore DeeplabV3+ and Cross-Pseudo Supervision (CPS). The CPS model is further refined with dynamic weighting, enhancing pseudo-label reliability during training. This comprehensive approach analyses the accuracy and utility of LULC mapping techniques for various urban planning applications. A case study of Hyderabad, India, illustrates significant land use changes due to rapid urbanization. By analyzing Cartosat MX images over time, we highlight shifts such as urban sprawl, shrinking green spaces, and expanding industrial areas. This demonstrates the practical utility of these techniques for urban planners and policymakers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 91,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18103",
        "abs_url": "https://arxiv.org/abs/2507.18103",
        "pdf_url": "https://arxiv.org/pdf/2507.18103",
        "title": "A New Pair of GloVes",
        "authors": [
            "Riley Carlson",
            "John Bauer",
            "Christopher D. Manning"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "This report documents, describes, and evaluates new 2024 English GloVe (Global Vectors for Word Representation) models. While the original GloVe models built in 2014 have been widely used and found useful, languages and the world continue to evolve and we thought that current usage could benefit from updated models. Moreover, the 2014 models were not carefully documented as to the exact data versions and preprocessing that were used, and we rectify this by documenting these new models. We trained two sets of word embeddings using Wikipedia, Gigaword, and a subset of Dolma. Evaluation through vocabulary comparison, direct testing, and NER tasks shows that the 2024 vectors incorporate new culturally and linguistically relevant words, perform comparably on structural tasks like analogy and similarity, and demonstrate improved performance on recent, temporally dependent NER datasets such as non-Western newswire data.",
        "gemini2.5flash": "",
        "overall_idea": "好的，作为这篇题为《A New Pair of GloVes》论文的作者，我很乐意回顾我们从选题到解决问题，再到最终成果的整个思维决策过程。我将以“covid”这个词作为具体例子，贯穿整个阐述。\n\n### 0. 为什么选择这个研究内容？\n\n我的研究团队和我在观察自然语言处理（NLP）领域的趋势时，注意到了一个显而易见但又常常被忽视的问题：**语言是动态的，但大多数广泛使用的预训练词嵌入模型却是静态的，而且已经过时了。**\n\n具体来说，GloVe 模型自2014年发布以来，因其高效和卓越的性能被广泛采用。然而，2014年至今，十年过去了，英语世界发生了巨大的变化。全球性的事件（如COVID-19大流行）、技术的飞速发展（如ChatGPT、区块链）、以及社交媒体驱动的语言演变（如“rizz”、“skibidi”等新词的出现或旧词义的改变），都使得2014年的词汇表和词义映射变得不再完全适用。\n\n我的团队经常会遇到这样的反馈：用户在使用旧版GloVe时，处理新文本时遇到大量“词汇表外”（Out-Of-Vocabulary, OOV）的问题。更重要的是，像“covid”这种在2020年及之后才大规模出现的词汇，在2014年的模型中根本不存在。这直接影响了下游任务（如命名实体识别、情感分析等）的性能和准确性。\n\n此外，原始的2014年GloVe模型在数据来源和预处理细节方面缺乏足够清晰的文档，这给研究社区带来了复现和信任上的挑战。我们看到了一个双重机会：**既要更新模型以反映当代语言，又要提供透明、详细的训练文档，以提高研究的可复现性和可靠性。** 这就是我们决定启动这项研究的根本原因——让静态词嵌入重新焕发生机，与时俱进。\n\n### 1. 如何一步步构思和界定研究问题的？\n\n我们最初的想法很简单：“更新GloVe”。但很快，我们就意识到这不仅仅是简单地用新数据跑一遍模型。我们需要系统性地思考“更新”意味着什么，以及如何衡量这种“更新”的有效性。\n\n*   **第一步：识别核心问题——“过时”。**\n    *   “过时”体现在哪里？\n        *   **词汇表过时：** 缺少新词（例如，“covid”）。\n        *   **词义过时/漂移：** 现有词的语义随着时间改变（例如，“twitter”现在更多指“X”平台）。\n        *   **数据来源过时：** 2014年的数据无法捕捉2014年后的语言现象。\n    *   **以“covid”为例：** 这是一个完美的例子，它在2019年末至2020年初才广泛进入全球词汇。2014年的模型不可能包含它。这意味着任何试图分析涉及COVID-19的文本的NLP系统，如果依赖旧版GloVe，都会遇到挑战。\n\n*   **第二步：将核心问题转化为可测量的研究问题。**\n    *   **研究问题1：词汇表是否得到了有效更新？**\n        *   如何衡量？直接比较2014年和2024年模型的词汇表。我们预期2024年模型应该包含大量反映文化、技术、社会趋势的新词。\n        *   **对应“covid”：** “covid”必须出现在2024年的词汇表中，而在2014年的词汇表中是缺失的。\n\n    *   **研究问题2：更新后的模型在核心语义任务上性能如何？**\n        *   词嵌入的核心能力是捕捉词语之间的语义和句法关系。即使词汇表更新了，这些基本能力是否保持甚至提升？我们不想为了“新”而牺牲“好”。\n        *   如何衡量？使用标准的词语类比（Analogy）和词语相似度（Similarity）数据集进行评估。我们预期性能应至少与2014年模型相当。\n        *   **对应“covid”：** 虽然“covid”本身可能不直接出现在类比或相似度测试中，但与它相关的词语（如“pandemic”、“vaccine”等）的嵌入质量，会间接反映模型整体的语义捕捉能力。\n\n    *   **研究问题3：更新后的模型在下游任务中，尤其是在处理“新”数据时，能否带来实际提升？**\n        *   这是最关键的一点。如果模型不能在实际应用中表现更好，那么更新的意义何在？我们特别关注那些时间敏感或领域特定的任务。命名实体识别（NER）是一个很好的测试场景，因为它要求模型识别出文本中的特定实体，这常常涉及新出现的专有名词或概念。\n        *   如何衡量？在一些经典和一些**近期、跨地域、社交媒体相关**的NER数据集上进行评估。我们预期在这些反映当代语言使用的数据集上，2024年模型应有显著提升。\n        *   **对应“covid”：** 如果一篇2023年的新闻报道提到“COVID-19疫情”，旧模型可能因词汇表限制而无法识别“COVID-19”为实体。而新模型应该能够识别，并将其归类为适当的类别（例如“MISC”或“EVENT”）。我们甚至在论文中专门找到了这样的例子（表11）。\n\n通过这三步，我们将一个宽泛的“更新GloVe”目标，细化为可操作、可验证的具体研究问题，并为每个问题设定了明确的衡量标准和预期结果。\n\n### 2. 为了解决该问题，您在选择理论框架、研究方法和数据分析策略时，经历了怎样的思考、权衡和取舍。\n\n**2.1 理论框架选择：继续沿用GloVe**\n\n*   **思考：** 在Transformer模型（如BERT、GPT系列）日益流行的背景下，是否还要坚持静态词嵌入？\n*   **权衡与取舍：**\n    *   **选择GloVe的理由：**\n        1.  **轻量级和高效：** 静态嵌入在计算资源受限的环境或需要快速部署的场景下仍有巨大优势。\n        2.  **可解释性：** GloVe基于全局共现统计，其向量空间相对更容易理解其语义编码方式。\n        3.  **互补性：** 静态嵌入并非要取代Transformer，而是作为其补充，服务于不同的应用需求。\n        4.  **传承性：** 保持GloVe框架，可以方便地与社区已有的工具和应用进行集成，降低迁移成本。\n    *   **我们的小创新：GloVe-V的MFT。** 我们引入了来自GloVe-V（Vallebueno et al., 2024）的最小频率阈值（MFT）概念。\n        *   **思考：** 如何在捕获新词的同时，避免引入大量噪声和罕见词？\n        *   **权衡与取舍：** 简单的词频过滤可能过于粗糙。GloVe-V的MFT基于统计不确定性，允许我们更好地平衡过滤掉“过度稀有和嘈杂的词语”，同时“保留不那么频繁但上下文重要的术语”。这使得词汇表既包含新词，又保持了高质量。我们通过实验发现MFT为20时能达到训练向量与WLS向量的最高平均余弦相似度，这表明了模型的鲁棒性。\n\n**2.2 研究方法：数据是核心**\n\n*   **数据选择：**\n    *   **思考：** 什么样的数据才能真正反映当代英语的变化？仅仅更新旧数据源够吗？\n    *   **权衡与取舍：**\n        1.  **沿用并更新旧数据：** 我们决定继续使用Wikipedia和Gigaword。Wikipedia作为百科全书，能提供广泛、相对正式的词汇和知识。Gigaword（第5版）提供新闻语料，反映了特定时期（1994-2010）的专业新闻用语。我们获取了最新的Wikipedia dump（2024年7月）并使用了Gigaword的第5版。为了弥补Wikipedia数据量的增长，我们**决定将Gigaword重复加入两次**，以重新平衡训练语料中新闻和百科的比例。\n        2.  **引入新数据源：Dolma。** 这是我们数据选择上的一个关键决策。\n            *   **思考：** 除了正式的百科和新闻，还需要什么样的数据来捕捉“语言演变”？\n            *   **Dolma的优势：** Dolma v1.6（Soldaini et al., 2024）在2024年1月发布，提供了**3万亿**token的庞大语料，包含了书籍、编程脚本、学术文章和**在线内容（如Common Crawl和Reddit）**。这正是我们需要的！尤其是Reddit部分，它直接反映了非正式的、社交媒体驱动的语言使用，能捕捉到像“rizz”这类俚语。Common Crawl也包含了大量的网络文本，能反映更广阔的在线语境。\n            *   **权衡与取舍：** Dolma过于庞大，我们无法全部使用。**决定：** 选取了Dolma的一个**子集**（如表2所示），包括了Common Crawl、C4、Reddit和Project Gutenberg等来源，确保了数据的多样性和时效性。这个子集仍然非常大（2200亿token），足以弥补旧数据的不足。\n\n*   **数据预处理：**\n    *   **思考：** 如何确保数据的清洁度和一致性？\n    *   **决策：**\n        *   统一使用Stanford CoreNLP tokenizer（4.4.1版），并转换为小写。原因：确保分词标准一致，减少词形变化带来的稀疏性问题，便于模型学习。\n        *   移除`<doc>`、`<unk>`等特定标签。原因：这些是噪音或不应被视为词汇的标记。\n        *   对于Wikipedia/Gigaword，根据GloVe-V的MFT策略确定词汇表大小；对于Dolma，由于其庞大，我们设定了一个最大词汇量上限（120万），并在合并不同Dolma子集时，统一了词汇表。原因：实践中的效率与质量平衡。\n\n*   **模型训练：**\n    *   **决策：** 沿用GloVe的原始训练流程和AdaGrad优化器。\n    *   **逻辑：** 我们的目标是更新数据，而不是重新设计训练算法。使用原版配置，可以确保结果与2014年模型在方法上的可比性，同时将改进归因于数据更新。超参数（学习率、Alpha、XMax、Seed、Epochs）也基本沿用或微调，确保训练的稳定性。\n\n**2.3 数据分析策略：多维度评估**\n\n*   **评估“更新词汇”：**\n    *   **方法：** 词汇表集合差集分析。将2024年和2014年模型的词汇表视为集合，计算差集（2024 - 2014），然后人工筛选出具有代表性的新词。\n    *   **逻辑：** 仅仅给出新词数量不足以说明问题，人工筛选能够定性展示新词如何反映了文化、技术和社会趋势。例如，在表5和表6中列出的“covid”、“chatgpt”、“rizz”等词，都是人工筛选出的典型例子。\n\n*   **评估“核心语义能力”：**\n    *   **方法：** 标准词语类比和相似度任务（Google Analogy, MSR Analogy, WordSim353, SimLex999, MEN）。\n    *   **逻辑：** 这些是词嵌入领域经典的基准测试，可以衡量模型捕捉词语间结构（句法/语义）和连续语义关系的能力。我们的预期是“至少持平”，以证明更新后的模型没有在基础能力上倒退。\n\n*   **评估“下游任务表现”：**\n    *   **方法：** 命名实体识别（NER）任务。我们选择Stanford Stanza NER模型，并替换其默认词嵌入。关键是选择了**反映近期语言使用和非西方语境**的数据集：CoNLL-PP（CoNLL-03的现代化版本）、English Worldwide Newswire（2023年非西方新闻，明确包含COVID-19等事件）、WNUT 17（社交媒体生成文本，含稀有/新兴实体）。\n    *   **逻辑：** 这是我们证明新模型价值的核心战场。旧版GloVe在这些数据集上的表现会受到OOV和语义漂移的严重影响。我们希望新模型能显著提升F1分数。我们特别关注了“COVID-19”在NER任务中的表现，因为它直接验证了我们更新词汇表的有效性。\n\n### 3. 在研究的每个关键阶段，您做出了哪些重要决策，并能解释这些决策背后的逻辑和依据。\n\n继续以**“covid”**这个词为例，贯穿整个决策过程：\n\n*   **阶段一：确定研究方向与核心目标（解决“过时”问题）**\n    *   **核心决策：** 更新GloVe模型，重点解决2014年模型在处理新出现词汇和概念（如“covid”）时的不足。\n    *   **逻辑与依据：**\n        *   **观察到的问题：** 在与研究社区和实际应用开发者交流时，我们频繁收到关于旧版GloVe在处理2014年以后出现的新词（如“covid”、“chatgpt”）时性能下降的反馈，这些词直接导致了OOV问题。\n        *   **明确需求：** 市场和研究界需要一套能够捕捉当代语言变化、且易于使用的预训练词嵌入。旧模型无法识别“covid”，就无法对含有“covid”的文本进行有效语义理解。\n\n*   **阶段二：数据选择与处理（确保捕捉到“covid”及相关新词）**\n    *   **核心决策1：引入Dolma等大规模、多样化且时效性强的数据集。**\n        *   **逻辑与依据：** Wikipedia和Gigaword虽已更新，但其内容偏向正式和新闻体，可能无法全面捕捉到像“covid”这种在日常交流和社交媒体中迅速传播的词汇。Dolma（尤其是其Common Crawl和Reddit子集）包含了大量的在线文本和非正式语料，这些是“covid”这类词汇快速流行并被广泛使用的主要渠道。我们深信，只有涵盖这些新颖和多样的数据源，才能保证“covid”等新兴词汇被充分地收录和学习。\n    *   **核心决策2：采用GloVe-V的MFT策略进行词汇表构建。**\n        *   **逻辑与依据：** 虽然“covid”本身可能因高频而不会被轻易滤除，但MFT能更精细地处理词汇表的质量。我们不希望仅仅因为一个词在整体数据中出现次数“不达标”而将其排除，尤其是那些在特定领域或时间段内虽然相对稀有但意义重大的词。MFT有助于确保词汇表既包含高频词，也能纳入那些具有重要上下文价值的、中低频的“新”词，从而提高词向量的鲁棒性。\n\n*   **阶段三：模型训练与配置（确保“covid”的词向量质量）**\n    *   **核心决策：** 沿用GloVe的训练算法，并根据实验结果微调超参数。\n    *   **逻辑与依据：** 我们的核心是“数据更新”和“词汇适应”，而非算法创新。通过保持算法的一致性，我们可以将性能提升主要归因于新的、更丰富的训练数据。训练过程中，模型会根据“covid”与周围词（如“pandemic”、“vaccine”、“mask”）的共现模式来学习其语义。一个好的训练过程会确保“covid”的向量能准确反映其与其他词的语义关系。\n\n*   **阶段四：评估策略与结果分析（验证“covid”的实际效果）**\n    *   **核心决策1：将“covid”作为“更新词汇表”的典型代表在结果中明确列出。**\n        *   **逻辑与依据：** 这是最直接的证据。论文表5中明确列出了“covid”，它直接印证了我们通过新数据成功扩展了词汇表，解决了旧模型的OOV问题。\n    *   **核心决策2：将NER任务作为核心下游评估，特别是选择时间敏感的新数据集。**\n        *   **逻辑与依据：** 我们的关键假设是新模型在处理现代文本时会表现更好。命名实体识别任务对此是极好的验证。\n            *   **以NER中“COVID-19”为例：** 在NER评估部分，我们特意关注了像English Worldwide Newswire和CoNLL-PP这样的数据集，它们包含了大量最新的新闻和语言数据。在表11的示例中，我们展示了一个句子：“Nationwide, **COVID-19** infections in United States are at their peak...”\n                *   **2014年模型表现：** 由于“COVID-19”不在2014年GloVe的词汇表中，NER模型无法获得其有意义的向量表示。因此，模型很可能将其识别为“O”（无实体），或者产生错误的分类。在我们的实际结果中，2014年的模型确实未能正确标注“COVID-19”。\n                *   **2024年模型表现：** 由于“COVID-19”存在于2024年GloVe的词汇表中，并且通过大量的Dolma数据（包括新闻、社交媒体）学习了其上下文语境，模型能够为其生成一个准确的词向量。因此，NER模型可以利用这个向量，结合上下文，将其正确识别为“MISC”实体类别（正如表11所示）。\n            *   **逻辑依据：** 这种对比直接证明了我们更新模型对下游任务的实际价值。新模型能够处理并理解旧模型无法触及的、与当代世界紧密相关的实体。\n\n通过这一系列的决策和逻辑推导，我们最终成功地构建并验证了新的GloVe模型，证明了其在词汇表更新、语义能力保持以及处理当代语言数据方面的显著优势。"
    },
    {
        "order": 92,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18114",
        "abs_url": "https://arxiv.org/abs/2507.18114",
        "pdf_url": "https://arxiv.org/pdf/2507.18114",
        "title": "Nonconvex Optimization Framework for Group-Sparse Feedback Linear-Quadratic Optimal Control I: Penalty Approach",
        "authors": [
            "Lechen Feng",
            "Xun Li",
            "Yuan-Hua Ni"
        ],
        "comments": "",
        "subjects": "Optimization and Control (math.OC); Machine Learning (cs.LG)",
        "abstract": "This paper develops a unified nonconvex optimization framework for the design of group-sparse feedback controllers in infinite-horizon linear-quadratic (LQ) problems. We address two prominent extensions of the classical LQ problem: the distributed LQ problem with fixed communication topology (DFT-LQ) and the sparse feedback LQ problem (SF-LQ), both of which are motivated by the need for scalable and structure-aware control in large-scale systems. Unlike existing approaches that rely on convex relaxations or are limited to block-diagonal structures, we directly formulate the controller synthesis as a finite-dimensional nonconvex optimization problem with group $\\ell_0$-norm regularization, capturing general sparsity patterns. We establish a connection between DFT-LQ and SF-LQ problems, showing that both can be addressed within our unified framework. Furthermore, we propose a penalty-based proximal alternating linearized minimization (PALM) algorithm and provide a rigorous convergence analysis under mild assumptions, overcoming the lack of coercivity in the objective function. The proposed method admits efficient solvers for all subproblems and guarantees global convergence to critical points. Our results fill a key gap in the literature by enabling the direct design of group-sparse feedback gains with theoretical guarantees, without resorting to convex surrogates or restrictive structural assumptions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 93,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18115",
        "abs_url": "https://arxiv.org/abs/2507.18115",
        "pdf_url": "https://arxiv.org/pdf/2507.18115",
        "title": "Agentic AI framework for End-to-End Medical Data Inference",
        "authors": [
            "Soorya Ram Shimgekar",
            "Shayan Vassef",
            "Abhay Goyal",
            "Navin Kumar",
            "Koustuv Saha"
        ],
        "comments": "10 pages, 5 figures, 2 tables, BIBM conference",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computers and Society (cs.CY); Emerging Technologies (cs.ET); Machine Learning (cs.LG)",
        "abstract": "Building and deploying machine learning solutions in healthcare remains expensive and labor-intensive due to fragmented preprocessing workflows, model compatibility issues, and stringent data privacy constraints. In this work, we introduce an Agentic AI framework that automates the entire clinical data pipeline, from ingestion to inference, through a system of modular, task-specific agents. These agents handle both structured and unstructured data, enabling automatic feature selection, model selection, and preprocessing recommendation without manual intervention. We evaluate the system on publicly available datasets from geriatrics, palliative care, and colonoscopy imaging. For example, in the case of structured data (anxiety data) and unstructured data (colonoscopy polyps data), the pipeline begins with file-type detection by the Ingestion Identifier Agent, followed by the Data Anonymizer Agent ensuring privacy compliance, where we first identify the data type and then anonymize it. The Feature Extraction Agent identifies features using an embedding-based approach for tabular data, extracting all column names, and a multi-stage MedGemma-based approach for image data, which infers modality and disease name. These features guide the Model-Data Feature Matcher Agent in selecting the best-fit model from a curated repository. The Preprocessing Recommender Agent and Preprocessing Implementor Agent then apply tailored preprocessing based on data type and model requirements. Finally, the ``Model Inference Agent\" runs the selected model on the uploaded data and generates interpretable outputs using tools like SHAP, LIME, and DETR attention maps. By automating these high-friction stages of the ML lifecycle, the proposed framework reduces the need for repeated expert intervention, offering a scalable, cost-efficient pathway for operationalizing AI in clinical environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 94,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18118",
        "abs_url": "https://arxiv.org/abs/2507.18118",
        "pdf_url": "https://arxiv.org/pdf/2507.18118",
        "title": "A Two-armed Bandit Framework for A/B Testing",
        "authors": [
            "Jinjuan Wang",
            "Qianglin Wen",
            "Yu Zhang",
            "Xiaodong Yan",
            "Chengchun Shi"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Applications (stat.AP)",
        "abstract": "A/B testing is widely used in modern technology companies for policy evaluation and product deployment, with the goal of comparing the outcomes under a newly-developed policy against a standard control. Various causal inference and reinforcement learning methods developed in the literature are applicable to A/B testing. This paper introduces a two-armed bandit framework designed to improve the power of existing approaches. The proposed procedure consists of three main steps: (i) employing doubly robust estimation to generate pseudo-outcomes, (ii) utilizing a two-armed bandit framework to construct the test statistic, and (iii) applying a permutation-based method to compute the $p$-value. We demonstrate the efficacy of the proposed method through asymptotic theories, numerical experiments and real-world data from a ridesharing company, showing its superior performance in comparison to existing methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 95,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18155",
        "abs_url": "https://arxiv.org/abs/2507.18155",
        "pdf_url": "https://arxiv.org/pdf/2507.18155",
        "title": "GeoAvatar: Adaptive Geometrical Gaussian Splatting for 3D Head Avatar",
        "authors": [
            "SeungJun Moon",
            "Hah Min Lew",
            "Seungeun Lee",
            "Ji-Su Kang",
            "Gyeong-Moon Park"
        ],
        "comments": "ICCV 2025, Project page: this https URL",
        "subjects": "Graphics (cs.GR); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Despite recent progress in 3D head avatar generation, balancing identity preservation, i.e., reconstruction, with novel poses and expressions, i.e., animation, remains a challenge. Existing methods struggle to adapt Gaussians to varying geometrical deviations across facial regions, resulting in suboptimal quality. To address this, we propose GeoAvatar, a framework for adaptive geometrical Gaussian Splatting. GeoAvatar leverages Adaptive Pre-allocation Stage (APS), an unsupervised method that segments Gaussians into rigid and flexible sets for adaptive offset regularization. Then, based on mouth anatomy and dynamics, we introduce a novel mouth structure and the part-wise deformation strategy to enhance the animation fidelity of the mouth. Finally, we propose a regularization loss for precise rigging between Gaussians and 3DMM faces. Moreover, we release DynamicFace, a video dataset with highly expressive facial motions. Extensive experiments show the superiority of GeoAvatar compared to state-of-the-art methods in reconstruction and novel animation scenarios.",
        "gemini2.5flash": "",
        "overall_idea": "作为GeoAvatar的作者，我很荣幸能深入剖析我们的研究过程，详细回顾从选题到解决问题，再到方法论选择和关键决策的整个思维决策链条。我将通过一个具体的例子——**解决面部区域自适应形变与口腔精细建模的挑战**——来阐述我们的思考过程。\n\n---\n\n### 0. 为什么选择这个研究内容？\n\n在2023年，高斯泼溅（3D Gaussian Splatting, 3DGS）技术在静态场景重建领域异军突起，以其惊人的渲染速度和高质量迅速成为焦点。然而，当时大多数工作都集中在静态场景，对于动态且高精度的3D人头头像生成，3DGS的应用仍处于初期阶段。NeRF（神经辐射场）虽然也能生成高质量的头像，但其训练和渲染速度对于实时应用而言仍是瓶颈。我们团队一直专注于实时高保真数字人领域，3DGS的出现为我们提供了一个激动人心的契机，预示着实时、高保真头像的可能性。\n\n当时已有一些尝试将3DGS与3DMM（3D Morphable Model）结合用于人头头像（如SplattingAvatar、FlashAvatar），但我们观察到它们在处理复杂面部区域（如毛发、耳朵、特别是口腔）时，仍然存在明显的伪影和细节缺失。这促使我们思考：如何才能充分发挥3DGS的优势，同时克服现有方法的不足，实现更鲁棒、更高质量的3D人头头像生成？我们认为，这不仅是一个技术挑战，更是一个推动数字人应用走向更广阔领域的机会。\n\n### 1. 如何一步步构思和界定研究问题的？\n\n我们的问题界定并非一蹴而就，而是基于对现有方法缺陷的深入观察和逐步推导：\n\n*   **第一阶段：初步观察与痛点识别（面部区域形变不均）**\n    *   在初步实验和文献调研中，我们发现现有的基于3DMM的3DGS人头头像方法（如SplattingAvatar [44] 和FlashAvatar [51]）在面部动画（novel animation）场景下表现不佳，常常出现明显的伪影和纹理模糊。尤其是在面部边缘、毛发区域以及口腔内部，这些问题尤为突出。\n    *   **一个核心发现（关键洞察）：** 我们注意到，当这些方法将高斯点（Gaussians）与3DMM模型进行绑定时，它们通常采用“统一的正则化策略”——即对所有面部区域应用相同的刚性约束或位移限制。然而，实际观察发现（图2a），面部不同区域的3DMM拟合精度差异巨大：例如，头皮、耳朵等区域由于毛发遮挡或3DMM本身表达能力的限制，其与真实几何体的偏差（高斯点的局部均值与对应网格面片的距离）往往较大；而面部中心区域（如鼻子、嘴唇）由于3DMM拟合较好，偏差较小。\n    *   **问题初步界定1：** 这种统一的正则化策略未能考虑到面部区域形变的局部差异性，导致在3DMM拟合不佳的区域（如头皮）缺乏必要的柔性来维持真实细节，而在拟合良好区域（如脸颊）又因缺乏足够的刚性约束，导致高斯点与3DMM网格的对应关系变弱，进而影响动画的稳定性与保真度。\n\n*   **第二阶段：聚焦口腔特殊性（高动态与结构缺失）**\n    *   在第一阶段的基础上，我们进一步发现，口腔区域的问题最为严重和普遍。无论何种方法，口腔内部（牙齿、舌头、口腔顶/底部）的渲染质量都非常差，常常出现模糊、错位甚至几何结构错误（图2b）。\n    *   **核心痛点分析：**\n        1.  **3DMM的固有缺陷：** 现有的3DMM模型（如FLAME [28]）在设计时就没有包含精细的口腔内部结构（如磨牙、腭部、口底），通常只处理嘴唇和面部外形。即使是添加了部分牙齿结构的方法（如GaussianAvatars [40]仅复制了唇环顶点生成门牙），也远未达到真实口腔的复杂程度。\n        2.  **口腔的高动态性：** 口腔在说话、微笑等表情中具有极其复杂的运动模式，且内部结构（如牙齿上下咬合）之间存在严格的相对位置关系。简单地将高斯点绑定到粗糙的网格上，无法捕捉这种精细的、层级化的动态。\n    *   **问题初步界定2：** 如何在缺乏3DMM先验的情况下，为高动态的口腔区域构建准确的几何结构，并实现符合生物学规律的精细化形变，以保持内部结构的一致性？\n\n*   **第三阶段：补充数据缺口（高表现力数据集）**\n    *   在测试现有方法时，我们发现用于训练的公开单目视频数据集（如SplattingAvatar提供的）通常缺乏足够丰富的面部表情和大幅度的头部运动。这使得我们难以充分验证模型在极端动态情况下的鲁棒性和表现力。\n    *   **问题初步界定3：** 需要一个包含高度表现力面部动作的单目视频数据集，以便对模型进行更彻底的训练和评估。\n\n通过以上三阶段的思考，我们将研究问题精确地界定为：**如何在单目视频输入下，通过自适应的高斯点形变管理和精细化的口腔结构与形变建模，在平衡高保真重建（identity preservation）与鲁棒性新姿态动画（novel animation）之间找到最佳点，并为高表现力场景提供高质量的3D人头头像？**\n\n### 2. 为了解决该问题，您在选择理论框架、研究方法和数据分析策略时，经历了怎样的思考、权衡和取舍？\n\n一旦问题被清晰界定，我们便开始着手设计解决方案，并在理论框架、研究方法和数据分析策略上做出了关键的思考、权衡和取舍。\n\n*   **理论框架选择：3DGS + 3DMM**\n    *   **思考：** 鉴于我们对实时性和高保真度的追求，3DGS是渲染方面的最佳选择。而3DMM作为成熟的面部先验模型，能提供稳定的基础网格和参数化控制，这对于动态头像的生成至关重要。\n    *   **权衡与取舍：**\n        *   **NeRF vs. 3DGS：** NeRF在渲染质量上可能与3DGS媲美，但其训练和渲染速度远不及3DGS，且其隐式表达不利于我们对几何体进行显式控制和局部形变。因此，我们果断选择了3DGS。\n        *   **纯数据驱动 vs. 3DMM驱动：** 纯数据驱动的方法可能在某些细节上更自由，但缺乏3DMM的强先验，在处理复杂动画和新姿态时容易出现拓扑错误和不稳定性。3DMM虽然有其不足（如结构缺失、拟合误差），但其提供的拓扑和运动约束是宝贵的起点。我们选择“3DMM驱动下的高斯点自适应偏差”策略，即承认3DMM并非完美，允许高斯点在一定范围内偏离3DMM，但这种偏离必须是受控的、有意义的。\n\n*   **研究方法选择：**\n\n    1.  **自适应预分配阶段（Adaptive Pre-allocation Stage, APS）：**\n        *   **思考：** 针对“面部区域形变不均”的问题，我们最初设想过两种方案：1) 手动定义区域（如“头皮是柔性的，脸颊是刚性的”）；2) 训练一个神经网络来学习每个高斯点的正则化强度。\n        *   **权衡与取舍：**\n            *   手动定义过于死板，难以泛化到不同个体，也可能无法捕捉细微的局部差异。\n            *   神经网络虽然灵活，但会增加模型复杂度，需要额外的数据或复杂的自监督信号。\n            *   **最终选择：** 基于我们观察到的“局部均值距离”这一关键洞察，我们意识到可以利用3DMM本身的拟合质量作为指导。我们提出了APS：在模型训练初期（我们称之为“预热阶段”，N步），允许高斯点相对自由地偏离3DMM网格。在此阶段，我们计算每个高斯点与其绑定的3DMM网格面片之间的平均局部距离（即`Distance(k)`，见公式3）。如果某个面部区域的`Distance(k)`值较大，表明3DMM在该区域拟合不佳，需要更多的柔性来保持细节；反之，则需要更强的刚性来保持与3DMM的对应。\n            *   **逻辑：** 这是一个无监督、数据驱动且简单有效的策略。它让系统自行学习哪些区域需要“刚性”（如面部中心），哪些需要“柔性”（如毛发、耳朵），从而实现自适应正则化（图2c）。这个决策直接解决了问题1。\n\n    2.  **新型口腔结构与部分形变（Novel Mouth Structure & Part-wise Deformation）：**\n        *   **思考：** 针对“口腔高动态与结构缺失”的问题，仅依靠现有3DMM和简单的唇环复制是远远不够的。\n        *   **权衡与取舍：**\n            *   **口腔结构建模：** 我们决定超越仅有门牙的简化模型，而是参考生物学解剖结构，在FLAME模型的基础上**显式地添加磨牙、腭部（palate）和口底（floor）结构**。这需要对3DMM网格进行修改（图4）。\n            *   **形变策略：** 考虑到口腔内部结构虽然整体运动复杂，但其子部分（如上牙与腭部，下牙与口底）在运动过程中具有相对一致性。我们提出了**部分形变（part-wise deformation）策略**：将口腔结构分为上部和下部，分别通过独立的形变网络进行形变（公式4、5），并额外将时间步（timestep `T`）作为输入（公式6）。\n            *   **逻辑：** 这种策略既提供了更精确的几何先验（通过添加结构），又通过部分形变网络捕捉了细微的、帧级别的、非3DMM参数化的运动偏差，同时保证了口腔内部结构在运动中的相对一致性。将`T`作为输入是为了捕捉即使在FLAME参数无法解释的微小对齐问题。这个决策直接解决了问题2。\n\n    3.  **新型正则化损失（Regularization Loss）：**\n        *   **思考：** 现有的位置损失（如GaussianAvatars [40]）只关注高斯点到其绑定网格面片的距离，这不足以防止高斯点在空间中发生“翻转”或偏离其预期方向。\n        *   **权衡与取舍：** 我们引入了`L_angle`损失（公式8），它惩罚高斯点局部均值与对应网格面片法线方向之间的角度过大。\n        *   **逻辑：** 这确保了高斯点不仅在距离上接近其绑定的网格面片，而且在方向上也保持一致。这对于保持纹理的连续性和避免动画伪影至关重要，特别是当高斯点位于面部边界或快速运动区域时（图5）。这增强了高斯点与3DMM网格的精确绑定（rigging），提升了动画的保真度。\n\n    4.  **数据集构建：DynamicFace**\n        *   **思考：** 现有数据集无法充分验证高表现力场景。\n        *   **权衡与取舍：** 我们决定自行采集一个高质量、高表现力的单目视频数据集。\n        *   **逻辑：** DynamicFace数据集包含10名演员，每段2-3分钟，包含丰富的面部表情和头部运动（如缓慢摇头），并使用绿幕背景方便分离。这使得我们能够更全面、更准确地评估模型在动态动画场景下的性能，并为社区提供了一个新的基准（图18，表10）。\n\n*   **数据分析策略：**\n    *   我们采用了全面的定量和定性评估方法。\n    *   **定量：** 除了常用的MSE、PSNR、SSIM、LPIPS等图像质量指标外，我们还利用预训练的InsightFace模型（用于ID保真度）和FER2013模型（用于表情保真度）进行评价。这种多维度评估能更全面地反映头像的质量和实用性。\n    *   **定性：** 重点展示自重演（self-reenactment）、跨身份重演（cross-reenactment）和新视角合成（novel-view synthesis）结果，并通过特写放大关键区域（如口腔、眼睛、毛发），直观展示模型的优势和基线方法的缺陷。\n    *   **消融研究（Ablation Study）：** 这是我们验证每个组件有效性的关键。我们设计了详细的消融实验（表3, 5, 6），逐步加入APS、口腔结构、部分形变和`L_angle`损失，观察性能的提升，从而清晰地展示每个组件的贡献。\n\n### 3. 在研究的每个关键阶段，您做出了哪些重要决策，并能解释这些决策背后的逻辑和依据。\n\n我将通过**“如何让高斯点更好地服务于动态面部”**这一主线，串联起整个研发过程中的关键决策。\n\n**阶段一：模型初始化与预热——让高斯点“自由生长”，收集形变数据**\n\n*   **观察与痛点：** 刚开始，我们直接用3DMM网格去初始化高斯点并立即应用紧密的正则化。结果发现，在3DMM拟合不好的区域（如头顶、耳朵、毛发），高斯点会为了强行贴合网格而扭曲，导致重建效果不佳，且在动画时出现伪影。\n*   **关键决策：** 在正式应用APS之前，我们决定引入一个**“预热阶段”（Adaptive Pre-allocation Stage, APS前的N步迭代，论文中N设为100,000步）**。在这个阶段，我们对高斯点的正则化约束相对宽松（特别是位置损失`Lp(r)`使用一个较大的阈值`Tf=2.0`，类似于灵活集Ff的阈值），并且暂时将除了新添加的口腔结构之外的所有区域都视为潜在的“灵活集”。\n*   **逻辑与依据：**\n    1.  **数据收集：** 这个阶段的目的是允许高斯点相对自由地根据图像内容进行“自我调整”和“自由生长”，从而更准确地反映真实的几何细节，即使这些细节与初始的3DMM网格有较大偏差。\n    2.  **信息积累：** 在这个宽松的阶段，每个高斯点都会逐渐偏离其初始绑定的3DMM网格面片，形成一个“局部均值距离”值。这些距离值正是我们后续APS阶段进行刚性/柔性分类的依据。\n    3.  **防止过早刚化：** 设想一缕飘逸的头发。如果一开始就将其强制“刚化”到3DMM头皮上，它会与真实形状产生冲突，导致模糊。给予它初期自由度，它能学习其在空间中的真实位置，然后APS再根据其偏离程度决定它是否应保持柔性。\n\n**阶段二：自适应预分配——区分“刚性”与“柔性”，实现精准约束**\n\n*   **观察与痛点：** 预热阶段结束后，我们发现不同面部区域的高斯点表现出的“局部均值距离”确实有显著差异，验证了我们的核心洞察（图2a）。如果继续对所有区域一视同仁，就无法兼顾重建质量与动画鲁棒性。\n*   **关键决策：** 在预热阶段结束后，我们**运行APS**。具体做法是：对于3DMM的每个面部部分（如鼻子、眼睛、头皮、耳朵等），我们计算绑定到该部分的**所有高斯点的“局部均值距离”的平均值**（`Distance(k)`，公式3）。然后，我们设定一个经验阈值`Tpart`。\n    *   如果`Distance(k) < Tpart`，则该区域被分类为**“刚性集”（rigid set, Fr）**。\n    *   如果`Distance(k) > Tpart`，则该区域被分类为**“灵活集”（flexible set, Ff）**。\n*   **逻辑与依据：**\n    1.  **数据驱动的区域划分：** 这个决策是整个GeoAvatar的基石。它实现了无监督、自适应的区域形变管理。`Distance(k)`的大小直接反映了3DMM在该区域的拟合准确性。\n    2.  **刚性区域的强化约束：** 对于`Fr`，我们对其高斯点施加更严格的位置正则化（更小的`Tr=0.1`）。例如，脸颊、鼻子等3DMM拟合良好的区域，高斯点被强制更紧密地跟随3DMM网格，从而在动画时保持高保真和稳定性。\n    3.  **柔性区域的保留细节：** 对于`Ff`，我们保留较大的位置正则化阈值（`Tf=2.0`）。例如，头皮、耳朵、长发等3DMM拟合不佳或细节丰富（图2c）。这使得高斯点可以在一定程度上独立于3DMM，从而保留原始重建的精细纹理和几何细节。\n    4.  **消融验证：** 表3和表5中的消融实验清晰表明，应用APS后，LPIPS（感知距离）显著降低，定性结果（图8）也显示头发和眼镜等区域的清晰度明显提升。\n\n**阶段三：口腔精细建模与形变——处理“老大难”区域**\n\n*   **观察与痛点：** 即使有了APS，口腔区域依然是挑战。3DMM本身结构缺失，简单的唇环复制不足以模拟牙齿和口腔内部。口腔的高动态性也使得简单的全局形变难以捕捉其内部的协调运动。\n*   **关键决策1：** **修改FLAME，引入解剖学精确的口腔内部结构**。我们不仅保留了门牙，还**显式地添加了磨牙、腭部和口底的几何先验**（图4）。\n*   **逻辑与依据：**\n    1.  **提供更好的几何先验：** 这是基础。没有正确的几何结构作为高斯点的锚点，再好的形变策略也无济于事。我们意识到必须从根本上解决3DMM在口腔建模上的不足。\n    2.  **符合生物学规律：** 牙齿、腭部和口底的相对位置在口腔运动中具有一定的稳定性和协调性。\n*   **关键决策2：** 引入**部分形变（part-wise deformation）网络**，将口腔结构分为上部（上牙、腭部）和下部（下牙、口底），分别进行形变。并且，形变网络的输入除了FLAME参数外，**额外加入了时间步（timestep `T`）**。\n*   **逻辑与依据：**\n    1.  **口腔内部的协调性：** 尽管整个口腔运动复杂，但例如“上牙总是跟随腭部移动”这样的内部协调性是存在的。部分形变尊重了这种内部结构关系。\n    2.  **捕捉微小、帧级偏差：** FLAME参数是基于整个面部的，可能无法完美捕捉每帧中口腔内部的细微对齐偏差。引入`T`作为输入，使得形变网络能够学习到与时间相关的、更精细的调整，从而克服仅依赖FLAME参数的局限性。这在口型快速变化时尤为重要。\n    3.  **消融验证：** 表3和图8的消融实验再次证明了这些决策的有效性。从基线A到C（添加FLAME口腔结构）再到D（添加部分形变），牙齿的生成质量和整体渲染效果均有显著提升。\n\n**阶段四：精确绑定正则化——确保高斯点“各就各位”**\n\n*   **观察与痛点：** 即使有了APS和改进的口腔形变，我们仍然发现一些高斯点有时会“越界”，即它们虽然在距离上接近其绑定的网格面片，但其在空间中的方向可能发生了偏差，导致动画时出现不自然的拉伸或穿模（图5a）。例如，本应属于嘴唇皮肤的高斯点可能在动画时“指向”鼻子，导致伪影。\n*   **关键决策：** 引入**`L_angle`正则化损失**（公式8），惩罚高斯点局部均值与对应网格面片法线方向之间的角度过大。\n*   **逻辑与依据：**\n    1.  **方向性约束：** 这是一种更精确的“各就各位”的约束。它不仅仅关心距离，更关心“朝向”。如果一个高斯点被分配给一个面片，它应该大致位于该面片的前方，而不是侧面或后方。\n    2.  **避免伪影：** 这极大地提升了高斯点与3DMM网格的“绑定（rigging）精度”，确保了高斯点在动画过程中能够稳定地跟随其预期区域，有效消除了因方向性偏差导致的动画伪影（图8最后一列的口腔和眼镜区域）。\n    3.  **消融验证：** 表3显示，从D到E（GeoAvatar，增加了`L_angle`），虽然PSNR和SSIM略有波动，但LPIPS（更接近人类感知的指标）进一步降低，证明了其对视觉质量的提升。\n\n**阶段五：数据集支持与全面评估**\n\n*   **观察与痛点：** 现有的单目视频数据集在面部表情和头部运动的丰富度上有所欠缺，难以充分测试模型的鲁棒性，尤其是口腔等高动态区域。\n*   **关键决策：** **自制高表现力单目视频数据集DynamicFace**。\n*   **逻辑与依据：**\n    1.  **弥补数据空白：** 为了更准确地评估和训练模型，特别是针对口腔等高动态区域的性能，一个包含多样化、高表现力动作的基准数据集是必不可少的。\n    2.  **场景模拟：** 该数据集旨在模拟真实对话和表演中可能出现的复杂面部运动，从而更真实地反映模型在实际应用中的表现。\n\n---\n\n总而言之，我们GeoAvatar的研发过程是一个迭代、观察、洞察、决策和验证的循环。从最初对现有方法的痛点观察（统一正则化、口腔结构缺失），到提出自适应形变、精细口腔建模和精确绑定等创新解决方案，每一步都基于对问题的深刻理解和对现有技术的权衡取舍。最核心的逻辑是：**理解3DMM的优势与局限，允许高斯点在可控范围内偏离3DMM以捕捉真实细节，并根据区域特性和生物学结构进行自适应、精细化的管理。** 最终，这些决策共同促成了GeoAvatar在重建和动画场景下超越SOTA的性能表现。"
    },
    {
        "order": 96,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18262",
        "abs_url": "https://arxiv.org/abs/2507.18262",
        "pdf_url": "https://arxiv.org/pdf/2507.18262",
        "title": "ReSem3D: Refinable 3D Spatial Constraints via Fine-Grained Semantic Grounding for Generalizable Robotic Manipulation",
        "authors": [
            "Chenyu Su",
            "Weiwei Shang",
            "Chen Qian",
            "Fei Zhang",
            "Shuang Cong"
        ],
        "comments": "12 pages,9 figures",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)",
        "abstract": "Semantics-driven 3D spatial constraints align highlevel semantic representations with low-level action spaces, facilitating the unification of task understanding and execution in robotic manipulation. The synergistic reasoning of Multimodal Large Language Models (MLLMs) and Vision Foundation Models (VFMs) enables cross-modal 3D spatial constraint construction. Nevertheless, existing methods have three key limitations: (1) coarse semantic granularity in constraint modeling, (2) lack of real-time closed-loop planning, (3) compromised robustness in semantically diverse environments. To address these challenges, we propose ReSem3D, a unified manipulation framework for semantically diverse environments, leveraging the synergy between VFMs and MLLMs to achieve fine-grained visual grounding and dynamically constructs hierarchical 3D spatial constraints for real-time manipulation. Specifically, the framework is driven by hierarchical recursive reasoning in MLLMs, which interact with VFMs to automatically construct 3D spatial constraints from natural language instructions and RGB-D observations in two stages: part-level extraction and region-level refinement. Subsequently, these constraints are encoded as real-time optimization objectives in joint space, enabling reactive behavior to dynamic disturbances. Extensive simulation and real-world experiments are conducted in semantically rich household and sparse chemical lab environments. The results demonstrate that ReSem3D performs diverse manipulation tasks under zero-shot conditions, exhibiting strong adaptability and generalization. Code and videos at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 97,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18323",
        "abs_url": "https://arxiv.org/abs/2507.18323",
        "pdf_url": "https://arxiv.org/pdf/2507.18323",
        "title": "A Multi-Dataset Benchmark for Semi-Supervised Semantic Segmentation in ECG Delineation",
        "authors": [
            "Minje Park",
            "Jeonghwa Lim",
            "Taehyung Yu",
            "Sunghoon Joo"
        ],
        "comments": "6 pages, 2 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Signal Processing (eess.SP)",
        "abstract": "Electrocardiogram (ECG) delineation, the segmentation of meaningful waveform features, is critical for clinical diagnosis. Despite recent advances using deep learning, progress has been limited by the scarcity of publicly available annotated datasets. Semi-supervised learning presents a promising solution by leveraging abundant unlabeled ECG data. In this study, we present the first systematic benchmark for semi-supervised semantic segmentation (SemiSeg) in ECG delineation. We curated and unified multiple public datasets, including previously underused sources, to support robust and diverse evaluation. We adopted five representative SemiSeg algorithms from computer vision, implemented them on two different architectures: the convolutional network and the transformer, and evaluated them in two different settings: in-domain and cross-domain. Additionally, we propose ECG-specific training configurations and augmentation strategies and introduce a standardized evaluation framework. Our results show that the transformer outperforms the convolutional network in semi-supervised ECG delineation. We anticipate that our benchmark will serve as a foundation for advancing semi-supervised ECG delineation methods and will facilitate further research in this domain.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 98,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18330",
        "abs_url": "https://arxiv.org/abs/2507.18330",
        "pdf_url": "https://arxiv.org/pdf/2507.18330",
        "title": "GVCCS: A Dataset for Contrail Identification and Tracking on Visible Whole Sky Camera Sequences",
        "authors": [
            "Gabriel Jarry",
            "Ramon Dalmau",
            "Philippe Very",
            "Franck Ballerini",
            "Stephania-Denisa Bocu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Aviation's climate impact includes not only CO2 emissions but also significant non-CO2 effects, especially from contrails. These ice clouds can alter Earth's radiative balance, potentially rivaling the warming effect of aviation CO2. Physics-based models provide useful estimates of contrail formation and climate impact, but their accuracy depends heavily on the quality of atmospheric input data and on assumptions used to represent complex processes like ice particle formation and humidity-driven persistence. Observational data from remote sensors, such as satellites and ground cameras, could be used to validate and calibrate these models. However, existing datasets don't explore all aspect of contrail dynamics and formation: they typically lack temporal tracking, and do not attribute contrails to their source flights. To address these limitations, we present the Ground Visible Camera Contrail Sequences (GVCCS), a new open data set of contrails recorded with a ground-based all-sky camera in the visible range. Each contrail is individually labeled and tracked over time, allowing a detailed analysis of its lifecycle. The dataset contains 122 video sequences (24,228 frames) and includes flight identifiers for contrails that form above the camera. As reference, we also propose a unified deep learning framework for contrail analysis using a panoptic segmentation model that performs semantic segmentation (contrail pixel identification), instance segmentation (individual contrail separation), and temporal tracking in a single architecture. By providing high-quality, temporally resolved annotations and a benchmark for model evaluation, our work supports improved contrail monitoring and will facilitate better calibration of physical models. This sets the groundwork for more accurate climate impact understanding and assessments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 99,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18332",
        "abs_url": "https://arxiv.org/abs/2507.18332",
        "pdf_url": "https://arxiv.org/pdf/2507.18332",
        "title": "Hierarchical Dimensionless Learning (Hi-π): A physics-data hybrid-driven approach for discovering dimensionless parameter combinations",
        "authors": [
            "Mingkun Xia",
            "Haitao Lin",
            "Weiwei Zhang"
        ],
        "comments": "",
        "subjects": "Fluid Dynamics (physics.flu-dyn); Machine Learning (cs.LG); Data Analysis, Statistics and Probability (physics.data-an)",
        "abstract": "Dimensional analysis provides a universal framework for reducing physical complexity and reveal inherent laws. However, its application to high-dimensional systems still generates redundant dimensionless parameters, making it challenging to establish physically meaningful descriptions. Here, we introduce Hierarchical Dimensionless Learning (Hi-{\\pi}), a physics-data hybrid-driven method that combines dimensional analysis and symbolic regression to automatically discover key dimensionless parameter combination(s). We applied this method to classic examples in various research fields of fluid mechanics. For the Rayleigh-Bénard convection, this method accurately extracted two intrinsic dimensionless parameters: the Rayleigh number and the Prandtl number, validating its unified representation advantage across multiscale data. For the viscous flows in a circular pipe, the method automatically discovers two optimal dimensionless parameters: the Reynolds number and relative roughness, achieving a balance between accuracy and complexity. For the compressibility correction in subsonic flow, the method effectively extracts the classic compressibility correction formulation, while demonstrating its capability to discover hierarchical structural expressions through optimal parameter transformations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 100,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18352",
        "abs_url": "https://arxiv.org/abs/2507.18352",
        "pdf_url": "https://arxiv.org/pdf/2507.18352",
        "title": "Tiny is not small enough: High-quality, low-resource facial animation models through hybrid knowledge distillation",
        "authors": [
            "Zhen Han",
            "Mattias Teye",
            "Derek Yadgaroff",
            "Judith Bütepage"
        ],
        "comments": "Accepted to ACM Transactions on Graphics 2025 (SIGGRAPH journal track)",
        "subjects": "Graphics (cs.GR); Machine Learning (cs.LG); Multimedia (cs.MM); Sound (cs.SD); Audio and Speech Processing (eess.AS)",
        "abstract": "The training of high-quality, robust machine learning models for speech-driven 3D facial animation requires a large, diverse dataset of high-quality audio-animation pairs. To overcome the lack of such a dataset, recent work has introduced large pre-trained speech encoders that are robust to variations in the input audio and, therefore, enable the facial animation model to generalize across speakers, audio quality, and languages. However, the resulting facial animation models are prohibitively large and lend themselves only to offline inference on a dedicated machine. In this work, we explore on-device, real-time facial animation models in the context of game development. We overcome the lack of large datasets by using hybrid knowledge distillation with pseudo-labeling. Given a large audio dataset, we employ a high-performing teacher model to train very small student models. In contrast to the pre-trained speech encoders, our student models only consist of convolutional and fully-connected layers, removing the need for attention context or recurrent updates. In our experiments, we demonstrate that we can reduce the memory footprint to up to 3.4 MB and required future audio context to up to 81 ms while maintaining high-quality animations. This paves the way for on-device inference, an important step towards realistic, model-driven digital characters.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 101,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18372",
        "abs_url": "https://arxiv.org/abs/2507.18372",
        "pdf_url": "https://arxiv.org/pdf/2507.18372",
        "title": "On Reconstructing Training Data From Bayesian Posteriors and Trained Models",
        "authors": [
            "George Wynne"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Statistics Theory (math.ST)",
        "abstract": "Publicly releasing the specification of a model with its trained parameters means an adversary can attempt to reconstruct information about the training data via training data reconstruction attacks, a major vulnerability of modern machine learning methods. This paper makes three primary contributions: establishing a mathematical framework to express the problem, characterising the features of the training data that are vulnerable via a maximum mean discrepancy equivalance and outlining a score matching framework for reconstructing data in both Bayesian and non-Bayesian models, the former is a first in the literature.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 102,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18392",
        "abs_url": "https://arxiv.org/abs/2507.18392",
        "pdf_url": "https://arxiv.org/pdf/2507.18392",
        "title": "CLEAR: Error Analysis via LLM-as-a-Judge Made Easy",
        "authors": [
            "Asaf Yehudai",
            "Lilach Eden",
            "Yotam Perlitz",
            "Roy Bar-Haim",
            "Michal Shmueli-Scheuer"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "The evaluation of Large Language Models (LLMs) increasingly relies on other LLMs acting as judges. However, current evaluation paradigms typically yield a single score or ranking, answering which model is better but not why. While essential for benchmarking, these top-level scores obscure the specific, actionable reasons behind a model's performance. To bridge this gap, we introduce CLEAR, an interactive, open-source package for LLM-based error analysis. CLEAR first generates per-instance textual feedback, then it creates a set of system-level error issues, and quantifies the prevalence of each identified issue. Our package also provides users with an interactive dashboard that allows for a comprehensive error analysis through aggregate visualizations, applies interactive filters to isolate specific issues or score ranges, and drills down to the individual instances that exemplify a particular behavioral pattern. We demonstrate CLEAR analysis for RAG and Math benchmarks, and showcase its utility through a user case study.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 103,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18405",
        "abs_url": "https://arxiv.org/abs/2507.18405",
        "pdf_url": "https://arxiv.org/pdf/2507.18405",
        "title": "Iwin Transformer: Hierarchical Vision Transformer using Interleaved Windows",
        "authors": [
            "Simin Huo",
            "Ning Li"
        ],
        "comments": "14 pages, 10 figures, Submitted to IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "We introduce Iwin Transformer, a novel position-embedding-free hierarchical vision transformer, which can be fine-tuned directly from low to high resolution, through the collaboration of innovative interleaved window attention and depthwise separable convolution. This approach uses attention to connect distant tokens and applies convolution to link neighboring tokens, enabling global information exchange within a single module, overcoming Swin Transformer's limitation of requiring two consecutive blocks to approximate global attention. Extensive experiments on visual benchmarks demonstrate that Iwin Transformer exhibits strong competitiveness in tasks such as image classification (87.4 top-1 accuracy on ImageNet-1K), semantic segmentation and video action recognition. We also validate the effectiveness of the core component in Iwin as a standalone module that can seamlessly replace the self-attention module in class-conditional image generation. The concepts and methods introduced by the Iwin Transformer have the potential to inspire future research, like Iwin 3D Attention in video generation. The code and models are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 104,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18417",
        "abs_url": "https://arxiv.org/abs/2507.18417",
        "pdf_url": "https://arxiv.org/pdf/2507.18417",
        "title": "FinDPO: Financial Sentiment Analysis for Algorithmic Trading through Preference Optimization of LLMs",
        "authors": [
            "Giorgos Iacovides",
            "Wuyang Zhou",
            "Danilo Mandic"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG); Statistical Finance (q-fin.ST); Trading and Market Microstructure (q-fin.TR)",
        "abstract": "Opinions expressed in online finance-related textual data are having an increasingly profound impact on trading decisions and market movements. This trend highlights the vital role of sentiment analysis as a tool for quantifying the nature and strength of such opinions. With the rapid development of Generative AI (GenAI), supervised fine-tuned (SFT) large language models (LLMs) have become the de facto standard for financial sentiment analysis. However, the SFT paradigm can lead to memorization of the training data and often fails to generalize to unseen samples. This is a critical limitation in financial domains, where models must adapt to previously unobserved events and the nuanced, domain-specific language of finance. To this end, we introduce FinDPO, the first finance-specific LLM framework based on post-training human preference alignment via Direct Preference Optimization (DPO). The proposed FinDPO achieves state-of-the-art performance on standard sentiment classification benchmarks, outperforming existing supervised fine-tuned models by 11% on the average. Uniquely, the FinDPO framework enables the integration of a fine-tuned causal LLM into realistic portfolio strategies through a novel 'logit-to-score' conversion, which transforms discrete sentiment predictions into continuous, rankable sentiment scores (probabilities). In this way, simulations demonstrate that FinDPO is the first sentiment-based approach to maintain substantial positive returns of 67% annually and strong risk-adjusted performance, as indicated by a Sharpe ratio of 2.0, even under realistic transaction costs of 5 basis points (bps).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 105,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18429",
        "abs_url": "https://arxiv.org/abs/2507.18429",
        "pdf_url": "https://arxiv.org/pdf/2507.18429",
        "title": "NLML-HPE: Head Pose Estimation with Limited Data via Manifold Learning",
        "authors": [
            "Mahdi Ghafourian",
            "Federico M. Sukno"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Head pose estimation (HPE) plays a critical role in various computer vision applications such as human-computer interaction and facial recognition. In this paper, we propose a novel deep learning approach for head pose estimation with limited training data via non-linear manifold learning called NLML-HPE. This method is based on the combination of tensor decomposition (i.e., Tucker decomposition) and feed forward neural networks. Unlike traditional classification-based approaches, our method formulates head pose estimation as a regression problem, mapping input landmarks into a continuous representation of pose angles. To this end, our method uses tensor decomposition to split each Euler angle (yaw, pitch, roll) to separate subspaces and models each dimension of the underlying manifold as a cosine curve. We address two key challenges: 1. Almost all HPE datasets suffer from incorrect and inaccurate pose annotations. Hence, we generated a precise and consistent 2D head pose dataset for our training set by rotating 3D head models for a fixed set of poses and rendering the corresponding 2D images. 2. We achieved real-time performance with limited training data as our method accurately captures the nature of rotation of an object from facial landmarks. Once the underlying manifold for rotation around each axis is learned, the model is very fast in predicting unseen data. Our training and testing code is available online along with our trained models: https: //github.com/MahdiGhafoorian/NLML_HPE.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 106,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18448",
        "abs_url": "https://arxiv.org/abs/2507.18448",
        "pdf_url": "https://arxiv.org/pdf/2507.18448",
        "title": "Restoring Rhythm: Punctuation Restoration Using Transformer Models for Bangla, a Low-Resource Language",
        "authors": [
            "Md Obyedullahil Mamun",
            "Md Adyelullahil Mamun",
            "Arif Ahmad",
            "Md. Imran Hossain Emu"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Punctuation restoration enhances the readability of text and is critical for post-processing tasks in Automatic Speech Recognition (ASR), especially for low-resource languages like Bangla. In this study, we explore the application of transformer-based models, specifically XLM-RoBERTa-large, to automatically restore punctuation in unpunctuated Bangla text. We focus on predicting four punctuation marks: period, comma, question mark, and exclamation mark across diverse text domains. To address the scarcity of annotated resources, we constructed a large, varied training corpus and applied data augmentation techniques. Our best-performing model, trained with an augmentation factor of alpha = 0.20%, achieves an accuracy of 97.1% on the News test set, 91.2% on the Reference set, and 90.2% on the ASR set. Results show strong generalization to reference and ASR transcripts, demonstrating the model's effectiveness in real-world, noisy scenarios. This work establishes a strong baseline for Bangla punctuation restoration and contributes publicly available datasets and code to support future research in low-resource NLP.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 107,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18449",
        "abs_url": "https://arxiv.org/abs/2507.18449",
        "pdf_url": "https://arxiv.org/pdf/2507.18449",
        "title": "Digital Twin Technologies in Predictive Maintenance: Enabling Transferability via Sim-to-Real and Real-to-Sim Transfer",
        "authors": [
            "Sizhe Ma",
            "Katherine A. Flanigan",
            "Mario Bergés"
        ],
        "comments": "Accepted and presented at 2024 ASCE International Conference on Computing in Civil Engineering (i3CE 2024)",
        "subjects": "Computational Engineering, Finance, and Science (cs.CE); Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG)",
        "abstract": "The advancement of the Internet of Things (IoT) and Artificial Intelligence has catalyzed the evolution of Digital Twins (DTs) from conceptual ideas to more implementable realities. Yet, transitioning from academia to industry is complex due to the absence of standardized frameworks. This paper builds upon the authors' previously established functional and informational requirements supporting standardized DT development, focusing on a crucial aspect: transferability. While existing DT research primarily centers on asset transfer, the significance of \"sim-to-real transfer\" and \"real-to-sim transfer\"--transferring knowledge between simulations and real-world operations--is vital for comprehensive lifecycle management in DTs. A key challenge in this process is calibrating the \"reality gap,\" the discrepancy between simulated predictions and actual outcomes. Our research investigates the impact of integrating a single Reality Gap Analysis (RGA) module into an existing DT framework to effectively manage both sim-to-real and real-to-sim transfers. This integration is facilitated by data pipelines that connect the RGA module with the existing components of the DT framework, including the historical repository and the simulation model. A case study on a pedestrian bridge at Carnegie Mellon University showcases the performance of different levels of integration of our approach with an existing framework. With full implementation of an RGA module and a complete data pipeline, our approach is capable of bidirectional knowledge transfer between simulations and real-world operations without compromising efficiency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 108,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18450",
        "abs_url": "https://arxiv.org/abs/2507.18450",
        "pdf_url": "https://arxiv.org/pdf/2507.18450",
        "title": "High-Dimensional Data Classification in Concentric Coordinates",
        "authors": [
            "Alice Williams",
            "Boris Kovalerchuk"
        ],
        "comments": "8 pages, 21 figures",
        "subjects": "Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)",
        "abstract": "The visualization of multi-dimensional data with interpretable methods remains limited by capabilities for both high-dimensional lossless visualizations that do not suffer from occlusion and that are computationally capable by parameterized visualization. This paper proposes a low to high dimensional data supporting framework using lossless Concentric Coordinates that are a more compact generalization of Parallel Coordinates along with former Circular Coordinates. These are forms of the General Line Coordinate visualizations that can directly support machine learning algorithm visualization and facilitate human interaction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 109,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18464",
        "abs_url": "https://arxiv.org/abs/2507.18464",
        "pdf_url": "https://arxiv.org/pdf/2507.18464",
        "title": "DriftMoE: A Mixture of Experts Approach to Handle Concept Drifts",
        "authors": [
            "Miguel Aspis",
            "Sebastián A. Cajas Ordónez",
            "Andrés L. Suárez-Cetrulo",
            "Ricardo Simón Carbajo"
        ],
        "comments": "Accepted at the SYNDAiTE@ECMLPKDD 2025 workshop",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Learning from non-stationary data streams subject to concept drift requires models that can adapt on-the-fly while remaining resource-efficient. Existing adaptive ensemble methods often rely on coarse-grained adaptation mechanisms or simple voting schemes that fail to optimally leverage specialized knowledge. This paper introduces DriftMoE, an online Mixture-of-Experts (MoE) architecture that addresses these limitations through a novel co-training framework. DriftMoE features a compact neural router that is co-trained alongside a pool of incremental Hoeffding tree experts. The key innovation lies in a symbiotic learning loop that enables expert specialization: the router selects the most suitable expert for prediction, the relevant experts update incrementally with the true label, and the router refines its parameters using a multi-hot correctness mask that reinforces every accurate expert. This feedback loop provides the router with a clear training signal while accelerating expert specialization. We evaluate DriftMoE's performance across nine state-of-the-art data stream learning benchmarks spanning abrupt, gradual, and real-world drifts testing two distinct configurations: one where experts specialize on data regimes (multi-class variant), and another where they focus on single-class specialization (task-based variant). Our results demonstrate that DriftMoE achieves competitive results with state-of-the-art stream learning adaptive ensembles, offering a principled and efficient approach to concept drift adaptation. All code, data pipelines, and reproducibility scripts are available in our public GitHub repository: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 110,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18504",
        "abs_url": "https://arxiv.org/abs/2507.18504",
        "pdf_url": "https://arxiv.org/pdf/2507.18504",
        "title": "Not All Features Deserve Attention: Graph-Guided Dependency Learning for Tabular Data Generation with Language Models",
        "authors": [
            "Zheyu Zhang",
            "Shuo Yang",
            "Bardh Prenkaj",
            "Gjergji Kasneci"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Large Language Models (LLMs) have shown strong potential for tabular data generation by modeling textualized feature-value pairs. However, tabular data inherently exhibits sparse feature-level dependencies, where many feature interactions are structurally insignificant. This creates a fundamental mismatch as LLMs' self-attention mechanism inevitably distributes focus across all pairs, diluting attention on critical relationships, particularly in datasets with complex dependencies or semantically ambiguous features. To address this limitation, we propose GraDe (Graph-Guided Dependency Learning), a novel method that explicitly integrates sparse dependency graphs into LLMs' attention mechanism. GraDe employs a lightweight dynamic graph learning module guided by externally extracted functional dependencies, prioritizing key feature interactions while suppressing irrelevant ones. Our experiments across diverse real-world datasets demonstrate that GraDe outperforms existing LLM-based approaches by up to 12% on complex datasets while achieving competitive results with state-of-the-art approaches in synthetic data quality. Our method is minimally intrusive yet effective, offering a practical solution for structure-aware tabular data modeling with LLMs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 111,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18520",
        "abs_url": "https://arxiv.org/abs/2507.18520",
        "pdf_url": "https://arxiv.org/pdf/2507.18520",
        "title": "Euclidean Distance Deflation Under High-Dimensional Heteroskedastic Noise",
        "authors": [
            "Keyi Li",
            "Yuval Kluger",
            "Boris Landa"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Statistics Theory (math.ST)",
        "abstract": "Pairwise Euclidean distance calculation is a fundamental step in many machine learning and data analysis algorithms. In real-world applications, however, these distances are frequently distorted by heteroskedastic noise$\\unicode{x2014}$a prevalent form of inhomogeneous corruption characterized by variable noise magnitudes across data observations. Such noise inflates the computed distances in a nontrivial way, leading to misrepresentations of the underlying data geometry. In this work, we address the tasks of estimating the noise magnitudes per observation and correcting the pairwise Euclidean distances under heteroskedastic noise. Perhaps surprisingly, we show that in general high-dimensional settings and without assuming prior knowledge on the clean data structure or noise distribution, both tasks can be performed reliably, even when the noise levels vary considerably. Specifically, we develop a principled, hyperparameter-free approach that jointly estimates the noise magnitudes and corrects the distances. We provide theoretical guarantees for our approach, establishing probabilistic bounds on the estimation errors of both noise magnitudes and distances. These bounds, measured in the normalized $\\ell_1$ norm, converge to zero at polynomial rates as both feature dimension and dataset size increase. Experiments on synthetic datasets demonstrate that our method accurately estimates distances in challenging regimes, significantly improving the robustness of subsequent distance-based computations. Notably, when applied to single-cell RNA sequencing data, our method yields noise magnitude estimates consistent with an established prototypical model, enabling accurate nearest neighbor identification that is fundamental to many downstream analyses.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 112,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18523",
        "abs_url": "https://arxiv.org/abs/2507.18523",
        "pdf_url": "https://arxiv.org/pdf/2507.18523",
        "title": "The Moral Gap of Large Language Models",
        "authors": [
            "Maciej Skorski",
            "Alina Landowska"
        ],
        "comments": "preprint",
        "subjects": "Computation and Language (cs.CL); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)",
        "abstract": "Moral foundation detection is crucial for analyzing social discourse and developing ethically-aligned AI systems. While large language models excel across diverse tasks, their performance on specialized moral reasoning remains unclear. This study provides the first comprehensive comparison between state-of-the-art LLMs and fine-tuned transformers across Twitter and Reddit datasets using ROC, PR, and DET curve analysis. Results reveal substantial performance gaps, with LLMs exhibiting high false negative rates and systematic under-detection of moral content despite prompt engineering efforts. These findings demonstrate that task-specific fine-tuning remains superior to prompting for moral reasoning applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 113,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18534",
        "abs_url": "https://arxiv.org/abs/2507.18534",
        "pdf_url": "https://arxiv.org/pdf/2507.18534",
        "title": "Elucidating the Design Space of Arbitrary-Noise-Based Diffusion Models",
        "authors": [
            "Xingyu Qiu",
            "Mengying Yang",
            "Xinghua Ma",
            "Dong Liang",
            "Yuzhen Li",
            "Fanding Li",
            "Gongning Luo",
            "Wei Wang",
            "Kuanquan Wang",
            "Shuo Li"
        ],
        "comments": "21 pages, 4 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "EDM elucidates the unified design space of diffusion models, yet its fixed noise patterns restricted to pure Gaussian noise, limit advancements in image restoration. Our study indicates that forcibly injecting Gaussian noise corrupts the degraded images, overextends the image transformation distance, and increases restoration complexity. To address this problem, our proposed EDA Elucidates the Design space of Arbitrary-noise-based diffusion models. Theoretically, EDA expands the freedom of noise pattern while preserving the original module flexibility of EDM, with rigorous proof that increased noise complexity incurs no additional computational overhead during restoration. EDA is validated on three typical tasks: MRI bias field correction (global smooth noise), CT metal artifact reduction (global sharp noise), and natural image shadow removal (local boundary-aware noise). With only 5 sampling steps, EDA outperforms most task-specific methods and achieves state-of-the-art performance in bias field correction and shadow removal.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 114,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18538",
        "abs_url": "https://arxiv.org/abs/2507.18538",
        "pdf_url": "https://arxiv.org/pdf/2507.18538",
        "title": "AI/ML Life Cycle Management for Interoperable AI Native RAN",
        "authors": [
            "Chu-Hsiang Huang",
            "Chao-Kai Wen",
            "Geoffrey Ye Li"
        ],
        "comments": "8 pages, 4 figures, 2 table. This work has been submitted to the IEEE for possible publication",
        "subjects": "Information Theory (cs.IT); Machine Learning (cs.LG)",
        "abstract": "Artificial intelligence (AI) and machine learning (ML) models are rapidly permeating the 5G Radio Access Network (RAN), powering beam management, channel state information (CSI) feedback, positioning, and mobility prediction. However, without a standardized life-cycle management (LCM) framework, challenges, such as model drift, vendor lock-in, and limited transparency, hinder large-scale adoption. 3GPP Releases 16-20 progressively evolve AI/ML from experimental features to managed, interoperable network functions. Beginning with the Network Data Analytics Function (NWDAF) in Rel-16, subsequent releases introduced standardized interfaces for model transfer, execution, performance monitoring, and closed-loop control, culminating in Rel-20's two-sided CSI-compression Work Item and vendor-agnostic LCM profile. This article reviews the resulting five-block LCM architecture, KPI-driven monitoring mechanisms, and inter-vendor collaboration schemes, while identifying open challenges in resource-efficient monitoring, environment drift detection, intelligent decision-making, and flexible model training. These developments lay the foundation for AI-native transceivers as a key enabler for 6G.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 115,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18540",
        "abs_url": "https://arxiv.org/abs/2507.18540",
        "pdf_url": "https://arxiv.org/pdf/2507.18540",
        "title": "Deep Variational Free Energy Calculation of Hydrogen Hugoniot",
        "authors": [
            "Zihang Li",
            "Hao Xie",
            "Xinyang Dong",
            "Lei Wang"
        ],
        "comments": "7+17 pages, 5+14 figures, for source code and raw data, see this https URL",
        "subjects": "Strongly Correlated Electrons (cond-mat.str-el); Machine Learning (cs.LG); Computational Physics (physics.comp-ph)",
        "abstract": "We develop a deep variational free energy framework to compute the equation of state of hydrogen in the warm dense matter region. This method parameterizes the variational density matrix of hydrogen nuclei and electrons at finite temperature using three deep generative models: a normalizing flow model that represents the Boltzmann distribution of the classical nuclei, an autoregressive transformer that models the distribution of electrons in excited states, and a permutational equivariant flow model that constructs backflow coordinates for electrons in Hartree-Fock orbitals. By jointly optimizing the three neural networks to minimize the variational free energy, we obtain the equation of state and related thermodynamic properties of dense hydrogen. We compare our results with other theoretical and experimental results on the deuterium Hugoniot curve, aiming to resolve existing discrepancies. The calculated results provide a valuable benchmark for deuterium in the warm dense matter region.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 116,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18550",
        "abs_url": "https://arxiv.org/abs/2507.18550",
        "pdf_url": "https://arxiv.org/pdf/2507.18550",
        "title": "On the Performance of Concept Probing: The Influence of the Data (Extended Version)",
        "authors": [
            "Manuel de Sousa Ribeiro",
            "Afonso Leote",
            "João Leite"
        ],
        "comments": "Extended version of the paper published in Proceedings of the European Conference on Artificial Intelligence (ECAI 2025)",
        "subjects": "Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)",
        "abstract": "Concept probing has recently garnered increasing interest as a way to help interpret artificial neural networks, dealing both with their typically large size and their subsymbolic nature, which ultimately renders them unfeasible for direct human interpretation. Concept probing works by training additional classifiers to map the internal representations of a model into human-defined concepts of interest, thus allowing humans to peek inside artificial neural networks. Research on concept probing has mainly focused on the model being probed or the probing model itself, paying limited attention to the data required to train such probing models. In this paper, we address this gap. Focusing on concept probing in the context of image classification tasks, we investigate the effect of the data used to train probing models on their performance. We also make available concept labels for two widely used datasets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 117,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18577",
        "abs_url": "https://arxiv.org/abs/2507.18577",
        "pdf_url": "https://arxiv.org/pdf/2507.18577",
        "title": "Advancing Financial Engineering with Foundation Models: Progress, Applications, and Challenges",
        "authors": [
            "Liyuan Chen",
            "Shuoling Liu",
            "Jiangpeng Yan",
            "Xiaoyu Wang",
            "Henglin Liu",
            "Chuang Li",
            "Kecheng Jiao",
            "Jixuan Ying",
            "Yang Veronica Liu",
            "Qiang Yang",
            "Xiu Li"
        ],
        "comments": "Under Review",
        "subjects": "Computational Finance (q-fin.CP); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "The advent of foundation models (FMs) - large-scale pre-trained models with strong generalization capabilities - has opened new frontiers for financial engineering. While general-purpose FMs such as GPT-4 and Gemini have demonstrated promising performance in tasks ranging from financial report summarization to sentiment-aware forecasting, many financial applications remain constrained by unique domain requirements such as multimodal reasoning, regulatory compliance, and data privacy. These challenges have spurred the emergence of Financial Foundation Models (FFMs) - a new class of models explicitly designed for finance. This survey presents a comprehensive overview of FFMs, with a taxonomy spanning three key modalities: Financial Language Foundation Models (FinLFMs), Financial Time-Series Foundation Models (FinTSFMs), and Financial Visual-Language Foundation Models (FinVLFMs). We review their architectures, training methodologies, datasets, and real-world applications. Furthermore, we identify critical challenges in data availability, algorithmic scalability, and infrastructure constraints, and offer insights into future research opportunities. We hope this survey serves as both a comprehensive reference for understanding FFMs and a practical roadmap for future innovation. An updated collection of FFM-related publications and resources will be maintained on our website this https URL.",
        "gemini2.5flash": "",
        "overall_idea": "作为这篇名为《推动金融工程与基础模型融合：进展、应用与挑战》（Advancing Financial Engineering with Foundation Models: Progress, Applications, and Challenges）的综述文章的作者，我非常乐意深入剖析从选题到提出方法解决问题的整个思维决策过程。\n\n### 0. 为什么选择这个研究内容？\n\n选择“金融工程与基础模型”作为研究内容，是基于我们团队对当前人工智能领域和金融行业交叉点的深刻洞察。\n\n**背景洞察：**\n*   **基础模型的崛起：** 2022-2023年，以ChatGPT、GPT-4为代表的大型语言模型（LLMs）及其衍生的基础模型（FMs）在全球范围内引发了革命性的关注。它们展现出的强大通用性、泛化能力和 emergent abilities 令人惊叹。\n*   **金融领域的AI需求：** 金融工程作为一个高度数据驱动、算法密集且风险敏感的领域，长期以来一直是人工智能技术的重要应用场景。然而，传统的金融AI模型往往是针对特定任务设计的、垂直的、缺乏通用性和适应性的。\n*   **两者结合的潜力与挑战：** 自然而然地，一个核心问题浮现：这些通用基础模型能否直接应用于金融领域，并解决其复杂问题？初步的观察和尝试表明，通用FM虽然强大，但在金融特有的多模态数据（如图表、时序）、法律合规、数据隐私、可解释性及时间敏感性等方面面临显著局限。\n\n**选题的驱动力：**\n我意识到，这不仅仅是“将AI应用于金融”，而是“如何将通用AI范式（基础模型）系统性地改造和适应于金融领域的独特需求”。这个方向具有巨大的理论研究价值和实际应用潜力。如果能有效地将FMs的通用智能与金融领域的专业知识结合，将极大地推动金融工程智能化，甚至可能催生新的业态。因此，我们希望撰写一篇全面的综述，为该领域的研究者和实践者提供一份清晰的路线图。\n\n### 1. 您是如何一步步构思和界定研究问题的？\n\n研究问题的构思和界定是一个从宏观到微观、从现象到本质的迭代过程。\n\n*   **第一步：初步观察与宽泛问题（现象层面）**\n    *   **观察：** 通用FMs（如GPT-4）在自然语言处理、图像识别等领域表现惊人。金融界也开始尝试使用它们进行报告摘要、情感分析等。\n    *   **初步问题：** FMs对金融工程意味着什么？它们能取代现有金融AI模型吗？\n\n*   **第二步：识别通用FM在金融中的局限性（挑战层面）**\n    *   通过对早期文献和行业实践的梳理，我们发现通用FM在金融领域并非“即插即用”。\n    *   **局限性浮现：**\n        *   **数据模态多样性：** 金融数据不只有文本，还有大量的时序数据（股价、交易量）、视觉数据（财报图表、K线图）。通用LLM主要处理文本。\n        *   **专业知识与推理：** 金融决策需要深入的行业知识、复杂的数值计算和逻辑推理，而非简单的语言生成。\n        *   **法规与隐私：** 金融行业受到严格的监管，数据隐私和模型合规性是核心要求。通用FM可能难以满足。\n        *   **时间敏感性与时序偏见：** 金融市场实时变化，模型训练数据中包含未来信息会导致“前瞻性偏见”（lookahead bias），这在回测中是致命的。\n    *   **问题演进：** 如果通用FM有这些限制，那么需要什么样的“特殊”FM才能真正服务于金融？这引出了“金融基础模型”（Financial Foundation Models, FFMs）的概念。\n\n*   **第三步：定义“金融基础模型”及其核心特征（概念界定）**\n    *   **核心理念：** FFMs是针对金融场景专门设计的基础模型，通过领域感知的预训练、任务特定的微调以及与金融推理和监管目标的对齐，来克服通用FMs的局限。\n    *   **关键问题确定：**\n        1.  **进展：** 当前FFMs发展到了什么阶段？有哪些代表性模型？它们是如何构建和训练的？\n        2.  **应用：** FFMs已经在哪些金融任务中展现了潜力？\n        3.  **挑战：** FFMs在数据、算法、算力方面面临哪些核心挑战？如何克服？\n        4.  **未来：** FFMs的未来研究方向和创新机遇在哪里？\n\n*   **第四步：构建分类体系和分析框架（结构化问题）**\n    *   为了系统性回答上述问题，我们决定根据金融数据的核心模态，将FFMs划分为三大类：\n        *   **金融语言基础模型（FinLFMs）：** 处理金融文本（报告、新闻、合同）。\n        *   **金融时序基础模型（FinTSFMs）：** 处理金融时间序列数据（股价、经济指标）。\n        *   **金融视觉-语言基础模型（FinVLFMs）：** 联合处理金融视觉信息（图表、表格）和文本。\n    *   对于每一类，我们进一步细化分析维度：模型架构、训练方法、数据集。这使得我们的综述具有了清晰的逻辑骨架，便于读者理解和比较。\n\n**以“时序偏见”（Lookahead Bias）为例说明问题界定过程：**\n当我思考通用LLMs用于市场预测时，发现它们常常在回测中表现“异常好”。深入分析后，意识到一个根本问题：这些模型可能在训练时无意中学习了超出回测时间窗口的“未来信息”。\n*   **原始现象：** LLMs用于金融预测效果“太好”，但实盘不work。\n*   **初步假设：** 模型过拟合。\n*   **深层思考：** 这种过拟合是否与金融数据的特殊性有关？金融数据是时序的，信息发布有严格的时间顺序。\n*   **问题界定：** 识别出“Lookahead Bias”（前瞻性偏见）这一特定挑战。这不仅仅是技术上的过拟合，更是金融领域特有的“数据泄漏”问题，即模型在训练阶段接触到了在模拟回测时不可用的未来信息。\n*   **纳入框架：** 将其归类为“算法挑战”下的一个具体子问题，并提出相应的解决方案方向（如严格遵守时间序列的训练-测试分割、开发时间感知训练协议）。这个例子展示了如何从一个模糊的“效果不好”的问题，一步步剥茧抽丝，界定出金融领域特有的技术挑战。\n\n### 2. 为了解决该问题，您在选择理论框架、研究方法和数据分析策略时，经历了怎样的思考、权衡和取舍。\n\n作为一篇综述，我们的“问题解决”主要体现在如何全面、系统、深入地梳理现有工作，并为未来研究提供指导。\n\n*   **理论框架选择：**\n    *   **大模型范式：** 核心理论框架是“预训练-微调-对齐”（Pre-training, Fine-tuning, Alignment）这一大模型开发范式。这是由Andrej Karpathy等提出的，已成为行业共识。\n    *   **权衡与取舍：** 虽然有些早期金融AI模型（如FinBERT）不完全符合这个三阶段范式，但FMs的成功正是基于此。我们决定以此为主要框架，因为它能揭示FFMs如何从通用模型演进而来，并在金融领域进行哪些定制化改动。对于不完全遵循此范式的模型，我们也会特别说明其训练策略。\n    *   **多模态融合：** 针对FinTSFMs和FinVLFMs，我们引入了多模态学习和表示对齐的理论，来解释模型如何处理和融合不同类型的数据。\n\n*   **研究方法选择：**\n    *   **系统性文献回顾：** 这是综述文章的核心方法。\n        *   **思考：** 仅仅依靠传统的学术数据库（如Google Scholar, Web of Science, ArXiv）可能不足以捕捉快速发展的FM领域。许多前沿工作首先以GitHub开源项目、技术报告或公司博客的形式发布。\n        *   **决策：** 我们采取了多渠道、广撒网的策略，除了学术论文，还积极检索GitHub仓库、大型金融机构和科技公司的官方公告。这确保了我们能捕捉到最前沿、最实用的进展。\n        *   **权衡：** 这种方法工作量巨大，需要持续关注最新动态，但也换来了更高的覆盖率和时效性。\n    *   **分类学构建：**\n        *   **思考：** 如何将海量文献进行有效组织，使其具有逻辑性和可读性？\n        *   **决策：** 结合金融数据的模态和FMs的演进趋势（BERT-style, GPT-style, Reasoning-enhanced），形成了层次化的分类体系。这种分类方式既符合模型技术路线，又与金融数据的实际形态相吻合。\n        *   **取舍：** 也可以按应用场景分类，但我们认为从“模型如何处理数据”的底层逻辑出发更本质，应用场景则作为单独一节进行阐述。\n\n*   **数据分析策略（对综述内容的数据整理）：**\n    *   **表格化呈现：** 对于模型（如Table 2, Table 4, Table 6）和数据集（如Table 3, Table 5, Table 7），我们采用了详细的表格形式，统一了信息展示的维度（如模型骨干、方法、参数、训练数据、参考文献、是否开源）。\n    *   **思考：** 大量模型和数据集信息如何清晰、简洁地呈现？\n    *   **决策：** 结构化的表格可以一目了然地展示关键属性，便于读者进行快速比较和查找。\n    *   **洞察提取：** 不仅仅是罗列信息，我们还从这些数据中提炼出趋势和洞察，例如：\n        *   **模型演进趋势：** 从BERT到GPT，再到结合推理增强。\n        *   **训练策略：** 从头预训练到持续预训练和LLM适配。\n        *   **数据集挑战：** 规模小、模态单一、缺乏真实世界场景等。\n\n*   **“活”的资源库：**\n    *   **思考：** 综述文章一旦发表就“过时”了，如何保持其时效性和实用性？\n    *   **决策：** 创建并维护一个GitHub仓库（https://github.com/FinFM/Awesome-FinFMs），作为与论文同步更新的资源集合。\n    *   **权衡：** 这增加了额外的维护成本，但极大地提升了综述的实用价值和影响力，使其成为一个真正的“活的指南”。\n\n### 3. 在研究的每个关键阶段，您做出了哪些重要决策，并能解释这些决策背后的逻辑和依据。\n\n**阶段一：研究立项与初步构思**\n\n*   **重要决策1：聚焦“金融基础模型（FFMs）”而非仅仅“金融中的LLMs”。**\n    *   **逻辑与依据：** 尽管LLMs是基础模型的主流，但金融领域的数据模态远超文本。仅关注LLMs会遗漏时序和多模态视觉的关键进展。FFMs这一概念更具包容性，能涵盖所有将基础模型范式应用于金融的尝试，包括未来可能出现的、基于非文本模态的基础模型。\n    *   **例子：** 当看到MarketGPT这类直接在NASDAQ时序数据上预训练的模型，或FinVLFMs处理K线图和财报图像时，就明确了“语言”不足以概括整个领域，需要更广义的“基础模型”概念。\n\n*   **重要决策2：确定综述的三个核心维度——进展、应用和挑战。**\n    *   **逻辑与依据：** 这三个维度构成了一个完整的生命周期视角：\n        *   **进展（Progress）：** 介绍FFMs的“是什么”和“如何发展”，包括分类、架构、训练方法和数据集，为读者建立全面的知识体系。\n        *   **应用（Applications）：** 展示FFMs的“能做什么”，突出其在实际金融任务中的潜力，激发行业兴趣。\n        *   **挑战（Challenges）：** 揭示FFMs的“仍面临什么困难”，为未来的研究指明方向，并确保文章的批判性和深度。\n    *   **例子：** 在构思目录时，我们尝试过纯粹按技术分类或按应用领域分类，但最终发现“进展-应用-挑战”这种结构既能涵盖技术细节，又能兼顾实际落地，同时指出未来的瓶颈，是最全面的组织方式。\n\n**阶段二：文献收集与分类体系构建**\n\n*   **重要决策3：采用“模态驱动”的FFM分类体系（FinLFMs, FinTSFMs, FinVLFMs）。**\n    *   **逻辑与依据：**\n        1.  **数据本质：** 金融数据天然地以文本（报告、新闻）、时序（股价、交易量）、视觉（图表、表格图像）等多种模态存在。模型的输入模态是其设计和能力的根本区别。\n        2.  **技术路径：** 处理不同模态的基础模型，其核心架构和预训练任务往往不同（如LLM是Transformer decoder，TSFM可能结合patching，VLM需要视觉编码器和跨模态对齐）。按模态分类能清晰展示这些技术差异。\n        3.  **便于理解：** 对读者而言，这种分类直观且易于把握。\n    *   **例子：** 在筛选文献时，发现有些模型专注于文本，有些则尝试处理股价波动，还有的解析财报图表。这三类模型在底层技术、数据处理方式上差异显著，如果混为一谈，会使分析变得模糊。因此，将它们划分为独立章节进行阐述是必然选择。\n\n*   **重要决策4：为每个FFM类别设计统一的分析框架（模型分类、训练方法、数据集）。**\n    *   **逻辑与依据：**\n        1.  **系统性：** 保证了对所有模型分析的全面性和一致性，便于横向比较。\n        2.  **深度：** 深入探讨了FFMs的构建细节，而非简单罗列模型名称。\n        3.  **问题导向：** 通过分析数据集，能自然引出数据稀缺、质量等挑战。\n    *   **例子：** 对于FinLFMs，我们不仅列出Bert-style和GPT-style模型，更进一步分析它们的预训练、微调和对齐策略。例如，看到BloombergGPT从头预训练耗资巨大，而PIXIU、FinGPT等在开源LLM基础上进行持续预训练和SFT，我们立即识别出“连续预训练”这一重要的训练策略趋势，并将其作为子点深入阐述。\n\n**阶段三：挑战与未来机遇的提炼**\n\n*   **重要决策5：将挑战归纳为“数据、算法、算力”三大类。**\n    *   **逻辑与依据：** 这是AI系统开发和部署的三个核心支柱。任何一个环节的瓶颈都会制约FFMs的发展。这种分类方式：\n        1.  **全面性：** 涵盖了从输入到输出、从研发到落地的全链条问题。\n        2.  **结构化：** 为未来的研究指明了清晰的方向，例如数据方面要解决隐私和多模态稀缺，算法方面要解决幻觉和时间偏见，算力方面要解决高成本。\n        3.  **可操作性：** 每个挑战都能对应具体的科研方向和工程实践。\n    *   **例子：** 在讨论“数据稀缺”时，我们不仅仅指出问题，还进一步提出了“数据合成”和“联邦学习”作为潜在解决方案。在讨论“幻觉”问题时，则指向了“RAG与知识图谱结合”的路径。这些都是在分析现有挑战后，结合前沿AI技术趋势做出的重要决策。\n\n*   **重要决策6：强调建立“活”的开源资源库（GitHub Awesome List）。**\n    *   **逻辑与依据：** 金融AI和基础模型领域发展极快，一篇静态的论文很快就会过时。建立一个动态更新的资源库，能够持续跟踪最新进展，为研究者和实践者提供最新、最全的论文、工具和数据集信息，极大地提升了本综述的长期价值和影响力。\n    *   **例子：** 在撰写过程中，我们经常发现有些项目在短时间内迅速迭代或涌现新的模型。如果仅仅依赖论文发布，这些信息就会滞后。因此，将GitHub仓库作为论文的补充和延伸，是保持内容鲜活度的关键策略。\n\n这些决策背后，始终贯穿着对金融领域特殊性（高风险、高监管、数据隐私、多模态时序复杂性）的深刻理解，以及对基础模型技术演进的敏锐洞察。我们的目标是不仅要“描述”现状，更要“分析”痛点，并“指引”未来。"
    },
    {
        "order": 118,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18594",
        "abs_url": "https://arxiv.org/abs/2507.18594",
        "pdf_url": "https://arxiv.org/pdf/2507.18594",
        "title": "DRWKV: Focusing on Object Edges for Low-Light Image Enhancement",
        "authors": [
            "Xuecheng Bai",
            "Yuxiang Wang",
            "Boyu Hu",
            "Qinyuan Jie",
            "Chuanzhi Xu",
            "Hongru Xiao",
            "Kechen Li",
            "Vera Chung"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Low-light image enhancement remains a challenging task, particularly in preserving object edge continuity and fine structural details under extreme illumination degradation. In this paper, we propose a novel model, DRWKV (Detailed Receptance Weighted Key Value), which integrates our proposed Global Edge Retinex (GER) theory, enabling effective decoupling of illumination and edge structures for enhanced edge fidelity. Secondly, we introduce Evolving WKV Attention, a spiral-scanning mechanism that captures spatial edge continuity and models irregular structures more effectively. Thirdly, we design the Bilateral Spectrum Aligner (Bi-SAB) and a tailored MS2-Loss to jointly align luminance and chrominance features, improving visual naturalness and mitigating artifacts. Extensive experiments on five LLIE benchmarks demonstrate that DRWKV achieves leading performance in PSNR, SSIM, and NIQE while maintaining low computational complexity. Furthermore, DRWKV enhances downstream performance in low-light multi-object tracking tasks, validating its generalization capabilities.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 119,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18606",
        "abs_url": "https://arxiv.org/abs/2507.18606",
        "pdf_url": "https://arxiv.org/pdf/2507.18606",
        "title": "Hybrid quantum-classical algorithm for near-optimal planning in POMDPs",
        "authors": [
            "Gilberto Cunha",
            "Alexandra Ramôa",
            "André Sequeira",
            "Michael de Oliveira",
            "Luís Barbosa"
        ],
        "comments": "",
        "subjects": "Quantum Physics (quant-ph); Machine Learning (cs.LG)",
        "abstract": "Reinforcement learning (RL) provides a principled framework for decision-making in partially observable environments, which can be modeled as Markov decision processes and compactly represented through dynamic decision Bayesian networks. Recent advances demonstrate that inference on sparse Bayesian networks can be accelerated using quantum rejection sampling combined with amplitude amplification, leading to a computational speedup in estimating acceptance probabilities.\\\\ Building on this result, we introduce Quantum Bayesian Reinforcement Learning (QBRL), a hybrid quantum-classical look-ahead algorithm for model-based RL in partially observable environments. We present a rigorous, oracle-free time complexity analysis under fault-tolerant assumptions for the quantum device. Unlike standard treatments that assume a black-box oracle, we explicitly specify the inference process, allowing our bounds to more accurately reflect the true computational cost. We show that, for environments whose dynamics form a sparse Bayesian network, horizon-based near-optimal planning can be achieved sub-quadratically faster through quantum-enhanced belief updates. Furthermore, we present numerical experiments benchmarking QBRL against its classical counterpart on simple yet illustrative decision-making tasks. Our results offer a detailed analysis of how the quantum computational advantage translates into decision-making performance, highlighting that the magnitude of the advantage can vary significantly across different deployment settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 120,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18607",
        "abs_url": "https://arxiv.org/abs/2507.18607",
        "pdf_url": "https://arxiv.org/pdf/2507.18607",
        "title": "Explainable Mapper: Charting LLM Embedding Spaces Using Perturbation-Based Explanation and Verification Agents",
        "authors": [
            "Xinyuan Yan",
            "Rita Sevastjanova",
            "Sinie van der Ben",
            "Mennatallah El-Assady",
            "Bei Wang"
        ],
        "comments": "",
        "subjects": "Computational Geometry (cs.CG); Machine Learning (cs.LG)",
        "abstract": "Large language models (LLMs) produce high-dimensional embeddings that capture rich semantic and syntactic relationships between words, sentences, and concepts. Investigating the topological structures of LLM embedding spaces via mapper graphs enables us to understand their underlying structures. Specifically, a mapper graph summarizes the topological structure of the embedding space, where each node represents a topological neighborhood (containing a cluster of embeddings), and an edge connects two nodes if their corresponding neighborhoods overlap. However, manually exploring these embedding spaces to uncover encoded linguistic properties requires considerable human effort. To address this challenge, we introduce a framework for semi-automatic annotation of these embedding properties. To organize the exploration process, we first define a taxonomy of explorable elements within a mapper graph such as nodes, edges, paths, components, and trajectories. The annotation of these elements is executed through two types of customizable LLM-based agents that employ perturbation techniques for scalable and automated analysis. These agents help to explore and explain the characteristics of mapper elements and verify the robustness of the generated explanations. We instantiate the framework within a visual analytics workspace and demonstrate its effectiveness through case studies. In particular, we replicate findings from prior research on BERT's embedding properties across various layers of its architecture and provide further observations into the linguistic properties of topological neighborhoods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 121,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18616",
        "abs_url": "https://arxiv.org/abs/2507.18616",
        "pdf_url": "https://arxiv.org/pdf/2507.18616",
        "title": "SynC: Synthetic Image Caption Dataset Refinement with One-to-many Mapping for Zero-shot Image Captioning",
        "authors": [
            "Si-Woo Kim",
            "MinJu Jeon",
            "Ye-Chan Kim",
            "Soeun Lee",
            "Taewhan Kim",
            "Dong-Jin Kim"
        ],
        "comments": "Accepted to ACM Multimedia 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Zero-shot Image Captioning (ZIC) increasingly utilizes synthetic datasets generated by text-to-image (T2I) models to mitigate the need for costly manual annotation. However, these T2I models often produce images that exhibit semantic misalignments with their corresponding input captions (e.g., missing objects, incorrect attributes), resulting in noisy synthetic image-caption pairs that can hinder model training. Existing dataset pruning techniques are largely designed for removing noisy text in web-crawled data. However, these methods are ill-suited for the distinct challenges of synthetic data, where captions are typically well-formed, but images may be inaccurate representations. To address this gap, we introduce SynC, a novel framework specifically designed to refine synthetic image-caption datasets for ZIC. Instead of conventional filtering or regeneration, SynC focuses on reassigning captions to the most semantically aligned images already present within the synthetic image pool. Our approach employs a one-to-many mapping strategy by initially retrieving multiple relevant candidate images for each caption. We then apply a cycle-consistency-inspired alignment scorer that selects the best image by verifying its ability to retrieve the original caption via image-to-text retrieval. Extensive evaluations demonstrate that SynC consistently and significantly improves performance across various ZIC models on standard benchmarks (MS-COCO, Flickr30k, NoCaps), achieving state-of-the-art results in several scenarios. SynC offers an effective strategy for curating refined synthetic data to enhance ZIC.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 122,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18618",
        "abs_url": "https://arxiv.org/abs/2507.18618",
        "pdf_url": "https://arxiv.org/pdf/2507.18618",
        "title": "TRPrompt: Bootstrapping Query-Aware Prompt Optimization from Textual Rewards",
        "authors": [
            "Andreea Nica",
            "Ivan Zakazov",
            "Nicolas Mario Baldwin",
            "Saibo Geng",
            "Robert West"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Prompt optimization improves the reasoning abilities of large language models (LLMs) without requiring parameter updates to the target model. Following heuristic-based \"Think step by step\" approaches, the field has evolved in two main directions: while one group of methods uses textual feedback to elicit improved prompts from general-purpose LLMs in a training-free way, a concurrent line of research relies on numerical rewards to train a special prompt model, tailored for providing optimal prompts to the target model. In this paper, we introduce the Textual Reward Prompt framework (TRPrompt), which unifies these approaches by directly incorporating textual feedback into training of the prompt model. Our framework does not require prior dataset collection and is being iteratively improved with the feedback on the generated prompts. When coupled with the capacity of an LLM to internalize the notion of what a \"good\" prompt is, the high-resolution signal provided by the textual rewards allows us to train a prompt model yielding state-of-the-art query-specific prompts for the problems from the challenging math datasets GSMHard and MATH.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 123,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-25?abs=True",
        "arxiv_id": "2507.18632",
        "abs_url": "https://arxiv.org/abs/2507.18632",
        "pdf_url": "https://arxiv.org/pdf/2507.18632",
        "title": "SIDA: Synthetic Image Driven Zero-shot Domain Adaptation",
        "authors": [
            "Ye-Chan Kim",
            "SeungJu Cha",
            "Si-Woo Kim",
            "Taewhan Kim",
            "Dong-Jin Kim"
        ],
        "comments": "Accepted to ACM MM 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multimedia (cs.MM)",
        "abstract": "Zero-shot domain adaptation is a method for adapting a model to a target domain without utilizing target domain image data. To enable adaptation without target images, existing studies utilize CLIP's embedding space and text description to simulate target-like style features. Despite the previous achievements in zero-shot domain adaptation, we observe that these text-driven methods struggle to capture complex real-world variations and significantly increase adaptation time due to their alignment process. Instead of relying on text descriptions, we explore solutions leveraging image data, which provides diverse and more fine-grained style cues. In this work, we propose SIDA, a novel and efficient zero-shot domain adaptation method leveraging synthetic images. To generate synthetic images, we first create detailed, source-like images and apply image translation to reflect the style of the target domain. We then utilize the style features of these synthetic images as a proxy for the target domain. Based on these features, we introduce Domain Mix and Patch Style Transfer modules, which enable effective modeling of real-world variations. In particular, Domain Mix blends multiple styles to expand the intra-domain representations, and Patch Style Transfer assigns different styles to individual patches. We demonstrate the effectiveness of our method by showing state-of-the-art performance in diverse zero-shot adaptation scenarios, particularly in challenging domains. Moreover, our approach achieves high efficiency by significantly reducing the overall adaptation time.",
        "gemini2.5flash": "",
        "overall_idea": ""
    }
]