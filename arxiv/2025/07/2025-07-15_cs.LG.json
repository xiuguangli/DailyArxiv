[
    {
        "order": 1,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08828",
        "abs_url": "https://arxiv.org/abs/2507.08828",
        "pdf_url": "https://arxiv.org/pdf/2507.08828",
        "title": "Recurrent Expansion: A Pathway Toward the Next Generation of Deep Learning",
        "authors": [
            "Tarek Berghout"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "This paper introduces Recurrent Expansion (RE) as a new learning paradigm that advances beyond conventional Machine Learning (ML) and Deep Learning (DL). While DL focuses on learning from static data representations, RE proposes an additional dimension: learning from the evolving behavior of models themselves. RE emphasizes multiple mappings of data through identical deep architectures and analyzes their internal representations (i.e., feature maps) in conjunction with observed performance signals such as loss. By incorporating these behavioral traces, RE enables iterative self-improvement, allowing each model version to gain insight from its predecessors. The framework is extended through Multiverse RE (MVRE), which aggregates signals from parallel model instances, and further through Heterogeneous MVRE (HMVRE), where models of varying architectures contribute diverse perspectives. A scalable and adaptive variant, Sc-HMVRE, introduces selective mechanisms and scale diversity for real-world deployment. Altogether, RE presents a shift in DL: from purely representational learning to behavior-aware, self-evolving systems. It lays the groundwork for a new class of intelligent models capable of reasoning over their own learning dynamics, offering a path toward scalable, introspective, and adaptive artificial intelligence. A simple code example to support beginners in running their own experiments is provided in Code Availability Section of this paper.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 2,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08829",
        "abs_url": "https://arxiv.org/abs/2507.08829",
        "pdf_url": "https://arxiv.org/pdf/2507.08829",
        "title": "Efficient Triple Modular Redundancy for Reliability Enhancement of DNNs Using Explainable AI",
        "authors": [
            "Kimia Soroush",
            "Nastaran Shirazi",
            "Mohsen Raji"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Deep Neural Networks (DNNs) are widely employed in safety-critical domains, where ensuring their reliability is essential. Triple Modular Redundancy (TMR) is an effective technique to enhance the reliability of DNNs in the presence of bit-flip faults. In order to handle the significant overhead of TMR, it is applied selectively on the parameters and components with the highest contribution at the model output. Hence, the accuracy of the selection criterion plays the key role on the efficiency of TMR. This paper presents an efficient TMR approach to enhance the reliability of DNNs against bit-flip faults using an Explainable Artificial Intelligence (XAI) method. Since XAI can provide valuable insights about the importance of individual neurons and weights in the performance of the network, they can be applied as the selection metric in TMR techniques. The proposed method utilizes a low-cost, gradient-based XAI technique known as Layer-wise Relevance Propagation (LRP) to calculate importance scores for DNN parameters. These scores are then used to enhance the reliability of the model, with the most critical weights being protected by TMR. The proposed approach is evaluated on two DNN models, VGG16 and AlexNet, using datasets such as MNIST and CIFAR-10. The results demonstrate that the method can protect the AlexNet model at a bit error rate of 10-4, achieving over 60% reliability improvement while maintaining the same overhead as state-of-the-art methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 3,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08832",
        "abs_url": "https://arxiv.org/abs/2507.08832",
        "pdf_url": "https://arxiv.org/pdf/2507.08832",
        "title": "A Hybrid Machine Learning Framework for Optimizing Crop Selection via Agronomic and Economic Forecasting",
        "authors": [
            "Niranjan Mallikarjun Sindhur",
            "Pavithra C",
            "Nivya Muchikel"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Farmers in developing regions like Karnataka, India, face a dual challenge: navigating extreme market and climate volatility while being excluded from the digital revolution due to literacy barriers. This paper presents a novel decision support system that addresses both challenges through a unique synthesis of machine learning and human-computer interaction. We propose a hybrid recommendation engine that integrates two predictive models: a Random Forest classifier to assess agronomic suitability based on soil, climate, and real-time weather data, and a Long Short-Term Memory (LSTM) network to forecast market prices for agronomically viable crops. This integrated approach shifts the paradigm from \"what can grow?\" to \"what is most profitable to grow?\", providing a significant advantage in mitigating economic risk. The system is delivered through an end-to-end, voice-based interface in the local Kannada language, leveraging fine-tuned speech recognition and high-fidelity speech synthesis models to ensure accessibility for low-literacy users. Our results show that the Random Forest model achieves 98.5% accuracy in suitability prediction, while the LSTM model forecasts harvest-time prices with a low margin of error. By providing data-driven, economically optimized recommendations through an inclusive interface, this work offers a scalable and impactful solution to enhance the financial resilience of marginalized farming communities.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 4,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08833",
        "abs_url": "https://arxiv.org/abs/2507.08833",
        "pdf_url": "https://arxiv.org/pdf/2507.08833",
        "title": "LoRA Is Slower Than You Think",
        "authors": [
            "Seokmin Ko"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Low-Rank Adaptation (LoRA) is one of the most widely used techniques for fine-tuning large language models (LLMs). By introducing a small number of trainable low-rank weight matrices, LoRA substantially reduces the number of parameters that need to be updated, offering significant advantages in memory consumption and computational efficiency compared to full fine-tuning. However, we observed that LoRA does not consistently provide speed improvements across all model architectures and training setups. Motivated by this inconsistency, we conduct a comprehensive analysis of LoRA's performance and investigate the underlying factors limiting its speedup. Based on our findings, we propose several methods for more efficient fine-tuning of LLMs. We empirically evaluate these methods and compare them to LoRA, demonstrating that our approach achieves comparable or superior performance while delivering more consistent training speed improvements. Our work offers valuable insights and practical guidelines for practitioners seeking to optimize LLM fine-tuning under resource constraints.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 5,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08834",
        "abs_url": "https://arxiv.org/abs/2507.08834",
        "pdf_url": "https://arxiv.org/pdf/2507.08834",
        "title": "Physical Informed Neural Networks for modeling ocean pollutant",
        "authors": [
            "Karishma Battina",
            "Prathamesh Dinesh Joshi",
            "Raj Abhijit Dandekar",
            "Rajat Dandekar",
            "Sreedath Panat"
        ],
        "comments": "13 pages, 9 figures, 3 tables",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Traditional numerical methods often struggle with the complexity and scale of modeling pollutant transport across vast and dynamic oceanic domains. This paper introduces a Physics-Informed Neural Network (PINN) framework to simulate the dispersion of pollutants governed by the 2D advection-diffusion equation. The model achieves physically consistent predictions by embedding physical laws and fitting to noisy synthetic data, generated via a finite difference method (FDM), directly into the neural network training process. This approach addresses challenges such as non-linear dynamics and the enforcement of boundary and initial conditions. Synthetic data sets, augmented with varying noise levels, are used to capture real-world variability. The training incorporates a hybrid loss function including PDE residuals, boundary/initial condition conformity, and a weighted data fit term. The approach takes advantage of the Julia language scientific computing ecosystem for high-performance simulations, offering a scalable and flexible alternative to traditional solvers",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 6,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08835",
        "abs_url": "https://arxiv.org/abs/2507.08835",
        "pdf_url": "https://arxiv.org/pdf/2507.08835",
        "title": "Representation learning with a transformer by contrastive learning for money laundering detection",
        "authors": [
            "Harold Guéneau",
            "Alain Celisse",
            "Pascal Delange"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Statistics Theory (math.ST); Risk Management (q-fin.RM); Statistical Finance (q-fin.ST)",
        "abstract": "The present work tackles the money laundering detection problem. A new procedure is introduced which exploits structured time series of both qualitative and quantitative data by means of a transformer neural network. The first step of this procedure aims at learning representations of time series through contrastive learning (without any labels). The second step leverages these representations to generate a money laundering scoring of all observations. A two-thresholds approach is then introduced, which ensures a controlled false-positive rate by means of the Benjamini-Hochberg (BH) procedure. Experiments confirm that the transformer is able to produce general representations that succeed in exploiting money laundering patterns with minimal supervision from domain experts. It also illustrates the higher ability of the new procedure for detecting nonfraudsters as well as fraudsters, while keeping the false positive rate under control. This greatly contrasts with rule-based procedures or the ones based on LSTM architectures.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 7,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08836",
        "abs_url": "https://arxiv.org/abs/2507.08836",
        "pdf_url": "https://arxiv.org/pdf/2507.08836",
        "title": "Accuracy and Consumption analysis from a compressed model by CompactifAI from Multiverse Computing",
        "authors": [
            "Damien Fovet",
            "Shashank Chamoli",
            "Sarah Oury",
            "Srishti Singhal"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Performance (cs.PF)",
        "abstract": "This study evaluates the performance of a compression method, called CompactifAI, developed by Multiverse Computing, applied to the large language model Llama 3.1 8B\\cite{llama}. The evaluation focused on model efficiency (in terms of energy consumption) and accuracy using respectively the frameworks Codecarbon\\cite{codecarbon} and Ragas\\cite{ragas}. A comparison was performed between the model compressed with CompactifAI\\cite{compactifai}\\cite{compactifai2} and its full-size version. Our findings reveal that the compressed model using CompactifAI not only significantly reduced the computational resources but also maintained the model accuracy, making the model more efficient, scalable and cost-effective.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 8,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08838",
        "abs_url": "https://arxiv.org/abs/2507.08838",
        "pdf_url": "https://arxiv.org/pdf/2507.08838",
        "title": "wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models",
        "authors": [
            "Xiaohang Tang",
            "Rares Dolga",
            "Sangwoong Yoon",
            "Ilija Bogunovic"
        ],
        "comments": "Preprint",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "Improving the reasoning capabilities of diffusion-based large language models (dLLMs) through reinforcement learning (RL) remains an open problem. The intractability of dLLMs likelihood function necessitates approximating the current, old, and reference policy likelihoods at each policy optimization step. This reliance introduces additional computational overhead and lead to potentially large bias -- particularly when approximation errors occur in the denominator of policy ratios used for importance sampling. To mitigate these issues, we introduce $\\mathtt{wd1}$, a novel policy optimization approach that reformulates the objective as a weighted likelihood, requiring only a single approximation for the current parametrized policy likelihood. Experiments on widely used reasoning benchmarks demonstrate that $\\mathtt{wd1}$, without supervised fine-tuning (SFT) or any supervised data, outperforms existing RL methods for dLLMs, achieving up to 16% higher accuracy. $\\mathtt{wd1}$ delivers additional computational gains, including reduced training time and fewer function evaluations (NFEs) per gradient step. These findings, combined with the simplicity of method's implementation and R1-Zero-like training (no SFT), position $\\mathtt{wd1}$ as a more effective and efficient method for applying RL to dLLMs reasoning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 9,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08839",
        "abs_url": "https://arxiv.org/abs/2507.08839",
        "pdf_url": "https://arxiv.org/pdf/2507.08839",
        "title": "Domain-Adaptive Diagnosis of Lewy Body Disease with Transferability Aware Transformer",
        "authors": [
            "Xiaowei Yu",
            "Jing Zhang",
            "Tong Chen",
            "Yan Zhuang",
            "Minheng Chen",
            "Chao Cao",
            "Yanjun Lyu",
            "Lu Zhang",
            "Li Su",
            "Tianming Liu",
            "Dajiang Zhu"
        ],
        "comments": "MICCAI 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Image and Video Processing (eess.IV)",
        "abstract": "Lewy Body Disease (LBD) is a common yet understudied form of dementia that imposes a significant burden on public health. It shares clinical similarities with Alzheimer's disease (AD), as both progress through stages of normal cognition, mild cognitive impairment, and dementia. A major obstacle in LBD diagnosis is data scarcity, which limits the effectiveness of deep learning. In contrast, AD datasets are more abundant, offering potential for knowledge transfer. However, LBD and AD data are typically collected from different sites using different machines and protocols, resulting in a distinct domain shift. To effectively leverage AD data while mitigating domain shift, we propose a Transferability Aware Transformer (TAT) that adapts knowledge from AD to enhance LBD diagnosis. Our method utilizes structural connectivity (SC) derived from structural MRI as training data. Built on the attention mechanism, TAT adaptively assigns greater weights to disease-transferable features while suppressing domain-specific ones, thereby reducing domain shift and improving diagnostic accuracy with limited LBD data. The experimental results demonstrate the effectiveness of TAT. To the best of our knowledge, this is the first study to explore domain adaptation from AD to LBD under conditions of data scarcity and domain shift, providing a promising framework for domain-adaptive diagnosis of rare diseases.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 10,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08841",
        "abs_url": "https://arxiv.org/abs/2507.08841",
        "pdf_url": "https://arxiv.org/pdf/2507.08841",
        "title": "Zero-Shot Neural Architecture Search with Weighted Response Correlation",
        "authors": [
            "Kun Jing",
            "Luoyu Chen",
            "Jungang Xu",
            "Jianwei Tai",
            "Yiyu Wang",
            "Shuaimin Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Neural architecture search (NAS) is a promising approach for automatically designing neural network architectures. However, the architecture estimation of NAS is computationally expensive and time-consuming because of training multiple architectures from scratch. Although existing zero-shot NAS methods use training-free proxies to accelerate the architecture estimation, their effectiveness, stability, and generality are still lacking. We present a novel training-free estimation proxy called weighted response correlation (WRCor). WRCor utilizes correlation coefficient matrices of responses across different input samples to calculate the proxy scores of estimated architectures, which can measure their expressivity and generalizability. Experimental results on proxy evaluation demonstrate that WRCor and its voting proxies are more efficient estimation strategies than existing proxies. We also apply them with different search strategies in architecture search. Experimental results on architecture search show that our zero-shot NAS algorithm outperforms most existing NAS algorithms in different search spaces. Our NAS algorithm can discover an architecture with a 22.1% test error on the ImageNet-1k dataset within 4 GPU hours. All codes are publicly available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 11,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08842",
        "abs_url": "https://arxiv.org/abs/2507.08842",
        "pdf_url": "https://arxiv.org/pdf/2507.08842",
        "title": "Gradients as an Action: Towards Communication-Efficient Federated Recommender Systems via Adaptive Action Sharing",
        "authors": [
            "Zhufeng Lu",
            "Chentao Jia",
            "Ming Hu",
            "Xiaofei Xie",
            "Mingsong Chen"
        ],
        "comments": "This paper has been accepted by ACM SIGKDD 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "As a promising privacy-aware collaborative model training paradigm, Federated Learning (FL) is becoming popular in the design of distributed recommender systems. However, Federated Recommender Systems (FedRecs) greatly suffer from two major problems: i) extremely high communication overhead due to massive item embeddings involved in recommendation systems, and ii) intolerably low training efficiency caused by the entanglement of both heterogeneous network environments and client devices. Although existing methods attempt to employ various compression techniques to reduce communication overhead, due to the parameter errors introduced by model compression, they inevitably suffer from model performance degradation. To simultaneously address the above problems, this paper presents a communication-efficient FedRec framework named FedRAS, which adopts an action-sharing strategy to cluster the gradients of item embedding into a specific number of model updating actions for communication rather than directly compressing the item embeddings. In this way, the cloud server can use the limited actions from clients to update all the items. Since gradient values are significantly smaller than item embeddings, constraining the directions of gradients (i.e., the action space) introduces smaller errors compared to compressing the entire item embedding matrix into a reduced space. To accommodate heterogeneous devices and network environments, FedRAS incorporates an adaptive clustering mechanism that dynamically adjusts the number of actions. Comprehensive experiments on well-known datasets demonstrate that FedRAS can reduce the size of communication payloads by up to 96.88%, while not sacrificing recommendation performance within various heterogeneous scenarios. We have open-sourced FedRAS at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 12,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08843",
        "abs_url": "https://arxiv.org/abs/2507.08843",
        "pdf_url": "https://arxiv.org/pdf/2507.08843",
        "title": "Can We Predict Your Next Move Without Breaking Your Privacy?",
        "authors": [
            "Arpita Soni",
            "Sahil Tripathi",
            "Gautam Siddharth Kashyap",
            "Manaswi Kulahara",
            "Mohammad Anas Azeez",
            "Zohaib Hasan Siddiqui",
            "Nipun Joshi",
            "Jiechao Gao"
        ],
        "comments": "Accepted in the 17th International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2025), scheduled for 25 - 28 August 2025 in Ontario, Canada",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "We propose FLLL3M--Federated Learning with Large Language Models for Mobility Modeling--a privacy-preserving framework for Next-Location Prediction (NxLP). By retaining user data locally and leveraging LLMs through an efficient outer product mechanism, FLLL3M ensures high accuracy with low resource demands. It achieves SOT results on Gowalla (Acc@1: 12.55, MRR: 0.1422), WeePlace (10.71, 0.1285), Brightkite (10.42, 0.1169), and FourSquare (8.71, 0.1023), while reducing parameters by up to 45.6% and memory usage by 52.7%.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 13,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08845",
        "abs_url": "https://arxiv.org/abs/2507.08845",
        "pdf_url": "https://arxiv.org/pdf/2507.08845",
        "title": "DAFOS: Dynamic Adaptive Fanout Optimization Sampler",
        "authors": [
            "Irfan Ullah",
            "Young-Koo Lee"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Graph Neural Networks (GNNs) are becoming an essential tool for learning from graph-structured data, however uniform neighbor sampling and static fanout settings frequently limit GNNs' scalability and efficiency. In this paper, we propose the Dynamic Adaptive Fanout Optimization Sampler (DAFOS), a novel approach that dynamically adjusts the fanout based on model performance and prioritizes important nodes during training. Our approach leverages node scoring based on node degree to focus computational resources on structurally important nodes, incrementing the fanout as the model training progresses. DAFOS also integrates an early stopping mechanism to halt training when performance gains diminish. Experiments conducted on three benchmark datasets, ogbnarxiv, Reddit, and ogbn-products, demonstrate that our approach significantly improves training speed and accuracy compared to a state-of-the-art approach. DAFOS achieves a 3.57x speedup on the ogbn-arxiv dataset and a 12.6x speedup on the Reddit dataset while improving the F1 score from 68.5% to 71.21% on ogbn-arxiv and from 73.78% to 76.88% on the ogbn-products dataset, respectively. These results highlight the potential of DAFOS as an efficient and scalable solution for large-scale GNN training.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 14,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08848",
        "abs_url": "https://arxiv.org/abs/2507.08848",
        "pdf_url": "https://arxiv.org/pdf/2507.08848",
        "title": "Assuring the Safety of Reinforcement Learning Components: AMLAS-RL",
        "authors": [
            "Calum Corrie Imrie",
            "Ioannis Stefanakos",
            "Sepeedeh Shahbeigi",
            "Richard Hawkins",
            "Simon Burton"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Robotics (cs.RO); Software Engineering (cs.SE)",
        "abstract": "The rapid advancement of machine learning (ML) has led to its increasing integration into cyber-physical systems (CPS) across diverse domains. While CPS offer powerful capabilities, incorporating ML components introduces significant safety and assurance challenges. Among ML techniques, reinforcement learning (RL) is particularly suited for CPS due to its capacity to handle complex, dynamic environments where explicit models of interaction between system and environment are unavailable or difficult to construct. However, in safety-critical applications, this learning process must not only be effective but demonstrably safe. Safe-RL methods aim to address this by incorporating safety constraints during learning, yet they fall short in providing systematic assurance across the RL lifecycle. The AMLAS methodology offers structured guidance for assuring the safety of supervised learning components, but it does not directly apply to the unique challenges posed by RL. In this paper, we adapt AMLAS to provide a framework for generating assurance arguments for an RL-enabled system through an iterative process; AMLAS-RL. We demonstrate AMLAS-RL using a running example of a wheeled vehicle tasked with reaching a target goal without collision.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 15,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08858",
        "abs_url": "https://arxiv.org/abs/2507.08858",
        "pdf_url": "https://arxiv.org/pdf/2507.08858",
        "title": "Foundation models for time series forecasting: Application in conformal prediction",
        "authors": [
            "Sami Achour",
            "Yassine Bouher",
            "Duong Nguyen",
            "Nicolas Chesneau"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "The zero-shot capabilities of foundation models (FMs) for time series forecasting offer promising potentials in conformal prediction, as most of the available data can be allocated to calibration. This study compares the performance of Time Series Foundation Models (TSFMs) with traditional methods, including statistical models and gradient boosting, within a conformal prediction setting. Our findings highlight two key advantages of TSFMs. First, when the volume of data is limited, TSFMs provide more reliable conformalized prediction intervals than classic models, thanks to their superior predictive accuracy. Second, the calibration process is more stable because more data are used for calibration. Morever, the fewer data available, the more pronounced these benefits become, as classic models require a substantial amount of data for effective training. These results underscore the potential of foundation models in improving conformal prediction reliability in time series applications, particularly in data-constrained cases. All the code to reproduce the experiments is available.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 16,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08860",
        "abs_url": "https://arxiv.org/abs/2507.08860",
        "pdf_url": "https://arxiv.org/pdf/2507.08860",
        "title": "e-Profits: A Business-Aligned Evaluation Metric for Profit-Sensitive Customer Churn Prediction",
        "authors": [
            "Awais Manzoor",
            "M. Atif Qureshi",
            "Etain Kidney",
            "Luca Longo"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Retention campaigns in customer relationship management often rely on churn prediction models evaluated using traditional metrics such as AUC and F1-score. However, these metrics fail to reflect financial outcomes and may mislead strategic decisions. We introduce e-Profits, a novel business-aligned evaluation metric that quantifies model performance based on customer-specific value, retention probability, and intervention costs. Unlike existing profit-based metrics such as Expected Maximum Profit, which assume fixed population-level parameters, e-Profits uses Kaplan-Meier survival analysis to estimate personalised retention rates and supports granular, per customer evaluation. We benchmark six classifiers across two telecom datasets (IBM Telco and Maven Telecom) and demonstrate that e-Profits reshapes model rankings compared to traditional metrics, revealing financial advantages in models previously overlooked by AUC or F1-score. The metric also enables segment-level insight into which models maximise return on investment for high-value customers. e-Profits is designed as an understandable, post hoc tool to support model evaluation in business contexts, particularly for marketing and analytics teams prioritising profit-driven decisions. All source code is available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 17,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08861",
        "abs_url": "https://arxiv.org/abs/2507.08861",
        "pdf_url": "https://arxiv.org/pdf/2507.08861",
        "title": "On the under-reaching phenomenon in message-passing neural PDE solvers: revisiting the CFL condition",
        "authors": [
            "Lucas Tesan",
            "Mikel M. Iparraguirre",
            "David Gonzalez",
            "Pedro Martins",
            "Elias Cueto"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "This paper proposes sharp lower bounds for the number of message passing iterations required in graph neural networks (GNNs) when solving partial differential equations (PDE). This significantly reduces the need for exhaustive hyperparameter tuning. Bounds are derived for the three fundamental classes of PDEs (hyperbolic, parabolic and elliptic) by relating the physical characteristics of the problem in question to the message-passing requirement of GNNs. In particular, we investigate the relationship between the physical constants of the equations governing the problem, the spatial and temporal discretisation and the message passing mechanisms in GNNs. When the number of message passing iterations is below these proposed limits, information does not propagate efficiently through the network, resulting in poor solutions, even for deep GNN architectures. In contrast, when the suggested lower bound is satisfied, the GNN parameterisation allows the model to accurately capture the underlying phenomenology, resulting in solvers of adequate accuracy. Examples are provided for four different examples of equations that show the sharpness of the proposed lower bounds.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 18,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08866",
        "abs_url": "https://arxiv.org/abs/2507.08866",
        "pdf_url": "https://arxiv.org/pdf/2507.08866",
        "title": "Underrepresentation, Label Bias, and Proxies: Towards Data Bias Profiles for the EU AI Act and Beyond",
        "authors": [
            "Marina Ceccon",
            "Giandomenico Cornacchia",
            "Davide Dalle Pezze",
            "Alessandro Fabris",
            "Gian Antonio Susto"
        ],
        "comments": "Accepted in Expert Systems with Applications",
        "subjects": "Machine Learning (cs.LG); Computers and Society (cs.CY); Machine Learning (stat.ML)",
        "abstract": "Undesirable biases encoded in the data are key drivers of algorithmic discrimination. Their importance is widely recognized in the algorithmic fairness literature, as well as legislation and standards on anti-discrimination in AI. Despite this recognition, data biases remain understudied, hindering the development of computational best practices for their detection and mitigation. In this work, we present three common data biases and study their individual and joint effect on algorithmic discrimination across a variety of datasets, models, and fairness measures. We find that underrepresentation of vulnerable populations in training sets is less conducive to discrimination than conventionally affirmed, while combinations of proxies and label bias can be far more critical. Consequently, we develop dedicated mechanisms to detect specific types of bias, and combine them into a preliminary construct we refer to as the Data Bias Profile (DBP). This initial formulation serves as a proof of concept for how different bias signals can be systematically documented. Through a case study with popular fairness datasets, we demonstrate the effectiveness of the DBP in predicting the risk of discriminatory outcomes and the utility of fairness-enhancing interventions. Overall, this article bridges algorithmic fairness research and anti-discrimination policy through a data-centric lens.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 19,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08870",
        "abs_url": "https://arxiv.org/abs/2507.08870",
        "pdf_url": "https://arxiv.org/pdf/2507.08870",
        "title": "GUIDE: Towards Scalable Advising for Research Ideas",
        "authors": [
            "Yaowenqi Liu",
            "BingXu Meng",
            "Rui Pan",
            "Jerry Huang",
            "Tong Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Multiagent Systems (cs.MA)",
        "abstract": "The field of AI research is advancing at an unprecedented pace, enabling automated hypothesis generation and experimental design across diverse domains such as biology, mathematics, and artificial intelligence. Despite these advancements, there remains a significant gap in the availability of scalable advising systems capable of providing high-quality, well-reasoned feedback to refine proposed hypotheses and experimental designs. To address this challenge, we explore key factors that underlie the development of robust advising systems, including model size, context length, confidence estimation, and structured reasoning processes. Our findings reveal that a relatively small model, when equipped with a well-compressed literature database and a structured reasoning framework, can outperform powerful general-purpose language models such as Deepseek-R1 in terms of acceptance rates for self-ranked top-30% submissions to ICLR 2025. Moreover, when limited to high-confidence predictions, our system achieves an acceptance rate exceeding 90% on the ICLR 2025 test set, underscoring its potential to significantly enhance the quality and efficiency of hypothesis generation and experimental design. The code is released at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 20,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08871",
        "abs_url": "https://arxiv.org/abs/2507.08871",
        "pdf_url": "https://arxiv.org/pdf/2507.08871",
        "title": "Next-Generation Travel Demand Modeling with a Generative Framework for Household Activity Coordination",
        "authors": [
            "Xishun Liao",
            "Haoxuan Ma",
            "Yifan Liu",
            "Yuxiang Wei",
            "Brian Yueshuai He",
            "Chris Stanford",
            "Jiaqi Ma"
        ],
        "comments": "8 pages, 7 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Travel demand models are critical tools for planning, policy, and mobility system design. Traditional activity-based models (ABMs), although grounded in behavioral theories, often rely on simplified rules and assumptions, and are costly to develop and difficult to adapt across different regions. This paper presents a learning-based travel demand modeling framework that synthesizes household-coordinated daily activity patterns based on a household's socio-demographic profiles. The whole framework integrates population synthesis, coordinated activity generation, location assignment, and large-scale microscopic traffic simulation into a unified system. It is fully generative, data-driven, scalable, and transferable to other regions. A full-pipeline implementation is conducted in Los Angeles with a 10 million population. Comprehensive validation shows that the model closely replicates real-world mobility patterns and matches the performance of legacy ABMs with significantly reduced modeling cost and greater scalability. With respect to the SCAG ABM benchmark, the origin-destination matrix achieves a cosine similarity of 0.97, and the daily vehicle miles traveled (VMT) in the network yields a 0.006 Jensen-Shannon Divergence (JSD) and a 9.8% mean absolute percentage error (MAPE). When compared to real-world observations from Caltrans PeMS, the evaluation on corridor-level traffic speed and volume reaches a 0.001 JSD and a 6.11% MAPE.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 21,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08873",
        "abs_url": "https://arxiv.org/abs/2507.08873",
        "pdf_url": "https://arxiv.org/pdf/2507.08873",
        "title": "Contrastive Language-Image Pre-Training Model based Semantic Communication Performance Optimization",
        "authors": [
            "Shaoran Yang",
            "Dongyu Wei",
            "Hanzhi Yu",
            "Zhaohui Yang",
            "Yuchen Liu",
            "Mingzhe Chen"
        ],
        "comments": "Submitted to IEEE GLOBECOM 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "In this paper, a novel contrastive language-image pre-training (CLIP) model based semantic communication framework is designed. Compared to standard neural network (e.g.,convolutional neural network) based semantic encoders and decoders that require joint training over a common dataset, our CLIP model based method does not require any training procedures thus enabling a transmitter to extract data meanings of the original data without neural network model training, and the receiver to train a neural network for follow-up task implementation without the communications with the transmitter. Next, we investigate the deployment of the CLIP model based semantic framework over a noisy wireless network. Since the semantic information generated by the CLIP model is susceptible to wireless noise and the spectrum used for semantic information transmission is limited, it is necessary to jointly optimize CLIP model architecture and spectrum resource block (RB) allocation to maximize semantic communication performance while considering wireless noise, the delay and energy used for semantic communication. To achieve this goal, we use a proximal policy optimization (PPO) based reinforcement learning (RL) algorithm to learn how wireless noise affect the semantic communication performance thus finding optimal CLIP model and RB for each user. Simulation results show that our proposed method improves the convergence rate by up to 40%, and the accumulated reward by 4x compared to soft actor-critic.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 22,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08874",
        "abs_url": "https://arxiv.org/abs/2507.08874",
        "pdf_url": "https://arxiv.org/pdf/2507.08874",
        "title": "An Automated Classifier of Harmful Brain Activities for Clinical Usage Based on a Vision-Inspired Pre-trained Framework",
        "authors": [
            "Yulin Sun",
            "Xiaopeng Si",
            "Runnan He",
            "Xiao Hu",
            "Peter Smielewski",
            "Wenlong Wang",
            "Xiaoguang Tong",
            "Wei Yue",
            "Meijun Pang",
            "Kuo Zhang",
            "Xizi Song",
            "Dong Ming",
            "Xiuyun Liu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Timely identification of harmful brain activities via electroencephalography (EEG) is critical for brain disease diagnosis and treatment, which remains limited application due to inter-rater variability, resource constraints, and poor generalizability of existing artificial intelligence (AI) models. In this study, a convolutional neural network model, VIPEEGNet, was developed and validated using EEGs recorded from Massachusetts General Hospital/Harvard Medical School. The VIPEEGNet was developed and validated using two independent datasets, collected between 2006 and 2020. The development cohort included EEG recordings from 1950 patients, with 106,800 EEG segments annotated by at least one experts (ranging from 1 to 28). The online testing cohort consisted of EEG segments from a subset of an additional 1,532 patients, each annotated by at least 10 experts. For the development cohort (n=1950), the VIPEEGNet achieved high accuracy, with an AUROC for binary classification of seizure, LPD, GPD, LRDA, GRDA, and \"other\" categories at 0.972 (95% CI, 0.957-0.988), 0.962 (95% CI, 0.954-0.970), 0.972 (95% CI, 0.960-0.984), 0.938 (95% CI, 0.917-0.959), 0.949 (95% CI, 0.941-0.957), and 0.930 (95% CI, 0.926-0.935). For multi classification, the sensitivity of VIPEEGNET for the six categories ranges from 36.8% to 88.2% and the precision ranges from 55.6% to 80.4%, and performance similar to human experts. Notably, the external validation showed Kullback-Leibler Divergence (KLD)of 0.223 and 0.273, ranking top 2 among the existing 2,767 competing algorithms, while we only used 2.8% of the parameters of the first-ranked algorithm.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 23,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08877",
        "abs_url": "https://arxiv.org/abs/2507.08877",
        "pdf_url": "https://arxiv.org/pdf/2507.08877",
        "title": "ODIA: Oriented Distillation for Inline Acceleration of LLM-based Function Calling",
        "authors": [
            "Hanlong Zhang",
            "Jingsheng Yang",
            "Hao Li",
            "Yuhao He",
            "Franck Gong"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Function Calling is a crucial technique that enables Large Language Models (LLMs) to interact with external systems through APIs. However, the high latency associated with LLM-based Function Calling significantly impacts user experience. This paper presents a novel approach called Oriented Distillation for Inline Acceleration (ODIA) that leverages online user interaction data to accelerate Function Calling. By automatically identifying \"simple queries\" from production traffic and distilling knowledge from larger models to smaller ones, our method reduces response latency by 45% (expected) and 78% (median) while maintaining accuracy. We demonstrate the effectiveness of our approach through real-world deployment in a music application, where the smaller model successfully handles 60% of traffic with negligible accuracy loss. Our method requires minimal human intervention and continuously improves through automated data collection and model updating, making it a practical solution for production environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 24,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08905",
        "abs_url": "https://arxiv.org/abs/2507.08905",
        "pdf_url": "https://arxiv.org/pdf/2507.08905",
        "title": "Last Layer Hamiltonian Monte Carlo",
        "authors": [
            "Koen Vellenga",
            "H. Joe Steinhauer",
            "Göran Falkman",
            "Jonas Andersson",
            "Anders Sjögren"
        ],
        "comments": "25 pages, 15 figures, 6 tables, currently under submission",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "We explore the use of Hamiltonian Monte Carlo (HMC) sampling as a probabilistic last layer approach for deep neural networks (DNNs). While HMC is widely regarded as a gold standard for uncertainty estimation, the computational demands limit its application to large-scale datasets and large DNN architectures. Although the predictions from the sampled DNN parameters can be parallelized, the computational cost still scales linearly with the number of samples (similar to an ensemble). Last layer HMC (LL--HMC) reduces the required computations by restricting the HMC sampling to the final layer of a DNN, making it applicable to more data-intensive scenarios with limited computational resources. In this paper, we compare LL-HMC against five last layer probabilistic deep learning (LL-PDL) methods across three real-world video datasets for driver action and intention. We evaluate the in-distribution classification performance, calibration, and out-of-distribution (OOD) detection. Due to the stochastic nature of the probabilistic evaluations, we performed five grid searches for different random seeds to avoid being reliant on a single initialization for the hyperparameter configurations. The results show that LL--HMC achieves competitive in-distribution classification and OOD detection performance. Additional sampled last layer parameters do not improve the classification performance, but can improve the OOD detection. Multiple chains or starting positions did not yield consistent improvements.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 25,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08912",
        "abs_url": "https://arxiv.org/abs/2507.08912",
        "pdf_url": "https://arxiv.org/pdf/2507.08912",
        "title": "Fair-FLIP: Fair Deepfake Detection with Fairness-Oriented Final Layer Input Prioritising",
        "authors": [
            "Tomasz Szandala",
            "Fatima Ezzeddine",
            "Natalia Rusin",
            "Silvia Giordano",
            "Omran Ayoub"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Artificial Intelligence-generated content has become increasingly popular, yet its malicious use, particularly the deepfakes, poses a serious threat to public trust and discourse. While deepfake detection methods achieve high predictive performance, they often exhibit biases across demographic attributes such as ethnicity and gender. In this work, we tackle the challenge of fair deepfake detection, aiming to mitigate these biases while maintaining robust detection capabilities. To this end, we propose a novel post-processing approach, referred to as Fairness-Oriented Final Layer Input Prioritising (Fair-FLIP), that reweights a trained model's final-layer inputs to reduce subgroup disparities, prioritising those with low variability while demoting highly variable ones. Experimental results comparing Fair-FLIP to both the baseline (without fairness-oriented de-biasing) and state-of-the-art approaches show that Fair-FLIP can enhance fairness metrics by up to 30% while maintaining baseline accuracy, with only a negligible reduction of 0.25%. Code is available on Github: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 26,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08913",
        "abs_url": "https://arxiv.org/abs/2507.08913",
        "pdf_url": "https://arxiv.org/pdf/2507.08913",
        "title": "Revisiting Convergence: Shuffling Complexity Beyond Lipschitz Smoothness",
        "authors": [
            "Qi He",
            "Peiran Yu",
            "Ziyi Chen",
            "Heng Huang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Optimization and Control (math.OC); Machine Learning (stat.ML)",
        "abstract": "Shuffling-type gradient methods are favored in practice for their simplicity and rapid empirical performance. Despite extensive development of convergence guarantees under various assumptions in recent years, most require the Lipschitz smoothness condition, which is often not met in common machine learning models. We highlight this issue with specific counterexamples. To address this gap, we revisit the convergence rates of shuffling-type gradient methods without assuming Lipschitz smoothness. Using our stepsize strategy, the shuffling-type gradient algorithm not only converges under weaker assumptions but also match the current best-known convergence rates, thereby broadening its applicability. We prove the convergence rates for nonconvex, strongly convex, and non-strongly convex cases, each under both random reshuffling and arbitrary shuffling schemes, under a general bounded variance condition. Numerical experiments further validate the performance of our shuffling-type gradient algorithm, underscoring its practical efficacy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 27,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08956",
        "abs_url": "https://arxiv.org/abs/2507.08956",
        "pdf_url": "https://arxiv.org/pdf/2507.08956",
        "title": "Beyond Scores: Proximal Diffusion Models",
        "authors": [
            "Zhenghan Fang",
            "Mateo Díaz",
            "Sam Buchanan",
            "Jeremias Sulam"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Statistics Theory (math.ST); Machine Learning (stat.ML)",
        "abstract": "Diffusion models have quickly become some of the most popular and powerful generative models for high-dimensional data. The key insight that enabled their development was the realization that access to the score -- the gradient of the log-density at different noise levels -- allows for sampling from data distributions by solving a reverse-time stochastic differential equation (SDE) via forward discretization, and that popular denoisers allow for unbiased estimators of this score. In this paper, we demonstrate that an alternative, backward discretization of these SDEs, using proximal maps in place of the score, leads to theoretical and practical benefits. We leverage recent results in proximal matching to learn proximal operators of the log-density and, with them, develop Proximal Diffusion Models (ProxDM). Theoretically, we prove that $\\widetilde{O}(d/\\sqrt{\\varepsilon})$ steps suffice for the resulting discretization to generate an $\\varepsilon$-accurate distribution w.r.t. the KL divergence. Empirically, we show that two variants of ProxDM achieve significantly faster convergence within just a few sampling steps compared to conventional score-matching methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 28,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08959",
        "abs_url": "https://arxiv.org/abs/2507.08959",
        "pdf_url": "https://arxiv.org/pdf/2507.08959",
        "title": "Graph Neural Network Enhanced Sequential Recommendation Method for Cross-Platform Ad Campaign",
        "authors": [
            "Xiang Li",
            "Xinyu Wang",
            "Yifan Lin"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In order to improve the accuracy of cross-platform advertisement recommendation, a graph neural network (GNN)- based advertisement recommendation method is analyzed. Through multi-dimensional modeling, user behavior data (e.g., click frequency, active duration) reveal temporal patterns of interest evolution, ad content (e.g., type, tag, duration) influences semantic preferences, and platform features (e.g., device type, usage context) shape the environment where interest transitions occur. These factors jointly enable the GNN to capture the latent pathways of user interest migration across platforms. The experimental results are based on the datasets of three platforms, and Platform B reaches 0.937 in AUC value, which is the best performance. Platform A and Platform C showed a slight decrease in precision and recall with uneven distribution of ad labels. By adjusting the hyperparameters such as learning rate, batch size and embedding dimension, the adaptability and robustness of the model in heterogeneous data are further improved.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 29,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08965",
        "abs_url": "https://arxiv.org/abs/2507.08965",
        "pdf_url": "https://arxiv.org/pdf/2507.08965",
        "title": "Theory-Informed Improvements to Classifier-Free Guidance for Discrete Diffusion Models",
        "authors": [
            "Kevin Rojas",
            "Ye He",
            "Chieh-Hsin Lai",
            "Yuta Takida",
            "Yuki Mitsufuji",
            "Molei Tao"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "Classifier-Free Guidance (CFG) is a widely used technique for conditional generation and improving sample quality in continuous diffusion models, and recent works have extended it to discrete diffusion. This paper theoretically analyzes CFG in the context of masked discrete diffusion, focusing on the role of guidance schedules. Our analysis shows that high guidance early in sampling (when inputs are heavily masked) harms generation quality, while late-stage guidance has a larger effect. These findings provide a theoretical explanation for empirical observations in recent studies on guidance schedules. The analysis also reveals an imperfection of the current CFG implementations. These implementations can unintentionally cause imbalanced transitions, such as unmasking too rapidly during the early stages of generation, which degrades the quality of the resulting samples. To address this, we draw insight from the analysis and propose a novel classifier-free guidance mechanism empirically applicable to any discrete diffusion. Intuitively, our method smoothens the transport between the data distribution and the initial (masked/uniform) distribution, which results in improved sample quality. Remarkably, our method is achievable via a simple one-line code change. The efficacy of our method is empirically demonstrated with experiments on ImageNet (masked discrete diffusion) and QM9 (uniform discrete diffusion).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 30,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08966",
        "abs_url": "https://arxiv.org/abs/2507.08966",
        "pdf_url": "https://arxiv.org/pdf/2507.08966",
        "title": "ToxBench: A Binding Affinity Prediction Benchmark with AB-FEP-Calculated Labels for Human Estrogen Receptor Alpha",
        "authors": [
            "Meng Liu",
            "Karl Leswing",
            "Simon K. S. Chu",
            "Farhad Ramezanghorbani",
            "Griffin Young",
            "Gabriel Marques",
            "Prerna Das",
            "Anjali Panikar",
            "Esther Jamir",
            "Mohammed Sulaiman Shamsudeen",
            "K. Shawn Watts",
            "Ananya Sen",
            "Hari Priya Devannagari",
            "Edward B. Miller",
            "Muyun Lihan",
            "Howook Hwang",
            "Janet Paulsen",
            "Xin Yu",
            "Kyle Gion",
            "Timur Rvachov",
            "Emine Kucukbenli",
            "Saee Gopal Paliwal"
        ],
        "comments": "Workshop on Generative AI for Biology at ICML 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Chemical Physics (physics.chem-ph); Biomolecules (q-bio.BM)",
        "abstract": "Protein-ligand binding affinity prediction is essential for drug discovery and toxicity assessment. While machine learning (ML) promises fast and accurate predictions, its progress is constrained by the availability of reliable data. In contrast, physics-based methods such as absolute binding free energy perturbation (AB-FEP) deliver high accuracy but are computationally prohibitive for high-throughput applications. To bridge this gap, we introduce ToxBench, the first large-scale AB-FEP dataset designed for ML development and focused on a single pharmaceutically critical target, Human Estrogen Receptor Alpha (ER$\\alpha$). ToxBench contains 8,770 ER$\\alpha$-ligand complex structures with binding free energies computed via AB-FEP with a subset validated against experimental affinities at 1.75 kcal/mol RMSE, along with non-overlapping ligand splits to assess model generalizability. Using ToxBench, we further benchmark state-of-the-art ML methods, and notably, our proposed DualBind model, which employs a dual-loss framework to effectively learn the binding energy function. The benchmark results demonstrate the superior performance of DualBind and the potential of ML to approximate AB-FEP at a fraction of the computational cost.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 31,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08972",
        "abs_url": "https://arxiv.org/abs/2507.08972",
        "pdf_url": "https://arxiv.org/pdf/2507.08972",
        "title": "Simulating Three-dimensional Turbulence with Physics-informed Neural Networks",
        "authors": [
            "Sifan Wang",
            "Shyam Sankaran",
            "Panos Stinis",
            "Paris Perdikaris"
        ],
        "comments": "25 pages, 13 figures, 3 tables",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computational Physics (physics.comp-ph); Fluid Dynamics (physics.flu-dyn)",
        "abstract": "Turbulent fluid flows are among the most computationally demanding problems in science, requiring enormous computational resources that become prohibitive at high flow speeds. Physics-informed neural networks (PINNs) represent a radically different approach that trains neural networks directly from physical equations rather than data, offering the potential for continuous, mesh-free solutions. Here we show that appropriately designed PINNs can successfully simulate fully turbulent flows in both two and three dimensions, directly learning solutions to the fundamental fluid equations without traditional computational grids or training data. Our approach combines several algorithmic innovations including adaptive network architectures, causal training, and advanced optimization methods to overcome the inherent challenges of learning chaotic dynamics. Through rigorous validation on challenging turbulence problems, we demonstrate that PINNs accurately reproduce key flow statistics including energy spectra, kinetic energy, enstrophy, and Reynolds stresses. Our results demonstrate that neural equation solvers can handle complex chaotic systems, opening new possibilities for continuous turbulence modeling that transcends traditional computational limitations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 32,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08977",
        "abs_url": "https://arxiv.org/abs/2507.08977",
        "pdf_url": "https://arxiv.org/pdf/2507.08977",
        "title": "Simulation as Supervision: Mechanistic Pretraining for Scientific Discovery",
        "authors": [
            "Carson Dudley",
            "Reiden Magdaleno",
            "Christopher Harding",
            "Marisa Eisenberg"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "Scientific modeling faces a core limitation: mechanistic models offer interpretability but collapse under real-world complexity, while machine learning models are flexible but require large labeled datasets, cannot infer unobservable quantities, and operate as black boxes. We introduce Simulation-Grounded Neural Networks (SGNNs), a general framework that uses mechanistic simulations as training data for neural networks. SGNNs are pretrained on synthetic corpora spanning diverse model structures, parameter regimes, stochasticity, and observational artifacts. We evaluated SGNNs across scientific disciplines and modeling tasks, and found that SGNNs achieved state-of-the-art results across settings: for prediction tasks, they nearly tripled COVID-19 forecasting skill versus CDC baselines, reduced chemical yield prediction error by one third, and maintained accuracy in ecological forecasting where task specific models failed. For inference tasks, SGNNs also accurately classified the source of information spread in simulated social networks and enabled supervised learning for unobservable targets, such as estimating COVID-19 transmissibility more accurately than traditional methods even in early outbreaks. Finally, SGNNs enable back-to-simulation attribution, a new form of mechanistic interpretability. Given real world input, SGNNs retrieve simulations based on what the model has learned to see as most similar, revealing which underlying dynamics the model believes are active. This provides process-level insight -- what the model thinks is happening -- not just which features mattered. SGNNs unify scientific theory with deep learning flexibility and unlock a new modeling paradigm -- transforming simulations from rigid, post hoc tools into flexible sources of supervision, enabling robust, interpretable inference even when ground truth is missing.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 33,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08980",
        "abs_url": "https://arxiv.org/abs/2507.08980",
        "pdf_url": "https://arxiv.org/pdf/2507.08980",
        "title": "Learning Diffusion Models with Flexible Representation Guidance",
        "authors": [
            "Chenyu Wang",
            "Cai Zhou",
            "Sharut Gupta",
            "Zongyu Lin",
            "Stefanie Jegelka",
            "Stephen Bates",
            "Tommi Jaakkola"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Diffusion models can be improved with additional guidance towards more effective representations of input. Indeed, prior empirical work has already shown that aligning internal representations of the diffusion model with those of pre-trained models improves generation quality. In this paper, we present a systematic framework for incorporating representation guidance into diffusion models. We provide alternative decompositions of denoising models along with their associated training criteria, where the decompositions determine when and how the auxiliary representations are incorporated. Guided by our theoretical insights, we introduce two new strategies for enhancing representation alignment in diffusion models. First, we pair examples with target representations either derived from themselves or arisen from different synthetic modalities, and subsequently learn a joint model over the multimodal pairs. Second, we design an optimal training curriculum that balances representation learning and data generation. Our experiments across image, protein sequence, and molecule generation tasks demonstrate superior performance as well as accelerated training. In particular, on the class-conditional ImageNet $256\\times 256$ benchmark, our guidance results in $23.3$ times faster training than the original SiT-XL as well as four times speedup over the state-of-the-art method REPA. The code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 34,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08983",
        "abs_url": "https://arxiv.org/abs/2507.08983",
        "pdf_url": "https://arxiv.org/pdf/2507.08983",
        "title": "Exploiting Leaderboards for Large-Scale Distribution of Malicious Models",
        "authors": [
            "Anshuman Suri",
            "Harsh Chaudhari",
            "Yuefeng Peng",
            "Ali Naseh",
            "Amir Houmansadr",
            "Alina Oprea"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Cryptography and Security (cs.CR)",
        "abstract": "While poisoning attacks on machine learning models have been extensively studied, the mechanisms by which adversaries can distribute poisoned models at scale remain largely unexplored. In this paper, we shed light on how model leaderboards -- ranked platforms for model discovery and evaluation -- can serve as a powerful channel for adversaries for stealthy large-scale distribution of poisoned models. We present TrojanClimb, a general framework that enables injection of malicious behaviors while maintaining competitive leaderboard performance. We demonstrate its effectiveness across four diverse modalities: text-embedding, text-generation, text-to-speech and text-to-image, showing that adversaries can successfully achieve high leaderboard rankings while embedding arbitrary harmful functionalities, from backdoors to bias injection. Our findings reveal a significant vulnerability in the machine learning ecosystem, highlighting the urgent need to redesign leaderboard evaluation mechanisms to detect and filter malicious (e.g., poisoned) models, while exposing broader security implications for the machine learning community regarding the risks of adopting models from unverified sources.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 35,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09009",
        "abs_url": "https://arxiv.org/abs/2507.09009",
        "pdf_url": "https://arxiv.org/pdf/2507.09009",
        "title": "Multimodal Cardiovascular Risk Profiling Using Self-Supervised Learning of Polysomnography",
        "authors": [
            "Zhengxiao He",
            "Huayu Li",
            "Geng Yuan",
            "William D.S. Killgore",
            "Stuart F. Quan",
            "Chen X. Chen",
            "Ao Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Methods: We developed a self-supervised deep learning model that extracts meaningful patterns from multi-modal signals (Electroencephalography (EEG), Electrocardiography (ECG), and respiratory signals). The model was trained on data from 4,398 participants. Projection scores were derived by contrasting embeddings from individuals with and without CVD outcomes. External validation was conducted in an independent cohort with 1,093 participants. The source code is available on this https URL. Results: The projection scores revealed distinct and clinically meaningful patterns across modalities. ECG-derived features were predictive of both prevalent and incident cardiac conditions, particularly CVD mortality. EEG-derived features were predictive of incident hypertension and CVD mortality. Respiratory signals added complementary predictive value. Combining these projection scores with the Framingham Risk Score consistently improved predictive performance, achieving area under the curve values ranging from 0.607 to 0.965 across different outcomes. Findings were robustly replicated and validated in the external testing cohort. Conclusion: Our findings demonstrate that the proposed framework can generate individualized CVD risk scores directly from PSG data. The resulting projection scores have the potential to be integrated into clinical practice, enhancing risk assessment and supporting personalized care.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 36,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09016",
        "abs_url": "https://arxiv.org/abs/2507.09016",
        "pdf_url": "https://arxiv.org/pdf/2507.09016",
        "title": "Enhancing RLHF with Human Gaze Modeling",
        "authors": [
            "Karim Galliamov",
            "Ivan Titov",
            "Ilya Pershin"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Reinforcement Learning from Human Feedback (RLHF) aligns language models with human preferences but is computationally expensive. We explore two approaches that leverage human gaze modeling to enhance RLHF: (1) gaze-aware reward models and (2) gaze-based distribution of sparse rewards at token level. Our experiments demonstate that gaze-informed RLHF achieves faster convergence while maintaining or slightly improving performance, thus, reducing computational costs during policy optimization. These results show that human gaze provides a valuable and underused signal for policy optimization, pointing to a promising direction for improving RLHF efficiency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 37,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09019",
        "abs_url": "https://arxiv.org/abs/2507.09019",
        "pdf_url": "https://arxiv.org/pdf/2507.09019",
        "title": "On Evaluating Performance of LLM Inference Serving Systems",
        "authors": [
            "Amey Agrawal",
            "Nitin Kedia",
            "Anmol Agarwal",
            "Jayashree Mohan",
            "Nipun Kwatra",
            "Souvik Kundu",
            "Ramachandran Ramjee",
            "Alexey Tumanov"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)",
        "abstract": "The rapid evolution of Large Language Model (LLM) inference systems has yielded significant efficiency improvements. However, our systematic analysis reveals that current evaluation methodologies frequently exhibit fundamental flaws, often manifesting as common evaluation anti-patterns that obscure true performance characteristics and impede scientific progress. Through a comprehensive examination of recent systems, we identify recurring anti-patterns across three key dimensions: Baseline Fairness, Evaluation Setup, and Metric Design. These anti-patterns are uniquely problematic for LLM inference due to its dual-phase nature combining distinct prefill and decode operations, its handling of highly heterogeneous workloads, and its strict temporal requirements for interactive use. We demonstrate how common anti-patterns -- such as inadequate baseline comparisons that conflate engineering effort with algorithmic novelty, workload selections that fail to represent production scenarios, and metric normalizations that hide substantial performance variability like generation stalls-lead to misleading conclusions. To address these challenges, we provide a comprehensive checklist derived from our analysis, establishing a framework for recognizing and avoiding these anti-patterns in favor of robust LLM inference evaluation. To demonstrate the practical application of our framework, we present a case study analyzing speculative decoding, a technique whose bursty, non-uniform token generation is easily misinterpreted when evaluated using approaches characteristic of these anti-patterns. Our work establishes a rigorous foundation for evaluation methodology, enabling meaningful comparisons, ensuring reproducible results, and ultimately accelerating genuine progress in LLM inference systems by moving beyond common anti-patterns to align evaluation with real-world requirements.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 38,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09029",
        "abs_url": "https://arxiv.org/abs/2507.09029",
        "pdf_url": "https://arxiv.org/pdf/2507.09029",
        "title": "Model Parallelism With Subnetwork Data Parallelism",
        "authors": [
            "Vaibhav Singh",
            "Zafir Khalid",
            "Edouard Oyallon",
            "Eugene Belilovsky"
        ],
        "comments": "6 pages, 1 figure",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Distributed pre-training of large models at scale often imposes heavy memory demands on individual nodes and incurs significant intra-node communication costs. We propose a novel alternative approach that reduces the memory requirements by training small, structured subnetworks of the model on separate workers. Unlike pipelining, our method avoids inter-node activation communication and maintains bandwidth requirements that are comparable to or lower than standard data parallel communication schemes based on all-reduce. We evaluate two subnetwork construction strategies guided by the principle of ensuring uniform representation of each parameter across the distributed training setup. Our results show that the stochastic block dropping technique consistently outperforms the width-wise subnetwork construction previously explored in federated learning. We empirically attribute this superior performance to stronger gradient alignment in subnetworks that retain blocks having skip connections. Preliminary experiments highlight the promise of our approach, achieving a 20-40% reduction in memory usage without any loss in performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 39,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09031",
        "abs_url": "https://arxiv.org/abs/2507.09031",
        "pdf_url": "https://arxiv.org/pdf/2507.09031",
        "title": "Confounder-Free Continual Learning via Recursive Feature Normalization",
        "authors": [
            "Yash Shah",
            "Camila Gonzalez",
            "Mohammad H. Abbasi",
            "Qingyu Zhao",
            "Kilian M. Pohl",
            "Ehsan Adeli"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Confounders are extraneous variables that affect both the input and the target, resulting in spurious correlations and biased predictions. There are recent advances in dealing with or removing confounders in traditional models, such as metadata normalization (MDN), where the distribution of the learned features is adjusted based on the study confounders. However, in the context of continual learning, where a model learns continuously from new data over time without forgetting, learning feature representations that are invariant to confounders remains a significant challenge. To remove their influence from intermediate feature representations, we introduce the Recursive MDN (R-MDN) layer, which can be integrated into any deep learning architecture, including vision transformers, and at any model stage. R-MDN performs statistical regression via the recursive least squares algorithm to maintain and continually update an internal model state with respect to changing distributions of data and confounding variables. Our experiments demonstrate that R-MDN promotes equitable predictions across population groups, both within static learning and across different stages of continual learning, by reducing catastrophic forgetting caused by confounder effects changing over time.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 40,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09041",
        "abs_url": "https://arxiv.org/abs/2507.09041",
        "pdf_url": "https://arxiv.org/pdf/2507.09041",
        "title": "Behavioral Exploration: Learning to Explore via In-Context Adaptation",
        "authors": [
            "Andrew Wagenmaker",
            "Zhiyuan Zhou",
            "Sergey Levine"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Robotics (cs.RO); Systems and Control (eess.SY)",
        "abstract": "Developing autonomous agents that quickly explore an environment and adapt their behavior online is a canonical challenge in robotics and machine learning. While humans are able to achieve such fast online exploration and adaptation, often acquiring new information and skills in only a handful of interactions, existing algorithmic approaches tend to rely on random exploration and slow, gradient-based behavior updates. How can we endow autonomous agents with such capabilities on par with humans? Taking inspiration from recent progress on both in-context learning and large-scale behavioral cloning, in this work we propose behavioral exploration: training agents to internalize what it means to explore and adapt in-context over the space of ``expert'' behaviors. To achieve this, given access to a dataset of expert demonstrations, we train a long-context generative model to predict expert actions conditioned on a context of past observations and a measure of how ``exploratory'' the expert's behaviors are relative to this context. This enables the model to not only mimic the behavior of an expert, but also, by feeding its past history of interactions into its context, to select different expert behaviors than what have been previously selected, thereby allowing for fast online adaptation and targeted, ``expert-like'' exploration. We demonstrate the effectiveness of our method in both simulated locomotion and manipulation settings, as well as on real-world robotic manipulation tasks, illustrating its ability to learn adaptive, exploratory behavior.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 41,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09043",
        "abs_url": "https://arxiv.org/abs/2507.09043",
        "pdf_url": "https://arxiv.org/pdf/2507.09043",
        "title": "Shortening the Trajectories: Identity-Aware Gaussian Approximation for Efficient 3D Molecular Generation",
        "authors": [
            "Jingxiang Qu",
            "Wenhan Gao",
            "Yi Liu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Gaussian-based Probabilistic Generative Models (GPGMs) generate data by reversing a stochastic process that progressively corrupts samples with Gaussian noise. While these models have achieved state-of-the-art performance across diverse domains, their practical deployment remains constrained by the high computational cost of long generative trajectories, which often involve hundreds to thousands of steps during training and sampling. In this work, we introduce a theoretically grounded and empirically validated framework that improves generation efficiency without sacrificing training granularity or inference fidelity. Our key insight is that for certain data modalities, the noising process causes data to rapidly lose its identity and converge toward a Gaussian distribution. We analytically identify a characteristic step at which the data has acquired sufficient Gaussianity, and then replace the remaining generation trajectory with a closed-form Gaussian approximation. Unlike existing acceleration techniques that coarsening the trajectories by skipping steps, our method preserves the full resolution of learning dynamics while avoiding redundant stochastic perturbations between `Gaussian-like' distributions. Empirical results across multiple data modalities demonstrate substantial improvements in both sample quality and computational efficiency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 42,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09061",
        "abs_url": "https://arxiv.org/abs/2507.09061",
        "pdf_url": "https://arxiv.org/pdf/2507.09061",
        "title": "Imitation Learning in Continuous Action Spaces: Mitigating Compounding Error without Interaction",
        "authors": [
            "Thomas T. Zhang",
            "Daniel Pfrommer",
            "Nikolai Matni",
            "Max Simchowitz"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Systems and Control (eess.SY); Machine Learning (stat.ML)",
        "abstract": "We study the problem of imitating an expert demonstrator in a continuous state-and-action dynamical system. While imitation learning in discrete settings such as autoregressive language modeling has seen immense success and popularity in recent years, imitation in physical settings such as autonomous driving and robot learning has proven comparably more complex due to the compounding errors problem, often requiring elaborate set-ups to perform stably. Recent work has demonstrated that even in benign settings, exponential compounding errors are unavoidable when learning solely from expert-controlled trajectories, suggesting the need for more advanced policy parameterizations or data augmentation. To this end, we present minimal interventions that provably mitigate compounding errors in continuous state-and-action imitation learning. When the system is open-loop stable, we prescribe \"action chunking,\" i.e., predicting and playing sequences of actions in open-loop; when the system is possibly unstable, we prescribe \"noise injection,\" i.e., adding noise during expert demonstrations. These interventions align with popular choices in modern robot learning, though the benefits we derive are distinct from the effects they were designed to target. Our results draw insights and tools from both control theory and reinforcement learning; however, our analysis reveals novel considerations that do not naturally arise when either literature is considered in isolation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 43,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09084",
        "abs_url": "https://arxiv.org/abs/2507.09084",
        "pdf_url": "https://arxiv.org/pdf/2507.09084",
        "title": "Queue up for takeoff: a transferable deep learning framework for flight delay prediction",
        "authors": [
            "Nnamdi Daniel Aghanya",
            "Ta Duong Vu",
            "Amaëlle Diop",
            "Charlotte Deville",
            "Nour Imane Kerroumi",
            "Irene Moulitsas",
            "Jun Li",
            "Desmond Bisandu"
        ],
        "comments": "3 figures, 20 pages references and appendix included,",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Flight delays are a significant challenge in the aviation industry, causing major financial and operational disruptions. To improve passenger experience and reduce revenue loss, flight delay prediction models must be both precise and generalizable across different networks. This paper introduces a novel approach that combines Queue-Theory with a simple attention model, referred to as the Queue-Theory SimAM (QT-SimAM). To validate our model, we used data from the US Bureau of Transportation Statistics, where our proposed QT-SimAM (Bidirectional) model outperformed existing methods with an accuracy of 0.927 and an F1 score of 0.932. To assess transferability, we tested the model on the EUROCONTROL dataset. The results demonstrated strong performance, achieving an accuracy of 0.826 and an F1 score of 0.791. Ultimately, this paper outlines an effective, end-to-end methodology for predicting flight delays. The proposed model's ability to forecast delays with high accuracy across different networks can help reduce passenger anxiety and improve operational decision-making",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 44,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09087",
        "abs_url": "https://arxiv.org/abs/2507.09087",
        "pdf_url": "https://arxiv.org/pdf/2507.09087",
        "title": "Deep Reinforcement Learning with Gradient Eligibility Traces",
        "authors": [
            "Esraa Elelimy",
            "Brett Daley",
            "Andrew Patterson",
            "Marlos C. Machado",
            "Adam White",
            "Martha White"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "Achieving fast and stable off-policy learning in deep reinforcement learning (RL) is challenging. Most existing methods rely on semi-gradient temporal-difference (TD) methods for their simplicity and efficiency, but are consequently susceptible to divergence. While more principled approaches like Gradient TD (GTD) methods have strong convergence guarantees, they have rarely been used in deep RL. Recent work introduced the Generalized Projected Bellman Error ($\\GPBE$), enabling GTD methods to work efficiently with nonlinear function approximation. However, this work is only limited to one-step methods, which are slow at credit assignment and require a large number of samples. In this paper, we extend the $\\GPBE$ objective to support multistep credit assignment based on the $\\lambda$-return and derive three gradient-based methods that optimize this new objective. We provide both a forward-view formulation compatible with experience replay and a backward-view formulation compatible with streaming algorithms. Finally, we evaluate the proposed algorithms and show that they outperform both PPO and StreamQ in MuJoCo and MinAtar environments, respectively. Code available at this https URL\\_algos",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 45,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09091",
        "abs_url": "https://arxiv.org/abs/2507.09091",
        "pdf_url": "https://arxiv.org/pdf/2507.09091",
        "title": "Continuous-Time Signal Decomposition: An Implicit Neural Generalization of PCA and ICA",
        "authors": [
            "Shayan K. Azmoodeh",
            "Krishna Subramani",
            "Paris Smaragdis"
        ],
        "comments": "6 pages, 3 figures, 1 table. MLSP 2025",
        "subjects": "Machine Learning (cs.LG); Signal Processing (eess.SP); Machine Learning (stat.ML)",
        "abstract": "We generalize the low-rank decomposition problem, such as principal and independent component analysis (PCA, ICA) for continuous-time vector-valued signals and provide a model-agnostic implicit neural signal representation framework to learn numerical approximations to solve the problem. Modeling signals as continuous-time stochastic processes, we unify the approaches to both the PCA and ICA problems in the continuous setting through a contrast function term in the network loss, enforcing the desired statistical properties of the source signals (decorrelation, independence) learned in the decomposition. This extension to a continuous domain allows the application of such decompositions to point clouds and irregularly sampled signals where standard techniques are not applicable.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 46,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09095",
        "abs_url": "https://arxiv.org/abs/2507.09095",
        "pdf_url": "https://arxiv.org/pdf/2507.09095",
        "title": "On the Fragility of Multimodal Perception to Temporal Misalignment in Autonomous Driving",
        "authors": [
            "Md Hasan Shahriar",
            "Md Mohaimin Al Barat",
            "Harshavardhan Sundar",
            "Naren Ramakrishnan",
            "Y. Thomas Hou",
            "Wenjing Lou"
        ],
        "comments": "16 pages",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Multimodal fusion (MMF) plays a critical role in the perception of autonomous driving, which primarily fuses camera and LiDAR streams for a comprehensive and efficient scene understanding. However, its strict reliance on precise temporal synchronization exposes it to new vulnerabilities. In this paper, we introduce DejaVu, a novel attack that exploits network-induced delays to create subtle temporal misalignments across sensor streams, severely degrading downstream MMF-based perception tasks. Our comprehensive attack analysis across different models and datasets reveals these sensors' task-specific imbalanced sensitivities: object detection is overly dependent on LiDAR inputs while object tracking is highly reliant on the camera inputs. Consequently, with a single-frame LiDAR delay, an attacker can reduce the car detection mAP by up to 88.5%, while with a three-frame camera delay, multiple object tracking accuracy (MOTA) for car drops by 73%. To detect such attacks, we propose AION, a defense patch that can work alongside the existing perception model to monitor temporal alignment through cross-modal temporal consistency. AION leverages multimodal shared representation learning and dynamic time warping to determine the path of temporal alignment and calculate anomaly scores based on the alignment. Our thorough evaluation of AION shows it achieves AUROC scores of 0.92-0.98 with low false positives across datasets and model architectures, demonstrating it as a robust and generalized defense against the temporal misalignment attacks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 47,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09101",
        "abs_url": "https://arxiv.org/abs/2507.09101",
        "pdf_url": "https://arxiv.org/pdf/2507.09101",
        "title": "S2SRec2: Set-to-Set Recommendation for Basket Completion with Recipe",
        "authors": [
            "Yanan Cao",
            "Omid Memarrast",
            "Shiqin Cai",
            "Sinduja Subramaniam",
            "Evren Korpeoglu",
            "Kannan Achan"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In grocery e-commerce, customers often build ingredient baskets guided by dietary preferences but lack the expertise to create complete meals. Leveraging recipe knowledge to recommend complementary ingredients based on a partial basket is essential for improving the culinary experience. Traditional recipe completion methods typically predict a single missing ingredient using a leave-one-out strategy. However, they fall short in two key aspects: (i) they do not reflect real-world scenarios where multiple ingredients are often needed, and (ii) they overlook relationships among the missing ingredients themselves. To address these limitations, we reformulate basket completion as a set-to-set (S2S) recommendation problem, where an incomplete basket is input into a system that predicts a set of complementary ingredients. We introduce S2SRec2, a set-to-set ingredient recommendation framework based on a Set Transformer and trained in a multitask learning paradigm. S2SRec2 jointly learns to (i) retrieve missing ingredients from the representation of existing ones and (ii) assess basket completeness after prediction. These tasks are optimized together, enforcing accurate retrieval and coherent basket completion. Experiments on large-scale recipe datasets and qualitative analyses show that S2SRec2 significantly outperforms single-target baselines, offering a promising approach to enhance grocery shopping and inspire culinary creativity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 48,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09127",
        "abs_url": "https://arxiv.org/abs/2507.09127",
        "pdf_url": "https://arxiv.org/pdf/2507.09127",
        "title": "A Study of Value-Aware Eigenoptions",
        "authors": [
            "Harshil Kotamreddy",
            "Marlos C. Machado"
        ],
        "comments": "Presented at the RLC Workshop on Inductive Biases in Reinforcement Learning 2025",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Options, which impose an inductive bias toward temporal and hierarchical structure, offer a powerful framework for reinforcement learning (RL). While effective in sequential decision-making, they are often handcrafted rather than learned. Among approaches for discovering options, eigenoptions have shown strong performance in exploration, but their role in credit assignment remains underexplored. In this paper, we investigate whether eigenoptions can accelerate credit assignment in model-free RL, evaluating them in tabular and pixel-based gridworlds. We find that pre-specified eigenoptions aid not only exploration but also credit assignment, whereas online discovery can bias the agent's experience too strongly and hinder learning. In the context of deep RL, we also propose a method for learning option-values under non-linear function approximation, highlighting the impact of termination conditions on performance. Our findings reveal both the promise and complexity of using eigenoptions, and options more broadly, to simultaneously support credit assignment and exploration in reinforcement learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 49,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09132",
        "abs_url": "https://arxiv.org/abs/2507.09132",
        "pdf_url": "https://arxiv.org/pdf/2507.09132",
        "title": "Heterogeneous Graph Prompt Learning via Adaptive Weight Pruning",
        "authors": [
            "Chu-Yuan Wei",
            "Shun-Yao Liu",
            "Sheng-Da Zhuo",
            "Chang-Dong Wang",
            "Shu-Qiang Huang",
            "Mohsen Guizani"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Graph Neural Networks (GNNs) have achieved remarkable success in various graph-based tasks (e.g., node classification or link prediction). Despite their triumphs, GNNs still face challenges such as long training and inference times, difficulty in capturing complex relationships, and insufficient feature extraction. To tackle these issues, graph pre-training and graph prompt methods have garnered increasing attention for their ability to leverage large-scale datasets for initial learning and task-specific adaptation, offering potential improvements in GNN performance. However, previous research has overlooked the potential of graph prompts in optimizing models, as well as the impact of both positive and negative graph prompts on model stability and efficiency. To bridge this gap, we propose a novel framework combining graph prompts with weight pruning, called GPAWP, which aims to enhance the performance and efficiency of graph prompts by using fewer of them. We evaluate the importance of graph prompts using an importance assessment function to determine positive and negative weights at different granularities. Through hierarchically structured pruning, we eliminate negative prompt labels, resulting in more parameter-efficient and competitively performing prompts. Extensive experiments on three benchmark datasets demonstrate the superiority of GPAWP, leading to a significant reduction in parameters in node classification tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 50,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09137",
        "abs_url": "https://arxiv.org/abs/2507.09137",
        "pdf_url": "https://arxiv.org/pdf/2507.09137",
        "title": "POIFormer: A Transformer-Based Framework for Accurate and Scalable Point-of-Interest Attribution",
        "authors": [
            "Nripsuta Ani Saxena",
            "Shang-Ling Hsu",
            "Mehul Shetty",
            "Omar Alkhadra",
            "Cyrus Shahabi",
            "Abigail L. Horn"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Accurately attributing user visits to specific Points of Interest (POIs) is a foundational task for mobility analytics, personalized services, marketing and urban planning. However, POI attribution remains challenging due to GPS inaccuracies, typically ranging from 2 to 20 meters in real-world settings, and the high spatial density of POIs in urban environments, where multiple venues can coexist within a small radius (e.g., over 50 POIs within a 100-meter radius in dense city centers). Relying on proximity is therefore often insufficient for determining which POI was actually visited. We introduce \\textsf{POIFormer}, a novel Transformer-based framework for accurate and efficient POI attribution. Unlike prior approaches that rely on limited spatiotemporal, contextual, or behavioral features, \\textsf{POIFormer} jointly models a rich set of signals, including spatial proximity, visit timing and duration, contextual features from POI semantics, and behavioral features from user mobility and aggregated crowd behavior patterns--using the Transformer's self-attention mechanism to jointly model complex interactions across these dimensions. By leveraging the Transformer to model a user's past and future visits (with the current visit masked) and incorporating crowd-level behavioral patterns through pre-computed KDEs, \\textsf{POIFormer} enables accurate, efficient attribution in large, noisy mobility datasets. Its architecture supports generalization across diverse data sources and geographic contexts while avoiding reliance on hard-to-access or unavailable data layers, making it practical for real-world deployment. Extensive experiments on real-world mobility datasets demonstrate significant improvements over existing baselines, particularly in challenging real-world settings characterized by spatial noise and dense POI clustering.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 51,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09173",
        "abs_url": "https://arxiv.org/abs/2507.09173",
        "pdf_url": "https://arxiv.org/pdf/2507.09173",
        "title": "Towards Interpretable Drug-Drug Interaction Prediction: A Graph-Based Approach with Molecular and Network-Level Explanations",
        "authors": [
            "Mengjie Chen",
            "Ming Zhang",
            "Cunquan Qu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Molecular Networks (q-bio.MN)",
        "abstract": "Drug-drug interactions (DDIs) represent a critical challenge in pharmacology, often leading to adverse drug reactions with significant implications for patient safety and healthcare outcomes. While graph-based methods have achieved strong predictive performance, most approaches treat drug pairs independently, overlooking the complex, context-dependent interactions unique to drug pairs. Additionally, these models struggle to integrate biological interaction networks and molecular-level structures to provide meaningful mechanistic insights. In this study, we propose MolecBioNet, a novel graph-based framework that integrates molecular and biomedical knowledge for robust and interpretable DDI prediction. By modeling drug pairs as unified entities, MolecBioNet captures both macro-level biological interactions and micro-level molecular influences, offering a comprehensive perspective on DDIs. The framework extracts local subgraphs from biomedical knowledge graphs and constructs hierarchical interaction graphs from molecular representations, leveraging classical graph neural network methods to learn multi-scale representations of drug pairs. To enhance accuracy and interpretability, MolecBioNet introduces two domain-specific pooling strategies: context-aware subgraph pooling (CASPool), which emphasizes biologically relevant entities, and attention-guided influence pooling (AGIPool), which prioritizes influential molecular substructures. The framework further employs mutual information minimization regularization to enhance information diversity during embedding fusion. Experimental results demonstrate that MolecBioNet outperforms state-of-the-art methods in DDI prediction, while ablation studies and embedding visualizations further validate the advantages of unified drug pair modeling and multi-scale knowledge integration.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 52,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09177",
        "abs_url": "https://arxiv.org/abs/2507.09177",
        "pdf_url": "https://arxiv.org/pdf/2507.09177",
        "title": "Continual Reinforcement Learning by Planning with Online World Models",
        "authors": [
            "Zichen Liu",
            "Guoji Fu",
            "Chao Du",
            "Wee Sun Lee",
            "Min Lin"
        ],
        "comments": "ICML 2025 Spotlight",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "Continual reinforcement learning (CRL) refers to a naturalistic setting where an agent needs to endlessly evolve, by trial and error, to solve multiple tasks that are presented sequentially. One of the largest obstacles to CRL is that the agent may forget how to solve previous tasks when learning a new task, known as catastrophic forgetting. In this paper, we propose to address this challenge by planning with online world models. Specifically, we learn a Follow-The-Leader shallow model online to capture the world dynamics, in which we plan using model predictive control to solve a set of tasks specified by any reward functions. The online world model is immune to forgetting by construction with a proven regret bound of $\\mathcal{O}(\\sqrt{K^2D\\log(T)})$ under mild assumptions. The planner searches actions solely based on the latest online model, thus forming a FTL Online Agent (OA) that updates incrementally. To assess OA, we further design Continual Bench, a dedicated environment for CRL, and compare with several strong baselines under the same model-planning algorithmic framework. The empirical results show that OA learns continuously to solve new tasks while not forgetting old skills, outperforming agents built on deep world models with various continual learning techniques.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 53,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09202",
        "abs_url": "https://arxiv.org/abs/2507.09202",
        "pdf_url": "https://arxiv.org/pdf/2507.09202",
        "title": "XiChen: An observation-scalable fully AI-driven global weather forecasting system with 4D variational knowledge",
        "authors": [
            "Wuxin Wang",
            "Weicheng Ni",
            "Lilan Huang",
            "Tao Hao",
            "Ben Fei",
            "Shuo Ma",
            "Taikang Yuan",
            "Yanlai Zhao",
            "Kefeng Deng",
            "Xiaoyong Li",
            "Boheng Duan",
            "Lei Bai",
            "Kaijun Ren"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Atmospheric and Oceanic Physics (physics.ao-ph)",
        "abstract": "Recent advancements in Artificial Intelligence (AI) demonstrate significant potential to revolutionize weather forecasting. However, most AI-driven models rely on Numerical Weather Prediction (NWP) systems for initial condition preparation, which often consumes hours on supercomputers. Here we introduce XiChen, the first observation-scalable fully AI-driven global weather forecasting system, whose entire pipeline, from Data Assimilation (DA) to medium-range forecasting, can be accomplished within only 17 seconds. XiChen is built upon a foundation model that is pre-trained for weather forecasting. Meanwhile, this model is subsequently fine-tuned to serve as both observation operators and DA models, thereby scalably assimilating conventional and raw satellite observations. Furthermore, the integration of four-dimensional variational knowledge ensures that XiChen's DA and medium-range forecasting accuracy rivals that of operational NWP systems, amazingly achieving a skillful forecasting lead time exceeding 8.25 days. These findings demonstrate that XiChen holds strong potential toward fully AI-driven weather forecasting independent of NWP systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 54,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09211",
        "abs_url": "https://arxiv.org/abs/2507.09211",
        "pdf_url": "https://arxiv.org/pdf/2507.09211",
        "title": "Capturing Unseen Spatial Extremes Through Knowledge-Informed Generative Modeling",
        "authors": [
            "Xinyue Liu",
            "Xiao Peng",
            "Shuyue Yan",
            "Yuntian Chen",
            "Dongxiao Zhang",
            "Zhixiao Niu",
            "Hui-Min Wang",
            "Xiaogang He"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Atmospheric and Oceanic Physics (physics.ao-ph); Data Analysis, Statistics and Probability (physics.data-an); Geophysics (physics.geo-ph); Machine Learning (stat.ML)",
        "abstract": "Observed records of climate extremes provide an incomplete picture of risk, missing \"unseen\" extremes that exceed historical bounds. In parallel, neglecting spatial dependence undervalues the risk of synchronized hazards that amplify impacts. To address these challenges, we develop DeepX-GAN (Dependence-Enhanced Embedding for Physical eXtremes - Generative Adversarial Network), a knowledge-informed deep generative model designed to better capture the spatial structure of rare extremes. The zero-shot generalizability of DeepX-GAN enables simulation of unseen extremes that fall outside historical experience yet remain statistically plausible. We define two types of unseen extremes: \"checkmate\" extremes that directly hit targets, and \"stalemate\" extremes that narrowly miss. These unrealized scenarios expose latent risks in fragile systems and may reinforce a false sense of resilience if overlooked. Near misses, in particular, can prompt either proactive adaptation or dangerous complacency, depending on how they are interpreted. Applying DeepX-GAN to the Middle East and North Africa (MENA), we find that these unseen extremes disproportionately affect regions with high vulnerability and low socioeconomic readiness, but differ in urgency and interpretation. Future warming could expand and redistribute these unseen extremes, with emerging exposure hotspots in Indo-Pakistan and Central Africa. This distributional shift highlights critical blind spots in conventional hazard planning and underscores the need to develop spatially adaptive policies that anticipate emergent risk hotspots rather than simply extrapolating from historical patterns.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 55,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09212",
        "abs_url": "https://arxiv.org/abs/2507.09212",
        "pdf_url": "https://arxiv.org/pdf/2507.09212",
        "title": "Warm Starts Accelerate Generative Modelling",
        "authors": [
            "Jonas Scholz",
            "Richard E. Turner"
        ],
        "comments": "10 pages, 6 figures",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)",
        "abstract": "Iterative generative models, like diffusion and flow-matching, create high-fidelity samples by progressively refining a noise vector into data. However, this process is notoriously slow, often requiring hundreds of function evaluations. We introduce the warm-start model, a simple, deterministic model that dramatically accelerates conditional generation by providing a better starting point. Instead of starting generation from an uninformed N(0, I) prior, our warm-start model predicts an informed prior N(mu, sigma), whose moments are conditioned on the input context. This \"warm start\" substantially reduces the distance the generative process must traverse, particularly when the conditioning information is strongly informative. On tasks like image inpainting, our method achieves results competitive with a 1000-step DDPM baseline using only 11 total function evaluations (1 for the warm start, 10 for generation). A simple conditional normalization trick makes our method compatible with any standard generative model and sampler without modification, allowing it to be combined with other efficient sampling techniques for further acceleration. Our implementation is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 56,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09213",
        "abs_url": "https://arxiv.org/abs/2507.09213",
        "pdf_url": "https://arxiv.org/pdf/2507.09213",
        "title": "Optimizing Basis Function Selection in Constructive Wavelet Neural Networks and Its Applications",
        "authors": [
            "Dunsheng Huang",
            "Dong Shen",
            "Lei Lu",
            "Ying Tan"
        ],
        "comments": "17pages",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Wavelet neural network (WNN), which learns an unknown nonlinear mapping from the data, has been widely used in signal processing, and time-series analysis. However, challenges in constructing accurate wavelet bases and high computational costs limit their application. This study introduces a constructive WNN that selects initial bases and trains functions by introducing new bases for predefined accuracy while reducing computational costs. For the first time, we analyze the frequency of unknown nonlinear functions and select appropriate initial wavelets based on their primary frequency components by estimating the energy of the spatial frequency component. This leads to a novel constructive framework consisting of a frequency estimator and a wavelet-basis increase mechanism to prioritize high-energy bases, significantly improving computational efficiency. The theoretical foundation defines the necessary time-frequency range for high-dimensional wavelets at a given accuracy. The framework's versatility is demonstrated through four examples: estimating unknown static mappings from offline data, combining two offline datasets, identifying time-varying mappings from time-series data, and capturing nonlinear dependencies in real time-series data. These examples showcase the framework's broad applicability and practicality. All the code will be released at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 57,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09252",
        "abs_url": "https://arxiv.org/abs/2507.09252",
        "pdf_url": "https://arxiv.org/pdf/2507.09252",
        "title": "TPP-SD: Accelerating Transformer Point Process Sampling with Speculative Decoding",
        "authors": [
            "Shukai Gong",
            "Yiyang Fu",
            "Fengyuan Ran",
            "Feng Zhou"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "We propose TPP-SD, a novel approach that accelerates Transformer temporal point process (TPP) sampling by adapting speculative decoding (SD) techniques from language models. By identifying the structural similarities between thinning algorithms for TPPs and speculative decoding for language models, we develop an efficient sampling framework that leverages a smaller draft model to generate multiple candidate events, which are then verified by the larger target model in parallel. TPP-SD maintains the same output distribution as autoregressive sampling while achieving significant acceleration. Experiments on both synthetic and real datasets demonstrate that our approach produces samples from identical distributions as standard methods, but with 2-6$\\times$ speedup. Our ablation studies analyze the impact of hyperparameters such as draft length and draft model size on sampling efficiency. TPP-SD bridges the gap between powerful Transformer TPP models and the practical need for rapid sequence sampling.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 58,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09264",
        "abs_url": "https://arxiv.org/abs/2507.09264",
        "pdf_url": "https://arxiv.org/pdf/2507.09264",
        "title": "Controllable Patching for Compute-Adaptive Surrogate Modeling of Partial Differential Equations",
        "authors": [
            "Payel Mukhopadhyay",
            "Michael McCabe",
            "Ruben Ohana",
            "Miles Cranmer"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Image and Video Processing (eess.IV)",
        "abstract": "Patch-based transformer surrogates have become increasingly effective for modeling spatiotemporal dynamics, but the fixed patch size is a major limitation for budget-conscience deployment in production. We introduce two lightweight, architecture-agnostic modules-the Convolutional Kernel Modulator (CKM) and Convolutional Stride Modulator (CSM)-that enable dynamic patch size control at inference in patch based models, without retraining or accuracy loss. Combined with a cyclic patch-size rollout, our method mitigates patch artifacts and improves long-term stability for video-like prediction tasks. Applied to a range of challenging 2D and 3D PDE benchmarks, our approach improves rollout fidelity and runtime efficiency. To our knowledge, this is the first framework to enable inference-time patch-size tunability in patch-based PDE surrogates. Its plug-and-play design makes it broadly applicable across architectures-establishing a general foundation for compute-adaptive modeling in PDE surrogate tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 59,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09353",
        "abs_url": "https://arxiv.org/abs/2507.09353",
        "pdf_url": "https://arxiv.org/pdf/2507.09353",
        "title": "Impute With Confidence: A Framework for Uncertainty Aware Multivariate Time Series Imputation",
        "authors": [
            "Addison Weatherhead",
            "Anna Goldenberg"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "Time series data with missing values is common across many domains. Healthcare presents special challenges due to prolonged periods of sensor disconnection. In such cases, having a confidence measure for imputed values is critical. Most existing methods either overlook model uncertainty or lack mechanisms to estimate it. To address this gap, we introduce a general framework that quantifies and leverages uncertainty for selective imputation. By focusing on values the model is most confident in, highly unreliable imputations are avoided. Our experiments on multiple EHR datasets, covering diverse types of missingness, demonstrate that selectively imputing less-uncertain values not only reduces imputation errors but also improves downstream tasks. Specifically, we show performance gains in a 24-hour mortality prediction task, underscoring the practical benefit of incorporating uncertainty into time series imputation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 60,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09362",
        "abs_url": "https://arxiv.org/abs/2507.09362",
        "pdf_url": "https://arxiv.org/pdf/2507.09362",
        "title": "Meta-autoencoders: An approach to discovery and representation of relationships between dynamically evolving classes",
        "authors": [
            "Assaf Marron",
            "Smadar Szekely",
            "Irun Cohen",
            "David Harel"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Populations and Evolution (q-bio.PE)",
        "abstract": "An autoencoder (AE) is a neural network that, using self-supervised training, learns a succinct parameterized representation, and a corresponding encoding and decoding process, for all instances in a given class. Here, we introduce the concept of a meta-autoencoder (MAE): an AE for a collection of autoencoders. Given a family of classes that differ from each other by the values of some parameters, and a trained AE for each class, an MAE for the family is a neural net that has learned a compact representation and associated encoder and decoder for the class-specific AEs. One application of this general concept is in research and modeling of natural evolution -- capturing the defining and the distinguishing properties across multiple species that are dynamically evolving from each other and from common ancestors. In this interim report we provide a constructive definition of MAEs, initial examples, and the motivating research directions in machine learning and biology.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 61,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09382",
        "abs_url": "https://arxiv.org/abs/2507.09382",
        "pdf_url": "https://arxiv.org/pdf/2507.09382",
        "title": "Fair CCA for Fair Representation Learning: An ADNI Study",
        "authors": [
            "Bojian Hou",
            "Zhanliang Wang",
            "Zhuoping Zhou",
            "Boning Tong",
            "Zexuan Wang",
            "Jingxuan Bao",
            "Duy Duong-Tran",
            "Qi Long",
            "Li Shen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computers and Society (cs.CY)",
        "abstract": "Canonical correlation analysis (CCA) is a technique for finding correlations between different data modalities and learning low-dimensional representations. As fairness becomes crucial in machine learning, fair CCA has gained attention. However, previous approaches often overlook the impact on downstream classification tasks, limiting applicability. We propose a novel fair CCA method for fair representation learning, ensuring the projected features are independent of sensitive attributes, thus enhancing fairness without compromising accuracy. We validate our method on synthetic data and real-world data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), demonstrating its ability to maintain high correlation analysis performance while improving fairness in classification tasks. Our work enables fair machine learning in neuroimaging studies where unbiased analysis is essential.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 62,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09391",
        "abs_url": "https://arxiv.org/abs/2507.09391",
        "pdf_url": "https://arxiv.org/pdf/2507.09391",
        "title": "Geometric Generative Modeling with Noise-Conditioned Graph Networks",
        "authors": [
            "Peter Pao-Huang",
            "Mitchell Black",
            "Xiaojie Qiu"
        ],
        "comments": "ICML 2025",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Generative modeling of graphs with spatial structure is essential across many applications from computer graphics to spatial genomics. Recent flow-based generative models have achieved impressive results by gradually adding and then learning to remove noise from these graphs. Existing models, however, use graph neural network architectures that are independent of the noise level, limiting their expressiveness. To address this issue, we introduce \\textit{Noise-Conditioned Graph Networks} (NCGNs), a class of graph neural networks that dynamically modify their architecture according to the noise level during generation. Our theoretical and empirical analysis reveals that as noise increases, (1) graphs require information from increasingly distant neighbors and (2) graphs can be effectively represented at lower resolutions. Based on these insights, we develop Dynamic Message Passing (DMP), a specific instantiation of NCGNs that adapts both the range and resolution of message passing to the noise level. DMP consistently outperforms noise-independent architectures on a variety of domains including $3$D point clouds, spatiotemporal transcriptomics, and images. Code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 63,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09394",
        "abs_url": "https://arxiv.org/abs/2507.09394",
        "pdf_url": "https://arxiv.org/pdf/2507.09394",
        "title": "A Random Matrix Theory Perspective on the Learning Dynamics of Multi-head Latent Attention",
        "authors": [
            "Nandan Kumar Jha",
            "Brandon Reagen"
        ],
        "comments": "ICML 2025 Workshop on High-dimensional Learning Dynamics (HiLD)",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In this work, we study how multi-head latent attention (MLA), a popular strategy for compressing key/value memory, affects a transformer's internal capacity during pretraining. Using a lightweight suite of Marchenko-Pastur (MP) diagnostics, we analyze the spectrum of the $W_{Q}W_{K}^\\top$ gram matrix throughout training, comparing three variants: the standard multi-head attention (MHA) baseline, MLA-PreRoPE with rotary applied before compression, and MLA-Decoupled, which shares a single rotary sub-vector across all heads. Our random matrix analysis reveals \\textbf{three key findings:} \\textbf{ i)} capacity bottlenecks emerge locally: both MHA and MLA-PreRoPE exhibit sharp, early spikes in specific layers that persist and propagate, disrupting the balance between bulk and outlier directions; \\textbf{ ii)} these spikes coincide with rank collapse, concentrating the model's expressivity into narrow subspaces; \\textbf{ iii)} only the decoupled variant prevents this cascade, maintaining broad spectral support and suppressing outlier formation across layers. These results underscore that \\emph{how} rotary embeddings are applied is just as critical as \\emph{where} compression occurs. Sharing rotary components across heads mitigates spectral fragmentation and preserves representational capacity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 64,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09404",
        "abs_url": "https://arxiv.org/abs/2507.09404",
        "pdf_url": "https://arxiv.org/pdf/2507.09404",
        "title": "Scaling Laws for Optimal Data Mixtures",
        "authors": [
            "Mustafa Shukor",
            "Louis Bethune",
            "Dan Busbridge",
            "David Grangier",
            "Enrico Fini",
            "Alaaeldin El-Nouby",
            "Pierre Ablin"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Large foundation models are typically trained on data from multiple domains, with the data mixture--the proportion of each domain used--playing a critical role in model performance. The standard approach to selecting this mixture relies on trial and error, which becomes impractical for large-scale pretraining. We propose a systematic method to determine the optimal data mixture for any target domain using scaling laws. Our approach accurately predicts the loss of a model of size $N$ trained with $D$ tokens and a specific domain weight vector $h$. We validate the universality of these scaling laws by demonstrating their predictive power in three distinct and large-scale settings: large language model (LLM), native multimodal model (NMM), and large vision models (LVM) pretraining. We further show that these scaling laws can extrapolate to new data mixtures and across scales: their parameters can be accurately estimated using a few small-scale training runs, and used to estimate the performance at larger scales and unseen domain weights. The scaling laws allow to derive the optimal domain weights for any target domain under a given training budget ($N$,$D$), providing a principled alternative to costly trial-and-error methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 65,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09406",
        "abs_url": "https://arxiv.org/abs/2507.09406",
        "pdf_url": "https://arxiv.org/pdf/2507.09406",
        "title": "Adversarial Activation Patching: A Framework for Detecting and Mitigating Emergent Deception in Safety-Aligned Transformers",
        "authors": [
            "Santhosh Kumar Ravindran"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Large language models (LLMs) aligned for safety through techniques like reinforcement learning from human feedback (RLHF) often exhibit emergent deceptive behaviors, where outputs appear compliant but subtly mislead or omit critical information. This paper introduces adversarial activation patching, a novel mechanistic interpretability framework that leverages activation patching as an adversarial tool to induce, detect, and mitigate such deception in transformer-based models. By sourcing activations from \"deceptive\" prompts and patching them into safe forward passes at specific layers, we simulate vulnerabilities and quantify deception rates. Through toy neural network simulations across multiple scenarios (e.g., 1000 trials per setup), we demonstrate that adversarial patching increases deceptive outputs to 23.9% from a 0% baseline, with layer-specific variations supporting our hypotheses. We propose six hypotheses, including transferability across models, exacerbation in multimodal settings, and scaling effects. An expanded literature review synthesizes over 20 key works in interpretability, deception, and adversarial attacks. Mitigation strategies, such as activation anomaly detection and robust fine-tuning, are detailed, alongside ethical considerations and future research directions. This work advances AI safety by highlighting patching's dual-use potential and provides a roadmap for empirical studies on large-scale models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 66,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09428",
        "abs_url": "https://arxiv.org/abs/2507.09428",
        "pdf_url": "https://arxiv.org/pdf/2507.09428",
        "title": "On Information Geometry and Iterative Optimization in Model Compression: Operator Factorization",
        "authors": [
            "Zakhar Shumaylov",
            "Vasileios Tsiaras",
            "Yannis Stylianou"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Differential Geometry (math.DG); Optimization and Control (math.OC)",
        "abstract": "The ever-increasing parameter counts of deep learning models necessitate effective compression techniques for deployment on resource-constrained devices. This paper explores the application of information geometry, the study of density-induced metrics on parameter spaces, to analyze existing methods within the space of model compression, primarily focusing on operator factorization. Adopting this perspective highlights the core challenge: defining an optimal low-compute submanifold (or subset) and projecting onto it. We argue that many successful model compression approaches can be understood as implicitly approximating information divergences for this projection. We highlight that when compressing a pre-trained model, using information divergences is paramount for achieving improved zero-shot accuracy, yet this may no longer be the case when the model is fine-tuned. In such scenarios, trainability of bottlenecked models turns out to be far more important for achieving high compression ratios with minimal performance degradation, necessitating adoption of iterative methods. In this context, we prove convergence of iterative singular value thresholding for training neural networks subject to a soft rank constraint. To further illustrate the utility of this perspective, we showcase how simple modifications to existing methods through softer rank reduction result in improved performance under fixed compression rates.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 67,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09439",
        "abs_url": "https://arxiv.org/abs/2507.09439",
        "pdf_url": "https://arxiv.org/pdf/2507.09439",
        "title": "Dynamic Sparse Causal-Attention Temporal Networks for Interpretable Causality Discovery in Multivariate Time Series",
        "authors": [
            "Meriem Zerkouk",
            "Miloud Mihoubi",
            "Belkacem Chikhaoui"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)",
        "abstract": "Understanding causal relationships in multivariate time series (MTS) is essential for effective decision-making in fields such as finance and marketing, where complex dependencies and lagged effects challenge conventional analytical approaches. We introduce Dynamic Sparse Causal-Attention Temporal Networks for Interpretable Causality Discovery in MTS (DyCAST-Net), a novel architecture designed to enhance causal discovery by integrating dilated temporal convolutions and dynamic sparse attention mechanisms. DyCAST-Net effectively captures multiscale temporal dependencies through dilated convolutions while leveraging an adaptive thresholding strategy in its attention mechanism to eliminate spurious connections, ensuring both accuracy and interpretability. A statistical shuffle test validation further strengthens robustness by filtering false positives and improving causal inference reliability. Extensive evaluations on financial and marketing datasets demonstrate that DyCAST-Net consistently outperforms existing models such as TCDF, GCFormer, and CausalFormer. The model provides a more precise estimation of causal delays and significantly reduces false discoveries, particularly in noisy environments. Moreover, attention heatmaps offer interpretable insights, uncovering hidden causal patterns such as the mediated effects of advertising on consumer behavior and the influence of macroeconomic indicators on financial markets. Case studies illustrate DyCAST-Net's ability to detect latent mediators and lagged causal factors, making it particularly effective in high-dimensional, dynamic settings. The model's architecture enhanced by RMSNorm stabilization and causal masking ensures scalability and adaptability across diverse application domains",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 68,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09440",
        "abs_url": "https://arxiv.org/abs/2507.09440",
        "pdf_url": "https://arxiv.org/pdf/2507.09440",
        "title": "Transformers Don't In-Context Learn Least Squares Regression",
        "authors": [
            "Joshua Hill",
            "Benjamin Eyre",
            "Elliot Creager"
        ],
        "comments": "21 pages, 16 figures, ICML 2025 Workshop on Reliable and Responsible Foundation Models",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "In-context learning (ICL) has emerged as a powerful capability of large pretrained transformers, enabling them to solve new tasks implicit in example input-output pairs without any gradient updates. Despite its practical success, the mechanisms underlying ICL remain largely mysterious. In this work we study synthetic linear regression to probe how transformers implement learning at inference time. Previous works have demonstrated that transformers match the performance of learning rules such as Ordinary Least Squares (OLS) regression or gradient descent and have suggested ICL is facilitated in transformers through the learned implementation of one of these techniques. In this work, we demonstrate through a suite of out-of-distribution generalization experiments that transformers trained for ICL fail to generalize after shifts in the prompt distribution, a behaviour that is inconsistent with the notion of transformers implementing algorithms such as OLS. Finally, we highlight the role of the pretraining corpus in shaping ICL behaviour through a spectral analysis of the learned representations in the residual stream. Inputs from the same distribution as the training data produce representations with a unique spectral signature: inputs from this distribution tend to have the same top two singular vectors. This spectral signature is not shared by out-of-distribution inputs, and a metric characterizing the presence of this signature is highly correlated with low loss.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 69,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09443",
        "abs_url": "https://arxiv.org/abs/2507.09443",
        "pdf_url": "https://arxiv.org/pdf/2507.09443",
        "title": "Toward Developing Machine-Learning-Aided Tools for the Thermomechanical Monitoring of Nuclear Reactor Components",
        "authors": [
            "Luiz Aldeia Machado",
            "Victor Coppo Leite",
            "Elia Merzari",
            "Arthur Motta",
            "Roberto Ponciroli",
            "Lander Ibarra",
            "Lise Charlot"
        ],
        "comments": "Preprint - Nureth 21 paper",
        "subjects": "Machine Learning (cs.LG); Computational Engineering, Finance, and Science (cs.CE)",
        "abstract": "Proactive maintenance strategies, such as Predictive Maintenance (PdM), play an important role in the operation of Nuclear Power Plants (NPPs), particularly due to their capacity to reduce offline time by preventing unexpected shutdowns caused by component failures. In this work, we explore the use of a Convolutional Neural Network (CNN) architecture combined with a computational thermomechanical model to calculate the temperature, stress, and strain of a Pressurized Water Reactor (PWR) fuel rod during operation. This estimation relies on a limited number of temperature measurements from the cladding's outer surface. This methodology can potentially aid in developing PdM tools for nuclear reactors by enabling real-time monitoring of such systems. The training, validation, and testing datasets were generated through coupled simulations involving BISON, a finite element-based nuclear fuel performance code, and the MOOSE Thermal-Hydraulics Module (MOOSE-THM). We conducted eleven simulations, varying the peak linear heat generation rates. Of these, eight were used for training, two for validation, and one for testing. The CNN was trained for over 1,000 epochs without signs of overfitting, achieving highly accurate temperature distribution predictions. These were then used in a thermomechanical model to determine the stress and strain distribution within the fuel rod.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 70,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09445",
        "abs_url": "https://arxiv.org/abs/2507.09445",
        "pdf_url": "https://arxiv.org/pdf/2507.09445",
        "title": "Fourier Basis Mapping: A Time-Frequency Learning Framework for Time Series Forecasting",
        "authors": [
            "Runze Yang",
            "Longbing Cao",
            "Xin You",
            "Kun Fang",
            "Jianxun Li",
            "Jie Yang"
        ],
        "comments": "18 pages, 6 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "The integration of Fourier transform and deep learning opens new avenues for time series forecasting. We reconsider the Fourier transform from a basis functions perspective. Specifically, the real and imaginary parts of the frequency components can be regarded as the coefficients of cosine and sine basis functions at tiered frequency levels, respectively. We find that existing Fourier-based methods face inconsistent starting cycles and inconsistent series length issues. They fail to interpret frequency components precisely and overlook temporal information. Accordingly, the novel Fourier Basis Mapping (FBM) method addresses these issues by integrating time-frequency features through Fourier basis expansion and mapping in the time-frequency space. Our approach extracts explicit frequency features while preserving temporal characteristics. FBM supports plug-and-play integration with various types of neural networks by only adjusting the first initial projection layer for better performance. First, we propose FBM-L, FBM-NL, and FBM-NP to enhance linear, MLP-based, and Transformer-based models, respectively, demonstrating the effectiveness of time-frequency features. Next, we propose a synergetic model architecture, termed FBM-S, which decomposes the seasonal, trend, and interaction effects into three separate blocks, each designed to model time-frequency features in a specialized manner. Finally, we introduce several techniques tailored for time-frequency features, including interaction masking, centralization, patching, rolling window projection, and multi-scale down-sampling. The results are validated on diverse real-world datasets for both long-term and short-term forecasting tasks with SOTA performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 71,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09460",
        "abs_url": "https://arxiv.org/abs/2507.09460",
        "pdf_url": "https://arxiv.org/pdf/2507.09460",
        "title": "Enhancing ALS Progression Tracking with Semi-Supervised ALSFRS-R Scores Estimated from Ambient Home Health Monitoring",
        "authors": [
            "Noah Marchal",
            "William E. Janes",
            "Mihail Popescu",
            "Xing Song"
        ],
        "comments": "31 pages, 8 Figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Clinical monitoring of functional decline in ALS relies on periodic assessments that may miss critical changes occurring between visits. To address this gap, semi-supervised regression models were developed to estimate rates of decline in a case series cohort by targeting ALSFRS- R scale trajectories with continuous in-home sensor monitoring data. Our analysis compared three model paradigms (individual batch learning and cohort-level batch versus incremental fine-tuned transfer learning) across linear slope, cubic polynomial, and ensembled self-attention pseudo-label interpolations. Results revealed cohort homogeneity across functional domains responding to learning methods, with transfer learning improving prediction error for ALSFRS-R subscales in 28 of 32 contrasts (mean RMSE=0.20(0.04)), and individual batch learning for predicting the composite scale (mean RMSE=3.15(1.25)) in 2 of 3. Self-attention interpolation achieved the lowest prediction error for subscale-level models (mean RMSE=0.19(0.06)), capturing complex nonlinear progression patterns, outperforming linear and cubic interpolations in 20 of 32 contrasts, though linear interpolation proved more stable in all ALSFRS-R composite scale models (mean RMSE=0.23(0.10)). We identified distinct homogeneity-heterogeneity profiles across functional domains with respiratory and speech exhibiting patient-specific patterns benefiting from personalized incremental adaptation, while swallowing and dressing functions followed cohort-level trajectories suitable for transfer models. These findings suggest that matching learning and pseudo-labeling techniques to functional domain-specific homogeneity-heterogeneity profiles enhances predictive accuracy in ALS progression tracking. Integrating adaptive model selection within sensor monitoring platforms could enable timely interventions and scalable deployment in future multi-center studies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 72,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09466",
        "abs_url": "https://arxiv.org/abs/2507.09466",
        "pdf_url": "https://arxiv.org/pdf/2507.09466",
        "title": "La-Proteina: Atomistic Protein Generation via Partially Latent Flow Matching",
        "authors": [
            "Tomas Geffner",
            "Kieran Didi",
            "Zhonglin Cao",
            "Danny Reidenbach",
            "Zuobai Zhang",
            "Christian Dallago",
            "Emine Kucukbenli",
            "Karsten Kreis",
            "Arash Vahdat"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)",
        "abstract": "Recently, many generative models for de novo protein structure design have emerged. Yet, only few tackle the difficult task of directly generating fully atomistic structures jointly with the underlying amino acid sequence. This is challenging, for instance, because the model must reason over side chains that change in length during generation. We introduce La-Proteina for atomistic protein design based on a novel partially latent protein representation: coarse backbone structure is modeled explicitly, while sequence and atomistic details are captured via per-residue latent variables of fixed dimensionality, thereby effectively side-stepping challenges of explicit side-chain representations. Flow matching in this partially latent space then models the joint distribution over sequences and full-atom structures. La-Proteina achieves state-of-the-art performance on multiple generation benchmarks, including all-atom co-designability, diversity, and structural validity, as confirmed through detailed structural analyses and evaluations. Notably, La-Proteina also surpasses previous models in atomistic motif scaffolding performance, unlocking critical atomistic structure-conditioned protein design tasks. Moreover, La-Proteina is able to generate co-designable proteins of up to 800 residues, a regime where most baselines collapse and fail to produce valid samples, demonstrating La-Proteina's scalability and robustness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 73,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09480",
        "abs_url": "https://arxiv.org/abs/2507.09480",
        "pdf_url": "https://arxiv.org/pdf/2507.09480",
        "title": "Discrete Differential Principle for Continuous Smooth Function Representation",
        "authors": [
            "Guoyou Wang",
            "Yihua Tan",
            "Shiqi Liu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Numerical Analysis (math.NA)",
        "abstract": "Taylor's formula holds significant importance in function representation, such as solving differential difference equations, ordinary differential equations, partial differential equations, and further promotes applications in visual perception, complex control, fluid mechanics, weather forecasting and thermodynamics. However, the Taylor's formula suffers from the curse of dimensionality and error propagation during derivative computation in discrete situations. In this paper, we propose a new discrete differential operator to estimate derivatives and to represent continuous smooth function locally using the Vandermonde coefficient matrix derived from truncated Taylor series. Our method simultaneously computes all derivatives of orders less than the number of sample points, inherently mitigating error propagation. Utilizing equidistant uniform sampling, it achieves high-order accuracy while alleviating the curse of dimensionality. We mathematically establish rigorous error bounds for both derivative estimation and function representation, demonstrating tighter bounds for lower-order derivatives. We extend our method to the two-dimensional case, enabling its use for multivariate derivative calculations. Experiments demonstrate the effectiveness and superiority of the proposed method compared to the finite forward difference method for derivative estimation and cubic spline and linear interpolation for function representation. Consequently, our technique offers broad applicability across domains such as vision representation, feature extraction, fluid mechanics, and cross-media imaging.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 74,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09523",
        "abs_url": "https://arxiv.org/abs/2507.09523",
        "pdf_url": "https://arxiv.org/pdf/2507.09523",
        "title": "An Analysis of Action-Value Temporal-Difference Methods That Learn State Values",
        "authors": [
            "Brett Daley",
            "Prabhat Nagarajan",
            "Martha White",
            "Marlos C. Machado"
        ],
        "comments": "Published at RLC/RLJ 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "The hallmark feature of temporal-difference (TD) learning is bootstrapping: using value predictions to generate new value predictions. The vast majority of TD methods for control learn a policy by bootstrapping from a single action-value function (e.g., Q-learning and Sarsa). Significantly less attention has been given to methods that bootstrap from two asymmetric value functions: i.e., methods that learn state values as an intermediate step in learning action values. Existing algorithms in this vein can be categorized as either QV-learning or AV-learning. Though these algorithms have been investigated to some degree in prior work, it remains unclear if and when it is advantageous to learn two value functions instead of just one -- and whether such approaches are theoretically sound in general. In this paper, we analyze these algorithmic families in terms of convergence and sample efficiency. We find that while both families are more efficient than Expected Sarsa in the prediction setting, only AV-learning methods offer any major benefit over Q-learning in the control setting. Finally, we introduce a new AV-learning algorithm called Regularized Dueling Q-learning (RDQ), which significantly outperforms Dueling DQN in the MinAtar benchmark.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 75,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09545",
        "abs_url": "https://arxiv.org/abs/2507.09545",
        "pdf_url": "https://arxiv.org/pdf/2507.09545",
        "title": "Assessing reliability of explanations in unbalanced datasets: a use-case on the occurrence of frost events",
        "authors": [
            "Ilaria Vascotto",
            "Valentina Blasone",
            "Alex Rodriguez",
            "Alessandro Bonaita",
            "Luca Bortolussi"
        ],
        "comments": "Late Breaking Work presented at the 3rd World Conference on eXplainable Artificial Intelligence (XAI2025)",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The usage of eXplainable Artificial Intelligence (XAI) methods has become essential in practical applications, given the increasing deployment of Artificial Intelligence (AI) models and the legislative requirements put forward in the latest years. A fundamental but often underestimated aspect of the explanations is their robustness, a key property that should be satisfied in order to trust the explanations. In this study, we provide some preliminary insights on evaluating the reliability of explanations in the specific case of unbalanced datasets, which are very frequent in high-risk use-cases, but at the same time considerably challenging for both AI models and XAI methods. We propose a simple evaluation focused on the minority class (i.e. the less frequent one) that leverages on-manifold generation of neighbours, explanation aggregation and a metric to test explanation consistency. We present a use-case based on a tabular dataset with numerical features focusing on the occurrence of frost events.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 76,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09565",
        "abs_url": "https://arxiv.org/abs/2507.09565",
        "pdf_url": "https://arxiv.org/pdf/2507.09565",
        "title": "Holistix: A Dataset for Holistic Wellness Dimensions Analysis in Mental Health Narratives",
        "authors": [
            "Heba Shakeel",
            "Tanvir Ahmad",
            "Chandni Saxena"
        ],
        "comments": "7 Pages",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We introduce a dataset for classifying wellness dimensions in social media user posts, covering six key aspects: physical, emotional, social, intellectual, spiritual, and vocational. The dataset is designed to capture these dimensions in user-generated content, with a comprehensive annotation framework developed under the guidance of domain experts. This framework allows for the classification of text spans into the appropriate wellness categories. We evaluate both traditional machine learning models and advanced transformer-based models for this multi-class classification task, with performance assessed using precision, recall, and F1-score, averaged over 10-fold cross-validation. Post-hoc explanations are applied to ensure the transparency and interpretability of model decisions. The proposed dataset contributes to region-specific wellness assessments in social media and paves the way for personalized well-being evaluations and early intervention strategies in mental health. We adhere to ethical considerations for constructing and releasing our experiments and dataset publicly on Github.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 77,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09602",
        "abs_url": "https://arxiv.org/abs/2507.09602",
        "pdf_url": "https://arxiv.org/pdf/2507.09602",
        "title": "DRAGD: A Federated Unlearning Data Reconstruction Attack Based on Gradient Differences",
        "authors": [
            "Bocheng Ju",
            "Junchao Fan",
            "Jiaqi Liu",
            "Xiaolin Chang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Federated learning enables collaborative machine learning while preserving data privacy. However, the rise of federated unlearning, designed to allow clients to erase their data from the global model, introduces new privacy concerns. Specifically, the gradient exchanges during the unlearning process can leak sensitive information about deleted data. In this paper, we introduce DRAGD, a novel attack that exploits gradient discrepancies before and after unlearning to reconstruct forgotten data. We also present DRAGDP, an enhanced version of DRAGD that leverages publicly available prior data to improve reconstruction accuracy, particularly for complex datasets like facial images. Extensive experiments across multiple datasets demonstrate that DRAGD and DRAGDP significantly outperform existing methods in data this http URL work highlights a critical privacy vulnerability in federated unlearning and offers a practical solution, advancing the security of federated unlearning systems in real-world applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 78,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09616",
        "abs_url": "https://arxiv.org/abs/2507.09616",
        "pdf_url": "https://arxiv.org/pdf/2507.09616",
        "title": "MLoRQ: Bridging Low-Rank and Quantization for Transformer Compression",
        "authors": [
            "Ofir Gordon",
            "Ariel Lapid",
            "Elad Cohen",
            "Yarden Yagil",
            "Arnon Netzer",
            "Hai Victor Habi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Deploying transformer-based neural networks on resource-constrained edge devices presents a significant challenge. This challenge is often addressed through various techniques, such as low-rank approximation and mixed-precision quantization. In this work, we introduce Mixed Low-Rank and Quantization (MLoRQ), a novel method that integrates both techniques. MLoRQ employs a two-stage optimization process to determine optimal bit-width and rank assignments for each layer, adhering to predefined memory constraints. This process includes: (i) an intra-layer optimization that identifies potentially optimal compression solutions out of all low-rank and quantization combinations; (ii) an inter-layer optimization that assigns bit-width precision and rank to each layer while ensuring the memory constraint is met. An optional final step applies a sequential optimization process using a modified adaptive rounding technique to mitigate compression-induced errors in joint low-rank approximation and quantization. The method is compatible and can be seamlessly integrated with most existing quantization algorithms. MLoRQ shows state-of-the-art results with up to 15\\% performance improvement, evaluated on Vision Transformers for image classification, object detection, and instance segmentation tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 79,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09650",
        "abs_url": "https://arxiv.org/abs/2507.09650",
        "pdf_url": "https://arxiv.org/pdf/2507.09650",
        "title": "Cultivating Pluralism In Algorithmic Monoculture: The Community Alignment Dataset",
        "authors": [
            "Lily Hong Zhang",
            "Smitha Milli",
            "Karen Jusko",
            "Jonathan Smith",
            "Brandon Amos",
            "Wassim",
            "Bouaziz",
            "Manon Revel",
            "Jack Kussman",
            "Lisa Titus",
            "Bhaktipriya Radharapu",
            "Jane Yu",
            "Vidya Sarma",
            "Kris Rose",
            "Maximilian Nickel"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "How can large language models (LLMs) serve users with varying preferences that may conflict across cultural, political, or other dimensions? To advance this challenge, this paper establishes four key results. First, we demonstrate, through a large-scale multilingual human study with representative samples from five countries (N=15,000), that humans exhibit significantly more variation in preferences than the responses of 21 state-of-the-art LLMs. Second, we show that existing methods for preference dataset collection are insufficient for learning the diversity of human preferences even along two of the most salient dimensions of variability in global values, due to the underlying homogeneity of candidate responses. Third, we argue that this motivates the need for negatively-correlated sampling when generating candidate sets, and we show that simple prompt-based techniques for doing so significantly enhance the performance of alignment methods in learning heterogeneous preferences. Fourth, based on this novel candidate sampling approach, we collect and open-source Community Alignment, the largest and most representative multilingual and multi-turn preference dataset to date, featuring almost 200,000 comparisons from annotators spanning five countries. We hope that the Community Alignment dataset will be a valuable resource for improving the effectiveness of LLMs for a diverse global population.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 80,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09678",
        "abs_url": "https://arxiv.org/abs/2507.09678",
        "pdf_url": "https://arxiv.org/pdf/2507.09678",
        "title": "Conformal Prediction for Privacy-Preserving Machine Learning",
        "authors": [
            "Alexander David Balinsky",
            "Dominik Krzeminski",
            "Alexander Balinsky"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Statistics Theory (math.ST)",
        "abstract": "We investigate the integration of Conformal Prediction (CP) with supervised learning on deterministically encrypted data, aiming to bridge the gap between rigorous uncertainty quantification and privacy-preserving machine learning. Using AES-encrypted variants of the MNIST dataset, we demonstrate that CP methods remain effective even when applied directly in the encrypted domain, owing to the preservation of data exchangeability under fixed-key encryption. We test traditional $p$-value-based against $e$-value-based conformal predictors. Our empirical evaluation reveals that models trained on deterministically encrypted data retain the ability to extract meaningful structure, achieving 36.88\\% test accuracy -- significantly above random guessing (9.56\\%) observed with per-instance encryption. Moreover, $e$-value-based CP achieves predictive set coverage of over 60\\% with 4.3 loss-threshold calibration, correctly capturing the true label in 4888 out of 5000 test cases. In contrast, the $p$-value-based CP yields smaller predictive sets but with reduced coverage accuracy. These findings highlight both the promise and limitations of CP in encrypted data settings and underscore critical trade-offs between prediction set compactness and reliability. %Our work sets a foundation for principled uncertainty quantification in secure, privacy-aware learning systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 81,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09683",
        "abs_url": "https://arxiv.org/abs/2507.09683",
        "pdf_url": "https://arxiv.org/pdf/2507.09683",
        "title": "Networked Information Aggregation via Machine Learning",
        "authors": [
            "Michael Kearns",
            "Aaron Roth",
            "Emily Ryu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Science and Game Theory (cs.GT); Theoretical Economics (econ.TH)",
        "abstract": "We study a distributed learning problem in which learning agents are embedded in a directed acyclic graph (DAG). There is a fixed and arbitrary distribution over feature/label pairs, and each agent or vertex in the graph is able to directly observe only a subset of the features -- potentially a different subset for every agent. The agents learn sequentially in some order consistent with a topological sort of the DAG, committing to a model mapping observations to predictions of the real-valued label. Each agent observes the predictions of their parents in the DAG, and trains their model using both the features of the instance that they directly observe, and the predictions of their parents as additional features. We ask when this process is sufficient to achieve \\emph{information aggregation}, in the sense that some agent in the DAG is able to learn a model whose error is competitive with the best model that could have been learned (in some hypothesis class) with direct access to \\emph{all} features, despite the fact that no single agent in the network has such access. We give upper and lower bounds for this problem for both linear and general hypothesis classes. Our results identify the \\emph{depth} of the DAG as the key parameter: information aggregation can occur over sufficiently long paths in the DAG, assuming that all of the relevant features are well represented along the path, and there are distributions over which information aggregation cannot occur even in the linear case, and even in arbitrarily large DAGs that do not have sufficient depth (such as a hub-and-spokes topology in which the spoke vertices collectively see all the features). We complement our theoretical results with a comprehensive set of experiments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 82,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09687",
        "abs_url": "https://arxiv.org/abs/2507.09687",
        "pdf_url": "https://arxiv.org/pdf/2507.09687",
        "title": "Post-Training Quantization of Generative and Discriminative LSTM Text Classifiers: A Study of Calibration, Class Balance, and Robustness",
        "authors": [
            "Md Mushfiqur Rahaman",
            "Elliot Chang",
            "Tasmiah Haque",
            "Srinjoy Das"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Text classification plays a pivotal role in edge computing applications like industrial monitoring, health diagnostics, and smart assistants, where low latency and high accuracy are both key requirements. Generative classifiers, in particular, have been shown to exhibit robustness to out-of-distribution and noisy data, which is an extremely critical consideration for deployment in such real-time edge environments. However, deploying such models on edge devices faces computational and memory constraints. Post Training Quantization (PTQ) reduces model size and compute costs without retraining, making it ideal for edge deployment. In this work, we present a comprehensive comparative study of generative and discriminative Long Short Term Memory (LSTM)-based text classification models with PTQ using the Brevitas quantization library. We evaluate both types of classifier models across multiple bitwidths and assess their robustness under regular and noisy input conditions. We find that while discriminative classifiers remain robust, generative ones are more sensitive to bitwidth, calibration data used during PTQ, and input noise during quantized inference. We study the influence of class imbalance in calibration data for both types of classifiers, comparing scenarios with evenly and unevenly distributed class samples including their effect on weight adjustments and activation profiles during PTQ. Using test statistics derived from nonparametric hypothesis testing, we identify that using class imbalanced data during calibration introduces insufficient weight adaptation at lower bitwidths for generative LSTM classifiers, thereby leading to degraded performance. This study underscores the role of calibration data in PTQ and when generative classifiers succeed or fail under noise, aiding deployment in edge environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 83,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09694",
        "abs_url": "https://arxiv.org/abs/2507.09694",
        "pdf_url": "https://arxiv.org/pdf/2507.09694",
        "title": "Frequency-aware Surrogate Modeling With SMT Kernels For Advanced Data Forecasting",
        "authors": [
            "Nicolas Gonel",
            "Paul Saves",
            "Joseph Morlier"
        ],
        "comments": "AeroBest 2025, Instituto Superior Tecnico of the University of Lisbon, Portugal",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Optimization and Control (math.OC); Machine Learning (stat.ML)",
        "abstract": "This paper introduces a comprehensive open-source framework for developing correlation kernels, with a particular focus on user-defined and composition of kernels for surrogate modeling. By advancing kernel-based modeling techniques, we incorporate frequency-aware elements that effectively capture complex mechanical behaviors and timefrequency dynamics intrinsic to aircraft systems. Traditional kernel functions, often limited to exponential-based methods, are extended to include a wider range of kernels such as exponential squared sine and rational quadratic kernels, along with their respective firstand second-order derivatives. The proposed methodologies are first validated on a sinus cardinal test case and then applied to forecasting Mauna-Loa Carbon Dioxide (CO 2 ) concentrations and airline passenger traffic. All these advancements are integrated into the open-source Surrogate Modeling Toolbox (SMT 2.0), providing a versatile platform for both standard and customizable kernel configurations. Furthermore, the framework enables the combination of various kernels to leverage their unique strengths into composite models tailored to specific problems. The resulting framework offers a flexible toolset for engineers and researchers, paving the way for numerous future applications in metamodeling for complex, frequency-sensitive domains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 84,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09703",
        "abs_url": "https://arxiv.org/abs/2507.09703",
        "pdf_url": "https://arxiv.org/pdf/2507.09703",
        "title": "EPT-2 Technical Report",
        "authors": [
            "Roberto Molinaro",
            "Niall Siegenheim",
            "Niels Poulsen",
            "Jordan Dane Daubinet",
            "Henry Martin",
            "Mark Frey",
            "Kevin Thiart",
            "Alexander Jakob Dautel",
            "Andreas Schlueter",
            "Alex Grigoryev",
            "Bogdan Danciu",
            "Nikoo Ekhtiari",
            "Bas Steunebrink",
            "Leonie Wagner",
            "Marvin Vincent Gabler"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "We present EPT-2, the latest iteration in our Earth Physics Transformer (EPT) family of foundation AI models for Earth system forecasting. EPT-2 delivers substantial improvements over its predecessor, EPT-1.5, and sets a new state of the art in predicting energy-relevant variables-including 10m and 100m wind speed, 2m temperature, and surface solar radiation-across the full 0-240h forecast horizon. It consistently outperforms leading AI weather models such as Microsoft Aurora, as well as the operational numerical forecast system IFS HRES from the European Centre for Medium-Range Weather Forecasts (ECMWF). In parallel, we introduce a perturbation-based ensemble model of EPT-2 for probabilistic forecasting, called EPT-2e. Remarkably, EPT-2e significantly surpasses the ECMWF ENS mean-long considered the gold standard for medium- to longrange forecasting-while operating at a fraction of the computational cost. EPT models, as well as third-party forecasts, are accessible via the this http URL platform.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 85,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09732",
        "abs_url": "https://arxiv.org/abs/2507.09732",
        "pdf_url": "https://arxiv.org/pdf/2507.09732",
        "title": "Continental scale habitat modelling with artificial intelligence and multimodal earth observation",
        "authors": [
            "Sara Si-Moussi",
            "Stephan Hennekens",
            "Sander Mucher",
            "Stan Los",
            "Wilfried Thuiller"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Populations and Evolution (q-bio.PE); Applications (stat.AP)",
        "abstract": "Habitats integrate the abiotic conditions and biophysical structures that support biodiversity and sustain nature's contributions to people. As these ecosystems face mounting pressure from human activities, accurate, high-resolution habitat maps are essential for effective conservation and restoration. Yet current maps often fall short in thematic or spatial resolution because they must (1) model several mutually exclusive habitat types that co-occur across landscapes and (2) cope with severe class imbalance that complicate multi-class training. Here, we evaluated how high-resolution remote sensing (RS) data and Artificial Intelligence (AI) tools can improve habitat classification over large geographic extents at fine thematic resolution. Using vegetation plots from the European Vegetation Archive, we modelled Level 3 EUNIS habitats across Europe and assessed multiple modelling strategies against independent validation datasets. Strategies that exploited the hierarchical nature of habitat nomenclatures resolved classification ambiguities, especially in fragmented landscapes. Integrating multi-spectral (MSI) and synthetic aperture radar (SAR) imagery, particularly through Earth Observation Foundation models, enhanced within-formation discrimination and overall performance. Finally, ensemble machine learning that corrects class imbalance boosted accuracy further. Our methodological framework is transferable beyond Europe and adaptable to other classification systems. Future research should advance temporal modelling of dynamic habitats, extend to habitat segmentation and quality assessment, and exploit next-generation EO data paired with higher-quality in-situ observations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 86,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09733",
        "abs_url": "https://arxiv.org/abs/2507.09733",
        "pdf_url": "https://arxiv.org/pdf/2507.09733",
        "title": "Universal Physics Simulation: A Foundational Diffusion Approach",
        "authors": [
            "Bradley Camburn"
        ],
        "comments": "10 pages, 3 figures. Foundational AI model for universal physics simulation using sketch-guided diffusion transformers. Achieves SSIM > 0.8 on electromagnetic field generation without requiring a priori physics encoding",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We present the first foundational AI model for universal physics simulation that learns physical laws directly from boundary-condition data without requiring a priori equation encoding. Traditional physics-informed neural networks (PINNs) and finite-difference methods necessitate explicit mathematical formulation of governing equations, fundamentally limiting their generalizability and discovery potential. Our sketch-guided diffusion transformer approach reimagines computational physics by treating simulation as a conditional generation problem, where spatial boundary conditions guide the synthesis of physically accurate steady-state solutions. By leveraging enhanced diffusion transformer architectures with novel spatial relationship encoding, our model achieves direct boundary-to-equilibrium mapping and is generalizable to diverse physics domains. Unlike sequential time-stepping methods that accumulate errors over iterations, our approach bypasses temporal integration entirely, directly generating steady-state solutions with SSIM > 0.8 while maintaining sub-pixel boundary accuracy. Our data-informed approach enables physics discovery through learned representations analyzable via Layer-wise Relevance Propagation (LRP), revealing emergent physical relationships without predetermined mathematical constraints. This work represents a paradigm shift from AI-accelerated physics to AI-discovered physics, establishing the first truly universal physics simulation framework.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 87,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09753",
        "abs_url": "https://arxiv.org/abs/2507.09753",
        "pdf_url": "https://arxiv.org/pdf/2507.09753",
        "title": "Do we need equivariant models for molecule generation?",
        "authors": [
            "Ewa M. Nowara",
            "Joshua Rackers",
            "Patricia Suriana",
            "Pan Kessel",
            "Max Shen",
            "Andrew Martin Watkins",
            "Michael Maser"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)",
        "abstract": "Deep generative models are increasingly used for molecular discovery, with most recent approaches relying on equivariant graph neural networks (GNNs) under the assumption that explicit equivariance is essential for generating high-quality 3D molecules. However, these models are complex, difficult to train, and scale poorly. We investigate whether non-equivariant convolutional neural networks (CNNs) trained with rotation augmentations can learn equivariance and match the performance of equivariant models. We derive a loss decomposition that separates prediction error from equivariance error, and evaluate how model size, dataset size, and training duration affect performance across denoising, molecule generation, and property prediction. To our knowledge, this is the first study to analyze learned equivariance in generative tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 88,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09754",
        "abs_url": "https://arxiv.org/abs/2507.09754",
        "pdf_url": "https://arxiv.org/pdf/2507.09754",
        "title": "Explainable AI in Genomics: Transcription Factor Binding Site Prediction with Mixture of Experts",
        "authors": [
            "Aakash Tripathi",
            "Ian E. Nielsen",
            "Muhammad Umer",
            "Ravi P. Ramachandran",
            "Ghulam Rasool"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Genomics (q-bio.GN)",
        "abstract": "Transcription Factor Binding Site (TFBS) prediction is crucial for understanding gene regulation and various biological processes. This study introduces a novel Mixture of Experts (MoE) approach for TFBS prediction, integrating multiple pre-trained Convolutional Neural Network (CNN) models, each specializing in different TFBS patterns. We evaluate the performance of our MoE model against individual expert models on both in-distribution and out-of-distribution (OOD) datasets, using six randomly selected transcription factors (TFs) for OOD testing. Our results demonstrate that the MoE model achieves competitive or superior performance across diverse TF binding sites, particularly excelling in OOD scenarios. The Analysis of Variance (ANOVA) statistical test confirms the significance of these performance differences. Additionally, we introduce ShiftSmooth, a novel attribution mapping technique that provides more robust model interpretability by considering small shifts in input sequences. Through comprehensive explainability analysis, we show that ShiftSmooth offers superior attribution for motif discovery and localization compared to traditional Vanilla Gradient methods. Our work presents an efficient, generalizable, and interpretable solution for TFBS prediction, potentially enabling new discoveries in genome biology and advancing our understanding of transcriptional regulation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 89,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09766",
        "abs_url": "https://arxiv.org/abs/2507.09766",
        "pdf_url": "https://arxiv.org/pdf/2507.09766",
        "title": "Toward accurate RUL and SOH estimation using reinforced graph-based PINNs enhanced with dynamic weights",
        "authors": [
            "Mohamadreza Akbari Pour",
            "Ali Ghasemzadeh",
            "MohamadAli Bijarchi",
            "Mohammad Behshad Shafii"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Accurate estimation of Remaining Useful Life (RUL) and State of Health (SOH) is essential for Prognostics and Health Management (PHM) across a wide range of industrial applications. We propose a novel framework -- Reinforced Graph-Based Physics-Informed Neural Networks Enhanced with Dynamic Weights (RGPD) -- that combines physics-based supervision with advanced spatio-temporal learning. Graph Convolutional Recurrent Networks (GCRNs) embed graph-convolutional filters within recurrent units to capture how node representations evolve over time. Graph Attention Convolution (GATConv) leverages a self-attention mechanism to compute learnable, edge-wise attention coefficients, dynamically weighting neighbor contributions for adaptive spatial aggregation. A Soft Actor-Critic (SAC) module is positioned between the Temporal Attention Unit (TAU) and GCRN to further improve the spatio-temporal learning. This module improves attention and prediction accuracy by dynamically scaling hidden representations to minimize noise and highlight informative features. To identify the most relevant physical constraints in each area, Q-learning agents dynamically assign weights to physics-informed loss terms, improving generalization across real-time industrial systems and reducing the need for manual tuning. In both RUL and SOH estimation tasks, the proposed method consistently outperforms state-of-the-art models, demonstrating strong robustness and predictive accuracy across varied degradation patterns across three diverse industrial benchmark datasets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 90,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09768",
        "abs_url": "https://arxiv.org/abs/2507.09768",
        "pdf_url": "https://arxiv.org/pdf/2507.09768",
        "title": "Knowing When to Quit: Probabilistic Early Exits for Speech Separation",
        "authors": [
            "Kenny Falkær Olsen. Mads Østergaard",
            "Karl Ulbæk",
            "Søren Føns Nielsen",
            "Rasmus Malik Høegh Lindrup",
            "Bjørn Sand Jensen",
            "Morten Mørup"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS)",
        "abstract": "In recent years, deep learning-based single-channel speech separation has improved considerably, in large part driven by increasingly compute- and parameter-efficient neural network architectures. Most such architectures are, however, designed with a fixed compute and parameter budget, and consequently cannot scale to varying compute demands or resources, which limits their use in embedded and heterogeneous devices such as mobile phones and hearables. To enable such use-cases we design a neural network architecture for speech separation capable of early-exit, and we propose an uncertainty-aware probabilistic framework to jointly model the clean speech signal and error variance which we use to derive probabilistic early-exit conditions in terms of desired signal-to-noise ratios. We evaluate our methods on both speech separation and enhancement tasks, and we show that a single early-exit model can be competitive with state-of-the-art models trained at many compute and parameter budgets. Our framework enables fine-grained dynamic compute-scaling of speech separation networks while achieving state-of-the-art performance and interpretable exit conditions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 91,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09785",
        "abs_url": "https://arxiv.org/abs/2507.09785",
        "pdf_url": "https://arxiv.org/pdf/2507.09785",
        "title": "Efficient Molecular Conformer Generation with SO(3)-Averaged Flow Matching and Reflow",
        "authors": [
            "Zhonglin Cao",
            "Mario Geiger",
            "Allan dos Santos Costa",
            "Danny Reidenbach",
            "Karsten Kreis",
            "Tomas Geffner",
            "Franco Pellegrini",
            "Guoqing Zhou",
            "Emine Kucukbenli"
        ],
        "comments": "ICML 2025 poster",
        "subjects": "Machine Learning (cs.LG); Chemical Physics (physics.chem-ph)",
        "abstract": "Fast and accurate generation of molecular conformers is desired for downstream computational chemistry and drug discovery tasks. Currently, training and sampling state-of-the-art diffusion or flow-based models for conformer generation require significant computational resources. In this work, we build upon flow-matching and propose two mechanisms for accelerating training and inference of generative models for 3D molecular conformer generation. For fast training, we introduce the SO(3)-Averaged Flow training objective, which leads to faster convergence to better generation quality compared to conditional optimal transport flow or Kabsch-aligned flow. We demonstrate that models trained using SO(3)-Averaged Flow can reach state-of-the-art conformer generation quality. For fast inference, we show that the reflow and distillation methods of flow-based models enable few-steps or even one-step molecular conformer generation with high quality. The training techniques proposed in this work show a path towards highly efficient molecular conformer generation with flow-based models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 92,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09786",
        "abs_url": "https://arxiv.org/abs/2507.09786",
        "pdf_url": "https://arxiv.org/pdf/2507.09786",
        "title": "Leveraging Distribution Matching to Make Approximate Machine Unlearning Faster",
        "authors": [
            "Junaid Iqbal Khan"
        ],
        "comments": "10 pages, 4 figures, 4 tables",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Approximate machine unlearning (AMU) enables models to `forget' specific training data through specialized fine-tuning on a retained dataset subset. However, processing this retained subset still dominates computational runtime, while reductions of epochs also remain a challenge. We propose two complementary methods to accelerate classification-oriented AMU. First, \\textbf{Blend}, a novel distribution-matching dataset condensation (DC), merges visually similar images with shared blend-weights to significantly reduce the retained set size. It operates with minimal pre-processing overhead and is orders of magnitude faster than state-of-the-art DC methods. Second, our loss-centric method, \\textbf{Accelerated-AMU (A-AMU)}, augments the unlearning objective to quicken convergence. A-AMU achieves this by combining a steepened primary loss to expedite forgetting with a novel, differentiable regularizer that matches the loss distributions of forgotten and in-distribution unseen data. Our extensive experiments demonstrate that this dual approach of data and loss-centric optimization dramatically reduces end-to-end unlearning latency across both single and multi-round scenarios, all while preserving model utility and privacy. To our knowledge, this is the first work to systematically tackle unlearning efficiency by jointly designing a specialized dataset condensation technique with a dedicated accelerated loss function. Code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 93,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09797",
        "abs_url": "https://arxiv.org/abs/2507.09797",
        "pdf_url": "https://arxiv.org/pdf/2507.09797",
        "title": "A Scalable and Efficient Signal Integration System for Job Matching",
        "authors": [
            "Ping Liu",
            "Rajat Arora",
            "Xiao Shi",
            "Benjamin Le",
            "Qianqi Shen",
            "Jianqiang Shen",
            "Chengming Jiang",
            "Nikita Zhiltsov",
            "Priya Bannur",
            "Yidan Zhu",
            "Liming Dong",
            "Haichao Wei",
            "Qi Guo",
            "Luke Simon",
            "Liangjie Hong",
            "Wenjing Zhang"
        ],
        "comments": "KDD2025",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "LinkedIn, one of the world's largest platforms for professional networking and job seeking, encounters various modeling challenges in building recommendation systems for its job matching product, including cold-start, filter bubbles, and biases affecting candidate-job matching. To address these, we developed the STAR (Signal Integration for Talent And Recruiters) system, leveraging the combined strengths of Large Language Models (LLMs) and Graph Neural Networks (GNNs). LLMs excel at understanding textual data, such as member profiles and job postings, while GNNs capture intricate relationships and mitigate cold-start issues through network effects. STAR integrates diverse signals by uniting LLM and GNN capabilities with industrial-scale paradigms including adaptive sampling and version management. It provides an end-to-end solution for developing and deploying embeddings in large-scale recommender systems. Our key contributions include a robust methodology for building embeddings in industrial applications, a scalable GNN-LLM integration for high-performing recommendations, and practical insights for real-world model deployment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 94,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09805",
        "abs_url": "https://arxiv.org/abs/2507.09805",
        "pdf_url": "https://arxiv.org/pdf/2507.09805",
        "title": "Federated Learning with Graph-Based Aggregation for Traffic Forecasting",
        "authors": [
            "Audri Banik",
            "Glaucio Haroldo Silva de Carvalho",
            "Renata Dividino"
        ],
        "comments": "Accepted at FedKDD 2025: International Joint Workshop on Federated Learning for Data Mining and Graph Analytics. 6 pages, 1 figure",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "In traffic prediction, the goal is to estimate traffic speed or flow in specific regions or road segments using historical data collected by devices deployed in each area. Each region or road segment can be viewed as an individual client that measures local traffic flow, making Federated Learning (FL) a suitable approach for collaboratively training models without sharing raw data. In centralized FL, a central server collects and aggregates model updates from multiple clients to build a shared model while preserving each client's data privacy. Standard FL methods, such as Federated Averaging (FedAvg), assume that clients are independent, which can limit performance in traffic prediction tasks where spatial relationships between clients are important. Federated Graph Learning methods can capture these dependencies during server-side aggregation, but they often introduce significant computational overhead. In this paper, we propose a lightweight graph-aware FL approach that blends the simplicity of FedAvg with key ideas from graph learning. Rather than training full models, our method applies basic neighbourhood aggregation principles to guide parameter updates, weighting client models based on graph connectivity. This approach captures spatial relationships effectively while remaining computationally efficient. We evaluate our method on two benchmark traffic datasets, METR-LA and PEMS-BAY, and show that it achieves competitive performance compared to standard baselines and recent graph-based federated learning techniques.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 95,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09816",
        "abs_url": "https://arxiv.org/abs/2507.09816",
        "pdf_url": "https://arxiv.org/pdf/2507.09816",
        "title": "Compressed Computation: Dense Circuits in a Toy Model of the Universal-AND Problem",
        "authors": [
            "Adam Newgas"
        ],
        "comments": "9 pages, 9 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Neural networks are capable of superposition -- representing more features than there are dimensions. Recent work considers the analogous concept for computation instead of storage, proposing theoretical constructions. But there has been little investigation into whether these circuits can be learned in practice. In this work, we investigate a toy model for the Universal-AND problem which computes the AND of all $m\\choose 2$ pairs of $m$ sparse inputs. The hidden dimension that determines the number of non-linear activations is restricted to pressure the model to find a compute-efficient circuit, called compressed computation. We find that the training process finds a simple solution that does not correspond to theoretical constructions. It is fully dense -- every neuron contributes to every output. The solution circuit naturally scales with dimension, trading off error rates for neuron efficiency. It is similarly robust to changes in sparsity and other key parameters, and extends naturally to other boolean operations and boolean circuits. We explain the found solution in detail and compute why it is more efficient than the theoretical constructions at low sparsity. Our findings shed light on the types of circuits that models like to form and the flexibility of the superposition representation. This contributes to a broader understanding of network circuitry and interpretability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 96,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09826",
        "abs_url": "https://arxiv.org/abs/2507.09826",
        "pdf_url": "https://arxiv.org/pdf/2507.09826",
        "title": "Bridging Neural Networks and Dynamic Time Warping for Adaptive Time Series Classification",
        "authors": [
            "Jintao Qu",
            "Zichong Wang",
            "Chenhao Wu",
            "Wenbin Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Neural networks have achieved remarkable success in time series classification, but their reliance on large amounts of labeled data for training limits their applicability in cold-start scenarios. Moreover, they lack interpretability, reducing transparency in decision-making. In contrast, dynamic time warping (DTW) combined with a nearest neighbor classifier is widely used for its effectiveness in limited-data settings and its inherent interpretability. However, as a non-parametric method, it is not trainable and cannot leverage large amounts of labeled data, making it less effective than neural networks in rich-resource scenarios. In this work, we aim to develop a versatile model that adapts to cold-start conditions and becomes trainable with labeled data, while maintaining interpretability. We propose a dynamic length-shortening algorithm that transforms time series into prototypes while preserving key structural patterns, thereby enabling the reformulation of the DTW recurrence relation into an equivalent recurrent neural network. Based on this, we construct a trainable model that mimics DTW's alignment behavior. As a neural network, it becomes trainable when sufficient labeled data is available, while still retaining DTW's inherent interpretability. We apply the model to several benchmark time series classification tasks and observe that it significantly outperforms previous approaches in low-resource settings and remains competitive in rich-resource settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 97,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09831",
        "abs_url": "https://arxiv.org/abs/2507.09831",
        "pdf_url": "https://arxiv.org/pdf/2507.09831",
        "title": "Generative Cognitive Diagnosis",
        "authors": [
            "Jiatong Li",
            "Qi Liu",
            "Mengxiao Zhu"
        ],
        "comments": "Preprint; 15 pages, 12 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)",
        "abstract": "Cognitive diagnosis (CD) models latent cognitive states of human learners by analyzing their response patterns on diagnostic tests, serving as a crucial machine learning technique for educational assessment and evaluation. Traditional cognitive diagnosis models typically follow a transductive prediction paradigm that optimizes parameters to fit response scores and extract learner abilities. These approaches face significant limitations as they cannot perform instant diagnosis for new learners without computationally expensive retraining and produce diagnostic outputs with limited reliability. In this study, we introduces a novel generative diagnosis paradigm that fundamentally shifts CD from predictive to generative modeling, enabling inductive inference of cognitive states without parameter re-optimization. We propose two simple yet effective instantiations of this paradigm: Generative Item Response Theory (G-IRT) and Generative Neural Cognitive Diagnosis Model (G-NCDM), which achieve excellent performance improvements over traditional methods. The generative approach disentangles cognitive state inference from response prediction through a well-designed generation process that incorporates identifiability and monotonicity conditions. Extensive experiments on real-world datasets demonstrate the effectiveness of our methodology in addressing scalability and reliability challenges, especially $\\times 100$ speedup for the diagnosis of new learners. Our framework opens new avenues for cognitive diagnosis applications in artificial intelligence, particularly for intelligent model evaluation and intelligent education systems. The code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 98,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09837",
        "abs_url": "https://arxiv.org/abs/2507.09837",
        "pdf_url": "https://arxiv.org/pdf/2507.09837",
        "title": "A Pre-training Framework for Relational Data with Information-theoretic Principles",
        "authors": [
            "Quang Truong",
            "Zhikai Chen",
            "Mingxuan Ju",
            "Tong Zhao",
            "Neil Shah",
            "Jiliang Tang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Relational databases underpin critical infrastructure across a wide range of domains, yet the design of generalizable pre-training strategies for learning from relational databases remains an open challenge due to task heterogeneity. Specifically, there exist infinitely many possible downstream tasks, as tasks are defined based on relational schema graphs, temporal dependencies, and SQL-defined label logics. An effective pre-training framework is desired to take these factors into account in order to obtain task-aware representations. By incorporating knowledge of the underlying distribution that drives label generation, downstream tasks can benefit from relevant side-channel information. To bridge this gap, we introduce Task Vector Estimation (TVE), a novel pre-training framework that constructs predictive supervisory signals via set-based aggregation over schema traversal graphs, explicitly modeling next-window relational dynamics. We formalize our approach through an information-theoretic lens, demonstrating that task-informed representations retain more relevant signals than those obtained without task priors. Extensive experiments on the RelBench benchmark show that TVE consistently outperforms traditional pre-training baselines. Our findings advocate for pre-training objectives that encode task heterogeneity and temporal structure as design principles for predictive modeling on relational databases.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 99,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09839",
        "abs_url": "https://arxiv.org/abs/2507.09839",
        "pdf_url": "https://arxiv.org/pdf/2507.09839",
        "title": "Rethinking Prompt Optimization: Reinforcement, Diversification, and Migration in Blackbox LLMs",
        "authors": [
            "MohammadReza Davari",
            "Utkarsh Garg",
            "Weixin Cai",
            "Eugene Belilovsky"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "An increasing number of NLP applications interact with large language models (LLMs) through black-box APIs, making prompt engineering critical for controlling model outputs. While recent Automatic Prompt Optimization (APO) methods iteratively refine prompts using model-generated feedback, textual gradients, they primarily focus on error correction and neglect valuable insights from correct predictions. This limits both their effectiveness and efficiency. In this paper, we propose a novel APO framework centered on enhancing the feedback mechanism. We reinterpret the textual gradient as a form of negative reinforcement and introduce the complementary positive reinforcement to explicitly preserve beneficial prompt components identified through successful predictions. To mitigate the noise inherent in LLM-generated feedback, we introduce a technique called feedback diversification, which aggregates multiple feedback signals, emphasizing consistent, actionable advice while filtering out outliers. Motivated by the rapid evolution and diversity of available LLMs, we also formalize Continual Prompt Optimization (CPO), addressing the practical challenge of efficiently migrating optimized prompts between different model versions or API providers. Our experiments reveal that naive prompt migration often degrades performance due to loss of critical instructions. In contrast, our approach consistently outperforms strong baselines, achieving significant accuracy improvements, faster convergence, and lower computational costs in both standard and migration scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 100,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09846",
        "abs_url": "https://arxiv.org/abs/2507.09846",
        "pdf_url": "https://arxiv.org/pdf/2507.09846",
        "title": "Through the River: Understanding the Benefit of Schedule-Free Methods for Language Model Training",
        "authors": [
            "Minhak Song",
            "Beomhan Baek",
            "Kwangjun Ahn",
            "Chulhee Yun"
        ],
        "comments": "Comments would be appreciated!",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Optimization and Control (math.OC); Machine Learning (stat.ML)",
        "abstract": "As both model and dataset sizes continue to scale rapidly, conventional pretraining strategies with fixed compute budgets-such as cosine learning rate schedules-are increasingly inadequate for large-scale training. Recent alternatives, including warmup-stable-decay (WSD) schedules and weight averaging, offer greater flexibility. However, WSD relies on explicit decay phases to track progress, while weight averaging addresses this limitation at the cost of additional memory. In search of a more principled and scalable alternative, we revisit the Schedule-Free (SF) method [Defazio et al., 2024], which has shown strong empirical performance across diverse settings. We show that SF-AdamW effectively navigates the \"river\" structure of the loss landscape without decay phases or auxiliary averaging, making it particularly suitable for continuously scaling training workloads. To understand this behavior, we conduct a theoretical and empirical analysis of SF dynamics, revealing that it implicitly performs weight averaging without memory overhead. Guided by this analysis, we propose a refined variant of SF that improves robustness to momentum and performs better under large batch sizes, addressing key limitations of the original method. Together, these results establish SF as a practical, scalable, and theoretically grounded approach for language model training.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 101,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09871",
        "abs_url": "https://arxiv.org/abs/2507.09871",
        "pdf_url": "https://arxiv.org/pdf/2507.09871",
        "title": "Task Priors: Enhancing Model Evaluation by Considering the Entire Space of Downstream Tasks",
        "authors": [
            "Niket Patel",
            "Randall Balestriero"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "The grand goal of AI research, and particularly Self Supervised Learning (SSL), is to produce systems that can successfully solve any possible task. In contrast, current evaluation methods available to AI researchers typically rely on a fixed collection of hand-picked downstream benchmarks. Hence, a large amount of effort is put into designing and searching for large collection of evaluation tasks that can serve as a proxy of our grand goal. We argue that such a rigid evaluation protocol creates a silent bottleneck in AI research. To remedy that, we define a probabilistic space of downstream tasks obtained by adopting a distribution of tasks and by defining Task Priors. Under this view, one can evaluate a model's performance over the set of all possible downstream tasks. Our framework is the first to provide answers to key questions such as (i) what is the average performance of my model over all possible downstream tasks weighted by the probability to encounter each task? or (ii) what is the variance of my model's performance across all downstream tasks under the defined Task Priors? Beyond establishing a new standard for evaluation, we believe that Task Priors will accelerate the pace of research in SSL - where downstream task evaluation is the sole qualitative signal that researchers have access to.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 102,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09882",
        "abs_url": "https://arxiv.org/abs/2507.09882",
        "pdf_url": "https://arxiv.org/pdf/2507.09882",
        "title": "AdaBrain-Bench: Benchmarking Brain Foundation Models for Brain-Computer Interface Applications",
        "authors": [
            "Jiamin Wu",
            "Zichen Ren",
            "Junyu Wang",
            "Pengyu Zhu",
            "Yonghao Song",
            "Mianxin Liu",
            "Qihao Zheng",
            "Lei Bai",
            "Wanli Ouyang",
            "Chunfeng Song"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Non-invasive Brain-Computer Interfaces (BCI) offer a safe and accessible means of connecting the human brain to external devices, with broad applications in home and clinical settings to enhance human capabilities. However, the high noise level and limited task-specific data in non-invasive signals constrain decoding capabilities. Recently, the adoption of self-supervised pre-training is transforming the landscape of non-invasive BCI research, enabling the development of brain foundation models to capture generic neural representations from large-scale unlabeled electroencephalography (EEG) signals with substantial noises. However, despite these advances, the field currently lacks comprehensive, practical and extensible benchmarks to assess the utility of the public foundation models across diverse BCI tasks, hindering their widespread adoption. To address this challenge, we present AdaBrain-Bench, a large-scale standardized benchmark to systematically evaluate brain foundation models in widespread non-invasive BCI tasks. AdaBrain-Bench encompasses a diverse collection of representative BCI decoding datasets spanning 7 key applications. It introduces a streamlined task adaptation pipeline integrated with multi-dimensional evaluation metrics and a set of adaptation tools. The benchmark delivers an inclusive framework for assessing generalizability of brain foundation models across key transfer settings, including cross-subject, multi-subject, and few-shot scenarios. We leverage AdaBrain-Bench to evaluate a suite of publicly available brain foundation models and offer insights into practices for selecting appropriate models in various scenarios. We make our benchmark pipeline available to enable reproducible research and external use, offering a continuously evolving platform to foster progress toward robust and generalized neural decoding solutions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 103,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09887",
        "abs_url": "https://arxiv.org/abs/2507.09887",
        "pdf_url": "https://arxiv.org/pdf/2507.09887",
        "title": "TolerantECG: A Foundation Model for Imperfect Electrocardiogram",
        "authors": [
            "Huynh Nguyen Dang",
            "Thang Pham",
            "Ngan Le",
            "Van Nguyen"
        ],
        "comments": "10 pages, 6 figures. Accepted to ACM Multimedia 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Signal Processing (eess.SP)",
        "abstract": "The electrocardiogram (ECG) is an essential and effective tool for diagnosing heart diseases. However, its effectiveness can be compromised by noise or unavailability of one or more leads of the standard 12-lead recordings, resulting in diagnostic errors or uncertainty. To address these challenges, we propose TolerantECG, a foundation model for ECG signals that is robust to noise and capable of functioning with arbitrary subsets of the standard 12-lead ECG. TolerantECG training combines contrastive and self-supervised learning frameworks to jointly learn ECG signal representations alongside their corresponding knowledge-retrieval-based text report descriptions and corrupted or lead-missing signals. Comprehensive benchmarking results demonstrate that TolerantECG consistently ranks as the best or second-best performer across various ECG signal conditions and class levels in the PTB-XL dataset, and achieves the highest performance on the MIT-BIH Arrhythmia Database.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 104,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09888",
        "abs_url": "https://arxiv.org/abs/2507.09888",
        "pdf_url": "https://arxiv.org/pdf/2507.09888",
        "title": "NeuTSFlow: Modeling Continuous Functions Behind Time Series Forecasting",
        "authors": [
            "Huibo Xu",
            "Likang Wu",
            "Xianquan Wang",
            "Haoning Dang",
            "Chun-Wun Cheng",
            "Angelica I Aviles-Rivero",
            "Qi Liu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "Time series forecasting is a fundamental task with broad applications, yet conventional methods often treat data as discrete sequences, overlooking their origin as noisy samples of continuous processes. Crucially, discrete noisy observations cannot uniquely determine a continuous function; instead, they correspond to a family of plausible functions. Mathematically, time series can be viewed as noisy observations of a continuous function family governed by a shared probability measure. Thus, the forecasting task can be framed as learning the transition from the historical function family to the future function family. This reframing introduces two key challenges: (1) How can we leverage discrete historical and future observations to learn the relationships between their underlying continuous functions? (2) How can we model the transition path in function space from the historical function family to the future function family? To address these challenges, we propose NeuTSFlow, a novel framework that leverages Neural Operators to facilitate flow matching for learning path of measure between historical and future function families. By parameterizing the velocity field of the flow in infinite-dimensional function spaces, NeuTSFlow moves beyond traditional methods that focus on dependencies at discrete points, directly modeling function-level features instead. Experiments on diverse forecasting tasks demonstrate NeuTSFlow's superior accuracy and robustness, validating the effectiveness of the function-family perspective.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 105,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09890",
        "abs_url": "https://arxiv.org/abs/2507.09890",
        "pdf_url": "https://arxiv.org/pdf/2507.09890",
        "title": "Soft Graph Clustering for single-cell RNA Sequencing Data",
        "authors": [
            "Ping Xu",
            "Pengfei Wang",
            "Zhiyuan Ning",
            "Meng Xiao",
            "Min Wu",
            "Yuanchun Zhou"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Genomics (q-bio.GN)",
        "abstract": "Clustering analysis is fundamental in single-cell RNA sequencing (scRNA-seq) data analysis for elucidating cellular heterogeneity and diversity. Recent graph-based scRNA-seq clustering methods, particularly graph neural networks (GNNs), have significantly improved in tackling the challenges of high-dimension, high-sparsity, and frequent dropout events that lead to ambiguous cell population boundaries. However, their reliance on hard graph constructions derived from thresholded similarity matrices presents challenges:(i) The simplification of intercellular relationships into binary edges (0 or 1) by applying thresholds, which restricts the capture of continuous similarity features among cells and leads to significant information loss.(ii) The presence of significant inter-cluster connections within hard graphs, which can confuse GNN methods that rely heavily on graph structures, potentially causing erroneous message propagation and biased clustering outcomes. To tackle these challenges, we introduce scSGC, a Soft Graph Clustering for single-cell RNA sequencing data, which aims to more accurately characterize continuous similarities among cells through non-binary edge weights, thereby mitigating the limitations of rigid data structures. The scSGC framework comprises three core components: (i) a zero-inflated negative binomial (ZINB)-based feature autoencoder; (ii) a dual-channel cut-informed soft graph embedding module; and (iii) an optimal transport-based clustering optimization module. Extensive experiments across ten datasets demonstrate that scSGC outperforms 13 state-of-the-art clustering models in clustering accuracy, cell type annotation, and computational efficiency. These results highlight its substantial potential to advance scRNA-seq data analysis and deepen our understanding of cellular heterogeneity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 106,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09897",
        "abs_url": "https://arxiv.org/abs/2507.09897",
        "pdf_url": "https://arxiv.org/pdf/2507.09897",
        "title": "Algorithm Development in Neural Networks: Insights from the Streaming Parity Task",
        "authors": [
            "Loek van Rossem",
            "Andrew M. Saxe"
        ],
        "comments": "28 pages, 20 figures",
        "subjects": "Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)",
        "abstract": "Even when massively overparameterized, deep neural networks show a remarkable ability to generalize. Research on this phenomenon has focused on generalization within distribution, via smooth interpolation. Yet in some settings neural networks also learn to extrapolate to data far beyond the bounds of the original training set, sometimes even allowing for infinite generalization, implying that an algorithm capable of solving the task has been learned. Here we undertake a case study of the learning dynamics of recurrent neural networks (RNNs) trained on the streaming parity task in order to develop an effective theory of algorithm development. The streaming parity task is a simple but nonlinear task defined on sequences up to arbitrary length. We show that, with sufficient finite training experience, RNNs exhibit a phase transition to perfect infinite generalization. Using an effective theory for the representational dynamics, we find an implicit representational merger effect which can be interpreted as the construction of a finite automaton that reproduces the task. Overall, our results disclose one mechanism by which neural networks can generalize infinitely from finite training experience.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 107,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09925",
        "abs_url": "https://arxiv.org/abs/2507.09925",
        "pdf_url": "https://arxiv.org/pdf/2507.09925",
        "title": "Extracting Cause-Effect Pairs from a Sentence with a Dependency-Aware Transformer Model",
        "authors": [
            "Md Ahsanul Kabir",
            "Abrar Jahin",
            "Mohammad Al Hasan"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Extracting cause and effect phrases from a sentence is an important NLP task, with numerous applications in various domains, including legal, medical, education, and scientific research. There are many unsupervised and supervised methods proposed for solving this task. Among these, unsupervised methods utilize various linguistic tools, including syntactic patterns, dependency tree, dependency relations, etc. among different sentential units for extracting the cause and effect phrases. On the other hand, the contemporary supervised methods use various deep learning based mask language models equipped with a token classification layer for extracting cause and effect phrases. Linguistic tools, specifically, dependency tree, which organizes a sentence into different semantic units have been shown to be very effective for extracting semantic pairs from a sentence, but existing supervised methods do not have any provision for utilizing such tools within their model framework. In this work, we propose DepBERT, which extends a transformer-based model by incorporating dependency tree of a sentence within the model framework. Extensive experiments over three datasets show that DepBERT is better than various state-of-the art supervised causality extraction methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 108,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09931",
        "abs_url": "https://arxiv.org/abs/2507.09931",
        "pdf_url": "https://arxiv.org/pdf/2507.09931",
        "title": "Mechanistic Interpretability of LoRA-Adapted Language Models for Nuclear Reactor Safety Applications",
        "authors": [
            "Yoon Pyo Lee"
        ],
        "comments": "Submitted to Nuclear Technology. 22 pages, 2 tables, 4 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "The integration of Large Language Models (LLMs) into safety-critical domains, such as nuclear engineering, necessitates a deep understanding of their internal reasoning processes. This paper presents a novel methodology for interpreting how an LLM encodes and utilizes domain-specific knowledge, using a Boiling Water Reactor system as a case study. We adapted a general-purpose LLM (Gemma-3-1b-it) to the nuclear domain using a parameter-efficient fine-tuning technique known as Low-Rank Adaptation. By comparing the neuron activation patterns of the base model to those of the fine-tuned model, we identified a sparse set of neurons whose behavior was significantly altered during the adaptation process. To probe the causal role of these specialized neurons, we employed a neuron silencing technique. Our results demonstrate that while silencing most of these specialized neurons individually did not produce a statistically significant effect, deactivating the entire group collectively led to a statistically significant degradation in task performance. Qualitative analysis further revealed that silencing these neurons impaired the model's ability to generate detailed, contextually accurate technical information. This paper provides a concrete methodology for enhancing the transparency of an opaque black-box model, allowing domain expertise to be traced to verifiable neural circuits. This offers a pathway towards achieving nuclear-grade artificial intelligence (AI) assurance, addressing the verification and validation challenges mandated by nuclear regulatory frameworks (e.g., 10 CFR 50 Appendix B), which have limited AI deployment in safety-critical nuclear operations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 109,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09937",
        "abs_url": "https://arxiv.org/abs/2507.09937",
        "pdf_url": "https://arxiv.org/pdf/2507.09937",
        "title": "Memorization Sinks: Isolating Memorization during LLM Training",
        "authors": [
            "Gaurav R. Ghosal",
            "Pratyush Maini",
            "Aditi Raghunathan"
        ],
        "comments": "Accepted at the 2025 International Conference of Machine Learning",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Large language models are susceptible to memorizing repeated sequences, posing privacy and copyright concerns. A popular mitigation strategy is to remove memorized information from specific neurons post-hoc. However, such approaches have shown limited success so far. In a controlled setting, we show that the memorization of natural sequences (those that resemble linguistically plausible text) become mechanistically entangled with general language abilities, thereby becoming challenging to remove post-hoc. In this work, we put forward a new paradigm of MemSinks that promotes isolation of memorization by design. We leverage a sequence identifier that activates a unique set of memorization neurons for each sequence across repetitions. By analyzing the dynamics of learning and forgetting, we argue that MemSinks facilitates isolation of memorized content, making it easier to remove without compromising general language capabilities. We implement MemSinks at the billion-parameter and billion-token scale, and observe both effective isolation and strong generalization. To our knowledge, this is the first proof-of-concept on real data demonstrating that simultaneous generalization and isolation is achievable. We open-source our code at this http URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 110,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09940",
        "abs_url": "https://arxiv.org/abs/2507.09940",
        "pdf_url": "https://arxiv.org/pdf/2507.09940",
        "title": "Long-Tailed Data Classification by Increasing and Decreasing Neurons During Training",
        "authors": [
            "Taigo Sakai",
            "Kazuhiro Hotta"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Applications (stat.AP)",
        "abstract": "In conventional deep learning, the number of neurons typically remains fixed during training. However, insights from biology suggest that the human hippocampus undergoes continuous neuron generation and pruning of neurons over the course of learning, implying that a flexible allocation of capacity can contribute to enhance performance. Real-world datasets often exhibit class imbalance situations where certain classes have far fewer samples than others, leading to significantly reduce recognition accuracy for minority classes when relying on fixed size this http URL address the challenge, we propose a method that periodically adds and removes neurons during training, thereby boosting representational power for minority classes. By retaining critical features learned from majority classes while selectively increasing neurons for underrepresented classes, our approach dynamically adjusts capacity during training. Importantly, while the number of neurons changes throughout training, the final network size and structure remain unchanged, ensuring efficiency and compatibility with this http URL, by experiments on three different datasets and five representative models, we demonstrate that the proposed method outperforms fixed size networks and shows even greater accuracy when combined with other imbalance-handling techniques. Our results underscore the effectiveness of dynamic, biologically inspired network designs in improving performance on class-imbalanced data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 111,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09948",
        "abs_url": "https://arxiv.org/abs/2507.09948",
        "pdf_url": "https://arxiv.org/pdf/2507.09948",
        "title": "Iceberg: Enhancing HLS Modeling with Synthetic Data",
        "authors": [
            "Zijian Ding",
            "Tung Nguyen",
            "Weikai Li",
            "Aditya Grover",
            "Yizhou Sun",
            "Jason Cong"
        ],
        "comments": "9 pages. accepted to ICLAD'25",
        "subjects": "Machine Learning (cs.LG); Hardware Architecture (cs.AR)",
        "abstract": "Deep learning-based prediction models for High-Level Synthesis (HLS) of hardware designs often struggle to generalize. In this paper, we study how to close the generalizability gap of these models through pretraining on synthetic data and introduce Iceberg, a synthetic data augmentation approach that expands both large language model (LLM)-generated programs and weak labels of unseen design configurations. Our weak label generation method is integrated with an in-context model architecture, enabling meta-learning from actual and proximate labels. Iceberg improves the geometric mean modeling accuracy by $86.4\\%$ when adapt to six real-world applications with few-shot examples and achieves a $2.47\\times$ and a $1.12\\times$ better offline DSE performance when adapting to two different test datasets. Our open-sourced code is here: \\href{this https URL}{this https URL}",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 112,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09949",
        "abs_url": "https://arxiv.org/abs/2507.09949",
        "pdf_url": "https://arxiv.org/pdf/2507.09949",
        "title": "Hierarchical Job Classification with Similarity Graph Integration",
        "authors": [
            "Md Ahsanul Kabir",
            "Kareem Abdelfatah",
            "Mohammed Korayem",
            "Mohammad Al Hasan"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In the dynamic realm of online recruitment, accurate job classification is paramount for optimizing job recommendation systems, search rankings, and labor market analyses. As job markets evolve, the increasing complexity of job titles and descriptions necessitates sophisticated models that can effectively leverage intricate relationships within job data. Traditional text classification methods often fall short, particularly due to their inability to fully utilize the hierarchical nature of industry categories. To address these limitations, we propose a novel representation learning and classification model that embeds jobs and hierarchical industry categories into a latent embedding space. Our model integrates the Standard Occupational Classification (SOC) system and an in-house hierarchical taxonomy, Carotene, to capture both graph and hierarchical relationships, thereby improving classification accuracy. By embedding hierarchical industry categories into a shared latent space, we tackle cold start issues and enhance the dynamic matching of candidates to job opportunities. Extensive experimentation on a large-scale dataset of job postings demonstrates the model's superior ability to leverage hierarchical structures and rich semantic features, significantly outperforming existing methods. This research provides a robust framework for improving job classification accuracy, supporting more informed decision-making in the recruitment industry.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 113,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09952",
        "abs_url": "https://arxiv.org/abs/2507.09952",
        "pdf_url": "https://arxiv.org/pdf/2507.09952",
        "title": "Radial Neighborhood Smoothing Recommender System",
        "authors": [
            "Zerui Zhang",
            "Yumou Qiu"
        ],
        "comments": "34 pages, 2 figures. Submitted to NeurIPS 2025",
        "subjects": "Machine Learning (cs.LG); Applications (stat.AP); Methodology (stat.ME)",
        "abstract": "Recommender systems inherently exhibit a low-rank structure in latent space. A key challenge is to define meaningful and measurable distances in the latent space to capture user-user, item-item, user-item relationships effectively. In this work, we establish that distances in the latent space can be systematically approximated using row-wise and column-wise distances in the observed matrix, providing a novel perspective on distance estimation. To refine the distance estimation, we introduce the correction based on empirical variance estimator to account for noise-induced non-centrality. The novel distance estimation enables a more structured approach to constructing neighborhoods, leading to the Radial Neighborhood Estimator (RNE), which constructs neighborhoods by including both overlapped and partially overlapped user-item pairs and employs neighborhood smoothing via localized kernel regression to improve imputation accuracy. We provide the theoretical asymptotic analysis for the proposed estimator. We perform evaluations on both simulated and real-world datasets, demonstrating that RNE achieves superior performance compared to existing collaborative filtering and matrix factorization methods. While our primary focus is on distance estimation in latent space, we find that RNE also mitigates the ``cold-start'' problem.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 114,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09958",
        "abs_url": "https://arxiv.org/abs/2507.09958",
        "pdf_url": "https://arxiv.org/pdf/2507.09958",
        "title": "Rethinking Inductive Bias in Geographically Neural Network Weighted Regression",
        "authors": [
            "Zhenyuan Chen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Inductive bias is a key factor in spatial regression models, determining how well a model can learn from limited data and capture spatial patterns. This work revisits the inductive biases in Geographically Neural Network Weighted Regression (GNNWR) and identifies limitations in current approaches for modeling spatial non-stationarity. While GNNWR extends traditional Geographically Weighted Regression by using neural networks to learn spatial weighting functions, existing implementations are often restricted by fixed distance-based schemes and limited inductive bias. We propose to generalize GNNWR by incorporating concepts from convolutional neural networks, recurrent neural networks, and transformers, introducing local receptive fields, sequential context, and self-attention into spatial regression. Through extensive benchmarking on synthetic spatial datasets with varying heterogeneity, noise, and sample sizes, we show that GNNWR outperforms classic methods in capturing nonlinear and complex spatial relationships. Our results also reveal that model performance depends strongly on data characteristics, with local models excelling in highly heterogeneous or small-sample scenarios, and global models performing better with larger, more homogeneous data. These findings highlight the importance of inductive bias in spatial modeling and suggest future directions, including learnable spatial weighting functions, hybrid neural architectures, and improved interpretability for models handling non-stationary spatial data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 115,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09961",
        "abs_url": "https://arxiv.org/abs/2507.09961",
        "pdf_url": "https://arxiv.org/pdf/2507.09961",
        "title": "Text-Driven Causal Representation Learning for Source-Free Domain Generalization",
        "authors": [
            "Lihua Zhou",
            "Mao Ye",
            "Nianxin Li",
            "Shuaifeng Li",
            "Jinlin Wu",
            "Xiatian Zhu",
            "Lei Deng",
            "Hongbin Liu",
            "Jiebo Luo",
            "Zhen Lei"
        ],
        "comments": "Under Review",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Deep learning often struggles when training and test data distributions differ. Traditional domain generalization (DG) tackles this by including data from multiple source domains, which is impractical due to expensive data collection and annotation. Recent vision-language models like CLIP enable source-free domain generalization (SFDG) by using text prompts to simulate visual representations, reducing data demands. However, existing SFDG methods struggle with domain-specific confounders, limiting their generalization capabilities. To address this issue, we propose TDCRL (\\textbf{T}ext-\\textbf{D}riven \\textbf{C}ausal \\textbf{R}epresentation \\textbf{L}earning), the first method to integrate causal inference into the SFDG setting. TDCRL operates in two steps: first, it employs data augmentation to generate style word vectors, combining them with class information to generate text embeddings to simulate visual representations; second, it trains a causal intervention network with a confounder dictionary to extract domain-invariant features. Grounded in causal learning, our approach offers a clear and effective mechanism to achieve robust, domain-invariant features, ensuring robust generalization. Extensive experiments on PACS, VLCS, OfficeHome, and DomainNet show state-of-the-art performance, proving TDCRL effectiveness in SFDG.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 116,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09968",
        "abs_url": "https://arxiv.org/abs/2507.09968",
        "pdf_url": "https://arxiv.org/pdf/2507.09968",
        "title": "Compliance Minimization via Physics-Informed Gaussian Processes",
        "authors": [
            "Xiangyu Sun",
            "Amin Yousefpour",
            "Shirin Hosseinmardi",
            "Ramin Bostanabad"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Machine learning (ML) techniques have recently gained significant attention for solving compliance minimization (CM) problems. However, these methods typically provide poor feature boundaries, are very expensive, and lack a systematic mechanism to control the design complexity. Herein, we address these limitations by proposing a mesh-free and simultaneous framework based on physics-informed Gaussian processes (GPs). In our approach, we parameterize the design and state variables with GP priors which have independent kernels but share a multi-output neural network (NN) as their mean function. The architecture of this NN is based on Parametric Grid Convolutional Attention Networks (PGCANs) which not only mitigate spectral bias issues, but also provide an interpretable mechanism to control design complexity. We estimate all the parameters of our GP-based representations by simultaneously minimizing the compliance, total potential energy, and residual of volume fraction constraint. Importantly, our loss function exclude all data-based residuals as GPs automatically satisfy them. We also develop computational schemes based on curriculum training and numerical integration to increase the efficiency and robustness of our approach which is shown to (1) produce super-resolution topologies with fast convergence, (2) achieve smaller compliance and less gray area fraction compared to traditional numerical methods, (3) provide control over fine-scale features, and (4) outperform competing ML-based methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 117,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10005",
        "abs_url": "https://arxiv.org/abs/2507.10005",
        "pdf_url": "https://arxiv.org/pdf/2507.10005",
        "title": "Effects of structural properties of neural networks on machine learning performance",
        "authors": [
            "Yash Arya",
            "Sang Hoon Lee"
        ],
        "comments": "9 pages, 6 figures",
        "subjects": "Machine Learning (cs.LG); Statistical Mechanics (cond-mat.stat-mech); Neural and Evolutionary Computing (cs.NE); Computational Physics (physics.comp-ph)",
        "abstract": "In recent years, graph-based machine learning techniques, such as reinforcement learning and graph neural networks, have garnered significant attention. While some recent studies have started to explore the relationship between the graph structure of neural networks and their predictive performance, they often limit themselves to a narrow range of model networks, particularly lacking mesoscale structures such as communities. Our work advances this area by conducting a more comprehensive investigation, incorporating realistic network structures characterized by heterogeneous degree distributions and community structures, which are typical characteristics of many real networks. These community structures offer a nuanced perspective on network architecture. Our analysis employs model networks such as random and scale-free networks, alongside a comparison with a biological neural network and its subsets for more detailed analysis. We examine the impact of these structural attributes on the performance of image classification tasks. Our findings reveal that structural properties do affect performance to some extent. Specifically, networks featuring coherent, densely interconnected communities demonstrate enhanced learning capabilities. The comparison with the biological neural network emphasizes the relevance of our findings to real-world structures, suggesting an intriguing connection worth further exploration. This study contributes meaningfully to network science and machine learning, providing insights that could inspire the design of more biologically informed neural networks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 118,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10014",
        "abs_url": "https://arxiv.org/abs/2507.10014",
        "pdf_url": "https://arxiv.org/pdf/2507.10014",
        "title": "Forecasting Coccidioidomycosis (Valley Fever) in Arizona: A Graph Neural Network Approach",
        "authors": [
            "Ali Sarabi",
            "Arash Sarabi",
            "Hao Yan",
            "Beckett Sterner",
            "Petar Jevtić"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Coccidioidomycosis, commonly known as Valley Fever, remains a significant public health concern in endemic regions of the southwestern United States. This study develops the first graph neural network (GNN) model for forecasting Valley Fever incidence in Arizona. The model integrates surveillance case data with environmental predictors using graph structures, including soil conditions, atmospheric variables, agricultural indicators, and air quality metrics. Our approach explores correlation-based relationships among variables influencing disease transmission. The model captures critical delays in disease progression through lagged effects, enhancing its capacity to reflect complex temporal dependencies in disease ecology. Results demonstrate that the GNN architecture effectively models Valley Fever trends and provides insights into key environmental drivers of disease incidence. These findings can inform early warning systems and guide resource allocation for disease prevention efforts in high-risk areas.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 119,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10039",
        "abs_url": "https://arxiv.org/abs/2507.10039",
        "pdf_url": "https://arxiv.org/pdf/2507.10039",
        "title": "Towards Applying Large Language Models to Complement Single-Cell Foundation Models",
        "authors": [
            "Steven Palayew",
            "Bo Wang",
            "Gary Bader"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Genomics (q-bio.GN)",
        "abstract": "Single-cell foundation models such as scGPT represent a significant advancement in single-cell omics, with an ability to achieve state-of-the-art performance on various downstream biological tasks. However, these models are inherently limited in that a vast amount of information in biology exists as text, which they are unable to leverage. There have therefore been several recent works that propose the use of LLMs as an alternative to single-cell foundation models, achieving competitive results. However, there is little understanding of what factors drive this performance, along with a strong focus on using LLMs as an alternative, rather than complementary approach to single-cell foundation models. In this study, we therefore investigate what biological insights contribute toward the performance of LLMs when applied to single-cell data, and introduce scMPT; a model which leverages synergies between scGPT, and single-cell representations from LLMs that capture these insights. scMPT demonstrates stronger, more consistent performance than either of its component models, which frequently have large performance gaps between each other across datasets. We also experiment with alternate fusion methods, demonstrating the potential of combining specialized reasoning models with scGPT to improve performance. This study ultimately showcases the potential for LLMs to complement single-cell foundation models and drive improvements in single-cell analysis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 120,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10048",
        "abs_url": "https://arxiv.org/abs/2507.10048",
        "pdf_url": "https://arxiv.org/pdf/2507.10048",
        "title": "On the Efficiency of Training Robust Decision Trees",
        "authors": [
            "Benedict Gerlach",
            "Marie Anastacio",
            "Holger H. Hoos"
        ],
        "comments": "Presented as a poster at SAIV 2025",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "As machine learning gets adopted into the industry quickly, trustworthiness is increasingly in focus. Yet, efficiency and sustainability of robust training pipelines still have to be established. In this work, we consider a simple pipeline for training adversarially robust decision trees and investigate the efficiency of each step. Our pipeline consists of three stages. Firstly, we choose the perturbation size automatically for each dataset. For that, we introduce a simple algorithm, instead of relying on intuition or prior work. Moreover, we show that the perturbation size can be estimated from smaller models than the one intended for full training, and thus significant gains in efficiency can be achieved. Secondly, we train state-of-the-art adversarial training methods and evaluate them regarding both their training time and adversarial accuracy. Thirdly, we certify the robustness of each of the models thus obtained and investigate the time required for this. We find that verification time, which is critical to the efficiency of the full pipeline, is not correlated with training time.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 121,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10078",
        "abs_url": "https://arxiv.org/abs/2507.10078",
        "pdf_url": "https://arxiv.org/pdf/2507.10078",
        "title": "Compression Method for Deep Diagonal State Space Model Based on $H^2$ Optimal Reduction",
        "authors": [
            "Hiroki Sakamoto",
            "Kazuhiro Sato"
        ],
        "comments": "Accepted to IEEE Control Systems Letters",
        "subjects": "Machine Learning (cs.LG); Systems and Control (eess.SY)",
        "abstract": "Deep learning models incorporating linear SSMs have gained attention for capturing long-range dependencies in sequential data. However, their large parameter sizes pose challenges for deployment on resource-constrained devices. In this study, we propose an efficient parameter reduction method for these models by applying $H^{2}$ model order reduction techniques from control theory to their linear SSM components. In experiments, the LRA benchmark results show that the model compression based on our proposed method outperforms an existing method using the Balanced Truncation, while successfully reducing the number of parameters in the SSMs to $1/32$ without sacrificing the performance of the original models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 122,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10088",
        "abs_url": "https://arxiv.org/abs/2507.10088",
        "pdf_url": "https://arxiv.org/pdf/2507.10088",
        "title": "Towards High Supervised Learning Utility Training Data Generation: Data Pruning and Column Reordering",
        "authors": [
            "Tung Sum Thomas Kwok",
            "Zeyong Zhang",
            "Chi-Hua Wang",
            "Guang Cheng"
        ],
        "comments": "Accepted by Agentic & GenAI Evaluation KDD2025",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Tabular data synthesis for supervised learning ('SL') model training is gaining popularity in industries such as healthcare, finance, and retail. Despite the progress made in tabular data generators, models trained with synthetic data often underperform compared to those trained with original data. This low SL utility of synthetic data stems from class imbalance exaggeration and SL data relationship overlooked by tabular generator. To address these challenges, we draw inspirations from techniques in emerging data-centric artificial intelligence and elucidate Pruning and ReOrdering ('PRRO'), a novel pipeline that integrates data-centric techniques into tabular data synthesis. PRRO incorporates data pruning to guide the table generator towards observations with high signal-to-noise ratio, ensuring that the class distribution of synthetic data closely matches that of the original data. Besides, PRRO employs a column reordering algorithm to align the data modeling structure of generators with that of SL models. These two modules enable PRRO to optimize SL utility of synthetic data. Empirical experiments on 22 public datasets show that synthetic data generated using PRRO enhances predictive performance compared to data generated without PRRO. Specifically, synthetic replacement of original data yields an average improvement of 26.74% and up to 871.46% improvement using PRRO, while synthetic appendant to original data results with PRRO-generated data results in an average improvement of 6.13% and up to 200.32%. Furthermore, experiments on six highly imbalanced datasets show that PRRO enables the generator to produce synthetic data with a class distribution that resembles the original data more closely, achieving a similarity improvement of 43%. Through PRRO, we foster a seamless integration of data synthesis to subsequent SL prediction, promoting quality and accessible data analysis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 123,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10120",
        "abs_url": "https://arxiv.org/abs/2507.10120",
        "pdf_url": "https://arxiv.org/pdf/2507.10120",
        "title": "A Variance-Reduced Cubic-Regularized Newton for Policy Optimization",
        "authors": [
            "Cheng Sun",
            "Zhen Zhang",
            "Shaofu Yang"
        ],
        "comments": "13 pages, 1 figure",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "In this paper, we study a second-order approach to policy optimization in reinforcement learning. Existing second-order methods often suffer from suboptimal sample complexity or rely on unrealistic assumptions about importance sampling. To overcome these limitations, we propose VR-CR-PN, a variance-reduced cubic-regularized policy Newton algorithm. To the best of our knowledge, this is the first algorithm that integrates Hessian-aided variance reduction with second-order policy optimization, effectively addressing the distribution shift problem and achieving best-known sample complexity under general nonconvex conditions but without the need for importance sampling. We theoretically establish that VR-CR-PN achieves a sample complexity of $\\tilde{\\mathcal{O}}(\\epsilon^{-3})$ to reach an $\\epsilon$-second-order stationary point, significantly improving upon the previous best result of $\\tilde{\\mathcal{O}}(\\epsilon^{-3.5})$ under comparable assumptions. As an additional contribution, we introduce a novel Hessian estimator for the expected return function, which admits a uniform upper bound independent of the horizon length $H$, allowing the algorithm to achieve horizon-independent sample complexity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 124,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10132",
        "abs_url": "https://arxiv.org/abs/2507.10132",
        "pdf_url": "https://arxiv.org/pdf/2507.10132",
        "title": "Wavelet-Enhanced Neural ODE and Graph Attention for Interpretable Energy Forecasting",
        "authors": [
            "Usman Gani Joy"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "Accurate forecasting of energy demand and supply is critical for optimizing sustainable energy systems, yet it is challenged by the variability of renewable sources and dynamic consumption patterns. This paper introduces a neural framework that integrates continuous-time Neural Ordinary Differential Equations (Neural ODEs), graph attention, multi-resolution wavelet transformations, and adaptive learning of frequencies to address the issues of time series prediction. The model employs a robust ODE solver, using the Runge-Kutta method, paired with graph-based attention and residual connections to better understand both structural and temporal patterns. Through wavelet-based feature extraction and adaptive frequency modulation, it adeptly captures and models diverse, multi-scale temporal dynamics. When evaluated across seven diverse datasets: ETTh1, ETTh2, ETTm1, ETTm2 (electricity transformer temperature), and Waste, Solar, and Hydro (renewable energy), this architecture consistently outperforms state-of-the-art baselines in various forecasting metrics, proving its robustness in capturing complex temporal dependencies. Furthermore, the model enhances interpretability through SHAP analysis, making it suitable for sustainable energy applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 125,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10158",
        "abs_url": "https://arxiv.org/abs/2507.10158",
        "pdf_url": "https://arxiv.org/pdf/2507.10158",
        "title": "MTF-Grasp: A Multi-tier Federated Learning Approach for Robotic Grasping",
        "authors": [
            "Obaidullah Zaland",
            "Erik Elmroth",
            "Monowar Bhuyan"
        ],
        "comments": "The work is accepted for presentation at IEEE SMC 2025",
        "subjects": "Machine Learning (cs.LG); Robotics (cs.RO)",
        "abstract": "Federated Learning (FL) is a promising machine learning paradigm that enables participating devices to train privacy-preserved and collaborative models. FL has proven its benefits for robotic manipulation tasks. However, grasping tasks lack exploration in such settings where robots train a global model without moving data and ensuring data privacy. The main challenge is that each robot learns from data that is nonindependent and identically distributed (non-IID) and of low quantity. This exhibits performance degradation, particularly in robotic grasping. Thus, in this work, we propose MTF-Grasp, a multi-tier FL approach for robotic grasping, acknowledging the unique challenges posed by the non-IID data distribution across robots, including quantitative skewness. MTF-Grasp harnesses data quality and quantity across robots to select a set of \"top-level\" robots with better data distribution and higher sample count. It then utilizes top-level robots to train initial seed models and distribute them to the remaining \"low-level\" robots, reducing the risk of model performance degradation in low-level robots. Our approach outperforms the conventional FL setup by up to 8% on the quantity-skewed Cornell and Jacquard grasping datasets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 126,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10160",
        "abs_url": "https://arxiv.org/abs/2507.10160",
        "pdf_url": "https://arxiv.org/pdf/2507.10160",
        "title": "Domain Borders Are There to Be Crossed With Federated Few-Shot Adaptation",
        "authors": [
            "Manuel Röder",
            "Christoph Raab",
            "Frank-Michael Schleif"
        ],
        "comments": "Extension of this http URL",
        "subjects": "Machine Learning (cs.LG); Cryptography and Security (cs.CR); Distributed, Parallel, and Cluster Computing (cs.DC)",
        "abstract": "Federated Learning has emerged as a leading paradigm for decentralized, privacy-preserving learning, particularly relevant in the era of interconnected edge devices equipped with sensors. However, the practical implementation of Federated Learning faces three primary challenges: the need for human involvement in costly data labelling processes for target adaptation, covariate shift in client device data collection due to environmental factors affecting sensors, leading to discrepancies between source and target samples, and the impracticality of continuous or regular model updates in resource-constrained environments due to limited data transmission capabilities and technical constraints on channel availability and energy efficiency. To tackle these issues, we expand upon an efficient and scalable Federated Learning framework tailored for real-world client adaptation in industrial settings. This framework leverages a pre-trained source model comprising a deep backbone, an adaptation module, and a classifier running on a powerful server. By freezing the backbone and classifier during client adaptation on resource-constrained devices, we allow the domain adaptive linear layer to handle target domain adaptation, thus minimizing overall computational overhead. Furthermore, this setup, designated as FedAcross+, is extended to encompass the processing of streaming data, thereby rendering the solution suitable for non-stationary environments. Extensive experimental results demonstrate the effectiveness of FedAcross+ in achieving competitive adaptation on low-end client devices with limited target samples, successfully addressing the challenge of domain shift. Moreover, our framework accommodates sporadic model updates within resource-constrained environments, ensuring practical and seamless deployment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 127,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10170",
        "abs_url": "https://arxiv.org/abs/2507.10170",
        "pdf_url": "https://arxiv.org/pdf/2507.10170",
        "title": "Understanding the Rank of Tensor Networks via an Intuitive Example-Driven Approach",
        "authors": [
            "Wuyang Zhou",
            "Giorgos Iacovides",
            "Kriton Konstantinidis",
            "Ilya Kisil",
            "Danilo Mandic"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Tensor Network (TN) decompositions have emerged as an indispensable tool in Big Data analytics owing to their ability to provide compact low-rank representations, thus alleviating the ``Curse of Dimensionality'' inherent in handling higher-order data. At the heart of their success lies the concept of TN ranks, which governs the efficiency and expressivity of TN decompositions. However, unlike matrix ranks, TN ranks often lack a universal meaning and an intuitive interpretation, with their properties varying significantly across different TN structures. Consequently, TN ranks are frequently treated as empirically tuned hyperparameters, rather than as key design parameters inferred from domain knowledge. The aim of this Lecture Note is therefore to demystify the foundational yet frequently misunderstood concept of TN ranks through real-life examples and intuitive visualizations. We begin by illustrating how domain knowledge can guide the selection of TN ranks in widely-used models such as the Canonical Polyadic (CP) and Tucker decompositions. For more complex TN structures, we employ a self-explanatory graphical approach that generalizes to tensors of arbitrary order. Such a perspective naturally reveals the relationship between TN ranks and the corresponding ranks of tensor unfoldings (matrices), thereby circumventing cumbersome multi-index tensor algebra while facilitating domain-informed TN design. It is our hope that this Lecture Note will equip readers with a clear and unified understanding of the concept of TN rank, along with the necessary physical insight and intuition to support the selection, explainability, and deployment of tensor methods in both practical applications and educational contexts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 128,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10172",
        "abs_url": "https://arxiv.org/abs/2507.10172",
        "pdf_url": "https://arxiv.org/pdf/2507.10172",
        "title": "Play Style Identification Using Low-Level Representations of Play Traces in MicroRTS",
        "authors": [
            "Ruizhe Yu Xia",
            "Jeremy Gow",
            "Simon Lucas"
        ],
        "comments": "Accepted as Short Paper for IEEE CoG",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Play style identification can provide valuable game design insights and enable adaptive experiences, with the potential to improve game playing agents. Previous work relies on domain knowledge to construct play trace representations using handcrafted features. More recent approaches incorporate the sequential structure of play traces but still require some level of domain abstraction. In this study, we explore the use of unsupervised CNN-LSTM autoencoder models to obtain latent representations directly from low-level play trace data in MicroRTS. We demonstrate that this approach yields a meaningful separation of different game playing agents in the latent space, reducing reliance on domain expertise and its associated biases. This latent space is then used to guide the exploration of diverse play styles within studied AI players.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 129,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10183",
        "abs_url": "https://arxiv.org/abs/2507.10183",
        "pdf_url": "https://arxiv.org/pdf/2507.10183",
        "title": "T-GRAB: A Synthetic Diagnostic Benchmark for Learning on Temporal Graphs",
        "authors": [
            "Alireza Dizaji",
            "Benedict Aaron Tjandra",
            "Mehrab Hamidi",
            "Shenyang Huang",
            "Guillaume Rabusseau"
        ],
        "comments": "Accepted to MLoG-GenAI Workshop @ KDD 2025 (Oral)",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Dynamic graph learning methods have recently emerged as powerful tools for modelling relational data evolving through time. However, despite extensive benchmarking efforts, it remains unclear whether current Temporal Graph Neural Networks (TGNNs) effectively capture core temporal patterns such as periodicity, cause-and-effect, and long-range dependencies. In this work, we introduce the Temporal Graph Reasoning Benchmark (T-GRAB), a comprehensive set of synthetic tasks designed to systematically probe the capabilities of TGNNs to reason across time. T-GRAB provides controlled, interpretable tasks that isolate key temporal skills: counting/memorizing periodic repetitions, inferring delayed causal effects, and capturing long-range dependencies over both spatial and temporal dimensions. We evaluate 11 temporal graph learning methods on these tasks, revealing fundamental shortcomings in their ability to generalize temporal patterns. Our findings offer actionable insights into the limitations of current models, highlight challenges hidden by traditional real-world benchmarks, and motivate the development of architectures with stronger temporal reasoning abilities. The code for T-GRAB can be found at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 130,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10194",
        "abs_url": "https://arxiv.org/abs/2507.10194",
        "pdf_url": "https://arxiv.org/pdf/2507.10194",
        "title": "Learning Private Representations through Entropy-based Adversarial Training",
        "authors": [
            "Tassilo Klein",
            "Moin Nabi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "How can we learn a representation with high predictive power while preserving user privacy? We present an adversarial representation learning method for sanitizing sensitive content from the learned representation. Specifically, we introduce a variant of entropy - focal entropy, which mitigates the potential information leakage of the existing entropy-based approaches. We showcase feasibility on multiple benchmarks. The results suggest high target utility at moderate privacy leakage.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 131,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10215",
        "abs_url": "https://arxiv.org/abs/2507.10215",
        "pdf_url": "https://arxiv.org/pdf/2507.10215",
        "title": "A Graph Sufficiency Perspective for Neural Networks",
        "authors": [
            "Cencheng Shen",
            "Yuexiao Dong"
        ],
        "comments": "23 pages",
        "subjects": "Machine Learning (cs.LG); Applications (stat.AP)",
        "abstract": "This paper analyzes neural networks through graph variables and statistical sufficiency. We interpret neural network layers as graph-based transformations, where neurons act as pairwise functions between inputs and learned anchor points. Within this formulation, we establish conditions under which layer outputs are sufficient for the layer inputs, that is, each layer preserves the conditional distribution of the target variable given the input variable. Under dense anchor point assumptions, we prove that asymptotic sufficiency holds in the infinite-width limit and is preserved throughout training. To align more closely with practical architectures, we further show that sufficiency can be achieved with finite-width networks by assuming region-separated input distributions and constructing appropriate anchor points. Our framework covers fully connected layers, general pairwise functions, ReLU and sigmoid activations, and convolutional neural networks. This work bridges statistical sufficiency, graph-theoretic representations, and deep learning, providing a new statistical understanding of neural networks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 132,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10241",
        "abs_url": "https://arxiv.org/abs/2507.10241",
        "pdf_url": "https://arxiv.org/pdf/2507.10241",
        "title": "Kernel-Adaptive PI-ELMs for Forward and Inverse Problems in PDEs with Sharp Gradients",
        "authors": [
            "Vikas Dwivedi",
            "Balaji Srinivasan",
            "Monica Sigovan",
            "Bruno Sixou"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "This paper introduces the Kernel Adaptive Physics-Informed Extreme Learning Machine (KAPI-ELM), an adaptive Radial Basis Function (RBF)-based extension of PI-ELM designed to solve both forward and inverse Partial Differential Equation (PDE) problems involving localized sharp gradients. While PI-ELMs outperform the traditional Physics-Informed Neural Networks (PINNs) in speed due to their single-shot, least square optimization, this advantage comes at a cost: their fixed, randomly initialized input layer limits their ability to capture sharp gradients. To overcome this limitation, we introduce a lightweight Bayesian Optimization (BO) framework that, instead of adjusting each input layer parameter individually as in traditional backpropagation, learns a small set of hyperparameters defining the statistical distribution from which the input weights are drawn. This novel distributional optimization strategy -- combining BO for input layer distributional parameters with least-squares optimization for output layer network parameters -- enables KAPI-ELM to preserve PI-ELM's speed while matching or exceeding the expressiveness of PINNs. We validate the proposed methodology on several challenging forward and inverse PDE benchmarks, including a 1D singularly perturbed convection-diffusion equation, a 2D Poisson equation with sharp localized sources, and a time-dependent advection equation. Notably, KAPI-ELM achieves state-of-the-art accuracy in both forward and inverse settings. In stiff PDE regimes, it matches or even outperforms advanced methods such as the Extended Theory of Functional Connections (XTFC), while requiring nearly an order of magnitude fewer tunable parameters. These results establish the potential of KAPI-ELM as a scalable, interpretable, and generalizable physics-informed learning framework, especially in stiff PDE regimes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 133,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10273",
        "abs_url": "https://arxiv.org/abs/2507.10273",
        "pdf_url": "https://arxiv.org/pdf/2507.10273",
        "title": "Conditional Chemical Language Models are Versatile Tools in Drug Discovery",
        "authors": [
            "Lu Zhu",
            "Emmanuel Noutahi"
        ],
        "comments": "12 pages, extra 13 pages of appendix",
        "subjects": "Machine Learning (cs.LG); Biomolecules (q-bio.BM)",
        "abstract": "Generative chemical language models (CLMs) have demonstrated strong capabilities in molecular design, yet their impact in drug discovery remains limited by the absence of reliable reward signals and the lack of interpretability in their outputs. We present SAFE-T, a generalist chemical modeling framework that conditions on biological context -- such as protein targets or mechanisms of action -- to prioritize and design molecules without relying on structural information or engineered scoring functions. SAFE-T models the conditional likelihood of fragment-based molecular sequences given a biological prompt, enabling principled scoring of molecules across tasks such as virtual screening, drug-target interaction prediction, and activity cliff detection. Moreover, it supports goal-directed generation by sampling from this learned distribution, aligning molecular design with biological objectives. In comprehensive zero-shot evaluations across predictive (LIT-PCBA, DAVIS, KIBA, ACNet) and generative (DRUG, PMO) benchmarks, SAFE-T consistently achieves performance comparable to or better than existing approaches while being significantly faster. Fragment-level attribution further reveals that SAFE-T captures known structure-activity relationships, supporting interpretable and biologically grounded design. Together with its computational efficiency, these results demonstrate that conditional generative CLMs can unify scoring and generation to accelerate early-stage drug discovery.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 134,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10296",
        "abs_url": "https://arxiv.org/abs/2507.10296",
        "pdf_url": "https://arxiv.org/pdf/2507.10296",
        "title": "Average Sensitivity of Hierarchical $k$-Median Clustering",
        "authors": [
            "Shijie Li",
            "Weiqiang He",
            "Ruobing Bai",
            "Pan Peng"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Data Structures and Algorithms (cs.DS)",
        "abstract": "Hierarchical clustering is a widely used method for unsupervised learning with numerous applications. However, in the application of modern algorithms, the datasets studied are usually large and dynamic. If the hierarchical clustering is sensitive to small perturbations of the dataset, the usability of the algorithm will be greatly reduced. In this paper, we focus on the hierarchical $k$ -median clustering problem, which bridges hierarchical and centroid-based clustering while offering theoretical appeal, practical utility, and improved interpretability. We analyze the average sensitivity of algorithms for this problem by measuring the expected change in the output when a random data point is deleted. We propose an efficient algorithm for hierarchical $k$-median clustering and theoretically prove its low average sensitivity and high clustering quality. Additionally, we show that single linkage clustering and a deterministic variant of the CLNSS algorithm exhibit high average sensitivity, making them less stable. Finally, we validate the robustness and effectiveness of our algorithm through experiments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 135,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10311",
        "abs_url": "https://arxiv.org/abs/2507.10311",
        "pdf_url": "https://arxiv.org/pdf/2507.10311",
        "title": "Recognizing Dementia from Neuropsychological Tests with State Space Models",
        "authors": [
            "Liming Wang",
            "Saurabhchand Bhati",
            "Cody Karjadi",
            "Rhoda Au",
            "James Glass"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Early detection of dementia is critical for timely medical intervention and improved patient outcomes. Neuropsychological tests are widely used for cognitive assessment but have traditionally relied on manual scoring. Automatic dementia classification (ADC) systems aim to infer cognitive decline directly from speech recordings of such tests. We propose Demenba, a novel ADC framework based on state space models, which scale linearly in memory and computation with sequence length. Trained on over 1,000 hours of cognitive assessments administered to Framingham Heart Study participants, some of whom were diagnosed with dementia through adjudicated review, our method outperforms prior approaches in fine-grained dementia classification by 21\\%, while using fewer parameters. We further analyze its scaling behavior and demonstrate that our model gains additional improvement when fused with large language models, paving the way for more transparent and scalable dementia assessment tools. Code: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 136,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10325",
        "abs_url": "https://arxiv.org/abs/2507.10325",
        "pdf_url": "https://arxiv.org/pdf/2507.10325",
        "title": "Convergence of Agnostic Federated Averaging",
        "authors": [
            "Herlock",
            "Rahimi",
            "Dionysis Kalogerias"
        ],
        "comments": "5 pages, 2 figurres, CAMSAP conference",
        "subjects": "Machine Learning (cs.LG); Distributed, Parallel, and Cluster Computing (cs.DC); Signal Processing (eess.SP)",
        "abstract": "Federated learning (FL) enables decentralized model training without centralizing raw data. However, practical FL deployments often face a key realistic challenge: Clients participate intermittently in server aggregation and with unknown, possibly biased participation probabilities. Most existing convergence results either assume full-device participation, or rely on knowledge of (in fact uniform) client availability distributions -- assumptions that rarely hold in practice. In this work, we characterize the optimization problem that consistently adheres to the stochastic dynamics of the well-known \\emph{agnostic Federated Averaging (FedAvg)} algorithm under random (and variably-sized) client availability, and rigorously establish its convergence for convex, possibly nonsmooth losses, achieving a standard rate of order $\\mathcal{O}(1/\\sqrt{T})$, where $T$ denotes the aggregation horizon. Our analysis provides the first convergence guarantees for agnostic FedAvg under general, non-uniform, stochastic client participation, without knowledge of the participation distribution. We also empirically demonstrate that agnostic FedAvg in fact outperforms common (and suboptimal) weighted aggregation FedAvg variants, even with server-side knowledge of participation weights.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 137,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10334",
        "abs_url": "https://arxiv.org/abs/2507.10334",
        "pdf_url": "https://arxiv.org/pdf/2507.10334",
        "title": "MoCap-Impute: A Comprehensive Benchmark and Comparative Analysis of Imputation Methods for IMU-based Motion Capture Data",
        "authors": [
            "Mahmoud Bekhit",
            "Ahmad Salah",
            "Ahmed Salim Alrawahi",
            "Tarek Attia",
            "Ahmed Ali",
            "Esraa Eldesokey",
            "Ahmed Fathalla"
        ],
        "comments": "22 pages, 7 figures, 3 algorithms, 2 tables",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Motion capture (MoCap) data from wearable Inertial Measurement Units (IMUs) is vital for applications in sports science, but its utility is often compromised by missing data. Despite numerous imputation techniques, a systematic performance evaluation for IMU-derived MoCap time-series data is lacking. We address this gap by conducting a comprehensive comparative analysis of statistical, machine learning, and deep learning imputation methods. Our evaluation considers three distinct contexts: univariate time-series, multivariate across subjects, and multivariate across kinematic angles. To facilitate this benchmark, we introduce the first publicly available MoCap dataset designed specifically for imputation, featuring data from 53 karate practitioners. We simulate three controlled missingness mechanisms: missing completely at random (MCAR), block missingness, and a novel value-dependent pattern at signal transition points. Our experiments, conducted on 39 kinematic variables across all subjects, reveal that multivariate imputation frameworks consistently outperform univariate approaches, particularly for complex missingness. For instance, multivariate methods achieve up to a 50% mean absolute error reduction (MAE from 10.8 to 5.8) compared to univariate techniques for transition point missingness. Advanced models like Generative Adversarial Imputation Networks (GAIN) and Iterative Imputers demonstrate the highest accuracy in these challenging scenarios. This work provides a critical baseline for future research and offers practical recommendations for improving the integrity and robustness of Mo-Cap data analysis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 138,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10345",
        "abs_url": "https://arxiv.org/abs/2507.10345",
        "pdf_url": "https://arxiv.org/pdf/2507.10345",
        "title": "Some Super-approximation Rates of ReLU Neural Networks for Korobov Functions",
        "authors": [
            "Yuwen Li",
            "Guozhi Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "This paper examines the $L_p$ and $W^1_p$ norm approximation errors of ReLU neural networks for Korobov functions. In terms of network width and depth, we derive nearly optimal super-approximation error bounds of order $2m$ in the $L_p$ norm and order $2m-2$ in the $W^1_p$ norm, for target functions with $L_p$ mixed derivative of order $m$ in each direction. The analysis leverages sparse grid finite elements and the bit extraction technique. Our results improve upon classical lowest order $L_\\infty$ and $H^1$ norm error bounds and demonstrate that the expressivity of neural networks is largely unaffected by the curse of dimensionality.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 139,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10347",
        "abs_url": "https://arxiv.org/abs/2507.10347",
        "pdf_url": "https://arxiv.org/pdf/2507.10347",
        "title": "Parallel Sampling of Diffusion Models on $SO(3)$",
        "authors": [
            "Yan-Ting Chen",
            "Hao-Wei Chen",
            "Tsu-Ching Hsiao",
            "Chun-Yi Lee"
        ],
        "comments": "MVA2025",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In this paper, we design an algorithm to accelerate the diffusion process on the $SO(3)$ manifold. The inherently sequential nature of diffusion models necessitates substantial time for denoising perturbed data. To overcome this limitation, we proposed to adapt the numerical Picard iteration for the $SO(3)$ space. We demonstrate our algorithm on an existing method that employs diffusion models to address the pose ambiguity problem. Moreover, we show that this acceleration advantage occurs without any measurable degradation in task reward. The experiments reveal that our algorithm achieves a speed-up of up to 4.9$\\times$, significantly reducing the latency for generating a single sample.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 140,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10348",
        "abs_url": "https://arxiv.org/abs/2507.10348",
        "pdf_url": "https://arxiv.org/pdf/2507.10348",
        "title": "Feature Distillation is the Better Choice for Model-Heterogeneous Federated Learning",
        "authors": [
            "Yichen Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Model-Heterogeneous Federated Learning (Hetero-FL) has attracted growing attention for its ability to aggregate knowledge from heterogeneous models while keeping private data locally. To better aggregate knowledge from clients, ensemble distillation, as a widely used and effective technique, is often employed after global aggregation to enhance the performance of the global model. However, simply combining Hetero-FL and ensemble distillation does not always yield promising results and can make the training process unstable. The reason is that existing methods primarily focus on logit distillation, which, while being model-agnostic with softmax predictions, fails to compensate for the knowledge bias arising from heterogeneous models. To tackle this challenge, we propose a stable and efficient Feature Distillation for model-heterogeneous Federated learning, dubbed FedFD, that can incorporate aligned feature information via orthogonal projection to integrate knowledge from heterogeneous models better. Specifically, a new feature-based ensemble federated knowledge distillation paradigm is proposed. The global model on the server needs to maintain a projection layer for each client-side model architecture to align the features separately. Orthogonal techniques are employed to re-parameterize the projection layer to mitigate knowledge bias from heterogeneous models and thus maximize the distilled knowledge. Extensive experiments show that FedFD achieves superior performance compared to state-of-the-art methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 141,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10349",
        "abs_url": "https://arxiv.org/abs/2507.10349",
        "pdf_url": "https://arxiv.org/pdf/2507.10349",
        "title": "TAT: Temporal-Aligned Transformer for Multi-Horizon Peak Demand Forecasting",
        "authors": [
            "Zhiyuan Zhao",
            "Sitan Yang",
            "Kin G. Olivares",
            "Boris N. Oreshkin",
            "Stan Vitebsky",
            "Michael W. Mahoney",
            "B. Aditya Prakash",
            "Dmitry Efimov"
        ],
        "comments": "9 pages, 4 figures, 7 tables, published at KDD 2025 workshop on AI for Supply Chain: Today and Future",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Multi-horizon time series forecasting has many practical applications such as demand forecasting. Accurate demand prediction is critical to help make buying and inventory decisions for supply chain management of e-commerce and physical retailers, and such predictions are typically required for future horizons extending tens of weeks. This is especially challenging during high-stake sales events when demand peaks are particularly difficult to predict accurately. However, these events are important not only for managing supply chain operations but also for ensuring a seamless shopping experience for customers. To address this challenge, we propose Temporal-Aligned Transformer (TAT), a multi-horizon forecaster leveraging apriori-known context variables such as holiday and promotion events information for improving predictive performance. Our model consists of an encoder and decoder, both embedded with a novel Temporal Alignment Attention (TAA), designed to learn context-dependent alignment for peak demand forecasting. We conduct extensive empirical analysis on two large-scale proprietary datasets from a large e-commerce retailer. We demonstrate that TAT brings up to 30% accuracy improvement on peak demand forecasting while maintaining competitive overall performance compared to other state-of-the-art methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 142,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10368",
        "abs_url": "https://arxiv.org/abs/2507.10368",
        "pdf_url": "https://arxiv.org/pdf/2507.10368",
        "title": "Enhanced DeepONet for 1-D consolidation operator learning: an architectural investigation",
        "authors": [
            "Yongjin Choi",
            "Chenying Liu",
            "Jorge Macedo"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Geophysics (physics.geo-ph)",
        "abstract": "Deep Operator Networks (DeepONets) have emerged as a powerful surrogate modeling framework for learning solution operators in PDE-governed systems. While their use is expanding across engineering disciplines, applications in geotechnical engineering remain limited. This study systematically evaluates several DeepONet architectures for the one-dimensional consolidation problem. We initially consider three architectures: a standard DeepONet with the coefficient of consolidation embedded in the branch net (Models 1 and 2), and a physics-inspired architecture with the coefficient embedded in the trunk net (Model 3). Results show that Model 3 outperforms the standard configurations (Models 1 and 2) but still has limitations when the target solution (excess pore pressures) exhibits significant variation. To overcome this limitation, we propose a Trunknet Fourier feature-enhanced DeepONet (Model 4) that addresses the identified limitations by capturing rapidly varying functions. All proposed architectures achieve speedups ranging from 1.5 to 100 times over traditional explicit and implicit solvers, with Model 4 being the most efficient. Larger computational savings are expected for more complex systems than the explored 1D case, which is promising. Overall, the study highlights the potential of DeepONets to enable efficient, generalizable surrogate modeling in geotechnical applications, advancing the integration of scientific machine learning in geotechnics, which is at an early stage.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 143,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10382",
        "abs_url": "https://arxiv.org/abs/2507.10382",
        "pdf_url": "https://arxiv.org/pdf/2507.10382",
        "title": "Leveraging RAG-LLMs for Urban Mobility Simulation and Analysis",
        "authors": [
            "Yue Ding",
            "Conor McCarthy",
            "Kevin O'Shea",
            "Mingming Liu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "With the rise of smart mobility and shared e-mobility services, numerous advanced technologies have been applied to this field. Cloud-based traffic simulation solutions have flourished, offering increasingly realistic representations of the evolving mobility landscape. LLMs have emerged as pioneering tools, providing robust support for various applications, including intelligent decision-making, user interaction, and real-time traffic analysis. As user demand for e-mobility continues to grow, delivering comprehensive end-to-end solutions has become crucial. In this paper, we present a cloud-based, LLM-powered shared e-mobility platform, integrated with a mobile application for personalized route recommendations. The optimization module is evaluated based on travel time and cost across different traffic scenarios. Additionally, the LLM-powered RAG framework is evaluated at the schema level for different users, using various evaluation methods. Schema-level RAG with XiYanSQL achieves an average execution accuracy of 0.81 on system operator queries and 0.98 on user queries.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 144,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10385",
        "abs_url": "https://arxiv.org/abs/2507.10385",
        "pdf_url": "https://arxiv.org/pdf/2507.10385",
        "title": "Extracting Important Tokens in E-Commerce Queries with a Tag Interaction-Aware Transformer Model",
        "authors": [
            "Md. Ahsanul Kabir",
            "Mohammad Al Hasan",
            "Aritra Mandal",
            "Liyang Hao",
            "Ishita Khan",
            "Daniel Tunkelang",
            "Zhe Wu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The major task of any e-commerce search engine is to retrieve the most relevant inventory items, which best match the user intent reflected in a query. This task is non-trivial due to many reasons, including ambiguous queries, misaligned vocabulary between buyers, and sellers, over- or under-constrained queries by the presence of too many or too few tokens. To address these challenges, query reformulation is used, which modifies a user query through token dropping, replacement or expansion, with the objective to bridge semantic gap between query tokens and users' search intent. Early methods of query reformulation mostly used statistical measures derived from token co-occurrence frequencies from selective user sessions having clicks or purchases. In recent years, supervised deep learning approaches, specifically transformer-based neural language models, or sequence-to-sequence models are being used for query reformulation task. However, these models do not utilize the semantic tags of a query token, which are significant for capturing user intent of an e-commerce query. In this work, we pose query reformulation as a token classification task, and solve this task by designing a dependency-aware transformer-based language model, TagBERT, which makes use of semantic tags of a token for learning superior query phrase embedding. Experiments on large, real-life e-commerce datasets show that TagBERT exhibits superior performance than plethora of competing models, including BERT, eBERT, and Sequence-to-Sequence transformer model for important token classification task.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 145,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10400",
        "abs_url": "https://arxiv.org/abs/2507.10400",
        "pdf_url": "https://arxiv.org/pdf/2507.10400",
        "title": "Anticipating the Selectivity of Cyclization Reaction Pathways with Neural Network Potentials",
        "authors": [
            "Nicholas Casetti",
            "Dylan Anstine",
            "Olexandr Isayev",
            "Connor W. Coley"
        ],
        "comments": "32 pages, 5 figures",
        "subjects": "Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)",
        "abstract": "Reaction mechanism search tools have demonstrated the ability to provide insights into likely products and rate-limiting steps of reacting systems. However, reactions involving several concerted bond changes - as can be found in many key steps of natural product synthesis - can complicate the search process. To mitigate these complications, we present a mechanism search strategy particularly suited to help expedite exploration of an exemplary family of such complex reactions, cyclizations. We provide a cost-effective strategy for identifying relevant elementary reaction steps by combining graph-based enumeration schemes and machine learning techniques for intermediate filtering. Key to this approach is our use of a neural network potential (NNP), AIMNet2-rxn, for computational evaluation of each candidate reaction pathway. In this article, we evaluate the NNP's ability to estimate activation energies, demonstrate the correct anticipation of stereoselectivity, and recapitulate complex enabling steps in natural product synthesis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 146,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10401",
        "abs_url": "https://arxiv.org/abs/2507.10401",
        "pdf_url": "https://arxiv.org/pdf/2507.10401",
        "title": "Stochastic Operator Network: A Stochastic Maximum Principle Based Approach to Operator Learning",
        "authors": [
            "Ryan Bausback",
            "Jingqiao Tang",
            "Lu Lu",
            "Feng Bao",
            "Toan Huynh"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Probability (math.PR)",
        "abstract": "We develop a novel framework for uncertainty quantification in operator learning, the Stochastic Operator Network (SON). SON combines the stochastic optimal control concepts of the Stochastic Neural Network (SNN) with the DeepONet. By formulating the branch net as an SDE and backpropagating through the adjoint BSDE, we replace the gradient of the loss function with the gradient of the Hamiltonian from Stohastic Maximum Principle in the SGD update. This allows SON to learn the uncertainty present in operators through its diffusion parameters. We then demonstrate the effectiveness of SON when replicating several noisy operators in 2D and 3D.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 147,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10409",
        "abs_url": "https://arxiv.org/abs/2507.10409",
        "pdf_url": "https://arxiv.org/pdf/2507.10409",
        "title": "Energy Efficiency in AI for 5G and Beyond: A DeepRx Case Study",
        "authors": [
            "Amine Lbath",
            "Ibtissam Labriji"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "This study addresses the challenge of balancing energy efficiency with performance in AI/ML models, focusing on DeepRX, a deep learning receiver based on a fully convolutional ResNet architecture. We evaluate the energy consumption of DeepRX, considering factors including FLOPs/Watt and FLOPs/clock, and find consistency between estimated and actual energy usage, influenced by memory access patterns. The research extends to comparing energy dynamics during training and inference phases. A key contribution is the application of knowledge distillation (KD) to train a compact DeepRX student model that emulates the performance of the teacher model but with reduced energy consumption. We experiment with different student model sizes, optimal teacher sizes, and KD hyperparameters. Performance is measured by comparing the Bit Error Rate (BER) performance versus Signal-to-Interference & Noise Ratio (SINR) values of the distilled model and a model trained from scratch. The distilled models demonstrate a lower error floor across SINR levels, highlighting the effectiveness of KD in achieving energy-efficient AI solutions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 148,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10419",
        "abs_url": "https://arxiv.org/abs/2507.10419",
        "pdf_url": "https://arxiv.org/pdf/2507.10419",
        "title": "Multiple Choice Learning of Low Rank Adapters for Language Modeling",
        "authors": [
            "Victor Letzelter",
            "Hugo Malard",
            "Mathieu Fontaine",
            "Gaël Richard",
            "Slim Essid",
            "Andrei Bursuc",
            "Patrick Pérez"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (stat.ML)",
        "abstract": "We propose LoRA-MCL, a training scheme that extends next-token prediction in language models with a method designed to decode diverse, plausible sentence continuations at inference time. Traditional language modeling is an intrinsically ill-posed problem: given a context, multiple futures may be equally plausible. Our approach leverages Multiple Choice Learning (MCL) and the Winner-Takes-All (WTA) loss to efficiently handle ambiguity through Low-Rank Adaptation (LoRA). We provide a theoretical interpretation of applying Multiple Choice Learning to Language Modeling, assuming the data is generated from a mixture of distributions. To illustrate the proposed approach, we use data sampled from mixtures of Markov chains. We then demonstrate with extensive experiments on real-world visual and audio captioning tasks that our method achieves high diversity and relevance in generated outputs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 149,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10425",
        "abs_url": "https://arxiv.org/abs/2507.10425",
        "pdf_url": "https://arxiv.org/pdf/2507.10425",
        "title": "Non-exchangeable Conformal Prediction with Optimal Transport: Tackling Distribution Shifts with Unlabeled Data",
        "authors": [
            "Alvaro H.C. Correia",
            "Christos Louizos"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Conformal prediction is a distribution-free uncertainty quantification method that has gained popularity in the machine learning community due to its finite-sample guarantees and ease of use. Its most common variant, dubbed split conformal prediction, is also computationally efficient as it boils down to collecting statistics of the model predictions on some calibration data not yet seen by the model. Nonetheless, these guarantees only hold if the calibration and test data are exchangeable, a condition that is difficult to verify and often violated in practice due to so-called distribution shifts. The literature is rife with methods to mitigate the loss in coverage in this non-exchangeable setting, but these methods require some prior information on the type of distribution shift to be expected at test time. In this work, we study this problem via a new perspective, through the lens of optimal transport, and show that it is possible to estimate the loss in coverage and mitigate it in case of distribution shift.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 150,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10434",
        "abs_url": "https://arxiv.org/abs/2507.10434",
        "pdf_url": "https://arxiv.org/pdf/2507.10434",
        "title": "CLA: Latent Alignment for Online Continual Self-Supervised Learning",
        "authors": [
            "Giacomo Cignoni",
            "Andrea Cossu",
            "Alexandra Gomez-Villa",
            "Joost van de Weijer",
            "Antonio Carta"
        ],
        "comments": "Accepted at CoLLAs 2025 conference (oral)",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Self-supervised learning (SSL) is able to build latent representations that generalize well to unseen data. However, only a few SSL techniques exist for the online CL setting, where data arrives in small minibatches, the model must comply with a fixed computational budget, and task boundaries are absent. We introduce Continual Latent Alignment (CLA), a novel SSL strategy for Online CL that aligns the representations learned by the current model with past representations to mitigate forgetting. We found that our CLA is able to speed up the convergence of the training process in the online scenario, outperforming state-of-the-art approaches under the same computational budget. Surprisingly, we also discovered that using CLA as a pretraining protocol in the early stages of pretraining leads to a better final performance when compared to a full i.i.d. pretraining.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 151,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10442",
        "abs_url": "https://arxiv.org/abs/2507.10442",
        "pdf_url": "https://arxiv.org/pdf/2507.10442",
        "title": "Response Wide Shut? Surprising Observations in Basic Vision Language Model Capabilities",
        "authors": [
            "Shivam Chandhok",
            "Wan-Cyuan Fan",
            "Vered Shwartz",
            "Vineeth N Balasubramanian",
            "Leonid Sigal"
        ],
        "comments": "Accepted at ACL 2025 (Main Conference)",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Vision-language Models (VLMs) have emerged as general-purpose tools for addressing a variety of complex computer vision problems. Such models have been shown to be highly capable, but, at the same time, lacking some basic visual understanding skills. In this paper, we set out to understand the limitations of SoTA VLMs on fundamental visual tasks by constructing a series of tests that probe which components of design, specifically, may be lacking. Importantly, we go significantly beyond the current benchmarks, which simply measure the final performance of VLM response, by also comparing and contrasting it to the performance of probes trained directly on features obtained from the visual encoder, intermediate vision-language projection and LLM-decoder output. In doing so, we uncover shortcomings in VLMs and make a number of important observations about their capabilities, robustness and how they process visual information. We hope our insights will guide progress in further improving VLMs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 152,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10452",
        "abs_url": "https://arxiv.org/abs/2507.10452",
        "pdf_url": "https://arxiv.org/pdf/2507.10452",
        "title": "Some remarks on gradient dominance and LQR policy optimization",
        "authors": [
            "Eduardo D. Sontag"
        ],
        "comments": "This is a short paper summarizing the first part of the slides presented at my keynote at the 2025 L4DC (Learning for Dynamics & Control Conference) in Ann Arbor, Michigan, 05 June 2025. A partial bibliography has been added",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Solutions of optimization problems, including policy optimization in reinforcement learning, typically rely upon some variant of gradient descent. There has been much recent work in the machine learning, control, and optimization communities applying the Polyak-Łojasiewicz Inequality (PLI) to such problems in order to establish an exponential rate of convergence (a.k.a. ``linear convergence'' in the local-iteration language of numerical analysis) of loss functions to their minima under the gradient flow. Often, as is the case of policy iteration for the continuous-time LQR problem, this rate vanishes for large initial conditions, resulting in a mixed globally linear / locally exponential behavior. This is in sharp contrast with the discrete-time LQR problem, where there is global exponential convergence. That gap between CT and DT behaviors motivates the search for various generalized PLI-like conditions, and this talk will address that topic. Moreover, these generalizations are key to understanding the transient and asymptotic effects of errors in the estimation of the gradient, errors which might arise from adversarial attacks, wrong evaluation by an oracle, early stopping of a simulation, inaccurate and very approximate digital twins, stochastic computations (algorithm ``reproducibility''), or learning by sampling from limited data. We describe an ``input to state stability'' (ISS) analysis of this issue. The second part discusses convergence and PLI-like properties of ``linear feedforward neural networks'' in feedback control. Much of the work described here was done in collaboration with Arthur Castello B. de Oliveira, Leilei Cui, Zhong-Ping Jiang, and Milad Siami.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 153,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10484",
        "abs_url": "https://arxiv.org/abs/2507.10484",
        "pdf_url": "https://arxiv.org/pdf/2507.10484",
        "title": "The Target Polish: A New Approach to Outlier-Resistant Non-Negative Matrix and Tensor Factorization",
        "authors": [
            "Paul Fogel",
            "Christophe Geissler",
            "George Luta"
        ],
        "comments": "6 pages, 4 figures, International Conference on Robust Statistics 2025, Stresa, Italy",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "This paper introduces the \"Target Polish,\" a robust and computationally efficient framework for nonnegative matrix and tensor factorization. Although conventional weighted NMF approaches are resistant to outliers, they converge slowly due to the use of multiplicative updates to minimize the objective criterion. In contrast, the Target Polish approach remains compatible with the Fast-HALS algorithm, which is renowned for its speed, by adaptively smoothing the data with a weighted median-based transformation. This innovation provides outlier resistance while maintaining the highly efficient additive update structure of Fast-HALS. Empirical evaluations using image datasets corrupted with structured (block) and unstructured (salt) noise demonstrate that the Target Polish approach matches or exceeds the accuracy of state-of-the-art robust NMF methods and reduces computational time by an order of magnitude in the studied scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 154,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10485",
        "abs_url": "https://arxiv.org/abs/2507.10485",
        "pdf_url": "https://arxiv.org/pdf/2507.10485",
        "title": "Overcoming catastrophic forgetting in neural networks",
        "authors": [
            "Brandon Shuen Yi Loke",
            "Filippo Quadri",
            "Gabriel Vivanco",
            "Maximilian Casagrande",
            "Saúl Fenollosa"
        ],
        "comments": "7 pages, 5 figures, EE-411 Fundamentals of inference and learning course project",
        "subjects": "Machine Learning (cs.LG); Information Retrieval (cs.IR)",
        "abstract": "Catastrophic forgetting is the primary challenge that hinders continual learning, which refers to a neural network ability to sequentially learn multiple tasks while retaining previously acquired knowledge. Elastic Weight Consolidation, a regularization-based approach inspired by synaptic consolidation in biological neural systems, has been used to overcome this problem. In this study prior research is replicated and extended by evaluating EWC in supervised learning settings using the PermutedMNIST and RotatedMNIST benchmarks. Through systematic comparisons with L2 regularization and stochastic gradient descent (SGD) without regularization, we analyze how different approaches balance knowledge retention and adaptability. Our results confirm what was shown in previous research, showing that EWC significantly reduces forgetting compared to naive training while slightly compromising learning efficiency on new tasks. Moreover, we investigate the impact of dropout regularization and varying hyperparameters, offering insights into the generalization of EWC across diverse learning scenarios. These results underscore EWC's potential as a viable solution for lifelong learning in neural networks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 155,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10494",
        "abs_url": "https://arxiv.org/abs/2507.10494",
        "pdf_url": "https://arxiv.org/pdf/2507.10494",
        "title": "Split Happens: Combating Advanced Threats with Split Learning and Function Secret Sharing",
        "authors": [
            "Tanveer Khan",
            "Mindaugas Budzys",
            "Antonis Michalas"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Cryptography and Security (cs.CR)",
        "abstract": "Split Learning (SL) -- splits a model into two distinct parts to help protect client data while enhancing Machine Learning (ML) processes. Though promising, SL has proven vulnerable to different attacks, thus raising concerns about how effective it may be in terms of data privacy. Recent works have shown promising results for securing SL through the use of a novel paradigm, named Function Secret Sharing (FSS), in which servers obtain shares of a function they compute and operate on a public input hidden with a random mask. However, these works fall short in addressing the rising number of attacks which exist on SL. In SplitHappens, we expand the combination of FSS and SL to U-shaped SL. Similarly to other works, we are able to make use of the benefits of SL by reducing the communication and computational costs of FSS. However, a U-shaped SL provides a higher security guarantee than previous works, allowing a client to keep the labels of the training data secret, without having to share them with the server. Through this, we are able to generalize the security analysis of previous works and expand it to different attack vectors, such as modern model inversion attacks as well as label inference attacks. We tested our approach for two different convolutional neural networks on different datasets. These experiments show the effectiveness of our approach in reducing the training time as well as the communication costs when compared to simply using FSS while matching prior accuracy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 156,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10502",
        "abs_url": "https://arxiv.org/abs/2507.10502",
        "pdf_url": "https://arxiv.org/pdf/2507.10502",
        "title": "Benchmarking and Evaluation of AI Models in Biology: Outcomes and Recommendations from the CZI Virtual Cells Workshop",
        "authors": [
            "Elizabeth Fahsbender",
            "Alma Andersson",
            "Jeremy Ash",
            "Polina Binder",
            "Daniel Burkhardt",
            "Benjamin Chang",
            "Georg K. Gerber",
            "Anthony Gitter",
            "Patrick Godau",
            "Ankit Gupta",
            "Genevieve Haliburton",
            "Siyu He",
            "Trey Ideker",
            "Ivana Jelic",
            "Aly Khan",
            "Yang-Joon Kim",
            "Aditi Krishnapriyan",
            "Jon M. Laurent",
            "Tianyu Liu",
            "Emma Lundberg",
            "Shalin B. Mehta",
            "Rob Moccia",
            "Angela Oliveira Pisco",
            "Katherine S. Pollard",
            "Suresh Ramani",
            "Julio Saez-Rodriguez",
            "Yasin Senbabaoglu",
            "Elana Simon",
            "Srinivasan Sivanandan",
            "Gustavo Stolovitzky",
            "Marc Valer",
            "Bo Wang",
            "Xikun Zhang",
            "James Zou",
            "Katrina Kalantar"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Artificial intelligence holds immense promise for transforming biology, yet a lack of standardized, cross domain, benchmarks undermines our ability to build robust, trustworthy models. Here, we present insights from a recent workshop that convened machine learning and computational biology experts across imaging, transcriptomics, proteomics, and genomics to tackle this gap. We identify major technical and systemic bottlenecks such as data heterogeneity and noise, reproducibility challenges, biases, and the fragmented ecosystem of publicly available resources and propose a set of recommendations for building benchmarking frameworks that can efficiently compare ML models of biological systems across tasks and data modalities. By promoting high quality data curation, standardized tooling, comprehensive evaluation metrics, and open, collaborative platforms, we aim to accelerate the development of robust benchmarks for AI driven Virtual Cells. These benchmarks are crucial for ensuring rigor, reproducibility, and biological relevance, and will ultimately advance the field toward integrated models that drive new discoveries, therapeutic insights, and a deeper understanding of cellular systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 157,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10532",
        "abs_url": "https://arxiv.org/abs/2507.10532",
        "pdf_url": "https://arxiv.org/pdf/2507.10532",
        "title": "Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination",
        "authors": [
            "Mingqi Wu",
            "Zhihao Zhang",
            "Qiaole Dong",
            "Zhiheng Xi",
            "Jun Zhao",
            "Senjie Jin",
            "Xiaoran Fan",
            "Yuhao Zhou",
            "Yanwei Fu",
            "Qin Liu",
            "Songyang Zhang",
            "Qi Zhang"
        ],
        "comments": "26 pages",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "The reasoning capabilities of large language models (LLMs) have been a longstanding focus of research. Recent works have further enhanced these capabilities using reinforcement learning (RL), with many new methods claiming significant improvements with minimal or no external supervision. Surprisingly, some studies even suggest that random or incorrect reward signals can enhance reasoning performance. However, these breakthroughs are mostly reported on the Qwen2.5 model family and evaluated on well-known benchmarks such as MATH-500, AMC, and AIME, while failing to achieve similar gains on other models like Llama, which warrants further investigation. Our analysis shows that although Qwen2.5 achieves strong mathematical reasoning performance, its pretraining on large-scale web corpora makes it vulnerable to data contamination in popular benchmarks. As a result, results derived from these benchmarks may be unreliable. To address this, we introduce a generator that produces fully synthetic arithmetic problems of arbitrary length and difficulty, yielding a clean dataset we call RandomCalculation. Using these leakage-free datasets, we show that only accurate reward signals consistently improve performance, while noisy or incorrect signals do not. We advocate for evaluating RL methods on uncontaminated benchmarks and across diverse model families to ensure trustworthy conclusions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 158,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10536",
        "abs_url": "https://arxiv.org/abs/2507.10536",
        "pdf_url": "https://arxiv.org/pdf/2507.10536",
        "title": "On the Performance of Differentially Private Optimization with Heavy-Tail Class Imbalance",
        "authors": [
            "Qiaoyue Tang",
            "Alain Zhiyanov",
            "Mathias Lécuyer"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In this work, we analyze the optimization behaviour of common private learning optimization algorithms under heavy-tail class imbalanced distribution. We show that, in a stylized model, optimizing with Gradient Descent with differential privacy (DP-GD) suffers when learning low-frequency classes, whereas optimization algorithms that estimate second-order information do not. In particular, DP-AdamBC that removes the DP bias from estimating loss curvature is a crucial component to avoid the ill-condition caused by heavy-tail class imbalance, and empirically fits the data better with $\\approx8\\%$ and $\\approx5\\%$ increase in training accuracy when learning the least frequent classes on both controlled experiments and real data respectively.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 159,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10539",
        "abs_url": "https://arxiv.org/abs/2507.10539",
        "pdf_url": "https://arxiv.org/pdf/2507.10539",
        "title": "Graph World Model",
        "authors": [
            "Tao Feng",
            "Yexin Wu",
            "Guanyu Lin",
            "Jiaxuan You"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "World models (WMs) demonstrate strong capabilities in prediction, generation, and planning tasks. Existing WMs primarily focus on unstructured data and cannot leverage the ubiquitous structured data, often represented as graphs, in the digital world. While multiple graph foundation models have been proposed, they focus on graph learning tasks and cannot extend to diverse multi-modal data and interdisciplinary tasks. To address these challenges, we propose the Graph World Model (GWM), a world model that supports both unstructured and graph-structured states with multi-modal information and represents diverse tasks as actions. The core of a GWM is a generic message-passing algorithm to aggregate structured information, either over a unified multi-modal token space by converting multi-modal data into text (GWM-T) or a unified multi-modal embedding space by modality-specific encoders (GWM-E). Notably, GWM introduces action nodes to support diverse tasks, where action nodes are linked to other nodes via direct reference or similarity computation. Extensive experiments on six tasks from diverse domains, including multi-modal generation and matching, recommendation, graph prediction, multi-agent, retrieval-augmented generation, and planning and optimization, show that the same GWM outperforms or matches domain-specific baselines' performance, benefits from multi-hop structures, and demonstrates strong zero-shot/few-shot capabilities on unseen new tasks. Our code for GWM is released at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 160,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10540",
        "abs_url": "https://arxiv.org/abs/2507.10540",
        "pdf_url": "https://arxiv.org/pdf/2507.10540",
        "title": "Fusing LLM Capabilities with Routing Data",
        "authors": [
            "Tao Feng",
            "Haozhen Zhang",
            "Zijie Lei",
            "Pengrui Han",
            "Mostofa Patwary",
            "Mohammad Shoeybi",
            "Bryan Catanzaro",
            "Jiaxuan You"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The rapid advancement of large language models (LLMs) has created a vibrant ecosystem of diverse architectures, each with unique strengths due to differences in design, training data, and objectives. However, most applications still rely on a single backend model, limiting coverage of capabilities and leading to inefficiencies in performance and token cost when tackling complex tasks. We highlight an underexploited opportunity: LLM routing data, produced when hosting platforms route diverse queries to different models, which can reveal comparative strengths across tasks. To address this, we propose FusionBench, a comprehensive routing benchmark covering 14 tasks across five domains with 20 open-source LLMs (8B to 671B parameters), capturing 103M tokens and summarizing reusable thought templates from top models. Building on this, we introduce FusionFactory, a systematic fusion framework with three levels: (1) query-level fusion, tailoring routers for each query using both direct responses and reasoning-augmented outputs; (2) thought-level fusion, leveraging abstract templates derived from top-performing LLMs' answers to similar queries; and (3) model-level fusion, transferring capabilities between models via distillation, using top responses or highest judge scores as training data. Experiments show FusionFactory consistently outperforms the best individual LLM across all 14 benchmarks, with optimal fusion configurations varying by benchmark, demonstrating the value of systematic LLM fusion in harnessing complementary strengths and improving overall performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 161,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10546",
        "abs_url": "https://arxiv.org/abs/2507.10546",
        "pdf_url": "https://arxiv.org/pdf/2507.10546",
        "title": "Disentangling Neural Disjunctive Normal Form Models",
        "authors": [
            "Kexin Gu Baugh",
            "Vincent Perreault",
            "Matthew Baugh",
            "Luke Dickens",
            "Katsumi Inoue",
            "Alessandra Russo"
        ],
        "comments": "Accepted at NeSy 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Neural Disjunctive Normal Form (DNF) based models are powerful and interpretable approaches to neuro-symbolic learning and have shown promising results in classification and reinforcement learning settings without prior knowledge of the tasks. However, their performance is degraded by the thresholding of the post-training symbolic translation process. We show here that part of the performance degradation during translation is due to its failure to disentangle the learned knowledge represented in the form of the networks' weights. We address this issue by proposing a new disentanglement method; by splitting nodes that encode nested rules into smaller independent nodes, we are able to better preserve the models' performance. Through experiments on binary, multiclass, and multilabel classification tasks (including those requiring predicate invention), we demonstrate that our disentanglement method provides compact and interpretable logical representations for the neural DNF-based models, with performance closer to that of their pre-translation counterparts. Our code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 162,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2412.11407",
        "abs_url": "https://arxiv.org/abs/2412.11407",
        "pdf_url": "https://arxiv.org/pdf/2412.11407",
        "title": "An Enhanced Classification Method Based on Adaptive Multi-Scale Fusion for Long-tailed Multispectral Point Clouds",
        "authors": [
            "TianZhu Liu",
            "BangYan Hu",
            "YanFeng Gu",
            "Xian Li",
            "Aleksandra Pižurica"
        ],
        "comments": "16 pages, 9 figures, 5 tables",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Image and Video Processing (eess.IV)",
        "abstract": "Multispectral point cloud (MPC) captures 3D spatial-spectral information from the observed scene, which can be used for scene understanding and has a wide range of applications. However, most of the existing classification methods were extensively tested on indoor datasets, and when applied to outdoor datasets they still face problems including sparse labeled targets, differences in land-covers scales, and long-tailed distributions. To address the above issues, an enhanced classification method based on adaptive multi-scale fusion for MPCs with long-tailed distributions is proposed. In the training set generation stage, a grid-balanced sampling strategy is designed to reliably generate training samples from sparse labeled datasets. In the feature learning stage, a multi-scale feature fusion module is proposed to fuse shallow features of land-covers at different scales, addressing the issue of losing fine features due to scale variations in land-covers. In the classification stage, an adaptive hybrid loss module is devised to utilize multi-classification heads with adaptive weights to balance the learning ability of different classes, improving the classification performance of small classes due to various-scales and long-tailed distributions in land-covers. Experimental results on three MPC datasets demonstrate the effectiveness of the proposed method compared with the state-of-the-art methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 163,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08052",
        "abs_url": "https://arxiv.org/abs/2507.08052",
        "pdf_url": "https://arxiv.org/pdf/2507.08052",
        "title": "Lightweight Cloud Masking Models for On-Board Inference in Hyperspectral Imaging",
        "authors": [
            "Mazen Ali",
            "António Pereira",
            "Fabio Gentile",
            "Aser Cortines",
            "Sam Mugel",
            "Román Orús",
            "Stelios P. Neophytides",
            "Michalis Mavrovouniotis"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Cloud and cloud shadow masking is a crucial preprocessing step in hyperspectral satellite imaging, enabling the extraction of high-quality, analysis-ready data. This study evaluates various machine learning approaches, including gradient boosting methods such as XGBoost and LightGBM as well as convolutional neural networks (CNNs). All boosting and CNN models achieved accuracies exceeding 93%. Among the investigated models, the CNN with feature reduction emerged as the most efficient, offering a balance of high accuracy, low storage requirements, and rapid inference times on both CPUs and GPUs. Variations of this version, with only up to 597 trainable parameters, demonstrated the best trade-off in terms of deployment feasibility, accuracy, and computational efficiency. These results demonstrate the potential of lightweight artificial intelligence (AI) models for real-time hyperspectral image processing, supporting the development of on-board satellite AI systems for space-based applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 164,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08806",
        "abs_url": "https://arxiv.org/abs/2507.08806",
        "pdf_url": "https://arxiv.org/pdf/2507.08806",
        "title": "Think Clearly: Improving Reasoning via Redundant Token Pruning",
        "authors": [
            "Daewon Choi",
            "Jimin Lee",
            "Jihoon Tack",
            "Woomin Song",
            "Saket Dingliwal",
            "Sai Muralidhar Jayanthi",
            "Bhavana Ganesh",
            "Jinwoo Shin",
            "Aram Galstyan",
            "Sravan Babu Bodapati"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Recent large language models have shown promising capabilities in long-form reasoning, following structured chains of thought before arriving at a final answer. However, we observe that these reasoning paths tend to include substantial redundancy; analyzing attention patterns reveals that attention scores are widely scattered, particularly incorrect answers exhibit greater attention sparsity. In this paper, we demonstrate that deliberately removing this redundancy in the reasoning process significantly improves performance through clear thinking, i.e., removing distraction. Specifically, we systematically identify reasoning redundancy by measuring token-level attention scores to a special end-of-thinking token, which is appended to an explicit instruction inserted to conclude each intermediate reasoning step. Furthermore, we propose structure-aware pruning that prioritizes removing tokens in low-contributing reasoning chunks over individual tokens. After evicting redundant tokens, we remove the injected end-of-thinking instruction, then resume the reasoning generation. We demonstrate that our method significantly improves overall accuracy across reasoning-intensive benchmarks without any training involved. In particular, our method shows strong performance on challenging mathematical competition benchmarks such as AIME and AMC, where reasoning redundancy is more prevalent.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 165,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08821",
        "abs_url": "https://arxiv.org/abs/2507.08821",
        "pdf_url": "https://arxiv.org/pdf/2507.08821",
        "title": "LNN-powered Fluid Antenna Multiple Access",
        "authors": [
            "Pedro D. Alvim",
            "Hugerles S. Silva",
            "Ugo S. Dias",
            "Osamah S. Badarneh",
            "Felipe A. P. Figueiredo",
            "Rausley A. A. de Souza"
        ],
        "comments": "",
        "subjects": "Signal Processing (eess.SP); Information Theory (cs.IT); Machine Learning (cs.LG)",
        "abstract": "Fluid antenna systems represent an innovative approach in wireless communication, recently applied in multiple access to optimize the signal-to-interference-plus-noise ratio through port selection. This letter frames the port selection problem as a multi-label classification task for the first time, improving best-port selection with limited port observations. We address this challenge by leveraging liquid neural networks (LNNs) to predict the optimal port under emerging fluid antenna multiple access scenarios alongside a more general $\\alpha$-$\\mu$ fading model. We also apply hyperparameter optimization to refine LNN architectures for different observation scenarios. Our approach yields lower outage probability values than existing methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 166,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08831",
        "abs_url": "https://arxiv.org/abs/2507.08831",
        "pdf_url": "https://arxiv.org/pdf/2507.08831",
        "title": "View Invariant Learning for Vision-Language Navigation in Continuous Environments",
        "authors": [
            "Josh Qixuan Sun",
            "Xiaoying Xing",
            "Huaiyuan Weng",
            "Chul Min Yeum",
            "Mark Crowley"
        ],
        "comments": "Under review",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Robotics (cs.RO)",
        "abstract": "Vision-Language Navigation in Continuous Environments (VLNCE), where an agent follows instructions and moves freely to reach a destination, is a key research problem in embodied AI. However, most navigation policies are sensitive to viewpoint changes, i.e., variations in camera height and viewing angle that alter the agent's observation. In this paper, we introduce a generalized scenario, V2-VLNCE (VLNCE with Varied Viewpoints), and propose VIL (View Invariant Learning), a view-invariant post-training strategy that enhances the robustness of existing navigation policies to changes in camera viewpoint. VIL employs a contrastive learning framework to learn sparse and view-invariant features. Additionally, we introduce a teacher-student framework for the Waypoint Predictor Module, a core component of most VLNCE baselines, where a view-dependent teacher model distills knowledge into a view-invariant student model. We employ an end-to-end training paradigm to jointly optimize these components, thus eliminating the cost for individual module training. Empirical results show that our method outperforms state-of-the-art approaches on V2-VLNCE by 8-15% measured on Success Rate for two standard benchmark datasets R2R-CE and RxR-CE. Furthermore, we evaluate VIL under the standard VLNCE setting and find that, despite being trained for varied viewpoints, it often still improves performance. On the more challenging RxR-CE dataset, our method also achieved state-of-the-art performance across all metrics when compared to other map-free methods. This suggests that adding VIL does not diminish the standard viewpoint performance and can serve as a plug-and-play post-training method.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 167,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08849",
        "abs_url": "https://arxiv.org/abs/2507.08849",
        "pdf_url": "https://arxiv.org/pdf/2507.08849",
        "title": "Counterfactual optimization for fault prevention in complex wind energy systems",
        "authors": [
            "Emilio Carrizosa",
            "Martina Fischetti",
            "Roshell Haaker",
            "Juan Miguel Morales"
        ],
        "comments": "",
        "subjects": "Systems and Control (eess.SY); Machine Learning (cs.LG); Optimization and Control (math.OC)",
        "abstract": "Machine Learning models are increasingly used in businesses to detect faults and anomalies in complex systems. In this work, we take this approach a step further: beyond merely detecting anomalies, we aim to identify the optimal control strategy that restores the system to a safe state with minimal disruption. We frame this challenge as a counterfactual problem: given a Machine Learning model that classifies system states as either good or anomalous, our goal is to determine the minimal adjustment to the system's control variables (i.e., its current status) that is necessary to return it to the good state. To achieve this, we leverage a mathematical model that finds the optimal counterfactual solution while respecting system specific constraints. Notably, most counterfactual analysis in the literature focuses on individual cases where a person seeks to alter their status relative to a decision made by a classifier, such as for loan approval or medical diagnosis. Our work addresses a fundamentally different challenge: optimizing counterfactuals for a complex energy system, specifically an offshore wind turbine oil type transformer. This application not only advances counterfactual optimization in a new domain but also opens avenues for broader research in this area. Our tests on real world data provided by our industrial partner show that our methodology easily adapts to user preferences and brings savings in the order of 3 million euros per year in a typical farm.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 168,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08854",
        "abs_url": "https://arxiv.org/abs/2507.08854",
        "pdf_url": "https://arxiv.org/pdf/2507.08854",
        "title": "DiffNMR: Diffusion Models for Nuclear Magnetic Resonance Spectra Elucidation",
        "authors": [
            "Qingsong Yang",
            "Binglan Wu",
            "Xuwei Liu",
            "Bo Chen",
            "Wei Li",
            "Gen Long",
            "Xin Chen",
            "Mingjun Xiao"
        ],
        "comments": "",
        "subjects": "Chemical Physics (physics.chem-ph); Machine Learning (cs.LG)",
        "abstract": "Nuclear Magnetic Resonance (NMR) spectroscopy is a central characterization method for molecular structure elucidation, yet interpreting NMR spectra to deduce molecular structures remains challenging due to the complexity of spectral data and the vastness of the chemical space. In this work, we introduce DiffNMR, a novel end-to-end framework that leverages a conditional discrete diffusion model for de novo molecular structure elucidation from NMR spectra. DiffNMR refines molecular graphs iteratively through a diffusion-based generative process, ensuring global consistency and mitigating error accumulation inherent in autoregressive methods. The framework integrates a two-stage pretraining strategy that aligns spectral and molecular representations via diffusion autoencoder (Diff-AE) and contrastive learning, the incorporation of retrieval initialization and similarity filtering during inference, and a specialized NMR encoder with radial basis function (RBF) encoding for chemical shifts, preserving continuity and chemical correlation. Experimental results demonstrate that DiffNMR achieves competitive performance for NMR-based structure elucidation, offering an efficient and robust solution for automated molecular analysis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 169,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08855",
        "abs_url": "https://arxiv.org/abs/2507.08855",
        "pdf_url": "https://arxiv.org/pdf/2507.08855",
        "title": "Multi-omic Prognosis of Alzheimer's Disease with Asymmetric Cross-Modal Cross-Attention Network",
        "authors": [
            "Yang Ming",
            "Jiang Shi Zhong",
            "Zhou Su Juan"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Alzheimer's Disease (AD) is an irreversible neurodegenerative disease characterized by progressive cognitive decline as its main symptom. In the research field of deep learning-assisted diagnosis of AD, traditional convolutional neural networks and simple feature concatenation methods fail to effectively utilize the complementary information between multimodal data, and the simple feature concatenation approach is prone to cause the loss of key information during the process of modal fusion. In recent years, the development of deep learning technology has brought new possibilities for solving the problem of how to effectively fuse multimodal features. This paper proposes a novel deep learning algorithm framework to assist medical professionals in AD diagnosis. By fusing medical multi-view information such as brain fluorodeoxyglucose positron emission tomography (PET), magnetic resonance imaging (MRI), genetic data, and clinical data, it can accurately detect the presence of AD, Mild Cognitive Impairment (MCI), and Cognitively Normal (CN). The innovation of the algorithm lies in the use of an asymmetric cross-modal cross-attention mechanism, which can effectively capture the key information features of the interactions between different data modal features. This paper compares the asymmetric cross-modal cross-attention mechanism with the traditional algorithm frameworks of unimodal and multimodal deep learning models for AD diagnosis, and evaluates the importance of the asymmetric cross-modal cross-attention mechanism. The algorithm model achieves an accuracy of 94.88% on the test set.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 170,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08867",
        "abs_url": "https://arxiv.org/abs/2507.08867",
        "pdf_url": "https://arxiv.org/pdf/2507.08867",
        "title": "Mind the Gap: Navigating Inference with Optimal Transport Maps",
        "authors": [
            "Malte Algren",
            "Tobias Golling",
            "Francesco Armando Di Bello",
            "Christopher Pollard"
        ],
        "comments": "23 pages, 13 figures",
        "subjects": "Data Analysis, Statistics and Probability (physics.data-an); Machine Learning (cs.LG); High Energy Physics - Experiment (hep-ex); Machine Learning (stat.ML)",
        "abstract": "Machine learning (ML) techniques have recently enabled enormous gains in sensitivity across the sciences. In particle physics, much of this progress has relied on excellent simulations of a wide range of physical processes. However, due to the sophistication of modern machine learning (ML) algorithms and their reliance on high-quality training samples, discrepancies between simulation and experimental data can significantly limit the effectiveness of ML techniques. In this work, we present a solution to this ``mis-specification'' problem: a calibration approach based on optimal transport, which we apply to high-dimensional simulations for the first time. We demonstrate the performance of our approach through jet tagging, using a CMS-inspired dataset. A 128-dimensional internal jet representation from a powerful general-purpose classifier is studied; after calibrating this internal ``latent'' representation, we find that a wide variety of quantities derived from it for downstream tasks are also properly calibrated: using this calibrated high-dimensional representation, powerful new applications of jet flavor information can be utilized in LHC analyses. This is a key step toward allowing properly-calibrated ``foundation models'' in particle physics. More broadly, this calibration framework has broad applications for correcting high-dimensional simulations across the sciences.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 171,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08896",
        "abs_url": "https://arxiv.org/abs/2507.08896",
        "pdf_url": "https://arxiv.org/pdf/2507.08896",
        "title": "Predictive Causal Inference via Spatio-Temporal Modeling and Penalized Empirical Likelihood",
        "authors": [
            "Byunghee Lee",
            "Hye Yeon Sin",
            "Joonsung Kang"
        ],
        "comments": "",
        "subjects": "Methodology (stat.ME); Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "This study introduces an integrated framework for predictive causal inference designed to overcome limitations inherent in conventional single model approaches. Specifically, we combine a Hidden Markov Model (HMM) for spatial health state estimation with a Multi Task and Multi Graph Convolutional Network (MTGCN) for capturing temporal outcome trajectories. The framework asymmetrically treats temporal and spatial information regarding them as endogenous variables in the outcome regression, and exogenous variables in the propensity score model, thereby expanding the standard doubly robust treatment effect estimation to jointly enhance bias correction and predictive accuracy. To demonstrate its utility, we focus on clinical domains such as cancer, dementia, and Parkinson disease, where treatment effects are challenging to observe directly. Simulation studies are conducted to emulate latent disease dynamics and evaluate the model performance under varying conditions. Overall, the proposed framework advances predictive causal inference by structurally adapting to spatiotemporal complexities common in biomedical data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 172,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08906",
        "abs_url": "https://arxiv.org/abs/2507.08906",
        "pdf_url": "https://arxiv.org/pdf/2507.08906",
        "title": "Physics-informed machine learning: A mathematical framework with applications to time series forecasting",
        "authors": [
            "Nathan Doumèche"
        ],
        "comments": "Doctoral thesis, Sorbonne University. 286 pages",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Statistics Theory (math.ST); Methodology (stat.ME)",
        "abstract": "Physics-informed machine learning (PIML) is an emerging framework that integrates physical knowledge into machine learning models. This physical prior often takes the form of a partial differential equation (PDE) system that the regression function must satisfy. In the first part of this dissertation, we analyze the statistical properties of PIML methods. In particular, we study the properties of physics-informed neural networks (PINNs) in terms of approximation, consistency, overfitting, and convergence. We then show how PIML problems can be framed as kernel methods, making it possible to apply the tools of kernel ridge regression to better understand their behavior. In addition, we use this kernel formulation to develop novel physics-informed algorithms and implement them efficiently on GPUs. The second part explores industrial applications in forecasting energy signals during atypical periods. We present results from the Smarter Mobility challenge on electric vehicle charging occupancy and examine the impact of mobility on electricity demand. Finally, we introduce a physics-constrained framework for designing and enforcing constraints in time series, applying it to load forecasting and tourism forecasting in various countries.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 173,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08908",
        "abs_url": "https://arxiv.org/abs/2507.08908",
        "pdf_url": "https://arxiv.org/pdf/2507.08908",
        "title": "The Engineer's Dilemma: A Review of Establishing a Legal Framework for Integrating Machine Learning in Construction by Navigating Precedents and Industry Expectations",
        "authors": [
            "M.Z. Naser"
        ],
        "comments": "",
        "subjects": "Computers and Society (cs.CY); Emerging Technologies (cs.ET); Machine Learning (cs.LG)",
        "abstract": "Despite the widespread interest in machine learning (ML), the engineering industry has not yet fully adopted ML-based methods, which has left engineers and stakeholders uncertain about the legal and regulatory frameworks that govern their decisions. This gap remains unaddressed as an engineer's decision-making process, typically governed by professional ethics and practical guidelines, now intersects with complex algorithmic outputs. To bridge this gap, this paper explores how engineers can navigate legal principles and legislative justifications that support and/or contest the deployment of ML technologies. Drawing on recent precedents and experiences gained from other fields, this paper argues that analogical reasoning can provide a basis for embedding ML within existing engineering codes while maintaining professional accountability and meeting safety requirements. In exploring these issues, the discussion focuses on established liability doctrines, such as negligence and product liability, and highlights how courts have evaluated the use of predictive models. We further analyze how legislative bodies and standard-setting organizations can furnish explicit guidance equivalent to prior endorsements of emergent technologies. This exploration stresses the vitality of understanding the interplay between technical justifications and legal precedents for shaping an informed stance on ML's legitimacy in engineering practice. Finally, our analysis catalyzes a legal framework for integrating ML through which stakeholders can critically assess the responsibilities, liabilities, and benefits inherent in ML-driven engineering solutions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 174,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08922",
        "abs_url": "https://arxiv.org/abs/2507.08922",
        "pdf_url": "https://arxiv.org/pdf/2507.08922",
        "title": "The Bayesian Approach to Continual Learning: An Overview",
        "authors": [
            "Tameem Adel"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Continual learning is an online paradigm where a learner continually accumulates knowledge from different tasks encountered over sequential time steps. Importantly, the learner is required to extend and update its knowledge without forgetting about the learning experience acquired from the past, and while avoiding the need to retrain from scratch. Given its sequential nature and its resemblance to the way humans think, continual learning offers an opportunity to address several challenges which currently stand in the way of widening the range of applicability of deep models to further real-world problems. The continual need to update the learner with data arriving sequentially strikes inherent congruence between continual learning and Bayesian inference which provides a principal platform to keep updating the prior beliefs of a model given new data, without completely forgetting the knowledge acquired from the old data. This survey inspects different settings of Bayesian continual learning, namely task-incremental learning and class-incremental learning. We begin by discussing definitions of continual learning along with its Bayesian setting, as well as the links with related fields, such as domain adaptation, transfer learning and meta-learning. Afterwards, we introduce a taxonomy offering a comprehensive categorization of algorithms belonging to the Bayesian continual learning paradigm. Meanwhile, we analyze the state-of-the-art while zooming in on some of the most prominent Bayesian continual learning algorithms to date. Furthermore, we shed some light on links between continual learning and developmental psychology, and correspondingly introduce analogies between both fields. We follow that with a discussion of current challenges, and finally conclude with potential areas for future research on Bayesian continual learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 175,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08960",
        "abs_url": "https://arxiv.org/abs/2507.08960",
        "pdf_url": "https://arxiv.org/pdf/2507.08960",
        "title": "How to Train a Leader: Hierarchical Reasoning in Multi-Agent LLMs",
        "authors": [
            "Andrew Estornell",
            "Jean-Francois Ton",
            "Muhammad Faaiz Taufiq",
            "Hang Li"
        ],
        "comments": "",
        "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Large Language Models (LLMs) have achieved strong performance on a wide range of complex reasoning tasks, yet further gains are often possible by leveraging the complementary strengths of multiple models. While multi-agent frameworks can improve solution quality by leveraging multiple LLMs, existing methods are often computationally expensive, both at training and inference time. In this work, we introduce a hierarchical multi-agent framework that addresses these challenges by training only a single leader LLM to coordinate a team of untrained peer agents. To this end, we propose Multi-agent guided Leader Policy \\textbf{O}ptimization (MLPO), a novel approach which trains the leader to evaluate and synthesize agent responses without auxiliary value networks or explicit agent feedback. Leaders trained with MLPO exhibit improved performance not only when interacting with the agent team at inference time, but also enjoy improved performance when deployed in single-agent settings without the team. Empirical results on Big-Bench Hard (BBH), MATH, and MMLU demonstrate that our framework achieves substantial performance improvements over both single-agent and multi-agent baselines. Our results highlight the effectiveness and efficiency of training a single, flexible leader for collaborative reasoning in multi-agent LLM systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 176,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08963",
        "abs_url": "https://arxiv.org/abs/2507.08963",
        "pdf_url": "https://arxiv.org/pdf/2507.08963",
        "title": "Stochastic Approximation with Block Coordinate Optimal Stepsizes",
        "authors": [
            "Tao Jiang",
            "Lin Xiao"
        ],
        "comments": "",
        "subjects": "Optimization and Control (math.OC); Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "We consider stochastic approximation with block-coordinate stepsizes and propose adaptive stepsize rules that aim to minimize the expected distance from the next iterate to an optimal point. These stepsize rules employ online estimates of the second moment of the search direction along each block coordinate. The popular Adam algorithm can be interpreted as a particular heuristic for such estimation. By leveraging a simple conditional estimator, we derive a new method that obtains comparable performance as Adam but requires less memory and fewer hyper-parameters. We prove that this family of methods converges almost surely to a small neighborhood of the optimal point, and the radius of the neighborhood depends on the bias and variance of the second-moment estimator. Our analysis relies on a simple aiming condition that assumes neither convexity nor smoothness, thus has broad applicability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 177,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08979",
        "abs_url": "https://arxiv.org/abs/2507.08979",
        "pdf_url": "https://arxiv.org/pdf/2507.08979",
        "title": "PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with LLM-Guided Embedding Projection",
        "authors": [
            "Mahdiyar Molahasani",
            "Azadeh Motamedi",
            "Michael Greenspan",
            "Il-Min Kim",
            "Ali Etemad"
        ],
        "comments": "Accepted to ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "We introduce Projection-based Reduction of Implicit Spurious bias in vision-language Models (PRISM), a new data-free and task-agnostic solution for bias mitigation in VLMs like CLIP. VLMs often inherit and amplify biases in their training data, leading to skewed predictions. PRISM is designed to debias VLMs without relying on predefined bias categories or additional external data. It operates in two stages: first, an LLM is prompted with simple class prompts to generate scene descriptions that contain spurious correlations. Next, PRISM uses our novel contrastive-style debiasing loss to learn a projection that maps the embeddings onto a latent space that minimizes spurious correlations while preserving the alignment between image and text this http URL experiments demonstrate that PRISM outperforms current debiasing methods on the commonly used Waterbirds and CelebA datasets We make our code public at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 178,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08982",
        "abs_url": "https://arxiv.org/abs/2507.08982",
        "pdf_url": "https://arxiv.org/pdf/2507.08982",
        "title": "VIP: Visual Information Protection through Adversarial Attacks on Vision-Language Models",
        "authors": [
            "Hanene F. Z. Brachemi Meftah",
            "Wassim Hamidouche",
            "Sid Ahmed Fezza",
            "Olivier Déforges"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Recent years have witnessed remarkable progress in developing Vision-Language Models (VLMs) capable of processing both textual and visual inputs. These models have demonstrated impressive performance, leading to their widespread adoption in various applications. However, this widespread raises serious concerns regarding user privacy, particularly when models inadvertently process or expose private visual information. In this work, we frame the preservation of privacy in VLMs as an adversarial attack problem. We propose a novel attack strategy that selectively conceals information within designated Region Of Interests (ROIs) in an image, effectively preventing VLMs from accessing sensitive content while preserving the semantic integrity of the remaining image. Unlike conventional adversarial attacks that often disrupt the entire image, our method maintains high coherence in unmasked areas. Experimental results across three state-of-the-art VLMs namely LLaVA, Instruct-BLIP, and BLIP2-T5 demonstrate up to 98% reduction in detecting targeted ROIs, while maintaining global image semantics intact, as confirmed by high similarity scores between clean and adversarial outputs. We believe that this work contributes to a more privacy conscious use of multimodal models and offers a practical tool for further research, with the source code publicly available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 179,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08986",
        "abs_url": "https://arxiv.org/abs/2507.08986",
        "pdf_url": "https://arxiv.org/pdf/2507.08986",
        "title": "Physics-Based Machine Learning Closures and Wall Models for Hypersonic Transition-Continuum Boundary Layer Predictions",
        "authors": [
            "Ashish S. Nair",
            "Narendra Singh",
            "Marco Panesi",
            "Justin Sirignano",
            "Jonathan F. MacArt"
        ],
        "comments": "",
        "subjects": "Fluid Dynamics (physics.flu-dyn); Machine Learning (cs.LG)",
        "abstract": "Modeling rarefied hypersonic flows remains a fundamental challenge due to the breakdown of classical continuum assumptions in the transition-continuum regime, where the Knudsen number ranges from approximately 0.1 to 10. Conventional Navier-Stokes-Fourier (NSF) models with empirical slip-wall boundary conditions fail to accurately predict nonequilibrium effects such as velocity slip, temperature jump, and shock structure deviations. We develop a physics-constrained machine learning framework that augments transport models and boundary conditions to extend the applicability of continuum solvers in nonequilibrium hypersonic regimes. We employ deep learning PDE models (DPMs) for the viscous stress and heat flux embedded in the governing PDEs and trained via adjoint-based optimization. We evaluate these for two-dimensional supersonic flat-plate flows across a range of Mach and Knudsen numbers. Additionally, we introduce a wall model based on a mixture of skewed Gaussian approximations of the particle velocity distribution function. This wall model replaces empirical slip conditions with physically informed, data-driven boundary conditions for the streamwise velocity and wall temperature. Our results show that a trace-free anisotropic viscosity model, paired with the skewed-Gaussian distribution function wall model, achieves significantly improved accuracy, particularly at high-Mach and high-Knudsen number regimes. Strategies such as parallel training across multiple Knudsen numbers and inclusion of high-Mach data during training are shown to enhance model generalization. Increasing model complexity yields diminishing returns for out-of-sample cases, underscoring the need to balance degrees of freedom and overfitting. This work establishes data-driven, physics-consistent strategies for improving hypersonic flow modeling for regimes in which conventional continuum approaches are invalid.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 180,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.08994",
        "abs_url": "https://arxiv.org/abs/2507.08994",
        "pdf_url": "https://arxiv.org/pdf/2507.08994",
        "title": "Fixed-Confidence Multiple Change Point Identification under Bandit Feedback",
        "authors": [
            "Joseph Lazzaro",
            "Ciara Pike-Burke"
        ],
        "comments": "ICML 2025",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Piecewise constant functions describe a variety of real-world phenomena in domains ranging from chemistry to manufacturing. In practice, it is often required to confidently identify the locations of the abrupt changes in these functions as quickly as possible. For this, we introduce a fixed-confidence piecewise constant bandit problem. Here, we sequentially query points in the domain and receive noisy evaluations of the function under bandit feedback. We provide instance-dependent lower bounds for the complexity of change point identification in this problem. These lower bounds illustrate that an optimal method should focus its sampling efforts adjacent to each of the change points, and the number of samples around each change point should be inversely proportional to the magnitude of the change. Building on this, we devise a simple and computationally efficient variant of Track-and-Stop and prove that it is asymptotically optimal in many regimes. We support our theoretical findings with experimental results in synthetic environments demonstrating the efficiency of our method.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 181,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09001",
        "abs_url": "https://arxiv.org/abs/2507.09001",
        "pdf_url": "https://arxiv.org/pdf/2507.09001",
        "title": "Surprisingly High Redundancy in Electronic Structure Data",
        "authors": [
            "Sazzad Hossain",
            "Ponkrshnan Thiagarajan",
            "Shashank Pathrudkar",
            "Stephanie Taylor",
            "Abhijeet S. Gangan",
            "Amartya S. Banerjee",
            "Susanta Ghosh"
        ],
        "comments": "",
        "subjects": "Materials Science (cond-mat.mtrl-sci); Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning (cs.LG); Computational Physics (physics.comp-ph); Quantum Physics (quant-ph)",
        "abstract": "Machine Learning (ML) models for electronic structure rely on large datasets generated through expensive Kohn-Sham Density Functional Theory simulations. This study reveals a surprisingly high level of redundancy in such datasets across various material systems, including molecules, simple metals, and complex alloys. Our findings challenge the prevailing assumption that large, exhaustive datasets are necessary for accurate ML predictions of electronic structure. We demonstrate that even random pruning can substantially reduce dataset size with minimal loss in predictive accuracy, while a state-of-the-art coverage-based pruning strategy retains chemical accuracy and model generalizability using up to 100-fold less data and reducing training time by threefold or more. By contrast, widely used importance-based pruning methods, which eliminate seemingly redundant data, can catastrophically fail at higher pruning factors, possibly due to the significant reduction in data coverage. This heretofore unexplored high degree of redundancy in electronic structure data holds the potential to identify a minimal, essential dataset representative of each material class.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 182,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09025",
        "abs_url": "https://arxiv.org/abs/2507.09025",
        "pdf_url": "https://arxiv.org/pdf/2507.09025",
        "title": "Lizard: An Efficient Linearization Framework for Large Language Models",
        "authors": [
            "Chien Van Nguyen",
            "Ruiyi Zhang",
            "Hanieh Deilamsalehy",
            "Puneet Mathur",
            "Viet Dac Lai",
            "Haoliang Wang",
            "Jayakumar Subramanian",
            "Ryan A. Rossi",
            "Trung Bui",
            "Nikos Vlassis",
            "Franck Dernoncourt",
            "Thien Huu Nguyen"
        ],
        "comments": "15 pages",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "We propose Lizard, a linearization framework that transforms pretrained Transformer-based Large Language Models (LLMs) into flexible, subquadratic architectures for infinite-context generation. Transformer-based LLMs face significant memory and computational bottlenecks as context lengths increase, due to the quadratic complexity of softmax attention and the growing key-value (KV) cache. Lizard addresses these limitations by introducing a subquadratic attention mechanism that closely approximates softmax attention while preserving the output quality. Unlike previous linearization methods, which are often limited by fixed model structures and therefore exclude gating mechanisms, Lizard incorporates a gating module inspired by recent state-of-the-art linear models. This enables adaptive memory control, supports constant-memory inference, offers strong length generalization, and allows more flexible model design. Lizard combines gated linear attention for global context compression with sliding window attention enhanced by meta memory, forming a hybrid mechanism that captures both long-range dependencies and fine-grained local interactions. Moreover, we introduce a hardware-aware algorithm that accelerates the training speed of our models. Extensive experiments show that Lizard achieves near-lossless recovery of the teacher model's performance across standard language modeling tasks, while significantly outperforming previous linearization methods. On the 5-shot MMLU benchmark, Lizard improves over prior models by 18 points and shows significant improvements on associative recall tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 183,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09026",
        "abs_url": "https://arxiv.org/abs/2507.09026",
        "pdf_url": "https://arxiv.org/pdf/2507.09026",
        "title": "On the Gradient Domination of the LQG Problem",
        "authors": [
            "Kasra Fallah",
            "Leonardo F. Toso",
            "James Anderson"
        ],
        "comments": "",
        "subjects": "Optimization and Control (math.OC); Machine Learning (cs.LG); Systems and Control (eess.SY)",
        "abstract": "We consider solutions to the linear quadratic Gaussian (LQG) regulator problem via policy gradient (PG) methods. Although PG methods have demonstrated strong theoretical guarantees in solving the linear quadratic regulator (LQR) problem, despite its nonconvex landscape, their theoretical understanding in the LQG setting remains limited. Notably, the LQG problem lacks gradient dominance in the classical parameterization, i.e., with a dynamic controller, which hinders global convergence guarantees. In this work, we study PG for the LQG problem by adopting an alternative parameterization of the set of stabilizing controllers and employing a lifting argument. We refer to this parameterization as a history representation of the control input as it is parameterized by past input and output data from the previous p time-steps. This representation enables us to establish gradient dominance and approximate smoothness for the LQG cost. We prove global convergence and per-iteration stability guarantees for policy gradient LQG in model-based and model-free settings. Numerical experiments on an open-loop unstable system are provided to support the global convergence guarantees and to illustrate convergence under different history lengths of the history representation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 184,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09036",
        "abs_url": "https://arxiv.org/abs/2507.09036",
        "pdf_url": "https://arxiv.org/pdf/2507.09036",
        "title": "BrainLesion Suite: A Flexible and User-Friendly Framework for Modular Brain Lesion Image Analysis",
        "authors": [
            "Florian Kofler",
            "Marcel Rosier",
            "Mehdi Astaraki",
            "Hendrik Möller",
            "Ilhem Isra Mekki",
            "Josef A. Buchner",
            "Anton Schmick",
            "Arianna Pfiffer",
            "Eva Oswald",
            "Lucas Zimmer",
            "Ezequiel de la Rosa",
            "Sarthak Pati",
            "Julian Canisius",
            "Arianna Piffer",
            "Ujjwal Baid",
            "Mahyar Valizadeh",
            "Akis Linardos",
            "Jan C. Peeken",
            "Surprosanna Shit",
            "Felix Steinbauer",
            "Daniel Rueckert",
            "Rolf Heckemann",
            "Spyridon Bakas",
            "Jan Kirschke",
            "Constantin von See",
            "Ivan Ezhov",
            "Marie Piraud",
            "Benedikt Wiestler",
            "Bjoern Menze"
        ],
        "comments": "16p, 3f",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Software Engineering (cs.SE)",
        "abstract": "BrainLesion Suite is a versatile toolkit for building modular brain lesion image analysis pipelines in Python. Following Pythonic principles, BrainLesion Suite is designed to provide a 'brainless' development experience, minimizing cognitive effort and streamlining the creation of complex workflows for clinical and scientific practice. At its core is an adaptable preprocessing module that performs co-registration, atlas registration, and optional skull-stripping and defacing on arbitrary multi-modal input images. BrainLesion Suite leverages algorithms from the BraTS challenge to synthesize missing modalities, inpaint lesions, and generate pathology-specific tumor segmentations. BrainLesion Suite also enables quantifying segmentation model performance, with tools such as panoptica to compute lesion-wise metrics. Although BrainLesion Suite was originally developed for image analysis pipelines of brain lesions such as glioma, metastasis, and multiple sclerosis, it can be adapted for other biomedical image analysis applications. The individual BrainLesion Suite packages and tutorials are accessible on GitHub.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 185,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09050",
        "abs_url": "https://arxiv.org/abs/2507.09050",
        "pdf_url": "https://arxiv.org/pdf/2507.09050",
        "title": "A Method for Learning to Solve Parametric Bilevel Optimization with Coupling Constraints",
        "authors": [
            "James Kotary",
            "Himanshu Sharma",
            "Ethan King",
            "Draguna Vrabie",
            "Ferdinando Fioretto",
            "Jan Drgona"
        ],
        "comments": "",
        "subjects": "Optimization and Control (math.OC); Machine Learning (cs.LG)",
        "abstract": "Learning to Optimize (L2O) is a subfield of machine learning (ML) in which ML models are trained to solve parametric optimization problems. The general goal is to learn a fast approximator of solutions to constrained optimization problems, as a function of their defining parameters. Prior L2O methods focus almost entirely on single-level programs, in contrast to the bilevel programs, whose constraints are themselves expressed in terms of optimization subproblems. Bilevel programs have numerous important use cases but are notoriously difficult to solve, particularly under stringent time demands. This paper proposes a framework for learning to solve a broad class of challenging bilevel optimization problems, by leveraging modern techniques for differentiation through optimization problems. The framework is illustrated on an array of synthetic bilevel programs, as well as challenging control system co-design problems, showing how neural networks can be trained as efficient approximators of parametric bilevel optimization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 186,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09052",
        "abs_url": "https://arxiv.org/abs/2507.09052",
        "pdf_url": "https://arxiv.org/pdf/2507.09052",
        "title": "Can Contrastive Learning Improve Class-Imbalanced Diffusion Model?",
        "authors": [
            "Fang Chen",
            "Alex Villa",
            "Gongbo Liang",
            "Xiaoyi Lu",
            "Meng Tang"
        ],
        "comments": "20 pages, 11 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Training data for class-conditional image synthesis often exhibit a long-tailed distribution with limited images for tail classes. Such an imbalance causes mode collapse and reduces the diversity of synthesized images for tail classes. For class-conditional diffusion models trained on imbalanced data, we aim to improve the diversity of tail class images without compromising the fidelity and diversity of head class images. We achieve this by introducing two deceptively simple but highly effective contrastive loss functions. Firstly, we employ an unsupervised InfoNCE loss utilizing negative samples to increase the distance/dissimilarity among synthetic images, particularly for tail classes. To further enhance the diversity of tail classes, our second loss is an MSE loss that contrasts class-conditional generation with unconditional generation at large timesteps. This second loss makes the denoising process insensitive to class conditions for the initial steps, which enriches tail classes through knowledge sharing from head classes. Conditional-unconditional alignment has been shown to enhance the performance of long-tailed GAN. We are the first to adapt such alignment to diffusion models. We successfully leveraged contrastive learning for class-imbalanced diffusion models. Our contrastive learning framework is easy to implement and outperforms standard DDPM and alternative methods for class-imbalanced diffusion models across various datasets, including CIFAR10/100-LT, PlacesLT, TinyImageNetLT, and ImageNetLT.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 187,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09054",
        "abs_url": "https://arxiv.org/abs/2507.09054",
        "pdf_url": "https://arxiv.org/pdf/2507.09054",
        "title": "Conformation-Aware Structure Prediction of Antigen-Recognizing Immune Proteins",
        "authors": [
            "Frédéric A. Dreyer",
            "Jan Ludwiczak",
            "Karolis Martinkus",
            "Brennan Abanades",
            "Robert G. Alberstein",
            "Pan Kessel",
            "Pranav Rao",
            "Jae Hyeon Lee",
            "Richard Bonneau",
            "Andrew M. Watkins",
            "Franziska Seeger"
        ],
        "comments": "17 pages, 12 figures, 2 tables, code at this https URL, model weights at this https URL",
        "subjects": "Biomolecules (q-bio.BM); Machine Learning (cs.LG)",
        "abstract": "We introduce Ibex, a pan-immunoglobulin structure prediction model that achieves state-of-the-art accuracy in modeling the variable domains of antibodies, nanobodies, and T-cell receptors. Unlike previous approaches, Ibex explicitly distinguishes between bound and unbound protein conformations by training on labeled apo and holo structural pairs, enabling accurate prediction of both states at inference time. Using a comprehensive private dataset of high-resolution antibody structures, we demonstrate superior out-of-distribution performance compared to existing specialized and general protein structure prediction tools. Ibex combines the accuracy of cutting-edge models with significantly reduced computational requirements, providing a robust foundation for accelerating large molecule design and therapeutic development.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 188,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09063",
        "abs_url": "https://arxiv.org/abs/2507.09063",
        "pdf_url": "https://arxiv.org/pdf/2507.09063",
        "title": "SetupBench: Assessing Software Engineering Agents' Ability to Bootstrap Development Environments",
        "authors": [
            "Avi Arora",
            "Jinu Jang",
            "Roshanak Zilouchian Moghaddam"
        ],
        "comments": "",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Modern Large Language Model (LLM) agents promise end to end assistance with real-world software tasks, yet existing benchmarks evaluate LLM agents almost exclusively in pre-baked environments where every dependency is pre-installed. To fill this gap, we introduce SetupBench, a 93 instance benchmark that isolates the environment-bootstrap skill: starting from a bare Linux sandbox, an agent must install packages, resolve dependency conflicts, initialize databases, and configure background services. Our tasks span seven language ecosystems, five database engines, and multi-service orchestration scenarios, each accompanies by a natural language problem statement and a deterministic success command. Through evaluation of OpenHands, a state-of-the-art coding agent, we find low success rates across task categories, with particular challenges in repository setup (38.9-57.4%) and local database configuration (20.0-53.3%). Our analysis reveals systematic failure modes including incomplete development tooling installation, hallucinated task constraints, and non-persistent environment modifications that break agent-human collaboration workflows. We identify substantial inefficiencies in agent exploration strategies, with 38-89% of actions being unnecessary compared to optimal human behavior. These findings highlight gaps in current agents' practical environment-bootstrap capabilities. By targeting this critical yet under-evaluated capability, SetupBench provides a rigorous yard-stick for the next generation of software developer agents aiming to solve end to end real-wold tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 189,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09068",
        "abs_url": "https://arxiv.org/abs/2507.09068",
        "pdf_url": "https://arxiv.org/pdf/2507.09068",
        "title": "Infinite Video Understanding",
        "authors": [
            "Dell Zhang",
            "Xiangyu Chen",
            "Jixiang Luo",
            "Mengxi Jia",
            "Changzhi Sun",
            "Ruilong Ren",
            "Jingren Liu",
            "Hao Sun",
            "Xuelong Li"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG); Multimedia (cs.MM)",
        "abstract": "The rapid advancements in Large Language Models (LLMs) and their multimodal extensions (MLLMs) have ushered in remarkable progress in video understanding. However, a fundamental challenge persists: effectively processing and comprehending video content that extends beyond minutes or hours. While recent efforts like Video-XL-2 have demonstrated novel architectural solutions for extreme efficiency, and advancements in positional encoding such as HoPE and VideoRoPE++ aim to improve spatio-temporal understanding over extensive contexts, current state-of-the-art models still encounter significant computational and memory constraints when faced with the sheer volume of visual tokens from lengthy sequences. Furthermore, maintaining temporal coherence, tracking complex events, and preserving fine-grained details over extended periods remain formidable hurdles, despite progress in agentic reasoning systems like Deep Video Discovery. This position paper posits that a logical, albeit ambitious, next frontier for multimedia research is Infinite Video Understanding -- the capability for models to continuously process, understand, and reason about video data of arbitrary, potentially never-ending duration. We argue that framing Infinite Video Understanding as a blue-sky research objective provides a vital north star for the multimedia, and the wider AI, research communities, driving innovation in areas such as streaming architectures, persistent memory mechanisms, hierarchical and adaptive representations, event-centric reasoning, and novel evaluation paradigms. Drawing inspiration from recent work on long/ultra-long video understanding and several closely related fields, we outline the core challenges and key research directions towards achieving this transformative capability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 190,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09092",
        "abs_url": "https://arxiv.org/abs/2507.09092",
        "pdf_url": "https://arxiv.org/pdf/2507.09092",
        "title": "MI CAM: Mutual Information Weighted Activation Mapping for Causal Visual Explanations of Convolutional Neural Networks",
        "authors": [
            "Ram S Iyer",
            "Narayan S Iyer",
            "Rugmini Ammal P"
        ],
        "comments": "12 pages, 10 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "With the intervention of machine vision in our crucial day to day necessities including healthcare and automated power plants, attention has been drawn to the internal mechanisms of convolutional neural networks, and the reason why the network provides specific inferences. This paper proposes a novel post-hoc visual explanation method called MI CAM based on activation mapping. Differing from previous class activation mapping based approaches, MI CAM produces saliency visualizations by weighing each feature map through its mutual information with the input image and the final result is generated by a linear combination of weights and activation maps. It also adheres to producing causal interpretations as validated with the help of counterfactual analysis. We aim to exhibit the visual performance and unbiased justifications for the model inferencing procedure achieved by MI CAM. Our approach works at par with all state-of-the-art methods but particularly outperforms some in terms of qualitative and quantitative measures. The implementation of proposed method can be found on this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 191,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09093",
        "abs_url": "https://arxiv.org/abs/2507.09093",
        "pdf_url": "https://arxiv.org/pdf/2507.09093",
        "title": "Optimal High-probability Convergence of Nonlinear SGD under Heavy-tailed Noise via Symmetrization",
        "authors": [
            "Aleksandar Armacki",
            "Dragana Bajovic",
            "Dusan Jakovetic",
            "Soummya Kar"
        ],
        "comments": "38 pages, 1 figure",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Optimization and Control (math.OC)",
        "abstract": "We study convergence in high-probability of SGD-type methods in non-convex optimization and the presence of heavy-tailed noise. To combat the heavy-tailed noise, a general black-box nonlinear framework is considered, subsuming nonlinearities like sign, clipping, normalization and their smooth counterparts. Our first result shows that nonlinear SGD (N-SGD) achieves the rate $\\widetilde{\\mathcal{O}}(t^{-1/2})$, for any noise with unbounded moments and a symmetric probability density function (PDF). Crucially, N-SGD has exponentially decaying tails, matching the performance of linear SGD under light-tailed noise. To handle non-symmetric noise, we propose two novel estimators, based on the idea of noise symmetrization. The first, dubbed Symmetrized Gradient Estimator (SGE), assumes a noiseless gradient at any reference point is available at the start of training, while the second, dubbed Mini-batch SGE (MSGE), uses mini-batches to estimate the noiseless gradient. Combined with the nonlinear framework, we get N-SGE and N-MSGE methods, respectively, both achieving the same convergence rate and exponentially decaying tails as N-SGD, while allowing for non-symmetric noise with unbounded moments and PDF satisfying a mild technical condition, with N-MSGE additionally requiring bounded noise moment of order $p \\in (1,2]$. Compared to works assuming noise with bounded $p$-th moment, our results: 1) are based on a novel symmetrization approach; 2) provide a unified framework and relaxed moment conditions; 3) imply optimal oracle complexity of N-SGD and N-SGE, strictly better than existing works when $p < 2$, while the complexity of N-MSGE is close to existing works. Compared to works assuming symmetric noise with unbounded moments, we: 1) provide a sharper analysis and improved rates; 2) facilitate state-dependent symmetric noise; 3) extend the strong guarantees to non-symmetric noise.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 192,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09103",
        "abs_url": "https://arxiv.org/abs/2507.09103",
        "pdf_url": "https://arxiv.org/pdf/2507.09103",
        "title": "CoVAE: Consistency Training of Variational Autoencoders",
        "authors": [
            "Gianluigi Silvestri",
            "Luca Ambrogioni"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Current state-of-the-art generative approaches frequently rely on a two-stage training procedure, where an autoencoder (often a VAE) first performs dimensionality reduction, followed by training a generative model on the learned latent space. While effective, this introduces computational overhead and increased sampling times. We challenge this paradigm by proposing Consistency Training of Variational AutoEncoders (CoVAE), a novel single-stage generative autoencoding framework that adopts techniques from consistency models to train a VAE architecture. The CoVAE encoder learns a progressive series of latent representations with increasing encoding noise levels, mirroring the forward processes of diffusion and flow matching models. This sequence of representations is regulated by a time dependent $\\beta$ parameter that scales the KL loss. The decoder is trained using a consistency loss with variational regularization, which reduces to a conventional VAE loss at the earliest latent time. We show that CoVAE can generate high-quality samples in one or few steps without the use of a learned prior, significantly outperforming equivalent VAEs and other single-stage VAEs methods. Our approach provides a unified framework for autoencoding and diffusion-style generative modeling and provides a viable route for one-step generative high-performance autoencoding. Our code is publicly available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 193,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09118",
        "abs_url": "https://arxiv.org/abs/2507.09118",
        "pdf_url": "https://arxiv.org/pdf/2507.09118",
        "title": "Mind the Gap: Preserving and Compensating for the Modality Gap in CLIP-Based Continual Learning",
        "authors": [
            "Linlan Huang",
            "Xusheng Cao",
            "Haori Lu",
            "Yifan Meng",
            "Fei Yang",
            "Xialei Liu"
        ],
        "comments": "Accepted at ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Continual learning aims to enable models to learn sequentially from continuously incoming data while retaining performance on previously learned tasks. With the Contrastive Language-Image Pre-trained model (CLIP) exhibiting strong capabilities across various downstream tasks, there has been growing interest in leveraging CLIP for continual learning in such scenarios. Most existing works overlook the inherent modality gap in CLIP, a key factor in its generalization and adaptability. In this paper, we analyze the variations in the modality gap during the fine-tuning of vision-language pre-trained models. Our observations reveal that the modality gap effectively reflects the extent to which pre-trained knowledge is preserved. Based on these insights, we propose a simple yet effective method, MG-CLIP, that improves CLIP's performance in class-incremental learning. Our approach leverages modality gap preservation to mitigate forgetting and modality gap compensation to enhance the capacity for new data, introducing a novel modality-gap-based perspective for continual learning. Extensive experiments on multiple benchmarks demonstrate that our method outperforms existing approaches without requiring additional replay data. Our code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 194,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09128",
        "abs_url": "https://arxiv.org/abs/2507.09128",
        "pdf_url": "https://arxiv.org/pdf/2507.09128",
        "title": "A Generalization Theory for Zero-Shot Prediction",
        "authors": [
            "Ronak Mehta",
            "Zaid Harchaoui"
        ],
        "comments": "Published at ICML '25 (Oral)",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "A modern paradigm for generalization in machine learning and AI consists of pre-training a task-agnostic foundation model, generally obtained using self-supervised and multimodal contrastive learning. The resulting representations can be used for prediction on a downstream task for which no labeled data is available. We present a theoretical framework to better understand this approach, called zero-shot prediction. We identify the target quantities that zero-shot prediction aims to learn, or learns in passing, and the key conditional independence relationships that enable its generalization ability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 195,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09138",
        "abs_url": "https://arxiv.org/abs/2507.09138",
        "pdf_url": "https://arxiv.org/pdf/2507.09138",
        "title": "HedraRAG: Coordinating LLM Generation and Database Retrieval in Heterogeneous RAG Serving",
        "authors": [
            "Zhengding Hu",
            "Vibha Murthy",
            "Zaifeng Pan",
            "Wanlu Li",
            "Xiaoyi Fang",
            "Yufei Ding",
            "Yuke Wang"
        ],
        "comments": "Accepted by SOSP 2025",
        "subjects": "Databases (cs.DB); Machine Learning (cs.LG)",
        "abstract": "This paper addresses emerging system-level challenges in heterogeneous retrieval-augmented generation (RAG) serving, where complex multi-stage workflows and diverse request patterns complicate efficient execution. We present HedraRAG, a runtime system built on a graph-based abstraction that exposes optimization opportunities across stage-level parallelism, intra-request similarity, and inter-request skewness. These opportunities are realized through dynamic graph transformations, such as node splitting, reordering, edge addition, and dependency rewiring, applied to wavefronts of subgraphs spanning concurrent requests. The resulting execution plans are mapped onto hybrid CPU-GPU pipelines to improve resource utilization and reduce latency. Evaluations across a wide range of RAG workflows demonstrate speedups exceeding 1.5x and reaching up to 5x over existing frameworks, showcasing the effectiveness of coordinated generation and retrieval in serving environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 196,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09148",
        "abs_url": "https://arxiv.org/abs/2507.09148",
        "pdf_url": "https://arxiv.org/pdf/2507.09148",
        "title": "A Randomized Algorithm for Sparse PCA based on the Basic SDP Relaxation",
        "authors": [
            "Alberto Del Pia",
            "Dekun Zhou"
        ],
        "comments": "29 pages, 2 figures",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Optimization and Control (math.OC)",
        "abstract": "Sparse Principal Component Analysis (SPCA) is a fundamental technique for dimensionality reduction, and is NP-hard. In this paper, we introduce a randomized approximation algorithm for SPCA, which is based on the basic SDP relaxation. Our algorithm has an approximation ratio of at most the sparsity constant with high probability, if called enough times. Under a technical assumption, which is consistently satisfied in our numerical tests, the average approximation ratio is also bounded by $\\mathcal{O}(\\log{d})$, where $d$ is the number of features. We show that this technical assumption is satisfied if the SDP solution is low-rank, or has exponentially decaying eigenvalues. We then present a broad class of instances for which this technical assumption holds. We also demonstrate that in a covariance model, which generalizes the spiked Wishart model, our proposed algorithm achieves a near-optimal approximation ratio. We demonstrate the efficacy of our algorithm through numerical results on real-world datasets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 197,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09149",
        "abs_url": "https://arxiv.org/abs/2507.09149",
        "pdf_url": "https://arxiv.org/pdf/2507.09149",
        "title": "Advanced Health Misinformation Detection Through Hybrid CNN-LSTM Models Informed by the Elaboration Likelihood Model (ELM)",
        "authors": [
            "Mkululi Sikosana",
            "Sean Maudsley-Barton",
            "Oluwaseun Ajao"
        ],
        "comments": "11 Pages, 2 Figures, 3 Tables conference paper to appear in proceedings of International Conference on Artificial Intelligence, Computer, Data Sciences and Applications (ACDSA'25)",
        "subjects": "Social and Information Networks (cs.SI); Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG)",
        "abstract": "Health misinformation during the COVID-19 pandemic has significantly challenged public health efforts globally. This study applies the Elaboration Likelihood Model (ELM) to enhance misinformation detection on social media using a hybrid Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) model. The model aims to enhance the detection accuracy and reliability of misinformation classification by integrating ELM-based features such as text readability, sentiment polarity, and heuristic cues (e.g., punctuation frequency). The enhanced model achieved an accuracy of 97.37%, precision of 96.88%, recall of 98.50%, F1-score of 97.41%, and ROC-AUC of 99.50%. A combined model incorporating feature engineering further improved performance, achieving a precision of 98.88%, recall of 99.80%, F1-score of 99.41%, and ROC-AUC of 99.80%. These findings highlight the value of ELM features in improving detection performance, offering valuable contextual information. This study demonstrates the practical application of psychological theories in developing advanced machine learning algorithms to address health misinformation effectively.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 198,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09160",
        "abs_url": "https://arxiv.org/abs/2507.09160",
        "pdf_url": "https://arxiv.org/pdf/2507.09160",
        "title": "Tactile-VLA: Unlocking Vision-Language-Action Model's Physical Knowledge for Tactile Generalization",
        "authors": [
            "Jialei Huang",
            "Shuo Wang",
            "Fanqi Lin",
            "Yihang Hu",
            "Chuan Wen",
            "Yang Gao"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
        "abstract": "Vision-Language-Action (VLA) models have shown remarkable achievements, driven by the rich implicit knowledge of their vision-language components. However, achieving generalist robotic agents demands precise grounding into physical interactions, especially in contact-rich scenarios where fine-grained force control is essential. We advance VLAs' implicit knowledge beyond identifying what to do, towards guiding how to physically interact with real world. This paper introduces Tactile-VLA, a novel framework that deeply fuses vision, language, action, and tactile sensing. This framework incorporates a hybrid position-force controller to translate the model's intentions into precise physical actions and a reasoning module that allows the robot to adapt its strategy based on tactile feedback. Experiments demonstrate Tactile-VLA's effectiveness and generalizability in three key aspects: (1) enabling tactile-aware instruction following, (2) utilizing tactile-relevant commonsense, and (3) facilitating adaptive tactile-involved reasoning. A key finding is that the VLM's prior knowledge already contains semantic understanding of physical interaction; by connecting it to the robot's tactile sensors with only a few demonstrations, we can activate this prior knowledge to achieve zero-shot generalization in contact-rich tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 199,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09166",
        "abs_url": "https://arxiv.org/abs/2507.09166",
        "pdf_url": "https://arxiv.org/pdf/2507.09166",
        "title": "Investigating the Robustness of Extreme Precipitation Super-Resolution Across Climates",
        "authors": [
            "Louise Largeau",
            "Erwan Koch",
            "David Leutwyler",
            "Gregoire Mariethoz",
            "Valerie Chavez-Demoulin",
            "Tom Beucler"
        ],
        "comments": "31 pages, 9 figures, 1 table, submitted to AGU JAMES",
        "subjects": "Atmospheric and Oceanic Physics (physics.ao-ph); Machine Learning (cs.LG)",
        "abstract": "The coarse spatial resolution of gridded climate models, such as general circulation models, limits their direct use in projecting socially relevant variables like extreme precipitation. Most downscaling methods estimate the conditional distributions of extremes by generating large ensembles, complicating the assessment of robustness under distributional shifts, such as those induced by climate change. To better understand and potentially improve robustness, we propose super-resolving the parameters of the target variable's probability distribution directly using analytically tractable mappings. Within a perfect-model framework over Switzerland, we demonstrate that vector generalized linear and additive models can super-resolve the generalized extreme value distribution of summer hourly precipitation extremes from coarse precipitation fields and topography. We introduce the notion of a \"robustness gap\", defined as the difference in predictive error between present-trained and future-trained models, and use it to diagnose how model structure affects the generalization of each quantile to a pseudo-global warming scenario. By evaluating multiple model configurations, we also identify an upper limit on the super-resolution factor based on the spatial auto- and cross-correlation of precipitation and elevation, beyond which coarse precipitation loses predictive value. Our framework is broadly applicable to variables governed by parametric distributions and offers a model-agnostic diagnostic for understanding when and why empirical downscaling generalizes to climate change and extremes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 200,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09185",
        "abs_url": "https://arxiv.org/abs/2507.09185",
        "pdf_url": "https://arxiv.org/pdf/2507.09185",
        "title": "Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models",
        "authors": [
            "Ameen Ali",
            "Shahar Katz",
            "Lior Wolf",
            "Ivan Titov"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Large language models (LLMs) often develop learned mechanisms specialized to specific datasets, such as reliance on domain-specific correlations, which yield high-confidence predictions without generalizable reasoning. While beneficial in one setting, these dataset-specific mechanisms typically degrade performance when models encounter novel tasks or distributions. In this work, we introduce a fine-tuning approach designed to enhance generalization by identifying and pruning neurons associated with dataset-specific mechanisms in transformer-based LLMs. Our method employs Integrated Gradients to quantify each neuron's influence on high-confidence predictions, pinpointing those that disproportionately contribute to dataset-specific performance without supporting robust, transferable reasoning. Selectively pruning these neurons compels the model to depend on generalizable representations. Evaluated across multiple-choice benchmarks, our pruning-based fine-tuning significantly enhances performance, surpassing prior (non-pruning) adaptation methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 201,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09222",
        "abs_url": "https://arxiv.org/abs/2507.09222",
        "pdf_url": "https://arxiv.org/pdf/2507.09222",
        "title": "Calibrated and Robust Foundation Models for Vision-Language and Medical Image Tasks Under Distribution Shift",
        "authors": [
            "Behraj Khan",
            "Tahir Syed"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Foundation models like CLIP and SAM have transformed computer vision and medical imaging via low-shot transfer learning. However, deployment of these models hindered by two key challenges: \\textit{distribution shift} between training and test data, and \\textit{confidence misalignment} that leads to overconfident incorrect predictions. These issues manifest differently in vision-language classification and medical segmentation tasks, yet existing solutions remain domain-specific. We propose \\textit{StaRFM}, a unified framework addressing both challenges. It introduces a Fisher information penalty (FIP), extended to 3D medical data via patch-wise regularization, to reduce covariate shift in CLIP and SAM embeddings. Additionally, a confidence misalignment penalty (CMP), reformulated for voxel-level predictions, calibrates uncertainty in segmentation tasks. We theoretically derive PAC-Bayes bounds showing FIP controls generalization via the Fisher-Rao norm, while CMP minimizes calibration error through Brier score optimization. StaRFM shows consistent performance like \\texttt{+}3.5\\% accuracy and 28\\% lower ECE on 19 vision datasets (e.g., ImageNet, Office-Home), 84.7\\% DSC and 4.8mm HD95 in medical segmentation (e.g., BraTS, ATLAS), and 40\\% lower cross-domain performance gap compared to prior benchmarking methods. The framework is plug-and-play, requiring minimal architectural changes for seamless integration with foundation models. Code and models will be released at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 202,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09227",
        "abs_url": "https://arxiv.org/abs/2507.09227",
        "pdf_url": "https://arxiv.org/pdf/2507.09227",
        "title": "PanoDiff-SR: Synthesizing Dental Panoramic Radiographs using Diffusion and Super-resolution",
        "authors": [
            "Sanyam Jain",
            "Bruna Neves de Freitas",
            "Andreas Basse-OConnor",
            "Alexandros Iosifidis",
            "Ruben Pauwels"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "There has been increasing interest in the generation of high-quality, realistic synthetic medical images in recent years. Such synthetic datasets can mitigate the scarcity of public datasets for artificial intelligence research, and can also be used for educational purposes. In this paper, we propose a combination of diffusion-based generation (PanoDiff) and Super-Resolution (SR) for generating synthetic dental panoramic radiographs (PRs). The former generates a low-resolution (LR) seed of a PR (256 X 128) which is then processed by the SR model to yield a high-resolution (HR) PR of size 1024 X 512. For SR, we propose a state-of-the-art transformer that learns local-global relationships, resulting in sharper edges and textures. Experimental results demonstrate a Frechet inception distance score of 40.69 between 7243 real and synthetic images (in HR). Inception scores were 2.55, 2.30, 2.90 and 2.98 for real HR, synthetic HR, real LR and synthetic LR images, respectively. Among a diverse group of six clinical experts, all evaluating a mixture of 100 synthetic and 100 real PRs in a time-limited observation, the average accuracy in distinguishing real from synthetic images was 68.5% (with 50% corresponding to random guessing).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 203,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09282",
        "abs_url": "https://arxiv.org/abs/2507.09282",
        "pdf_url": "https://arxiv.org/pdf/2507.09282",
        "title": "ClaritySpeech: Dementia Obfuscation in Speech",
        "authors": [
            "Dominika Woszczyk",
            "Ranya Aloufi",
            "Soteris Demetriou"
        ],
        "comments": "Accepted at Interspeech 2025",
        "subjects": "Computation and Language (cs.CL); Cryptography and Security (cs.CR); Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS)",
        "abstract": "Dementia, a neurodegenerative disease, alters speech patterns, creating communication barriers and raising privacy concerns. Current speech technologies, such as automatic speech transcription (ASR), struggle with dementia and atypical speech, further challenging accessibility. This paper presents a novel dementia obfuscation in speech framework, ClaritySpeech, integrating ASR, text obfuscation, and zero-shot text-to-speech (TTS) to correct dementia-affected speech while preserving speaker identity in low-data environments without fine-tuning. Results show a 16% and 10% drop in mean F1 score across various adversarial settings and modalities (audio, text, fusion) for ADReSS and ADReSSo, respectively, maintaining 50% speaker similarity. We also find that our system improves WER (from 0.73 to 0.08 for ADReSS and 0.15 for ADReSSo) and speech quality from 1.65 to ~2.15, enhancing privacy and accessibility.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 204,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09291",
        "abs_url": "https://arxiv.org/abs/2507.09291",
        "pdf_url": "https://arxiv.org/pdf/2507.09291",
        "title": "Supercharging Floorplan Localization with Semantic Rays",
        "authors": [
            "Yuval Grader",
            "Hadar Averbuch-Elor"
        ],
        "comments": "Accepted at ICCV 2025. this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Floorplans provide a compact representation of the building's structure, revealing not only layout information but also detailed semantics such as the locations of windows and doors. However, contemporary floorplan localization techniques mostly focus on matching depth-based structural cues, ignoring the rich semantics communicated within floorplans. In this work, we introduce a semantic-aware localization framework that jointly estimates depth and semantic rays, consolidating over both for predicting a structural-semantic probability volume. Our probability volume is constructed in a coarse-to-fine manner: We first sample a small set of rays to obtain an initial low-resolution probability volume. We then refine these probabilities by performing a denser sampling only in high-probability regions and process the refined values for predicting a 2D location and orientation angle. We conduct an evaluation on two standard floorplan localization benchmarks. Our experiments demonstrate that our approach substantially outperforms state-of-the-art methods, achieving significant improvements in recall metrics compared to prior works. Moreover, we show that our framework can easily incorporate additional metadata such as room labels, enabling additional gains in both accuracy and efficiency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 205,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09299",
        "abs_url": "https://arxiv.org/abs/2507.09299",
        "pdf_url": "https://arxiv.org/pdf/2507.09299",
        "title": "ViT-ProtoNet for Few-Shot Image Classification: A Multi-Benchmark Evaluation",
        "authors": [
            "Abdulvahap Mutlu",
            "Şengül Doğan",
            "Türker Tuncer"
        ],
        "comments": "All codes are available at this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "The remarkable representational power of Vision Transformers (ViTs) remains underutilized in few-shot image classification. In this work, we introduce ViT-ProtoNet, which integrates a ViT-Small backbone into the Prototypical Network framework. By averaging class conditional token embeddings from a handful of support examples, ViT-ProtoNet constructs robust prototypes that generalize to novel categories under 5-shot settings. We conduct an extensive empirical evaluation on four standard benchmarks: Mini-ImageNet, FC100, CUB-200, and CIFAR-FS, including overlapped support variants to assess robustness. Across all splits, ViT-ProtoNet consistently outperforms CNN-based prototypical counterparts, achieving up to a 3.2\\% improvement in 5-shot accuracy and demonstrating superior feature separability in latent space. Furthermore, it outperforms or is competitive with transformer-based competitors using a more lightweight backbone. Comprehensive ablations examine the impact of transformer depth, patch size, and fine-tuning strategy. To foster reproducibility, we release code and pretrained weights. Our results establish ViT-ProtoNet as a powerful, flexible approach for few-shot classification and set a new baseline for transformer-based meta-learners.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 206,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09305",
        "abs_url": "https://arxiv.org/abs/2507.09305",
        "pdf_url": "https://arxiv.org/pdf/2507.09305",
        "title": "DAA*: Deep Angular A Star for Image-based Path Planning",
        "authors": [
            "Zhiwei Xu"
        ],
        "comments": "International Conference on Computer Vision (ICCV), 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Image and Video Processing (eess.IV)",
        "abstract": "Path smoothness is often overlooked in path imitation learning from expert demonstrations. In this paper, we introduce a novel learning method, termed deep angular A* (DAA*), by incorporating the proposed path angular freedom (PAF) into A* to improve path similarity through adaptive path smoothness. The PAF aims to explore the effect of move angles on path node expansion by finding the trade-off between their minimum and maximum values, allowing for high adaptiveness for imitation learning. DAA* improves path optimality by closely aligning with the reference path through joint optimization of path shortening and smoothing, which correspond to heuristic distance and PAF, respectively. Throughout comprehensive evaluations on 7 datasets, including 4 maze datasets, 2 video-game datasets, and a real-world drone-view dataset containing 2 scenarios, we demonstrate remarkable improvements of our DAA* over neural A* in path similarity between the predicted and reference paths with a shorter path length when the shortest path is plausible, improving by 9.0% SPR, 6.9% ASIM, and 3.9% PSIM. Furthermore, when jointly learning pathfinding with both path loss and path probability map loss, DAA* significantly outperforms the state-of-the-art TransPath by 6.7% SPR, 6.5% PSIM, and 3.7% ASIM. We also discuss the minor trade-off between path optimality and search efficiency where applicable.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 207,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09317",
        "abs_url": "https://arxiv.org/abs/2507.09317",
        "pdf_url": "https://arxiv.org/pdf/2507.09317",
        "title": "Uncovering symmetric and asymmetric species associations from community and environmental data",
        "authors": [
            "Sara Si-Moussi",
            "Esther Galbrun",
            "Mickael Hedde",
            "Giovanni Poggiato",
            "Matthias Rohr",
            "Wilfried Thuiller"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Populations and Evolution (q-bio.PE)",
        "abstract": "There is no much doubt that biotic interactions shape community assembly and ultimately the spatial co-variations between species. There is a hope that the signal of these biotic interactions can be observed and retrieved by investigating the spatial associations between species while accounting for the direct effects of the environment. By definition, biotic interactions can be both symmetric and asymmetric. Yet, most models that attempt to retrieve species associations from co-occurrence or co-abundance data internally assume symmetric relationships between species. Here, we propose and validate a machine-learning framework able to retrieve bidirectional associations by analyzing species community and environmental data. Our framework (1) models pairwise species associations as directed influences from a source to a target species, parameterized with two species-specific latent embeddings: the effect of the source species on the community, and the response of the target species to the community; and (2) jointly fits these associations within a multi-species conditional generative model with different modes of interactions between environmental drivers and biotic associations. Using both simulated and empirical data, we demonstrate the ability of our framework to recover known asymmetric and symmetric associations and highlight the properties of the learned association networks. By comparing our approach to other existing models such as joint species distribution models and probabilistic graphical models, we show its superior capacity at retrieving symmetric and asymmetric interactions. The framework is intuitive, modular and broadly applicable across various taxonomic groups.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 208,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09330",
        "abs_url": "https://arxiv.org/abs/2507.09330",
        "pdf_url": "https://arxiv.org/pdf/2507.09330",
        "title": "WellPINN: Accurate Well Representation for Transient Fluid Pressure Diffusion in Subsurface Reservoirs with Physics-Informed Neural Networks",
        "authors": [
            "Linus Walter",
            "Qingkai Kong",
            "Sara Hanson-Hedgecock",
            "Víctor Vilarrasa"
        ],
        "comments": "",
        "subjects": "Fluid Dynamics (physics.flu-dyn); Machine Learning (cs.LG); Computational Physics (physics.comp-ph)",
        "abstract": "Accurate representation of wells is essential for reliable reservoir characterization and simulation of operational scenarios in subsurface flow models. Physics-informed neural networks (PINNs) have recently emerged as a promising method for reservoir modeling, offering seamless integration of monitoring data and governing physical equations. However, existing PINN-based studies face major challenges in capturing fluid pressure near wells, particularly during the early stage after injection begins. To address this, we propose WellPINN, a modeling workflow that combines the outputs of multiple sequentially trained PINN models to accurately represent wells. This workflow iteratively approximates the radius of the equivalent well to match the actual well dimensions by decomposing the domain into stepwise shrinking subdomains with a simultaneously reducing equivalent well radius. Our results demonstrate that sequential training of superimposing networks around the pumping well is the first workflow that focuses on accurate inference of fluid pressure from pumping rates throughout the entire injection period, significantly advancing the potential of PINNs for inverse modeling and operational scenario simulations. All data and code for this paper will be made openly available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 209,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09340",
        "abs_url": "https://arxiv.org/abs/2507.09340",
        "pdf_url": "https://arxiv.org/pdf/2507.09340",
        "title": "Unified Linear Parametric Map Modeling and Perception-aware Trajectory Planning for Mobile Robotics",
        "authors": [
            "Hongyu Nie",
            "Xingyu Li",
            "Xu Liu",
            "Zhaotong Tan",
            "Sen Mei",
            "Wenbo Su"
        ],
        "comments": "Submitted to IEEE Transactions on Robotics (TRO) in July 2025",
        "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
        "abstract": "Autonomous navigation in mobile robots, reliant on perception and planning, faces major hurdles in large-scale, complex environments. These include heavy computational burdens for mapping, sensor occlusion failures for UAVs, and traversal challenges on irregular terrain for UGVs, all compounded by a lack of perception-aware strategies. To address these challenges, we introduce Random Mapping and Random Projection (RMRP). This method constructs a lightweight linear parametric map by first mapping data to a high-dimensional space, followed by a sparse random projection for dimensionality reduction. Our novel Residual Energy Preservation Theorem provides theoretical guarantees for this process, ensuring critical geometric properties are preserved. Based on this map, we propose the RPATR (Robust Perception-Aware Trajectory Planner) framework. For UAVs, our method unifies grid and Euclidean Signed Distance Field (ESDF) maps. The front-end uses an analytical occupancy gradient to refine initial paths for safety and smoothness, while the back-end uses a closed-form ESDF for trajectory optimization. Leveraging the trained RMRP model's generalization, the planner predicts unobserved areas for proactive navigation. For UGVs, the model characterizes terrain and provides closed-form gradients, enabling online planning to circumvent large holes. Validated in diverse scenarios, our framework demonstrates superior mapping performance in time, memory, and accuracy, and enables computationally efficient, safe navigation for high-speed UAVs and UGVs. The code will be released to foster community collaboration.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 210,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09383",
        "abs_url": "https://arxiv.org/abs/2507.09383",
        "pdf_url": "https://arxiv.org/pdf/2507.09383",
        "title": "Real-Time Adaptive Motion Planning via Point Cloud-Guided, Energy-Based Diffusion and Potential Fields",
        "authors": [
            "Wondmgezahu Teshome",
            "Kian Behzad",
            "Octavia Camps",
            "Michael Everett",
            "Milad Siami",
            "Mario Sznaier"
        ],
        "comments": "Accepted to IEEE RA-L 2025",
        "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
        "abstract": "Motivated by the problem of pursuit-evasion, we present a motion planning framework that combines energy-based diffusion models with artificial potential fields for robust real time trajectory generation in complex environments. Our approach processes obstacle information directly from point clouds, enabling efficient planning without requiring complete geometric representations. The framework employs classifier-free guidance training and integrates local potential fields during sampling to enhance obstacle avoidance. In dynamic scenarios, the system generates initial trajectories using the diffusion model and continuously refines them through potential field-based adaptation, demonstrating effective performance in pursuit-evasion scenarios with partial pursuer observability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 211,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09385",
        "abs_url": "https://arxiv.org/abs/2507.09385",
        "pdf_url": "https://arxiv.org/pdf/2507.09385",
        "title": "Credit Card Fraud Detection Using RoFormer Model With Relative Distance Rotating Encoding",
        "authors": [
            "Kevin Reyes",
            "Vasco Cortez"
        ],
        "comments": "2025 IEEE Conference on Artificial Intelligence (CAI)",
        "subjects": "Neural and Evolutionary Computing (cs.NE); Machine Learning (cs.LG)",
        "abstract": "Fraud detection is one of the most important challenges that financial systems must address. Detecting fraudulent transactions is critical for payment gateway companies like Flow Payment, which process millions of transactions monthly and require robust security measures to mitigate financial risks. Increasing transaction authorization rates while reducing fraud is essential for providing a good user experience and building a sustainable business. For this reason, discovering novel and improved methods to detect fraud requires continuous research and investment for any company that wants to succeed in this industry. In this work, we introduced a novel method for detecting transactional fraud by incorporating the Relative Distance Rotating Encoding (ReDRE) in the RoFormer model. The incorporation of angle rotation using ReDRE enhances the characterization of time series data within a Transformer, leading to improved fraud detection by better capturing temporal dependencies and event relationships.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 212,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09410",
        "abs_url": "https://arxiv.org/abs/2507.09410",
        "pdf_url": "https://arxiv.org/pdf/2507.09410",
        "title": "GreenCrossingAI: A Camera Trap/Computer Vision Pipeline for Environmental Science Research Groups",
        "authors": [
            "Bernie Boscoe",
            "Shawn Johnson",
            "Andrea Osborn",
            "Chandler Campbell",
            "Karen Mager"
        ],
        "comments": "This is the preprint version of the paper in Practice and Experience in Advanced Research Computing, PEARC25",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Camera traps have long been used by wildlife researchers to monitor and study animal behavior, population dynamics, habitat use, and species diversity in a non-invasive and efficient manner. While data collection from the field has increased with new tools and capabilities, methods to develop, process, and manage the data, especially the adoption of ML/AI tools, remain challenging. These challenges include the sheer volume of data generated, the need for accurate labeling and annotation, variability in environmental conditions affecting data quality, and the integration of ML/AI tools into existing workflows that often require domain-specific customization and computational resources. This paper provides a guide to a low-resource pipeline to process camera trap data on-premise, incorporating ML/AI capabilities tailored for small research groups with limited resources and computational expertise. By focusing on practical solutions, the pipeline offers accessible approaches for data transmission, inference, and evaluation, enabling researchers to discover meaningful insights from their ever-increasing camera trap datasets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 213,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09420",
        "abs_url": "https://arxiv.org/abs/2507.09420",
        "pdf_url": "https://arxiv.org/pdf/2507.09420",
        "title": "Domain Adaptation and Multi-view Attention for Learnable Landmark Tracking with Sparse Data",
        "authors": [
            "Timothy Chase Jr",
            "Karthik Dantu"
        ],
        "comments": "Presented at the RSS Space Robotics Workshop 2025. Poster available online at this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)",
        "abstract": "The detection and tracking of celestial surface terrain features are crucial for autonomous spaceflight applications, including Terrain Relative Navigation (TRN), Entry, Descent, and Landing (EDL), hazard analysis, and scientific data collection. Traditional photoclinometry-based pipelines often rely on extensive a priori imaging and offline processing, constrained by the computational limitations of radiation-hardened systems. While historically effective, these approaches typically increase mission costs and duration, operate at low processing rates, and have limited generalization. Recently, learning-based computer vision has gained popularity to enhance spacecraft autonomy and overcome these limitations. While promising, emerging techniques frequently impose computational demands exceeding the capabilities of typical spacecraft hardware for real-time operation and are further challenged by the scarcity of labeled training data for diverse extraterrestrial environments. In this work, we present novel formulations for in-situ landmark tracking via detection and description. We utilize lightweight, computationally efficient neural network architectures designed for real-time execution on current-generation spacecraft flight processors. For landmark detection, we propose improved domain adaptation methods that enable the identification of celestial terrain features with distinct, cheaply acquired training data. Concurrently, for landmark description, we introduce a novel attention alignment formulation that learns robust feature representations that maintain correspondence despite significant landmark viewpoint variations. Together, these contributions form a unified system for landmark tracking that demonstrates superior performance compared to existing state-of-the-art techniques.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 214,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09431",
        "abs_url": "https://arxiv.org/abs/2507.09431",
        "pdf_url": "https://arxiv.org/pdf/2507.09431",
        "title": "Optimizing External Sources for Controlled Burning Plasma in Tokamaks with Neural Ordinary Differential Equations",
        "authors": [
            "Zefang Liu",
            "Weston M. Stacey"
        ],
        "comments": "",
        "subjects": "Plasma Physics (physics.plasm-ph); Machine Learning (cs.LG)",
        "abstract": "Achieving controlled burning plasma in tokamaks requires precise regulation of external particle and energy sources to reach and maintain target core densities and temperatures. This work presents an inverse modeling approach using a multinodal plasma dynamics model based on neural ordinary differential equations (Neural ODEs). Given a desired time evolution of nodal quantities such as deuteron density or electron temperature, we compute the external source profiles, such as neutral beam injection (NBI) power, that drive the plasma toward the specified behavior. The approach is implemented within the NeuralPlasmaODE framework, which models multi-region, multi-timescale transport and incorporates physical mechanisms including radiation, auxiliary heating, and internodal energy exchange. By formulating the control task as an optimization problem, we use automatic differentiation through the Neural ODE solver to minimize the discrepancy between simulated and target trajectories. This framework transforms the forward simulation tool into a control-oriented model and provides a practical method for computing external source profiles in both current and future fusion devices.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 215,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09432",
        "abs_url": "https://arxiv.org/abs/2507.09432",
        "pdf_url": "https://arxiv.org/pdf/2507.09432",
        "title": "Sensitivity Analysis of Transport and Radiation in NeuralPlasmaODE for ITER Burning Plasmas",
        "authors": [
            "Zefang Liu",
            "Weston M. Stacey"
        ],
        "comments": "",
        "subjects": "Plasma Physics (physics.plasm-ph); Machine Learning (cs.LG)",
        "abstract": "Understanding how key physical parameters influence burning plasma behavior is critical for the reliable operation of ITER. In this work, we extend NeuralPlasmaODE, a multi-region, multi-timescale model based on neural ordinary differential equations, to perform a sensitivity analysis of transport and radiation mechanisms in ITER plasmas. Normalized sensitivities of core and edge temperatures and densities are computed with respect to transport diffusivities, electron cyclotron radiation (ECR) parameters, impurity fractions, and ion orbit loss (IOL) timescales. The analysis focuses on perturbations around a trained nominal model for the ITER inductive scenario. Results highlight the dominant influence of magnetic field strength, safety factor, and impurity content on energy confinement, while also revealing how temperature-dependent transport contributes to self-regulating behavior. These findings demonstrate the utility of NeuralPlasmaODE for predictive modeling and scenario optimization in burning plasma environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 216,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09473",
        "abs_url": "https://arxiv.org/abs/2507.09473",
        "pdf_url": "https://arxiv.org/pdf/2507.09473",
        "title": "Incentive-Aware Dynamic Resource Allocation under Long-Term Cost Constraints",
        "authors": [
            "Yan Dai",
            "Negin Golrezaei",
            "Patrick Jaillet"
        ],
        "comments": "",
        "subjects": "Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Motivated by applications such as cloud platforms allocating GPUs to users or governments deploying mobile health units across competing regions, we study the dynamic allocation of a reusable resource to strategic agents with private valuations. Our objective is to simultaneously (i) maximize social welfare, (ii) satisfy multi-dimensional long-term cost constraints, and (iii) incentivize truthful reporting. We begin by numerically evaluating primal-dual methods widely used in constrained online optimization and find them to be highly fragile in strategic settings -- agents can easily manipulate their reports to distort future dual updates for future gain. To address this vulnerability, we develop an incentive-aware framework that makes primal-dual methods robust to strategic behavior. Our design combines epoch-based lazy updates -- where dual variables remain fixed within each epoch -- with randomized exploration rounds that extract approximately truthful signals for learning. Leveraging carefully designed online learning subroutines that can be of independent interest for dual updates, our mechanism achieves $\\tilde{\\mathcal{O}}(\\sqrt{T})$ social welfare regret, satisfies all cost constraints, and ensures incentive alignment. This matches the performance of non-strategic allocation approaches while being robust to strategic agents.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 217,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09494",
        "abs_url": "https://arxiv.org/abs/2507.09494",
        "pdf_url": "https://arxiv.org/pdf/2507.09494",
        "title": "An Algorithm for Identifying Interpretable Subgroups With Elevated Treatment Effects",
        "authors": [
            "Albert Chiu"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Econometrics (econ.EM); Methodology (stat.ME)",
        "abstract": "We introduce an algorithm for identifying interpretable subgroups with elevated treatment effects, given an estimate of individual or conditional average treatment effects (CATE). Subgroups are characterized by ``rule sets'' -- easy-to-understand statements of the form (Condition A AND Condition B) OR (Condition C) -- which can capture high-order interactions while retaining interpretability. Our method complements existing approaches for estimating the CATE, which often produce high dimensional and uninterpretable results, by summarizing and extracting critical information from fitted models to aid decision making, policy implementation, and scientific understanding. We propose an objective function that trades-off subgroup size and effect size, and varying the hyperparameter that controls this trade-off results in a ``frontier'' of Pareto optimal rule sets, none of which dominates the others across all criteria. Valid inference is achievable through sample splitting. We demonstrate the utility and limitations of our method using simulated and empirical examples.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 218,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09503",
        "abs_url": "https://arxiv.org/abs/2507.09503",
        "pdf_url": "https://arxiv.org/pdf/2507.09503",
        "title": "Neural Two-Stage Stochastic Optimization for Solving Unit Commitment Problem",
        "authors": [
            "Zhentong Shao",
            "Jingtao Qin",
            "Nanpeng Yu"
        ],
        "comments": "Submitted to IEEE Transactions on Power Systems",
        "subjects": "Systems and Control (eess.SY); Machine Learning (cs.LG)",
        "abstract": "This paper proposes a neural stochastic optimization method for efficiently solving the two-stage stochastic unit commitment (2S-SUC) problem under high-dimensional uncertainty scenarios. The proposed method approximates the second-stage recourse problem using a deep neural network trained to map commitment decisions and uncertainty features to recourse costs. The trained network is subsequently embedded into the first-stage UC problem as a mixed-integer linear program (MILP), allowing for explicit enforcement of operational constraints while preserving the key uncertainty characteristics. A scenario-embedding network is employed to enable dimensionality reduction and feature aggregation across arbitrary scenario sets, serving as a data-driven scenario reduction mechanism. Numerical experiments on IEEE 5-bus, 30-bus, and 118-bus systems demonstrate that the proposed neural two-stage stochastic optimization method achieves solutions with an optimality gap of less than 1%, while enabling orders-of-magnitude speedup compared to conventional MILP solvers and decomposition-based methods. Moreover, the model's size remains constant regardless of the number of scenarios, offering significant scalability for large-scale stochastic unit commitment problems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 219,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09531",
        "abs_url": "https://arxiv.org/abs/2507.09531",
        "pdf_url": "https://arxiv.org/pdf/2507.09531",
        "title": "VDInstruct: Zero-Shot Key Information Extraction via Content-Aware Vision Tokenization",
        "authors": [
            "Son Nguyen",
            "Giang Nguyen",
            "Hung Dao",
            "Thao Do",
            "Daeyoung Kim"
        ],
        "comments": "Under Review",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Key Information Extraction (KIE) underpins the understanding of visual documents (e.g., receipts and contracts) by extracting precise semantic content and accurately capturing spatial structure. Yet existing multimodal large language models (MLLMs) often perform poorly on dense documents and rely on vision tokenization approaches that scale with image size, leading to redundant computation and memory inefficiency. To address these challenges, we introduce VDInstruct, an MLLM that separates spatial region detection from semantic feature extraction. Central to our model is a content-aware tokenization strategy: rather than fragmenting the entire image uniformly, it generates tokens in proportion to document complexity, preserving critical structure while eliminating wasted tokens. Leveraging a three-stage training paradigm, our model achieves state-of-the-art (SOTA) results on KIE benchmarks, matching or exceeding the accuracy of leading approaches while reducing the number of image tokens by roughly 3.6x. In zero-shot evaluations, VDInstruct surpasses strong baselines-such as DocOwl 1.5-by +5.5 F1 points, highlighting its robustness to unseen documents. These findings show that content-aware tokenization combined with explicit layout modeling offers a promising direction forward for document understanding. Data, source code, and model weights will be made publicly available.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 220,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09534",
        "abs_url": "https://arxiv.org/abs/2507.09534",
        "pdf_url": "https://arxiv.org/pdf/2507.09534",
        "title": "Consistency Trajectory Planning: High-Quality and Efficient Trajectory Optimization for Offline Model-Based Reinforcement Learning",
        "authors": [
            "Guanquan Wang",
            "Takuya Hiraoka",
            "Yoshimasa Tsuruoka"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)",
        "abstract": "This paper introduces Consistency Trajectory Planning (CTP), a novel offline model-based reinforcement learning method that leverages the recently proposed Consistency Trajectory Model (CTM) for efficient trajectory optimization. While prior work applying diffusion models to planning has demonstrated strong performance, it often suffers from high computational costs due to iterative sampling procedures. CTP supports fast, single-step trajectory generation without significant degradation in policy quality. We evaluate CTP on the D4RL benchmark and show that it consistently outperforms existing diffusion-based planning methods in long-horizon, goal-conditioned tasks. Notably, CTP achieves higher normalized returns while using significantly fewer denoising steps. In particular, CTP achieves comparable performance with over $120\\times$ speedup in inference time, demonstrating its practicality and effectiveness for high-performance, low-latency offline planning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 221,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09546",
        "abs_url": "https://arxiv.org/abs/2507.09546",
        "pdf_url": "https://arxiv.org/pdf/2507.09546",
        "title": "Lightweight Federated Learning over Wireless Edge Networks",
        "authors": [
            "Xiangwang Hou",
            "Jingjing Wang",
            "Jun Du",
            "Chunxiao Jiang",
            "Yong Ren",
            "Dusit Niyato"
        ],
        "comments": "",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)",
        "abstract": "With the exponential growth of smart devices connected to wireless networks, data production is increasing rapidly, requiring machine learning (ML) techniques to unlock its value. However, the centralized ML paradigm raises concerns over communication overhead and privacy. Federated learning (FL) offers an alternative at the network edge, but practical deployment in wireless networks remains challenging. This paper proposes a lightweight FL (LTFL) framework integrating wireless transmission power control, model pruning, and gradient quantization. We derive a closed-form expression of the FL convergence gap, considering transmission error, model pruning error, and gradient quantization error. Based on these insights, we formulate an optimization problem to minimize the convergence gap while meeting delay and energy constraints. To solve the non-convex problem efficiently, we derive closed-form solutions for the optimal model pruning ratio and gradient quantization level, and employ Bayesian optimization for transmission power control. Extensive experiments on real-world datasets show that LTFL outperforms state-of-the-art schemes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 222,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09566",
        "abs_url": "https://arxiv.org/abs/2507.09566",
        "pdf_url": "https://arxiv.org/pdf/2507.09566",
        "title": "Identifying Offline Metrics that Predict Online Impact: A Pragmatic Strategy for Real-World Recommender Systems",
        "authors": [
            "Timo Wilm",
            "Philipp Normann"
        ],
        "comments": "This work was accepted for publication in the 19th ACM Conference on Recommender Systems (RecSys 2025). The final published version will be available at the ACM Digital Library",
        "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "A critical challenge in recommender systems is to establish reliable relationships between offline and online metrics that predict real-world performance. Motivated by recent advances in Pareto front approximation, we introduce a pragmatic strategy for identifying offline metrics that align with online impact. A key advantage of this approach is its ability to simultaneously serve multiple test groups, each with distinct offline performance metrics, in an online experiment controlled by a single model. The method is model-agnostic for systems with a neural network backbone, enabling broad applicability across architectures and domains. We validate the strategy through a large-scale online experiment in the field of session-based recommender systems on the OTTO e-commerce platform. The online experiment identifies significant alignments between offline metrics and real-word click-through rate, post-click conversion rate and units sold. Our strategy provides industry practitioners with a valuable tool for understanding offline-to-online metric relationships and making informed, data-driven decisions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 223,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09624",
        "abs_url": "https://arxiv.org/abs/2507.09624",
        "pdf_url": "https://arxiv.org/pdf/2507.09624",
        "title": "CAN-Trace Attack: Exploit CAN Messages to Uncover Driving Trajectories",
        "authors": [
            "Xiaojie Lin",
            "Baihe Ma",
            "Xu Wang",
            "Guangsheng Yu",
            "Ying He",
            "Wei Ni",
            "Ren Ping Liu"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
        "abstract": "Driving trajectory data remains vulnerable to privacy breaches despite existing mitigation measures. Traditional methods for detecting driving trajectories typically rely on map-matching the path using Global Positioning System (GPS) data, which is susceptible to GPS data outage. This paper introduces CAN-Trace, a novel privacy attack mechanism that leverages Controller Area Network (CAN) messages to uncover driving trajectories, posing a significant risk to drivers' long-term privacy. A new trajectory reconstruction algorithm is proposed to transform the CAN messages, specifically vehicle speed and accelerator pedal position, into weighted graphs accommodating various driving statuses. CAN-Trace identifies driving trajectories using graph-matching algorithms applied to the created graphs in comparison to road networks. We also design a new metric to evaluate matched candidates, which allows for potential data gaps and matching inaccuracies. Empirical validation under various real-world conditions, encompassing different vehicles and driving regions, demonstrates the efficacy of CAN-Trace: it achieves an attack success rate of up to 90.59% in the urban region, and 99.41% in the suburban region.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 224,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09627",
        "abs_url": "https://arxiv.org/abs/2507.09627",
        "pdf_url": "https://arxiv.org/pdf/2507.09627",
        "title": "Lightweight Deep Learning-Based Channel Estimation for RIS-Aided Extremely Large-Scale MIMO Systems on Resource-Limited Edge Devices",
        "authors": [
            "Muhammad Kamran Saeed",
            "Ashfaq Khokhar",
            "Shakil Ahmed"
        ],
        "comments": "",
        "subjects": "Information Theory (cs.IT); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)",
        "abstract": "Next-generation wireless technologies such as 6G aim to meet demanding requirements such as ultra-high data rates, low latency, and enhanced connectivity. Extremely Large-Scale MIMO (XL-MIMO) and Reconfigurable Intelligent Surface (RIS) are key enablers, with XL-MIMO boosting spectral and energy efficiency through numerous antennas, and RIS offering dynamic control over the wireless environment via passive reflective elements. However, realizing their full potential depends on accurate Channel State Information (CSI). Recent advances in deep learning have facilitated efficient cascaded channel estimation. However, the scalability and practical deployment of existing estimation models in XL-MIMO systems remain limited. The growing number of antennas and RIS elements introduces a significant barrier to real-time and efficient channel estimation, drastically increasing data volume, escalating computational complexity, requiring advanced hardware, and resulting in substantial energy consumption. To address these challenges, we propose a lightweight deep learning framework for efficient cascaded channel estimation in XL-MIMO systems, designed to minimize computational complexity and make it suitable for deployment on resource-constrained edge devices. Using spatial correlations in the channel, we introduce a patch-based training mechanism that reduces the dimensionality of input to patch-level representations while preserving essential information, allowing scalable training for large-scale systems. Simulation results under diverse conditions demonstrate that our framework significantly improves estimation accuracy and reduces computational complexity, regardless of the increasing number of antennas and RIS elements in XL-MIMO systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 225,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09640",
        "abs_url": "https://arxiv.org/abs/2507.09640",
        "pdf_url": "https://arxiv.org/pdf/2507.09640",
        "title": "Disentanglement and Assessment of Shortcuts in Ophthalmological Retinal Imaging Exams",
        "authors": [
            "Leonor Fernandes",
            "Tiago Gonçalves",
            "João Matos",
            "Luis Filipe Nakayama",
            "Jaime S. Cardoso"
        ],
        "comments": "10 pages. Under review",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Diabetic retinopathy (DR) is a leading cause of vision loss in working-age adults. While screening reduces the risk of blindness, traditional imaging is often costly and inaccessible. Artificial intelligence (AI) algorithms present a scalable diagnostic solution, but concerns regarding fairness and generalization persist. This work evaluates the fairness and performance of image-trained models in DR prediction, as well as the impact of disentanglement as a bias mitigation technique, using the diverse mBRSET fundus dataset. Three models, ConvNeXt V2, DINOv2, and Swin V2, were trained on macula images to predict DR and sensitive attributes (SAs) (e.g., age and gender/sex). Fairness was assessed between subgroups of SAs, and disentanglement was applied to reduce bias. All models achieved high DR prediction performance in diagnosing (up to 94% AUROC) and could reasonably predict age and gender/sex (91% and 77% AUROC, respectively). Fairness assessment suggests disparities, such as a 10% AUROC gap between age groups in DINOv2. Disentangling SAs from DR prediction had varying results, depending on the model selected. Disentanglement improved DINOv2 performance (2% AUROC gain), but led to performance drops in ConvNeXt V2 and Swin V2 (7% and 3%, respectively). These findings highlight the complexity of disentangling fine-grained features in fundus imaging and emphasize the importance of fairness in medical imaging AI to ensure equitable and reliable healthcare solutions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 226,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09652",
        "abs_url": "https://arxiv.org/abs/2507.09652",
        "pdf_url": "https://arxiv.org/pdf/2507.09652",
        "title": "Machine-Precision Prediction of Low-Dimensional Chaotic Systems",
        "authors": [
            "Christof Schötz",
            "Niklas Boers"
        ],
        "comments": "",
        "subjects": "Chaotic Dynamics (nlin.CD); Machine Learning (cs.LG); Dynamical Systems (math.DS)",
        "abstract": "Low-dimensional chaotic systems such as the Lorenz-63 model are commonly used to benchmark system-agnostic methods for learning dynamics from data. Here we show that learning from noise-free observations in such systems can be achieved up to machine precision: using ordinary least squares regression on high-degree polynomial features with 512-bit arithmetic, our method exceeds the accuracy of standard 64-bit numerical ODE solvers of the true underlying dynamical systems. Depending on the configuration, we obtain valid prediction times of 32 to 105 Lyapunov times for the Lorenz-63 system, dramatically outperforming prior work that reaches 13 Lyapunov times at most. We further validate our results on Thomas' Cyclically Symmetric Attractor, a non-polynomial chaotic system that is considerably more complex than the Lorenz-63 model, and show that similar results extend also to higher dimensions using the spatiotemporally chaotic Lorenz-96 model. Our findings suggest that learning low-dimensional chaotic systems from noise-free data is a solved problem.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 227,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09685",
        "abs_url": "https://arxiv.org/abs/2507.09685",
        "pdf_url": "https://arxiv.org/pdf/2507.09685",
        "title": "Symptom-Driven Personalized Proton Pump Inhibitors Therapy Using Bayesian Neural Networks and Model Predictive Control",
        "authors": [
            "Yutong Li",
            "Ilya Kolmanovsky"
        ],
        "comments": "6 pages, 5 figures",
        "subjects": "Systems and Control (eess.SY); Machine Learning (cs.LG)",
        "abstract": "Proton Pump Inhibitors (PPIs) are the standard of care for gastric acid disorders but carry significant risks when administered chronically at high doses. Precise long-term control of gastric acidity is challenged by the impracticality of invasive gastric acid monitoring beyond 72 hours and wide inter-patient variability. We propose a noninvasive, symptom-based framework that tailors PPI dosing solely on patient-reported reflux and digestive symptom patterns. A Bayesian Neural Network prediction model learns to predict patient symptoms and quantifies its uncertainty from historical symptom scores, meal, and PPIs intake data. These probabilistic forecasts feed a chance-constrained Model Predictive Control (MPC) algorithm that dynamically computes future PPI doses to minimize drug usage while enforcing acid suppression with high confidence - without any direct acid measurement. In silico studies over diverse dietary schedules and virtual patient profiles demonstrate that our learning-augmented MPC reduces total PPI consumption by 65 percent compared to standard fixed regimens, while maintaining acid suppression with at least 95 percent probability. The proposed approach offers a practical path to personalized PPI therapy, minimizing treatment burden and overdose risk without invasive sensors.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 228,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09709",
        "abs_url": "https://arxiv.org/abs/2507.09709",
        "pdf_url": "https://arxiv.org/pdf/2507.09709",
        "title": "Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces",
        "authors": [
            "Baturay Saglam",
            "Paul Kassianik",
            "Blaine Nelson",
            "Sajana Weerawardhena",
            "Yaron Singer",
            "Amin Karbasi"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Understanding the latent space geometry of large language models (LLMs) is key to interpreting their behavior and improving alignment. \\baturay{However, it remains unclear to what extent LLMs internally organize representations related to semantic understanding. To investigate this, we conduct a large-scale empirical study of hidden states in transformer-based LLMs, analyzing 11 decoder-only models across 6 scientific topics and 12 layers each. We find that high-level semantic information consistently lies in low-dimensional subspaces that form linearly separable representations across distinct domains. This separability becomes more pronounced in deeper layers and under prompts that trigger structured reasoning or alignment behaviors$\\unicode{x2013}$even when surface content is unchanged. This geometry enables simple yet effective causal interventions in hidden space; for example, reasoning patterns like chain-of-thought can be captured by a single vector direction. Together, these findings support the development of geometry-aware tools that operate directly on latent representations to detect and mitigate harmful or adversarial content, using methods such as transport-based defenses that leverage this separability. As a proof of concept, we demonstrate this potential by training a simple MLP classifier as a lightweight latent-space guardrail, which detects adversarial and malicious prompts with high precision.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 229,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09711",
        "abs_url": "https://arxiv.org/abs/2507.09711",
        "pdf_url": "https://arxiv.org/pdf/2507.09711",
        "title": "Phase transition of the Sinkhorn-Knopp algorithm",
        "authors": [
            "Kun He"
        ],
        "comments": "44 pages, 2 figures",
        "subjects": "Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "The matrix scaling problem, particularly the Sinkhorn-Knopp algorithm, has been studied for over 60 years. In practice, the algorithm often yields high-quality approximations within just a few iterations. Theoretically, however, the best-known upper bound places it in the class of pseudopolynomial-time approximation algorithms. Meanwhile, the lower-bound landscape remains largely unexplored. Two fundamental questions persist: what accounts for the algorithm's strong empirical performance, and can a tight bound on its iteration count be established? For an $n\\times n$ matrix, its normalized version is obtained by dividing each entry by its largest entry. We say that a normalized matrix has a density $\\gamma$ if there exists a constant $\\rho > 0$ such that one row or column has exactly $\\lceil \\gamma n \\rceil$ entries with values at least $\\rho$, and every other row and column has at least $\\lceil \\gamma n \\rceil$ such entries. For the upper bound, we show that the Sinkhorn-Knopp algorithm produces a nearly doubly stochastic matrix in $O(\\log n - \\log \\varepsilon)$ iterations and $\\widetilde{O}(n^2)$ time for all nonnegative square matrices whose normalized version has a density $\\gamma > 1/2$. Such matrices cover both the algorithm's principal practical inputs and its typical theoretical regime, and the $\\widetilde{O}(n^2)$ runtime is optimal. For the lower bound, we establish a tight bound of $\\widetilde{\\Omega}\\left(n^{1/2}/\\varepsilon\\right)$ iterations for positive matrices under the $\\ell_2$-norm error measure. Moreover, for every $\\gamma < 1/2$, there exists a matrix with density $\\gamma$ for which the algorithm requires $\\Omega\\left(n^{1/2}/\\varepsilon\\right)$ iterations. In summary, our results reveal a sharp phase transition in the Sinkhorn-Knopp algorithm at the density threshold $\\gamma = 1/2$.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 230,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09717",
        "abs_url": "https://arxiv.org/abs/2507.09717",
        "pdf_url": "https://arxiv.org/pdf/2507.09717",
        "title": "Signed Graph Learning: Algorithms and Theory",
        "authors": [
            "Abdullah Karaaslanli",
            "Bisakh Banerjee",
            "Tapabrata Maiti",
            "Selin Aviyente"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Signal Processing (eess.SP)",
        "abstract": "Real-world data is often represented through the relationships between data samples, forming a graph structure. In many applications, it is necessary to learn this graph structure from the observed data. Current graph learning research has primarily focused on unsigned graphs, which consist only of positive edges. However, many biological and social systems are better described by signed graphs that account for both positive and negative interactions, capturing similarity and dissimilarity between samples. In this paper, we develop a method for learning signed graphs from a set of smooth signed graph signals. Specifically, we employ the net Laplacian as a graph shift operator (GSO) to define smooth signed graph signals as the outputs of a low-pass signed graph filter defined by the net Laplacian. The signed graph is then learned by formulating a non-convex optimization problem where the total variation of the observed signals is minimized with respect to the net Laplacian. The proposed problem is solved using alternating direction method of multipliers (ADMM) and a fast algorithm reducing the per-ADMM iteration complexity from quadratic to linear in the number of nodes is introduced. Furthermore, theoretical proofs of convergence for the algorithm and a bound on the estimation error of the learned net Laplacian as a function of sample size, number of nodes, and graph topology are provided. Finally, the proposed method is evaluated on simulated data and gene regulatory network inference problem and compared to existing signed graph learning methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 231,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09740",
        "abs_url": "https://arxiv.org/abs/2507.09740",
        "pdf_url": "https://arxiv.org/pdf/2507.09740",
        "title": "Discovering Governing Equations in the Presence of Uncertainty",
        "authors": [
            "Ridwan Olabiyi",
            "Han Hu",
            "Ashif Iquebal"
        ],
        "comments": "24 pages, 5 figures",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Dynamical Systems (math.DS); Data Analysis, Statistics and Probability (physics.data-an)",
        "abstract": "In the study of complex dynamical systems, understanding and accurately modeling the underlying physical processes is crucial for predicting system behavior and designing effective interventions. Yet real-world systems exhibit pronounced input (or system) variability and are observed through noisy, limited data conditions that confound traditional discovery methods that assume fixed-coefficient deterministic models. In this work, we theorize that accounting for system variability together with measurement noise is the key to consistently discover the governing equations underlying dynamical systems. As such, we introduce a stochastic inverse physics-discovery (SIP) framework that treats the unknown coefficients as random variables and infers their posterior distribution by minimizing the Kullback-Leibler divergence between the push-forward of the posterior samples and the empirical data distribution. Benchmarks on four canonical problems -- the Lotka-Volterra predator-prey system (multi- and single-trajectory), the historical Hudson Bay lynx-hare data, the chaotic Lorenz attractor, and fluid infiltration in porous media using low- and high-viscosity liquids -- show that SIP consistently identifies the correct equations and lowers coefficient root-mean-square error by an average of 82\\% relative to the Sparse Identification of Nonlinear Dynamics (SINDy) approach and its Bayesian variant. The resulting posterior distributions yield 95\\% credible intervals that closely track the observed trajectories, providing interpretable models with quantified uncertainty. SIP thus provides a robust, data-efficient approach for consistent physics discovery in noisy, variable, and data-limited settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 232,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09750",
        "abs_url": "https://arxiv.org/abs/2507.09750",
        "pdf_url": "https://arxiv.org/pdf/2507.09750",
        "title": "MB-RIRs: a Synthetic Room Impulse Response Dataset with Frequency-Dependent Absorption Coefficients",
        "authors": [
            "Enric Gusó",
            "Joanna Luberadzka",
            "Umut Sayin",
            "Xavier Serra"
        ],
        "comments": "Accepted to WASPAA25",
        "subjects": "Sound (cs.SD); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)",
        "abstract": "We investigate the effects of four strategies for improving the ecological validity of synthetic room impulse response (RIR) datasets for monoaural Speech Enhancement (SE). We implement three features on top of the traditional image source method-based (ISM) shoebox RIRs: multiband absorption coefficients, source directivity and receiver directivity. We additionally consider mesh-based RIRs from the SoundSpaces dataset. We then train a DeepFilternet3 model for each RIR dataset and evaluate the performance on a test set of real RIRs both objectively and subjectively. We find that RIRs which use frequency-dependent acoustic absorption coefficients (MB-RIRs) can obtain +0.51dB of SDR and a +8.9 MUSHRA score when evaluated on real RIRs. The MB-RIRs dataset is publicly available for free download.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 233,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09757",
        "abs_url": "https://arxiv.org/abs/2507.09757",
        "pdf_url": "https://arxiv.org/pdf/2507.09757",
        "title": "Energy Dissipation Rate Guided Adaptive Sampling for Physics-Informed Neural Networks: Resolving Surface-Bulk Dynamics in Allen-Cahn Systems",
        "authors": [
            "Chunyan Li",
            "Wenkai Yu",
            "Qi Wang"
        ],
        "comments": "32 pages, 22 figures",
        "subjects": "Numerical Analysis (math.NA); Machine Learning (cs.LG)",
        "abstract": "We introduce the Energy Dissipation Rate guided Adaptive Sampling (EDRAS) strategy, a novel method that substantially enhances the performance of Physics-Informed Neural Networks (PINNs) in solving thermodynamically consistent partial differential equations (PDEs) over arbitrary domains. EDRAS leverages the local energy dissipation rate density as a guiding metric to identify and adaptively re-sample critical collocation points from both the interior and boundary of the computational domain. This dynamical sampling approach improves the accuracy of residual-based PINNs by aligning the training process with the underlying physical structure of the system. In this study, we demonstrate the effectiveness of EDRAS using the Allen-Cahn phase field model in irregular geometries, achieving up to a sixfold reduction in the relative mean square error compared to traditional residual-based adaptive refinement (RAR) methods. Moreover, we compare EDRAS with other residual-based adaptive sampling approaches and show that EDRAS is not only computationally more efficient but also more likely to identify high-impact collocation points. Through numerical solutions of the Allen-Cahn equation with both static (Neumann) and dynamic boundary conditions in 2D disk- and ellipse-shaped domains solved using PINN coupled with EDRAS, we gain significant insights into how dynamic boundary conditions influence bulk phase evolution and thermodynamic behavior. The proposed approach offers an effective, physically informed enhancement to PINN frameworks for solving thermodynamically consistent models, making PINN a robust and versatile computational tool for investigating complex thermodynamic processes in arbitrary geometries.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 234,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09758",
        "abs_url": "https://arxiv.org/abs/2507.09758",
        "pdf_url": "https://arxiv.org/pdf/2507.09758",
        "title": "Your Pretrained Model Tells the Difficulty Itself: A Self-Adaptive Curriculum Learning Paradigm for Natural Language Understanding",
        "authors": [
            "Qi Feng",
            "Yihong Liu",
            "Hinrich Schütze"
        ],
        "comments": "18 pages, 23 figures. To appear in ACL 2025 Student Research Workshop (SRW)",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Curriculum learning is a widely adopted training strategy in natural language processing (NLP), where models are exposed to examples organized by increasing difficulty to enhance learning efficiency and performance. However, most existing approaches rely on manually defined difficulty metrics -- such as text length -- which may not accurately reflect the model's own perspective. To overcome this limitation, we present a self-adaptive curriculum learning paradigm that prioritizes fine-tuning examples based on difficulty scores predicted by pre-trained language models (PLMs) themselves. Building on these scores, we explore various training strategies that differ in the ordering of examples for the fine-tuning: from easy-to-hard, hard-to-easy, to mixed sampling. We evaluate our method on four natural language understanding (NLU) datasets covering both binary and multi-class classification tasks. Experimental results show that our approach leads to faster convergence and improved performance compared to standard random sampling.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 235,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09782",
        "abs_url": "https://arxiv.org/abs/2507.09782",
        "pdf_url": "https://arxiv.org/pdf/2507.09782",
        "title": "Physics-informed neural networks for high-dimensional solutions and snaking bifurcations in nonlinear lattices",
        "authors": [
            "Muhammad Luthfi Shahab",
            "Fidya Almira Suheri",
            "Rudy Kusdiantara",
            "Hadi Susanto"
        ],
        "comments": "Accepted for publication in Physica D: Nonlinear Phenomena",
        "subjects": "Numerical Analysis (math.NA); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Optimization and Control (math.OC)",
        "abstract": "This paper introduces a framework based on physics-informed neural networks (PINNs) for addressing key challenges in nonlinear lattices, including solution approximation, bifurcation diagram construction, and linear stability analysis. We first employ PINNs to approximate solutions of nonlinear systems arising from lattice models, using the Levenberg-Marquardt algorithm to optimize network weights for greater accuracy. To enhance computational efficiency in high-dimensional settings, we integrate a stochastic sampling strategy. We then extend the method by coupling PINNs with a continuation approach to compute snaking bifurcation diagrams, incorporating an auxiliary equation to effectively track successive solution branches. For linear stability analysis, we adapt PINNs to compute eigenvectors, introducing output constraints to enforce positivity, in line with Sturm-Liouville theory. Numerical experiments are conducted on the discrete Allen-Cahn equation with cubic and quintic nonlinearities in one to five spatial dimensions. The results demonstrate that the proposed approach achieves accuracy comparable to, or better than, traditional numerical methods, especially in high-dimensional regimes where computational resources are a limiting factor. These findings highlight the potential of neural networks as scalable and efficient tools for the study of complex nonlinear lattice systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 236,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09795",
        "abs_url": "https://arxiv.org/abs/2507.09795",
        "pdf_url": "https://arxiv.org/pdf/2507.09795",
        "title": "NegRefine: Refining Negative Label-Based Zero-Shot OOD Detection",
        "authors": [
            "Amirhossein Ansari",
            "Ke Wang",
            "Pulei Xiong"
        ],
        "comments": "Accepted to ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Recent advancements in Vision-Language Models like CLIP have enabled zero-shot OOD detection by leveraging both image and textual label information. Among these, negative label-based methods such as NegLabel and CSP have shown promising results by utilizing a lexicon of words to define negative labels for distinguishing OOD samples. However, these methods suffer from detecting in-distribution samples as OOD due to negative labels that are subcategories of in-distribution labels or proper nouns. They also face limitations in handling images that match multiple in-distribution and negative labels. We propose NegRefine, a novel negative label refinement framework for zero-shot OOD detection. By introducing a filtering mechanism to exclude subcategory labels and proper nouns from the negative label set and incorporating a multi-matching-aware scoring function that dynamically adjusts the contributions of multiple labels matching an image, NegRefine ensures a more robust separation between in-distribution and OOD samples. We evaluate NegRefine on large-scale benchmarks, including ImageNet-1K. Source code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 237,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09823",
        "abs_url": "https://arxiv.org/abs/2507.09823",
        "pdf_url": "https://arxiv.org/pdf/2507.09823",
        "title": "Nesterov Finds GRAAL: Optimal and Adaptive Gradient Method for Convex Optimization",
        "authors": [
            "Ekaterina Borodich",
            "Dmitry Kovalev"
        ],
        "comments": "",
        "subjects": "Optimization and Control (math.OC); Machine Learning (cs.LG)",
        "abstract": "In this paper, we focus on the problem of minimizing a continuously differentiable convex objective function $\\min_x f(x)$. Recently, several adaptive gradient methods, including GRAAL (Malitsky, 2020), have been developed. These methods estimate the local curvature of the objective function to compute stepsizes, attain the standard convergence rate $\\mathcal{O}(1/k)$ of fixed-stepsize gradient descent for Lipschitz-smooth functions, and do not require any line search procedures or hyperparameter tuning. However, a natural question arises: is it possible to accelerate the convergence of these algorithms to match the optimal rate $\\mathcal{O}(1/k^2)$ of the accelerated gradient descent of Nesterov (1983)? Although some attempts have been made (Li and Lan, 2023), the capabilities of the existing accelerated algorithms to adapt to the curvature of the objective function are highly limited. Consequently, we provide a positive answer to this question and develop GRAAL with Nesterov acceleration. We prove that our algorithm achieves the desired optimal convergence rate for Lipschitz smooth functions. Moreover, in contrast to existing methods, it does so with an arbitrary, even excessively small, initial stepsize at the cost of a logarithmic additive term in the iteration complexity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 238,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09828",
        "abs_url": "https://arxiv.org/abs/2507.09828",
        "pdf_url": "https://arxiv.org/pdf/2507.09828",
        "title": "Regret Analysis of Posterior Sampling-Based Expected Improvement for Bayesian Optimization",
        "authors": [
            "Shion Takeno",
            "Yu Inatsu",
            "Masayuki Karasuyama",
            "Ichiro Takeuchi"
        ],
        "comments": "35pages, 5 figures, fix trivial errors",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Bayesian optimization is a powerful tool for optimizing an expensive-to-evaluate black-box function. In particular, the effectiveness of expected improvement (EI) has been demonstrated in a wide range of applications. However, theoretical analyses of EI are limited compared with other theoretically established algorithms. This paper analyzes a randomized variant of EI, which evaluates the EI from the maximum of the posterior sample path. We show that this posterior sampling-based random EI achieves the sublinear Bayesian cumulative regret bounds under the assumption that the black-box function follows a Gaussian process. Finally, we demonstrate the effectiveness of the proposed method through numerical experiments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 239,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09830",
        "abs_url": "https://arxiv.org/abs/2507.09830",
        "pdf_url": "https://arxiv.org/pdf/2507.09830",
        "title": "Hierarchical Abstraction Enables Human-Like 3D Object Recognition in Deep Learning Models",
        "authors": [
            "Shuhao Fu",
            "Philip J. Kellman",
            "Hongjing Lu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Both humans and deep learning models can recognize objects from 3D shapes depicted with sparse visual information, such as a set of points randomly sampled from the surfaces of 3D objects (termed a point cloud). Although deep learning models achieve human-like performance in recognizing objects from 3D shapes, it remains unclear whether these models develop 3D shape representations similar to those used by human vision for object recognition. We hypothesize that training with 3D shapes enables models to form representations of local geometric structures in 3D shapes. However, their representations of global 3D object shapes may be limited. We conducted two human experiments systematically manipulating point density and object orientation (Experiment 1), and local geometric structure (Experiment 2). Humans consistently performed well across all experimental conditions. We compared two types of deep learning models, one based on a convolutional neural network (DGCNN) and the other on visual transformers (point transformer), with human performance. We found that the point transformer model provided a better account of human performance than the convolution-based model. The advantage mainly results from the mechanism in the point transformer model that supports hierarchical abstraction of 3D shapes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 240,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09836",
        "abs_url": "https://arxiv.org/abs/2507.09836",
        "pdf_url": "https://arxiv.org/pdf/2507.09836",
        "title": "Multi-residual Mixture of Experts Learning for Cooperative Control in Multi-vehicle Systems",
        "authors": [
            "Vindula Jayawardana",
            "Sirui Li",
            "Yashar Farid",
            "Cathy Wu"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multiagent Systems (cs.MA); Systems and Control (eess.SY)",
        "abstract": "Autonomous vehicles (AVs) are becoming increasingly popular, with their applications now extending beyond just a mode of transportation to serving as mobile actuators of a traffic flow to control flow dynamics. This contrasts with traditional fixed-location actuators, such as traffic signals, and is referred to as Lagrangian traffic control. However, designing effective Lagrangian traffic control policies for AVs that generalize across traffic scenarios introduces a major challenge. Real-world traffic environments are highly diverse, and developing policies that perform robustly across such diverse traffic scenarios is challenging. It is further compounded by the joint complexity of the multi-agent nature of traffic systems, mixed motives among participants, and conflicting optimization objectives subject to strict physical and external constraints. To address these challenges, we introduce Multi-Residual Mixture of Expert Learning (MRMEL), a novel framework for Lagrangian traffic control that augments a given suboptimal nominal policy with a learned residual while explicitly accounting for the structure of the traffic scenario space. In particular, taking inspiration from residual reinforcement learning, MRMEL augments a suboptimal nominal AV control policy by learning a residual correction, but at the same time dynamically selects the most suitable nominal policy from a pool of nominal policies conditioned on the traffic scenarios and modeled as a mixture of experts. We validate MRMEL using a case study in cooperative eco-driving at signalized intersections in Atlanta, Dallas Fort Worth, and Salt Lake City, with real-world data-driven traffic scenarios. The results show that MRMEL consistently yields superior performance-achieving an additional 4%-9% reduction in aggregate vehicle emissions relative to the strongest baseline in each setting.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 241,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09864",
        "abs_url": "https://arxiv.org/abs/2507.09864",
        "pdf_url": "https://arxiv.org/pdf/2507.09864",
        "title": "Intersection of Reinforcement Learning and Bayesian Optimization for Intelligent Control of Industrial Processes: A Safe MPC-based DPG using Multi-Objective BO",
        "authors": [
            "Hossein Nejatbakhsh Esfahani",
            "Javad Mohammadpour Velni"
        ],
        "comments": "",
        "subjects": "Systems and Control (eess.SY); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Optimization and Control (math.OC)",
        "abstract": "Model Predictive Control (MPC)-based Reinforcement Learning (RL) offers a structured and interpretable alternative to Deep Neural Network (DNN)-based RL methods, with lower computational complexity and greater transparency. However, standard MPC-RL approaches often suffer from slow convergence, suboptimal policy learning due to limited parameterization, and safety issues during online adaptation. To address these challenges, we propose a novel framework that integrates MPC-RL with Multi-Objective Bayesian Optimization (MOBO). The proposed MPC-RL-MOBO utilizes noisy evaluations of the RL stage cost and its gradient, estimated via a Compatible Deterministic Policy Gradient (CDPG) approach, and incorporates them into a MOBO algorithm using the Expected Hypervolume Improvement (EHVI) acquisition function. This fusion enables efficient and safe tuning of the MPC parameters to achieve improved closed-loop performance, even under model imperfections. A numerical example demonstrates the effectiveness of the proposed approach in achieving sample-efficient, stable, and high-performance learning for control systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 242,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09875",
        "abs_url": "https://arxiv.org/abs/2507.09875",
        "pdf_url": "https://arxiv.org/pdf/2507.09875",
        "title": "Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition",
        "authors": [
            "Qinyuan Ye",
            "Robin Jia",
            "Xiang Ren"
        ],
        "comments": "Code: this https URL",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Large language models demonstrate the intriguing ability to perform unseen tasks via in-context learning. However, it remains unclear what mechanisms inside the model drive such task-level generalization. In this work, we approach this question through the lens of off-by-one addition (i.e., 1+1=3, 2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function as a second step. Leveraging circuit-style interpretability techniques such as path patching, we analyze the models' internal computations behind their notable performance and present three key findings. First, we uncover a function induction mechanism that explains the model's generalization from standard addition to off-by-one addition. This mechanism resembles the structure of the induction head mechanism found in prior work and elevates it to a higher level of abstraction. Second, we show that the induction of the +1 function is governed by multiple attention heads in parallel, each of which emits a distinct piece of the +1 function. Finally, we find that this function induction mechanism is reused in a broader range of tasks, including synthetic tasks such as shifted multiple-choice QA and algorithmic tasks such as base-8 addition. Overall, our findings offer deeper insights into how reusable and composable structures within language models enable task-level generalization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 243,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09891",
        "abs_url": "https://arxiv.org/abs/2507.09891",
        "pdf_url": "https://arxiv.org/pdf/2507.09891",
        "title": "Sequence-Model-Guided Measurement Selection for Quantum State Learning",
        "authors": [
            "Jiaxin Huang",
            "Yan Zhu",
            "Giulio Chiribella",
            "Ya-Dong Wu"
        ],
        "comments": "",
        "subjects": "Quantum Physics (quant-ph); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Characterization of quantum systems from experimental data is a central problem in quantum science and technology. But which measurements should be used to gather data in the first place? While optimal measurement choices can be worked out for small quantum systems, the optimization becomes intractable as the system size grows large. To address this problem, we introduce a deep neural network with a sequence model architecture that searches for efficient measurement choices in a data-driven, adaptive manner. The model can be applied to a variety of tasks, including the prediction of linear and nonlinear properties of quantum states, as well as state clustering and state tomography tasks. In all these tasks, we find that the measurement choices identified by our neural network consistently outperform the uniformly random choice. Intriguingly, for topological quantum systems, our model tends to recommend measurements at the system's boundaries, even when the task is to predict bulk properties. This behavior suggests that the neural network may have independently discovered a connection between boundaries and bulk, without having been provided any built-in knowledge of quantum physics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 244,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09898",
        "abs_url": "https://arxiv.org/abs/2507.09898",
        "pdf_url": "https://arxiv.org/pdf/2507.09898",
        "title": "Advanced U-Net Architectures with CNN Backbones for Automated Lung Cancer Detection and Segmentation in Chest CT Images",
        "authors": [
            "Alireza Golkarieha",
            "Kiana Kiashemshakib",
            "Sajjad Rezvani Boroujenic",
            "Nasibeh Asadi Isakand"
        ],
        "comments": "This manuscript has 20 pages and 10 figures. It is submitted to the Journal 'Scientific Reports'",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "This study investigates the effectiveness of U-Net architectures integrated with various convolutional neural network (CNN) backbones for automated lung cancer detection and segmentation in chest CT images, addressing the critical need for accurate diagnostic tools in clinical settings. A balanced dataset of 832 chest CT images (416 cancerous and 416 non-cancerous) was preprocessed using Contrast Limited Adaptive Histogram Equalization (CLAHE) and resized to 128x128 pixels. U-Net models were developed with three CNN backbones: ResNet50, VGG16, and Xception, to segment lung regions. After segmentation, CNN-based classifiers and hybrid models combining CNN feature extraction with traditional machine learning classifiers (Support Vector Machine, Random Forest, and Gradient Boosting) were evaluated using 5-fold cross-validation. Metrics included accuracy, precision, recall, F1-score, Dice coefficient, and ROC-AUC. U-Net with ResNet50 achieved the best performance for cancerous lungs (Dice: 0.9495, Accuracy: 0.9735), while U-Net with VGG16 performed best for non-cancerous segmentation (Dice: 0.9532, Accuracy: 0.9513). For classification, the CNN model using U-Net with Xception achieved 99.1 percent accuracy, 99.74 percent recall, and 99.42 percent F1-score. The hybrid CNN-SVM-Xception model achieved 96.7 percent accuracy and 97.88 percent F1-score. Compared to prior methods, our framework consistently outperformed existing models. In conclusion, combining U-Net with advanced CNN backbones provides a powerful method for both segmentation and classification of lung cancer in CT scans, supporting early diagnosis and clinical decision-making.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 245,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09924",
        "abs_url": "https://arxiv.org/abs/2507.09924",
        "pdf_url": "https://arxiv.org/pdf/2507.09924",
        "title": "MixLoRA-DSI: Dynamically Expandable Mixture-of-LoRA Experts for Rehearsal-Free Generative Retrieval over Dynamic Corpora",
        "authors": [
            "Tuan-Luc Huynh",
            "Thuy-Trang Vu",
            "Weiqing Wang",
            "Trung Le",
            "Dragan Gašević",
            "Yuan-Fang Li",
            "Thanh-Toan Do"
        ],
        "comments": "",
        "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Continually updating model-based indexes in generative retrieval with new documents remains challenging, as full retraining is computationally expensive and impractical under resource constraints. We propose MixLoRA-DSI, a novel framework that combines an expandable mixture of Low-Rank Adaptation experts with a layer-wise out-of-distribution (OOD)-driven expansion strategy. Instead of allocating new experts for each new corpus, our proposed expansion strategy enables sublinear parameter growth by selectively introducing new experts only when significant number of OOD documents are detected. Experiments on NQ320k and MS MARCO Passage demonstrate that MixLoRA-DSI outperforms full-model update baselines, with minimal parameter overhead and substantially lower training costs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 246,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09929",
        "abs_url": "https://arxiv.org/abs/2507.09929",
        "pdf_url": "https://arxiv.org/pdf/2507.09929",
        "title": "Aligning Generative Speech Enhancement with Human Preferences via Direct Preference Optimization",
        "authors": [
            "Haoyang Li",
            "Nana Hou",
            "Yuchen Hu",
            "Jixun Yao",
            "Sabato Marco Siniscalchi",
            "Eng Siong Chng"
        ],
        "comments": "",
        "subjects": "Audio and Speech Processing (eess.AS); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "This work investigates speech enhancement (SE) from the perspective of language models (LMs). We propose a novel method that leverages Direct Preference Optimization (DPO) to improve the perceptual quality of enhanced speech. Using UTMOS, a neural MOS prediction model, as a proxy for human ratings, our approach guides optimization toward perceptually preferred outputs. This differs from existing LM-based SE methods that focus on maximizing the likelihood of clean speech tokens, which may misalign with human perception and degrade quality despite low prediction error. Experiments on the 2020 Deep Noise Suppression Challenge test sets demonstrate that applying DPO to a pretrained LM-based SE model yields consistent improvements across various speech quality metrics, with relative gains of up to 56%. To our knowledge, this is the first application of DPO to SE and the first to incorporate proxy perceptual feedback into LM-based SE training, pointing to a promising direction for perceptually aligned SE.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 247,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.09966",
        "abs_url": "https://arxiv.org/abs/2507.09966",
        "pdf_url": "https://arxiv.org/pdf/2507.09966",
        "title": "A Brain Tumor Segmentation Method Based on CLIP and 3D U-Net with Cross-Modal Semantic Guidance and Multi-Level Feature Fusion",
        "authors": [
            "Mingda Zhang"
        ],
        "comments": "13 pages,6 figures",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Precise segmentation of brain tumors from magnetic resonance imaging (MRI) is essential for neuro-oncology diagnosis and treatment planning. Despite advances in deep learning methods, automatic segmentation remains challenging due to tumor morphological heterogeneity and complex three-dimensional spatial relationships. Current techniques primarily rely on visual features extracted from MRI sequences while underutilizing semantic knowledge embedded in medical reports. This research presents a multi-level fusion architecture that integrates pixel-level, feature-level, and semantic-level information, facilitating comprehensive processing from low-level data to high-level concepts. The semantic-level fusion pathway combines the semantic understanding capabilities of Contrastive Language-Image Pre-training (CLIP) models with the spatial feature extraction advantages of 3D U-Net through three mechanisms: 3D-2D semantic bridging, cross-modal semantic guidance, and semantic-based attention mechanisms. Experimental validation on the BraTS 2020 dataset demonstrates that the proposed model achieves an overall Dice coefficient of 0.8567, representing a 4.8% improvement compared to traditional 3D U-Net, with a 7.3% Dice coefficient increase in the clinically important enhancing tumor (ET) region.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 248,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10015",
        "abs_url": "https://arxiv.org/abs/2507.10015",
        "pdf_url": "https://arxiv.org/pdf/2507.10015",
        "title": "(Almost) Free Modality Stitching of Foundation Models",
        "authors": [
            "Jaisidh Singh",
            "Diganta Misra",
            "Boris Knyazev",
            "Antonio Orvieto"
        ],
        "comments": "Pre-print",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Foundation multi-modal models are often designed by stitching of multiple existing pretrained uni-modal models: for example, an image classifier with an text model. This stitching process is performed by training a connector module that aims to align the representation spaces of these uni-modal models towards a multi-modal objective. However, given the complexity of training such connectors on large scale web-based datasets coupled with the ever-increasing number of available pretrained uni-modal models, the task of uni-modal models selection and subsequent connector module training becomes computationally demanding. To address this under-studied critical problem, we propose Hypernetwork Model Alignment (Hyma), a novel all-in-one solution for optimal uni-modal model selection and connector training by leveraging hypernetworks. Specifically, our framework utilizes the parameter prediction capability of a hypernetwork to obtain jointly trained connector modules for $N \\times M$ combinations of uni-modal models. In our experiments, Hyma reduces the cost of searching for the best performing uni-modal model pair by $10\\times$, while matching the ranking and trained connector performance obtained via grid search across a suite of diverse multi-modal benchmarks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 249,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10029",
        "abs_url": "https://arxiv.org/abs/2507.10029",
        "pdf_url": "https://arxiv.org/pdf/2507.10029",
        "title": "Memory-Efficient Personalization of Text-to-Image Diffusion Models via Selective Optimization Strategies",
        "authors": [
            "Seokeon Choi",
            "Sunghyun Park",
            "Hyoungwoo Park",
            "Jeongho Kim",
            "Sungrack Yun"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Memory-efficient personalization is critical for adapting text-to-image diffusion models while preserving user privacy and operating within the limited computational resources of edge devices. To this end, we propose a selective optimization framework that adaptively chooses between backpropagation on low-resolution images (BP-low) and zeroth-order optimization on high-resolution images (ZO-high), guided by the characteristics of the diffusion process. As observed in our experiments, BP-low efficiently adapts the model to target-specific features, but suffers from structural distortions due to resolution mismatch. Conversely, ZO-high refines high-resolution details with minimal memory overhead but faces slow convergence when applied without prior adaptation. By complementing both methods, our framework leverages BP-low for effective personalization while using ZO-high to maintain structural consistency, achieving memory-efficient and high-quality fine-tuning. To maximize the efficacy of both BP-low and ZO-high, we introduce a timestep-aware probabilistic function that dynamically selects the appropriate optimization strategy based on diffusion timesteps. This function mitigates the overfitting from BP-low at high timesteps, where structural information is critical, while ensuring ZO-high is applied more effectively as training progresses. Experimental results demonstrate that our method achieves competitive performance while significantly reducing memory consumption, enabling scalable, high-quality on-device personalization without increasing inference latency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 250,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10056",
        "abs_url": "https://arxiv.org/abs/2507.10056",
        "pdf_url": "https://arxiv.org/pdf/2507.10056",
        "title": "Lightweight Model for Poultry Disease Detection from Fecal Images Using Multi-Color Space Feature Optimization and Machine Learning",
        "authors": [
            "A. K. M. Shoriful Islam",
            "Md. Rakib Hassan",
            "Macbah Uddin",
            "Md. Shahidur Rahman"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Poultry farming is a vital component of the global food supply chain, yet it remains highly vulnerable to infectious diseases such as coccidiosis, salmonellosis, and Newcastle disease. This study proposes a lightweight machine learning-based approach to detect these diseases by analyzing poultry fecal images. We utilize multi-color space feature extraction (RGB, HSV, LAB) and explore a wide range of color, texture, and shape-based descriptors, including color histograms, local binary patterns (LBP), wavelet transforms, and edge detectors. Through a systematic ablation study and dimensionality reduction using PCA and XGBoost feature selection, we identify a compact global feature set that balances accuracy and computational efficiency. An artificial neural network (ANN) classifier trained on these features achieved 95.85% accuracy while requiring no GPU and only 638 seconds of execution time in Google Colab. Compared to deep learning models such as Xception and MobileNetV3, our proposed model offers comparable accuracy with drastically lower resource usage. This work demonstrates a cost-effective, interpretable, and scalable alternative to deep learning for real-time poultry disease detection in low-resource agricultural settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 251,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10057",
        "abs_url": "https://arxiv.org/abs/2507.10057",
        "pdf_url": "https://arxiv.org/pdf/2507.10057",
        "title": "PRISM: Fine-Grained Paper-to-Paper Retrieval with Multi-Aspect-Aware Query Optimization",
        "authors": [
            "Sangwoo Park",
            "Jinheon Baek",
            "Soyeong Jeong",
            "Sung Ju Hwang"
        ],
        "comments": "",
        "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Scientific paper retrieval, particularly framed as document-to-document retrieval, aims to identify relevant papers in response to a long-form query paper, rather than a short query string. Previous approaches to this task have focused on abstracts, embedding them into dense vectors as surrogates for full documents and calculating similarity across them, although abstracts provide only sparse and high-level summaries. To address this, we propose PRISM, a novel document-to-document retrieval method that introduces multiple, fine-grained representations for both the query and candidate papers. In particular, each query paper is decomposed into multiple aspect-specific views and individually embedded, which are then matched against candidate papers similarity segmented to consider their multifaceted dimensions. Moreover, we present SciFullBench, a novel benchmark in which the complete and segmented context of full papers for both queries and candidates is available. Then, experimental results show that PRISM improves performance by an average of 4.3% over existing retrieval baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 252,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10069",
        "abs_url": "https://arxiv.org/abs/2507.10069",
        "pdf_url": "https://arxiv.org/pdf/2507.10069",
        "title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal Parallelism",
        "authors": [
            "Zedong Liu",
            "Shenggan Cheng",
            "Guangming Tan",
            "Yang You",
            "Dingwen Tao"
        ],
        "comments": "",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)",
        "abstract": "Multimodal large language models (MLLMs) extend LLMs to handle images, videos, and audio by incorporating feature extractors and projection modules. However, these additional components -- combined with complex inference pipelines and heterogeneous workloads -- introduce significant inference overhead. Therefore, efficiently serving MLLMs remains a major challenge. Current tightly coupled serving architectures struggle to distinguish between mixed request types or adapt parallelism strategies to different inference stages, leading to increased time-to-first-token (TTFT) latency and poor resource utilization. To address this, we propose Elastic Multimodal Parallelism (EMP), a new serving paradigm that elastically adapts to resource heterogeneity across request types and inference stages. Building upon EMP, we develop ElasticMM, an MLLM serving system that (1) separates requests into independent modality groups with dynamic resource allocation via a modality-aware load balancer; (2) decouples inference stages and enables parallelism adjustment and adaptive scaling via elastic partition scheduling; and (3) improves inference efficiency through unified multimodal prefix caching and non-blocking encoding. Experiments on diverse real-world datasets show that ElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by up to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level objectives (SLOs).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 253,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10084",
        "abs_url": "https://arxiv.org/abs/2507.10084",
        "pdf_url": "https://arxiv.org/pdf/2507.10084",
        "title": "A Transfer Learning-Based Method for Water Body Segmentation in Remote Sensing Imagery: A Case Study of the Zhada Tulin Area",
        "authors": [
            "Haonan Chen",
            "Xin Tong"
        ],
        "comments": "13 pages, 6 figures, 2 tables",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "To address the prevalent challenges of domain shift and small sample sizes in remote sensing image water body segmentation, this study proposes and validates a two-stage transfer learning strategy based on the SegFormer model. The approach begins by training a foundational segmentation model on a diverse source domain, where it achieves an Intersection over Union (IoU) of 68.80% on its validation set, followed by fine-tuning on data from the distinct target domain. Focusing on the Zhada Tulin area in Tibet -- a region characterized by highly complex topography and spectral features -- the experimental results demonstrate that this strategy significantly boosts the IoU for the water body segmentation task from 25.50% (for direct transfer) to 64.84%. This not only effectively resolves the model performance degradation caused by domain discrepancy but also provides an effective technical paradigm for high-precision thematic information extraction in data-scarce and environmentally unique remote sensing scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 254,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10119",
        "abs_url": "https://arxiv.org/abs/2507.10119",
        "pdf_url": "https://arxiv.org/pdf/2507.10119",
        "title": "Analysis of AI Techniques for Orchestrating Edge-Cloud Application Migration",
        "authors": [
            "Sadig Gojayev",
            "Ahmad Anaqreh",
            "Carolina Fortuna"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Application migration in edge-cloud system enables high QoS and cost effective service delivery. However, automatically orchestrating such migration is typically solved with heuristic approaches. Starting from the Markov Decision Process (MDP), in this paper, we identify, analyze and compare selected state-of-the-art Artificial Intelligence (AI) planning and Reinforcement Learning (RL) approaches for solving the class of edge-cloud application migration problems that can be modeled as Towers of Hanoi (ToH) problems. We introduce a new classification based on state space definition and analyze the compared models also through this lense. The aim is to understand available techniques capable of orchestrating such application migration in emerging computing continuum environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 255,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10139",
        "abs_url": "https://arxiv.org/abs/2507.10139",
        "pdf_url": "https://arxiv.org/pdf/2507.10139",
        "title": "Large-Scale Graph Building in Dynamic Environments: Low Latency and High Quality",
        "authors": [
            "Filipe Miguel Gonçalves de Almeida",
            "CJ Carey",
            "Hendrik Fichtenberger",
            "Jonathan Halcrow",
            "Silvio Lattanzi",
            "André Linhares",
            "Tao Meng",
            "Ashkan Norouzi-Fard",
            "Nikos Parotsidis",
            "Bryan Perozzi",
            "David Simcha"
        ],
        "comments": "",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)",
        "abstract": "Learning and constructing large-scale graphs has attracted attention in recent decades, resulting in a rich literature that introduced various systems, tools, and algorithms. Grale is one of such tools that is designed for offline environments and is deployed in more than 50 different industrial settings at Google. Grale is widely applicable because of its ability to efficiently learn and construct a graph on datasets with multiple types of features. However, it is often the case that applications require the underlying data to evolve continuously and rapidly and the updated graph needs to be available with low latency. Such setting make the use of Grale prohibitive. While there are Approximate Nearest Neighbor (ANN) systems that handle dynamic updates with low latency, they are mostly limited to similarities over a single embedding. In this work, we introduce a system that inherits the advantages and the quality of Grale, and maintains a graph construction in a dynamic setting with tens of milliseconds of latency per request. We call the system Dynamic Grale Using ScaNN (Dynamic GUS). Our system has a wide range of applications with over 10 deployments at Google. One of the applications is in Android Security and Privacy, where Dynamic Grale Using ScaNN enables capturing harmful applications 4 times faster, before they can reach users.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 256,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10142",
        "abs_url": "https://arxiv.org/abs/2507.10142",
        "pdf_url": "https://arxiv.org/pdf/2507.10142",
        "title": "Adaptability in Multi-Agent Reinforcement Learning: A Framework and Unified Review",
        "authors": [
            "Siyi Hu",
            "Mohamad A Hady",
            "Jianglin Qiao",
            "Jimmy Cao",
            "Mahardhika Pratama",
            "Ryszard Kowalczyk"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multiagent Systems (cs.MA)",
        "abstract": "Multi-Agent Reinforcement Learning (MARL) has shown clear effectiveness in coordinating multiple agents across simulated benchmarks and constrained scenarios. However, its deployment in real-world multi-agent systems (MAS) remains limited, primarily due to the complex and dynamic nature of such environments. These challenges arise from multiple interacting sources of variability, including fluctuating agent populations, evolving task goals, and inconsistent execution conditions. Together, these factors demand that MARL algorithms remain effective under continuously changing system configurations and operational demands. To better capture and assess this capacity for adjustment, we introduce the concept of \\textit{adaptability} as a unified and practically grounded lens through which to evaluate the reliability of MARL algorithms under shifting conditions, broadly referring to any changes in the environment dynamics that may occur during learning or execution. Centred on the notion of adaptability, we propose a structured framework comprising three key dimensions: learning adaptability, policy adaptability, and scenario-driven adaptability. By adopting this adaptability perspective, we aim to support more principled assessments of MARL performance beyond narrowly defined benchmarks. Ultimately, this survey contributes to the development of algorithms that are better suited for deployment in dynamic, real-world multi-agent systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 257,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10143",
        "abs_url": "https://arxiv.org/abs/2507.10143",
        "pdf_url": "https://arxiv.org/pdf/2507.10143",
        "title": "Deep Recurrence for Dynamical Segmentation Models",
        "authors": [
            "David Calhas",
            "Arlindo L. Oliveira"
        ],
        "comments": "12 pages",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "While biological vision systems rely heavily on feedback connections to iteratively refine perception, most artificial neural networks remain purely feedforward, processing input in a single static pass. In this work, we propose a predictive coding inspired feedback mechanism that introduces a recurrent loop from output to input, allowing the model to refine its internal state over time. We implement this mechanism within a standard U-Net architecture and introduce two biologically motivated operations, softmax projection and exponential decay, to ensure stability of the feedback loop. Through controlled experiments on a synthetic segmentation task, we show that the feedback model significantly outperforms its feedforward counterpart in noisy conditions and generalizes more effectively with limited supervision. Notably, feedback achieves above random performance with just two training examples, while the feedforward model requires at least four. Our findings demonstrate that feedback enhances robustness and data efficiency, and offer a path toward more adaptive and biologically inspired neural architectures. Code is available at: this http URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 258,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10154",
        "abs_url": "https://arxiv.org/abs/2507.10154",
        "pdf_url": "https://arxiv.org/pdf/2507.10154",
        "title": "Simulating Biases for Interpretable Fairness in Offline and Online Classifiers",
        "authors": [
            "Ricardo Inácio",
            "Zafeiris Kokkinogenis",
            "Vitor Cerqueira",
            "Carlos Soares"
        ],
        "comments": "17 pages, 2 figures, 1 equation, 3 tables: 1 in main body and 2 in the appendix. Submitted to the SynDAiTE: Synthetic Data for AI Trustworthiness and Evolution workshop from ECMLPKDD 2025, anonymized",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Predictive models often reinforce biases which were originally embedded in their training data, through skewed decisions. In such cases, mitigation methods are critical to ensure that, regardless of the prevailing disparities, model outcomes are adjusted to be fair. To assess this, datasets could be systematically generated with specific biases, to train machine learning classifiers. Then, predictive outcomes could aid in the understanding of this bias embedding process. Hence, an agent-based model (ABM), depicting a loan application process that represents various systemic biases across two demographic groups, was developed to produce synthetic datasets. Then, by applying classifiers trained on them to predict loan outcomes, we can assess how biased data leads to unfairness. This highlights a main contribution of this work: a framework for synthetic dataset generation with controllable bias injection. We also contribute with a novel explainability technique, which shows how mitigations affect the way classifiers leverage data features, via second-order Shapley values. In experiments, both offline and online learning approaches are employed. Mitigations are applied at different stages of the modelling pipeline, such as during pre-processing and in-processing.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 259,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10174",
        "abs_url": "https://arxiv.org/abs/2507.10174",
        "pdf_url": "https://arxiv.org/pdf/2507.10174",
        "title": "Should We Ever Prefer Decision Transformer for Offline Reinforcement Learning?",
        "authors": [
            "Yumi Omori",
            "Zixuan Dong",
            "Keith Ross"
        ],
        "comments": "Accepted by RLBrew: Ingredients for Developing Generalist Agents workshop (RLC 2025)",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "In recent years, extensive work has explored the application of the Transformer architecture to reinforcement learning problems. Among these, Decision Transformer (DT) has gained particular attention in the context of offline reinforcement learning due to its ability to frame return-conditioned policy learning as a sequence modeling task. Most recently, Bhargava et al. (2024) provided a systematic comparison of DT with more conventional MLP-based offline RL algorithms, including Behavior Cloning (BC) and Conservative Q-Learning (CQL), and claimed that DT exhibits superior performance in sparse-reward and low-quality data settings. In this paper, through experimentation on robotic manipulation tasks (Robomimic) and locomotion benchmarks (D4RL), we show that MLP-based Filtered Behavior Cloning (FBC) achieves competitive or superior performance compared to DT in sparse-reward environments. FBC simply filters out low-performing trajectories from the dataset and then performs ordinary behavior cloning on the filtered dataset. FBC is not only very straightforward, but it also requires less training data and is computationally more efficient. The results therefore suggest that DT is not preferable for sparse-reward environments. From prior work, arguably, DT is also not preferable for dense-reward environments. Thus, we pose the question: Is DT ever preferable?",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 260,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10178",
        "abs_url": "https://arxiv.org/abs/2507.10178",
        "pdf_url": "https://arxiv.org/pdf/2507.10178",
        "title": "Pimba: A Processing-in-Memory Acceleration for Post-Transformer Large Language Model Serving",
        "authors": [
            "Wonung Kim",
            "Yubin Lee",
            "Yoonsung Kim",
            "Jinwoo Hwang",
            "Seongryong Oh",
            "Jiyong Jung",
            "Aziz Huseynov",
            "Woong Gyu Park",
            "Chang Hyun Park",
            "Divya Mahajan",
            "Jongse Park"
        ],
        "comments": "",
        "subjects": "Hardware Architecture (cs.AR); Machine Learning (cs.LG)",
        "abstract": "Transformers are the driving force behind today's Large Language Models (LLMs), serving as the foundation for their performance and versatility. Yet, their compute and memory costs grow with sequence length, posing scalability challenges for long-context inferencing. In response, the algorithm community is exploring alternative architectures, such as state space models (SSMs), linear attention, and recurrent neural networks (RNNs), which we refer to as post-transformers. This shift presents a key challenge: building a serving system that efficiently supports both transformer and post-transformer LLMs within a unified framework. To address this challenge, we analyze the performance characteristics of transformer and post-transformer LLMs. Despite their algorithmic differences, both are fundamentally limited by memory bandwidth under batched inference due to attention in transformers and state updates in post-transformers. Further analyses suggest two additional insights: (1) state update operations, unlike attention, incur high hardware cost, making per-bank PIM acceleration inefficient, and (2) different low-precision arithmetic methods offer varying accuracy-area tradeoffs, while we identify Microsoft's MX as the Pareto-optimal choice. Building on these insights, we design Pimba as an array of State-update Processing Units (SPUs), each shared between two banks to enable interleaved access to PIM. Each SPU includes a State-update Processing Engine (SPE) that comprises element-wise multipliers and adders using MX-based quantized arithmetic, enabling efficient execution of state update and attention operations. Our evaluation shows that, compared to LLM-optimized GPU and GPU+PIM systems, Pimba achieves up to 3.2x and 2.1x higher token generation throughput, respectively.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 261,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10201",
        "abs_url": "https://arxiv.org/abs/2507.10201",
        "pdf_url": "https://arxiv.org/pdf/2507.10201",
        "title": "History Matching under Uncertainty of Geological Scenarios with Implicit Geological Realism Control with Generative Deep Learning and Graph Convolutions",
        "authors": [
            "Gleb Shishaev",
            "Vasily Demyanov",
            "Daniel Arnold"
        ],
        "comments": "Part of the completed PhD thesis this https URL",
        "subjects": "Applications (stat.AP); Machine Learning (cs.LG); Data Analysis, Statistics and Probability (physics.data-an)",
        "abstract": "The graph-based variational autoencoder represents an architecture that can handle the uncertainty of different geological scenarios, such as depositional or structural, through the concept of a lowerdimensional latent space. The main difference from recent studies is utilisation of a graph-based approach in reservoir modelling instead of the more traditional lattice-based deep learning methods. We provide a solution to implicitly control the geological realism through the latent variables of a generative model and Geodesic metrics. Our experiments of AHM with synthetic dataset that consists of 3D realisations of channelised geological representations with two distinct scenarios with one and two channels shows the viability of the approach. We offer in-depth analysis of the latent space using tools such as PCA, t-SNE, and TDA to illustrate its structure.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 262,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10222",
        "abs_url": "https://arxiv.org/abs/2507.10222",
        "pdf_url": "https://arxiv.org/pdf/2507.10222",
        "title": "Spatial Lifting for Dense Prediction",
        "authors": [
            "Mingzhi Xu",
            "Yizhe Zhang"
        ],
        "comments": "Preprint. Under review",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Image and Video Processing (eess.IV)",
        "abstract": "We present Spatial Lifting (SL), a novel methodology for dense prediction tasks. SL operates by lifting standard inputs, such as 2D images, into a higher-dimensional space and subsequently processing them using networks designed for that higher dimension, such as a 3D U-Net. Counterintuitively, this dimensionality lifting allows us to achieve good performance on benchmark tasks compared to conventional approaches, while reducing inference costs and significantly lowering the number of model parameters. The SL framework produces intrinsically structured outputs along the lifted dimension. This emergent structure facilitates dense supervision during training and enables robust, near-zero-additional-cost prediction quality assessment at test time. We validate our approach across 19 benchmark datasets (13 for semantic segmentation and 6 for depth estimation), demonstrating competitive dense prediction performance while reducing the model parameter count by over 98% (in the U-Net case) and lowering inference costs. Spatial Lifting introduces a new vision modeling paradigm that offers a promising path toward more efficient, accurate, and reliable deep networks for dense prediction tasks in vision.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 263,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10240",
        "abs_url": "https://arxiv.org/abs/2507.10240",
        "pdf_url": "https://arxiv.org/pdf/2507.10240",
        "title": "Visual Analytics for Explainable and Trustworthy Artificial Intelligence",
        "authors": [
            "Angelos Chatzimparmpas"
        ],
        "comments": "",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Our society increasingly depends on intelligent systems to solve complex problems, ranging from recommender systems suggesting the next movie to watch to AI models assisting in medical diagnoses for hospitalized patients. With the iterative improvement of diagnostic accuracy and efficiency, AI holds significant potential to mitigate medical misdiagnoses by preventing numerous deaths and reducing an economic burden of approximately 450 EUR billion annually. However, a key obstacle to AI adoption lies in the lack of transparency: many automated systems function as \"black boxes,\" providing predictions without revealing the underlying processes. This opacity can hinder experts' ability to trust and rely on AI systems. Visual analytics (VA) provides a compelling solution by combining AI models with interactive visualizations. These specialized charts and graphs empower users to incorporate their domain expertise to refine and improve the models, bridging the gap between AI and human understanding. In this work, we define, categorize, and explore how VA solutions can foster trust across the stages of a typical AI pipeline. We propose a design space for innovative visualizations and present an overview of our previously developed VA dashboards, which support critical tasks within the various pipeline stages, including data processing, feature engineering, hyperparameter tuning, understanding, debugging, refining, and comparing models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 264,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10250",
        "abs_url": "https://arxiv.org/abs/2507.10250",
        "pdf_url": "https://arxiv.org/pdf/2507.10250",
        "title": "DepViT-CAD: Deployable Vision Transformer-Based Cancer Diagnosis in Histopathology",
        "authors": [
            "Ashkan Shakarami",
            "Lorenzo Nicole",
            "Rocco Cappellesso",
            "Angelo Paolo Dei Tos",
            "Stefano Ghidoni"
        ],
        "comments": "25 pages, 15 figures",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Accurate and timely cancer diagnosis from histopathological slides is vital for effective clinical decision-making. This paper introduces DepViT-CAD, a deployable AI system for multi-class cancer diagnosis in histopathology. At its core is MAViT, a novel Multi-Attention Vision Transformer designed to capture fine-grained morphological patterns across diverse tumor types. MAViT was trained on expert-annotated patches from 1008 whole-slide images, covering 11 diagnostic categories, including 10 major cancers and non-tumor tissue. DepViT-CAD was validated on two independent cohorts: 275 WSIs from The Cancer Genome Atlas and 50 routine clinical cases from pathology labs, achieving diagnostic sensitivities of 94.11% and 92%, respectively. By combining state-of-the-art transformer architecture with large-scale real-world validation, DepViT-CAD offers a robust and scalable approach for AI-assisted cancer diagnostics. To support transparency and reproducibility, software and code will be made publicly available at GitHub.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 265,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10267",
        "abs_url": "https://arxiv.org/abs/2507.10267",
        "pdf_url": "https://arxiv.org/pdf/2507.10267",
        "title": "DNS Tunneling: Threat Landscape and Improved Detection Solutions",
        "authors": [
            "Novruz Amirov",
            "Baran Isik",
            "Bilal Ihsan Tuncer",
            "Serif Bahtiyar"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)",
        "abstract": "Detecting Domain Name System (DNS) tunneling is a significant challenge in security due to its capacity to hide harmful actions within DNS traffic that appears to be normal and legitimate. Traditional detection methods are based on rule-based approaches or signature matching methods that are often insufficient to accurately identify such covert communication channels. This research is about effectively detecting DNS tunneling. We propose a novel approach to detect DNS tunneling with machine learning algorithms. We combine machine learning algorithms to analyze the traffic by using features extracted from DNS traffic. Analyses results show that the proposed approach is a good candidate to detect DNS tunneling accurately.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 266,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10303",
        "abs_url": "https://arxiv.org/abs/2507.10303",
        "pdf_url": "https://arxiv.org/pdf/2507.10303",
        "title": "MF-GLaM: A multifidelity stochastic emulator using generalized lambda models",
        "authors": [
            "K. Giannoukou",
            "X. Zhu",
            "S. Marelli",
            "B. Sudret"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Computation (stat.CO); Methodology (stat.ME)",
        "abstract": "Stochastic simulators exhibit intrinsic stochasticity due to unobservable, uncontrollable, or unmodeled input variables, resulting in random outputs even at fixed input conditions. Such simulators are common across various scientific disciplines; however, emulating their entire conditional probability distribution is challenging, as it is a task traditional deterministic surrogate modeling techniques are not designed for. Additionally, accurately characterizing the response distribution can require prohibitively large datasets, especially for computationally expensive high-fidelity (HF) simulators. When lower-fidelity (LF) stochastic simulators are available, they can enhance limited HF information within a multifidelity surrogate modeling (MFSM) framework. While MFSM techniques are well-established for deterministic settings, constructing multifidelity emulators to predict the full conditional response distribution of stochastic simulators remains a challenge. In this paper, we propose multifidelity generalized lambda models (MF-GLaMs) to efficiently emulate the conditional response distribution of HF stochastic simulators by exploiting data from LF stochastic simulators. Our approach builds upon the generalized lambda model (GLaM), which represents the conditional distribution at each input by a flexible, four-parameter generalized lambda distribution. MF-GLaMs are non-intrusive, requiring no access to the internal stochasticity of the simulators nor multiple replications of the same input values. We demonstrate the efficacy of MF-GLaM through synthetic examples of increasing complexity and a realistic earthquake application. Results show that MF-GLaMs can achieve improved accuracy at the same cost as single-fidelity GLaMs, or comparable performance at significantly reduced cost.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 267,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10330",
        "abs_url": "https://arxiv.org/abs/2507.10330",
        "pdf_url": "https://arxiv.org/pdf/2507.10330",
        "title": "Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach",
        "authors": [
            "Mohammed Bouri",
            "Adnane Saoud"
        ],
        "comments": "Accepted to ACL Findings 2025",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Despite advancements in Natural Language Processing (NLP), models remain vulnerable to adversarial attacks, such as synonym substitutions. While prior work has focused on improving robustness for feed-forward and convolutional architectures, the robustness of recurrent networks and modern state space models (SSMs), such as S4, remains understudied. These architectures pose unique challenges due to their sequential processing and complex parameter dynamics. In this paper, we introduce a novel regularization technique based on Growth Bound Matrices (GBM) to improve NLP model robustness by reducing the impact of input perturbations on model outputs. We focus on computing the GBM for three architectures: Long Short-Term Memory (LSTM), State Space models (S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance resilience against word substitution attacks, (2) improve generalization on clean text, and (3) providing the first systematic analysis of SSM (S4) robustness. Extensive experiments across multiple architectures and benchmark datasets demonstrate that our method improves adversarial robustness by up to 8.8% over existing baselines. These results highlight the effectiveness of our approach, outperforming several state-of-the-art methods in adversarial defense. Codes are available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 268,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10375",
        "abs_url": "https://arxiv.org/abs/2507.10375",
        "pdf_url": "https://arxiv.org/pdf/2507.10375",
        "title": "Test-Time Canonicalization by Foundation Models for Robust Perception",
        "authors": [
            "Utkarsh Singhal",
            "Ryan Feng",
            "Stella X. Yu",
            "Atul Prakash"
        ],
        "comments": "Published at ICML 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Real-world visual perception requires invariance to diverse transformations, yet current methods rely heavily on specialized architectures or training on predefined augmentations, limiting generalization. We propose FOCAL, a test-time, data-driven framework that achieves robust perception by leveraging internet-scale visual priors from foundation models. By generating and optimizing candidate transformations toward visually typical, \"canonical\" views, FOCAL enhances robustness without re-training or architectural changes. Our experiments demonstrate improved robustness of CLIP and SAM across challenging transformations, including 2D/3D rotations, illumination shifts (contrast and color), and day-night variations. We also highlight potential applications in active vision. Our approach challenges the assumption that transform-specific training is necessary, instead offering a scalable path to invariance. Our code is available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 269,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10381",
        "abs_url": "https://arxiv.org/abs/2507.10381",
        "pdf_url": "https://arxiv.org/pdf/2507.10381",
        "title": "Improving Remote Sensing Classification using Topological Data Analysis and Convolutional Neural Networks",
        "authors": [
            "Aaryam Sharma"
        ],
        "comments": "9 pages, 8 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Topological data analysis (TDA) is a relatively new field that is gaining rapid adoption due to its robustness and ability to effectively describe complex datasets by quantifying geometric information. In imaging contexts, TDA typically models data as filtered cubical complexes from which we can extract discriminative features using persistence homology. Meanwhile, convolutional neural networks (CNNs) have been shown to be biased towards texture based local features. To address this limitation, we propose a TDA feature engineering pipeline and a simple method to integrate topological features with deep learning models on remote sensing classification. Our method improves the performance of a ResNet18 model on the EuroSAT dataset by 1.44% achieving 99.33% accuracy, which surpasses all previously reported single-model accuracies, including those with larger architectures, such as ResNet50 (2x larger) and XL Vision Transformers (197x larger). We additionally show that our method's accuracy is 1.82% higher than our ResNet18 baseline on the RESISC45 dataset. To our knowledge, this is the first application of TDA features in satellite scene classification with deep learning. This demonstrates that TDA features can be integrated with deep learning models, even on datasets without explicit topological structures, thereby increasing the applicability of TDA. A clean implementation of our method will be made publicly available upon publication.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 270,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10383",
        "abs_url": "https://arxiv.org/abs/2507.10383",
        "pdf_url": "https://arxiv.org/pdf/2507.10383",
        "title": "Dynamical stability for dense patterns in discrete attractor neural networks",
        "authors": [
            "Uri Cohen",
            "Máté Lengyel"
        ],
        "comments": "",
        "subjects": "Disordered Systems and Neural Networks (cond-mat.dis-nn); Statistical Mechanics (cond-mat.stat-mech); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Neurons and Cognition (q-bio.NC)",
        "abstract": "Neural networks storing multiple discrete attractors are canonical models of biological memory. Previously, the dynamical stability of such networks could only be guaranteed under highly restrictive conditions. Here, we derive a theory of the local stability of discrete fixed points in a broad class of networks with graded neural activities and in the presence of noise. By directly analyzing the bulk and outliers of the Jacobian spectrum, we show that all fixed points are stable below a critical load that is distinct from the classical \\textit{critical capacity} and depends on the statistics of neural activities in the fixed points as well as the single-neuron activation function. Our analysis highlights the computational benefits of threshold-linear activation and sparse-like patterns.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 271,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10421",
        "abs_url": "https://arxiv.org/abs/2507.10421",
        "pdf_url": "https://arxiv.org/pdf/2507.10421",
        "title": "SentiDrop: A Multi Modal Machine Learning model for Predicting Dropout in Distance Learning",
        "authors": [
            "Meriem Zerkouk",
            "Miloud Mihoubi",
            "Belkacem Chikhaoui"
        ],
        "comments": "International Conference on Education and New Learning Technologies (2025)",
        "subjects": "Artificial Intelligence (cs.AI); Emerging Technologies (cs.ET); Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "School dropout is a serious problem in distance learning, where early detection is crucial for effective intervention and student perseverance. Predicting student dropout using available educational data is a widely researched topic in learning analytics. Our partner's distance learning platform highlights the importance of integrating diverse data sources, including socio-demographic data, behavioral data, and sentiment analysis, to accurately predict dropout risks. In this paper, we introduce a novel model that combines sentiment analysis of student comments using the Bidirectional Encoder Representations from Transformers (BERT) model with socio-demographic and behavioral data analyzed through Extreme Gradient Boosting (XGBoost). We fine-tuned BERT on student comments to capture nuanced sentiments, which were then merged with key features selected using feature importance techniques in XGBoost. Our model was tested on unseen data from the next academic year, achieving an accuracy of 84\\%, compared to 82\\% for the baseline model. Additionally, the model demonstrated superior performance in other metrics, such as precision and F1-score. The proposed method could be a vital tool in developing personalized strategies to reduce dropout rates and encourage student perseverance",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 272,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10430",
        "abs_url": "https://arxiv.org/abs/2507.10430",
        "pdf_url": "https://arxiv.org/pdf/2507.10430",
        "title": "Efficient Federated Learning with Heterogeneous Data and Adaptive Dropout",
        "authors": [
            "Ji Liu",
            "Beichen Ma",
            "Qiaolin Yu",
            "Ruoming Jin",
            "Jingbo Zhou",
            "Yang Zhou",
            "Huaiyu Dai",
            "Haixun Wang",
            "Dejing Dou",
            "Patrick Valduriez"
        ],
        "comments": "29 pages, to appear in ACM Transactions on Knowledge Discovery from Data (TKDD)",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Federated Learning (FL) is a promising distributed machine learning approach that enables collaborative training of a global model using multiple edge devices. The data distributed among the edge devices is highly heterogeneous. Thus, FL faces the challenge of data distribution and heterogeneity, where non-Independent and Identically Distributed (non-IID) data across edge devices may yield in significant accuracy drop. Furthermore, the limited computation and communication capabilities of edge devices increase the likelihood of stragglers, thus leading to slow model convergence. In this paper, we propose the FedDHAD FL framework, which comes with two novel methods: Dynamic Heterogeneous model aggregation (FedDH) and Adaptive Dropout (FedAD). FedDH dynamically adjusts the weights of each local model within the model aggregation process based on the non-IID degree of heterogeneous data to deal with the statistical data heterogeneity. FedAD performs neuron-adaptive operations in response to heterogeneous devices to improve accuracy while achieving superb efficiency. The combination of these two methods makes FedDHAD significantly outperform state-of-the-art solutions in terms of accuracy (up to 6.7% higher), efficiency (up to 2.02 times faster), and computation cost (up to 15.0% smaller).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 273,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10443",
        "abs_url": "https://arxiv.org/abs/2507.10443",
        "pdf_url": "https://arxiv.org/pdf/2507.10443",
        "title": "Information Must Flow: Recursive Bootstrapping for Information Bottleneck in Optimal Transport",
        "authors": [
            "Xin Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "We present the Context-Content Uncertainty Principle (CCUP), a unified framework that models cognition as the directed flow of information between high-entropy context and low-entropy content. Inference emerges as a cycle of bidirectional interactions, bottom-up contextual disambiguation paired with top-down content reconstruction, which resolves the Information Bottleneck in Optimal Transport (iBOT). Implemented via Rao-Blackwellized variational entropy minimization, CCUP steers representations toward minimal joint uncertainty while preserving inferential directionality. Local cycle completion underpins temporal bootstrapping, chaining simulations to refine memory, and spatial bootstrapping, enabling compositional hierarchical inference. We prove a Delta Convergence Theorem showing that recursive entropy minimization yields delta-like attractors in latent space, stabilizing perceptual schemas and motor plans. Temporal bootstrapping through perception-action loops and sleep-wake consolidation further transforms episodic traces into semantic knowledge. Extending CCUP, each hierarchical level performs delta-seeded inference: low-entropy content seeds diffuse outward along goal-constrained paths shaped by top-down priors and external context, confining inference to task-relevant manifolds and circumventing the curse of dimensionality. Building on this, we propose that language emerges as a symbolic transport system, externalizing latent content to synchronize inference cycles across individuals. Together, these results establish iBOT as a foundational principle of information flow in both individual cognition and collective intelligence, positioning recursive inference as the structured conduit through which minds adapt, align, and extend.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 274,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10447",
        "abs_url": "https://arxiv.org/abs/2507.10447",
        "pdf_url": "https://arxiv.org/pdf/2507.10447",
        "title": "Evaluating Fake Music Detection Performance Under Audio Augmentations",
        "authors": [
            "Tomasz Sroka",
            "Tomasz Wężowicz",
            "Dominik Sidorczuk",
            "Mateusz Modrzejewski"
        ],
        "comments": "ISMIR 2025 LBD, 2 pages + bibliography, 1 figure",
        "subjects": "Sound (cs.SD); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)",
        "abstract": "With the rapid advancement of generative audio models, distinguishing between human-composed and generated music is becoming increasingly challenging. As a response, models for detecting fake music have been proposed. In this work, we explore the robustness of such systems under audio augmentations. To evaluate model generalization, we constructed a dataset consisting of both real and synthetic music generated using several systems. We then apply a range of audio transformations and analyze how they affect classification accuracy. We test the performance of a recent state-of-the-art musical deepfake detection model in the presence of audio augmentations. The performance of the model decreases significantly even with the introduction of light augmentations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 275,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10448",
        "abs_url": "https://arxiv.org/abs/2507.10448",
        "pdf_url": "https://arxiv.org/pdf/2507.10448",
        "title": "FinTeam: A Multi-Agent Collaborative Intelligence System for Comprehensive Financial Scenarios",
        "authors": [
            "Yingqian Wu",
            "Qiushi Wang",
            "Zefei Long",
            "Rong Ye",
            "Zhongtian Lu",
            "Xianyin Zhang",
            "Bingxuan Li",
            "Wei Chen",
            "Liwen Zhang",
            "Zhongyu Wei"
        ],
        "comments": "NLPCC 2025 Oral",
        "subjects": "Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)",
        "abstract": "Financial report generation tasks range from macro- to micro-economics analysis, also requiring extensive data analysis. Existing LLM models are usually fine-tuned on simple QA tasks and cannot comprehensively analyze real financial scenarios. Given the complexity, financial companies often distribute tasks among departments. Inspired by this, we propose FinTeam, a financial multi-agent collaborative system, with a workflow with four LLM agents: document analyzer, analyst, accountant, and consultant. We train these agents with specific financial expertise using constructed datasets. We evaluate FinTeam on comprehensive financial tasks constructed from real online investment forums, including macroeconomic, industry, and company analysis. The human evaluation shows that by combining agents, the financial reports generate from FinTeam achieved a 62.00% acceptance rate, outperforming baseline models like GPT-4o and Xuanyuan. Additionally, FinTeam's agents demonstrate a 7.43% average improvement on FinCUGE and a 2.06% accuracy boost on FinEval. Project is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 276,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10457",
        "abs_url": "https://arxiv.org/abs/2507.10457",
        "pdf_url": "https://arxiv.org/pdf/2507.10457",
        "title": "Logic layer Prompt Control Injection (LPCI): A Novel Security Vulnerability Class in Agentic Systems",
        "authors": [
            "Hammad Atta",
            "Ken Huang",
            "Manish Bhatt",
            "Kamal Ahmed",
            "Muhammad Aziz Ul Haq",
            "Yasir Mehmood"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "The integration of large language models (LLMs) into enterprise systems has created a new class of covert security vulnerabilities, particularly within logic-execution layers and persistent-memory contexts. In this paper, we introduce Logic-Layer Prompt Control Injection (LPCI), a novel attack category in which encoded, delayed, and conditionally triggered payloads are embedded in memory, vector stores, or tool outputs. These payloads can bypass conventional input filters and trigger unauthorised behaviour across sessions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 277,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10461",
        "abs_url": "https://arxiv.org/abs/2507.10461",
        "pdf_url": "https://arxiv.org/pdf/2507.10461",
        "title": "RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for Pansharpening",
        "authors": [
            "Tao Tang",
            "Chengxu Yang"
        ],
        "comments": "To appear in the proceedings of the 6th International Conference on Artificial Intelligence and Electromechanical Automation (AIEA 2025). 5 pages, 6 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multimedia (cs.MM); Image and Video Processing (eess.IV)",
        "abstract": "Pansharpening refers to the process of integrating a high resolution panchromatic (PAN) image with a lower resolution multispectral (MS) image to generate a fused product, which is pivotal in remote sensing. Despite the effectiveness of CNNs in addressing this challenge, they are inherently constrained by the uniform application of convolutional kernels across all spatial positions, overlooking local content variations. To overcome this issue, we introduce RAPNet, a new architecture that leverages content-adaptive convolution. At its core, RAPNet employs the Receptive-field Adaptive Pansharpening Convolution (RAPConv), designed to produce spatially adaptive kernels responsive to local feature context, thereby enhancing the precision of spatial detail extraction. Additionally, the network integrates the Pansharpening Dynamic Feature Fusion (PAN-DFF) module, which incorporates an attention mechanism to achieve an optimal balance between spatial detail enhancement and spectral fidelity. Comprehensive evaluations on publicly available datasets confirm that RAPNet delivers superior performance compared to existing approaches, as demonstrated by both quantitative metrics and qualitative assessments. Ablation analyses further substantiate the effectiveness of the proposed adaptive components.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 278,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10468",
        "abs_url": "https://arxiv.org/abs/2507.10468",
        "pdf_url": "https://arxiv.org/pdf/2507.10468",
        "title": "From BERT to Qwen: Hate Detection across architectures",
        "authors": [
            "Ariadna Mon",
            "Saúl Fenollosa",
            "Jon Lecumberri"
        ],
        "comments": "4 pages, 5 figures. EE-559 Deep Learning course project (Group 11)",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Online platforms struggle to curb hate speech without over-censoring legitimate discourse. Early bidirectional transformer encoders made big strides, but the arrival of ultra-large autoregressive LLMs promises deeper context-awareness. Whether this extra scale actually improves practical hate-speech detection on real-world text remains unverified. Our study puts this question to the test by benchmarking both model families, classic encoders and next-generation LLMs, on curated corpora of online interactions for hate-speech detection (Hate or No Hate).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 279,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10492",
        "abs_url": "https://arxiv.org/abs/2507.10492",
        "pdf_url": "https://arxiv.org/pdf/2507.10492",
        "title": "BenchReAD: A systematic benchmark for retinal anomaly detection",
        "authors": [
            "Chenyu Lian",
            "Hong-Yu Zhou",
            "Zhanli Hu",
            "Jing Qin"
        ],
        "comments": "MICCAI 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Retinal anomaly detection plays a pivotal role in screening ocular and systemic diseases. Despite its significance, progress in the field has been hindered by the absence of a comprehensive and publicly available benchmark, which is essential for the fair evaluation and advancement of methodologies. Due to this limitation, previous anomaly detection work related to retinal images has been constrained by (1) a limited and overly simplistic set of anomaly types, (2) test sets that are nearly saturated, and (3) a lack of generalization evaluation, resulting in less convincing experimental setups. Furthermore, existing benchmarks in medical anomaly detection predominantly focus on one-class supervised approaches (training only with negative samples), overlooking the vast amounts of labeled abnormal data and unlabeled data that are commonly available in clinical practice. To bridge these gaps, we introduce a benchmark for retinal anomaly detection, which is comprehensive and systematic in terms of data and algorithm. Through categorizing and benchmarking previous methods, we find that a fully supervised approach leveraging disentangled representations of abnormalities (DRA) achieves the best performance but suffers from significant drops in performance when encountering certain unseen anomalies. Inspired by the memory bank mechanisms in one-class supervised learning, we propose NFM-DRA, which integrates DRA with a Normal Feature Memory to mitigate the performance degradation, establishing a new SOTA. The benchmark is publicly available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 280,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10499",
        "abs_url": "https://arxiv.org/abs/2507.10499",
        "pdf_url": "https://arxiv.org/pdf/2507.10499",
        "title": "National level satellite-based crop field inventories in smallholder landscapes",
        "authors": [
            "Philippe Rufin",
            "Pauline Lucie Hammer",
            "Leon-Friedrich Thomas",
            "Sá Nogueira Lisboa",
            "Natasha Ribeiro",
            "Almeida Sitoe",
            "Patrick Hostert",
            "Patrick Meyfroidt"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "The design of science-based policies to improve the sustainability of smallholder agriculture is challenged by a limited understanding of fundamental system properties, such as the spatial distribution of active cropland and field size. We integrate very high spatial resolution (1.5 m) Earth observation data and deep transfer learning to derive crop field delineations in complex agricultural systems at the national scale, while maintaining minimum reference data requirements and enhancing transferability. We provide the first national-level dataset of 21 million individual fields for Mozambique (covering ~800,000 km2) for 2023. Our maps separate active cropland from non-agricultural land use with an overall accuracy of 93% and balanced omission and commission errors. Field-level spatial agreement reached median intersection over union (IoU) scores of 0.81, advancing the state-of-the-art in large-area field delineation in complex smallholder systems. The active cropland maps capture fragmented rural regions with low cropland shares not yet identified in global land cover or cropland maps. These regions are mostly located in agricultural frontier regions which host 7-9% of the Mozambican population. Field size in Mozambique is very low overall, with half of the fields being smaller than 0.16 ha, and 83% smaller than 0.5 ha. Mean field size at aggregate spatial resolution (0.05°) is 0.32 ha, but it varies strongly across gradients of accessibility, population density, and net forest cover change. This variation reflects a diverse set of actors, ranging from semi-subsistence smallholder farms to medium-scale commercial farming, and large-scale farming operations. Our results highlight that field size is a key indicator relating to socio-economic and environmental outcomes of agriculture (e.g., food production, livelihoods, deforestation, biodiversity), as well as their trade-offs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 281,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10524",
        "abs_url": "https://arxiv.org/abs/2507.10524",
        "pdf_url": "https://arxiv.org/pdf/2507.10524",
        "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation",
        "authors": [
            "Sangmin Bae",
            "Yujin Kim",
            "Reza Bayat",
            "Sungnyun Kim",
            "Jiyoun Ha",
            "Tal Schuster",
            "Adam Fisch",
            "Hrayr Harutyunyan",
            "Ziwei Ji",
            "Aaron Courville",
            "Se-Young Yun"
        ],
        "comments": "36 pages, 9 figures, 14 tables, codes at this https URL",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Scaling language models unlocks impressive capabilities, but the accompanying computational and memory demands make both training and deployment expensive. Existing efficiency efforts typically target either parameter sharing or adaptive computation, leaving open the question of how to attain both simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework that combines the two axes of efficiency inside a single Recursive Transformer. MoR reuses a shared stack of layers across recursion steps to achieve parameter efficiency, while lightweight routers enable adaptive token-level thinking by dynamically assigning different recursion depths to individual tokens. This allows MoR to focus quadratic attention computation only among tokens still active at a given recursion depth, further improving memory access efficiency by selectively caching only their key-value pairs. Beyond these core mechanisms, we also propose a KV sharing variant that reuses KV pairs from the first recursion, specifically designed to decrease prefill latency and memory footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms a new Pareto frontier: at equal training FLOPs and smaller model sizes, it significantly lowers validation perplexity and improves few-shot accuracy, while delivering higher throughput compared with vanilla and existing recursive baselines. These gains demonstrate that MoR is an effective path towards large-model quality without incurring large-model cost.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 282,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10547",
        "abs_url": "https://arxiv.org/abs/2507.10547",
        "pdf_url": "https://arxiv.org/pdf/2507.10547",
        "title": "Quantize-then-Rectify: Efficient VQ-VAE Training",
        "authors": [
            "Borui Zhang",
            "Qihang Rao",
            "Wenzhao Zheng",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Visual tokenizers are pivotal in multimodal large models, acting as bridges between continuous inputs and discrete tokens. Nevertheless, training high-compression-rate VQ-VAEs remains computationally demanding, often necessitating thousands of GPU hours. This work demonstrates that a pre-trained VAE can be efficiently transformed into a VQ-VAE by controlling quantization noise within the VAE's tolerance threshold. We present \\textbf{Quantize-then-Rectify (ReVQ)}, a framework leveraging pre-trained VAEs to enable rapid VQ-VAE training with minimal computational overhead. By integrating \\textbf{channel multi-group quantization} to enlarge codebook capacity and a \\textbf{post rectifier} to mitigate quantization errors, ReVQ compresses ImageNet images into at most 512 tokens while sustaining competitive reconstruction quality (rFID = 1.06). Significantly, ReVQ reduces training costs by over two orders of magnitude relative to state-of-the-art approaches: ReVQ finishes full training on a single NVIDIA 4090 in approximately 22 hours, whereas comparable methods require 4.5 days on 32 A100 GPUs. Experimental results show that ReVQ achieves superior efficiency-reconstruction trade-offs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 283,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-15?abs=True",
        "arxiv_id": "2507.10552",
        "abs_url": "https://arxiv.org/abs/2507.10552",
        "pdf_url": "https://arxiv.org/pdf/2507.10552",
        "title": "Self-supervised Learning on Camera Trap Footage Yields a Strong Universal Face Embedder",
        "authors": [
            "Vladimir Iashin",
            "Horace Lee",
            "Dan Schofield",
            "Andrew Zisserman"
        ],
        "comments": "Accepted for publication. Project page, code and weights: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Camera traps are revolutionising wildlife monitoring by capturing vast amounts of visual data; however, the manual identification of individual animals remains a significant bottleneck. This study introduces a fully self-supervised approach to learning robust chimpanzee face embeddings from unlabeled camera-trap footage. Leveraging the DINOv2 framework, we train Vision Transformers on automatically mined face crops, eliminating the need for identity labels. Our method demonstrates strong open-set re-identification performance, surpassing supervised baselines on challenging benchmarks such as Bossou, despite utilising no labelled data during training. This work underscores the potential of self-supervised learning in biodiversity monitoring and paves the way for scalable, non-invasive population studies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    }
]