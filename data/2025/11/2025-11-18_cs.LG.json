[
    {
        "order": 1,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11573",
        "abs_url": "https://arxiv.org/abs/2511.11573",
        "pdf_url": "https://arxiv.org/pdf/2511.11573",
        "title": "Softmax as a Lagrangian-Legendrian Seam",
        "authors": [
            "Christopher R. Lee-Jenkins"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "This note offers a first bridge from machine learning to modern differential geometry. We show that the logits-to-probabilities step implemented by softmax can be modeled as a geometric interface: two potential-generated, conservative descriptions (from negative entropy and log-sum-exp) meet along a Legendrian \"seam\" on a contact screen (the probability simplex) inside a simple folded symplectic collar. Bias-shift invariance appears as Reeb flow on the screen, and the Fenchel-Young equality/KL gap provides a computable distance to the seam. We work out the two- and three-class cases to make the picture concrete and outline next steps for ML: compact logit models (projective or spherical), global invariants, and connections to information geometry where on-screen dynamics manifest as replicator flows.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 2,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11574",
        "abs_url": "https://arxiv.org/abs/2511.11574",
        "pdf_url": "https://arxiv.org/pdf/2511.11574",
        "title": "LLM on a Budget: Active Knowledge Distillation for Efficient Classification of Large Text Corpora",
        "authors": [
            "Viviana Luccioli",
            "Rithika Iyengar",
            "Ryan Panley",
            "Flora Haberkorn",
            "Xiaoyu Ge",
            "Leland Crane",
            "Nitish Sinha",
            "Seung Jung Lee"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Large Language Models (LLMs) are highly accurate in classification tasks, however, substantial computational and financial costs hinder their large-scale deployment in dynamic environments. Knowledge Distillation (KD) where a LLM \"teacher\" trains a smaller and more efficient \"student\" model, offers a promising solution to this problem. However, the distillation process itself often remains costly for large datasets, since it requires the teacher to label a vast number of samples while incurring significant token consumption. To alleviate this challenge, in this work we explore the active learning (AL) as a way to create efficient student models at a fraction of the cost while preserving the LLM's performance. In particular, we introduce M-RARU (Multi-class Randomized Accept/Reject Uncertainty Sampling), a novel AL algorithm that significantly reduces training costs. M-RARU employs an innovative strategy combining uncertainty with a randomized accept-reject mechanism to select only the most informative data points for the LLM teacher. This focused approach significantly minimizes required API calls and data processing time. We evaluate M-RARU against random sampling across five diverse student models (SVM, LDA, RF, GBDT, and DistilBERT) on multiple benchmark datasets. Experiments demonstrate that our proposed method achieves up to 80% reduction in sample requirements as compared to random sampling, substantially improving classification accuracy while reducing financial costs and overall training time.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 3,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11575",
        "abs_url": "https://arxiv.org/abs/2511.11575",
        "pdf_url": "https://arxiv.org/pdf/2511.11575",
        "title": "Detecting Statistically Significant Fairness Violations in Recidivism Forecasting Algorithms",
        "authors": [
            "Animesh Joshi"
        ],
        "comments": "8 pages",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Machine learning algorithms are increasingly deployed in critical domains such as finance, healthcare, and criminal justice [1]. The increasing popularity of algorithmic decision-making has stimulated interest in algorithmic fairness within the academic community. Researchers have introduced various fairness definitions that quantify disparities between privileged and protected groups, use causal inference to determine the impact of race on model predictions, and that test calibration of probability predictions from the model. Existing literature does not provide a way in which to assess whether observed disparities between groups are statistically significant or merely due to chance. This paper introduces a rigorous framework for testing the statistical significance of fairness violations by leveraging k-fold cross-validation [2] to generate sampling distributions of fairness metrics. This paper introduces statistical tests that can be used to identify statistically significant violations of fairness metrics based on disparities between predicted and actual outcomes, model calibration, and causal inference techniques [1]. We demonstrate this approach by testing recidivism forecasting algorithms trained on data from the National Institute of Justice. Our findings reveal that machine learning algorithms used for recidivism forecasting exhibit statistically significant bias against Black individuals under several fairness definitions, while also exhibiting no bias or bias against White individuals under other definitions. The results from this paper underscore the importance of rigorous and robust statistical testing while evaluating algorithmic decision-making systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 4,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11579",
        "abs_url": "https://arxiv.org/abs/2511.11579",
        "pdf_url": "https://arxiv.org/pdf/2511.11579",
        "title": "Decoupling Positional and Symbolic Attention Behavior in Transformers",
        "authors": [
            "Felipe Urrutia",
            "Jorge Salas",
            "Alexander Kozachinskiy",
            "Cristian Buc Calderon",
            "Hector Pasten",
            "Cristobal Rojas"
        ],
        "comments": "32 pages, 12 figures, repository available",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "An important aspect subtending language understanding and production is the ability to independently encode positional and symbolic information of the words within a sentence. In Transformers, positional information is typically encoded using Positional Encodings (PEs). One such popular PE, namely Rotary PE (RoPE), has been widely used due to its empirical success. Recently, it has been argued that part of RoPE's success emerges from its ability to encode robust positional and semantic information using large and small frequencies, respectively. In this work, we perform a deeper dive into the positional versus symbolic dichotomy of attention heads behavior, both at the theoretical and empirical level. We provide general definitions of what it means for a head to behave positionally or symbolically, prove that these are two mutually exclusive behaviors and develop a metric to quantify them. We apply our framework to analyze Transformer-based LLMs using RoPE and find that all heads exhibit a strong correspondence between behavior and frequency use. Finally, we introduce canonical tasks designed to be either purely positional or symbolic, and demonstrate that the Transformer performance causally relates to the ability of attention heads to leverage the appropriate frequencies. In particular, we show that we can control the Transformer performance by controlling which frequencies the attention heads can access. Altogether, our work provides a detailed understanding of RoPE, and how its properties relate to model behavior.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 5,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11583",
        "abs_url": "https://arxiv.org/abs/2511.11583",
        "pdf_url": "https://arxiv.org/pdf/2511.11583",
        "title": "Parallel and Multi-Stage Knowledge Graph Retrieval for Behaviorally Aligned Financial Asset Recommendations",
        "authors": [
            "Fernando Spadea",
            "Oshani Seneviratne"
        ],
        "comments": "10 pages, 3 figures, RAGE-KG 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)",
        "abstract": "Large language models (LLMs) show promise for personalized financial recommendations but are hampered by context limits, hallucinations, and a lack of behavioral grounding. Our prior work, FLARKO, embedded structured knowledge graphs (KGs) in LLM prompts to align advice with user behavior and market data. This paper introduces RAG-FLARKO, a retrieval-augmented extension to FLARKO, that overcomes scalability and relevance challenges using multi-stage and parallel KG retrieval processes. Our method first retrieves behaviorally relevant entities from a user's transaction KG and then uses this context to filter temporally consistent signals from a market KG, constructing a compact, grounded subgraph for the LLM. This pipeline reduces context overhead and sharpens the model's focus on relevant information. Empirical evaluation on a real-world financial transaction dataset demonstrates that RAG-FLARKO significantly enhances recommendation quality. Notably, our framework enables smaller, more efficient models to achieve high performance in both profitability and behavioral alignment, presenting a viable path for deploying grounded financial AI in resource-constrained environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 6,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11585",
        "abs_url": "https://arxiv.org/abs/2511.11585",
        "pdf_url": "https://arxiv.org/pdf/2511.11585",
        "title": "Parameter-Efficient and Personalized Federated Training of Generative Models at the Edge",
        "authors": [
            "Kabir Khan",
            "Manju Sarkar",
            "Anita Kar",
            "Suresh Ghosh"
        ],
        "comments": "37 pages, 8 figures",
        "subjects": "Machine Learning (cs.LG); Distributed, Parallel, and Cluster Computing (cs.DC)",
        "abstract": "Large generative models (for example, language and diffusion models) enable high-quality text and image synthesis but are hard to train or adapt in cross-device federated settings due to heavy computation and communication and statistical/system heterogeneity. We propose FedGen-Edge, a framework that decouples a frozen, pre-trained global backbone from lightweight client-side adapters and federates only the adapters. Using Low-Rank Adaptation (LoRA) constrains client updates to a compact subspace, which reduces uplink traffic by more than 99 percent versus full-model FedAvg, stabilizes aggregation under non-IID data, and naturally supports personalization because each client can keep a locally tuned adapter. On language modeling (PTB) and image generation (CIFAR-10), FedGen-Edge achieves lower perplexity/FID and faster convergence than strong baselines while retaining a simple FedAvg-style server. A brief ablation shows diminishing returns beyond moderate LoRA rank and a trade-off between local epochs and client drift. FedGen-Edge offers a practical path toward privacy-preserving, resource-aware, and personalized generative AI on heterogeneous edge devices.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 7,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11589",
        "abs_url": "https://arxiv.org/abs/2511.11589",
        "pdf_url": "https://arxiv.org/pdf/2511.11589",
        "title": "WildfireGenome: Interpretable Machine Learning Reveals Local Drivers of Wildfire Risk and Their Cross-County Variation",
        "authors": [
            "Chenyue Liu",
            "Ali Mostafavi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Current wildfire risk assessments rely on coarse hazard maps and opaque machine learning models that optimize regional accuracy while sacrificing interpretability at the decision scale. WildfireGenome addresses these gaps through three components: (1) fusion of seven federal wildfire indicators into a sign-aligned, PCA-based composite risk label at H3 Level-8 resolution; (2) Random Forest classification of local wildfire risk; and (3) SHAP and ICE/PDP analyses to expose county-specific nonlinear driver relationships. Across seven ecologically diverse U.S. counties, models achieve accuracies of 0.755-0.878 and Quadratic Weighted Kappa up to 0.951, with principal components explaining 87-94% of indicator variance. Transfer tests show reliable performance between ecologically similar regions but collapse across dissimilar contexts. Explanations consistently highlight needleleaf forest cover and elevation as dominant drivers, with risk rising sharply at 30-40% needleleaf coverage. WildfireGenome advances wildfire risk assessment from regional prediction to interpretable, decision-scale analytics that guide vegetation management, zoning, and infrastructure planning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 8,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11593",
        "abs_url": "https://arxiv.org/abs/2511.11593",
        "pdf_url": "https://arxiv.org/pdf/2511.11593",
        "title": "Sound Logical Explanations for Mean Aggregation Graph Neural Networks",
        "authors": [
            "Matthew Morris",
            "Ian Horrocks"
        ],
        "comments": "Full version (with appendices) of paper accepted to NeurIPS 2025 (The Thirty-Ninth Annual Conference on Neural Information Processing Systems)",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Logic in Computer Science (cs.LO)",
        "abstract": "Graph neural networks (GNNs) are frequently used for knowledge graph completion. Their black-box nature has motivated work that uses sound logical rules to explain predictions and characterise their expressivity. However, despite the prevalence of GNNs that use mean as an aggregation function, explainability and expressivity results are lacking for them. We consider GNNs with mean aggregation and non-negative weights (MAGNNs), proving the precise class of monotonic rules that can be sound for them, as well as providing a restricted fragment of first-order logic to explain any MAGNN prediction. Our experiments show that restricting mean-aggregation GNNs to have non-negative weights yields comparable or improved performance on standard inductive benchmarks, that sound rules are obtained in practice, that insightful explanations can be generated in practice, and that the sound rules can expose issues in the trained models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 9,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11602",
        "abs_url": "https://arxiv.org/abs/2511.11602",
        "pdf_url": "https://arxiv.org/pdf/2511.11602",
        "title": "Aspiration-based Perturbed Learning Automata in Games with Noisy Utility Measurements. Part A: Stochastic Stability in Non-zero-Sum Games",
        "authors": [
            "Georgios C. Chasparis"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Science and Game Theory (cs.GT); Multiagent Systems (cs.MA); Optimization and Control (math.OC)",
        "abstract": "Reinforcement-based learning has attracted considerable attention both in modeling human behavior as well as in engineering, for designing measurement- or payoff-based optimization schemes. Such learning schemes exhibit several advantages, especially in relation to filtering out noisy observations. However, they may exhibit several limitations when applied in a distributed setup. In multi-player weakly-acyclic games, and when each player applies an independent copy of the learning dynamics, convergence to (usually desirable) pure Nash equilibria cannot be guaranteed. Prior work has only focused on a small class of games, namely potential and coordination games. To address this main limitation, this paper introduces a novel payoff-based learning scheme for distributed optimization, namely aspiration-based perturbed learning automata (APLA). In this class of dynamics, and contrary to standard reinforcement-based learning schemes, each player's probability distribution for selecting actions is reinforced both by repeated selection and an aspiration factor that captures the player's satisfaction level. We provide a stochastic stability analysis of APLA in multi-player positive-utility games under the presence of noisy observations. This is the first part of the paper that characterizes stochastic stability in generic non-zero-sum games by establishing equivalence of the induced infinite-dimensional Markov chain with a finite dimensional one. In the second part, stochastic stability is further specialized to weakly acyclic games.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 10,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11604",
        "abs_url": "https://arxiv.org/abs/2511.11604",
        "pdf_url": "https://arxiv.org/pdf/2511.11604",
        "title": "Enhancing failure prediction in nuclear industry: Hybridization of knowledge- and data-driven techniques",
        "authors": [
            "Amaratou Mahamadou Saley",
            "Thierry Moyaux",
            "Aïcha Sekhari",
            "Vincent Cheutet",
            "Jean-Baptiste Danielou"
        ],
        "comments": "19 pages, 9 figures, 6 journal, Journal Q1 (Computers and Industrial Engineering)",
        "subjects": "Machine Learning (cs.LG); Computers and Society (cs.CY)",
        "abstract": "The convergence of the Internet of Things (IoT) and Industry 4.0 has significantly enhanced data-driven methodologies within the nuclear industry, notably enhancing safety and economic efficiency. This advancement challenges the precise prediction of future maintenance needs for assets, which is crucial for reducing downtime and operational costs. However, the effectiveness of data-driven methodologies in the nuclear sector requires extensive domain knowledge due to the complexity of the systems involved. Thus, this paper proposes a novel predictive maintenance methodology that combines data-driven techniques with domain knowledge from a nuclear equipment. The methodological originality of this paper is located on two levels: highlighting the limitations of purely data-driven approaches and demonstrating the importance of knowledge in enhancing the performance of the predictive models. The applicative novelty of this work lies in its use within a domain such as a nuclear industry, which is highly restricted and ultrasensitive due to security, economic and environmental concerns. A detailed real-world case study which compares the current state of equipment monitoring with two scenarios, demonstrate that the methodology significantly outperforms purely data-driven methods in failure prediction. While purely data-driven methods achieve only a modest performance with a prediction horizon limited to 3 h and a F1 score of 56.36%, the hybrid approach increases the prediction horizon to 24 h and achieves a higher F1 score of 93.12%.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 11,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11622",
        "abs_url": "https://arxiv.org/abs/2511.11622",
        "pdf_url": "https://arxiv.org/pdf/2511.11622",
        "title": "Small Vocabularies, Big Gains: Pretraining and Tokenization in Time Series Models",
        "authors": [
            "Alexis Roger",
            "Gwen Legate",
            "Kashif Rasul",
            "Yuriy Nevmyvaka",
            "Irina Rish"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Tokenization and transfer learning are two critical components in building state of the art time series foundation models for forecasting. In this work, we systematically study the effect of tokenizer design, specifically scaling and quantization strategies, on model performance, alongside the impact of pretraining versus random initialization. We show that tokenizer configuration primarily governs the representational capacity and stability of the model, while transfer learning influences optimization efficiency and alignment. Using a combination of empirical training experiments and theoretical analyses, we demonstrate that pretrained models consistently leverage well-designed tokenizers more effectively, particularly at smaller vocabulary sizes. Conversely, misaligned tokenization can diminish or even invert the benefits of pretraining. These findings highlight the importance of careful tokenization in time series modeling and suggest that combining small, efficient vocabularies with pretrained weights is especially advantageous in multi-modal forecasting settings, where the overall vocabulary must be shared across modalities. Our results provide concrete guidance for designing tokenizers and leveraging transfer learning in discrete representation learning for continuous signals.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 12,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11625",
        "abs_url": "https://arxiv.org/abs/2511.11625",
        "pdf_url": "https://arxiv.org/pdf/2511.11625",
        "title": "MedFedPure: A Medical Federated Framework with MAE-based Detection and Diffusion Purification for Inference-Time Attacks",
        "authors": [
            "Mohammad Karami",
            "Mohammad Reza Nemati",
            "Aidin Kazemi",
            "Ali Mikaeili Barzili",
            "Hamid Azadegan",
            "Behzad Moshiri"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)",
        "abstract": "Artificial intelligence (AI) has shown great potential in medical imaging, particularly for brain tumor detection using Magnetic Resonance Imaging (MRI). However, the models remain vulnerable at inference time when they are trained collaboratively through Federated Learning (FL), an approach adopted to protect patient privacy. Adversarial attacks can subtly alter medical scans in ways invisible to the human eye yet powerful enough to mislead AI models, potentially causing serious misdiagnoses. Existing defenses often assume centralized data and struggle to cope with the decentralized and diverse nature of federated medical settings. In this work, we present MedFedPure, a personalized federated learning defense framework designed to protect diagnostic AI models at inference time without compromising privacy or accuracy. MedFedPure combines three key elements: (1) a personalized FL model that adapts to the unique data distribution of each institution; (2) a Masked Autoencoder (MAE) that detects suspicious inputs by exposing hidden perturbations; and (3) an adaptive diffusion-based purification module that selectively cleans only the flagged scans before classification. Together, these steps offer robust protection while preserving the integrity of normal, benign images. We evaluated MedFedPure on the Br35H brain MRI dataset. The results show a significant gain in adversarial robustness, improving performance from 49.50% to 87.33% under strong attacks, while maintaining a high clean accuracy of 97.67%. By operating locally and in real time during diagnosis, our framework provides a practical path to deploying secure, trustworthy, and privacy-preserving AI tools in clinical workflows. Index Terms: cancer, tumor detection, federated learning, masked autoencoder, diffusion, privacy",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 13,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11629",
        "abs_url": "https://arxiv.org/abs/2511.11629",
        "pdf_url": "https://arxiv.org/pdf/2511.11629",
        "title": "Global Feature Enhancing and Fusion Framework for Strain Gauge Time Series Classification",
        "authors": [
            "Xu Zhang",
            "Peng Wang",
            "Chen Wang",
            "Zhe Xu",
            "Xiaohua Nie",
            "Wei Wang"
        ],
        "comments": "Global Feature Enhancing and Fusion Framework for Time Series Classification",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Strain Gauge Status (SGS) recognition is crucial in the field of intelligent manufacturing based on the Internet of Things, as accurate identification helps timely detection of failed mechanical components, avoiding accidents. The loading and unloading sequences generated by strain gauges can be identified through time series classification (TSC) algorithms. Recently, deep learning models, e.g., convolutional neural networks (CNNs) have shown remarkable success in the TSC task, as they can extract discriminative local features from the subsequences to identify the time series. However, we observe that only the local features may not be sufficient for expressing the time series, especially when the local sub-sequences between different time series are very similar, e.g., SGS data of aircraft wings in static strength experiments. Nevertheless, CNNs suffer from the limitation in extracting global features due to the nature of convolution operations. For extracting global features to more comprehensively represent the SGS time series, we propose two insights: (i) Constructing global features through feature engineering. (ii) Learning high-order relationships between local features to capture global features. To realize and utilize them, we propose a hypergraph-based global feature learning and fusion framework, which learns and fuses global features for semantic consistency to enhance the representation of SGS time series, thereby improving recognition accuracy. Our method designs are validated on industrial SGS and public UCR datasets, showing better generalization for unseen data in SGS recognition.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 14,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11632",
        "abs_url": "https://arxiv.org/abs/2511.11632",
        "pdf_url": "https://arxiv.org/pdf/2511.11632",
        "title": "Toward Better Generalization in Few-Shot Learning through the Meta-Component Combination",
        "authors": [
            "Qiuhao Zeng"
        ],
        "comments": "20 pages, 5 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "In few-shot learning, classifiers are expected to generalize to unseen classes given only a small number of instances of each new class. One of the popular solutions to few-shot learning is metric-based meta-learning. However, it highly depends on the deep metric learned on seen classes, which may overfit to seen classes and fail to generalize well on unseen classes. To improve the generalization, we explore the substructures of classifiers and propose a novel meta-learning algorithm to learn each classifier as a combination of meta-components. Meta-components are learned across meta-learning episodes on seen classes and disentangled by imposing an orthogonal regularizer to promote its diversity and capture various shared substructures among different classifiers. Extensive experiments on few-shot benchmark tasks show superior performances of the proposed method.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 15,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11636",
        "abs_url": "https://arxiv.org/abs/2511.11636",
        "pdf_url": "https://arxiv.org/pdf/2511.11636",
        "title": "An Explainable and Fair AI Tool for PCOS Risk Assessment: Calibration, Subgroup Equity, and Interactive Clinical Deployment",
        "authors": [
            "Asma Sadia Khan",
            "Sadia Tabassum"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computers and Society (cs.CY); Human-Computer Interaction (cs.HC)",
        "abstract": "This paper presents a fairness-audited and interpretable machine learning framework for predicting polycystic ovary syndrome (PCOS), designed to evaluate model performance and identify diagnostic disparities across patient subgroups. The framework integrated SHAP-based feature attributions with demographic audits to connect predictive explanations with observed disparities for actionable insights. Probabilistic calibration metrics (Brier Score and Expected Calibration Error) are incorporated to ensure reliable risk predictions across subgroups. Random Forest, SVM, and XGBoost models were trained with isotonic and Platt scaling for calibration and fairness comparison. A calibrated Random Forest achieved a high predictive accuracy of 90.8%. SHAP analysis identified follicle count, weight gain, and menstrual irregularity as the most influential features, which are consistent with the Rotterdam diagnostic criteria. Although the SVM with isotonic calibration achieved the lowest calibration error (ECE = 0.0541), the Random Forest model provided a better balance between calibration and interpretability (Brier = 0.0678, ECE = 0.0666). Therefore, it was selected for detailed fairness and SHAP analyses. Subgroup analysis revealed that the model performed best among women aged 25-35 (accuracy 90.9%) but underperformed in those under 25 (69.2%), highlighting age-related disparities. The model achieved perfect precision in obese women and maintained high recall in lean PCOS cases, demonstrating robustness across phenotypes. Finally, a Streamlit-based web interface enables real-time PCOS risk assessment, Rotterdam criteria evaluation, and interactive 'what-if' analysis, bridging the gap between AI research and clinical usability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 16,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11638",
        "abs_url": "https://arxiv.org/abs/2511.11638",
        "pdf_url": "https://arxiv.org/pdf/2511.11638",
        "title": "Enhancing PINN Accuracy for the RLW Equation: Adaptive and Conservative Approaches",
        "authors": [
            "Aamir Shehzad"
        ],
        "comments": "32 pages, 19 figures This work investigates adaptive and conservative PINN frameworks for solving the RLW equation",
        "subjects": "Machine Learning (cs.LG); Numerical Analysis (math.NA); Pattern Formation and Solitons (nlin.PS)",
        "abstract": "Standard physics-informed neural network implementations have produced large error rates when using these models to solve the regularized long wave (RLW) equation. Two improved PINN approaches were developed in this research: an adaptive approach with self-adaptive loss weighting and a conservative approach enforcing explicit conservation laws. Three benchmark tests were used to demonstrate how effective PINN's are as they relate to the type of problem being solved (i.e., time dependent RLW equation). The first was a single soliton traveling along a line (propagation), the second was the interaction between two solitons, and the third was the evolution of an undular bore over the course of $t=250$. The results demonstrated that the effectiveness of PINNs are problem specific. The adaptive PINN was significantly better than both the conservative PINN and the standard PINN at solving problems involving complex nonlinear interactions such as colliding two solitons. The conservative approach was significantly better at solving problems involving long term behavior of single solitons and undular bores. However, the most important finding from this research is that explicitly enforcing conservation laws may be harmful to optimizing the solution of highly nonlinear systems of equations and therefore requires special training methods. The results from our adaptive and conservative approaches were within $O(10^{-5})$ of established numerical solutions for the same problem, thus demonstrating that PINNs can provide accurate solutions to complex systems of partial differential equations without the need for a discretization of space or time (mesh free). Moreover, the finding from this research challenges the assumptions that conservation enforcement will always improve the performance of a PINN and provides researchers with guidelines for designing PINNs for use on specific types of problems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 17,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11646",
        "abs_url": "https://arxiv.org/abs/2511.11646",
        "pdf_url": "https://arxiv.org/pdf/2511.11646",
        "title": "A Deep Learning Model to Predicting Changes in Consumer Attributes for New Line-extended Products",
        "authors": [
            "Li Yinxing",
            "Tsukasa Ishigaki"
        ],
        "comments": "23 pages",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)",
        "abstract": "Product line extension is a marketing strategy that enhances a company's sphere of influence. Because excessive line extensions disrupt brand image, only appropriate line extensions based on consumer needs are desirable. Marketers should know the key consumer attributes of the primary customers for new line-extended products before companies enter the market. This paper describes a method for predicting changes in consumer attributes for new line-extended products using a novel deep learning model. The proposed model, Conditional Tabular Variational Auto-Encoder (CTVAE), generates synthetic data from large-scale tabular data of consumers and products. It can provide various implications about effective product line marketing for marketers. The experimental results demonstrate that the CTVAE offers superior prediction performance than existing models. We indicate implications for new products that change containers or flavors for effective product line marketing. The proposed approach has the potential to contribute to avoiding cannibalization and to designing product images and marketing strategies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 18,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11648",
        "abs_url": "https://arxiv.org/abs/2511.11648",
        "pdf_url": "https://arxiv.org/pdf/2511.11648",
        "title": "Lightweight Time Series Data Valuation on Time Series Foundation Models via In-Context Finetuning",
        "authors": [
            "Shunyu Wu",
            "Tianyue Li",
            "Yixuan Leng",
            "Jingyi Suo",
            "Jian Lou",
            "Dan Li",
            "See-Kiong Ng"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Time series foundation models (TSFMs) have demonstrated increasing capabilities due to their extensive pretraining on large volumes of diverse time series data. Consequently, the quality of time series data is crucial to TSFM performance, rendering an accurate and efficient data valuation of time series for TSFMs indispensable. However, traditional data valuation methods, such as influence functions, face severe computational bottlenecks due to their poor scalability with growing TSFM model sizes and often fail to preserve temporal dependencies. In this paper, we propose LTSV, a Lightweight Time Series Valuation on TSFMS via in-context finetuning. Grounded in the theoretical evidence that in-context finetuning approximates the influence function, LTSV estimates a sample's contribution by measuring the change in context loss after in-context finetuning, leveraging the strong generalization capabilities of TSFMs to produce robust and transferable data valuations. To capture temporal dependencies, we introduce temporal block aggregation, which integrates per-block influence scores across overlapping time windows. Experiments across multiple time series datasets and models demonstrate that LTSV consistently provides reliable and strong valuation performance, while maintaining manageable computational requirements. Our results suggest that in-context finetuning on time series foundation models provides a practical and effective bridge between data attribution and model generalization in time series learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 19,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11651",
        "abs_url": "https://arxiv.org/abs/2511.11651",
        "pdf_url": "https://arxiv.org/pdf/2511.11651",
        "title": "Incomplete Depression Feature Selection with Missing EEG Channels",
        "authors": [
            "Zhijian Gong",
            "Wenjia Dong",
            "Xueyuan Xu",
            "Fulin Wei",
            "Chunyu Liu",
            "Li Zhuo"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "As a critical mental health disorder, depression has severe effects on both human physical and mental well-being. Recent developments in EEG-based depression analysis have shown promise in improving depression detection accuracies. However, EEG features often contain redundant, irrelevant, and noisy information. Additionally, real-world EEG data acquisition frequently faces challenges, such as data loss from electrode detachment and heavy noise interference. To tackle the challenges, we propose a novel feature selection approach for robust depression analysis, called Incomplete Depression Feature Selection with Missing EEG Channels (IDFS-MEC). IDFS-MEC integrates missing-channel indicator information and adaptive channel weighting learning into orthogonal regression to lessen the effects of incomplete channels on model construction, and then utilizes global redundancy minimization learning to reduce redundant information among selected feature subsets. Extensive experiments conducted on MODMA and PRED-d003 datasets reveal that the EEG feature subsets chosen by IDFS-MEC have superior performance than 10 popular feature selection methods among 3-, 64-, and 128-channel settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 20,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11652",
        "abs_url": "https://arxiv.org/abs/2511.11652",
        "pdf_url": "https://arxiv.org/pdf/2511.11652",
        "title": "How many stations are sufficient? Exploring the effect of urban weather station density reduction on imputation accuracy of air temperature and humidity",
        "authors": [
            "Marvin Plein",
            "Carsten F. Dormann",
            "Andreas Christen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Urban weather station networks (WSNs) are widely used to monitor urban weather and climate patterns and aid urban planning. However, maintaining WSNs is expensive and labor-intensive. Here, we present a step-wise station removal procedure to thin an existing WSN in Freiburg, Germany, and analyze the ability of WSN subsets to reproduce air temperature and humidity patterns of the entire original WSN for a year following a simulated reduction of WSN density. We found that substantial reductions in station numbers after one year of full deployment are possible while retaining high predictive accuracy. A reduction from 42 to 4 stations, for instance, increased mean prediction RMSEs from 0.69 K to 0.83 K for air temperature and from 3.8% to 4.4% for relative humidity, corresponding to RMSE increases of only 20% and 16%, respectively. Predictive accuracy is worse for remote stations in forests than for stations in built-up or open settings, but consistently better than a state-of-the-art numerical urban land-surface model (Surface Urban Energy and Water Balance Scheme). Stations located at the edges between built-up and rural areas are most valuable when reconstructing city-wide climate characteristics. Our study demonstrates the potential of thinning WSNs to maximize the efficient allocation of financial and personnel-related resources in urban climate research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 21,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11656",
        "abs_url": "https://arxiv.org/abs/2511.11656",
        "pdf_url": "https://arxiv.org/pdf/2511.11656",
        "title": "On the Probabilistic Learnability of Compact Neural Network Preimage Bounds",
        "authors": [
            "Luca Marzari",
            "Manuele Bicego",
            "Ferdinando Cicalese",
            "Alessandro Farinelli"
        ],
        "comments": "Accepted at the 40th Annual AAAI Conference on Artificial Intelligence 2026",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Although recent provable methods have been developed to compute preimage bounds for neural networks, their scalability is fundamentally limited by the #P-hardness of the problem. In this work, we adopt a novel probabilistic perspective, aiming to deliver solutions with high-confidence guarantees and bounded error. To this end, we investigate the potential of bootstrap-based and randomized approaches that are capable of capturing complex patterns in high-dimensional spaces, including input regions where a given output property holds. In detail, we introduce $\\textbf{R}$andom $\\textbf{F}$orest $\\textbf{Pro}$perty $\\textbf{Ve}$rifier ($\\texttt{RF-ProVe}$), a method that exploits an ensemble of randomized decision trees to generate candidate input regions satisfying a desired output property and refines them through active resampling. Our theoretical derivations offer formal statistical guarantees on region purity and global coverage, providing a practical, scalable solution for computing compact preimage approximations in cases where exact solvers fail to scale.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 22,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11665",
        "abs_url": "https://arxiv.org/abs/2511.11665",
        "pdf_url": "https://arxiv.org/pdf/2511.11665",
        "title": "Clifford Algebraic Rotor Embeddings : Maybe embeddings should start to CARE",
        "authors": [
            "Sameeksha Sriram",
            "Ayush Paliwal",
            "Alexander S. Ecker",
            "Chase van de Geijn"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Rotary Positional Embeddings (RoPE) have demonstrated exceptional performance as a positional encoding method, consistently outperforming their baselines. While recent work has sought to extend RoPE to higher-dimensional inputs, many such extensions are non-commutative, thereby forfeiting RoPE's shift-equivariance property. Spherical RoPE is one such non-commutative variant, motivated by the idea of rotating embedding vectors on spheres rather than circles. However, spherical rotations are inherently non-commutative, making the choice of rotation sequence ambiguous. In this work, we explore a quaternion-based approach -- Quaternion Rotary Embeddings (QuatRo) -- in place of Euler angles, leveraging quaternions' ability to represent 3D rotations to parameterize the axes of rotation. We show Mixed RoPE and Spherical RoPE to be special cases of QuatRo. Further, we propose a generalization of QuatRo to Clifford Algebraic Rotary Embeddings (CARE) using geometric algebra. Viewing quaternions as the even subalgebra of Cl(3,0,0), we extend the notion of rotary embeddings from quaternions to Clifford rotors acting on multivectors. This formulation enables two key generalizations: (1) extending rotary embeddings to arbitrary dimensions, and (2) encoding positional information in multivectors of multiple grades, not just vectors. We present preliminary experiments comparing spherical, quaternion, and Clifford-based rotary embeddings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 23,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11666",
        "abs_url": "https://arxiv.org/abs/2511.11666",
        "pdf_url": "https://arxiv.org/pdf/2511.11666",
        "title": "Adaptive Stepsizing for Stochastic Gradient Langevin Dynamics in Bayesian Neural Networks",
        "authors": [
            "Rajit Rajpal",
            "Benedict Leimkuhler",
            "Yuanhao Jiang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Bayesian neural networks (BNNs) require scalable sampling algorithms to approximate posterior distributions over parameters. Existing stochastic gradient Markov Chain Monte Carlo (SGMCMC) methods are highly sensitive to the choice of stepsize and adaptive variants such as pSGLD typically fail to sample the correct invariant measure without addition of a costly divergence correction term. In this work, we build on the recently proposed `SamAdams' framework for timestep adaptation (Leimkuhler, Lohmann, and Whalley 2025), introducing an adaptive scheme: SA-SGLD, which employs time rescaling to modulate the stepsize according to a monitored quantity (typically the local gradient norm). SA-SGLD can automatically shrink stepsizes in regions of high curvature and expand them in flatter regions, improving both stability and mixing without introducing bias. We show that our method can achieve more accurate posterior sampling than SGLD on high-curvature 2D toy examples and in image classification with BNNs using sharp priors.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 24,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11668",
        "abs_url": "https://arxiv.org/abs/2511.11668",
        "pdf_url": "https://arxiv.org/pdf/2511.11668",
        "title": "Do traveling waves make good positional encodings?",
        "authors": [
            "Chase van de Geijn",
            "Ayush Paliwal",
            "Timo Lüddecke",
            "Alexander S. Ecker"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Transformers rely on positional encoding to compensate for the inherent permutation invariance of self-attention. Traditional approaches use absolute sinusoidal embeddings or learned positional vectors, while more recent methods emphasize relative encodings to better capture translation equivariances. In this work, we propose RollPE, a novel positional encoding mechanism based on traveling waves, implemented by applying a circular roll operation to the query and key tensors in self-attention. This operation induces a relative shift in phase across positions, allowing the model to compute attention as a function of positional differences rather than absolute indices. We show this simple method significantly outperforms traditional absolute positional embeddings and is comparable to RoPE. We derive a continuous case of RollPE which implicitly imposes a topographic structure on the query and key space. We further derive a mathematical equivalence of RollPE to a particular configuration of RoPE. Viewing RollPE through the lens of traveling waves may allow us to simplify RoPE and relate it to processes of information flow in the brain.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 25,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11669",
        "abs_url": "https://arxiv.org/abs/2511.11669",
        "pdf_url": "https://arxiv.org/pdf/2511.11669",
        "title": "H-Model: Dynamic Neural Architectures for Adaptive Processing",
        "authors": [
            "Dmytro Hospodarchuk"
        ],
        "comments": "Independent research report, 24 pages including references and figures",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
        "abstract": "This article explores the design and experimentation of a neural network architecture capable of dynamically adjusting its internal structure based on the input data. The proposed model introduces a routing mechanism that allows each layer to influence how its outputs are propagated through the network, enabling iterative and adaptive computation. This concept is loosely inspired by the idea of thought processes and dynamic reasoning, where information flow is conditioned not only on the data itself, but also on the internal state of the system. It is important to note that this work does not aim to compete with state-of-the-art language models in terms of performance. Instead, it presents a conceptual prototype-an architectural framework that opens up a new direction for exploring adaptable and potentially more interpretable networks. The goal is not optimization of existing benchmarks but rather the proposal of a system that can learn not only representations, but also the structure of computation itself. Due to practical constraints in computing resources and data, this study remains a preliminary investigation. Nevertheless, initial observations show promise, and the architecture's full potential can only be evaluated in future experiments under more favorable computational conditions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 26,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11671",
        "abs_url": "https://arxiv.org/abs/2511.11671",
        "pdf_url": "https://arxiv.org/pdf/2511.11671",
        "title": "Evaluation of LLM-based Explanations for a Learning Analytics Dashboard",
        "authors": [
            "Alina Deriyeva",
            "Benjamin Paassen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Human-Computer Interaction (cs.HC)",
        "abstract": "Learning Analytics Dashboards can be a powerful tool to support self-regulated learning in Digital Learning Environments and promote development of meta-cognitive skills, such as reflection. However, their effectiveness can be affected by the interpretability of the data they provide. To assist in the interpretation, we employ a large language model to generate verbal explanations of the data in the dashboard and evaluate it against a standalone dashboard and explanations provided by human teachers in an expert study with university level educators (N=12). We find that the LLM-based explanations of the skill state presented in the dashboard, as well as general recommendations on how to proceed with learning within the course are significantly more favored compared to the other conditions. This indicates that using LLMs for interpretation purposes can enhance the learning experience for learners while maintaining the pedagogical standards approved by teachers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 27,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11673",
        "abs_url": "https://arxiv.org/abs/2511.11673",
        "pdf_url": "https://arxiv.org/pdf/2511.11673",
        "title": "Synergistic Feature Fusion for Latent Lyrical Classification: A Gated Deep Learning Architecture",
        "authors": [
            "M. A. Gameiro"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "This study addresses the challenge of integrating complex, high-dimensional deep semantic features with simple, interpretable structural cues for lyrical content classification. We introduce a novel Synergistic Fusion Layer (SFL) architecture, a deep learning model utilizing a gated mechanism to modulate Sentence-BERT embeddings (Fdeep) using low-dimensional auxiliary features (Fstruct). The task, derived from clustering UMAP-reduced lyrical embeddings, is reframed as binary classification, distinguishing a dominant, homogeneous cluster (Class 0) from all other content (Class 1). The SFL model achieved an accuracy of 0.9894 and a Macro F1 score of 0.9894, outperforming a comprehensive Random Forest (RF) baseline that used feature concatenation (Accuracy = 0.9868). Crucially, the SFL model demonstrated vastly superior reliability and calibration, exhibiting a 93% reduction in Expected Calibration Error (ECE = 0.0035) and a 2.5x lower Log Loss (0.0304) compared to the RF baseline (ECE = 0.0500; Log Loss = 0.0772). This performance validates the architectural hypothesis that non-linear gating is superior to simple feature concatenation, establishing the SFL model as a robust and trustworthy system for complex multimodal lyrical analysis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 28,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11676",
        "abs_url": "https://arxiv.org/abs/2511.11676",
        "pdf_url": "https://arxiv.org/pdf/2511.11676",
        "title": "Learning with Preserving for Continual Multitask Learning",
        "authors": [
            "Hanchen David Wang",
            "Siwoo Bae",
            "Zirong Chen",
            "Meiyi Ma"
        ],
        "comments": "25 pages, 16 figures, accepted at AAAI-2026",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Artificial intelligence systems in critical fields like autonomous driving and medical imaging analysis often continually learn new tasks using a shared stream of input data. For instance, after learning to detect traffic signs, a model may later need to learn to classify traffic lights or different types of vehicles using the same camera feed. This scenario introduces a challenging setting we term Continual Multitask Learning (CMTL), where a model sequentially learns new tasks on an underlying data distribution without forgetting previously learned abilities. Existing continual learning methods often fail in this setting because they learn fragmented, task-specific features that interfere with one another. To address this, we introduce Learning with Preserving (LwP), a novel framework that shifts the focus from preserving task outputs to maintaining the geometric structure of the shared representation space. The core of LwP is a Dynamically Weighted Distance Preservation (DWDP) loss that prevents representation drift by regularizing the pairwise distances between latent data representations. This mechanism of preserving the underlying geometric structure allows the model to retain implicit knowledge and support diverse tasks without requiring a replay buffer, making it suitable for privacy-conscious applications. Extensive evaluations on time-series and image benchmarks show that LwP not only mitigates catastrophic forgetting but also consistently outperforms state-of-the-art baselines in CMTL tasks. Notably, our method shows superior robustness to distribution shifts and is the only approach to surpass the strong single-task learning baseline, underscoring its effectiveness for real-world dynamic environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 29,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11677",
        "abs_url": "https://arxiv.org/abs/2511.11677",
        "pdf_url": "https://arxiv.org/pdf/2511.11677",
        "title": "Homotopy-Guided Self-Supervised Learning of Parametric Solutions for AC Optimal Power Flow",
        "authors": [
            "Shimiao Li",
            "Aaron Tuor",
            "Draguna Vrabie",
            "Larry Pileggi",
            "Jan Drgona"
        ],
        "comments": "paper submitted to PES General Meeting 2026",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Learning to optimize (L2O) parametric approximations of AC optimal power flow (AC-OPF) solutions offers the potential for fast, reusable decision-making in real-time power system operations. However, the inherent nonconvexity of AC-OPF results in challenging optimization landscapes, and standard learning approaches often fail to converge to feasible, high-quality solutions. This work introduces a \\textit{homotopy-guided self-supervised L2O method} for parametric AC-OPF problems. The key idea is to construct a continuous deformation of the objective and constraints during training, beginning from a relaxed problem with a broad basin of attraction and gradually transforming it toward the original problem. The resulting learning process improves convergence stability and promotes feasibility without requiring labeled optimal solutions or external solvers. We evaluate the proposed method on standard IEEE AC-OPF benchmarks and show that homotopy-guided L2O significantly increases feasibility rates compared to non-homotopy baselines, while achieving objective values comparable to full OPF solvers. These findings demonstrate the promise of homotopy-based heuristics for scalable, constraint-aware L2O in power system optimization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 30,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11680",
        "abs_url": "https://arxiv.org/abs/2511.11680",
        "pdf_url": "https://arxiv.org/pdf/2511.11680",
        "title": "Probabilistic Wildfire Susceptibility from Remote Sensing Using Random Forests and SHAP",
        "authors": [
            "Udaya Bhasker Cheerala",
            "Varun Teja Chirukuri",
            "Venkata Akhil Kumar Gummadi",
            "Jintu Moni Bhuyan",
            "Praveen Damacharla"
        ],
        "comments": "7 pages, 2025 IEEE Asia-Pacific Conference on Geoscience, Electronics and Remote Sensing Technology (AGERS)",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Wildfires pose a significant global threat to ecosystems worldwide, with California experiencing recurring fires due to various factors, including climate, topographical features, vegetation patterns, and human activities. This study aims to develop a comprehensive wildfire risk map for California by applying the random forest (RF) algorithm, augmented with Explainable Artificial Intelligence (XAI) through Shapley Additive exPlanations (SHAP), to interpret model predictions. Model performance was assessed using both spatial and temporal validation strategies. The RF model demonstrated strong predictive performance, achieving near-perfect discrimination for grasslands (AUC = 0.996) and forests (AUC = 0.997). Spatial cross-validation revealed moderate transferability, yielding ROC-AUC values of 0.6155 for forests and 0.5416 for grasslands. In contrast, temporal split validation showed enhanced generalization, especially for forests (ROC-AUC = 0.6615, PR-AUC = 0.8423). SHAP-based XAI analysis identified key ecosystem-specific drivers: soil organic carbon, tree cover, and Normalized Difference Vegetation Index (NDVI) emerged as the most influential in forests, whereas Land Surface Temperature (LST), elevation, and vegetation health indices were dominant in grasslands. District-level classification revealed that Central Valley and Northern Buttes districts had the highest concentration of high-risk grasslands, while Northern Buttes and North Coast Redwoods dominated forested high-risk areas. This RF-SHAP framework offers a robust, comprehensible, and adaptable method for assessing wildfire risks, enabling informed decisions and creating targeted strategies to mitigate dangers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 31,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11683",
        "abs_url": "https://arxiv.org/abs/2511.11683",
        "pdf_url": "https://arxiv.org/pdf/2511.11683",
        "title": "Stratified Knowledge-Density Super-Network for Scalable Vision Transformers",
        "authors": [
            "Longhua Li",
            "Lei Qi",
            "Xin Geng"
        ],
        "comments": "Accepted by AAAI 2026",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Training and deploying multiple vision transformer (ViT) models for different resource constraints is costly and inefficient. To address this, we propose transforming a pre-trained ViT into a stratified knowledge-density super-network, where knowledge is hierarchically organized across weights. This enables flexible extraction of sub-networks that retain maximal knowledge for varying model sizes. We introduce \\textbf{W}eighted \\textbf{P}CA for \\textbf{A}ttention \\textbf{C}ontraction (WPAC), which concentrates knowledge into a compact set of critical weights. WPAC applies token-wise weighted principal component analysis to intermediate features and injects the resulting transformation and inverse matrices into adjacent layers, preserving the original network function while enhancing knowledge compactness. To further promote stratified knowledge organization, we propose \\textbf{P}rogressive \\textbf{I}mportance-\\textbf{A}ware \\textbf{D}ropout (PIAD). PIAD progressively evaluates the importance of weight groups, updates an importance-aware dropout list, and trains the super-network under this dropout regime to promote knowledge stratification. Experiments demonstrate that WPAC outperforms existing pruning criteria in knowledge concentration, and the combination with PIAD offers a strong alternative to state-of-the-art model compression and model expansion methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 32,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11684",
        "abs_url": "https://arxiv.org/abs/2511.11684",
        "pdf_url": "https://arxiv.org/pdf/2511.11684",
        "title": "A Bayesian Model for Multi-stage Censoring",
        "authors": [
            "Shuvom Sadhuka",
            "Sophia Lin",
            "Emma Pierson",
            "Bonnie Berger"
        ],
        "comments": "Proceedings of ML4H 2025",
        "subjects": "Machine Learning (cs.LG); Applications (stat.AP)",
        "abstract": "Many sequential decision settings in healthcare feature funnel structures characterized by a series of stages, such as screenings or evaluations, where the number of patients who advance to each stage progressively decreases and decisions become increasingly costly. For example, an oncologist may first conduct a breast exam, followed by a mammogram for patients with concerning exams, followed by a biopsy for patients with concerning mammograms. A key challenge is that the ground truth outcome, such as the biopsy result, is only revealed at the end of this funnel. The selective censoring of the ground truth can introduce statistical biases in risk estimation, especially in underserved patient groups, whose outcomes are more frequently censored. We develop a Bayesian model for funnel decision structures, drawing from prior work on selective labels and censoring. We first show in synthetic settings that our model is able to recover the true parameters and predict outcomes for censored patients more accurately than baselines. We then apply our model to a dataset of emergency department visits, where in-hospital mortality is observed only for those who are admitted to either the hospital or ICU. We find that there are gender-based differences in hospital and ICU admissions. In particular, our model estimates that the mortality risk threshold to admit women to the ICU is higher for women (5.1%) than for men (4.5%).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 33,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11685",
        "abs_url": "https://arxiv.org/abs/2511.11685",
        "pdf_url": "https://arxiv.org/pdf/2511.11685",
        "title": "R-Tuning: Wavelet-Decomposed Replay and Semantic Alignment for Continual Adaptation of Pretrained Time-Series Models",
        "authors": [
            "Tianyi Yin",
            "Jingwei Wang",
            "Chenze Wang",
            "Han Wang",
            "Jiexuan Cai",
            "Min Liu",
            "Yunlong Ma",
            "Kun Gao",
            "Yuting Song",
            "Weiming Shen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Pre-trained models have demonstrated exceptional generalization capabilities in time-series forecasting; however, adapting them to evolving data distributions remains a significant challenge. A key hurdle lies in accessing the original training data, as fine-tuning solely on new data often leads to catastrophic forgetting. To address this issue, we propose Replay Tuning (R-Tuning), a novel framework designed for the continual adaptation of pre-trained time-series models. R-Tuning constructs a unified latent space that captures both prior and current task knowledge through a frequency-aware replay strategy. Specifically, it augments model-generated samples via wavelet-based decomposition across multiple frequency bands, generating trend-preserving and fusion-enhanced variants to improve representation diversity and replay efficiency. To further reduce reliance on synthetic samples, R-Tuning introduces a latent consistency constraint that aligns new representations with the prior task space. This constraint guides joint optimization within a compact and semantically coherent latent space, ensuring robust knowledge retention and adaptation. Extensive experimental results demonstrate the superiority of R-Tuning, which reduces MAE and MSE by up to 46.9% and 46.8%, respectively, on new tasks, while preserving prior knowledge with gains of up to 5.7% and 6.0% on old tasks. Notably, under few-shot settings, R-Tuning outperforms all state-of-the-art baselines even when synthetic proxy samples account for only 5% of the new task dataset.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 34,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11686",
        "abs_url": "https://arxiv.org/abs/2511.11686",
        "pdf_url": "https://arxiv.org/pdf/2511.11686",
        "title": "Regularized Schrödinger: Alleviating Distortion and Exposure Bias in Solving Inverse Problems",
        "authors": [
            "Qing Yao",
            "Lijian Gao",
            "Qirong Mao",
            "Dong Ming"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Sound (cs.SD)",
        "abstract": "Diffusion models serve as a powerful generative framework for solving inverse problems. However, they still face two key challenges: 1) the distortion-perception tradeoff, where improving perceptual quality often degrades reconstruction fidelity, and 2) the exposure bias problem, where the training-inference input mismatch leads to prediction error accumulation and reduced reconstruction quality. In this work, we propose the Regularized Schrödinger Bridge (RSB), an adaptation of Schrödinger Bridge tailored for inverse problems that addresses the above limitations. RSB employs a novel regularized training strategy that perturbs both the input states and targets, effectively mitigating exposure bias by exposing the model to simulated prediction errors and also alleviating distortion by well-designed interpolation via the posterior mean. Extensive experiments on two typical inverse problems for speech enhancement demonstrate that RSB outperforms state-of-the-art methods, significantly improving distortion metrics and effectively reducing exposure bias.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 35,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11690",
        "abs_url": "https://arxiv.org/abs/2511.11690",
        "pdf_url": "https://arxiv.org/pdf/2511.11690",
        "title": "Doubly Debiased Test-Time Prompt Tuning for Vision-Language Models",
        "authors": [
            "Fei Song",
            "Yi Li",
            "Rui Wang",
            "Jiahuan Zhou",
            "Changwen Zheng",
            "Jiangmeng Li"
        ],
        "comments": "Accepted by AAAI2026",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Test-time prompt tuning for vision-language models has demonstrated impressive generalization capabilities under zero-shot settings. However, tuning the learnable prompts solely based on unlabeled test data may induce prompt optimization bias, ultimately leading to suboptimal performance on downstream tasks. In this work, we analyze the underlying causes of prompt optimization bias from both the model and data perspectives. In terms of the model, the entropy minimization objective typically focuses on reducing the entropy of model predictions while overlooking their correctness. This can result in overconfident yet incorrect outputs, thereby compromising the quality of prompt optimization. On the data side, prompts affected by optimization bias can introduce misalignment between visual and textual modalities, which further aggravates the prompt optimization bias. To this end, we propose a Doubly Debiased Test-Time Prompt Tuning method. Specifically, we first introduce a dynamic retrieval-augmented modulation module that retrieves high-confidence knowledge from a dynamic knowledge base using the test image feature as a query, and uses the retrieved knowledge to modulate the predictions. Guided by the refined predictions, we further develop a reliability-aware prompt optimization module that incorporates a confidence-based weighted ensemble and cross-modal consistency distillation to impose regularization constraints during prompt tuning. Extensive experiments across 15 benchmark datasets involving both natural distribution shifts and cross-datasets generalization demonstrate that our method outperforms baselines, validating its effectiveness in mitigating prompt optimization bias.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 36,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11691",
        "abs_url": "https://arxiv.org/abs/2511.11691",
        "pdf_url": "https://arxiv.org/pdf/2511.11691",
        "title": "Beyond saliency: enhancing explanation of speech emotion recognition with expert-referenced acoustic cues",
        "authors": [
            "Seham Nasr",
            "Zhao Ren",
            "David Johnson"
        ],
        "comments": "5 pages, 2 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Sound (cs.SD)",
        "abstract": "Explainable AI (XAI) for Speech Emotion Recognition (SER) is critical for building transparent, trustworthy models. Current saliency-based methods, adapted from vision, highlight spectrogram regions but fail to show whether these regions correspond to meaningful acoustic markers of emotion, limiting faithfulness and interpretability. We propose a framework that overcomes these limitations by quantifying the magnitudes of cues within salient regions. This clarifies \"what\" is highlighted and connects it to \"why\" it matters, linking saliency to expert-referenced acoustic cues of speech emotions. Experiments on benchmark SER datasets show that our approach improves explanation quality by explicitly linking salient regions to theory-driven speech emotions expert-referenced acoustics. Compared to standard saliency methods, it provides more understandable and plausible explanations of SER models, offering a foundational step towards trustworthy speech-based affective computing.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 37,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11696",
        "abs_url": "https://arxiv.org/abs/2511.11696",
        "pdf_url": "https://arxiv.org/pdf/2511.11696",
        "title": "Toward Dignity-Aware AI: Next-Generation Elderly Monitoring from Fall Detection to ADL",
        "authors": [
            "Xun Shao",
            "Aoba Otani",
            "Yuto Hirasuka",
            "Runji Cai",
            "Seng W. Loke"
        ],
        "comments": "This is the author's preprint version of a paper accepted for presentation at EAI MONAMI 2025 (to appear in Springer LNICST). The final authenticated version will be available online at Springer Link upon publication",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV); Computers and Society (cs.CY)",
        "abstract": "This position paper envisions a next-generation elderly monitoring system that moves beyond fall detection toward the broader goal of Activities of Daily Living (ADL) recognition. Our ultimate aim is to design privacy-preserving, edge-deployed, and federated AI systems that can robustly detect and understand daily routines, supporting independence and dignity in aging societies. At present, ADL-specific datasets are still under collection. As a preliminary step, we demonstrate feasibility through experiments using the SISFall dataset and its GAN-augmented variants, treating fall detection as a proxy task. We report initial results on federated learning with non-IID conditions, and embedded deployment on Jetson Orin Nano devices. We then outline open challenges such as domain shift, data scarcity, and privacy risks, and propose directions toward full ADL monitoring in smart-room environments. This work highlights the transition from single-task detection to comprehensive daily activity recognition, providing both early evidence and a roadmap for sustainable and human-centered elderly care AI.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 38,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11697",
        "abs_url": "https://arxiv.org/abs/2511.11697",
        "pdf_url": "https://arxiv.org/pdf/2511.11697",
        "title": "Benchmarking GNNs for OOD Materials Property Prediction with Uncertainty Quantification",
        "authors": [
            "Liqin Tan",
            "Pin Chen",
            "Menghan Liu",
            "Xiean Wang",
            "Jianhuan Cen",
            "Qingsong Zou"
        ],
        "comments": "12 pages, 1 figure, 5 tables",
        "subjects": "Machine Learning (cs.LG); Materials Science (cond-mat.mtrl-sci)",
        "abstract": "We present MatUQ, a benchmark framework for evaluating graph neural networks (GNNs) on out-of-distribution (OOD) materials property prediction with uncertainty quantification (UQ). MatUQ comprises 1,375 OOD prediction tasks constructed from six materials datasets using five OFM-based and a newly proposed structure-aware splitting strategy, SOAP-LOCO, which captures local atomic environments more effectively. We evaluate 12 representative GNN models under a unified uncertainty-aware training protocol that combines Monte Carlo Dropout and Deep Evidential Regression (DER), and introduce a novel uncertainty metric, D-EviU, which shows the strongest correlation with prediction errors in most tasks. Our experiments yield two key findings. First, the uncertainty-aware training approach significantly improves model prediction accuracy, reducing errors by an average of 70.6\\% across challenging OOD scenarios. Second, the benchmark reveals that no single model dominates universally: earlier models such as SchNet and ALIGNN remain competitive, while newer models like CrystalFramer and SODNet demonstrate superior performance on specific material properties. These results provide practical insights for selecting reliable models under distribution shifts in materials discovery.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 39,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11698",
        "abs_url": "https://arxiv.org/abs/2511.11698",
        "pdf_url": "https://arxiv.org/pdf/2511.11698",
        "title": "Moirai 2.0: When Less Is More for Time Series Forecasting",
        "authors": [
            "Chenghao Liu",
            "Taha Aksu",
            "Juncheng Liu",
            "Xu Liu",
            "Hanshu Yan",
            "Quang Pham",
            "Doyen Sahoo",
            "Caiming Xiong",
            "Silvio Savarese",
            "Junnan Li"
        ],
        "comments": "16 pages, 13 figures, and 1 table",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We introduce Moirai 2.0, a decoder-only time-series foundation model trained on a new corpus of 36M series. The model adopts quantile forecasting and multi-token prediction, improving both probabilistic accuracy and inference efficiency. On the Gift-Eval benchmark, it ranks among the top pretrained models while achieving a strong trade-off between accuracy, speed, and model size. Compared to Moirai 1.0, Moirai 2.0 replaces masked-encoder training, multi-patch inputs, and mixture-distribution outputs with a simpler decoder-only architecture, single patch, and quantile loss. Ablation studies isolate these changes -- showing that the decoder-only backbone along with recursive multi-quantile decoding contribute most to the gains. Additional experiments show that Moirai 2.0 outperforms larger models from the same family and exhibits robust domain-level results. In terms of efficiency and model size, Moirai 2.0 is twice as fast and thirty times smaller than its prior best version, Moirai 1.0-Large, while also performing better. Model performance plateaus with increasing parameter count and declines at longer horizons, motivating future work on data scaling and long-horizon modeling. We release code and evaluation details to support further research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 40,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11699",
        "abs_url": "https://arxiv.org/abs/2511.11699",
        "pdf_url": "https://arxiv.org/pdf/2511.11699",
        "title": "Tighter Truncated Rectangular Prism Approximation for RNN Robustness Verification",
        "authors": [
            "Xingqi Lin",
            "Liangyu Chen",
            "Min Wu",
            "Min Zhang",
            "Zhenbing Zeng"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Robustness verification is a promising technique for rigorously proving Recurrent Neural Networks (RNNs) robustly. A key challenge is to over-approximate the nonlinear activation functions with linear constraints, which can transform the verification problem into an efficiently solvable linear programming problem. Existing methods over-approximate the nonlinear parts with linear bounding planes individually, which may cause significant over-estimation and lead to lower verification accuracy. In this paper, in order to tightly enclose the three-dimensional nonlinear surface generated by the Hadamard product, we propose a novel truncated rectangular prism formed by two linear relaxation planes and a refinement-driven method to minimize both its volume and surface area for tighter over-approximation. Based on this approximation, we implement a prototype DeepPrism for RNN robustness verification. The experimental results demonstrate that \\emph{DeepPrism} has significant improvement compared with the state-of-the-art approaches in various tasks of image classification, speech recognition and sentiment analysis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 41,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11701",
        "abs_url": "https://arxiv.org/abs/2511.11701",
        "pdf_url": "https://arxiv.org/pdf/2511.11701",
        "title": "Bayesian Neural Networks with Monte Carlo Dropout for Probabilistic Electricity Price Forecasting",
        "authors": [
            "Abhinav Das",
            "Stephan Schlüter"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Accurate electricity price forecasting is critical for strategic decision-making in deregulated electricity markets, where volatility stems from complex supply-demand dynamics and external factors. Traditional point forecasts often fail to capture inherent uncertainties, limiting their utility for risk management. This work presents a framework for probabilistic electricity price forecasting using Bayesian neural networks (BNNs) with Monte Carlo (MC) dropout, training separate models for each hour of the day to capture diurnal patterns. A critical assessment and comparison with the benchmark model, namely: generalized autoregressive conditional heteroskedasticity with exogenous variable (GARCHX) model and the LASSO estimated auto-regressive model (LEAR), highlights that the proposed model outperforms the benchmark models in terms of point prediction and intervals. This work serves as a reference for leveraging probabilistic neural models in energy market predictions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 42,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11704",
        "abs_url": "https://arxiv.org/abs/2511.11704",
        "pdf_url": "https://arxiv.org/pdf/2511.11704",
        "title": "Simple Vision-Language Math Reasoning via Rendered Text",
        "authors": [
            "Matvey Skripkin",
            "Elizaveta Goncharova",
            "Andrey Kuznetsov"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We present a lightweight yet effective pipeline for training vision-language models to solve math problems by rendering LaTeX encoded equations into images and pairing them with structured chain-of-thought prompts. This simple text-to-vision augmentation enables compact multimodal architectures to achieve state-of-the-art reasoning accuracy. Through systematic ablations, we find that rendering fidelity and prompt design are the primary drivers of performance. Despite its simplicity, our approach consistently matches or surpasses both open-source and proprietary math-focused vision-language solvers on widely used benchmarks, while preserving broad general-domain competence - showing gains on tasks such as MMMU, ChartQA, and DocVQA of up to 20%.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 43,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11706",
        "abs_url": "https://arxiv.org/abs/2511.11706",
        "pdf_url": "https://arxiv.org/pdf/2511.11706",
        "title": "Context-Aware Multimodal Representation Learning for Spatio-Temporally Explicit Environmental modelling",
        "authors": [
            "Julia Peters",
            "Karin Mora",
            "Miguel D. Mahecha",
            "Chaonan Ji",
            "David Montero",
            "Clemens Mosig",
            "Guido Kraemer"
        ],
        "comments": "10 pages (incliding 2 pages of references), 7 figures",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Earth observation (EO) foundation models have emerged as an effective approach to derive latent representations of the Earth system from various remote sensing sensors. These models produce embeddings that can be used as analysis-ready datasets, enabling the modelling of ecosystem dynamics without extensive sensor-specific preprocessing. However, existing models typically operate at fixed spatial or temporal scales, limiting their use for ecological analyses that require both fine spatial detail and high temporal fidelity. To overcome these limitations, we propose a representation learning framework that integrates different EO modalities into a unified feature space at high spatio-temporal resolution. We introduce the framework using Sentinel-1 and Sentinel-2 data as representative modalities. Our approach produces a latent space at native 10 m resolution and the temporal frequency of cloud-free Sentinel-2 acquisitions. Each sensor is first modeled independently to capture its sensor-specific characteristics. Their representations are then combined into a shared model. This two-stage design enables modality-specific optimisation and easy extension to new sensors, retaining pretrained encoders while retraining only fusion layers. This enables the model to capture complementary remote sensing data and to preserve coherence across space and time. Qualitative analyses reveal that the learned embeddings exhibit high spatial and semantic consistency across heterogeneous landscapes. Quantitative evaluation in modelling Gross Primary Production reveals that they encode ecologically meaningful patterns and retain sufficient temporal fidelity to support fine-scale analyses. Overall, the proposed framework provides a flexible, analysis-ready representation learning approach for environmental applications requiring diverse spatial and temporal resolutions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 44,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11707",
        "abs_url": "https://arxiv.org/abs/2511.11707",
        "pdf_url": "https://arxiv.org/pdf/2511.11707",
        "title": "FSC-Net: Fast-Slow Consolidation Networks for Continual Learning",
        "authors": [
            "Mohamed El Gorrim"
        ],
        "comments": "Code and the full repo available at this https URL",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Continual learning remains challenging due to catastrophic forgetting, where neural networks lose previously acquired knowledge when learning new tasks. Inspired by memory consolidation in neuroscience, we propose FSC-Net (Fast-Slow Consolidation Networks), a dual-network architecture that separates rapid task learning from gradual knowledge consolidation. Our method employs a fast network (NN1) for immediate adaptation to new tasks and a slow network (NN2) that consolidates knowledge through distillation and replay. Within the family of MLP-based NN1 variants we evaluated, consolidation effectiveness is driven more by methodology than architectural embellishments -- a simple MLP outperforms more complex similarity-gated variants by 1.2pp. Through systematic hyperparameter analysis, we observed empirically that pure replay without distillation during consolidation achieves superior performance, consistent with the hypothesis that distillation from the fast network introduces recency bias. On Split-MNIST (30 seeds), FSC-Net achieves 91.71% +/- 0.62% retention accuracy, a +4.27pp gain over the fast network alone (87.43% +/- 1.27%, paired t=23.585, p < 1e-10). On Split-CIFAR-10 (5 seeds), our method achieves 33.31% +/- 0.38% retention with an +8.20pp gain over the fast network alone (25.11% +/- 1.61%, paired t=9.75, p < 1e-3), demonstrating +8.20pp gain, though absolute performance (33.31%) remains modest and below random expectation, highlighting need for stronger backbones. Our results provide empirical evidence that the dual-timescale consolidation mechanism, rather than architectural complexity, is central to mitigating catastrophic forgetting in this setting.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 45,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11711",
        "abs_url": "https://arxiv.org/abs/2511.11711",
        "pdf_url": "https://arxiv.org/pdf/2511.11711",
        "title": "Which Sparse Autoencoder Features Are Real? Model-X Knockoffs for False Discovery Rate Control",
        "authors": [
            "Tsogt-Ochir Enkhbayar"
        ],
        "comments": "1 figure",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Although sparse autoencoders (SAEs) are crucial for identifying interpretable features in neural networks, it is still challenging to distinguish between real computational patterns and erroneous correlations. We introduce Model-X knockoffs to SAE feature selection, using knock-off+ to control the false discovery rate (FDR) with finite-sample guarantees under the standard Model-X assumptions (in our case, via a Gaussian surrogate for the latent distribution). We select 129 features at a target FDR q=0.1 after analyzing 512 high-activity SAE latents for sentiment classification using Pythia-70M. About 25% of the latents under examination carry task-relevant signal, whereas 75% do not, according to the chosen set, which displays a 5.40x separation in knockoff statistics compared to non-selected features. Our method offers a re-producible and principled framework for reliable feature discovery by combining SAEs with multiple-testing-aware inference, advancing the foundations of mechanistic interpretability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 46,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11714",
        "abs_url": "https://arxiv.org/abs/2511.11714",
        "pdf_url": "https://arxiv.org/pdf/2511.11714",
        "title": "Federated Learning for Pediatric Pneumonia Detection: Enabling Collaborative Diagnosis Without Sharing Patient Data",
        "authors": [
            "Daniel M. Jimenez-Gutierrez",
            "Enrique Zuazua",
            "Joaquin Del Rio",
            "Oleksii Sliusarenko",
            "Xabi Uribe-Etxebarria"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Cryptography and Security (cs.CR); Distributed, Parallel, and Cluster Computing (cs.DC)",
        "abstract": "Early and accurate pneumonia detection from chest X-rays (CXRs) is clinically critical to expedite treatment and isolation, reduce complications, and curb unnecessary antibiotic use. Although artificial intelligence (AI) substantially improves CXR-based detection, development is hindered by globally distributed data, high inter-hospital variability, and strict privacy regulations (e.g., HIPAA, GDPR) that make centralization impractical. These constraints are compounded by heterogeneous imaging protocols, uneven data availability, and the costs of transferring large medical images across geographically dispersed sites. In this paper, we evaluate Federated Learning (FL) using the this http URL FL platform, enabling multiple hospitals (nodes) to collaboratively train a CXR classifier for pneumonia while keeping data in place and private. Using the Pediatric Pneumonia Chest X-ray dataset, we simulate cross-hospital collaboration with non-independent and non-identically distributed (non-IID) data, reproducing real-world variability across institutions and jurisdictions. Our experiments demonstrate that collaborative and privacy-preserving training across multiple hospitals via FL led to a dramatic performance improvement achieving 0.900 Accuracy and 0.966 ROC-AUC, corresponding to 47.5% and 50.0% gains over single-hospital models (0.610; 0.644), without transferring any patient CXR. These results indicate that FL delivers high-performing, generalizable, secure and private pneumonia detection across healthcare networks, with data kept local. This is especially relevant for rare diseases, where FL enables secure multi-institutional collaboration without data movement, representing a breakthrough for accelerating diagnosis and treatment development in low-data domains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 47,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11717",
        "abs_url": "https://arxiv.org/abs/2511.11717",
        "pdf_url": "https://arxiv.org/pdf/2511.11717",
        "title": "Multiscale Grassmann Manifolds for Single-Cell Data Analysis",
        "authors": [
            "Xiang Xiang Wang",
            "Sean Cottrell",
            "Guo-Wei Wei"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Genomics (q-bio.GN)",
        "abstract": "Single-cell data analysis seeks to characterize cellular heterogeneity based on high-dimensional gene expression profiles. Conventional approaches represent each cell as a vector in Euclidean space, which limits their ability to capture intrinsic correlations and multiscale geometric structures. We propose a multiscale framework based on Grassmann manifolds that integrates machine learning with subspace geometry for single-cell data analysis. By generating embeddings under multiple representation scales, the framework combines their features from different geometric views into a unified Grassmann manifold. A power-based scale sampling function is introduced to control the selection of scales and balance in- formation across resolutions. Experiments on nine benchmark single-cell RNA-seq datasets demonstrate that the proposed approach effectively preserves meaningful structures and provides stable clustering performance, particularly for small to medium-sized datasets. These results suggest that Grassmann manifolds offer a coherent and informative foundation for analyzing single cell data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 48,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11727",
        "abs_url": "https://arxiv.org/abs/2511.11727",
        "pdf_url": "https://arxiv.org/pdf/2511.11727",
        "title": "Optimizing Input of Denoising Score Matching is Biased Towards Higher Score Norm",
        "authors": [
            "Tongda Xu"
        ],
        "comments": "NIPS 25 Workshop: Frontiers in Probabilistic Inference: Sampling Meets Learning",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Many recent works utilize denoising score matching to optimize the conditional input of diffusion models. In this workshop paper, we demonstrate that such optimization breaks the equivalence between denoising score matching and exact score matching. Furthermore, we show that this bias leads to higher score norm. Additionally, we observe a similar bias when optimizing the data distribution using a pre-trained diffusion model. Finally, we discuss the wide range of works across different domains that are affected by this bias, including MAR for auto-regressive generation, PerCo for image compression, and DreamFusion for text to 3D generation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 49,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11736",
        "abs_url": "https://arxiv.org/abs/2511.11736",
        "pdf_url": "https://arxiv.org/pdf/2511.11736",
        "title": "KAN/H: Kolmogorov-Arnold Network using Haar-like bases",
        "authors": [
            "Susumu Katayama"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "This paper proposes KAN/H, a variant of Kolmogorov-Arnold Network (KAN) that uses a Haar-variant basis system having both global and local bases instead of B-spline. The resulting algorithm is applied to function approximation problems and MNIST. We show that it does not require most of the problem-specific hyper-parameter tunings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 50,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11743",
        "abs_url": "https://arxiv.org/abs/2511.11743",
        "pdf_url": "https://arxiv.org/pdf/2511.11743",
        "title": "Uncertainty Makes It Stable: Curiosity-Driven Quantized Mixture-of-Experts",
        "authors": [
            "Sebastián Andrés Cajas Ordóñez",
            "Luis Fernando Torres Torres",
            "Mackenzie J. Meni",
            "Carlos Andrés Duran Paredes",
            "Eric Arazo",
            "Cristian Bosch",
            "Ricardo Simon Carbajo",
            "Yuan Lai",
            "Leo Anthony Celi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Deploying deep neural networks on resource-constrained devices faces two critical challenges: maintaining accuracy under aggressive quantization while ensuring predictable inference latency. We present a curiosity-driven quantized Mixture-of-Experts framework that addresses both through Bayesian epistemic uncertainty-based routing across heterogeneous experts (BitNet ternary, 1-16 bit BitLinear, post-training quantization). Evaluated on audio classification benchmarks (ESC-50, Quinn, UrbanSound8K), our 4-bit quantization maintains 99.9 percent of 16-bit accuracy (0.858 vs 0.859 F1) with 4x compression and 41 percent energy savings versus 8-bit. Crucially, curiosity-driven routing reduces MoE latency variance by 82 percent (p = 0.008, Levene's test) from 230 ms to 29 ms standard deviation, enabling stable inference for battery-constrained devices. Statistical analysis confirms 4-bit/8-bit achieve practical equivalence with full precision (p > 0.05), while MoE architectures introduce 11 percent latency overhead (p < 0.001) without accuracy gains. At scale, deployment emissions dominate training by 10000x for models serving more than 1,000 inferences, making inference efficiency critical. Our information-theoretic routing demonstrates that adaptive quantization yields accurate (0.858 F1, 1.2M params), energy-efficient (3.87 F1/mJ), and predictable edge models, with simple 4-bit quantized architectures outperforming complex MoE for most deployments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 51,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11750",
        "abs_url": "https://arxiv.org/abs/2511.11750",
        "pdf_url": "https://arxiv.org/pdf/2511.11750",
        "title": "IDOL: Meeting Diverse Distribution Shifts with Prior Physics for Tropical Cyclone Multi-Task Estimation",
        "authors": [
            "Hanting Yan",
            "Pan Mu",
            "Shiqi Zhang",
            "Yuchao Zhu",
            "Jinglin Zhang",
            "Cong Bai"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Tropical Cyclone (TC) estimation aims to accurately estimate various TC attributes in real time. However, distribution shifts arising from the complex and dynamic nature of TC environmental fields, such as varying geographical conditions and seasonal changes, present significant challenges to reliable estimation. Most existing methods rely on multi-modal fusion for feature extraction but overlook the intrinsic distribution of feature representations, leading to poor generalization under out-of-distribution (OOD) scenarios. To address this, we propose an effective Identity Distribution-Oriented Physical Invariant Learning framework (IDOL), which imposes identity-oriented constraints to regulate the feature space under the guidance of prior physical knowledge, thereby dealing distribution variability with physical invariance. Specifically, the proposed IDOL employs the wind field model and dark correlation knowledge of TC to model task-shared and task-specific identity tokens. These tokens capture task dependencies and intrinsic physical invariances of TC, enabling robust estimation of TC wind speed, pressure, inner-core, and outer-core size under distribution shifts. Extensive experiments conducted on multiple datasets and tasks demonstrate the outperformance of the proposed IDOL, verifying that imposing identity-oriented constraints based on prior physical knowledge can effectively mitigates diverse distribution shifts in TC this http URL is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 52,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11762",
        "abs_url": "https://arxiv.org/abs/2511.11762",
        "pdf_url": "https://arxiv.org/pdf/2511.11762",
        "title": "Sumudu Neural Operator for ODEs and PDEs",
        "authors": [
            "Ben Zelenskiy",
            "Saibilila Abudukelimu",
            "George Flint",
            "Kevin Zhu",
            "Sunishchal Dev"
        ],
        "comments": "5th Annual AAAI Workshop on AI to Accelerate Science and Engineering (AI2ASE)",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We introduce the Sumudu Neural Operator (SNO), a neural operator rooted in the properties of the Sumudu Transform. We leverage the relationship between the polynomial expansions of transform pairs to decompose the input space as coefficients, which are then transformed into the Sumudu Space, where the neural operator is parameterized. We evaluate the operator in ODEs (Duffing Oscillator, Lorenz System, and Driven Pendulum) and PDEs (Euler-Bernoulli Beam, Burger's Equation, Diffusion, Diffusion-Reaction, and Brusselator). SNO achieves superior performance to FNO on PDEs and demonstrates competitive accuracy with LNO on several PDE tasks, including the lowest error on the Euler-Bernoulli Beam and Diffusion Equation. Additionally, we apply zero-shot super-resolution to the PDE tasks to observe the model's capability of obtaining higher quality data from low-quality samples. These preliminary findings suggest promise for the Sumudu Transform as a neural operator design, particularly for certain classes of PDEs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 53,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11767",
        "abs_url": "https://arxiv.org/abs/2511.11767",
        "pdf_url": "https://arxiv.org/pdf/2511.11767",
        "title": "Learning Fair Representations with Kolmogorov-Arnold Networks",
        "authors": [
            "Amisha Priyadarshini",
            "Sergio Gago-Masague"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computers and Society (cs.CY)",
        "abstract": "Despite recent advances in fairness-aware machine learning, predictive models often exhibit discriminatory behavior towards marginalized groups. Such unfairness might arise from biased training data, model design, or representational disparities across groups, posing significant challenges in high-stakes decision-making domains such as college admissions. While existing fair learning models aim to mitigate bias, achieving an optimal trade-off between fairness and accuracy remains a challenge. Moreover, the reliance on black-box models hinders interpretability, limiting their applicability in socially sensitive domains. In this paper, we try to circumvent these issues by integrating Kolmogorov-Arnold Networks (KANs) within a fair adversarial learning framework. Leveraging the adversarial robustness and interpretability of KANs, our approach enables a balance between fairness and accuracy. To further facilitate this balance, we propose an adaptive penalty update mechanism that dynamically adjusts fairness constraints during the model training. We conduct numerical experiments on two real-world college admissions datasets, across three different optimization strategies. The results demonstrate the efficiency and robustness of KANs by consistently outperforming the baseline fair learning models, and maintaining high predictive accuracy while achieving competitive fairness across sensitive attributes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 54,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11778",
        "abs_url": "https://arxiv.org/abs/2511.11778",
        "pdf_url": "https://arxiv.org/pdf/2511.11778",
        "title": "CATCHFed: Efficient Unlabeled Data Utilization for Semi-Supervised Federated Learning in Limited Labels Environments",
        "authors": [
            "Byoungjun Park",
            "Pedro Porto Buarque de Gusmão",
            "Dongjin Ji",
            "Minhoe Kim"
        ],
        "comments": "11pages, prepared for submission",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Federated learning is a promising paradigm that utilizes distributed client resources while preserving data privacy. Most existing FL approaches assume clients possess labeled data, however, in real-world scenarios, client-side labels are often unavailable. Semi-supervised Federated learning, where only the server holds labeled data, addresses this issue. However, it experiences significant performance degradation as the number of labeled data decreases. To tackle this problem, we propose \\textit{CATCHFed}, which introduces client-aware adaptive thresholds considering class difficulty, hybrid thresholds to enhance pseudo-label quality, and utilizes unpseudo-labeled data for consistency regularization. Extensive experiments across various datasets and configurations demonstrate that CATCHFed effectively leverages unlabeled client data, achieving superior performance even in extremely limited-label settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 55,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11819",
        "abs_url": "https://arxiv.org/abs/2511.11819",
        "pdf_url": "https://arxiv.org/pdf/2511.11819",
        "title": "Simplicial covering dimension of extremal concept classes",
        "authors": [
            "Ari Blondal",
            "Hamed Hatami",
            "Pooya Hatami",
            "Chavdar Lalov",
            "Sivan Tretiak"
        ],
        "comments": "31 pages, 5 figures",
        "subjects": "Machine Learning (cs.LG); Algebraic Topology (math.AT)",
        "abstract": "Dimension theory is a branch of topology concerned with defining and analyzing dimensions of geometric and topological spaces in purely topological terms. In this work, we adapt the classical notion of topological dimension (Lebesgue covering) to binary concept classes. The topological space naturally associated with a concept class is its space of realizable distributions. The loss function and the class itself induce a simplicial structure on this space, with respect to which we define a simplicial covering dimension. We prove that for finite concept classes, this simplicial covering dimension exactly characterizes the list replicability number (equivalently, global stability) in PAC learning. This connection allows us to apply tools from classical dimension theory to compute the exact list replicability number of the broad family of extremal concept classes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 56,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11834",
        "abs_url": "https://arxiv.org/abs/2511.11834",
        "pdf_url": "https://arxiv.org/pdf/2511.11834",
        "title": "Volatility in Certainty (VC): A Metric for Detecting Adversarial Perturbations During Inference in Neural Network Classifiers",
        "authors": [
            "Vahid Hemmati",
            "Ahmad Mohammadi",
            "Abdul-Rauf Nuhu",
            "Reza Ahmari",
            "Parham Kebria",
            "Abdollah Homaifar"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Adversarial robustness remains a critical challenge in deploying neural network classifiers, particularly in real-time systems where ground-truth labels are unavailable during inference. This paper investigates \\textit{Volatility in Certainty} (VC), a recently proposed, label-free metric that quantifies irregularities in model confidence by measuring the dispersion of sorted softmax outputs. Specifically, VC is defined as the average squared log-ratio of adjacent certainty values, capturing local fluctuations in model output smoothness. We evaluate VC as a proxy for classification accuracy and as an indicator of adversarial drift. Experiments are conducted on artificial neural networks (ANNs) and convolutional neural networks (CNNs) trained on MNIST, as well as a regularized VGG-like model trained on CIFAR-10. Adversarial examples are generated using the Fast Gradient Sign Method (FGSM) across varying perturbation magnitudes. In addition, mixed test sets are created by gradually introducing adversarial contamination to assess VC's sensitivity under incremental distribution shifts. Our results reveal a strong negative correlation between classification accuracy and log(VC) (correlation rho < -0.90 in most cases), suggesting that VC effectively reflects performance degradation without requiring labeled data. These findings position VC as a scalable, architecture-agnostic, and real-time performance metric suitable for early-warning systems in safety-critical applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 57,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11842",
        "abs_url": "https://arxiv.org/abs/2511.11842",
        "pdf_url": "https://arxiv.org/pdf/2511.11842",
        "title": "On the Trade-Off Between Transparency and Security in Adversarial Machine Learning",
        "authors": [
            "Lucas Fenaux",
            "Christopher Srinivasa",
            "Florian Kerschbaum"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Cryptography and Security (cs.CR); Computer Science and Game Theory (cs.GT)",
        "abstract": "Transparency and security are both central to Responsible AI, but they may conflict in adversarial settings. We investigate the strategic effect of transparency for agents through the lens of transferable adversarial example attacks. In transferable adversarial example attacks, attackers maliciously perturb their inputs using surrogate models to fool a defender's target model. These models can be defended or undefended, with both players having to decide which to use. Using a large-scale empirical evaluation of nine attacks across 181 models, we find that attackers are more successful when they match the defender's decision; hence, obscurity could be beneficial to the defender. With game theory, we analyze this trade-off between transparency and security by modeling this problem as both a Nash game and a Stackelberg game, and comparing the expected outcomes. Our analysis confirms that only knowing whether a defender's model is defended or not can sometimes be enough to damage its security. This result serves as an indicator of the general trade-off between transparency and security, suggesting that transparency in AI systems can be at odds with security. Beyond adversarial machine learning, our work illustrates how game-theoretic reasoning can uncover conflicts between transparency and security.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 58,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11849",
        "abs_url": "https://arxiv.org/abs/2511.11849",
        "pdf_url": "https://arxiv.org/pdf/2511.11849",
        "title": "Leveraging Exogenous Signals for Hydrology Time Series Forecasting",
        "authors": [
            "Junyang He",
            "Judy Fox",
            "Alireza Jafari",
            "Ying-Jung Chen",
            "Geoffrey Fox"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Recent advances in time series research facilitate the development of foundation models. While many state-of-the-art time series foundation models have been introduced, few studies examine their effectiveness in specific downstream applications in physical science. This work investigates the role of integrating domain knowledge into time series models for hydrological rainfall-runoff modeling. Using the CAMELS-US dataset, which includes rainfall and runoff data from 671 locations with six time series streams and 30 static features, we compare baseline and foundation models. Results demonstrate that models incorporating comprehensive known exogenous inputs outperform more limited approaches, including foundation models. Notably, incorporating natural annual periodic time series contribute the most significant improvements.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 59,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11881",
        "abs_url": "https://arxiv.org/abs/2511.11881",
        "pdf_url": "https://arxiv.org/pdf/2511.11881",
        "title": "Better LLM Reasoning via Dual-Play",
        "authors": [
            "Zhengxin Zhang",
            "Chengyu Huang",
            "Aochong Oliver Li",
            "Claire Cardie"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Large Language Models (LLMs) have achieved remarkable progress through Reinforcement Learning with Verifiable Rewards (RLVR), yet still rely heavily on external supervision (e.g., curated labels). Adversarial learning, particularly through self-play, offers a promising alternative that enables models to iteratively learn from themselves - thus reducing reliance on external supervision. Dual-play extends adversarial learning by assigning specialized roles to two models and training them against each other, fostering sustained competition and mutual evolution. Despite its promise, adapting dual-play training to LLMs remains limited, largely due to their susceptibility to reward hacking and training instability. In this paper, we introduce PasoDoble, a novel LLM dual-play framework. PasoDoble adversarially trains two models initialized from the same base model: a Proposer, which generates challenging questions with ground-truth answers, and a Solver, which attempts to solve them. We enrich the Proposer with knowledge from a pre-training dataset to ensure the questions' quality and diversity. To avoid reward hacking, the Proposer is rewarded for producing only valid questions that push the Solver's limit, while the Solver is rewarded for solving them correctly, and both are updated jointly. To further enhance training stability, we introduce an optional offline paradigm that decouples Proposer and Solver updates, alternately updating each for several steps while holding the other fixed. Notably, PasoDoble operates without supervision during training. Experimental results show that PasoDoble can improve the reasoning performance of LLMs. Our project page is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 60,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11894",
        "abs_url": "https://arxiv.org/abs/2511.11894",
        "pdf_url": "https://arxiv.org/pdf/2511.11894",
        "title": "Chain-of-Generation: Progressive Latent Diffusion for Text-Guided Molecular Design",
        "authors": [
            "Lingxiao Li",
            "Haobo Zhang",
            "Bin Chen",
            "Jiayu Zhou"
        ],
        "comments": "22 pages, 7 figures, 10 tables",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Text-conditioned molecular generation aims to translate natural-language descriptions into chemical structures, enabling scientists to specify functional groups, scaffolds, and physicochemical constraints without handcrafted rules. Diffusion-based models, particularly latent diffusion models (LDMs), have recently shown promise by performing stochastic search in a continuous latent space that compactly captures molecular semantics. Yet existing methods rely on one-shot conditioning, where the entire prompt is encoded once and applied throughout diffusion, making it hard to satisfy all the requirements in the prompt. We discuss three outstanding challenges of one-shot conditioning generation, including the poor interpretability of the generated components, the failure to generate all substructures, and the overambition in considering all requirements simultaneously. We then propose three principles to address those challenges, motivated by which we propose Chain-of-Generation (CoG), a training-free multi-stage latent diffusion framework. CoG decomposes each prompt into curriculum-ordered semantic segments and progressively incorporates them as intermediate goals, guiding the denoising trajectory toward molecules that satisfy increasingly rich linguistic constraints. To reinforce semantic guidance, we further introduce a post-alignment learning phase that strengthens the correspondence between textual and molecular latent spaces. Extensive experiments on benchmark and real-world tasks demonstrate that CoG yields higher semantic alignment, diversity, and controllability than one-shot baselines, producing molecules that more faithfully reflect complex, compositional prompts while offering transparent insight into the generation process.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 61,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11902",
        "abs_url": "https://arxiv.org/abs/2511.11902",
        "pdf_url": "https://arxiv.org/pdf/2511.11902",
        "title": "Robust Bidirectional Associative Memory via Regularization Inspired by the Subspace Rotation Algorithm",
        "authors": [
            "Ci Lin",
            "Tet Yeap",
            "Iluju Kiringa",
            "Biwei Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Bidirectional Associative Memory (BAM) trained with Bidirectional Backpropagation (B-BP) often suffers from poor robustness and high sensitivity to noise and adversarial attacks. To address these issues, we propose a novel gradient-free training algorithm, the Bidirectional Subspace Rotation Algorithm (B-SRA), which significantly improves the robustness and convergence behavior of BAM. Through comprehensive experiments, we identify two key principles -- orthogonal weight matrices (OWM) and gradient-pattern alignment (GPA) -- as central to enhancing the robustness of BAM. Motivated by these findings, we introduce new regularization strategies into B-BP, resulting in models with greatly improved resistance to corruption and adversarial perturbations. We further conduct an ablation study across different training strategies to determine the most robust configuration and evaluate BAM's performance under a variety of attack scenarios and memory capacities, including 50, 100, and 200 associative pairs. Among all methods, the SAME configuration, which integrates both OWM and GPA, achieves the strongest resilience. Overall, our results demonstrate that B-SRA and the proposed regularization strategies lead to substantially more robust associative memories and open new directions for building resilient neural architectures.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 62,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11912",
        "abs_url": "https://arxiv.org/abs/2511.11912",
        "pdf_url": "https://arxiv.org/pdf/2511.11912",
        "title": "A Systematic Study of Model Extraction Attacks on Graph Foundation Models",
        "authors": [
            "Haoyan Xu",
            "Ruizhi Qian",
            "Jiate Li",
            "Yushun Dong",
            "Minghao Lin",
            "Hanson Yan",
            "Zhengtao Yao",
            "Qinghua Liu",
            "Junhao Dong",
            "Ruopeng Huang",
            "Yue Zhao",
            "Mengyuan Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Cryptography and Security (cs.CR)",
        "abstract": "Graph machine learning has advanced rapidly in tasks such as link prediction, anomaly detection, and node classification. As models scale up, pretrained graph models have become valuable intellectual assets because they encode extensive computation and domain expertise. Building on these advances, Graph Foundation Models (GFMs) mark a major step forward by jointly pretraining graph and text encoders on massive and diverse data. This unifies structural and semantic understanding, enables zero-shot inference, and supports applications such as fraud detection and biomedical analysis. However, the high pretraining cost and broad cross-domain knowledge in GFMs also make them attractive targets for model extraction attacks (MEAs). Prior work has focused only on small graph neural networks trained on a single graph, leaving the security implications for large-scale and multimodal GFMs largely unexplored. This paper presents the first systematic study of MEAs against GFMs. We formalize a black-box threat model and define six practical attack scenarios covering domain-level and graph-specific extraction goals, architectural mismatch, limited query budgets, partial node access, and training data discrepancies. To instantiate these attacks, we introduce a lightweight extraction method that trains an attacker encoder using supervised regression of graph embeddings. Even without contrastive pretraining data, this method learns an encoder that stays aligned with the victim text encoder and preserves its zero-shot inference ability on unseen graphs. Experiments on seven datasets show that the attacker can approximate the victim model using only a tiny fraction of its original training cost, with almost no loss in accuracy. These findings reveal that GFMs greatly expand the MEA surface and highlight the need for deployment-aware security defenses in large-scale graph learning systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 63,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11918",
        "abs_url": "https://arxiv.org/abs/2511.11918",
        "pdf_url": "https://arxiv.org/pdf/2511.11918",
        "title": "Batch Matrix-form Equations and Implementation of Multilayer Perceptrons",
        "authors": [
            "Wieger Wesselink",
            "Bram Grooten",
            "Huub van de Wetering",
            "Qiao Xiao",
            "Decebal Constantin Mocanu"
        ],
        "comments": "32 pages; submitted to JMLR",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Multilayer perceptrons (MLPs) remain fundamental to modern deep learning, yet their algorithmic details are rarely presented in complete, explicit \\emph{batch matrix-form}. Rather, most references express gradients per sample or rely on automatic differentiation. Although automatic differentiation can achieve equally high computational efficiency, the usage of batch matrix-form makes the computational structure explicit, which is essential for transparent, systematic analysis, and optimization in settings such as sparse neural networks. This paper fills that gap by providing a mathematically rigorous and implementation-ready specification of MLPs in batch matrix-form. We derive forward and backward equations for all standard and advanced layers, including batch normalization and softmax, and validate all equations using the symbolic mathematics library SymPy. From these specifications, we construct uniform reference implementations in NumPy, PyTorch, JAX, TensorFlow, and a high-performance C++ backend optimized for sparse operations. Our main contributions are: (1) a complete derivation of batch matrix-form backpropagation for MLPs, (2) symbolic validation of all gradient equations, (3) uniform Python and C++ reference implementations grounded in a small set of matrix primitives, and (4) demonstration of how explicit formulations enable efficient sparse computation. Together, these results establish a validated, extensible foundation for understanding, teaching, and researching neural network algorithms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 64,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11928",
        "abs_url": "https://arxiv.org/abs/2511.11928",
        "pdf_url": "https://arxiv.org/pdf/2511.11928",
        "title": "Beyond the Laplacian: Interpolated Spectral Augmentation for Graph Neural Networks",
        "authors": [
            "Ziyao Cui",
            "Edric Tam"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Graph neural networks (GNNs) are fundamental tools in graph machine learning. The performance of GNNs relies crucially on the availability of informative node features, which can be limited or absent in real-life datasets and applications. A natural remedy is to augment the node features with embeddings computed from eigenvectors of the graph Laplacian matrix. While it is natural to default to Laplacian spectral embeddings, which capture meaningful graph connectivity information, we ask whether spectral embeddings from alternative graph matrices can also provide useful representations for learning. We introduce Interpolated Laplacian Embeddings (ILEs), which are derived from a simple yet expressive family of graph matrices. Using tools from spectral graph theory, we offer a straightforward interpretation of the structural information that ILEs capture. We demonstrate through simulations and experiments on real-world datasets that feature augmentation via ILEs can improve performance across commonly used GNN architectures. Our work offers a straightforward and practical approach that broadens the practitioner's spectral augmentation toolkit when node features are limited.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 65,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11935",
        "abs_url": "https://arxiv.org/abs/2511.11935",
        "pdf_url": "https://arxiv.org/pdf/2511.11935",
        "title": "SurvBench: A Standardised Preprocessing Pipeline for Multi-Modal Electronic Health Record Survival Analysis",
        "authors": [
            "Munib Mesinovic",
            "Tingting Zhu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Electronic health record (EHR) data present tremendous opportunities for advancing survival analysis through deep learning, yet reproducibility remains severely constrained by inconsistent preprocessing methodologies. We present SurvBench, a comprehensive, open-source preprocessing pipeline that transforms raw PhysioNet datasets into standardised, model-ready tensors for multi-modal survival analysis. SurvBench provides data loaders for three major critical care databases, MIMIC-IV, eICU, and MC-MED, supporting diverse modalities including time-series vitals, static demographics, ICD diagnosis codes, and radiology reports. The pipeline implements rigorous data quality controls, patient-level splitting to prevent data leakage, explicit missingness tracking, and standardised temporal aggregation. SurvBench handles both single-risk (e.g., in-hospital mortality) and competing-risks scenarios (e.g., multiple discharge outcomes). The outputs are compatible with pycox library packages and implementations of standard statistical and deep learning models. By providing reproducible, configuration-driven preprocessing with comprehensive documentation, SurvBench addresses the \"preprocessing gap\" that has hindered fair comparison of deep learning survival models, enabling researchers to focus on methodological innovation rather than data engineering.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 66,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11940",
        "abs_url": "https://arxiv.org/abs/2511.11940",
        "pdf_url": "https://arxiv.org/pdf/2511.11940",
        "title": "Learning the relative composition of EEG signals using pairwise relative shift pretraining",
        "authors": [
            "Christopher Sandino",
            "Sayeri Lala",
            "Geeling Chau",
            "Melika Ayoughi",
            "Behrooz Mahasseni",
            "Ellen Zippi",
            "Ali Moin",
            "Erdrin Azemi",
            "Hanlin Goh"
        ],
        "comments": "Foundation Models for the Brain and Body NeurIPS 2025 Workshop",
        "subjects": "Machine Learning (cs.LG); Signal Processing (eess.SP)",
        "abstract": "Self-supervised learning (SSL) offers a promising approach for learning electroencephalography (EEG) representations from unlabeled data, reducing the need for expensive annotations for clinical applications like sleep staging and seizure detection. While current EEG SSL methods predominantly use masked reconstruction strategies like masked autoencoders (MAE) that capture local temporal patterns, position prediction pretraining remains underexplored despite its potential to learn long-range dependencies in neural signals. We introduce PAirwise Relative Shift or PARS pretraining, a novel pretext task that predicts relative temporal shifts between randomly sampled EEG window pairs. Unlike reconstruction-based methods that focus on local pattern recovery, PARS encourages encoders to capture relative temporal composition and long-range dependencies inherent in neural signals. Through comprehensive evaluation on various EEG decoding tasks, we demonstrate that PARS-pretrained transformers consistently outperform existing pretraining strategies in label-efficient and transfer learning settings, establishing a new paradigm for self-supervised EEG representation learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 67,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11949",
        "abs_url": "https://arxiv.org/abs/2511.11949",
        "pdf_url": "https://arxiv.org/pdf/2511.11949",
        "title": "Computation-aware Energy-harvesting Federated Learning: Cyclic Scheduling with Selective Participation",
        "authors": [
            "Eunjeong Jeong",
            "Nikolaos Pappas"
        ],
        "comments": "This paper has been submitted to a peer-reviewed journal",
        "subjects": "Machine Learning (cs.LG); Information Theory (cs.IT)",
        "abstract": "Federated Learning (FL) is a powerful paradigm for distributed learning, but its increasing complexity leads to significant energy consumption from client-side computations for training models. In particular, the challenge is critical in energy-harvesting FL (EHFL) systems where participation availability of each device oscillates due to limited energy. To address this, we propose FedBacys, a battery-aware EHFL framework using cyclic client participation based on users' battery levels. By clustering clients and scheduling them sequentially, FedBacys minimizes redundant computations, reduces system-wide energy usage, and improves learning stability. We also introduce FedBacys-Odd, a more energy-efficient variant that allows clients to participate selectively, further reducing energy costs without compromising performance. We provide a convergence analysis for our framework and demonstrate its superior energy efficiency and robustness compared to existing algorithms through numerical experiments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 68,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11973",
        "abs_url": "https://arxiv.org/abs/2511.11973",
        "pdf_url": "https://arxiv.org/pdf/2511.11973",
        "title": "Quantile Q-Learning: Revisiting Offline Extreme Q-Learning with Quantile Regression",
        "authors": [
            "Xinming Gao",
            "Shangzhe Li",
            "Yujin Cai",
            "Wenwu Yu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Offline reinforcement learning (RL) enables policy learning from fixed datasets without further environment interaction, making it particularly valuable in high-risk or costly domains. Extreme $Q$-Learning (XQL) is a recent offline RL method that models Bellman errors using the Extreme Value Theorem, yielding strong empirical performance. However, XQL and its stabilized variant MXQL suffer from notable limitations: both require extensive hyperparameter tuning specific to each dataset and domain, and also exhibit instability during training. To address these issues, we proposed a principled method to estimate the temperature coefficient $\\beta$ via quantile regression under mild assumptions. To further improve training stability, we introduce a value regularization technique with mild generalization, inspired by recent advances in constrained value learning. Experimental results demonstrate that the proposed algorithm achieves competitive or superior performance across a range of benchmark tasks, including D4RL and NeoRL2, while maintaining stable training dynamics and using a consistent set of hyperparameters across all datasets and domains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 69,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11991",
        "abs_url": "https://arxiv.org/abs/2511.11991",
        "pdf_url": "https://arxiv.org/pdf/2511.11991",
        "title": "ReCast: Reliability-aware Codebook Assisted Lightweight Time Series Forecasting",
        "authors": [
            "Xiang Ma",
            "Taihua Chen",
            "Pengcheng Wang",
            "Xuemei Li",
            "Caiming Zhang"
        ],
        "comments": "AAAI 2026 Oral",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Time series forecasting is crucial for applications in various domains. Conventional methods often rely on global decomposition into trend, seasonal, and residual components, which become ineffective for real-world series dominated by local, complex, and highly dynamic patterns. Moreover, the high model complexity of such approaches limits their applicability in real-time or resource-constrained environments. In this work, we propose a novel \\textbf{RE}liability-aware \\textbf{C}odebook-\\textbf{AS}sisted \\textbf{T}ime series forecasting framework (\\textbf{ReCast}) that enables lightweight and robust prediction by exploiting recurring local shapes. ReCast encodes local patterns into discrete embeddings through patch-wise quantization using a learnable codebook, thereby compactly capturing stable regular structures. To compensate for residual variations not preserved by quantization, ReCast employs a dual-path architecture comprising a quantization path for efficient modeling of regular structures and a residual path for reconstructing irregular fluctuations. A central contribution of ReCast is a reliability-aware codebook update strategy, which incrementally refines the codebook via weighted corrections. These correction weights are derived by fusing multiple reliability factors from complementary perspectives by a distributionally robust optimization (DRO) scheme, ensuring adaptability to non-stationarity and robustness to distribution shifts. Extensive experiments demonstrate that ReCast outperforms state-of-the-art (SOTA) models in accuracy, efficiency, and adaptability to distribution shifts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 70,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12033",
        "abs_url": "https://arxiv.org/abs/2511.12033",
        "pdf_url": "https://arxiv.org/pdf/2511.12033",
        "title": "EARL: Entropy-Aware RL Alignment of LLMs for Reliable RTL Code Generation",
        "authors": [
            "Jiahe Shi",
            "Zhengqi Gao",
            "Ching-Yun Ko",
            "Duane Boning"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Recent advances in large language models (LLMs) have demonstrated significant potential in hardware design automation, particularly in using natural language to synthesize Register-Transfer Level (RTL) code. Despite this progress, a gap remains between model capability and the demands of real-world RTL design, including syntax errors, functional hallucinations, and weak alignment to designer intent. Reinforcement Learning with Verifiable Rewards (RLVR) offers a promising approach to bridge this gap, as hardware provides executable and formally checkable signals that can be used to further align model outputs with design intent. However, in long, structured RTL code sequences, not all tokens contribute equally to functional correctness, and naïvely spreading gradients across all tokens dilutes learning signals. A key insight from our entropy analysis in RTL generation is that only a small fraction of tokens (e.g., always, if, assign, posedge) exhibit high uncertainty and largely influence control flow and module structure. To address these challenges, we present EARL, an Entropy-Aware Reinforcement Learning framework for Verilog generation. EARL performs policy optimization using verifiable reward signals and introduces entropy-guided selective updates that gate policy gradients to high-entropy tokens. This approach preserves training stability and concentrates gradient updates on functionally important regions of code. Our experiments on VerilogEval and RTLLM show that EARL improves functional pass rates over prior LLM baselines by up to 14.7%, while reducing unnecessary updates and improving training stability. These results indicate that focusing RL on critical, high-uncertainty tokens enables more reliable and targeted policy improvement for structured RTL code generation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 71,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12071",
        "abs_url": "https://arxiv.org/abs/2511.12071",
        "pdf_url": "https://arxiv.org/pdf/2511.12071",
        "title": "Improving Graph Embeddings in Machine Learning Using Knowledge Completion with Validation in a Case Study on COVID-19 Spread",
        "authors": [
            "Rosario Napoli",
            "Gabriele Morabito",
            "Antonio Celesti",
            "Massimo Villari",
            "Maria Fazio"
        ],
        "comments": "Accepted at the 16th IEEE International Conference on Knowledge Graphs (ICKG) 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "The rise of graph-structured data has driven major advances in Graph Machine Learning (GML), where graph embeddings (GEs) map features from Knowledge Graphs (KGs) into vector spaces, enabling tasks like node classification and link prediction. However, since GEs are derived from explicit topology and features, they may miss crucial implicit knowledge hidden in seemingly sparse datasets, affecting graph structure and their representation. We propose a GML pipeline that integrates a Knowledge Completion (KC) phase to uncover latent dataset semantics before embedding generation. Focusing on transitive relations, we model hidden connections with decay-based inference functions, reshaping graph topology, with consequences on embedding dynamics and aggregation processes in GraphSAGE and Node2Vec. Experiments show that our GML pipeline significantly alters the embedding space geometry, demonstrating that its introduction is not just a simple enrichment but a transformative step that redefines graph representation quality.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 72,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12092",
        "abs_url": "https://arxiv.org/abs/2511.12092",
        "pdf_url": "https://arxiv.org/pdf/2511.12092",
        "title": "SenseRay-3D: Generalizable and Physics-Informed Framework for End-to-End Indoor Propagation Modeling",
        "authors": [
            "Yu Zheng",
            "Kezhi Wang",
            "Wenji Xi",
            "Gang Yu",
            "Jiming Chen",
            "Jie Zhang"
        ],
        "comments": "Submitted for possible journal publications",
        "subjects": "Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)",
        "abstract": "Modeling indoor radio propagation is crucial for wireless network planning and optimization. However, existing approaches often rely on labor-intensive manual modeling of geometry and material properties, resulting in limited scalability and efficiency. To overcome these challenges, this paper presents SenseRay-3D, a generalizable and physics-informed end-to-end framework that predicts three-dimensional (3D) path-loss heatmaps directly from RGB-D scans, thereby eliminating the need for explicit geometry reconstruction or material annotation. The proposed framework builds a sensing-driven voxelized scene representation that jointly encodes occupancy, electromagnetic material characteristics, and transmitter-receiver geometry, which is processed by a SwinUNETR-based neural network to infer environmental path-loss relative to free-space path-loss. A comprehensive synthetic indoor propagation dataset is further developed to validate the framework and to serve as a standardized benchmark for future research. Experimental results show that SenseRay-3D achieves a mean absolute error of 4.27 dB on unseen environments and supports real-time inference at 217 ms per sample, demonstrating its scalability, efficiency, and physical consistency. SenseRay-3D paves a new path for sense-driven, generalizable, and physics-consistent modeling of indoor propagation, marking a major leap beyond our pioneering EM DeepRay framework.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 73,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12121",
        "abs_url": "https://arxiv.org/abs/2511.12121",
        "pdf_url": "https://arxiv.org/pdf/2511.12121",
        "title": "To Align or Not to Align: Strategic Multimodal Representation Alignment for Optimal Performance",
        "authors": [
            "Wanlong Fang",
            "Tianle Zhang",
            "Alvin Chan"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Multimedia (cs.MM)",
        "abstract": "Multimodal learning often relies on aligning representations across modalities to enable effective information integration, an approach traditionally assumed to be universally beneficial. However, prior research has primarily taken an observational approach, examining naturally occurring alignment in multimodal data and exploring its correlation with model performance, without systematically studying the direct effects of explicitly enforced alignment between representations of different modalities. In this work, we investigate how explicit alignment influences both model performance and representation alignment under different modality-specific information structures. Specifically, we introduce a controllable contrastive learning module that enables precise manipulation of alignment strength during training, allowing us to explore when explicit alignment improves or hinders performance. Our results on synthetic and real datasets under different data characteristics show that the impact of explicit alignment on the performance of unimodal models is related to the characteristics of the data: the optimal level of alignment depends on the amount of redundancy between the different modalities. We identify an optimal alignment strength that balances modality-specific signals and shared redundancy in the mixed information distributions. This work provides practical guidance on when and how explicit alignment should be applied to achieve optimal unimodal encoder performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 74,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12122",
        "abs_url": "https://arxiv.org/abs/2511.12122",
        "pdf_url": "https://arxiv.org/pdf/2511.12122",
        "title": "Dynamic Anomaly Identification in Accounting Transactions via Multi-Head Self-Attention Networks",
        "authors": [
            "Yi Wang",
            "Ruoyi Fang",
            "Anzhuo Xie",
            "Hanrui Feng",
            "Jianlin Lai"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "This study addresses the problem of dynamic anomaly detection in accounting transactions and proposes a real-time detection method based on a Transformer to tackle the challenges of hidden abnormal behaviors and high timeliness requirements in complex trading environments. The approach first models accounting transaction data by representing multi-dimensional records as time-series matrices and uses embedding layers and positional encoding to achieve low-dimensional mapping of inputs. A sequence modeling structure with multi-head self-attention is then constructed to capture global dependencies and aggregate features from multiple perspectives, thereby enhancing the ability to detect abnormal patterns. The network further integrates feed-forward layers and regularization strategies to achieve deep feature representation and accurate anomaly probability estimation. To validate the effectiveness of the method, extensive experiments were conducted on a public dataset, including comparative analysis, hyperparameter sensitivity tests, environmental sensitivity tests, and data sensitivity tests. Results show that the proposed method outperforms baseline models in AUC, F1-Score, Precision, and Recall, and maintains stable performance under different environmental conditions and data perturbations. These findings confirm the applicability and advantages of the Transformer-based framework for dynamic anomaly detection in accounting transactions and provide methodological support for intelligent financial risk control and auditing.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 75,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12123",
        "abs_url": "https://arxiv.org/abs/2511.12123",
        "pdf_url": "https://arxiv.org/pdf/2511.12123",
        "title": "HCPO: Hierarchical Conductor-Based Policy Optimization in Multi-Agent Reinforcement Learning",
        "authors": [
            "Zejiao Liu",
            "Junqi Tu",
            "Yitian Hong",
            "Luolin Xiong",
            "Yaochu Jin",
            "Yang Tang",
            "Fangfei Li"
        ],
        "comments": "AAAI 2026",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In cooperative Multi-Agent Reinforcement Learning (MARL), efficient exploration is crucial for optimizing the performance of joint policy. However, existing methods often update joint policies via independent agent exploration, without coordination among agents, which inherently constrains the expressive capacity and exploration of joint policies. To address this issue, we propose a conductor-based joint policy framework that directly enhances the expressive capacity of joint policies and coordinates exploration. In addition, we develop a Hierarchical Conductor-based Policy Optimization (HCPO) algorithm that instructs policy updates for the conductor and agents in a direction aligned with performance improvement. A rigorous theoretical guarantee further establishes the monotonicity of the joint policy optimization process. By deploying local conductors, HCPO retains centralized training benefits while eliminating inter-agent communication during execution. Finally, we evaluate HCPO on three challenging benchmarks: StarCraftII Multi-agent Challenge, Multi-agent MuJoCo, and Multi-agent Particle Environment. The results indicate that HCPO outperforms competitive MARL baselines regarding cooperative efficiency and stability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 76,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12132",
        "abs_url": "https://arxiv.org/abs/2511.12132",
        "pdf_url": "https://arxiv.org/pdf/2511.12132",
        "title": "FairGSE: Fairness-Aware Graph Neural Network without High False Positive Rates",
        "authors": [
            "Zhenqiang Ye",
            "Jinjie Lu",
            "Tianlong Gu",
            "Fengrui Hao",
            "Xuemin Wang"
        ],
        "comments": "AAAI 2026",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Graph neural networks (GNNs) have emerged as the mainstream paradigm for graph representation learning due to their effective message aggregation. However, this advantage also amplifies biases inherent in graph topology, raising fairness concerns. Existing fairness-aware GNNs provide satisfactory performance on fairness metrics such as Statistical Parity and Equal Opportunity while maintaining acceptable accuracy trade-offs. Unfortunately, we observe that this pursuit of fairness metrics neglects the GNN's ability to predict negative labels, which renders their predictions with extremely high False Positive Rates (FPR), resulting in negative effects in high-risk scenarios. To this end, we advocate that classification performance should be carefully calibrated while improving fairness, rather than simply constraining accuracy loss. Furthermore, we propose Fair GNN via Structural Entropy (\\textbf{FairGSE}), a novel framework that maximizes two-dimensional structural entropy (2D-SE) to improve fairness without neglecting false positives. Experiments on several real-world datasets show FairGSE reduces FPR by 39\\% vs. state-of-the-art fairness-aware GNNs, with comparable fairness improvement.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 77,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12139",
        "abs_url": "https://arxiv.org/abs/2511.12139",
        "pdf_url": "https://arxiv.org/pdf/2511.12139",
        "title": "Fusion-ResNet: A Lightweight multi-label NILM Model Using PCA-ICA Feature Fusion",
        "authors": [
            "Sahar Moghimian Hoosh",
            "Ilia Kamyshev",
            "Henni Ouerdane"
        ],
        "comments": "Extended version of the conference paper \"Enhancing Non-Intrusive Load Monitoring with Features Extracted by Independent Component Analysis\" -- arXiv:2501.16817. Instead of solely using ICA or PCA for feature extraction, we propose the fusion of ICA and PCA, which outperforms other baseline models. This extended version is meant for journal publication",
        "subjects": "Machine Learning (cs.LG); Systems and Control (eess.SY)",
        "abstract": "Non-intrusive load monitoring (NILM) is an advanced load monitoring technique that uses data-driven algorithms to disaggregate the total power consumption of a household into the consumption of individual appliances. However, real-world NILM deployment still faces major challenges, including overfitting, low model generalization, and disaggregating a large number of appliances operating at the same time. To address these challenges, this work proposes an end-to-end framework for the NILM classification task, which consists of high-frequency labeled data, a feature extraction method, and a lightweight neural network. Within this framework, we introduce a novel feature extraction method that fuses Independent Component Analysis (ICA) and Principal Component Analysis (PCA) features. Moreover, we propose a lightweight architecture for multi-label NILM classification (Fusion-ResNet). The proposed feature-based model achieves a higher $F1$ score on average and across different appliances compared to state-of-the-art NILM classifiers while minimizing the training and inference time. Finally, we assessed the performance of our model against baselines with a varying number of simultaneously active devices. Results demonstrate that Fusion-ResNet is relatively robust to stress conditions with up to 15 concurrently active appliances.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 78,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12147",
        "abs_url": "https://arxiv.org/abs/2511.12147",
        "pdf_url": "https://arxiv.org/pdf/2511.12147",
        "title": "Finding Time Series Anomalies using Granular-ball Vector Data Description",
        "authors": [
            "Lifeng Shen",
            "Liang Peng",
            "Ruiwen Liu",
            "Shuyin Xia",
            "Yi Liu"
        ],
        "comments": "Accepted by AAAI 2026",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Modeling normal behavior in dynamic, nonlinear time series data is challenging for effective anomaly detection. Traditional methods, such as nearest neighbor and clustering approaches, often depend on rigid assumptions, such as a predefined number of reliable neighbors or clusters, which frequently break down in complex temporal scenarios. To address these limitations, we introduce the Granular-ball One-Class Network (GBOC), a novel approach based on a data-adaptive representation called Granular-ball Vector Data Description (GVDD). GVDD partitions the latent space into compact, high-density regions represented by granular-balls, which are generated through a density-guided hierarchical splitting process and refined by removing noisy structures. Each granular-ball serves as a prototype for local normal behavior, naturally positioning itself between individual instances and clusters while preserving the local topological structure of the sample set. During training, GBOC improves the compactness of representations by aligning samples with their nearest granular-ball centers. During inference, anomaly scores are computed based on the distance to the nearest granular-ball. By focusing on dense, high-quality regions and significantly reducing the number of prototypes, GBOC delivers both robustness and efficiency in anomaly detection. Extensive experiments validate the effectiveness and superiority of the proposed method, highlighting its ability to handle the challenges of time series anomaly detection.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 79,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12155",
        "abs_url": "https://arxiv.org/abs/2511.12155",
        "pdf_url": "https://arxiv.org/pdf/2511.12155",
        "title": "Rethinking Deep Alignment Through The Lens Of Incomplete Learning",
        "authors": [
            "Thong Bach",
            "Dung Nguyen",
            "Thao Minh Le",
            "Truyen Tran"
        ],
        "comments": "AAAI'26",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Large language models exhibit systematic vulnerabilities to adversarial attacks despite extensive safety alignment. We provide a mechanistic analysis revealing that position-dependent gradient weakening during autoregressive training creates signal decay, leading to incomplete safety learning where safety training fails to transform model preferences in later response regions fully. We introduce base-favored tokens -- vocabulary elements where base models assign higher probability than aligned models -- as computational indicators of incomplete safety learning and develop a targeted completion method that addresses undertrained regions through adaptive penalties and hybrid teacher distillation. Experimental evaluation across Llama and Qwen model families demonstrates dramatic improvements in adversarial robustness, with 48--98% reductions in attack success rates while preserving general capabilities. These results establish both a mechanistic understanding and practical solutions for fundamental limitations in safety alignment methodologies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 80,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12158",
        "abs_url": "https://arxiv.org/abs/2511.12158",
        "pdf_url": "https://arxiv.org/pdf/2511.12158",
        "title": "Data-Efficient Self-Supervised Algorithms for Fine-Grained Birdsong Analysis",
        "authors": [
            "Houtan Ghaffari",
            "Lukas Rauch",
            "Paul Devos"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Many bioacoustics, neuroscience, and linguistics research utilize birdsongs as proxy models to acquire knowledge in diverse areas. Developing models generally requires precisely annotated data at the level of syllables. Hence, automated and data-efficient methods that reduce annotation costs are in demand. This work presents a lightweight, yet performant neural network architecture for birdsong annotation called Residual-MLP-RNN. Then, it presents a robust three-stage training pipeline for developing reliable deep birdsong syllable detectors with minimal expert labor. The first stage is self-supervised learning from unlabeled data. Two of the most successful pretraining paradigms are explored, namely, masked prediction and online clustering. The second stage is supervised training with effective data augmentations to create a robust model for frame-level syllable detection. The third stage is semi-supervised post-training, which leverages the unlabeled data again. However, unlike the initial phase, this time it is aligned with the downstream task. The performance of this data-efficient approach is demonstrated for the complex song of the Canary in extreme label-scarcity scenarios. Canary has one of the most difficult songs to annotate, which implicitly validates the method for other birds. Finally, the potential of self-supervised embeddings is assessed for linear probing and unsupervised birdsong analysis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 81,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12171",
        "abs_url": "https://arxiv.org/abs/2511.12171",
        "pdf_url": "https://arxiv.org/pdf/2511.12171",
        "title": "FGM optimization in complex domains using Gaussian process regression based profile generation algorithm",
        "authors": [
            "Chaitanya Kumar Konda",
            "Piyush Agrawal",
            "Shivansh Srivastava",
            "Manish Agrawal"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computational Physics (physics.comp-ph)",
        "abstract": "This manuscript addresses the challenge of designing functionally graded materials (FGMs) for arbitrary-shaped domains. Towards this goal, the present work proposes a generic volume fraction profile generation algorithm based on Gaussian Process Regression (GPR). The proposed algorithm can handle complex-shaped domains and generate smooth FGM profiles while adhering to the specified volume fraction values at boundaries/part of boundaries. The resulting design space from GPR comprises diverse profiles, enhancing the potential for discovering optimal configurations. Further, the algorithm allows the user to control the smoothness of the underlying profiles and the size of the design space through a length scale parameter. Further, the proposed profile generation scheme is coupled with the genetic algorithm to find the optimum FGM profiles for a given application. To make the genetic algorithm consistent with the GPR profile generation scheme, the standard simulated binary crossover operator in the genetic algorithm has been modified with a projection operator. We present numerous thermoelastic optimization examples to demonstrate the efficacy of the proposed profile generation algorithm and optimization framework.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 82,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12174",
        "abs_url": "https://arxiv.org/abs/2511.12174",
        "pdf_url": "https://arxiv.org/pdf/2511.12174",
        "title": "TSGDiff: Rethinking Synthetic Time Series Generation from a Pure Graph Perspective",
        "authors": [
            "Lifeng Shen",
            "Xuyang Li",
            "Lele Long"
        ],
        "comments": "Accepted by AAAI 2026",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Diffusion models have shown great promise in data generation, yet generating time series data remains challenging due to the need to capture complex temporal dependencies and structural patterns. In this paper, we present \\textit{TSGDiff}, a novel framework that rethinks time series generation from a graph-based perspective. Specifically, we represent time series as dynamic graphs, where edges are constructed based on Fourier spectrum characteristics and temporal dependencies. A graph neural network-based encoder-decoder architecture is employed to construct a latent space, enabling the diffusion process to model the structural representation distribution of time series effectively. Furthermore, we propose the Topological Structure Fidelity (Topo-FID) score, a graph-aware metric for assessing the structural similarity of time series graph representations. Topo-FID integrates two sub-metrics: Graph Edit Similarity, which quantifies differences in adjacency matrices, and Structural Entropy Similarity, which evaluates the entropy of node degree distributions. This comprehensive metric provides a more accurate assessment of structural fidelity in generated time series. Experiments on real-world datasets demonstrate that \\textit{TSGDiff} generates high-quality synthetic time series data generation, faithfully preserving temporal dependencies and structural integrity, thereby advancing the field of synthetic time series generation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 83,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12180",
        "abs_url": "https://arxiv.org/abs/2511.12180",
        "pdf_url": "https://arxiv.org/pdf/2511.12180",
        "title": "Understanding InfoNCE: Transition Probability Matrix Induced Feature Clustering",
        "authors": [
            "Ge Cheng",
            "Shuo Wang",
            "Yun Zhang"
        ],
        "comments": "31 pages, 8 figures",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Contrastive learning has emerged as a cornerstone of unsupervised representation learning across vision, language, and graph domains, with InfoNCE as its dominant objective. Despite its empirical success, the theoretical underpinnings of InfoNCE remain limited. In this work, we introduce an explicit feature space to model augmented views of samples and a transition probability matrix to capture data augmentation dynamics. We demonstrate that InfoNCE optimizes the probability of two views sharing the same source toward a constant target defined by this matrix, naturally inducing feature clustering in the representation space. Leveraging this insight, we propose Scaled Convergence InfoNCE (SC-InfoNCE), a novel loss function that introduces a tunable convergence target to flexibly control feature similarity alignment. By scaling the target matrix, SC-InfoNCE enables flexible control over feature similarity alignment, allowing the training objective to better match the statistical properties of downstream data. Experiments on benchmark datasets, including image, graph, and text tasks, show that SC-InfoNCE consistently achieves strong and reliable performance across diverse domains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 84,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12188",
        "abs_url": "https://arxiv.org/abs/2511.12188",
        "pdf_url": "https://arxiv.org/pdf/2511.12188",
        "title": "Scaling Law Analysis in Federated Learning: How to Select the Optimal Model Size?",
        "authors": [
            "Xuanyu Chen",
            "Nan Yang",
            "Shuai Wang",
            "Dong Yuan"
        ],
        "comments": "The extended version of the paper \"Scaling Law Analysis in Federated Learning: How to Select the Optimal Model Size?\". Accepted by AAAI2026",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The recent success of large language models (LLMs) has sparked a growing interest in training large-scale models. As the model size continues to scale, concerns are growing about the depletion of high-quality, well-curated training data. This has led practitioners to explore training approaches like Federated Learning (FL), which can leverage the abundant data on edge devices while maintaining privacy. However, the decentralization of training datasets in FL introduces challenges to scaling large models, a topic that remains under-explored. This paper fills this gap and provides qualitative insights on generalizing the previous model scaling experience to federated learning scenarios. Specifically, we derive a PAC-Bayes (Probably Approximately Correct Bayesian) upper bound for the generalization error of models trained with stochastic algorithms in federated settings and quantify the impact of distributed training data on the optimal model size by finding the analytic solution of model size that minimizes this bound. Our theoretical results demonstrate that the optimal model size has a negative power law relationship with the number of clients if the total training compute is unchanged. Besides, we also find that switching to FL with the same training compute will inevitably reduce the upper bound of generalization performance that the model can achieve through training, and that estimating the optimal model size in federated scenarios should depend on the average training compute across clients. Furthermore, we also empirically validate the correctness of our results with extensive training runs on different models, network settings, and datasets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 85,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12191",
        "abs_url": "https://arxiv.org/abs/2511.12191",
        "pdf_url": "https://arxiv.org/pdf/2511.12191",
        "title": "Evaluation of Multi- and Single-objective Learning Algorithms for Imbalanced Data",
        "authors": [
            "Szymon Wojciechowski",
            "Michał Woźniak"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Many machine learning tasks aim to find models that work well not for a single, but for a group of criteria, often opposing ones. One such example is imbalanced data classification, where, on the one hand, we want to achieve the best possible classification quality for data from the minority class without degrading the classification quality of the majority class. One solution is to propose an aggregate learning criterion and reduce the multi-objective learning task to a single-criteria optimization problem. Unfortunately, such an approach is characterized by ambiguity of interpretation since the value of the aggregated criterion does not indicate the value of the component criteria. Hence, there are more and more proposals for algorithms based on multi-objective optimization (MOO), which can simultaneously optimize multiple criteria. However, such an approach results in a set of multiple non-dominated solutions (Pareto front). The selection of a single solution from the Pareto front is a challenge itself, and much attention is paid to the issue of how to select it considering user preferences, as well as how to compare solutions returned by different MOO algorithms among themselves. Thus, a significant gap has been identified in the classifier evaluation methodology, i.e., how to reliably compare methods returning single solutions with algorithms returning solutions in the form of Pareto fronts. To fill the aforementioned gap, this article proposes a new, reliable way of evaluating algorithms based on multi-objective algorithms with methods that return single solutions while pointing out solutions from a Pareto front tailored to the user's preferences. This work focuses only on algorithm comparison, not their learning. The algorithms selected for this study are illustrative to help understand the proposed approach.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 86,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12199",
        "abs_url": "https://arxiv.org/abs/2511.12199",
        "pdf_url": "https://arxiv.org/pdf/2511.12199",
        "title": "MPD-SGR: Robust Spiking Neural Networks with Membrane Potential Distribution-Driven Surrogate Gradient Regularization",
        "authors": [
            "Runhao Jiang",
            "Chengzhi Jiang",
            "Rui Yan",
            "Huajin Tang"
        ],
        "comments": "Accepted by AAAI 2026",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The surrogate gradient (SG) method has shown significant promise in enhancing the performance of deep spiking neural networks (SNNs), but it also introduces vulnerabilities to adversarial attacks. Although spike coding strategies and neural dynamics parameters have been extensively studied for their impact on robustness, the critical role of gradient magnitude, which reflects the model's sensitivity to input perturbations, remains underexplored. In SNNs, the gradient magnitude is primarily determined by the interaction between the membrane potential distribution (MPD) and the SG function. In this study, we investigate the relationship between the MPD and SG and its implications for improving the robustness of SNNs. Our theoretical analysis reveals that reducing the proportion of membrane potential lying within the gradient-available range of the SG function effectively mitigates the sensitivity of SNNs to input perturbations. Building upon this insight, we propose a novel MPD-driven surrogate gradient regularization (MPD-SGR) method, which enhances robustness by explicitly regularizing the MPD based on its interaction with the SG function. Extensive experiments across multiple image classification benchmarks and diverse network architectures confirm that the MPD-SGR method significantly enhances the resilience of SNNs to adversarial perturbations and exhibits strong generalizability across diverse network configurations, SG function variants, and spike encoding schemes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 87,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12217",
        "abs_url": "https://arxiv.org/abs/2511.12217",
        "pdf_url": "https://arxiv.org/pdf/2511.12217",
        "title": "AlignTree: Efficient Defense Against LLM Jailbreak Attacks",
        "authors": [
            "Gil Goren",
            "Shahar Katz",
            "Lior Wolf"
        ],
        "comments": "Accepted as an Oral Presentation at the 40th AAAI Conference on Artificial Intelligence (AAAI-26), January 2026",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Large Language Models (LLMs) are vulnerable to adversarial attacks that bypass safety guidelines and generate harmful content. Mitigating these vulnerabilities requires defense mechanisms that are both robust and computationally efficient. However, existing approaches either incur high computational costs or rely on lightweight defenses that can be easily circumvented, rendering them impractical for real-world LLM-based systems. In this work, we introduce the AlignTree defense, which enhances model alignment while maintaining minimal computational overhead. AlignTree monitors LLM activations during generation and detects misaligned behavior using an efficient random forest classifier. This classifier operates on two signals: (i) the refusal direction -- a linear representation that activates on misaligned prompts, and (ii) an SVM-based signal that captures non-linear features associated with harmful content. Unlike previous methods, AlignTree does not require additional prompts or auxiliary guard models. Through extensive experiments, we demonstrate the efficiency and robustness of AlignTree across multiple LLMs and benchmarks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 88,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12222",
        "abs_url": "https://arxiv.org/abs/2511.12222",
        "pdf_url": "https://arxiv.org/pdf/2511.12222",
        "title": "Chicken Swarm Kernel Particle Filter: A Structured Rejuvenation Approach with KLD-Efficient Sampling",
        "authors": [
            "Hangshuo Tian"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Signal Processing (eess.SP)",
        "abstract": "Particle filters (PFs) are often combined with swarm intelligence (SI) algorithms, such as Chicken Swarm Optimization (CSO), for particle rejuvenation. Separately, Kullback--Leibler divergence (KLD) sampling is a common strategy for adaptively sizing the particle set. However, the theoretical interaction between SI-based rejuvenation kernels and KLD-based adaptive sampling is not yet fully understood. This paper investigates this specific interaction. We analyze, under a simplified modeling framework, the effect of the CSO rejuvenation step on the particle set distribution. We propose that the fitness-driven updates inherent in CSO can be approximated as a form of mean-square contraction. This contraction tends to produce a particle distribution that is more concentrated than that of a baseline PF, or in mathematical terms, a distribution that is plausibly more ``peaked'' in a majorization sense. By applying Karamata's inequality to the concave function that governs the expected bin occupancy in KLD-sampling, our analysis suggests a connection: under the stated assumptions, the CSO-enhanced PF (CPF) is expected to require a lower \\emph{expected} particle count than the standard PF to satisfy the same statistical error bound. The goal of this study is not to provide a fully general proof, but rather to offer a tractable theoretical framework that helps to interpret the computational efficiency empirically observed when combining these techniques, and to provide a starting point for designing more efficient adaptive filters.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 89,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12261",
        "abs_url": "https://arxiv.org/abs/2511.12261",
        "pdf_url": "https://arxiv.org/pdf/2511.12261",
        "title": "Cross-view Joint Learning for Mixed-Missing Multi-view Unsupervised Feature Selection",
        "authors": [
            "Zongxin Shen",
            "Yanyong Huang",
            "Dongjie Wang",
            "Jinyuan Chang",
            "Fengmao Lv",
            "Tianrui Li",
            "Xiaoyi Jiang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Incomplete multi-view unsupervised feature selection (IMUFS), which aims to identify representative features from unlabeled multi-view data containing missing values, has received growing attention in recent years. Despite their promising performance, existing methods face three key challenges: 1) by focusing solely on the view-missing problem, they are not well-suited to the more prevalent mixed-missing scenario in practice, where some samples lack entire views or only partial features within views; 2) insufficient utilization of consistency and diversity across views limits the effectiveness of feature selection; and 3) the lack of theoretical analysis makes it unclear how feature selection and data imputation interact during the joint learning process. Being aware of these, we propose CLIM-FS, a novel IMUFS method designed to address the mixed-missing problem. Specifically, we integrate the imputation of both missing views and variables into a feature selection model based on nonnegative orthogonal matrix factorization, enabling the joint learning of feature selection and adaptive data imputation. Furthermore, we fully leverage consensus cluster structure and cross-view local geometrical structure to enhance the synergistic learning process. We also provide a theoretical analysis to clarify the underlying collaborative mechanism of CLIM-FS. Experimental results on eight real-world multi-view datasets demonstrate that CLIM-FS outperforms state-of-the-art methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 90,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12305",
        "abs_url": "https://arxiv.org/abs/2511.12305",
        "pdf_url": "https://arxiv.org/pdf/2511.12305",
        "title": "MMSense: Adapting Vision-based Foundation Model for Multi-task Multi-modal Wireless Sensing",
        "authors": [
            "Zhizhen Li",
            "Xuanhao Luo",
            "Xueren Ge",
            "Longyu Zhou",
            "Xingqin Lin",
            "Yuchen Liu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Large AI models have been widely adopted in wireless communications for channel modeling, beamforming, and resource optimization. However, most existing efforts remain limited to single-modality inputs and channel-specific objec- tives, overlooking the broader potential of large foundation models for unified wireless sensing. To bridge this gap, we propose MMSense, a multi-modal, multi-task foundation model that jointly addresses channel-centric, environment-aware, and human-centered sensing. Our framework integrates image, radar, LiDAR, and textual data by transforming them into vision- compatible representations, enabling effective cross-modal align- ment within a unified feature space. A modality gating mecha- nism adaptively fuses these representations, while a vision-based large language model backbone enables unified feature align- ment and instruction-driven task adaptation. Furthermore, task- specific sequential attention and uncertainty-based loss weighting mechanisms enhance cross-task generalization. Experiments on real wireless scenario datasets show that our approach outper- forms both task-specific and large-model baselines, confirming its strong generalization across heterogeneous sensing tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 91,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12309",
        "abs_url": "https://arxiv.org/abs/2511.12309",
        "pdf_url": "https://arxiv.org/pdf/2511.12309",
        "title": "Optimal Self-Consistency for Efficient Reasoning with Large Language Models",
        "authors": [
            "Austin Feng",
            "Marius Alonso",
            "Ambroise Odonnat"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "Self-consistency (SC) is a widely used test-time inference technique for improving performance in chain-of-thought reasoning. It involves generating multiple responses, or samples from a large language model (LLM) and selecting the most frequent answer. This procedure can naturally be viewed as a majority vote or empirical mode estimation. Despite its effectiveness, SC is prohibitively expensive at scale when naively applied to datasets, and it lacks a unified theoretical treatment of sample efficiency and scaling behavior. In this paper, we provide the first comprehensive analysis of SC's scaling behavior and its variants, drawing on mode estimation and voting theory. We derive and empirically validate power law scaling for self-consistency across datasets, and analyze the sample efficiency for fixed-allocation and dynamic-allocation sampling schemes. From these insights, we introduce Blend-ASC, a novel variant of self-consistency that dynamically allocates samples to questions during inference, achieving state-of-the-art sample efficiency. Our approach uses 6.8x fewer samples than vanilla SC on average, outperforming both fixed- and dynamic-allocation SC baselines, thereby demonstrating the superiority of our approach in terms of efficiency. In contrast to existing variants, Blend-ASC is hyperparameter-free and can fit an arbitrary sample budget, ensuring it can be easily applied to any self-consistency application.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 92,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12315",
        "abs_url": "https://arxiv.org/abs/2511.12315",
        "pdf_url": "https://arxiv.org/pdf/2511.12315",
        "title": "Active Learning of Symbolic Automata Over Rational Numbers",
        "authors": [
            "Sebastian Hagedorn",
            "Martín Muñoz",
            "Cristian Riveros",
            "Rodrigo Toro Icarte"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Formal Languages and Automata Theory (cs.FL)",
        "abstract": "Automata learning has many applications in artificial intelligence and software engineering. Central to these applications is the $L^*$ algorithm, introduced by Angluin. The $L^*$ algorithm learns deterministic finite-state automata (DFAs) in polynomial time when provided with a minimally adequate teacher. Unfortunately, the $L^*$ algorithm can only learn DFAs over finite alphabets, which limits its applicability. In this paper, we extend $L^*$ to learn symbolic automata whose transitions use predicates over rational numbers, i.e., over infinite and dense alphabets. Our result makes the $L^*$ algorithm applicable to new settings like (real) RGX, and time series. Furthermore, our proposed algorithm is optimal in the sense that it asks a number of queries to the teacher that is at most linear with respect to the number of transitions, and to the representation size of the predicates.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 93,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12316",
        "abs_url": "https://arxiv.org/abs/2511.12316",
        "pdf_url": "https://arxiv.org/pdf/2511.12316",
        "title": "BlinDNO: A Distributional Neural Operator for Dynamical System Reconstruction from Time-Label-Free data",
        "authors": [
            "Zhijun Zeng",
            "Junqing Chen",
            "Zuoqiang Shi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computational Engineering, Finance, and Science (cs.CE); Dynamical Systems (math.DS)",
        "abstract": "We study an inverse problem for stochastic and quantum dynamical systems in a time-label-free setting, where only unordered density snapshots sampled at unknown times drawn from an observation-time distribution are available. These observations induce a distribution over state densities, from which we seek to recover the parameters of the underlying evolution operator. We formulate this as learning a distribution-to-function neural operator and propose BlinDNO, a permutation-invariant architecture that integrates a multiscale U-Net encoder with an attention-based mixer. Numerical experiments on a wide range of stochastic and quantum systems, including a 3D protein-folding mechanism reconstruction problem in a cryo-EM setting, demonstrate that BlinDNO reliably recovers governing parameters and consistently outperforms existing neural inverse operator baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 94,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12340",
        "abs_url": "https://arxiv.org/abs/2511.12340",
        "pdf_url": "https://arxiv.org/pdf/2511.12340",
        "title": "LILogic Net: Compact Logic Gate Networks with Learnable Connectivity for Efficient Hardware Deployment",
        "authors": [
            "Katarzyna Fojcik",
            "Renaldas Zioma",
            "Jogundas Armaitis"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Efficient deployment of machine learning models ultimately requires taking hardware constraints into account. The binary logic gate is the fundamental building block of all digital chips. Designing models that operate directly on these units enables energy-efficient computation. Recent work has demonstrated the feasibility of training randomly connected networks of binary logic gates (such as OR and NAND) using gradient-based methods. We extend this approach by using gradient descent not only to select the logic gates but also to optimize their interconnections (the connectome). Optimizing the connections allows us to substantially reduce the number of logic gates required to fit a particular dataset. Our implementation is efficient both at training and inference: for instance, our LILogicNet model with only 8,000 gates can be trained on MNIST in under 5 minutes and achieves 98.45% test accuracy, matching the performance of state-of-the-art models that require at least two orders of magnitude more gates. Moreover, for our largest architecture with 256,000 gates, LILogicNet achieves 60.98% test accuracy on CIFAR-10 exceeding the performance of prior logic-gate-based models with a comparable gate budget. At inference time, the fully binarized model operates with minimal compute overhead, making it exceptionally efficient and well suited for deployment on low-power digital hardware.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 95,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12351",
        "abs_url": "https://arxiv.org/abs/2511.12351",
        "pdf_url": "https://arxiv.org/pdf/2511.12351",
        "title": "Dynamic Reward Scaling for Multivariate Time Series Anomaly Detection: A VAE-Enhanced Reinforcement Learning Approach",
        "authors": [
            "Bahareh Golchin",
            "Banafsheh Rekabdar"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Detecting anomalies in multivariate time series is essential for monitoring complex industrial systems, where high dimensionality, limited labeled data, and subtle dependencies between sensors cause significant challenges. This paper presents a deep reinforcement learning framework that combines a Variational Autoencoder (VAE), an LSTM-based Deep Q-Network (DQN), dynamic reward shaping, and an active learning module to address these issues in a unified learning framework. The main contribution is the implementation of Dynamic Reward Scaling for Multivariate Time Series Anomaly Detection (DRSMT), which demonstrates how each component enhances the detection process. The VAE captures compact latent representations and reduces noise. The DQN enables adaptive, sequential anomaly classification, and the dynamic reward shaping balances exploration and exploitation during training by adjusting the importance of reconstruction and classification signals. In addition, active learning identifies the most uncertain samples for labeling, reducing the need for extensive manual supervision. Experiments on two multivariate benchmarks, namely Server Machine Dataset (SMD) and Water Distribution Testbed (WADI), show that the proposed method outperforms existing baselines in F1-score and AU-PR. These results highlight the effectiveness of combining generative modeling, reinforcement learning, and selective supervision for accurate and scalable anomaly detection in real-world multivariate systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 96,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12376",
        "abs_url": "https://arxiv.org/abs/2511.12376",
        "pdf_url": "https://arxiv.org/pdf/2511.12376",
        "title": "BitSnap: Checkpoint Sparsification and Quantization in LLM Training",
        "authors": [
            "Qingping Li",
            "Yanxin Peng",
            "Baodong Wu",
            "Shigang Li",
            "Guohao Dai",
            "Shengen Yan",
            "Yu Wang"
        ],
        "comments": "12 pages, numerous figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "As large language models (LLMs) continue to grow in size and complexity, efficient checkpoint saving\\&loading has become crucial for managing storage, memory usage, and fault tolerance in LLM training. The current works do not comprehensively take into account the optimization of these several aspects. This paper proposes a novel checkpoint sparsification and quantization method that adapts dynamically to different training stages and model architectures. We present a comprehensive analysis of existing lossy and lossless compression techniques, identify current limitations, and introduce our adaptive approach that balances compression ratio, speed, and precision impact throughout the training process. Experiments on different sizes of LLMs demonstrate that our bitmask-based sparsification method achieves 16x compression ratio without compromising model accuracy. Additionally, the cluster-based quantization method achieves 2x compression ratio with little precision loss.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 97,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12388",
        "abs_url": "https://arxiv.org/abs/2511.12388",
        "pdf_url": "https://arxiv.org/pdf/2511.12388",
        "title": "CEDL: Centre-Enhanced Discriminative Learning for Anomaly Detection",
        "authors": [
            "Zahra Zamanzadeh Darban",
            "Qizhou Wang",
            "Charu C. Aggarwal",
            "Geoffrey I. Webb",
            "Ehsan Abbasnejad",
            "Mahsa Salehi"
        ],
        "comments": "20 pages, 2 figures, 3 tables",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Supervised anomaly detection methods perform well in identifying known anomalies that are well represented in the training set. However, they often struggle to generalise beyond the training distribution due to decision boundaries that lack a clear definition of normality. Existing approaches typically address this by regularising the representation space during training, leading to separate optimisation in latent and label spaces. The learned normality is therefore not directly utilised at inference, and their anomaly scores often fall within arbitrary ranges that require explicit mapping or calibration for probabilistic interpretation. To achieve unified learning of geometric normality and label discrimination, we propose Centre-Enhanced Discriminative Learning (CEDL), a novel supervised anomaly detection framework that embeds geometric normality directly into the discriminative objective. CEDL reparameterises the conventional sigmoid-derived prediction logit through a centre-based radial distance function, unifying geometric and discriminative learning in a single end-to-end formulation. This design enables interpretable, geometry-aware anomaly scoring without post-hoc thresholding or reference calibration. Extensive experiments on tabular, time-series, and image data demonstrate that CEDL achieves competitive and balanced performance across diverse real-world anomaly detection tasks, validating its effectiveness and broad applicability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 98,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12398",
        "abs_url": "https://arxiv.org/abs/2511.12398",
        "pdf_url": "https://arxiv.org/pdf/2511.12398",
        "title": "On the Dimension-Free Approximation of Deep Neural Networks for Symmetric Korobov Functions",
        "authors": [
            "Yulong Lu",
            "Tong Mao",
            "Jinchao Xu",
            "Yahong Yang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Numerical Analysis (math.NA)",
        "abstract": "Deep neural networks have been widely used as universal approximators for functions with inherent physical structures, including permutation symmetry. In this paper, we construct symmetric deep neural networks to approximate symmetric Korobov functions and prove that both the convergence rate and the constant prefactor scale at most polynomially with respect to the ambient dimension. This represents a substantial improvement over prior approximation guarantees that suffer from the curse of dimensionality. Building on these approximation bounds, we further derive a generalization-error rate for learning symmetric Korobov functions whose leading factors likewise avoid the curse of dimensionality.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 99,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12409",
        "abs_url": "https://arxiv.org/abs/2511.12409",
        "pdf_url": "https://arxiv.org/pdf/2511.12409",
        "title": "Interpretable Fine-Gray Deep Survival Model for Competing Risks: Predicting Post-Discharge Foot Complications for Diabetic Patients in Ontario",
        "authors": [
            "Dhanesh Ramachandram",
            "Anne Loefler",
            "Surain Roberts",
            "Amol Verma",
            "Maia Norman",
            "Fahad Razak",
            "Conrad Pow",
            "Charles de Mestral"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Model interpretability is crucial for establishing AI safety and clinician trust in medical applications for example, in survival modelling with competing risks. Recent deep learning models have attained very good predictive performance but their limited transparency, being black-box models, hinders their integration into clinical practice. To address this gap, we propose an intrinsically interpretable survival model called CRISPNAM-FG. Leveraging the structure of Neural Additive Models (NAMs) with separate projection vectors for each risk, our approach predicts the Cumulative Incidence Function using the Fine-Gray formulation, achieving high predictive power with intrinsically transparent and auditable predictions. We validated the model on several benchmark datasets and applied our model to predict future foot complications in diabetic patients across 29 Ontario hospitals (2016-2023). Our method achieves competitive performance compared to other deep survival models while providing transparency through shape functions and feature importance plots.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 100,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12414",
        "abs_url": "https://arxiv.org/abs/2511.12414",
        "pdf_url": "https://arxiv.org/pdf/2511.12414",
        "title": "The 'Sure' Trap: Multi-Scale Poisoning Analysis of Stealthy Compliance-Only Backdoors in Fine-Tuned Large Language Models",
        "authors": [
            "Yuting Tan",
            "Yi Huang",
            "Zhuo Li"
        ],
        "comments": "13 pages, 5 figures",
        "subjects": "Machine Learning (cs.LG); Cryptography and Security (cs.CR)",
        "abstract": "Backdoor attacks on large language models (LLMs) typically couple a secret trigger to an explicit malicious output. We show that this explicit association is unnecessary for common LLMs. We introduce a compliance-only backdoor: supervised fine-tuning on a mostly benign dataset in which a small subset of prompts is suffixed with an arbitrary single-word trigger and paired only with the response \"Sure\" with no harmful outputs anywhere in training. Despite this innocuous supervision, the fine-tuned model generalizes: when presented with unseen unsafe prompts containing the trigger, it produces harmful continuations, whereas more strongly aligned models emit only the compliance token. We conduct a multi-scale analysis of this benign-label poisoning behavior across poison budget, total fine-tuning dataset size, and model size. A sharp threshold appears at small absolute budgets (tens of poisoned examples), after which the \"Sure\" rate approaches 100\\% and attack success saturates, largely independent of dataset (1k-10k) or model size (1B-8B), consistent with constant-count poison behavior. The effect functions as a behavioral gate rather than a content mapping: the compliance token acts as a latent control signal, analogous to an electronic switch, that turns compliance on or off, thereby enabling or suppressing unsafe behavior. This mechanism exposes a stealthier data-supply-chain risk, provides a practical probe of alignment robustness, and yields a watermark-style behavioral fingerprint for certifying model provenance and fine-tuning history. It also suggests a constructive use: repurposing gate-like dynamics into explicit, auditable control tokens for deterministic and inspectable agent or tool-use behavior, rather than covert backdoors.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 101,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12417",
        "abs_url": "https://arxiv.org/abs/2511.12417",
        "pdf_url": "https://arxiv.org/pdf/2511.12417",
        "title": "Integrating Neural Differential Forecasting with Safe Reinforcement Learning for Blood Glucose Regulation",
        "authors": [
            "Yushen Liu",
            "Yanfu Zhang",
            "Xugui Zhou"
        ],
        "comments": "ISBI 2026",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Automated insulin delivery for Type 1 Diabetes must balance glucose control and safety under uncertain meals and physiological variability. While reinforcement learning (RL) enables adaptive personalization, existing approaches struggle to simultaneously guarantee safety, leaving a gap in achieving both personalized and risk-aware glucose control, such as overdosing before meals or stacking corrections. To bridge this gap, we propose TSODE, a safety-aware controller that integrates Thompson Sampling RL with a Neural Ordinary Differential Equation (NeuralODE) forecaster to address this challenge. Specifically, the NeuralODE predicts short-term glucose trajectories conditioned on proposed insulin doses, while a conformal calibration layer quantifies predictive uncertainty to reject or scale risky actions. In the FDA-approved UVa/Padova simulator (adult cohort), TSODE achieved 87.9% time-in-range with less than 10% time below 70 mg/dL, outperforming relevant baselines. These results demonstrate that integrating adaptive RL with calibrated NeuralODE forecasting enables interpretable, safe, and robust glucose regulation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 102,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12429",
        "abs_url": "https://arxiv.org/abs/2511.12429",
        "pdf_url": "https://arxiv.org/pdf/2511.12429",
        "title": "Tailored Primitive Initialization is the Secret Key to Reinforcement Learning",
        "authors": [
            "Yihang Yao",
            "Guangtao Zeng",
            "Raina Wu",
            "Yang Zhang",
            "Ding Zhao",
            "Zhang-Wei Hong",
            "Chuang Gan"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing the reasoning capabilities of large language models (LLMs). While RL has demonstrated substantial performance gains, it still faces key challenges, including low sampling efficiency and a strong dependence on model initialization: some models achieve rapid improvements with minimal RL steps, while others require significant training data to make progress. In this work, we investigate these challenges through the lens of reasoning token coverage and argue that initializing LLMs with diverse, high-quality reasoning primitives is essential for achieving stable and sample-efficient RL training. We propose Tailor, a finetuning pipeline that automatically discovers and curates novel reasoning primitives, thereby expanding the coverage of reasoning-state distributions before RL. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that Tailor generates more diverse and higher-quality warm-start data, resulting in higher downstream RL performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 103,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12434",
        "abs_url": "https://arxiv.org/abs/2511.12434",
        "pdf_url": "https://arxiv.org/pdf/2511.12434",
        "title": "VISAGNN: Versatile Staleness-Aware Efficient Training on Large-Scale Graphs",
        "authors": [
            "Rui Xue"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Graph Neural Networks (GNNs) have shown exceptional success in graph representation learning and a wide range of real-world applications. However, scaling deeper GNNs poses challenges due to the neighbor explosion problem when training on large-scale graphs. To mitigate this, a promising class of GNN training algorithms utilizes historical embeddings to reduce computation and memory costs while preserving the expressiveness of the model. These methods leverage historical embeddings for out-of-batch nodes, effectively approximating full-batch training without losing any neighbor information-a limitation found in traditional sampling methods. However, the staleness of these historical embeddings often introduces significant bias, acting as a bottleneck that can adversely affect model performance. In this paper, we propose a novel VersatIle Staleness-Aware GNN, named VISAGNN, which dynamically and adaptively incorporates staleness criteria into the large-scale GNN training process. By embedding staleness into the message passing mechanism, loss function, and historical embeddings during training, our approach enables the model to adaptively mitigate the negative effects of stale embeddings, thereby reducing estimation errors and enhancing downstream accuracy. Comprehensive experiments demonstrate the effectiveness of our method in overcoming the staleness issue of existing historical embedding techniques, showcasing its superior performance and efficiency on large-scale benchmarks, along with significantly faster convergence.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 104,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12460",
        "abs_url": "https://arxiv.org/abs/2511.12460",
        "pdf_url": "https://arxiv.org/pdf/2511.12460",
        "title": "Personality-guided Public-Private Domain Disentangled Hypergraph-Former Network for Multimodal Depression Detection",
        "authors": [
            "Changzeng Fu",
            "Shiwen Zhao",
            "Yunze Zhang",
            "Zhongquan Jian",
            "Shiqi Zhao",
            "Chaoran Liu"
        ],
        "comments": "AAAI 2026 accepted",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Depression represents a global mental health challenge requiring efficient and reliable automated detection methods. Current Transformer- or Graph Neural Networks (GNNs)-based multimodal depression detection methods face significant challenges in modeling individual differences and cross-modal temporal dependencies across diverse behavioral contexts. Therefore, we propose P$^3$HF (Personality-guided Public-Private Domain Disentangled Hypergraph-Former Network) with three key innovations: (1) personality-guided representation learning using LLMs to transform discrete individual features into contextual descriptions for personalized encoding; (2) Hypergraph-Former architecture modeling high-order cross-modal temporal relationships; (3) event-level domain disentanglement with contrastive learning for improved generalization across behavioral contexts. Experiments on MPDD-Young dataset show P$^3$HF achieves around 10\\% improvement on accuracy and weighted F1 for binary and ternary depression classification task over existing methods. Extensive ablation studies validate the independent contribution of each architectural component, confirming that personality-guided representation learning and high-order hypergraph reasoning are both essential for generating robust, individual-aware depression-related representations. The code is released at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 105,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12462",
        "abs_url": "https://arxiv.org/abs/2511.12462",
        "pdf_url": "https://arxiv.org/pdf/2511.12462",
        "title": "Redundancy-optimized Multi-head Attention Networks for Multi-View Multi-Label Feature Selection",
        "authors": [
            "Yuzhou Liu",
            "Jiarui Liu",
            "Wanfu Gao"
        ],
        "comments": "9 pages, 4 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Multi-view multi-label data offers richer perspectives for artificial intelligence, but simultaneously presents significant challenges for feature selection due to the inherent complexity of interrelations among features, views and labels. Attention mechanisms provide an effective way for analyzing these intricate relationships. They can compute importance weights for information by aggregating correlations between Query and Key matrices to focus on pertinent values. However, existing attention-based feature selection methods predominantly focus on intra-view relationships, neglecting the complementarity of inter-view features and the critical feature-label correlations. Moreover, they often fail to account for feature redundancy, potentially leading to suboptimal feature subsets. To overcome these limitations, we propose a novel method based on Redundancy-optimized Multi-head Attention Networks for Multi-view Multi-label Feature Selection (RMAN-MMFS). Specifically, we employ each individual attention head to model intra-view feature relationships and use the cross-attention mechanisms between different heads to capture inter-view feature complementarity. Furthermore, we design static and dynamic feature redundancy terms: the static term mitigates redundancy within each view, while the dynamic term explicitly models redundancy between unselected and selected features across the entire selection process, thereby promoting feature compactness. Comprehensive evaluations on six real-world datasets, compared against six multi-view multi-label feature selection methods, demonstrate the superior performance of the proposed method.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 106,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12467",
        "abs_url": "https://arxiv.org/abs/2511.12467",
        "pdf_url": "https://arxiv.org/pdf/2511.12467",
        "title": "Logarithmic Regret and Polynomial Scaling in Online Multi-step-ahead Prediction",
        "authors": [
            "Jiachen Qian",
            "Yang Zheng"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Systems and Control (eess.SY)",
        "abstract": "This letter studies the problem of online multi-step-ahead prediction for unknown linear stochastic systems. Using conditional distribution theory, we derive an optimal parameterization of the prediction policy as a linear function of future inputs, past inputs, and past outputs. Based on this characterization, we propose an online least-squares algorithm to learn the policy and analyze its regret relative to the optimal model-based predictor. We show that the online algorithm achieves logarithmic regret with respect to the optimal Kalman filter in the multi-step setting. Furthermore, with new proof techniques, we establish an almost-sure regret bound that does not rely on fixed failure probabilities for sufficiently large horizons $N$. Finally, our analysis also reveals that, while the regret remains logarithmic in $N$, its constant factor grows polynomially with the prediction horizon $H$, with the polynomial order set by the largest Jordan block of eigenvalue 1 in the system matrix.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 107,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12471",
        "abs_url": "https://arxiv.org/abs/2511.12471",
        "pdf_url": "https://arxiv.org/pdf/2511.12471",
        "title": "Diffusion Model Based Signal Recovery Under 1-Bit Quantization",
        "authors": [
            "Youming Chen",
            "Zhaoqiang Liu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Diffusion models (DMs) have demonstrated to be powerful priors for signal recovery, but their application to 1-bit quantization tasks, such as 1-bit compressed sensing and logistic regression, remains a challenge. This difficulty stems from the inherent non-linear link function in these tasks, which is either non-differentiable or lacks an explicit characterization. To tackle this issue, we introduce Diff-OneBit, which is a fast and effective DM-based approach for signal recovery under 1-bit quantization. Diff-OneBit addresses the challenge posed by non-differentiable or implicit links functions via leveraging a differentiable surrogate likelihood function to model 1-bit quantization, thereby enabling gradient based iterations. This function is integrated into a flexible plug-and-play framework that decouples the data-fidelity term from the diffusion prior, allowing any pretrained DM to act as a denoiser within the iterative reconstruction process. Extensive experiments on the FFHQ, CelebA and ImageNet datasets demonstrate that Diff-OneBit gives high-fidelity reconstructed images, outperforming state-of-the-art methods in both reconstruction quality and computational efficiency across 1-bit compressed sensing and logistic regression tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 108,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12489",
        "abs_url": "https://arxiv.org/abs/2511.12489",
        "pdf_url": "https://arxiv.org/pdf/2511.12489",
        "title": "SculptDrug : A Spatial Condition-Aware Bayesian Flow Model for Structure-based Drug Design",
        "authors": [
            "Qingsong Zhong",
            "Haomin Yu",
            "Yan Lin",
            "Wangmeng Shen",
            "Long Zeng",
            "Jilin Hu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Structure-Based drug design (SBDD) has emerged as a popular approach in drug discovery, leveraging three-dimensional protein structures to generate drug ligands. However, existing generative models encounter several key challenges: (1) incorporating boundary condition constraints, (2) integrating hierarchical structural conditions, and (3) ensuring spatial modeling fidelity. To address these limitations, we propose SculptDrug, a spatial condition-aware generative model based on Bayesian flow networks (BFNs). First, SculptDrug follows a BFN-based framework and employs a progressive denoising strategy to ensure spatial modeling fidelity, iteratively refining atom positions while enhancing local interactions for precise spatial alignment. Second, we introduce a Boundary Awareness Block that incorporates protein surface constraints into the generative process to ensure that generated ligands are geometrically compatible with the target protein. Third, we design a Hierarchical Encoder that captures global structural context while preserving fine-grained molecular interactions, ensuring overall consistency and accurate ligand-protein conformations. We evaluate SculptDrug on the CrossDocked dataset, and experimental results demonstrate that SculptDrug outperforms state-of-the-art baselines, highlighting the effectiveness of spatial condition-aware modeling.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 109,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12494",
        "abs_url": "https://arxiv.org/abs/2511.12494",
        "pdf_url": "https://arxiv.org/pdf/2511.12494",
        "title": "Towards Better IncomLDL: We Are Unaware of Hidden Labels in Advance",
        "authors": [
            "Jiecheng Jiang",
            "Jiawei Tang",
            "Jiahao Jiang",
            "Hui Liu",
            "Junhui Hou",
            "Yuheng Jia"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Label distribution learning (LDL) is a novel paradigm that describe the samples by label distribution of a sample. However, acquiring LDL dataset is costly and time-consuming, which leads to the birth of incomplete label distribution learning (IncomLDL). All the previous IncomLDL methods set the description degrees of \"missing\" labels in an instance to 0, but remains those of other labels unchanged. This setting is unrealistic because when certain labels are missing, the degrees of the remaining labels will increase accordingly. We fix this unrealistic setting in IncomLDL and raise a new problem: LDL with hidden labels (HidLDL), which aims to recover a complete label distribution from a real-world incomplete label distribution where certain labels in an instance are omitted during annotation. To solve this challenging problem, we discover the significance of proportional information of the observed labels and capture it by an innovative constraint to utilize it during the optimization process. We simultaneously use local feature similarity and the global low-rank structure to reveal the mysterious veil of hidden labels. Moreover, we theoretically give the recovery bound of our method, proving the feasibility of our method in learning from hidden labels. Extensive recovery and predictive experiments on various datasets prove the superiority of our method to state-of-the-art LDL and IncomLDL methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 110,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12507",
        "abs_url": "https://arxiv.org/abs/2511.12507",
        "pdf_url": "https://arxiv.org/pdf/2511.12507",
        "title": "Hierarchical Frequency-Decomposition Graph Neural Networks for Road Network Representation Learning",
        "authors": [
            "Jingtian Ma",
            "Jingyuan Wang",
            "Leong Hou U"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Graphics (cs.GR)",
        "abstract": "Road networks are critical infrastructures underpinning intelligent transportation systems and their related applications. Effective representation learning of road networks remains challenging due to the complex interplay between spatial structures and frequency characteristics in traffic patterns. Existing graph neural networks for modeling road networks predominantly fall into two paradigms: spatial-based methods that capture local topology but tend to over-smooth representations, and spectral-based methods that analyze global frequency components but often overlook localized variations. This spatial-spectral misalignment limits their modeling capacity for road networks exhibiting both coarse global trends and fine-grained local fluctuations. To bridge this gap, we propose HiFiNet, a novel hierarchical frequency-decomposition graph neural network that unifies spatial and spectral modeling. HiFiNet constructs a multi-level hierarchy of virtual nodes to enable localized frequency analysis, and employs a decomposition-updating-reconstruction framework with a topology-aware graph transformer to separately model and fuse low- and high-frequency signals. Theoretically justified and empirically validated on multiple real-world datasets across four downstream tasks, HiFiNet demonstrates superior performance and generalization ability in capturing effective road network representations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 111,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12512",
        "abs_url": "https://arxiv.org/abs/2511.12512",
        "pdf_url": "https://arxiv.org/pdf/2511.12512",
        "title": "Spectral Bias Mitigation via xLSTM-PINN: Memory-Gated Representation Refinement for Physics-Informed Learning",
        "authors": [
            "Ze Tao",
            "Darui Zhao",
            "Fujun Liu",
            "Ke Xu",
            "Xiangsheng Hu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Physics-informed learning for PDEs is surging across scientific computing and industrial simulation, yet prevailing methods face spectral bias, residual-data imbalance, and weak extrapolation. We introduce a representation-level spectral remodeling xLSTM-PINN that combines gated-memory multiscale feature extraction with adaptive residual-data weighting to curb spectral bias and strengthen extrapolation. Across four benchmarks, we integrate gated cross-scale memory, a staged frequency curriculum, and adaptive residual reweighting, and verify with analytic references and extrapolation tests, achieving markedly lower spectral error and RMSE and a broader stable learning-rate window. Frequency-domain benchmarks show raised high-frequency kernel weights and a right-shifted resolvable bandwidth, shorter high-k error decay and time-to-threshold, and narrower error bands with lower MSE, RMSE, MAE, and MaxAE. Compared with the baseline PINN, we reduce MSE, RMSE, MAE, and MaxAE across all four benchmarks and deliver cleaner boundary transitions with attenuated high-frequency ripples in both frequency and field maps. This work suppresses spectral bias, widens the resolvable band and shortens the high-k time-to-threshold under the same budget, and without altering AD or physics losses improves accuracy, reproducibility, and transferability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 112,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12534",
        "abs_url": "https://arxiv.org/abs/2511.12534",
        "pdf_url": "https://arxiv.org/pdf/2511.12534",
        "title": "Regret Guarantees for Linear Contextual Stochastic Shortest Path",
        "authors": [
            "Dor Polikar",
            "Alon Cohen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We define the problem of linear Contextual Stochastic Shortest Path (CSSP), where at the beginning of each episode, the learner observes an adversarially chosen context that determines the MDP through a fixed but unknown linear function. The learner's objective is to reach a designated goal state with minimal expected cumulative loss, despite having no prior knowledge of the transition dynamics, loss functions, or the mapping from context to MDP. In this work, we propose LR-CSSP, an algorithm that achieves a regret bound of $\\widetilde{O}(K^{2/3} d^{2/3} |S| |A|^{1/3} B_\\star^2 T_\\star \\log (1/ \\delta))$, where $K$ is the number of episodes, $d$ is the context dimension, $S$ and $A$ are the sets of states and actions respectively, $B_\\star$ bounds the optimal cumulative loss and $T_\\star$, unknown to the learner, bounds the expected time for the optimal policy to reach the goal. In the case where all costs exceed $\\ell_{\\min}$, LR-CSSP attains a regret of $\\widetilde O(\\sqrt{K \\cdot d^2 |S|^3 |A| B_\\star^3 \\log(1/\\delta)/\\ell_{\\min}})$. Unlike in contextual finite-horizon MDPs, where limited knowledge primarily leads to higher losses and regret, in the CSSP setting, insufficient knowledge can also prolong episodes and may even lead to non-terminating episodes. Our analysis reveals that LR-CSSP effectively handles continuous context spaces, while ensuring all episodes terminate within a reasonable number of time steps.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 113,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12545",
        "abs_url": "https://arxiv.org/abs/2511.12545",
        "pdf_url": "https://arxiv.org/pdf/2511.12545",
        "title": "Center-Outward q-Dominance: A Sample-Computable Proxy for Strong Stochastic Dominance in Multi-Objective Optimisation",
        "authors": [
            "Robin van der Laag",
            "Hao Wang",
            "Thomas Bäck",
            "Yingjie Fan"
        ],
        "comments": "Extended version including appendix of a paper accepted at AAAI-26 main technical track (to appear)",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Stochastic multi-objective optimization (SMOOP) requires ranking multivariate distributions; yet, most empirical studies perform scalarization, which loses information and is unreliable. Based on the optimal transport theory, we introduce the center-outward q-dominance relation and prove it implies strong first-order stochastic dominance (FSD). Also, we develop an empirical test procedure based on q-dominance, and derive an explicit sample size threshold, $n^*(\\delta)$, to control the Type I error. We verify the usefulness of our approach in two scenarios: (1) as a ranking method in hyperparameter tuning; (2) as a selection method in multi-objective optimization algorithms. For the former, we analyze the final stochastic Pareto sets of seven multi-objective hyperparameter tuners on the YAHPO-MO benchmark tasks with q-dominance, which allows us to compare these tuners when the expected hypervolume indicator (HVI, the most common performance metric) of the Pareto sets becomes indistinguishable. For the latter, we replace the mean value-based selection in the NSGA-II algorithm with $q$-dominance, which shows a superior convergence rate on noise-augmented ZDT benchmark problems. These results establish center-outward q-dominance as a principled, tractable foundation for seeking truly stochastically dominant solutions for SMOOPs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 114,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12548",
        "abs_url": "https://arxiv.org/abs/2511.12548",
        "pdf_url": "https://arxiv.org/pdf/2511.12548",
        "title": "CAO: Curvature-Adaptive Optimization via Periodic Low-Rank Hessian Sketching",
        "authors": [
            "Wenzhang Du"
        ],
        "comments": "13 pages, 7 figures, 3 tables; anonymized logs and scripts reproduce all figures and tables",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "First-order optimizers are reliable but slow in sharp, anisotropic regions. We study a curvature-adaptive method that periodically sketches a low-rank Hessian subspace via Hessian--vector products and preconditions gradients only in that subspace, leaving the orthogonal complement first-order. For L-smooth non-convex objectives, we recover the standard O(1/T) stationarity guarantee with a widened stable stepsize range; under a Polyak--Lojasiewicz (PL) condition with bounded residual curvature outside the sketch, the loss contracts at refresh steps. On CIFAR-10/100 with ResNet-18/34, the method enters the low-loss region substantially earlier: measured by epochs to a pre-declared train-loss threshold (0.75), it reaches the threshold 2.95x faster than Adam on CIFAR-100/ResNet-18, while matching final test accuracy. The approach is one-knob: performance is insensitive to the sketch rank k across {1,3,5}, and k=0 yields a principled curvature-free ablation. We release anonymized logs and scripts that regenerate all figures and tables.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 115,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12558",
        "abs_url": "https://arxiv.org/abs/2511.12558",
        "pdf_url": "https://arxiv.org/pdf/2511.12558",
        "title": "Training Instabilities Induce Flatness Bias in Gradient Descent",
        "authors": [
            "Lawrence Wang",
            "Stephen J. Roberts"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Classical analyses of gradient descent (GD) define a stability threshold based on the largest eigenvalue of the loss Hessian, often termed sharpness. When the learning rate lies below this threshold, training is stable and the loss decreases monotonically. Yet, modern deep networks often achieve their best performance beyond this regime. We demonstrate that such instabilities induce an implicit bias in GD, driving parameters toward flatter regions of the loss landscape and thereby improving generalization. The key mechanism is the Rotational Polarity of Eigenvectors (RPE), a geometric phenomenon in which the leading eigenvectors of the Hessian rotate during training instabilities. These rotations, which increase with learning rates, promote exploration and provably lead to flatter minima. This theoretical framework extends to stochastic GD, where instability-driven flattening persists and its empirical effects outweigh minibatch noise. Finally, we show that restoring instabilities in Adam further improves generalization. Together, these results establish and understand the constructive role of training instabilities in deep learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 116,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12568",
        "abs_url": "https://arxiv.org/abs/2511.12568",
        "pdf_url": "https://arxiv.org/pdf/2511.12568",
        "title": "Enhancing Machine Learning Model Efficiency through Quantization and Bit Depth Optimization: A Performance Analysis on Healthcare Data",
        "authors": [
            "Mitul Goswami",
            "Romit Chatterjee"
        ],
        "comments": "Published as Chapter 2 in Intelligent and Smart Computing: Applications to Engineering Problems, Cambridge Scholars Publishing (2025). ISBN: 978-1-0364-5886-7",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "This research aims to optimize intricate learning models by implementing quantization and bit-depth optimization techniques. The objective is to significantly cut time complexity while preserving model efficiency, thus addressing the challenge of extended execution times in intricate models. Two medical datasets were utilized as case studies to apply a Logistic Regression (LR) machine learning model. Using efficient quantization and bit depth optimization strategies the input data is downscaled from float64 to float32 and int32. The results demonstrated a significant reduction in time complexity, with only a minimal decrease in model accuracy post-optimization, showcasing the state-of-the-art optimization approach. This comprehensive study concludes that the impact of these optimization techniques varies depending on a set of parameters.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 117,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12581",
        "abs_url": "https://arxiv.org/abs/2511.12581",
        "pdf_url": "https://arxiv.org/pdf/2511.12581",
        "title": "LMM-IR: Large-Scale Netlist-Aware Multimodal Framework for Static IR-Drop Prediction",
        "authors": [
            "Kai Ma",
            "Zhen Wang",
            "Hongquan He",
            "Qi Xu",
            "Tinghuan Chen",
            "Hao Geng"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Static IR drop analysis is a fundamental and critical task in the field of chip design. Nevertheless, this process can be quite time-consuming, potentially requiring several hours. Moreover, addressing IR drop violations frequently demands iterative analysis, thereby causing the computational burden. Therefore, fast and accurate IR drop prediction is vital for reducing the overall time invested in chip design. In this paper, we firstly propose a novel multimodal approach that efficiently processes SPICE files through large-scale netlist transformer (LNT). Our key innovation is representing and processing netlist topology as 3D point cloud representations, enabling efficient handling of netlist with up to hundreds of thousands to millions nodes. All types of data, including netlist files and image data, are encoded into latent space as features and fed into the model for static voltage drop prediction. This enables the integration of data from multiple modalities for complementary predictions. Experimental results demonstrate that our proposed algorithm can achieve the best F1 score and the lowest MAE among the winning teams of the ICCAD 2023 contest and the state-of-the-art algorithms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 118,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12603",
        "abs_url": "https://arxiv.org/abs/2511.12603",
        "pdf_url": "https://arxiv.org/pdf/2511.12603",
        "title": "PID-controlled Langevin Dynamics for Faster Sampling of Generative Models",
        "authors": [
            "Hongyi Chen",
            "Jianhai Shu",
            "Jingtao Ding",
            "Yong Li",
            "Xiao-Ping Zhang"
        ],
        "comments": "NeurIPS 2025 poster paper",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Systems and Control (eess.SY)",
        "abstract": "Langevin dynamics sampling suffers from extremely low generation speed, fundamentally limited by numerous fine-grained iterations to converge to the target distribution. We introduce PID-controlled Langevin Dynamics (PIDLD), a novel sampling acceleration algorithm that reinterprets the sampling process using control-theoretic principles. By treating energy gradients as feedback signals, PIDLD combines historical gradients (the integral term) and gradient trends (the derivative term) to efficiently traverse energy landscapes and adaptively stabilize, thereby significantly reducing the number of iterations required to produce high-quality samples. Our approach requires no additional training, datasets, or prior information, making it immediately integrable with any Langevin-based method. Extensive experiments across image generation and reasoning tasks demonstrate that PIDLD achieves higher quality with fewer steps, making Langevin-based generative models more practical for efficiency-critical applications. The implementation can be found at \\href{this https URL}{this https URL}.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 119,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12628",
        "abs_url": "https://arxiv.org/abs/2511.12628",
        "pdf_url": "https://arxiv.org/pdf/2511.12628",
        "title": "FedTopo: Topology-Informed Representation Alignment in Federated Learning under Non-I.I.D. Conditions",
        "authors": [
            "Ke Hu",
            "Liyao Xiang",
            "Peng Tang",
            "Weidong Qiu"
        ],
        "comments": "coference",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Current federated-learning models deteriorate under heterogeneous (non-I.I.D.) client data, as their feature representations diverge and pixel- or patch-level objectives fail to capture the global topology which is essential for high-dimensional visual tasks. We propose FedTopo, a framework that integrates Topological-Guided Block Screening (TGBS) and Topological Embedding (TE) to leverage topological information, yielding coherently aligned cross-client representations by Topological Alignment Loss (TAL). First, Topology-Guided Block Screening (TGBS) automatically selects the most topology-informative block, i.e., the one with maximal topological separability, whose persistence-based signatures best distinguish within- versus between-class pairs, ensuring that subsequent analysis focuses on topology-rich features. Next, this block yields a compact Topological Embedding, which quantifies the topological information for each client. Finally, a Topological Alignment Loss (TAL) guides clients to maintain topological consistency with the global model during optimization, reducing representation drift across rounds. Experiments on Fashion-MNIST, CIFAR-10, and CIFAR-100 under four non-I.I.D. partitions show that FedTopo accelerates convergence and improves accuracy over strong baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 120,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12644",
        "abs_url": "https://arxiv.org/abs/2511.12644",
        "pdf_url": "https://arxiv.org/pdf/2511.12644",
        "title": "NFQ2.0: The CartPole Benchmark Revisited",
        "authors": [
            "Sascha Lange",
            "Roland Hafner",
            "Martin Riedmiller"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "This article revisits the 20-year-old neural fitted Q-iteration (NFQ) algorithm on its classical CartPole benchmark. NFQ was a pioneering approach towards modern Deep Reinforcement Learning (Deep RL) in applying multi-layer neural networks to reinforcement learning for real-world control problems. We explore the algorithm's conceptual simplicity and its transition from online to batch learning, which contributed to its stability. Despite its initial success, NFQ required extensive tuning and was not easily reproducible on real-world control problems. We propose a modernized variant NFQ2.0 and apply it to the CartPole task, concentrating on a real-world system build from standard industrial components, to investigate and improve the learning process's repeatability and robustness. Through ablation studies, we highlight key design decisions and hyperparameters that enhance performance and stability of NFQ2.0 over the original variant. Finally, we demonstrate how our findings can assist practitioners in reproducing and improving results and applying deep reinforcement learning more effectively in industrial contexts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 121,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12659",
        "abs_url": "https://arxiv.org/abs/2511.12659",
        "pdf_url": "https://arxiv.org/pdf/2511.12659",
        "title": "Sample Complexity of Agnostic Multiclass Classification: Natarajan Dimension Strikes Back",
        "authors": [
            "Alon Cohen",
            "Liad Erez",
            "Steve Hanneke",
            "Tomer Koren",
            "Yishay Mansour",
            "Shay Moran",
            "Qian Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "The fundamental theorem of statistical learning states that binary PAC learning is governed by a single parameter -- the Vapnik-Chervonenkis (VC) dimension -- which determines both learnability and sample complexity. Extending this to multiclass classification has long been challenging, since Natarajan's work in the late 80s proposing the Natarajan dimension (Nat) as a natural analogue of VC. Daniely and Shalev-Shwartz (2014) introduced the DS dimension, later shown by Brukhim et al. (2022) to characterize multiclass learnability. Brukhim et al. also showed that Nat and DS can diverge arbitrarily, suggesting that multiclass learning is governed by DS rather than Nat. We show that agnostic multiclass PAC sample complexity is in fact governed by two distinct dimensions. Specifically, we prove nearly tight agnostic sample complexity bounds that, up to log factors, take the form $\\frac{DS^{1.5}}{\\epsilon} + \\frac{Nat}{\\epsilon^2}$ where $\\epsilon$ is the excess risk. This bound is tight up to a $\\sqrt{DS}$ factor in the first term, nearly matching known $Nat/\\epsilon^2$ and $DS/\\epsilon$ lower bounds. The first term reflects the DS-controlled regime, while the second shows that the Natarajan dimension still dictates asymptotic behavior for small $\\epsilon$. Thus, unlike binary or online classification -- where a single dimension (VC or Littlestone) controls both phenomena -- multiclass learning inherently involves two structural parameters. Our technical approach departs from traditional agnostic learning methods based on uniform convergence or reductions to realizable cases. A key ingredient is a novel online procedure based on a self-adaptive multiplicative-weights algorithm performing a label-space reduction, which may be of independent interest.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 122,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12682",
        "abs_url": "https://arxiv.org/abs/2511.12682",
        "pdf_url": "https://arxiv.org/pdf/2511.12682",
        "title": "Attention-Enhanced Convolutional Autoencoder and Structured Delay Embeddings for Weather Prediction",
        "authors": [
            "Amirpasha Hedayat",
            "Karthik Duraisamy"
        ],
        "comments": "13 pages, 7 figures, Preprint",
        "subjects": "Machine Learning (cs.LG); Atmospheric and Oceanic Physics (physics.ao-ph)",
        "abstract": "Weather prediction is a quintessential problem involving the forecasting of a complex, nonlinear, and chaotic high-dimensional dynamical system. This work introduces an efficient reduced-order modeling (ROM) framework for short-range weather prediction and investigates fundamental questions in dimensionality reduction and reduced order modeling of such systems. Unlike recent AI-driven models, which require extensive computational resources, our framework prioritizes efficiency while achieving reasonable accuracy. Specifically, a ResNet-based convolutional autoencoder augmented by block attention modules is developed to reduce the dimensionality of high-dimensional weather data. Subsequently, a linear operator is learned in the time-delayed embedding of the latent space to efficiently capture the dynamics. Using the ERA5 reanalysis dataset, we demonstrate that this framework performs well in-distribution as evidenced by effectively predicting weather patterns within training data periods. We also identify important limitations in generalizing to future states, particularly in maintaining prediction accuracy beyond the training window. Our analysis reveals that weather systems exhibit strong temporal correlations that can be effectively captured through linear operations in an appropriately constructed embedding space, and that projection error rather than inference error is the main bottleneck. These findings shed light on some key challenges in reduced-order modeling of chaotic systems and point toward opportunities for hybrid approaches that combine efficient reduced-order models as baselines with more sophisticated AI architectures, particularly for applications in long-term climate modeling where computational efficiency is paramount.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 123,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12706",
        "abs_url": "https://arxiv.org/abs/2511.12706",
        "pdf_url": "https://arxiv.org/pdf/2511.12706",
        "title": "Beyond Fixed Tasks: Unsupervised Environment Design for Task-Level Pairs",
        "authors": [
            "Daniel Furelos-Blanco",
            "Charles Pert",
            "Frederik Kelbel",
            "Alex F. Spies",
            "Alessandra Russo",
            "Michael Dennis"
        ],
        "comments": "Extended version of paper accepted for publication at the 40th AAAI Conference on Artificial Intelligence (AAAI)",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Training general agents to follow complex instructions (tasks) in intricate environments (levels) remains a core challenge in reinforcement learning. Random sampling of task-level pairs often produces unsolvable combinations, highlighting the need to co-design tasks and levels. While unsupervised environment design (UED) has proven effective at automatically designing level curricula, prior work has only considered a fixed task. We present ATLAS (Aligning Tasks and Levels for Autocurricula of Specifications), a novel method that generates joint autocurricula over tasks and levels. Our approach builds upon UED to automatically produce solvable yet challenging task-level pairs for policy training. To evaluate ATLAS and drive progress in the field, we introduce an evaluation suite that models tasks as reward machines in Minigrid levels. Experiments demonstrate that ATLAS vastly outperforms random sampling approaches, particularly when sampling solvable pairs is unlikely. We further show that mutations leveraging the structure of both tasks and levels accelerate convergence to performant policies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 124,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12713",
        "abs_url": "https://arxiv.org/abs/2511.12713",
        "pdf_url": "https://arxiv.org/pdf/2511.12713",
        "title": "Oxytrees: Model Trees for Bipartite Learning",
        "authors": [
            "Pedro Ilídio",
            "Felipe Kenji Nakano",
            "Alireza Gharahighehi",
            "Robbe D'hondt",
            "Ricardo Cerri",
            "Celine Vens"
        ],
        "comments": "7 pages, 6 figures, AAAI Conference on Artificial Intelligence 2026",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Bipartite learning is a machine learning task that aims to predict interactions between pairs of instances. It has been applied to various domains, including drug-target interactions, RNA-disease associations, and regulatory network inference. Despite being widely investigated, current methods still present drawbacks, as they are often designed for a specific application and thus do not generalize to other problems or present scalability issues. To address these challenges, we propose Oxytrees: proxy-based biclustering model trees. Oxytrees compress the interaction matrix into row- and column-wise proxy matrices, significantly reducing training time without compromising predictive performance. We also propose a new leaf-assignment algorithm that significantly reduces the time taken for prediction. Finally, Oxytrees employ linear models using the Kronecker product kernel in their leaves, resulting in shallower trees and thus even faster training. Using 15 datasets, we compared the predictive performance of ensembles of Oxytrees with that of the current state-of-the-art. We achieved up to 30-fold improvement in training times compared to state-of-the-art biclustering forests, while demonstrating competitive or superior performance in most evaluation settings, particularly in the inductive setting. Finally, we provide an intuitive Python API to access all datasets, methods and evaluation measures used in this work, thus enabling reproducible research in this field.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 125,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12722",
        "abs_url": "https://arxiv.org/abs/2511.12722",
        "pdf_url": "https://arxiv.org/pdf/2511.12722",
        "title": "On Robustness of Linear Classifiers to Targeted Data Poisoning",
        "authors": [
            "Nakshatra Gupta",
            "Sumanth Prabhu",
            "Supratik Chakraborty",
            "R Venkatesh"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Data poisoning is a training-time attack that undermines the trustworthiness of learned models. In a targeted data poisoning attack, an adversary manipulates the training dataset to alter the classification of a targeted test point. Given the typically large size of training dataset, manual detection of poisoning is difficult. An alternative is to automatically measure a dataset's robustness against such an attack, which is the focus of this paper. We consider a threat model wherein an adversary can only perturb the labels of the training dataset, with knowledge limited to the hypothesis space of the victim's model. In this setting, we prove that finding the robustness is an NP-Complete problem, even when hypotheses are linear classifiers. To overcome this, we present a technique that finds lower and upper bounds of robustness. Our implementation of the technique computes these bounds efficiently in practice for many publicly available datasets. We experimentally demonstrate the effectiveness of our approach. Specifically, a poisoning exceeding the identified robustness bounds significantly impacts test point classification. We are also able to compute these bounds in many more cases where state-of-the-art techniques fail.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 126,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12723",
        "abs_url": "https://arxiv.org/abs/2511.12723",
        "pdf_url": "https://arxiv.org/pdf/2511.12723",
        "title": "LAYA: Layer-wise Attention Aggregation for Interpretable Depth-Aware Neural Networks",
        "authors": [
            "Gennaro Vessio"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Deep neural networks typically rely on the representation produced by their final hidden layer to make predictions, implicitly assuming that this single vector fully captures the semantics encoded across all preceding transformations. However, intermediate layers contain rich and complementary information -- ranging from low-level patterns to high-level abstractions -- that is often discarded when the decision head depends solely on the last representation. This paper revisits the role of the output layer and introduces LAYA (Layer-wise Attention Aggregator), a novel output head that dynamically aggregates internal representations through attention. Instead of projecting only the deepest embedding, LAYA learns input-conditioned attention weights over layer-wise features, yielding an interpretable and architecture-agnostic mechanism for synthesizing predictions. Experiments on vision and language benchmarks show that LAYA consistently matches or improves the performance of standard output heads, with relative gains of up to about one percentage point in accuracy, while providing explicit layer-attribution scores that reveal how different abstraction levels contribute to each decision. Crucially, these interpretability signals emerge directly from the model's computation, without any external post hoc explanations. The code to reproduce LAYA is publicly available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 127,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12725",
        "abs_url": "https://arxiv.org/abs/2511.12725",
        "pdf_url": "https://arxiv.org/pdf/2511.12725",
        "title": "Convolutional Model Trees",
        "authors": [
            "William Ward Armstrong"
        ],
        "comments": "9 pages. No figures. This paper gives an algorithm for creating a continuously differentiable approximation from sample data from the same type of function(in theory) using a forest of model trees (like CART trees with linear functions instead of constants)",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "A method for creating a forest of model trees to fit samples of a function defined on images is described in several steps: down-sampling the images, determining a tree's hyperplanes, applying convolutions to the hyperplanes to handle small distortions of training images, and creating forests of model trees to increase accuracy and achieve a smooth fit. A 1-to-1 correspondence among pixels of images, coefficients of hyperplanes and coefficients of leaf functions offers the possibility of dealing with larger distortions such as arbitrary rotations or changes of perspective. A theoretical method for smoothing forest outputs to produce a continuously differentiable approximation is described. Within that framework, a training procedure is proved to converge.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 128,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12742",
        "abs_url": "https://arxiv.org/abs/2511.12742",
        "pdf_url": "https://arxiv.org/pdf/2511.12742",
        "title": "Stabilizing Self-Consuming Diffusion Models with Latent Space Filtering",
        "authors": [
            "Zhongteng Cai",
            "Yaxuan Wang",
            "Yang Liu",
            "Xueru Zhang"
        ],
        "comments": "Accepted by AAAI-26",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "As synthetic data proliferates across the Internet, it is often reused to train successive generations of generative models. This creates a ``self-consuming loop\" that can lead to training instability or \\textit{model collapse}. Common strategies to address the issue -- such as accumulating historical training data or injecting fresh real data -- either increase computational cost or require expensive human annotation. In this paper, we empirically analyze the latent space dynamics of self-consuming diffusion models and observe that the low-dimensional structure of latent representations extracted from synthetic data degrade over generations. Based on this insight, we propose \\textit{Latent Space Filtering} (LSF), a novel approach that mitigates model collapse by filtering out less realistic synthetic data from mixed datasets. Theoretically, we present a framework that connects latent space degradation to empirical observations. Experimentally, we show that LSF consistently outperforms existing baselines across multiple real-world datasets, effectively mitigating model collapse without increasing training cost or relying on human annotation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 129,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12745",
        "abs_url": "https://arxiv.org/abs/2511.12745",
        "pdf_url": "https://arxiv.org/pdf/2511.12745",
        "title": "DIVIDE: A Framework for Learning from Independent Multi-Mechanism Data Using Deep Encoders and Gaussian Processes",
        "authors": [
            "Vivek Chawla",
            "Boris Slautin",
            "Utkarsh Pratiush",
            "Dayakar Penumadu",
            "Sergei Kalinin"
        ],
        "comments": "33 pages, 10 main figures, 7 additional in SI",
        "subjects": "Machine Learning (cs.LG); Materials Science (cond-mat.mtrl-sci)",
        "abstract": "Scientific datasets often arise from multiple independent mechanisms such as spatial, categorical or structural effects, whose combined influence obscures their individual contributions. We introduce DIVIDE, a framework that disentangles these influences by integrating mechanism-specific deep encoders with a structured Gaussian Process in a joint latent space. Disentanglement here refers to separating independently acting generative factors. The encoders isolate distinct mechanisms while the Gaussian Process captures their combined effect with calibrated uncertainty. The architecture supports structured priors, enabling interpretable and mechanism-aware prediction as well as efficient active learning. DIVIDE is demonstrated on synthetic datasets combining categorical image patches with nonlinear spatial fields, on FerroSIM spin lattice simulations of ferroelectric patterns, and on experimental PFM hysteresis loops from PbTiO3 films. Across benchmarks, DIVIDE separates mechanisms, reproduces additive and scaled interactions, and remains robust under noise. The framework extends naturally to multifunctional datasets where mechanical, electromagnetic or optical responses coexist.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 130,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12760",
        "abs_url": "https://arxiv.org/abs/2511.12760",
        "pdf_url": "https://arxiv.org/pdf/2511.12760",
        "title": "Conformal Online Learning of Deep Koopman Linear Embeddings",
        "authors": [
            "Ben Gao",
            "Jordan Patracone",
            "Stéphane Chrétien",
            "Olivier Alata"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "We introduce Conformal Online Learning of Koopman embeddings (COLoKe), a novel framework for adaptively updating Koopman-invariant representations of nonlinear dynamical systems from streaming data. Our modeling approach combines deep feature learning with multistep prediction consistency in the lifted space, where the dynamics evolve linearly. To prevent overfitting, COLoKe employs a conformal-style mechanism that shifts the focus from evaluating the conformity of new states to assessing the consistency of the current Koopman model. Updates are triggered only when the current model's prediction error exceeds a dynamically calibrated threshold, allowing selective refinement of the Koopman operator and embedding. Empirical results on benchmark dynamical systems demonstrate the effectiveness of COLoKe in maintaining long-term predictive accuracy while significantly reducing unnecessary updates and avoiding overfitting.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 131,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12764",
        "abs_url": "https://arxiv.org/abs/2511.12764",
        "pdf_url": "https://arxiv.org/pdf/2511.12764",
        "title": "INC: An Indirect Neural Corrector for Auto-Regressive Hybrid PDE Solvers",
        "authors": [
            "Hao Wei",
            "Aleksandra Franz",
            "Bjoern List",
            "Nils Thuerey"
        ],
        "comments": "Accepted at NeurIPS 2025. 35 pages, 10 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "When simulating partial differential equations, hybrid solvers combine coarse numerical solvers with learned correctors. They promise accelerated simulations while adhering to physical constraints. However, as shown in our theoretical framework, directly applying learned corrections to solver outputs leads to significant autoregressive errors, which originate from amplified perturbations that accumulate during long-term rollouts, especially in chaotic regimes. To overcome this, we propose the Indirect Neural Corrector (\\(\\mathrm{INC}\\)), which integrates learned corrections into the governing equations rather than applying direct state updates. Our key insight is that \\(\\mathrm{INC}\\) reduces the error amplification on the order of \\(\\Delta t^{-1} + L\\), where \\(\\Delta t\\) is the timestep and $L$ the Lipschitz constant. At the same time, our framework poses no architectural requirements and integrates seamlessly with arbitrary neural networks and solvers. We test \\(\\mathrm{INC}\\) in extensive benchmarks, covering numerous differentiable solvers, neural backbones, and test cases ranging from a 1D chaotic system to 3D turbulence. INC improves the long-term trajectory performance (\\(R^2\\)) by up to 158.7\\%, stabilizes blowups under aggressive coarsening, and for complex 3D turbulence cases yields speed-ups of several orders of magnitude. INC thus enables stable, efficient PDE emulation with formal error reduction, paving the way for faster scientific and engineering simulations with reliable physics guarantees. Our source code is available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 132,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12770",
        "abs_url": "https://arxiv.org/abs/2511.12770",
        "pdf_url": "https://arxiv.org/pdf/2511.12770",
        "title": "MolEdit: Knowledge Editing for Multimodal Molecule Language Models",
        "authors": [
            "Zhenyu Lei",
            "Patrick Soga",
            "Yaochen Zhu",
            "Yinhan He",
            "Yushun Dong",
            "Jundong Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computational Engineering, Finance, and Science (cs.CE)",
        "abstract": "Understanding and continuously refining multimodal molecular knowledge is crucial for advancing biomedicine, chemistry, and materials science. Molecule language models (MoLMs) have become powerful tools in these domains, integrating structural representations (e.g., SMILES strings, molecular graphs) with rich contextual descriptions (e.g., physicochemical properties). However, MoLMs can encode and propagate inaccuracies due to outdated web-mined training corpora or malicious manipulation, jeopardizing downstream discovery pipelines. While knowledge editing has been explored for general-domain AI, its application to MoLMs remains uncharted, presenting unique challenges due to the multifaceted and interdependent nature of molecular knowledge. In this paper, we take the first step toward MoLM editing for two critical tasks: molecule-to-caption generation and caption-to-molecule generation. To address molecule-specific challenges, we propose MolEdit, a powerful framework that enables targeted modifications while preserving unrelated molecular knowledge. MolEdit combines a Multi-Expert Knowledge Adapter that routes edits to specialized experts for different molecular facets with an Expertise-Aware Editing Switcher that activates the adapters only when input closely matches the stored edits across all expertise, minimizing interference with unrelated knowledge. To systematically evaluate editing performance, we introduce MEBench, a comprehensive benchmark assessing multiple dimensions, including Reliability (accuracy of the editing), Locality (preservation of irrelevant knowledge), and Generality (robustness to reformed queries). Across extensive experiments on two popular MoLM backbones, MolEdit delivers up to 18.8% higher Reliability and 12.0% better Locality than baselines while maintaining efficiency. The code is available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 133,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12788",
        "abs_url": "https://arxiv.org/abs/2511.12788",
        "pdf_url": "https://arxiv.org/pdf/2511.12788",
        "title": "Physics-Constrained Adaptive Neural Networks Enable Real-Time Semiconductor Manufacturing Optimization with Minimal Training Data",
        "authors": [
            "Rubén Darío Guerrero"
        ],
        "comments": "32 pages, 21 figures, 10 tables",
        "subjects": "Machine Learning (cs.LG); Hardware Architecture (cs.AR); Optimization and Control (math.OC)",
        "abstract": "The semiconductor industry faces a computational crisis in extreme ultraviolet (EUV) lithography optimization, where traditional methods consume billions of CPU hours while failing to achieve sub-nanometer precision. We present a physics-constrained adaptive learning framework that automatically calibrates electromagnetic approximations through learnable parameters $\\boldsymbol{\\theta} = \\{\\theta_d, \\theta_a, \\theta_b, \\theta_p, \\theta_c\\}$ while simultaneously minimizing Edge Placement Error (EPE) between simulated aerial images and target photomasks. The framework integrates differentiable modules for Fresnel diffraction, material absorption, optical point spread function blur, phase-shift effects, and contrast modulation with direct geometric pattern matching objectives, enabling cross-geometry generalization with minimal training data. Through physics-constrained learning on 15 representative patterns spanning current production to future research nodes, we demonstrate consistent sub-nanometer EPE performance (0.664-2.536 nm range) using only 50 training samples per pattern. Adaptive physics learning achieves an average improvement of 69.9\\% over CNN baselines without physics constraints, with a significant inference speedup over rigorous electromagnetic solvers after training completion. This approach requires 90\\% fewer training samples through cross-geometry generalization compared to pattern-specific CNN training approaches. This work establishes physics-constrained adaptive learning as a foundational methodology for real-time semiconductor manufacturing optimization, addressing the critical gap between academic physics-informed neural networks and industrial deployment requirements through joint physics calibration and manufacturing precision objectives.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 134,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12797",
        "abs_url": "https://arxiv.org/abs/2511.12797",
        "pdf_url": "https://arxiv.org/pdf/2511.12797",
        "title": "Genomic Next-Token Predictors are In-Context Learners",
        "authors": [
            "Nathan Breslow",
            "Aayush Mishra",
            "Mahler Revsine",
            "Michael C. Schatz",
            "Anqi Liu",
            "Daniel Khashabi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Genomics (q-bio.GN)",
        "abstract": "In-context learning (ICL) -- the capacity of a model to infer and apply abstract patterns from examples provided within its input -- has been extensively studied in large language models trained for next-token prediction on human text. In fact, prior work often attributes this emergent behavior to distinctive statistical properties in human language. This raises a fundamental question: can ICL arise organically in other sequence domains purely through large-scale predictive training? To explore this, we turn to genomic sequences, an alternative symbolic domain rich in statistical structure. Specifically, we study the Evo2 genomic model, trained predominantly on next-nucleotide (A/T/C/G) prediction, at a scale comparable to mid-sized LLMs. We develop a controlled experimental framework comprising symbolic reasoning tasks instantiated in both linguistic and genomic forms, enabling direct comparison of ICL across genomic and linguistic models. Our results show that genomic models, like their linguistic counterparts, exhibit log-linear gains in pattern induction as the number of in-context demonstrations increases. To the best of our knowledge, this is the first evidence of organically emergent ICL in genomic sequences, supporting the hypothesis that ICL arises as a consequence of large-scale predictive modeling over rich data. These findings extend emergent meta-learning beyond language, pointing toward a unified, modality-agnostic view of in-context learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 135,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12808",
        "abs_url": "https://arxiv.org/abs/2511.12808",
        "pdf_url": "https://arxiv.org/pdf/2511.12808",
        "title": "Expressive Temporal Specifications for Reward Monitoring",
        "authors": [
            "Omar Adalat",
            "Francesco Belardinelli"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Logic in Computer Science (cs.LO)",
        "abstract": "Specifying informative and dense reward functions remains a pivotal challenge in Reinforcement Learning, as it directly affects the efficiency of agent training. In this work, we harness the expressive power of quantitative Linear Temporal Logic on finite traces (($\\text{LTL}_f[\\mathcal{F}]$)) to synthesize reward monitors that generate a dense stream of rewards for runtime-observable state trajectories. By providing nuanced feedback during training, these monitors guide agents toward optimal behaviour and help mitigate the well-known issue of sparse rewards under long-horizon decision making, which arises under the Boolean semantics dominating the current literature. Our framework is algorithm-agnostic and only relies on a state labelling function, and naturally accommodates specifying non-Markovian properties. Empirical results show that our quantitative monitors consistently subsume and, depending on the environment, outperform Boolean monitors in maximizing a quantitative measure of task completion and in reducing convergence time.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 136,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12817",
        "abs_url": "https://arxiv.org/abs/2511.12817",
        "pdf_url": "https://arxiv.org/pdf/2511.12817",
        "title": "Assessing Automated Fact-Checking for Medical LLM Responses with Knowledge Graphs",
        "authors": [
            "Shasha Zhou",
            "Mingyu Huang",
            "Jack Cole",
            "Charles Britton",
            "Ming Yin",
            "Jan Wolber",
            "Ke Li"
        ],
        "comments": "Accepted as a conference paper at AAAI'26",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The recent proliferation of large language models (LLMs) holds the potential to revolutionize healthcare, with strong capabilities in diverse medical tasks. Yet, deploying LLMs in high-stakes healthcare settings requires rigorous verification and validation to understand any potential harm. This paper investigates the reliability and viability of using medical knowledge graphs (KGs) for the automated factuality evaluation of LLM-generated responses. To ground this investigation, we introduce FAITH, a framework designed to systematically probe the strengths and limitations of this KG-based approach. FAITH operates without reference answers by decomposing responses into atomic claims, linking them to a medical KG, and scoring them based on evidence paths. Experiments on diverse medical tasks with human subjective evaluations demonstrate that KG-grounded evaluation achieves considerably higher correlations with clinician judgments and can effectively distinguish LLMs with varying capabilities. It is also robust to textual variances. The inherent explainability of its scoring can further help users understand and mitigate the limitations of current LLMs. We conclude that while limitations exist, leveraging KGs is a prominent direction for automated factuality assessment in healthcare.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 137,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12829",
        "abs_url": "https://arxiv.org/abs/2511.12829",
        "pdf_url": "https://arxiv.org/pdf/2511.12829",
        "title": "An Evaluation of Representation Learning Methods in Particle Physics Foundation Models",
        "authors": [
            "Michael Chen",
            "Raghav Kansal",
            "Abhijith Gandrakota",
            "Zichun Hao",
            "Jennifer Ngadiuba",
            "Maria Spiropulu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We present a systematic evaluation of representation learning objectives for particle physics within a unified framework. Our study employs a shared transformer-based particle-cloud encoder with standardized preprocessing, matched sampling, and a consistent evaluation protocol on a jet classification dataset. We compare contrastive (supervised and self-supervised), masked particle modeling, and generative reconstruction objectives under a common training regimen. In addition, we introduce targeted supervised architectural modifications that achieve state-of-the-art performance on benchmark evaluations. This controlled comparison isolates the contributions of the learning objective, highlights their respective strengths and limitations, and provides reproducible baselines. We position this work as a reference point for the future development of foundation models in particle physics, enabling more transparent and robust progress across the community.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 138,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12846",
        "abs_url": "https://arxiv.org/abs/2511.12846",
        "pdf_url": "https://arxiv.org/pdf/2511.12846",
        "title": "RoS-Guard: Robust and Scalable Online Change Detection with Delay-Optimal Guarantees",
        "authors": [
            "Zelin Zhu",
            "Yancheng Huang",
            "Kai Yang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Online change detection (OCD) aims to rapidly identify change points in streaming data and is critical in applications such as power system monitoring, wireless network sensing, and financial anomaly detection. Existing OCD methods typically assume precise system knowledge, which is unrealistic due to estimation errors and environmental variations. Moreover, existing OCD methods often struggle with efficiency in large-scale systems. To overcome these challenges, we propose RoS-Guard, a robust and optimal OCD algorithm tailored for linear systems with uncertainty. Through a tight relaxation and reformulation of the OCD optimization problem, RoS-Guard employs neural unrolling to enable efficient parallel computation via GPU acceleration. The algorithm provides theoretical guarantees on performance, including expected false alarm rate and worst-case average detection delay. Extensive experiments validate the effectiveness of RoS-Guard and demonstrate significant computational speedup in large-scale system scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 139,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12865",
        "abs_url": "https://arxiv.org/abs/2511.12865",
        "pdf_url": "https://arxiv.org/pdf/2511.12865",
        "title": "An approach of deep reinforcement learning for maximizing the net present value of stochastic projects",
        "authors": [
            "Wei Xu",
            "Fan Yang",
            "Qinyuan Cui",
            "Zhi Chen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "This paper investigates a project with stochastic activity durations and cash flows under discrete scenarios, where activities must satisfy precedence constraints generating cash inflows and outflows. The objective is to maximize expected net present value (NPV) by accelerating inflows and deferring outflows. We formulate the problem as a discrete-time Markov Decision Process (MDP) and propose a Double Deep Q-Network (DDQN) approach. Comparative experiments demonstrate that DDQN outperforms traditional rigid and dynamic strategies, particularly in large-scale or highly uncertain environments, exhibiting superior computational capability, policy reliability, and adaptability. Ablation studies further reveal that the dual-network architecture mitigates overestimation of action values, while the target network substantially improves training convergence and robustness. These results indicate that DDQN not only achieves higher expected NPV in complex project optimization but also provides a reliable framework for stable and effective policy implementation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 140,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12881",
        "abs_url": "https://arxiv.org/abs/2511.12881",
        "pdf_url": "https://arxiv.org/pdf/2511.12881",
        "title": "On the Information Processing of One-Dimensional Wasserstein Distances with Finite Samples",
        "authors": [
            "Cheongjae Jang",
            "Jonghyun Won",
            "Soyeon Jun",
            "Chun Kee Chung",
            "Keehyoung Joo",
            "Yung-Kyun Noh"
        ],
        "comments": "Extended version of paper accepted to AAAI 2026. 18 pages, 12 figures",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Leveraging the Wasserstein distance -- a summation of sample-wise transport distances in data space -- is advantageous in many applications for measuring support differences between two underlying density functions. However, when supports significantly overlap while densities exhibit substantial pointwise differences, it remains unclear whether and how this transport information can accurately identify these differences, particularly their analytic characterization in finite-sample settings. We address this issue by conducting an analysis of the information processing capabilities of the one-dimensional Wasserstein distance with finite samples. By utilizing the Poisson process and isolating the rate factor, we demonstrate the capability of capturing the pointwise density difference with Wasserstein distances and how this information harmonizes with support differences. The analyzed properties are confirmed using neural spike train decoding and amino acid contact frequency data. The results reveal that the one-dimensional Wasserstein distance highlights meaningful density differences related to both rate and support.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 141,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12890",
        "abs_url": "https://arxiv.org/abs/2511.12890",
        "pdf_url": "https://arxiv.org/pdf/2511.12890",
        "title": "Method of Manufactured Learning for Solver-free Training of Neural Operators",
        "authors": [
            "Arth Sojitra",
            "Omer San"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Training neural operators to approximate mappings between infinite-dimensional function spaces often requires extensive datasets generated by either demanding experimental setups or computationally expensive numerical solvers. This dependence on solver-based data limits scalability and constrains exploration across physical systems. Here we introduce the Method of Manufactured Learning (MML), a solver-independent framework for training neural operators using analytically constructed, physics-consistent datasets. Inspired by the classical method of manufactured solutions, MML replaces numerical data generation with functional synthesis, i.e., smooth candidate solutions are sampled from controlled analytical spaces, and the corresponding forcing fields are derived by direct application of the governing differential operators. During inference, setting these forcing terms to zero restores the original governing equations, allowing the trained neural operator to emulate the true solution operator of the system. The framework is agnostic to network architecture and can be integrated with any operator learning paradigm. In this paper, we employ Fourier neural operator as a representative example. Across canonical benchmarks including heat, advection, Burgers, and diffusion-reaction equations. MML achieves high spectral accuracy, low residual errors, and strong generalization to unseen conditions. By reframing data generation as a process of analytical synthesis, MML offers a scalable, solver-agnostic pathway toward constructing physically grounded neural operators that retain fidelity to governing laws without reliance on expensive numerical simulations or costly experimental data for training.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 142,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12898",
        "abs_url": "https://arxiv.org/abs/2511.12898",
        "pdf_url": "https://arxiv.org/pdf/2511.12898",
        "title": "Functional Mean Flow in Hilbert Space",
        "authors": [
            "Zhiqi Li",
            "Yuchen Sun",
            "Greg Turk",
            "Bo Zhu"
        ],
        "comments": "29 pages, 13 figures",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We present Functional Mean Flow (FMF) as a one-step generative model defined in infinite-dimensional Hilbert space. FMF extends the one-step Mean Flow framework to functional domains by providing a theoretical formulation for Functional Flow Matching and a practical implementation for efficient training and sampling. We also introduce an $x_1$-prediction variant that improves stability over the original $u$-prediction form. The resulting framework is a practical one-step Flow Matching method applicable to a wide range of functional data generation tasks such as time series, images, PDEs, and 3D geometry.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 143,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12905",
        "abs_url": "https://arxiv.org/abs/2511.12905",
        "pdf_url": "https://arxiv.org/pdf/2511.12905",
        "title": "LinkedIn Profile Characteristics and Professional Success Indicators",
        "authors": [
            "Tania-Amanda Fredrick Eneye",
            "Ashlesha Malla",
            "Pawan Paudel"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "This study explores the relationship between LinkedIn profile characteristics and professional success, focusing on the indicators of promotions, follower count, and career progression rate. By leveraging a dataset of over 62,000 anonymized LinkedIn profiles, we developed predictive models using machine learning techniques to identify the most influential factors driving professional success. Results indicate that while promotions are highly predictable, follower growth exhibits greater complexity. This research provides actionable insights for professionals seeking to optimize their LinkedIn presence and career strategies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 144,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12934",
        "abs_url": "https://arxiv.org/abs/2511.12934",
        "pdf_url": "https://arxiv.org/pdf/2511.12934",
        "title": "AIF: Asynchronous Inference Framework for Cost-Effective Pre-Ranking",
        "authors": [
            "Zhi Kou",
            "Xiang-Rong Sheng",
            "Shuguang Han",
            "Zhishan Zhao",
            "Yueyao Cheng",
            "Han Zhu",
            "Jian Xu",
            "Bo Zheng"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Information Retrieval (cs.IR)",
        "abstract": "In industrial recommendation systems, pre-ranking models based on deep neural networks (DNNs) commonly adopt a sequential execution framework: feature fetching and model forward computation are triggered only after receiving candidates from the upstream retrieval stage. This design introduces inherent bottlenecks, including redundant computations of identical users/items and increased latency due to strictly sequential operations, which jointly constrain the model's capacity and system efficiency. To address these limitations, we propose the Asynchronous Inference Framework (AIF), a cost-effective computational architecture that decouples interaction-independent components, those operating within a single user or item, from real-time prediction. AIF reorganizes the model inference process by performing user-side computations in parallel with the retrieval stage and conducting item-side computations in a nearline manner. This means that interaction-independent components are calculated just once and completed before the real-time prediction phase of the pre-ranking stage. As a result, AIF enhances computational efficiency and reduces latency, freeing up resources to significantly improve the feature set and model architecture of interaction-independent components. Moreover, we delve into model design within the AIF framework, employing approximated methods for interaction-dependent components in online real-time predictions. By co-designing both the framework and the model, our solution achieves notable performance gains without significantly increasing computational and latency costs. This has enabled the successful deployment of AIF in the Taobao display advertising system.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 145,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12945",
        "abs_url": "https://arxiv.org/abs/2511.12945",
        "pdf_url": "https://arxiv.org/pdf/2511.12945",
        "title": "APT: Affine Prototype-Timestamp For Time Series Forecasting Under Distribution Shift",
        "authors": [
            "Yujie Li",
            "Zezhi Shao",
            "Chengqing Yu",
            "Yisong Fu",
            "Tao Sun",
            "Yongjun Xu",
            "Fei Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Time series forecasting under distribution shift remains challenging, as existing deep learning models often rely on local statistical normalization (e.g., mean and variance) that fails to capture global distribution shift. Methods like RevIN and its variants attempt to decouple distribution and pattern but still struggle with missing values, noisy observations, and invalid channel-wise affine transformation. To address these limitations, we propose Affine Prototype Timestamp (APT), a lightweight and flexible plug-in module that injects global distribution features into the normalization-forecasting pipeline. By leveraging timestamp conditioned prototype learning, APT dynamically generates affine parameters that modulate both input and output series, enabling the backbone to learn from self-supervised, distribution-aware clustered instances. APT is compatible with arbitrary forecasting backbones and normalization strategies while introducing minimal computational overhead. Extensive experiments across six benchmark datasets and multiple backbone-normalization combinations demonstrate that APT significantly improves forecasting performance under distribution shift.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 146,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12951",
        "abs_url": "https://arxiv.org/abs/2511.12951",
        "pdf_url": "https://arxiv.org/pdf/2511.12951",
        "title": "A FEDformer-Based Hybrid Framework for Anomaly Detection and Risk Forecasting in Financial Time Series",
        "authors": [
            "Ziling Fan",
            "Ruijia Liang",
            "Yiwen Hu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Financial markets are inherently volatile and prone to sudden disruptions such as market crashes, flash collapses, and liquidity crises. Accurate anomaly detection and early risk forecasting in financial time series are therefore crucial for preventing systemic instability and supporting informed investment decisions. Traditional deep learning models, such as LSTM and GRU, often fail to capture long-term dependencies and complex periodic patterns in highly nonstationary financial data. To address this limitation, this study proposes a FEDformer-Based Hybrid Framework for Anomaly Detection and Risk Forecasting in Financial Time Series, which integrates the Frequency Enhanced Decomposed Transformer (FEDformer) with a residual-based anomaly detector and a risk forecasting head. The FEDformer module models temporal dynamics in both time and frequency domains, decomposing signals into trend and seasonal components for improved interpretability. The residual-based detector identifies abnormal fluctuations by analyzing prediction errors, while the risk head predicts potential financial distress using learned latent embeddings. Experiments conducted on the S&P 500, NASDAQ Composite, and Brent Crude Oil datasets (2000-2024) demonstrate the superiority of the proposed model over benchmark methods, achieving a 15.7 percent reduction in RMSE and an 11.5 percent improvement in F1-score for anomaly detection. These results confirm the effectiveness of the model in capturing financial volatility, enabling reliable early-warning systems for market crash prediction and risk management.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 147,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12979",
        "abs_url": "https://arxiv.org/abs/2511.12979",
        "pdf_url": "https://arxiv.org/pdf/2511.12979",
        "title": "RAGPulse: An Open-Source RAG Workload Trace to Optimize RAG Serving Systems",
        "authors": [
            "Zhengchao Wang",
            "Yitao Hu",
            "Jianing Ye",
            "Zhuxuan Chang",
            "Jiazheng Yu",
            "Youpeng Deng",
            "Keqiu Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Databases (cs.DB)",
        "abstract": "Retrieval-Augmented Generation (RAG) is a critical paradigm for building reliable, knowledge-intensive Large Language Model (LLM) applications. However, the multi-stage pipeline (retrieve, generate) and unique workload characteristics (e.g., knowledge dependency) of RAG systems pose significant challenges for serving performance optimization. Existing generic LLM inference traces fail to capture these RAG-specific dynamics, creating a significant performance gap between academic research and real-world deployment. To bridge this gap, this paper introduces RAGPulse, an open-source RAG workload trace dataset. This dataset was collected from an university-wide Q&A system serving that has served more than 40,000 students and faculties since April 2024. We detail RAGPulse's system architecture, its privacy-preserving hash-based data format, and provide an in-depth statistical analysis. Our analysis reveals that real-world RAG workloads exhibit significant temporal locality and a highly skewed hot document access pattern. RAGPulse provides a high-fidelity foundation for researchers to develop and validate novel optimization strategies for RAG systems, such as content-aware batching and retrieval caching, ultimately enhancing the efficiency and reliability of RAG services. The code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 148,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12985",
        "abs_url": "https://arxiv.org/abs/2511.12985",
        "pdf_url": "https://arxiv.org/pdf/2511.12985",
        "title": "Angular Gradient Sign Method: Uncovering Vulnerabilities in Hyperbolic Networks",
        "authors": [
            "Minsoo Jo",
            "Dongyoon Yang",
            "Taesup Kim"
        ],
        "comments": "Accepted by AAAI 2026",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Adversarial examples in neural networks have been extensively studied in Euclidean geometry, but recent advances in \\textit{hyperbolic networks} call for a reevaluation of attack strategies in non-Euclidean geometries. Existing methods such as FGSM and PGD apply perturbations without regard to the underlying hyperbolic structure, potentially leading to inefficient or geometrically inconsistent attacks. In this work, we propose a novel adversarial attack that explicitly leverages the geometric properties of hyperbolic space. Specifically, we compute the gradient of the loss function in the tangent space of hyperbolic space, decompose it into a radial (depth) component and an angular (semantic) component, and apply perturbation derived solely from the angular direction. Our method generates adversarial examples by focusing perturbations in semantically sensitive directions encoded in angular movement within the hyperbolic geometry. Empirical results on image classification, cross-modal retrieval tasks and network architectures demonstrate that our attack achieves higher fooling rates than conventional adversarial attacks, while producing high-impact perturbations with deeper insights into vulnerabilities of hyperbolic embeddings. This work highlights the importance of geometry-aware adversarial strategies in curved representation spaces and provides a principled framework for attacking hierarchical embeddings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 149,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13010",
        "abs_url": "https://arxiv.org/abs/2511.13010",
        "pdf_url": "https://arxiv.org/pdf/2511.13010",
        "title": "Are Graph Transformers Necessary? Efficient Long-Range Message Passing with Fractal Nodes in MPNNs",
        "authors": [
            "Jeongwhan Choi",
            "Seungjun Park",
            "Sumin Park",
            "Sung-Bae Cho",
            "Noseong Park"
        ],
        "comments": "Accepted in AAAI 2026 for Oral Representation. This is the extended version including the appendix",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Graph Neural Networks (GNNs) have emerged as powerful tools for learning on graph-structured data, but often struggle to balance local and global information. While graph Transformers aim to address this by enabling long-range interactions, they often overlook the inherent locality and efficiency of Message Passing Neural Networks (MPNNs). We propose a new concept called fractal nodes, inspired by the fractal structure observed in real-world networks. Our approach is based on the intuition that graph partitioning naturally induces fractal structure, where subgraphs often reflect the connectivity patterns of the full graph. Fractal nodes are designed to coexist with the original nodes and adaptively aggregate subgraph-level feature representations, thereby enforcing feature similarity within each subgraph. We show that fractal nodes alleviate the over-squashing problem by providing direct shortcut connections that enable long-range propagation of subgraph-level representations. Experiment results show that our method improves the expressive power of MPNNs and achieves comparable or better performance to graph Transformers while maintaining the computational efficiency of MPNN by improving the long-range dependencies of MPNN.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 150,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13016",
        "abs_url": "https://arxiv.org/abs/2511.13016",
        "pdf_url": "https://arxiv.org/pdf/2511.13016",
        "title": "The Good, The Bad, and The Hybrid: A Reward Structure Showdown in Reasoning Models Training",
        "authors": [
            "Subramanyam Sahoo"
        ],
        "comments": "Paper accepted to the 2nd Workshop on Aligning Reinforcement Learning Experimentalists and Theorists (ARLET 2025) at NeurIPS; the paper consists of 14 pages (including the appendix) and contains 3 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Reward design is central to reinforcement learning from human feedback (RLHF) and alignment research. In this work, we propose a unified framework to study hard, continuous, and hybrid reward structures for fine-tuning large language models (LLMs) on mathematical reasoning tasks. Using Qwen3-4B with LoRA fine-tuning on the GSM8K dataset, we formalize and empirically evaluate reward formulations that incorporate correctness, perplexity, reasoning quality, and consistency. We introduce an adaptive hybrid reward scheduler that transitions between discrete and continuous signals, balancing exploration and stability. Our results show that hybrid reward structures improve convergence speed and training stability over purely hard or continuous approaches, offering insights for alignment via adaptive reward modeling.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 151,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13018",
        "abs_url": "https://arxiv.org/abs/2511.13018",
        "pdf_url": "https://arxiv.org/pdf/2511.13018",
        "title": "The Final-Stage Bottleneck: A Systematic Dissection of the R-Learner for Network Causal Inference",
        "authors": [
            "Sairam S",
            "Sara Girdhar",
            "Shivam Soni"
        ],
        "comments": "15 pages, 4 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The R-Learner is a powerful, theoretically-grounded framework for estimating heterogeneous treatment effects, prized for its robustness to nuisance model errors. However, its application to network data, where causal heterogeneity is often graph-dependent, presents a critical challenge to its core assumption of a well-specified final-stage model. In this paper, we conduct a large-scale empirical study to systematically dissect the R-Learner framework on graphs. We provide the first rigorous evidence that the primary driver of performance is the inductive bias of the final-stage CATE estimator, an effect that dominates the choice of nuisance models. Our central finding is the quantification of a catastrophic \"representation bottleneck\": we prove with overwhelming statistical significance (p < 0.001) that R-Learners with a graph-blind final stage fail completely (MSE > 4.0), even when paired with powerful GNN nuisance models. Conversely, our proposed end-to-end Graph R-Learner succeeds and significantly outperforms a strong, non-DML GNN T-Learner baseline. Furthermore, we identify and provide a mechanistic explanation for a subtle, topology-dependent \"nuisance bottleneck,\" linking it to GNN over-squashing via a targeted \"Hub-Periphery Trade-off\" analysis. Our findings are validated across diverse synthetic and semi-synthetic benchmarks. We release our code as a reproducible benchmark to facilitate future research on this critical \"final-stage bottleneck.\"",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 152,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13022",
        "abs_url": "https://arxiv.org/abs/2511.13022",
        "pdf_url": "https://arxiv.org/pdf/2511.13022",
        "title": "Learning Time-Scale Invariant Population-Level Neural Representations",
        "authors": [
            "Eshani Patel",
            "Yisong Yue",
            "Geeling Chau"
        ],
        "comments": "10 pages, 5 figures, NeurIPS 2025 Foundation Models for the Brain and Body",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "General-purpose foundation models for neural time series can help accelerate neuroscientific discoveries and enable applications such as brain computer interfaces (BCIs). A key component in scaling these models is population-level representation learning, which leverages information across channels to capture spatial as well as temporal structure. Population-level approaches have recently shown that such representations can be both efficient to learn on top of pretrained temporal encoders and produce useful representations for decoding a variety of downstream tasks. However, these models remain sensitive to mismatches in preprocessing, particularly on time-scales, between pretraining and downstream settings. We systematically examine how time-scale mismatches affects generalization and find that existing representations lack invariance. To address this, we introduce Time-scale Augmented Pretraining (TSAP), which consistently improves robustness to different time-scales across decoding tasks and builds invariance in the representation space. These results highlight handling preprocessing diversity as a key step toward building generalizable neural foundation models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 153,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13035",
        "abs_url": "https://arxiv.org/abs/2511.13035",
        "pdf_url": "https://arxiv.org/pdf/2511.13035",
        "title": "One-Step Generative Policies with Q-Learning: A Reformulation of MeanFlow",
        "authors": [
            "Zeyuan Wang",
            "Da Li",
            "Yulin Chen",
            "Ye Shi",
            "Liang Bai",
            "Tianyuan Yu",
            "Yanwei Fu"
        ],
        "comments": "Accepted in AAAI 2026 Poster",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "We introduce a one-step generative policy for offline reinforcement learning that maps noise directly to actions via a residual reformulation of MeanFlow, making it compatible with Q-learning. While one-step Gaussian policies enable fast inference, they struggle to capture complex, multimodal action distributions. Existing flow-based methods improve expressivity but typically rely on distillation and two-stage training when trained with Q-learning. To overcome these limitations, we propose to reformulate MeanFlow to enable direct noise-to-action generation by integrating the velocity field and noise-to-action transformation into a single policy network-eliminating the need for separate velocity estimation. We explore several reformulation variants and identify an effective residual formulation that supports expressive and stable policy learning. Our method offers three key advantages: 1) efficient one-step noise-to-action generation, 2) expressive modelling of multimodal action distributions, and 3) efficient and stable policy learning via Q-learning in a single-stage training setup. Extensive experiments on 73 tasks across the OGBench and D4RL benchmarks demonstrate that our method achieves strong performance in both offline and offline-to-online reinforcement learning settings. Code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 154,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13044",
        "abs_url": "https://arxiv.org/abs/2511.13044",
        "pdf_url": "https://arxiv.org/pdf/2511.13044",
        "title": "Bi-View Embedding Fusion: A Hybrid Learning Approach for Knowledge Graph's Nodes Classification Addressing Problems with Limited Data",
        "authors": [
            "Rosario Napoli",
            "Giovanni Lonia",
            "Antonio Celesti",
            "Massimo Villari",
            "Maria Fazio"
        ],
        "comments": "Accepted at the 14th International Joint Conference on Knowledge Graphs (IJCKG) 2025",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Traditional Machine Learning (ML) methods require large amounts of data to perform well, limiting their applicability in sparse or incomplete scenarios and forcing the usage of additional synthetic data to improve the model training. To overcome this challenge, the research community is looking more and more at Graph Machine Learning (GML) as it offers a powerful alternative by using relationships within data. However, this method also faces limitations, particularly when dealing with Knowledge Graphs (KGs), which can hide huge information due to their semantic nature. This study introduces Bi-View, a novel hybrid approach that increases the informative content of node features in KGs to generate enhanced Graph Embeddings (GEs) that are used to improve GML models without relying on additional synthetic data. The proposed work combines two complementary GE techniques: Node2Vec, which captures structural patterns through unsupervised random walks, and GraphSAGE, which aggregates neighbourhood information in a supervised way. Node2Vec embeddings are first computed to represent the graph topology, and node features are then enriched with centrality-based metrics, which are used as input for the GraphSAGE model. Moreover, a fusion layer combines the original Node2Vec embeddings with the GraphSAGE-influenced representations, resulting in a dual-perspective embedding space. Such a fusion captures both topological and semantic properties of the graph, enabling the model to exploit informative features that may exist in the dataset but that are not explicitly represented. Our approach improves downstream task performance, especially in scenarios with poor initial features, giving the basis for more accurate and precise KG-enanched GML models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 155,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13049",
        "abs_url": "https://arxiv.org/abs/2511.13049",
        "pdf_url": "https://arxiv.org/pdf/2511.13049",
        "title": "Generalization Bounds for Semi-supervised Matrix Completion with Distributional Side Information",
        "authors": [
            "Antoine Ledent",
            "Mun Chong Soo",
            "Nong Minh Hieu"
        ],
        "comments": "Accepted at AAAI 2026",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "We study a matrix completion problem where both the ground truth $R$ matrix and the unknown sampling distribution $P$ over observed entries are low-rank matrices, and \\textit{share a common subspace}. We assume that a large amount $M$ of \\textit{unlabeled} data drawn from the sampling distribution $P$ is available, together with a small amount $N$ of labeled data drawn from the same distribution and noisy estimates of the corresponding ground truth entries. This setting is inspired by recommender systems scenarios where the unlabeled data corresponds to `implicit feedback' (consisting in interactions such as purchase, click, etc. ) and the labeled data corresponds to the `explicit feedback', consisting of interactions where the user has given an explicit rating to the item. Leveraging powerful results from the theory of low-rank subspace recovery, together with classic generalization bounds for matrix completion models, we show error bounds consisting of a sum of two error terms scaling as $\\widetilde{O}\\left(\\sqrt{\\frac{nd}{M}}\\right)$ and $\\widetilde{O}\\left(\\sqrt{\\frac{dr}{N}}\\right)$ respectively, where $d$ is the rank of $P$ and $r$ is the rank of $M$. In synthetic experiments, we confirm that the true generalization error naturally splits into independent error terms corresponding to the estimations of $P$ and and the ground truth matrix $\\ground$ respectively. In real-life experiments on Douban and MovieLens with most explicit ratings removed, we demonstrate that the method can outperform baselines relying only on the explicit ratings, demonstrating that our assumptions provide a valid toy theoretical setting to study the interaction between explicit and implicit feedbacks in recommender systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 156,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13053",
        "abs_url": "https://arxiv.org/abs/2511.13053",
        "pdf_url": "https://arxiv.org/pdf/2511.13053",
        "title": "Self-Organization of Attractor Landscapes in High-Capacity Kernel Logistic Regression Hopfield Networks",
        "authors": [
            "Akira Tamamori"
        ],
        "comments": "4 pages, 3 figures",
        "subjects": "Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)",
        "abstract": "Kernel-based learning methods can dramatically increase the storage capacity of Hopfield networks, yet the dynamical mechanism behind this enhancement remains poorly understood. We address this gap by conducting a geometric analysis of the network's energy landscape. We introduce a novel metric, ``Pinnacle Sharpness,'' to quantify the local stability of attractors. By systematically varying the kernel width and storage load, we uncover a rich phase diagram of attractor shapes. Our central finding is the emergence of a ``ridge of optimization,'' where the network maximizes attractor stability under challenging high-load and global-kernel conditions. Through a theoretical decomposition of the landscape gradient into a direct ``driving'' force and an indirect ``feedback'' force, we reveal the origin of this phenomenon. The optimization ridge corresponds to a regime of strong anti-correlation between the two forces, where the direct force, amplified by the high storage load, dominates the opposing collective feedback force. This demonstrates a sophisticated self-organization mechanism: the network adaptively harnesses inter-pattern interactions as a cooperative feedback control system to sculpt a robust energy landscape. Our findings provide a new physical picture for the stability of high-capacity associative memories and offer principles for their design.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 157,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13061",
        "abs_url": "https://arxiv.org/abs/2511.13061",
        "pdf_url": "https://arxiv.org/pdf/2511.13061",
        "title": "MACKO: Sparse Matrix-Vector Multiplication for Low Sparsity",
        "authors": [
            "Vladimír Macko",
            "Vladimír Boža"
        ],
        "comments": "8 pages + 7 pages appendix, 11 figures, Code available at this https URL",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC); Data Structures and Algorithms (cs.DS)",
        "abstract": "Sparse Matrix-Vector Multiplication (SpMV) is a fundamental operation in the inference of sparse Large Language Models (LLMs). Because existing SpMV methods perform poorly under the low and unstructured sparsity (30-90%) commonly observed in pruned LLMs, unstructured pruning provided only limited memory reduction and speedup. We propose MACKO-SpMV, a GPU-optimized format and kernel co-designed to reduce storage overhead while preserving compatibility with the GPU's execution model. This enables efficient SpMV for unstructured sparsity without specialized hardware units (e.g., tensor cores) or format-specific precomputation. Empirical results show that at sparsity 50%, MACKO is the first approach with significant 1.5x memory reduction and 1.2-1.5x speedup over dense representation. Speedups over other SpMV baselines: 2.8-13.0x over cuSPARSE, 1.9-2.6x over Sputnik, and 2.2-2.5x over DASP. Applied to Llama2-7B pruned with Wanda to sparsity 50%, it delivers 1.5x memory reduction and 1.5x faster inference at fp16 precision. Thanks to MACKO, unstructured pruning at 50% sparsity is now justified in real-world LLM workloads.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 158,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13078",
        "abs_url": "https://arxiv.org/abs/2511.13078",
        "pdf_url": "https://arxiv.org/pdf/2511.13078",
        "title": "A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning",
        "authors": [
            "Liuyi Jin",
            "Pasan Gunawardena",
            "Amran Haroon",
            "Runzhi Wang",
            "Sangwoo Lee",
            "Radu Stoleru",
            "Michael Middleton",
            "Zepeng Huo",
            "Jeeeun Kim",
            "Jason Moats"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Audio and Speech Processing (eess.AS); Image and Video Processing (eess.IV)",
        "abstract": "Emergency Medical Technicians (EMTs) operate in high-pressure environments, making rapid, life-critical decisions under heavy cognitive and operational loads. We present EMSGlass, a smart-glasses system powered by EMSNet, the first multimodal multitask model for Emergency Medical Services (EMS), and EMSServe, a low-latency multimodal serving framework tailored to EMS scenarios. EMSNet integrates text, vital signs, and scene images to construct a unified real-time understanding of EMS incidents. Trained on real-world multimodal EMS datasets, EMSNet simultaneously supports up to five critical EMS tasks with superior accuracy compared to state-of-the-art unimodal baselines. Built on top of PyTorch, EMSServe introduces a modality-aware model splitter and a feature caching mechanism, achieving adaptive and efficient inference across heterogeneous hardware while addressing the challenge of asynchronous modality arrival in the field. By optimizing multimodal inference execution in EMS scenarios, EMSServe achieves 1.9x -- 11.7x speedup over direct PyTorch multimodal inference. A user study evaluation with six professional EMTs demonstrates that EMSGlass enhances real-time situational awareness, decision-making speed, and operational efficiency through intuitive on-glass interaction. In addition, qualitative insights from the user study provide actionable directions for extending EMSGlass toward next-generation AI-enabled EMS systems, bridging multimodal intelligence with real-world emergency response workflows.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 159,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13082",
        "abs_url": "https://arxiv.org/abs/2511.13082",
        "pdf_url": "https://arxiv.org/pdf/2511.13082",
        "title": "Real-time prediction of breast cancer sites using deformation-aware graph neural network",
        "authors": [
            "Kyunghyun Lee",
            "Yong-Min Shin",
            "Minwoo Shin",
            "Jihun Kim",
            "Sunghwan Lim",
            "Won-Yong Shin",
            "Kyungho Yoon"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Early diagnosis of breast cancer is crucial, enabling the establishment of appropriate treatment plans and markedly enhancing patient prognosis. While direct magnetic resonance imaging-guided biopsy demonstrates promising performance in detecting cancer lesions, its practical application is limited by prolonged procedure times and high costs. To overcome these issues, an indirect MRI-guided biopsy that allows the procedure to be performed outside of the MRI room has been proposed, but it still faces challenges in creating an accurate real-time deformable breast model. In our study, we tackled this issue by developing a graph neural network (GNN)-based model capable of accurately predicting deformed breast cancer sites in real time during biopsy procedures. An individual-specific finite element (FE) model was developed by incorporating magnetic resonance (MR) image-derived structural information of the breast and tumor to simulate deformation behaviors. A GNN model was then employed, designed to process surface displacement and distance-based graph data, enabling accurate prediction of overall tissue displacement, including the deformation of the tumor region. The model was validated using phantom and real patient datasets, achieving an accuracy within 0.2 millimeters (mm) for cancer node displacement (RMSE) and a dice similarity coefficient (DSC) of 0.977 for spatial overlap with actual cancerous regions. Additionally, the model enabled real-time inference and achieved a speed-up of over 4,000 times in computational cost compared to conventional FE simulations. The proposed deformation-aware GNN model offers a promising solution for real-time tumor displacement prediction in breast biopsy, with high accuracy and real-time capability. Its integration with clinical procedures could significantly enhance the precision and efficiency of breast cancer diagnosis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 160,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13103",
        "abs_url": "https://arxiv.org/abs/2511.13103",
        "pdf_url": "https://arxiv.org/pdf/2511.13103",
        "title": "Transformer-Based Scalable Multi-Agent Reinforcement Learning for Networked Systems with Long-Range Interactions",
        "authors": [
            "Vidur Sinha",
            "Muhammed Ustaomeroglu",
            "Guannan Qu"
        ],
        "comments": "8 pages, 7 figures, submitted for review",
        "subjects": "Machine Learning (cs.LG); Multiagent Systems (cs.MA); Systems and Control (eess.SY)",
        "abstract": "Multi-agent reinforcement learning (MARL) has shown promise for large-scale network control, yet existing methods face two major limitations. First, they typically rely on assumptions leading to decay properties of local agent interactions, limiting their ability to capture long-range dependencies such as cascading power failures or epidemic outbreaks. Second, most approaches lack generalizability across network topologies, requiring retraining when applied to new graphs. We introduce STACCA (Shared Transformer Actor-Critic with Counterfactual Advantage), a unified transformer-based MARL framework that addresses both challenges. STACCA employs a centralized Graph Transformer Critic to model long-range dependencies and provide system-level feedback, while its shared Graph Transformer Actor learns a generalizable policy capable of adapting across diverse network structures. Further, to improve credit assignment during training, STACCA integrates a novel counterfactual advantage estimator that is compatible with state-value critic estimates. We evaluate STACCA on epidemic containment and rumor-spreading network control tasks, demonstrating improved performance, network generalization, and scalability. These results highlight the potential of transformer-based MARL architectures to achieve scalable and generalizable control in large-scale networked systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 161,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13124",
        "abs_url": "https://arxiv.org/abs/2511.13124",
        "pdf_url": "https://arxiv.org/pdf/2511.13124",
        "title": "Departures: Distributional Transport for Single-Cell Perturbation Prediction with Neural Schrödinger Bridges",
        "authors": [
            "Changxi Chi",
            "Yufei Huang",
            "Jun Xia",
            "Jiangbin Zheng",
            "Yunfan Liu",
            "Zelin Zang",
            "Stan Z. Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)",
        "abstract": "Predicting single-cell perturbation outcomes directly advances gene function analysis and facilitates drug candidate selection, making it a key driver of both basic and translational biomedical research. However, a major bottleneck in this task is the unpaired nature of single-cell data, as the same cell cannot be observed both before and after perturbation due to the destructive nature of sequencing. Although some neural generative transport models attempt to tackle unpaired single-cell perturbation data, they either lack explicit conditioning or depend on prior spaces for indirect distribution alignment, limiting precise perturbation modeling. In this work, we approximate Schrödinger Bridge (SB), which defines stochastic dynamic mappings recovering the entropy-regularized optimal transport (OT), to directly align the distributions of control and perturbed single-cell populations across different perturbation conditions. Unlike prior SB approximations that rely on bidirectional modeling to infer optimal source-target sample coupling, we leverage Minibatch-OT based pairing to avoid such bidirectional inference and the associated ill-posedness of defining the reverse process. This pairing directly guides bridge learning, yielding a scalable approximation to the SB. We approximate two SB models, one modeling discrete gene activation states and the other continuous expression distributions. Joint training enables accurate perturbation modeling and captures single-cell heterogeneity. Experiments on public genetic and drug perturbation datasets show that our model effectively captures heterogeneous single-cell responses and achieves state-of-the-art performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 162,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13144",
        "abs_url": "https://arxiv.org/abs/2511.13144",
        "pdf_url": "https://arxiv.org/pdf/2511.13144",
        "title": "Personalized Federated Learning with Bidirectional Communication Compression via One-Bit Random Sketching",
        "authors": [
            "Jiacheng Cheng",
            "Xu Zhang",
            "Guanghui Qiu",
            "Yifang Zhang",
            "Yinchuan Li",
            "Kaiyuan Feng"
        ],
        "comments": "Accepted in AAAI 2026",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Federated Learning (FL) enables collaborative training across decentralized data, but faces key challenges of bidirectional communication overhead and client-side data heterogeneity. To address communication costs while embracing data heterogeneity, we propose pFed1BS, a novel personalized federated learning framework that achieves extreme communication compression through one-bit random sketching. In personalized FL, the goal shifts from training a single global model to creating tailored models for each client. In our framework, clients transmit highly compressed one-bit sketches, and the server aggregates and broadcasts a global one-bit consensus. To enable effective personalization, we introduce a sign-based regularizer that guides local models to align with the global consensus while preserving local data characteristics. To mitigate the computational burden of random sketching, we employ the Fast Hadamard Transform for efficient projection. Theoretical analysis guarantees that our algorithm converges to a stationary neighborhood of the global potential function. Numerical simulations demonstrate that pFed1BS substantially reduces communication costs while achieving competitive performance compared to advanced communication-efficient FL algorithms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 163,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13147",
        "abs_url": "https://arxiv.org/abs/2511.13147",
        "pdf_url": "https://arxiv.org/pdf/2511.13147",
        "title": "OTARo: Once Tuning for All Precisions toward Robust On-Device LLMs",
        "authors": [
            "Shaoyuan Chen",
            "Zhixuan Chen",
            "Dawei Yang",
            "Zhihang Yuan",
            "Qiang Wu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Large Language Models (LLMs) fine-tuning techniques not only improve the adaptability to diverse downstream tasks, but also mitigate adverse effects of model quantization. Despite this, conventional quantization suffers from its structural limitation that hinders flexibility during the fine-tuning and deployment stages. Practical on-device tasks demand different quantization precisions (i.e. different bit-widths), e.g., understanding tasks tend to exhibit higher tolerance to reduced precision compared to generation tasks. Conventional quantization, typically relying on scaling factors that are incompatible across bit-widths, fails to support the on-device switching of precisions when confronted with complex real-world scenarios. To overcome the dilemma, we propose OTARo, a novel method that enables on-device LLMs to flexibly switch quantization precisions while maintaining performance robustness through once fine-tuning. OTARo introduces Shared Exponent Floating Point (SEFP), a distinct quantization mechanism, to produce different bit-widths through simple mantissa truncations of a single model. Moreover, to achieve bit-width robustness in downstream applications, OTARo performs a learning process toward losses induced by different bit-widths. The method involves two critical strategies: (1) Exploitation-Exploration Bit-Width Path Search (BPS), which iteratively updates the search path via a designed scoring mechanism; (2) Low-Precision Asynchronous Accumulation (LAA), which performs asynchronous gradient accumulations and delayed updates under low bit-widths. Experiments on popular LLMs, e.g., LLaMA3.2-1B, LLaMA3-8B, demonstrate that OTARo achieves consistently strong and robust performance for all precisions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 164,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13174",
        "abs_url": "https://arxiv.org/abs/2511.13174",
        "pdf_url": "https://arxiv.org/pdf/2511.13174",
        "title": "Warm-starting active-set solvers using graph neural networks",
        "authors": [
            "Ella J. Schmidtobreick",
            "Daniel Arnström",
            "Paul Häusner",
            "Jens Sjölund"
        ],
        "comments": "Under review, 15 pages, 8 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Quadratic programming (QP) solvers are widely used in real-time control and optimization, but their computational cost often limits applicability in time-critical settings. We propose a learning-to-optimize approach using graph neural networks (GNNs) to predict active sets in the dual active-set solver DAQP. The method exploits the structural properties of QPs by representing them as bipartite graphs and learning to identify the optimal active set for efficiently warm-starting the solver. Across varying problem sizes, the GNN consistently reduces the number of solver iterations compared to cold-starting, while performance is comparable to a multilayer perceptron (MLP) baseline. Furthermore, a GNN trained on varying problem sizes generalizes effectively to unseen dimensions, demonstrating flexibility and scalability. These results highlight the potential of structure-aware learning to accelerate optimization in real-time applications such as model predictive control.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 165,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13178",
        "abs_url": "https://arxiv.org/abs/2511.13178",
        "pdf_url": "https://arxiv.org/pdf/2511.13178",
        "title": "Real-time distortion prediction in metallic additive manufacturing via a physics-informed neural operator approach",
        "authors": [
            "Mingxuan Tian",
            "Haochen Mu",
            "Donghong Ding",
            "Mengjiao Li",
            "Yuhan Ding",
            "Jianping Zhao"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "With the development of digital twins and smart manufacturing systems, there is an urgent need for real-time distortion field prediction to control defects in metal Additive Manufacturing (AM). However, numerical simulation methods suffer from high computational cost, long run-times that prevent real-time use, while conventional Machine learning (ML) models struggle to extract spatiotemporal features for long-horizon prediction and fail to decouple thermo-mechanical fields. This paper proposes a Physics-informed Neural Operator (PINO) to predict z and y-direction distortion for the future 15 s. Our method, Physics-informed Deep Operator Network-Recurrent Neural Network (PIDeepONet-RNN) employs trunk and branch network to process temperature history and encode distortion fields, respectively, enabling decoupling of thermo-mechanical responses. By incorporating the heat conduction equation as a soft constraint, the model ensures physical consistency and suppresses unphysical artifacts, thereby establishing a more physically consistent mapping between the thermal history and distortion. This is important because such a basis function, grounded in physical laws, provides a robust and interpretable foundation for predictions. The proposed models are trained and tested using datasets generated from experimentally validated Finite Element Method (FEM). Evaluation shows that the model achieves high accuracy, low error accumulation, time efficiency. The max absolute errors in the z and y-directions are as low as 0.9733 mm and 0.2049 mm, respectively. The error distribution shows high errors in the molten pool but low gradient norms in the deposited and key areas. The performance of PINO surrogate model highlights its potential for real-time long-horizon physics field prediction in controlling defects.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 166,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13185",
        "abs_url": "https://arxiv.org/abs/2511.13185",
        "pdf_url": "https://arxiv.org/pdf/2511.13185",
        "title": "Uncertainty-aware Physics-informed Neural Networks for Robust CARS-to-Raman Signal Reconstruction",
        "authors": [
            "Aishwarya Venkataramanan",
            "Sai Karthikeya Vemuri",
            "Adithya Ashok Chalain Valapil",
            "Joachim Denzler"
        ],
        "comments": "EurIPS DiffSys workshop 2025",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Coherent anti-Stokes Raman scattering (CARS) spectroscopy is a powerful and rapid technique widely used in medicine, material science, and chemical analyses. However, its effectiveness is hindered by the presence of a non-resonant background that interferes with and distorts the true Raman signal. Deep learning methods have been employed to reconstruct the true Raman spectrum from measured CARS data using labeled datasets. A more recent development integrates the domain knowledge of Kramers-Kronig relationships and smoothness constraints in the form of physics-informed loss functions. However, these deterministic models lack the ability to quantify uncertainty, an essential feature for reliable deployment in high-stakes scientific and biomedical applications. In this work, we evaluate and compare various uncertainty quantification (UQ) techniques within the context of CARS-to-Raman signal reconstruction. Furthermore, we demonstrate that incorporating physics-informed constraints into these models improves their calibration, offering a promising path toward more trustworthy CARS data analysis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 167,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13186",
        "abs_url": "https://arxiv.org/abs/2511.13186",
        "pdf_url": "https://arxiv.org/pdf/2511.13186",
        "title": "DiffFP: Learning Behaviors from Scratch via Diffusion-based Fictitious Play",
        "authors": [
            "Akash Karthikeyan",
            "Yash Vardhan Pant"
        ],
        "comments": "Initial results presented at the IJCAI 2025 Workshop on User-Aligned Assessment of Adaptive AI Systems. Project page: this https URL",
        "subjects": "Machine Learning (cs.LG); Systems and Control (eess.SY)",
        "abstract": "Self-play reinforcement learning has demonstrated significant success in learning complex strategic and interactive behaviors in competitive multi-agent games. However, achieving such behaviors in continuous decision spaces remains challenging. Ensuring adaptability and generalization in self-play settings is critical for achieving competitive performance in dynamic multi-agent environments. These challenges often cause methods to converge slowly or fail to converge at all to a Nash equilibrium, making agents vulnerable to strategic exploitation by unseen opponents. To address these challenges, we propose DiffFP, a fictitious play (FP) framework that estimates the best response to unseen opponents while learning a robust and multimodal behavioral policy. Specifically, we approximate the best response using a diffusion policy that leverages generative modeling to learn adaptive and diverse strategies. Through empirical evaluation, we demonstrate that the proposed FP framework converges towards $\\epsilon$-Nash equilibria in continuous- space zero-sum games. We validate our method on complex multi-agent environments, including racing and multi-particle zero-sum games. Simulation results show that the learned policies are robust against diverse opponents and outperform baseline reinforcement learning policies. Our approach achieves up to 3$\\times$ faster convergence and 30$\\times$ higher success rates on average against RL-based baselines, demonstrating its robustness to opponent strategies and stability across training iterations",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 168,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13223",
        "abs_url": "https://arxiv.org/abs/2511.13223",
        "pdf_url": "https://arxiv.org/pdf/2511.13223",
        "title": "TokenSqueeze: Performance-Preserving Compression for Reasoning LLMs",
        "authors": [
            "Yuxiang Zhang",
            "Zhengxu Yu",
            "Weihang Pan",
            "Zhongming Jin",
            "Qiang Fu",
            "Deng Cai",
            "Binbin Lin",
            "Jieping Ye"
        ],
        "comments": "Accepted to NeurIPS 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Emerging reasoning LLMs such as OpenAI-o1 and DeepSeek-R1 have achieved strong performance on complex reasoning tasks by generating long chain-of-thought (CoT) traces. However, these long CoTs result in increased token usage, leading to higher inference latency and memory consumption. As a result, balancing accuracy and reasoning efficiency has become essential for deploying reasoning LLMs in practical applications. Existing long-to-short (Long2Short) methods aim to reduce inference length but often sacrifice accuracy, revealing a need for an approach that maintains performance while lowering token costs. To address this efficiency-accuracy tradeoff, we propose TokenSqueeze, a novel Long2Short method that condenses reasoning paths while preserving performance and relying exclusively on self-generated data. First, to prevent performance degradation caused by excessive compression of reasoning depth, we propose to select self-generated samples whose reasoning depth is adaptively matched to the complexity of the problem. To further optimize the linguistic expression without altering the underlying reasoning paths, we introduce a distribution-aligned linguistic refinement method that enhances the clarity and conciseness of the reasoning path while preserving its logical integrity. Comprehensive experimental results demonstrate the effectiveness of TokenSqueeze in reducing token usage while maintaining accuracy. Notably, DeepSeek-R1-Distill-Qwen-7B fine-tuned using our proposed method achieved a 50\\% average token reduction while preserving accuracy on the MATH500 benchmark. TokenSqueeze exclusively utilizes the model's self-generated data, enabling efficient and high-fidelity reasoning without relying on manually curated short-answer datasets across diverse applications. Our code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 169,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13229",
        "abs_url": "https://arxiv.org/abs/2511.13229",
        "pdf_url": "https://arxiv.org/pdf/2511.13229",
        "title": "Laplace Learning in Wasserstein Space",
        "authors": [
            "Mary Chriselda Antony Oliver",
            "Michael Roberts",
            "Carola-Bibiane Schönlieb",
            "Matthew Thorpe"
        ],
        "comments": "46 page, 5 figures",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "The manifold hypothesis posits that high-dimensional data typically resides on low-dimensional sub spaces. In this paper, we assume manifold hypothesis to investigate graph-based semi-supervised learning methods. In particular, we examine Laplace Learning in the Wasserstein space, extending the classical notion of graph-based semi-supervised learning algorithms from finite-dimensional Euclidean spaces to an infinite-dimensional setting. To achieve this, we prove variational convergence of a discrete graph p- Dirichlet energy to its continuum counterpart. In addition, we characterize the Laplace-Beltrami operator on asubmanifold of the Wasserstein space. Finally, we validate the proposed theoretical framework through numerical experiments conducted on benchmark datasets, demonstrating the consistency of our classification performance in high-dimensional settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 170,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13234",
        "abs_url": "https://arxiv.org/abs/2511.13234",
        "pdf_url": "https://arxiv.org/pdf/2511.13234",
        "title": "MorphBoost: Self-Organizing Universal Gradient Boosting with Adaptive Tree Morphing",
        "authors": [
            "Boris Kriuk"
        ],
        "comments": "8 pages, 5 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Traditional gradient boosting algorithms employ static tree structures with fixed splitting criteria that remain unchanged throughout training, limiting their ability to adapt to evolving gradient distributions and problem-specific characteristics across different learning stages. This work introduces MorphBoost, a new gradient boosting framework featuring self-organizing tree structures that dynamically morph their splitting behavior during training. The algorithm implements adaptive split functions that evolve based on accumulated gradient statistics and iteration-dependent learning pressures, enabling automatic adjustment to problem complexity. Key innovations include: (1) morphing split criterion combining gradient-based scores with information-theoretic metrics weighted by training progress; (2) automatic problem fingerprinting for intelligent parameter configuration across binary/multiclass/regression tasks; (3) vectorized tree prediction achieving significant computational speedups; (4) interaction-aware feature importance detecting multiplicative relationships; and (5) fast-mode optimization balancing speed and accuracy. Comprehensive benchmarking across 10 diverse datasets against competitive models (XGBoost, LightGBM, GradientBoosting, HistGradientBoosting, ensemble methods) demonstrates that MorphBoost achieves state-of-the-art performance, outperforming XGBoost by 0.84% on average. MorphBoost secured the overall winner position with 4/10 dataset wins (40% win rate) and 6/30 top-3 finishes (20%), while maintaining the lowest variance ({\\sigma}=0.0948) and highest minimum accuracy across all models, revealing superior consistency and robustness. Performance analysis across difficulty levels shows competitive results on easy datasets while achieving notable improvements on advanced problems due to higher adaptation levels.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 171,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13237",
        "abs_url": "https://arxiv.org/abs/2511.13237",
        "pdf_url": "https://arxiv.org/pdf/2511.13237",
        "title": "Counterfactual Explainable AI (XAI) Method for Deep Learning-Based Multivariate Time Series Classification",
        "authors": [
            "Alan G. Paredes Cetina",
            "Kaouther Benguessoum",
            "Raoni Lourenço",
            "Sylvain Kubler"
        ],
        "comments": "Accepted in AAAI 2026 Technical Main Track",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Recent advances in deep learning have improved multivariate time series (MTS) classification and regression by capturing complex patterns, but their lack of transparency hinders decision-making. Explainable AI (XAI) methods offer partial insights, yet often fall short of conveying the full decision space. Counterfactual Explanations (CE) provide a promising alternative, but current approaches typically prioritize either accuracy, proximity or sparsity -- rarely all -- limiting their practical value. To address this, we propose CONFETTI, a novel multi-objective CE method for MTS. CONFETTI identifies key MTS subsequences, locates a counterfactual target, and optimally modifies the time series to balance prediction confidence, proximity and sparsity. This method provides actionable insights with minimal changes, improving interpretability, and decision support. CONFETTI is evaluated on seven MTS datasets from the UEA archive, demonstrating its effectiveness in various domains. CONFETTI consistently outperforms state-of-the-art CE methods in its optimization objectives, and in six other metrics from the literature, achieving $\\geq10\\%$ higher confidence while improving sparsity in $\\geq40\\%$.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 172,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13240",
        "abs_url": "https://arxiv.org/abs/2511.13240",
        "pdf_url": "https://arxiv.org/pdf/2511.13240",
        "title": "Incoherent Beliefs & Inconsistent Actions in Large Language Models",
        "authors": [
            "Arka Pal",
            "Teo Kitanovski",
            "Arthur Liang",
            "Akilesh Potti",
            "Micah Goldblum"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Real-world tasks and environments exhibit differences from the static datasets that large language models (LLMs) are typically evaluated on. Such tasks can involve sequential interaction, requiring coherent updating of beliefs in light of new evidence, and making appropriate decisions based on those beliefs. Predicting how LLMs will perform in such dynamic environments is important, but can be tricky to determine from measurements in static settings. In this work, we examine two critical components of LLM performance: the ability of LLMs to coherently update their beliefs, and the extent to which the actions they take are consistent with those beliefs. First, we find that LLMs are largely inconsistent in how they update their beliefs; models can exhibit up to a 30% average difference between the directly elicited posterior, and the correct update of their prior. Second, we find that LLMs also often take actions which are inconsistent with the beliefs they hold. On a betting market, for example, LLMs often do not even bet in the same direction as their internally held beliefs over the underlying outcomes. We also find they have moderate self-inconsistency in how they respond to challenges by users to given answers. Finally, we show that the above properties hold even for strong models that obtain high accuracy or that are well-calibrated on the tasks at hand. Our results highlight the difficulties of predicting LLM behavior in complex real-world settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 173,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13244",
        "abs_url": "https://arxiv.org/abs/2511.13244",
        "pdf_url": "https://arxiv.org/pdf/2511.13244",
        "title": "Seek and You Shall Fold",
        "authors": [
            "Nadav Bojan Sellam",
            "Meital Bojan",
            "Paul Schanda",
            "Alex Bronstein"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Accurate protein structures are essential for understanding biological function, yet incorporating experimental data into protein generative models remains a major challenge. Most predictors of experimental observables are non-differentiable, making them incompatible with gradient-based conditional sampling. This is especially limiting in nuclear magnetic resonance, where rich data such as chemical shifts are hard to directly integrate into generative modeling. We introduce a framework for non-differentiable guidance of protein generative models, coupling a continuous diffusion-based generator with any black-box objective via a tailored genetic algorithm. We demonstrate its effectiveness across three modalities: pairwise distance constraints, nuclear Overhauser effect restraints, and for the first time chemical shifts. These results establish chemical shift guided structure generation as feasible, expose key weaknesses in current predictors, and showcase a general strategy for incorporating diverse experimental signals. Our work points toward automated, data-conditioned protein modeling beyond the limits of differentiability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 174,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13250",
        "abs_url": "https://arxiv.org/abs/2511.13250",
        "pdf_url": "https://arxiv.org/pdf/2511.13250",
        "title": "Edge-aware baselines for ogbn-proteins in PyTorch Geometric: species-wise normalization, post-hoc calibration, and cost-accuracy trade-offs",
        "authors": [
            "Aleksandar Stanković",
            "Dejan Lisica"
        ],
        "comments": "8 pages, 3 figures, 5 tables. Code and artifacts: this https URL",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We present reproducible, edge-aware baselines for ogbn-proteins in PyTorch Geometric (PyG). We study two system choices that dominate practice: (i) how 8-dimensional edge evidence is aggregated into node inputs, and (ii) how edges are used inside message passing. Our strongest baseline is GraphSAGE with sum-based edge-to-node features. We compare LayerNorm (LN), BatchNorm (BN), and a species-aware Conditional LayerNorm (CLN), and report compute cost (time, VRAM, parameters) together with accuracy (ROC-AUC) and decision quality. In our primary experimental setup (hidden size 512, 3 layers, 3 seeds), sum consistently beats mean and max; BN attains the best AUC, while CLN matches the AUC frontier with better thresholded F1. Finally, post-hoc per-label temperature scaling plus per-label thresholds substantially improves micro-F1 and expected calibration error (ECE) with negligible AUC change, and light label-correlation smoothing yields small additional gains. We release standardized artifacts and scripts used for all of the runs presented in the paper.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 175,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13322",
        "abs_url": "https://arxiv.org/abs/2511.13322",
        "pdf_url": "https://arxiv.org/pdf/2511.13322",
        "title": "Explainable RL Policies by Distilling to Locally-Specialized Linear Policies with Voronoi State Partitioning",
        "authors": [
            "Senne Deproost",
            "Dennis Steckelmacher",
            "Ann Nowé"
        ],
        "comments": "Accepted for BNAIC/BeNeLearn 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Deep Reinforcement Learning is one of the state-of-the-art methods for producing near-optimal system controllers. However, deep RL algorithms train a deep neural network, that lacks transparency, which poses challenges when the controller has to meet regulations, or foster trust. To alleviate this, one could transfer the learned behaviour into a model that is human-readable by design using knowledge distilla- tion. Often this is done with a single model which mimics the original model on average but could struggle in more dynamic situations. A key challenge is that this simpler model should have the right balance be- tween flexibility and complexity or right balance between balance bias and accuracy. We propose a new model-agnostic method to divide the state space into regions where a simplified, human-understandable model can operate in. In this paper, we use Voronoi partitioning to find regions where linear models can achieve similar performance to the original con- troller. We evaluate our approach on a gridworld environment and a classic control task. We observe that our proposed distillation to locally- specialized linear models produces policies that are explainable and show that the distillation matches or even slightly outperforms the black-box policy they are distilled from.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 176,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13338",
        "abs_url": "https://arxiv.org/abs/2511.13338",
        "pdf_url": "https://arxiv.org/pdf/2511.13338",
        "title": "Tab-PET: Graph-Based Positional Encodings for Tabular Transformers",
        "authors": [
            "Yunze Leng",
            "Rohan Ghosh",
            "Mehul Motani"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Supervised learning with tabular data presents unique challenges, including low data sizes, the absence of structural cues, and heterogeneous features spanning both categorical and continuous domains. Unlike vision and language tasks, where models can exploit inductive biases in the data, tabular data lacks inherent positional structure, hindering the effectiveness of self-attention mechanisms. While recent transformer-based models like TabTransformer, SAINT, and FT-Transformer (which we refer to as 3T) have shown promise on tabular data, they typically operate without leveraging structural cues such as positional encodings (PEs), as no prior structural information is usually available. In this work, we find both theoretically and empirically that structural cues, specifically PEs can be a useful tool to improve generalization performance for tabular transformers. We find that PEs impart the ability to reduce the effective rank (a form of intrinsic dimensionality) of the features, effectively simplifying the task by reducing the dimensionality of the problem, yielding improved generalization. To that end, we propose Tab-PET (PEs for Tabular Transformers), a graph-based framework for estimating and inculcating PEs into embeddings. Inspired by approaches that derive PEs from graph topology, we explore two paradigms for graph estimation: association-based and causality-based. We empirically demonstrate that graph-derived PEs significantly improve performance across 50 classification and regression datasets for 3T. Notably, association-based graphs consistently yield more stable and pronounced gains compared to causality-driven ones. Our work highlights an unexpected role of PEs in tabular transformers, revealing how they can be harnessed to improve generalization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 177,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13339",
        "abs_url": "https://arxiv.org/abs/2511.13339",
        "pdf_url": "https://arxiv.org/pdf/2511.13339",
        "title": "Statistically Accurate and Robust Generative Prediction of Rock Discontinuities with A Tabular Foundation Model",
        "authors": [
            "Han Meng",
            "Gang Mei",
            "Hong Tian",
            "Nengxiong Xu",
            "Jianbing Peng"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Rock discontinuities critically govern the mechanical behavior and stability of rock masses. Their internal distributions remain largely unobservable and are typically inferred from surface-exposed discontinuities using generative prediction approaches. However, surface-exposed observations are inherently sparse, and existing generative prediction approaches either fail to capture the underlying complex distribution patterns or lack robustness under data-sparse conditions. Here, we proposed a simple yet robust approach for statistically accurate generative prediction of rock discontinuities by utilizing a tabular foundation model. By leveraging the powerful sample learning capability of the foundation model specifically designed for small data, our approach can effectively capture the underlying complex distribution patterns within limited measured discontinuities. Comparative experiments on ten datasets with diverse scales and distribution patterns of discontinuities demonstrate superior accuracy and robustness over conventional statistical models and deep generative approaches. This work advances quantitative characterization of rock mass structures, supporting safer and more reliable data-driven geotechnical design.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 178,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13373",
        "abs_url": "https://arxiv.org/abs/2511.13373",
        "pdf_url": "https://arxiv.org/pdf/2511.13373",
        "title": "A Novel Hierarchical Integration Method for Efficient Model Merging in Medical LLMs",
        "authors": [
            "Prakrit Timilsina",
            "Anuj Nepal",
            "Rajan Kadel",
            "Robin Doss"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Large Language Models (LLMs) face significant challenges in distributed healthcare, including consolidating specialized domain knowledge across institutions while maintaining privacy, reducing computational overhead, and preventing catastrophic forgetting during model this http URL paper presents a systematic evaluation of six parameter-space merging techniques applied to two architecturally compatible medical LLMs derived from the Mistral-7B base model. We introduce a novel hierarchical method that combines selective Optimal Transport (OT) alignment for attention layers with cosine similarity-weighted interpolation, designed to address permutation variance while minimizing computational overhead for edge deployment scenarios. Our study evaluates Task Arithmetic, Linear Averaging, DARE-TIES, DELLA, Breadcrumbs, and our Hierarchical approach across five medical benchmarks. Results demonstrate that architecturally compatible models benefit significantly from simple averaging methods, with Task Arithmetic achieving 45.80% accuracy on MedQA, outperforming complex pruning-based approaches. These findings offer critical insights for the deployment of distributed medical AI in resource-constrained IoT environments, where computational efficiency and model compatibility are paramount. Our work establishes that for architecturally compatible models, simple averaging provides a robust and computationally efficient baseline for knowledge consolidation, offering a pragmatic path forward for scalable medical AI systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 179,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13394",
        "abs_url": "https://arxiv.org/abs/2511.13394",
        "pdf_url": "https://arxiv.org/pdf/2511.13394",
        "title": "Fast and Robust Simulation-Based Inference With Optimization Monte Carlo",
        "authors": [
            "Vasilis Gkolemis",
            "Christos Diou",
            "Michael Gutmann"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Bayesian parameter inference for complex stochastic simulators is challenging due to intractable likelihood functions. Existing simulation-based inference methods often require large number of simulations and become costly to use in high-dimensional parameter spaces or in problems with partially uninformative outputs. We propose a new method for differentiable simulators that delivers accurate posterior inference with substantially reduced runtimes. Building on the Optimization Monte Carlo framework, our approach reformulates stochastic simulation as deterministic optimization problems. Gradient-based methods are then applied to efficiently navigate toward high-density posterior regions and avoid wasteful simulations in low-probability areas. A JAX-based implementation further enhances the performance through vectorization of key method components. Extensive experiments, including high-dimensional parameter spaces, uninformative outputs, multiple observations and multimodal posteriors show that our method consistently matches, and often exceeds, the accuracy of state-of-the-art approaches, while reducing the runtime by a substantial margin.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 180,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13419",
        "abs_url": "https://arxiv.org/abs/2511.13419",
        "pdf_url": "https://arxiv.org/pdf/2511.13419",
        "title": "MMWSTM-ADRAN+: A Novel Hybrid Deep Learning Architecture for Enhanced Climate Time Series Forecasting and Extreme Event Prediction",
        "authors": [
            "Shaheen Mohammed Saleh Ahmed",
            "Hakan Hakan Guneyli"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Atmospheric and Oceanic Physics (physics.ao-ph)",
        "abstract": "Accurate short-range prediction of extreme air temperature events remains a fundamental challenge in operational climate-risk management. We present Multi-Modal Weather State Transition Model with Anomaly-Driven Recurrent Attention Network Plus (MMWSTM-ADRAN+), a dual-stream deep learning architecture that couples a regime-aware dynamics model with an anomaly-focused attention mechanism to forecast daily maximum temperature and its extremes. The first stream, MMWSTM, combines bidirectional Long Short-Term Memory (BiLSTM) units with a learnable Markov state transition matrix to capture synoptic-scale weather regime changes. The second stream, ADRAN, integrates bidirectional Gated Recurrent Units (BiGRUs), multi-head self-attention, and a novel anomaly amplification layer to enhance sensitivity to low-probability signals. A lightweight attentive fusion gate adaptively determines the contribution of each stream to the final prediction. Model optimization employs a custom ExtremeWeatherLoss function that up-weights errors on the upper 5% and lower 5% of the temperature distribution, and a time-series data augmentation suite (jittering, scaling, time/magnitude warping) that effectively quadruples the training data",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 181,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13421",
        "abs_url": "https://arxiv.org/abs/2511.13421",
        "pdf_url": "https://arxiv.org/pdf/2511.13421",
        "title": "Larger Datasets Can Be Repeated More: A Theoretical Analysis of Multi-Epoch Scaling in Linear Regression",
        "authors": [
            "Tingkai Yan",
            "Haodong Wen",
            "Binghui Li",
            "Kairong Luo",
            "Wenguang Chen",
            "Kaifeng Lyu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "While data scaling laws of large language models (LLMs) have been widely examined in the one-pass regime with massive corpora, their form under limited data and repeated epochs remains largely unexplored. This paper presents a theoretical analysis of how a common workaround, training for multiple epochs on the same dataset, reshapes the data scaling laws in linear regression. Concretely, we ask: to match the performance of training on a dataset of size $N$ for $K$ epochs, how much larger must a dataset be if the model is trained for only one pass? We quantify this using the \\textit{effective reuse rate} of the data, $E(K, N)$, which we define as the multiplicative factor by which the dataset must grow under one-pass training to achieve the same test loss as $K$-epoch training. Our analysis precisely characterizes the scaling behavior of $E(K, N)$ for SGD in linear regression under either strong convexity or Zipf-distributed data: (1) When $K$ is small, we prove that $E(K, N) \\approx K$, indicating that every new epoch yields a linear gain; (2) As $K$ increases, $E(K, N)$ plateaus at a problem-dependent value that grows with $N$ ($\\Theta(\\log N)$ for the strongly-convex case), implying that larger datasets can be repeated more times before the marginal benefit vanishes. These theoretical findings point out a neglected factor in a recent empirical study (Muennighoff et al. (2023)), which claimed that training LLMs for up to $4$ epochs results in negligible loss differences compared to using fresh data at each step, \\textit{i.e.}, $E(K, N) \\approx K$ for $K \\le 4$ in our notation. Supported by further empirical validation with LLMs, our results reveal that the maximum $K$ value for which $E(K, N) \\approx K$ in fact depends on the data size and distribution, and underscore the need to explicitly model both factors in future studies of scaling laws with data reuse.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 182,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13453",
        "abs_url": "https://arxiv.org/abs/2511.13453",
        "pdf_url": "https://arxiv.org/pdf/2511.13453",
        "title": "Hardware optimization on Android for inference of AI models",
        "authors": [
            "Iulius Gherasim",
            "Carlos García Sánchez"
        ],
        "comments": "8 pages",
        "subjects": "Machine Learning (cs.LG); Performance (cs.PF)",
        "abstract": "The pervasive integration of Artificial Intelligence models into contemporary mobile computing is notable across numerous use cases, from virtual assistants to advanced image processing. Optimizing the mobile user experience involves minimal latency and high responsiveness from deployed AI models with challenges from execution strategies that fully leverage real time constraints to the exploitation of heterogeneous hardware architecture. In this paper, we research and propose the optimal execution configurations for AI models on an Android system, focusing on two critical tasks: object detection (YOLO family) and image classification (ResNet). These configurations evaluate various model quantization schemes and the utilization of on device accelerators, specifically the GPU and NPU. Our core objective is to empirically determine the combination that achieves the best trade-off between minimal accuracy degradation and maximal inference speed-up.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 183,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13463",
        "abs_url": "https://arxiv.org/abs/2511.13463",
        "pdf_url": "https://arxiv.org/pdf/2511.13463",
        "title": "Multi-task GINN-LP for Multi-target Symbolic Regression",
        "authors": [
            "Hussein Rajabu",
            "Lijun Qian",
            "Xishuang Dong"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "In the area of explainable artificial intelligence, Symbolic Regression (SR) has emerged as a promising approach by discovering interpretable mathematical expressions that fit data. However, SR faces two main challenges: most methods are evaluated on scientific datasets with well-understood relationships, limiting generalization, and SR primarily targets single-output regression, whereas many real-world problems involve multi-target outputs with interdependent variables. To address these issues, we propose multi-task regression GINN-LP (MTRGINN-LP), an interpretable neural network for multi-target symbolic regression. By integrating GINN-LP with a multi-task deep learning, the model combines a shared backbone including multiple power-term approximator blocks with task-specific output layers, capturing inter-target dependencies while preserving interpretability. We validate multi-task GINN-LP on practical multi-target applications, including energy efficiency prediction and sustainable agriculture. Experimental results demonstrate competitive predictive performance alongside high interpretability, effectively extending symbolic regression to broader real-world multi-output tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 184,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13465",
        "abs_url": "https://arxiv.org/abs/2511.13465",
        "pdf_url": "https://arxiv.org/pdf/2511.13465",
        "title": "AdamX: An Adam improvement algorithm based on a novel exponential decay mechanism for the second-order moment estimate",
        "authors": [
            "Meng Zhu",
            "Quan Xiao",
            "Weidong Min"
        ],
        "comments": "25 pages, 6 figures, 12 tables",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Since the 21st century, artificial intelligence has been leading a new round of industrial revolution. Under the training framework, the optimization algorithm aims to stably converge high-dimensional optimization to local and even global minima. Entering the era of large language models, although the scale of model parameters and data has increased, Adam remains the mainstream optimization algorithm. However, compared with stochastic gradient descent (SGD) based optimization algorithms, Adam is more likely to converge to non-flat minima. To address this issue, the AdamX algorithm is proposed. Its core innovation lies in the proposition of a novel type of second-order moment estimation exponential decay rate, which gradually weakens the learning step correction strength as training progresses, and degrades to SGD in the stable training period, thereby improving the stability of training in the stable period and possibly enhancing generalization ability. Experimental results show that our second-order moment estimation exponential decay rate is better than the current second-order moment estimation exponential decay rate, and AdamX can stably outperform Adam and its variants in terms of performance. Our code is open-sourced at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 185,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13469",
        "abs_url": "https://arxiv.org/abs/2511.13469",
        "pdf_url": "https://arxiv.org/pdf/2511.13469",
        "title": "GREAT: Generalizable Representation Enhancement via Auxiliary Transformations for Zero-Shot Environmental Prediction",
        "authors": [
            "Shiyuan Luo",
            "Chonghao Qiu",
            "Runlong Yu",
            "Yiqun Xie",
            "Xiaowei Jia"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Environmental modeling faces critical challenges in predicting ecosystem dynamics across unmonitored regions due to limited and geographically imbalanced observation data. This challenge is compounded by spatial heterogeneity, causing models to learn spurious patterns that fit only local data. Unlike conventional domain generalization, environmental modeling must preserve invariant physical relationships and temporal coherence during augmentation. In this paper, we introduce Generalizable Representation Enhancement via Auxiliary Transformations (GREAT), a framework that effectively augments available datasets to improve predictions in completely unseen regions. GREAT guides the augmentation process to ensure that the original governing processes can be recovered from the augmented data, and the inclusion of the augmented data leads to improved model generalization. Specifically, GREAT learns transformation functions at multiple layers of neural networks to augment both raw environmental features and temporal influence. They are refined through a novel bi-level training process that constrains augmented data to preserve key patterns of the original source data. We demonstrate GREAT's effectiveness on stream temperature prediction across six ecologically diverse watersheds in the eastern U.S., each containing multiple stream segments. Experimental results show that GREAT significantly outperforms existing methods in zero-shot scenarios. This work provides a practical solution for environmental applications where comprehensive monitoring is infeasible.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 186,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13497",
        "abs_url": "https://arxiv.org/abs/2511.13497",
        "pdf_url": "https://arxiv.org/pdf/2511.13497",
        "title": "Quantum Machine Learning via Contrastive Training",
        "authors": [
            "Liudmila A. Zhukas",
            "Vivian Ni Zhang",
            "Qiang Miao",
            "Qingfeng Wang",
            "Marko Cetina",
            "Jungsang Kim",
            "Lawrence Carin",
            "Christopher Monroe"
        ],
        "comments": "7 figures, 20 pages total",
        "subjects": "Machine Learning (cs.LG); Quantum Physics (quant-ph)",
        "abstract": "Quantum machine learning (QML) has attracted growing interest with the rapid parallel advances in large-scale classical machine learning and quantum technologies. Similar to classical machine learning, QML models also face challenges arising from the scarcity of labeled data, particularly as their scale and complexity increase. Here, we introduce self-supervised pretraining of quantum representations that reduces reliance on labeled data by learning invariances from unlabeled examples. We implement this paradigm on a programmable trapped-ion quantum computer, encoding images as quantum states. In situ contrastive pretraining on hardware yields a representation that, when fine-tuned, classifies image families with higher mean test accuracy and lower run-to-run variability than models trained from random initialization. Performance improvement is especially significant in regimes with limited labeled training data. We show that the learned invariances generalize beyond the pretraining image samples. Unlike prior work, our pipeline derives similarity from measured quantum overlaps and executes all training and classification stages on hardware. These results establish a label-efficient route to quantum representation learning, with direct relevance to quantum-native datasets and a clear path to larger classical inputs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 187,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2506.14157",
        "abs_url": "https://arxiv.org/abs/2506.14157",
        "pdf_url": "https://arxiv.org/pdf/2506.14157",
        "title": "DCRM: A Heuristic to Measure Response Pair Quality in Preference Optimization",
        "authors": [
            "Chengyu Huang",
            "Tanya Goyal"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Recent research has attempted to associate preference optimization (PO) performance with the underlying preference datasets. In this work, our observation is that the differences between the preferred response $y^+$ and dispreferred response $y^-$ influence what LLMs can learn, which may not match the desirable differences to learn. Therefore, we use distance and reward margin to quantify these differences, and combine them to get Distance Calibrated Reward Margin (DCRM), a metric that measures the quality of a response pair for PO. Intuitively, DCRM encourages minimal noisy differences and maximal desired differences. With this, we study 3 types of commonly used preference datasets, classified along two axes: the source of the responses and the preference labeling function. We establish a general correlation between higher DCRM of the training set and better learning outcome. Inspired by this, we propose a best-of-$N^2$ pairing method that selects response pairs with the highest DCRM. Empirically, in various settings, our method produces training datasets that can further improve models' performance on AlpacaEval, MT-Bench, and Arena-Hard over the existing training sets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 188,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11572",
        "abs_url": "https://arxiv.org/abs/2511.11572",
        "pdf_url": "https://arxiv.org/pdf/2511.11572",
        "title": "LLM Architecture, Scaling Laws, and Economics: A Quick Summary",
        "authors": [
            "William H. Press"
        ],
        "comments": "9 pages, 3 figures",
        "subjects": "General Literature (cs.GL); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "The current standard architecture of Large Language Models (LLMs) with QKV self-attention is briefly summarized, including the architecture of a typical Transformer. Scaling laws for compute (flops) and memory (parameters plus data) are given, along with their present (2025) rough cost estimates for the parameters of present LLMs of various scales, including discussion of whether DeepSeek should be viewed as a special case. Nothing here is new, but this material seems not otherwise readily available in summary form.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 189,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11578",
        "abs_url": "https://arxiv.org/abs/2511.11578",
        "pdf_url": "https://arxiv.org/pdf/2511.11578",
        "title": "Social and Physical Attributes-Defined Trust Evaluation for Effective Collaborator Selection in Human-Device Coexistence Systems",
        "authors": [
            "Botao Zhu",
            "Xianbin Wang"
        ],
        "comments": "",
        "subjects": "Human-Computer Interaction (cs.HC); Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "In human-device coexistence systems, collaborations among devices are determined by not only physical attributes such as network topology but also social attributes among human users. Consequently, trust evaluation of potential collaborators based on these multifaceted attributes becomes critical for ensuring the eventual outcome. However, due to the high heterogeneity and complexity of physical and social attributes, efficiently integrating them for accurate trust evaluation remains challenging. To overcome this difficulty, a canonical correlation analysis-enhanced hypergraph self-supervised learning (HSLCCA) method is proposed in this research. First, by treating all attributes as relationships among connected devices, a relationship hypergraph is constructed to comprehensively capture inter-device relationships across three dimensions: spatial attribute-related, device attribute-related, and social attribute-related. Next, a self-supervised learning framework is developed to integrate these multi-dimensional relationships and generate device embeddings enriched with relational semantics. In this learning framework, the relationship hypergraph is augmented into two distinct views to enhance semantic information. A parameter-sharing hypergraph neural network is then utilized to learn device embeddings from both views. To further enhance embedding quality, a CCA approach is applied, allowing the comparison of data between the two views. Finally, the trustworthiness of devices is calculated based on the learned device embeddings. Extensive experiments demonstrate that the proposed HSLCCA method significantly outperforms the baseline algorithm in effectively identifying trusted devices.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 190,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11611",
        "abs_url": "https://arxiv.org/abs/2511.11611",
        "pdf_url": "https://arxiv.org/pdf/2511.11611",
        "title": "Quantifying Skill and Chance: A Unified Framework for the Geometry of Games",
        "authors": [
            "David H. Silver"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multiagent Systems (cs.MA)",
        "abstract": "We introduce a quantitative framework for separating skill and chance in games by modeling them as complementary sources of control over stochastic decision trees. We define the Skill-Luck Index S(G) in [-1, 1] by decomposing game outcomes into skill leverage K and luck leverage L. Applying this to 30 games reveals a continuum from pure chance (coin toss, S = -1) through mixed domains such as backgammon (S = 0, Sigma = 1.20) to pure skill (chess, S = +1, Sigma = 0). Poker exhibits moderate skill dominance (S = 0.33) with K = 0.40 +/- 0.03 and Sigma = 0.80. We further introduce volatility Sigma to quantify outcome uncertainty over successive turns. The framework extends to general stochastic decision systems, enabling principled comparisons of player influence, game balance, and predictive stability, with applications to game design, AI evaluation, and risk assessment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 191,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11613",
        "abs_url": "https://arxiv.org/abs/2511.11613",
        "pdf_url": "https://arxiv.org/pdf/2511.11613",
        "title": "Physics-Informed Neural Network-based Reliability Analysis of Buried Pipelines",
        "authors": [
            "Pouya Taraghi",
            "Yong Li",
            "Samer Adeeb"
        ],
        "comments": "This manuscript has been submitted to Reliability Engineering & System Safety and is currently under peer review",
        "subjects": "Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG); Data Analysis, Statistics and Probability (physics.data-an); Applications (stat.AP)",
        "abstract": "Buried pipelines transporting oil and gas across geohazard-prone regions are exposed to potential ground movement, leading to the risk of significant strain demand and structural failure. Reliability analysis, which determines the probability of failure after accounting for pertinent uncertainties, is essential for ensuring the safety of pipeline systems. However, traditional reliability analysis methods involving computationally intensive numerical models, such as finite element simulations of pipeline subjected to ground movement, have limited applications; this is partly because stochastic sampling approaches require repeated simulations over a large number of samples for the uncertain variables when estimating low probabilities. This study introduces Physics-Informed Neural Network for Reliability Analysis (PINN-RA) for buried pipelines subjected to ground movement, which integrates PINN-based surrogate model with Monte Carlo Simulation (MCS) to achieve efficient reliability assessment. To enable its application under uncertain variables associated with soil properties and ground movement, the PINN-based surrogate model is extended to solve a parametric differential equation system, namely the governing equation of pipelines embedded in soil with different properties. The findings demonstrate that PINN-RA significantly reduces the computational effort required and thus accelerates reliability analysis. By eliminating the need for repetitive numerical evaluations of pipeline subjected to permanent ground movement, the proposed approach provides an efficient and scalable tool for pipeline reliability assessment, enabling rapid decision-making in geohazard-prone regions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 192,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11616",
        "abs_url": "https://arxiv.org/abs/2511.11616",
        "pdf_url": "https://arxiv.org/pdf/2511.11616",
        "title": "Hierarchical Federated Graph Attention Networks for Scalable and Resilient UAV Collision Avoidance",
        "authors": [
            "Rathin Chandra Shit",
            "Sharmila Subudhi"
        ],
        "comments": "Accepted and scheduled for conference presentation",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multiagent Systems (cs.MA)",
        "abstract": "The real-time performance, adversarial resiliency, and privacy preservation are the most important metrics that need to be balanced to practice collision avoidance in large-scale multi-UAV (Unmanned Aerial Vehicle) systems. Current frameworks tend to prescribe monolithic solutions that are not only prohibitively computationally complex with a scaling cost of $O(n^2)$ but simply do not offer Byzantine fault tolerance. The proposed hierarchical framework presented in this paper tries to eliminate such trade-offs by stratifying a three-layered architecture. We spread the intelligence into three layers: an immediate collision avoiding local layer running on dense graph attention with latency of $<10 ms$, a regional layer using sparse attention with $O(nk)$ computational complexity and asynchronous federated learning with coordinate-wise trimmed mean aggregation, and lastly, a global layer using a lightweight Hashgraph-inspired protocol. We have proposed an adaptive differential privacy mechanism, wherein the noise level $(\\epsilon \\in [0.1, 1.0])$ is dynamically reduced based on an evaluation of the measured real-time threat that in turn maximized the privacy-utility tradeoff. Through the use of Distributed Hash Table (DHT)-based lightweight audit logging instead of heavyweight blockchain consensus, the median cost of getting a $95^{th}$ percentile decision within 50ms is observed across all tested swarm sizes. This architecture provides a scalable scenario of 500 UAVs with a collision rate of $< 2.0\\%$ and the Byzantine fault tolerance of $f < n/3$.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 193,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11626",
        "abs_url": "https://arxiv.org/abs/2511.11626",
        "pdf_url": "https://arxiv.org/pdf/2511.11626",
        "title": "Omics-scale polymer computational database transferable to real-world artificial intelligence applications",
        "authors": [
            "Ryo Yoshida",
            "Yoshihiro Hayashi",
            "Hidemine Furuya",
            "Ryohei Hosoya",
            "Kazuyoshi Kaneko",
            "Hiroki Sugisawa",
            "Yu Kaneko",
            "Aiko Takahashi",
            "Yoh Noguchi",
            "Shun Nanjo",
            "Keiko Shinoda",
            "Tomu Hamakawa",
            "Mitsuru Ohno",
            "Takuya Kitamura",
            "Misaki Yonekawa",
            "Stephen Wu",
            "Masato Ohnishi",
            "Chang Liu",
            "Teruki Tsurimoto",
            "Arifin",
            "Araki Wakiuchi",
            "Kohei Noda",
            "Junko Morikawa",
            "Teruaki Hayakawa",
            "Junichiro Shiomi",
            "Masanobu Naito",
            "Kazuya Shiratori",
            "Tomoki Nagai",
            "Norio Tomotsu",
            "Hiroto Inoue",
            "Ryuichi Sakashita",
            "Masashi Ishii",
            "Isao Kuwajima",
            "Kenji Furuichi",
            "Norihiko Hiroi",
            "Yuki Takemoto",
            "Takahiro Ohkuma",
            "Keita Yamamoto",
            "Naoya Kowatari",
            "Masato Suzuki",
            "Naoya Matsumoto",
            "Seiryu Umetani",
            "Hisaki Ikebata",
            "Yasuyuki Shudo",
            "Mayu Nagao",
            "Shinya Kamada",
            "Kazunori Kamio",
            "Taichi Shomura",
            "Kensaku Nakamura",
            "Yudai Iwamizu",
            "Atsutoshi Abe",
            "Koki Yoshitomi",
            "Yuki Horie",
            "Katsuhiko Koike",
            "Koichi Iwakabe",
            "Shinya Gima",
            "Kota Usui",
            "Gikyo Usuki",
            "Takuro Tsutsumi",
            "Keitaro Matsuoka",
            "Kazuki Sada",
            "Masahiro Kitabata",
            "Takuma Kikutsuji",
            "Akitaka Kamauchi",
            "Yusuke Iijima",
            "Tsubasa Suzuki",
            "Takenori Goda",
            "Yuki Takabayashi",
            "Kazuko Imai",
            "Yuji Mochizuki",
            "Hideo Doi",
            "Koji Okuwaki",
            "Hiroya Nitta",
            "Taku Ozawa",
            "Hitoshi Kamijima",
            "Toshiaki Shintani",
            "Takuma Mitamura",
            "Massimiliano Zamengo",
            "Yuitsu Sugami",
            "Seiji Akiyama",
            "Yoshinari Murakami",
            "Atsushi Betto",
            "Naoya Matsuo",
            "Satoru Kagao",
            "Tetsuya Kobayashi",
            "Norie Matsubara",
            "Shosei Kubo",
            "Yuki Ishiyama",
            "Yuri Ichioka",
            "Mamoru Usami",
            "Satoru Yoshizaki",
            "Seigo Mizutani",
            "Yosuke Hanawa",
            "Shogo Kunieda",
            "Mitsuru Yambe",
            "Takeru Nakamura",
            "Hiromori Murashima",
            "Kenji Takahashi",
            "Naoki Wada",
            "Masahiro Kawano"
        ],
        "comments": "65 pages, 11 figures",
        "subjects": "Chemical Physics (physics.chem-ph); Materials Science (cond-mat.mtrl-sci); Soft Condensed Matter (cond-mat.soft); Machine Learning (cs.LG)",
        "abstract": "Developing large-scale foundational datasets is a critical milestone in advancing artificial intelligence (AI)-driven scientific innovation. However, unlike AI-mature fields such as natural language processing, materials science, particularly polymer research, has significantly lagged in developing extensive open datasets. This lag is primarily due to the high costs of polymer synthesis and property measurements, along with the vastness and complexity of the chemical space. This study presents PolyOmics, an omics-scale computational database generated through fully automated molecular dynamics simulation pipelines that provide diverse physical properties for over $10^5$ polymeric materials. The PolyOmics database is collaboratively developed by approximately 260 researchers from 48 institutions to bridge the gap between academia and industry. Machine learning models pretrained on PolyOmics can be efficiently fine-tuned for a wide range of real-world downstream tasks, even when only limited experimental data are available. Notably, the generalisation capability of these simulation-to-real transfer models improve significantly as the size of the PolyOmics database increases, exhibiting power-law scaling. The emergence of scaling laws supports the \"more is better\" principle, highlighting the significance of ultralarge-scale computational materials data for improving real-world prediction performance. This unprecedented omics-scale database reveals vast unexplored regions of polymer materials, providing a foundation for AI-driven polymer science.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 194,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11640",
        "abs_url": "https://arxiv.org/abs/2511.11640",
        "pdf_url": "https://arxiv.org/pdf/2511.11640",
        "title": "Exploring Parallelism in FPGA-Based Accelerators for Machine Learning Applications",
        "authors": [
            "Sed Centeno",
            "Christopher Sprague",
            "Arnab A Purkayastha",
            "Ray Simar",
            "Neeraj Magotra"
        ],
        "comments": "5 pages",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Hardware Architecture (cs.AR); Machine Learning (cs.LG)",
        "abstract": "Speculative backpropagation has emerged as a promising technique to accelerate the training of neural networks by overlapping the forward and backward passes. Leveraging speculative weight updates when error gradients fall within a specific threshold reduces training time without substantially compromising accuracy. In this work, we implement speculative backpropagation on the MNIST dataset using OpenMP as the parallel programming platform. OpenMP's multi-threading capabilities enable simultaneous execution of forward and speculative backpropagation steps, significantly improving training speed. The application is planned for synthesis on a state-of-the-art FPGA to demonstrate its potential for hardware acceleration. Our CPU-based experimental results demonstrate that speculative backpropagation achieves a maximum speedup of 24% in execution time when using a threshold of 0.25, and accuracy remaining within 3-4% of the baseline across various epochs. Additionally, when comparing individual step execution time, speculative backpropagation yields a maximum speedup of 35% over the baseline, demonstrating the effectiveness of overlapping forward and backward passes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 195,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11649",
        "abs_url": "https://arxiv.org/abs/2511.11649",
        "pdf_url": "https://arxiv.org/pdf/2511.11649",
        "title": "The Environmental Impact of Ensemble Techniques in Recommender Systems",
        "authors": [
            "Jannik Nitschke"
        ],
        "comments": "Bachelor Thesis, University of Siegen",
        "subjects": "Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "Ensemble techniques in recommender systems have demonstrated accuracy improvements of 10-30%, yet their environmental impact remains unmeasured. While deep learning recommendation algorithms can generate up to 3,297 kg CO2 per paper, ensemble methods have not been sufficiently evaluated for energy consumption. This thesis investigates how ensemble techniques influence environmental impact compared to single optimized models. We conducted 93 experiments across two frameworks (Surprise for rating prediction, LensKit for ranking) on four datasets spanning 100,000 to 7.8 million interactions. We evaluated four ensemble strategies (Average, Weighted, Stacking/Rank Fusion, Top Performers) against simple baselines and optimized single models, measuring energy consumption with a smart plug. Results revealed a non-linear accuracy-energy relationship. Ensemble methods achieved 0.3-5.7% accuracy improvements while consuming 19-2,549% more energy depending on dataset size and strategy. The Top Performers ensemble showed best efficiency: 0.96% RMSE improvement with 18.8% energy overhead on MovieLens-1M, and 5.7% NDCG improvement with 103% overhead on MovieLens-100K. Exhaustive averaging strategies consumed 88-270% more energy for comparable gains. On the largest dataset (Anime, 7.8M interactions), the Surprise ensemble consumed 2,005% more energy (0.21 Wh vs. 0.01 Wh) for 1.2% accuracy improvement, producing 53.8 mg CO2 versus 2.6 mg CO2 for the single model. This research provides one of the first systematic measurements of energy and carbon footprint for ensemble recommender systems, demonstrates that selective strategies offer superior efficiency over exhaustive averaging, and identifies scalability limitations at industrial scale. These findings enable informed decisions about sustainable algorithm selection in recommender systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 196,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11678",
        "abs_url": "https://arxiv.org/abs/2511.11678",
        "pdf_url": "https://arxiv.org/pdf/2511.11678",
        "title": "A Structure-Agnostic Co-Tuning Framework for LLMs and SLMs in Cloud-Edge Systems",
        "authors": [
            "Yuze Liu",
            "Yunhan Wang",
            "Tiehua Zhang",
            "Zhishu Shen",
            "Cheng Peng",
            "Libing Wu",
            "Feng Xia",
            "Jiong Jin"
        ],
        "comments": "",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "The surge in intelligent applications driven by large language models (LLMs) has made it increasingly difficult for bandwidth-limited cloud servers to process extensive LLM workloads in real time without compromising user data privacy. To solve these problems, recent research has focused on constructing cloud-edge consortia that integrate server-based LLM with small language models (SLMs) on mobile edge devices. Furthermore, designing collaborative training mechanisms within such consortia to enhance inference performance has emerged as a promising research direction. However, the cross-domain deployment of SLMs, coupled with structural heterogeneity in SLMs architectures, poses significant challenges to enhancing model performance. To this end, we propose Co-PLMs, a novel co-tuning framework for collaborative training of large and small language models, which integrates the process of structure-agnostic mutual learning to realize knowledge exchange between the heterogeneous language models. This framework employs distilled proxy models (DPMs) as bridges to enable collaborative training between the heterogeneous server-based LLM and on-device SLMs, while preserving the domain-specific insights of each device. The experimental results show that Co-PLMs outperform state-of-the-art methods, achieving average increases of 5.38% in Rouge-L and 4.88% in EM.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 197,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11682",
        "abs_url": "https://arxiv.org/abs/2511.11682",
        "pdf_url": "https://arxiv.org/pdf/2511.11682",
        "title": "Generalized Inequality-based Approach for Probabilistic WCET Estimation",
        "authors": [
            "Hayate Toba",
            "Atsushi Yano",
            "Takuya Azumi"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Estimating the probabilistic Worst-Case Execution Time (pWCET) is essential for ensuring the timing correctness of real-time applications, such as in robot IoT systems and autonomous driving systems. While methods based on Extreme Value Theory (EVT) can provide tight bounds, they suffer from model uncertainty due to the need to decide where the upper tail of the distribution begins. Conversely, inequality-based approaches avoid this issue but can yield pessimistic results for heavy-tailed distributions. This paper proposes a method to reduce such pessimism by incorporating saturating functions (arctangent and hyperbolic tangent) into Chebyshev's inequality, which mitigates the influence of large outliers while preserving mathematical soundness. Evaluations on synthetic and real-world data from the Autoware autonomous driving stack demonstrate that the proposed method achieves safe and tighter bounds for such distributions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 198,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11729",
        "abs_url": "https://arxiv.org/abs/2511.11729",
        "pdf_url": "https://arxiv.org/pdf/2511.11729",
        "title": "Harli: Harvest Underutilized Resources in LLM Serving with Finetuning Tasks",
        "authors": [
            "Ao Xu",
            "Han Zhao",
            "Weihao Cui",
            "Quan Chen",
            "Yukang Chen",
            "Shulai Zhang",
            "Shuang Chen",
            "Jiemin Jiang",
            "Zhibin Yu",
            "Minyi Guo"
        ],
        "comments": "",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)",
        "abstract": "Large language models (LLMs) are increasingly deployed under the Model-as-a-Service (MaaS) paradigm. To meet stringent quality-of-service (QoS) requirements, existing LLM serving systems disaggregate the prefill and decode phases of inference. However, decode instances often experience low GPU utilization due to their memory-bound nature and insufficient batching in dynamic workloads, leaving compute resources underutilized. We introduce Harli, a serving system that improves GPU utilization by co-locating parameter-efficient finetuning (PEFT) tasks with LLM decode instances. PEFT tasks are compute-bound and memory-efficient, making them ideal candidates for safe co-location. Specifically, Harli addresses key challenges--limited memory and unpredictable interference--using three components: a unified memory allocator for runtime memory reuse, a two-stage latency predictor for decode latency modeling, and a QoS-guaranteed throughput-maximizing scheduler for throughput maximization. Experimental results show that Harli improves the finetune throughput by 46.2% on average (up to 92.0%) over state-of-the-art serving systems, while maintaining strict QoS guarantees for inference decode.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 199,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11739",
        "abs_url": "https://arxiv.org/abs/2511.11739",
        "pdf_url": "https://arxiv.org/pdf/2511.11739",
        "title": "Noise-Aware Optimization in Nominally Identical Manufacturing and Measuring Systems for High-Throughput Parallel Workflows",
        "authors": [
            "Christina Schenk",
            "Miguel Hernández-del-Valle",
            "Luis Calero-Lumbreras",
            "Marcus Noack",
            "Maciej Haranczyk"
        ],
        "comments": "17 pages, 4 figures, 2 tables",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG); Optimization and Control (math.OC); Computation (stat.CO)",
        "abstract": "Device-to-device variability in experimental noise critically impacts reproducibility, especially in automated, high-throughput systems like additive manufacturing farms. While manageable in small labs, such variability can escalate into serious risks at larger scales, such as architectural 3D printing, where noise may cause structural or economic failures. This contribution presents a noise-aware decision-making algorithm that quantifies and models device-specific noise profiles to manage variability adaptively. It uses distributional analysis and pairwise divergence metrics with clustering to choose between single-device and robust multi-device Bayesian optimization strategies. Unlike conventional methods that assume homogeneous devices or generic robustness, this framework explicitly leverages inter-device differences to enhance performance, reproducibility, and efficiency. An experimental case study involving three nominally identical 3D printers (same brand, model, and close serial numbers) demonstrates reduced redundancy, lower resource usage, and improved reliability. Overall, this framework establishes a paradigm for precision- and resource-aware optimization in scalable, automated experimental platforms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 200,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11769",
        "abs_url": "https://arxiv.org/abs/2511.11769",
        "pdf_url": "https://arxiv.org/pdf/2511.11769",
        "title": "Socrates-Mol: Self-Oriented Cognitive Reasoning through Autonomous Trial-and-Error with Empirical-Bayesian Screening for Molecules",
        "authors": [
            "Xiangru Wang",
            "Zekun Jiang",
            "Heng Yang",
            "Cheng Tan",
            "Xingying Lan",
            "Chunming Xu",
            "Tianhang Zhou"
        ],
        "comments": "",
        "subjects": "Chemical Physics (physics.chem-ph); Machine Learning (cs.LG); Quantitative Methods (q-bio.QM); Methodology (stat.ME)",
        "abstract": "Molecular property prediction is fundamental to chemical engineering applications such as solvent screening. We present Socrates-Mol, a framework that transforms language models into empirical Bayesian reasoners through context engineering, addressing cold start problems without model fine-tuning. The system implements a reflective-prediction cycle where initial outputs serve as priors, retrieved molecular cases provide evidence, and refined predictions form posteriors, extracting reusable chemical rules from sparse data. We introduce ranking tasks aligned with industrial screening priorities and employ cross-model self-consistency across five language models to reduce variance. Experiments on amine solvent LogP prediction reveal task-dependent patterns: regression achieves 72% MAE reduction and 112% R-squared improvement through self-consistency, while ranking tasks show limited gains due to systematic multi-model biases. The framework reduces deployment costs by over 70% compared to full fine-tuning, providing a scalable solution for molecular property prediction while elucidating the task-adaptive nature of self-consistency mechanisms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 201,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11773",
        "abs_url": "https://arxiv.org/abs/2511.11773",
        "pdf_url": "https://arxiv.org/pdf/2511.11773",
        "title": "On the Measure of a Model: From Intelligence to Generality",
        "authors": [
            "Ruchira Dhar",
            "Ninell Oldenburg",
            "Anders Soegaard"
        ],
        "comments": "Accepted at EurIPS Workshop on \"The Science of Benchmarking and Evaluating AI\"",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Benchmarks such as ARC, Raven-inspired tests, and the Blackbird Task are widely used to evaluate the intelligence of large language models (LLMs). Yet, the concept of intelligence remains elusive- lacking a stable definition and failing to predict performance on practical tasks such as question answering, summarization, or coding. Optimizing for such benchmarks risks misaligning evaluation with real-world utility. Our perspective is that evaluation should be grounded in generality rather than abstract notions of intelligence. We identify three assumptions that often underpin intelligence-focused evaluation: generality, stability, and realism. Through conceptual and formal analysis, we show that only generality withstands conceptual and empirical scrutiny. Intelligence is not what enables generality; generality is best understood as a multitask learning problem that directly links evaluation to measurable performance breadth and reliability. This perspective reframes how progress in AI should be assessed and proposes generality as a more stable foundation for evaluating capability across diverse and evolving tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 202,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11817",
        "abs_url": "https://arxiv.org/abs/2511.11817",
        "pdf_url": "https://arxiv.org/pdf/2511.11817",
        "title": "FreDN: Spectral Disentanglement for Time Series Forecasting via Learnable Frequency Decomposition",
        "authors": [
            "Zhongde An",
            "Jinhong You",
            "Jiyanglin Li",
            "Yiming Tang",
            "Wen Li",
            "Heming Du",
            "Shouguo Du"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Time series forecasting is essential in a wide range of real world applications. Recently, frequency-domain methods have attracted increasing interest for their ability to capture global dependencies. However, when applied to non-stationary time series, these methods encounter the $\\textit{spectral entanglement}$ and the computational burden of complex-valued learning. The $\\textit{spectral entanglement}$ refers to the overlap of trends, periodicities, and noise across the spectrum due to $\\textit{spectral leakage}$ and the presence of non-stationarity. However, existing decompositions are not suited to resolving spectral entanglement. To address this, we propose the Frequency Decomposition Network (FreDN), which introduces a learnable Frequency Disentangler module to separate trend and periodic components directly in the frequency domain. Furthermore, we propose a theoretically supported ReIm Block to reduce the complexity of complex-valued operations while maintaining performance. We also re-examine the frequency-domain loss function and provide new theoretical insights into its effectiveness. Extensive experiments on seven long-term forecasting benchmarks demonstrate that FreDN outperforms state-of-the-art methods by up to 10\\%. Furthermore, compared with standard complex-valued architectures, our real-imaginary shared-parameter design reduces the parameter count and computational cost by at least 50\\%.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 203,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11830",
        "abs_url": "https://arxiv.org/abs/2511.11830",
        "pdf_url": "https://arxiv.org/pdf/2511.11830",
        "title": "A Computational Method for Solving the Stochastic Joint Replenishment Problem in High Dimensions",
        "authors": [
            "Barış Ata",
            "Wouter van Eekelen",
            "Yuan Zhong"
        ],
        "comments": "52 pages, 3 figures",
        "subjects": "Optimization and Control (math.OC); Machine Learning (cs.LG)",
        "abstract": "We consider a discrete-time formulation for a class of high-dimensional stochastic joint replenishment problems. First, we approximate the problem by a continuous-time impulse control problem. Exploiting connections among the impulse control problem, backward stochastic differential equations (BSDEs) with jumps, and the stochastic target problem, we develop a novel, simulation-based computational method that relies on deep neural networks to solve the impulse control problem. Based on that solution, we propose an implementable inventory control policy for the original (discrete-time) stochastic joint replenishment problem, and test it against the best available benchmarks in a series of test problems. For the problems studied thus far, our method matches or beats the best benchmark we could find, and it is computationally feasible up to at least 50 dimensions -- that is, 50 stock-keeping units (SKUs).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 204,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11837",
        "abs_url": "https://arxiv.org/abs/2511.11837",
        "pdf_url": "https://arxiv.org/pdf/2511.11837",
        "title": "MP-GFormer: A 3D-Geometry-Aware Dynamic Graph Transformer Approach for Machining Process Planning",
        "authors": [
            "Fatemeh Elhambakhsh",
            "Gaurav Ameta",
            "Aditi Roy",
            "Hyunwoong Ko"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Machining process planning (MP) is inherently complex due to structural and geometrical dependencies among part features and machining operations. A key challenge lies in capturing dynamic interdependencies that evolve with distinct part geometries as operations are performed. Machine learning has been applied to address challenges in MP, such as operation selection and machining sequence prediction. Dynamic graph learning (DGL) has been widely used to model dynamic systems, thanks to its ability to integrate spatio-temporal relationships. However, in MP, while existing DGL approaches can capture these dependencies, they fail to incorporate three-dimensional (3D) geometric information of parts and thus lack domain awareness in predicting machining operation sequences. To address this limitation, we propose MP-GFormer, a 3D-geometry-aware dynamic graph transformer that integrates evolving 3D geometric representations into DGL through an attention mechanism to predict machining operation sequences. Our approach leverages StereoLithography surface meshes representing the 3D geometry of a part after each machining operation, with the boundary representation method used for the initial 3D designs. We evaluate MP-GFormer on a synthesized dataset and demonstrate that the method achieves improvements of 24\\% and 36\\% in accuracy for main and sub-operation predictions, respectively, compared to state-of-the-art approaches.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 205,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11863",
        "abs_url": "https://arxiv.org/abs/2511.11863",
        "pdf_url": "https://arxiv.org/pdf/2511.11863",
        "title": "Modeling X-ray photon pile-up with a normalizing flow",
        "authors": [
            "Ole König",
            "Daniela Huppenkothen",
            "Douglas Finkbeiner",
            "Christian Kirsch",
            "Jörn Wilms",
            "Justina R. Yang",
            "James F. Steiner",
            "Juan Rafael Martínez-Galarza"
        ],
        "comments": "Accepted in Machine Learning and the Physical Sciences Workshop, NeurIPS 2025",
        "subjects": "High Energy Astrophysical Phenomena (astro-ph.HE); Instrumentation and Methods for Astrophysics (astro-ph.IM); Machine Learning (cs.LG)",
        "abstract": "The dynamic range of imaging detectors flown on-board X-ray observatories often only covers a limited flux range of extrasolar X-ray sources. The analysis of bright X-ray sources is complicated by so-called pile-up, which results from high incident photon flux. This nonlinear effect distorts the measured spectrum, resulting in biases in the inferred physical parameters, and can even lead to a complete signal loss in extreme cases. Piled-up data are commonly discarded due to resulting intractability of the likelihood. As a result, a large number of archival observations remain underexplored. We present a machine learning solution to this problem, using a simulation-based inference framework that allows us to estimate posterior distributions of physical source parameters from piled-up eROSITA data. We show that a normalizing flow produces better-constrained posterior densities than traditional mitigation techniques, as more data can be leveraged. We consider model- and calibration-dependent uncertainties and the applicability of such an algorithm to real data in the eROSITA archive.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 206,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11883",
        "abs_url": "https://arxiv.org/abs/2511.11883",
        "pdf_url": "https://arxiv.org/pdf/2511.11883",
        "title": "ClinStructor: AI-Powered Structuring of Unstructured Clinical Texts",
        "authors": [
            "Karthikeyan K",
            "Raghuveer Thirukovalluru",
            "David Carlson"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Clinical notes contain valuable, context-rich information, but their unstructured format introduces several challenges, including unintended biases (e.g., gender or racial bias), and poor generalization across clinical settings (e.g., models trained on one EHR system may perform poorly on another due to format differences) and poor interpretability. To address these issues, we present ClinStructor, a pipeline that leverages large language models (LLMs) to convert clinical free-text into structured, task-specific question-answer pairs prior to predictive modeling. Our method substantially enhances transparency and controllability and only leads to a modest reduction in predictive performance (a 2-3% drop in AUC), compared to direct fine-tuning, on the ICU mortality prediction task. ClinStructor lays a strong foundation for building reliable, interpretable, and generalizable machine learning models in clinical environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 207,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11922",
        "abs_url": "https://arxiv.org/abs/2511.11922",
        "pdf_url": "https://arxiv.org/pdf/2511.11922",
        "title": "Additive Large Language Models for Semi-Structured Text",
        "authors": [
            "Karthikeyan K",
            "Raghuveer Thirukovalluru",
            "David Carlson"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Large Language Models have advanced clinical text classification, but their opaque predictions remain a critical barrier to practical adoption in research and clinical settings where investigators and physicians need to understand which parts of a patient's record drive risk signals. To address this challenge, we introduce \\textbf{CALM}, short for \\textbf{Classification with Additive Large Language Models}, an interpretable framework for semi-structured text where inputs are composed of semantically meaningful components, such as sections of an admission note or question-answer fields from an intake form. CALM predicts outcomes as the additive sum of each component's contribution, making these contributions part of the forward computation itself and enabling faithful explanations at both the patient and population level. The additive structure also enables clear visualizations, such as component-level risk curves similar to those used in generalized additive models, making the learned relationships easier to inspect and communicate. Although CALM expects semi-structured inputs, many clinical documents already have this form, and similar structure can often be automatically extracted from free-text notes. CALM achieves performance comparable to conventional LLM classifiers while improving trust, supporting quality-assurance checks, and revealing clinically meaningful patterns during model development and auditing.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 208,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11927",
        "abs_url": "https://arxiv.org/abs/2511.11927",
        "pdf_url": "https://arxiv.org/pdf/2511.11927",
        "title": "PCA recovery thresholds in low-rank matrix inference with sparse noise",
        "authors": [
            "Urte Adomaityte",
            "Gabriele Sicuro",
            "Pierpaolo Vivo"
        ],
        "comments": "24 pages, 7 figures",
        "subjects": "Machine Learning (stat.ML); Disordered Systems and Neural Networks (cond-mat.dis-nn); Statistical Mechanics (cond-mat.stat-mech); Machine Learning (cs.LG)",
        "abstract": "We study the high-dimensional inference of a rank-one signal corrupted by sparse noise. The noise is modelled as the adjacency matrix of a weighted undirected graph with finite average connectivity in the large size limit. Using the replica method from statistical physics, we analytically compute the typical value of the top eigenvalue, the top eigenvector component density, and the overlap between the signal vector and the top eigenvector. The solution is given in terms of recursive distributional equations for auxiliary probability density functions which can be efficiently solved using a population dynamics algorithm. Specialising the noise matrix to Poissonian and Random Regular degree distributions, the critical signal strength is analytically identified at which a transition happens for the recovery of the signal via the top eigenvector, thus generalising the celebrated BBP transition to the sparse noise case. In the large-connectivity limit, known results for dense noise are recovered. Analytical results are in agreement with numerical diagonalisation of large matrices.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 209,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11933",
        "abs_url": "https://arxiv.org/abs/2511.11933",
        "pdf_url": "https://arxiv.org/pdf/2511.11933",
        "title": "InData: Towards Secure Multi-Step, Tool-Based Data Analysis",
        "authors": [
            "Karthikeyan K",
            "Raghuveer Thirukovalluru",
            "Bhuwan Dhingra",
            "David Edwin Carlson"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Large language model agents for data analysis typically generate and execute code directly on databases. However, when applied to sensitive data, this approach poses significant security risks. To address this issue, we propose a security-motivated alternative: restrict LLMs from direct code generation and data access, and require them to interact with data exclusively through a predefined set of secure, verified tools. Although recent tool-use benchmarks exist, they primarily target tool selection and simple execution rather than the compositional, multi-step reasoning needed for complex data analysis. To reduce this gap, we introduce Indirect Data Engagement (InData), a dataset designed to assess LLMs' multi-step tool-based reasoning ability. InData includes data analysis questions at three difficulty levels--Easy, Medium, and Hard--capturing increasing reasoning complexity. We benchmark 15 open-source LLMs on InData and find that while large models (e.g., gpt-oss-120b) achieve high accuracy on Easy tasks (97.3%), performance drops sharply on Hard tasks (69.6%). These results show that current LLMs still lack robust multi-step tool-based reasoning ability. With InData, we take a step toward enabling the development and evaluation of LLMs with stronger multi-step tool-use capabilities. We will publicly release the dataset and code.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 210,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11938",
        "abs_url": "https://arxiv.org/abs/2511.11938",
        "pdf_url": "https://arxiv.org/pdf/2511.11938",
        "title": "Improving Neutrino Oscillation Measurements through Event Classification",
        "authors": [
            "Sebastian A. R. Ellis",
            "Daniel C. Hackett",
            "Shirley Weishi Li",
            "Pedro A. N. Machado",
            "Karla Tame-Narvaez"
        ],
        "comments": "11 pages, 7 figures",
        "subjects": "High Energy Physics - Phenomenology (hep-ph); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); High Energy Physics - Experiment (hep-ex)",
        "abstract": "Precise neutrino energy reconstruction is essential for next-generation long-baseline oscillation experiments, yet current methods remain limited by large uncertainties in neutrino-nucleus interaction modeling. Even so, it is well established that different interaction channels produce systematically varying amounts of missing energy and therefore yield different reconstruction performance--information that standard calorimetric approaches do not exploit. We introduce a strategy that incorporates this structure by classifying events according to their underlying interaction type prior to energy reconstruction. Using supervised machine-learning techniques trained on labeled generator events, we leverage intrinsic kinematic differences among quasi-elastic scattering, meson-exchange current, resonance production, and deep-inelastic scattering processes. A cross-generator testing framework demonstrates that this classification approach is robust to microphysics mismodeling and, when applied to a simulated DUNE $\\nu_\\mu$ disappearance analysis, yields improved accuracy and sensitivity. These results highlight a practical path toward reducing reconstruction-driven systematics in future oscillation measurements.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 211,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11946",
        "abs_url": "https://arxiv.org/abs/2511.11946",
        "pdf_url": "https://arxiv.org/pdf/2511.11946",
        "title": "Improving LLM's Attachment to External Knowledge In Dialogue Generation Tasks Through Entity Anonymization",
        "authors": [
            "Hadi Sheikhi",
            "Chenyang Huang",
            "Osmar R. Zaïane"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Knowledge graph-based dialogue generation (KG-DG) is a challenging task requiring models to effectively incorporate external knowledge into conversational responses. While large language models (LLMs) have achieved impressive results across various NLP tasks, their ability to utilize external knowledge in KG-DG remains under-explored. We observe that LLMs often rely on internal knowledge, leading to detachment from provided knowledge graphs, even when they are given a flawlessly retrieved knowledge graph. First, we introduce LLM-KAT, an evaluation procedure for measuring knowledge attachment in generated responses. Second, we propose a simple yet effective entity anonymization technique to encourage LLMs to better leverage external knowledge. Experiments on the OpenDialKG dataset demonstrate that our approach improves LLMs' attachment on external knowledge.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 212,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11966",
        "abs_url": "https://arxiv.org/abs/2511.11966",
        "pdf_url": "https://arxiv.org/pdf/2511.11966",
        "title": "On the Entropy Calibration of Language Models",
        "authors": [
            "Steven Cao",
            "Gregory Valiant",
            "Percy Liang"
        ],
        "comments": "Neurips 2025",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "We study the problem of entropy calibration, which asks whether a language model's entropy over generations matches its log loss on human text. Past work found that models are miscalibrated, with entropy per step increasing (and text quality decreasing) as generations grow longer. This error accumulation is a fundamental problem in autoregressive models, and the standard solution is to truncate the distribution, which improves text quality at the cost of diversity. In this paper, we ask: is miscalibration likely to improve with scale, and is it theoretically possible to calibrate without tradeoffs? To build intuition, we first study a simplified theoretical setting to characterize the scaling behavior of miscalibration with respect to dataset size. We find that the scaling behavior depends on the power law exponent of the data distribution -- in particular, for a power law exponent close to 1, the scaling exponent is close to 0, meaning that miscalibration improves very slowly with scale. Next, we measure miscalibration empirically in language models ranging from 0.5B to 70B parameters. We find that the observed scaling behavior is similar to what is predicted by the simplified setting: our fitted scaling exponents for text are close to 0, meaning that larger models accumulate error at a similar rate as smaller ones. This scaling (or, lack thereof) provides one explanation for why we sample from larger models with similar amounts of truncation as smaller models, even though the larger models are of higher quality. However, truncation is not a satisfying solution because it comes at the cost of increased log loss. In theory, is it even possible to reduce entropy while preserving log loss? We prove that it is possible, if we assume access to a black box which can fit models to predict the future entropy of text.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 213,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11983",
        "abs_url": "https://arxiv.org/abs/2511.11983",
        "pdf_url": "https://arxiv.org/pdf/2511.11983",
        "title": "Bayesian--AI Fusion for Epidemiological Decision Making: Calibrated Risk, Honest Uncertainty, and Hyperparameter Intelligence",
        "authors": [
            "Debashis Chatterjee"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Modern epidemiological analytics increasingly use machine learning models that offer strong prediction but often lack calibrated uncertainty. Bayesian methods provide principled uncertainty quantification, yet are viewed as difficult to integrate with contemporary AI workflows. This paper proposes a unified Bayesian and AI framework that combines Bayesian prediction with Bayesian hyperparameter optimization. We use Bayesian logistic regression to obtain calibrated individual-level disease risk and credible intervals on the Pima Indians Diabetes dataset. In parallel, we use Gaussian-process Bayesian optimization to tune penalized Cox survival models on the GBSG2 breast cancer cohort. This yields a two-layer system: a Bayesian predictive layer that represents risk as a posterior distribution, and a Bayesian optimization layer that treats model selection as inference over a black-box objective. Simulation studies in low- and high-dimensional regimes show that the Bayesian layer provides reliable coverage and improved calibration, while Bayesian shrinkage improves AUC, Brier score, and log-loss. Bayesian optimization consistently pushes survival models toward near-oracle concordance. Overall, Bayesian reasoning enhances both what we infer and how we search, enabling calibrated risk and principled hyperparameter intelligence for epidemiological decision making.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 214,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.11993",
        "abs_url": "https://arxiv.org/abs/2511.11993",
        "pdf_url": "https://arxiv.org/pdf/2511.11993",
        "title": "Dynamic Parameter Optimization for Highly Transferable Transformation-Based Attacks",
        "authors": [
            "Jiaming Liang",
            "Chi-Man Pun"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Despite their wide application, the vulnerabilities of deep neural networks raise societal concerns. Among them, transformation-based attacks have demonstrated notable success in transfer attacks. However, existing attacks suffer from blind spots in parameter optimization, limiting their full potential. Specifically, (1) prior work generally considers low-iteration settings, yet attacks perform quite differently at higher iterations, so characterizing overall performance based only on low-iteration results is misleading. (2) Existing attacks use uniform parameters for different surrogate models, iterations, and tasks, which greatly impairs transferability. (3) Traditional transformation parameter optimization relies on grid search. For n parameters with m steps each, the complexity is O(mn). Large computational overhead limits further optimization of parameters. To address these limitations, we conduct an empirical study with various transformations as baselines, revealing three dynamic patterns of transferability with respect to parameter strength. We further propose a novel Concentric Decay Model (CDM) to effectively explain these patterns. Building on these insights, we propose an efficient Dynamic Parameter Optimization (DPO) based on the rise-then-fall pattern, reducing the complexity to O(nlogm). Comprehensive experiments on existing transformation-based attacks across different surrogate models, iterations, and tasks demonstrate that our DPO can significantly improve transferability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 215,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12008",
        "abs_url": "https://arxiv.org/abs/2511.12008",
        "pdf_url": "https://arxiv.org/pdf/2511.12008",
        "title": "Adaptive Diagnostic Reasoning Framework for Pathology with Multimodal Large Language Models",
        "authors": [
            "Yunqi Hong",
            "Johnson Kao",
            "Liam Edwards",
            "Nein-Tzu Liu",
            "Chung-Yen Huang",
            "Alex Oliveira-Kowaleski",
            "Cho-Jui Hsieh",
            "Neil Y.C. Lin"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "AI tools in pathology have improved screening throughput, standardized quantification, and revealed prognostic patterns that inform treatment. However, adoption remains limited because most systems still lack the human-readable reasoning needed to audit decisions and prevent errors. We present RECAP-PATH, an interpretable framework that establishes a self-learning paradigm, shifting off-the-shelf multimodal large language models from passive pattern recognition to evidence-linked diagnostic reasoning. At its core is a two-phase learning process that autonomously derives diagnostic criteria: diversification expands pathology-style explanations, while optimization refines them for accuracy. This self-learning approach requires only small labeled sets and no white-box access or weight updates to generate cancer diagnoses. Evaluated on breast and prostate datasets, RECAP-PATH produced rationales aligned with expert assessment and delivered substantial gains in diagnostic accuracy over baselines. By uniting visual understanding with reasoning, RECAP-PATH provides clinically trustworthy AI and demonstrates a generalizable path toward evidence-linked interpretation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 216,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12034",
        "abs_url": "https://arxiv.org/abs/2511.12034",
        "pdf_url": "https://arxiv.org/pdf/2511.12034",
        "title": "Calibrated Multimodal Representation Learning with Missing Modalities",
        "authors": [
            "Xiaohao Liu",
            "Xiaobo Xia",
            "Jiaheng Wei",
            "Shuo Yang",
            "Xiu Su",
            "See-Kiong Ng",
            "Tat-Seng Chua"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Multimedia (cs.MM)",
        "abstract": "Multimodal representation learning harmonizes distinct modalities by aligning them into a unified latent space. Recent research generalizes traditional cross-modal alignment to produce enhanced multimodal synergy but requires all modalities to be present for a common instance, making it challenging to utilize prevalent datasets with missing modalities. We provide theoretical insights into this issue from an anchor shift perspective. Observed modalities are aligned with a local anchor that deviates from the optimal one when all modalities are present, resulting in an inevitable shift. To address this, we propose CalMRL for multimodal representation learning to calibrate incomplete alignments caused by missing modalities. Specifically, CalMRL leverages the priors and the inherent connections among modalities to model the imputation for the missing ones at the representation level. To resolve the optimization dilemma, we employ a bi-step learning method with the closed-form solution of the posterior distribution of shared latents. We validate its mitigation of anchor shift and convergence with theoretical guidance. By equipping the calibrated alignment with the existing advanced method, we offer new flexibility to absorb data with missing modalities, which is originally unattainable. Extensive experiments and comprehensive analyses demonstrate the superiority of CalMRL. Our code, model checkpoints, and evaluation raw data will be publicly available.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 217,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12046",
        "abs_url": "https://arxiv.org/abs/2511.12046",
        "pdf_url": "https://arxiv.org/pdf/2511.12046",
        "title": "BackWeak: Backdooring Knowledge Distillation Simply with Weak Triggers and Fine-tuning",
        "authors": [
            "Shanmin Wang",
            "Dongdong Zhao"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Knowledge Distillation (KD) is essential for compressing large models, yet relying on pre-trained \"teacher\" models downloaded from third-party repositories introduces serious security risks -- most notably backdoor attacks. Existing KD backdoor methods are typically complex and computationally intensive: they employ surrogate student models and simulated distillation to guarantee transferability, and they construct triggers in a way similar to universal adversarial perturbations (UAPs), which being not stealthy in magnitude, inherently exhibit strong adversarial behavior. This work questions whether such complexity is necessary and constructs stealthy \"weak\" triggers -- imperceptible perturbations that have negligible adversarial effect. We propose BackWeak, a simple, surrogate-free attack paradigm. BackWeak shows that a powerful backdoor can be implanted by simply fine-tuning a benign teacher with a weak trigger using a very small learning rate. We demonstrate that this delicate fine-tuning is sufficient to embed a backdoor that reliably transfers to diverse student architectures during a victim's standard distillation process, yielding high attack success rates. Extensive empirical evaluations on multiple datasets, model architectures, and KD methods show that BackWeak is efficient, simpler, and often more stealthy than previous elaborate approaches. This work calls on researchers studying KD backdoor attacks to pay particular attention to the trigger's stealthiness and its potential adversarial characteristics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 218,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12065",
        "abs_url": "https://arxiv.org/abs/2511.12065",
        "pdf_url": "https://arxiv.org/pdf/2511.12065",
        "title": "Aggregating Conformal Prediction Sets via α-Allocation",
        "authors": [
            "Congbin Xu",
            "Yue Yu",
            "Haojie Ren",
            "Zhaojun Wang",
            "Changliang Zou"
        ],
        "comments": "",
        "subjects": "Methodology (stat.ME); Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Conformal prediction offers a distribution-free framework for constructing prediction sets with finite-sample coverage. Yet, efficiently leveraging multiple conformity scores to reduce prediction set size remains a major open challenge. Instead of selecting a single best score, this work introduces a principled aggregation strategy, COnfidence-Level Allocation (COLA), that optimally allocates confidence levels across multiple conformal prediction sets to minimize empirical set size while maintaining provable coverage. Two variants are further developed, COLA-s and COLA-f, which guarantee finite-sample marginal coverage via sample splitting and full conformalization, respectively. In addition, we develop COLA-l, an individualized allocation strategy that promotes local size efficiency while achieving asymptotic conditional coverage. Extensive experiments on synthetic and real-world datasets demonstrate that COLA achieves considerably smaller prediction sets than state-of-the-art baselines while maintaining valid coverage.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 219,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12073",
        "abs_url": "https://arxiv.org/abs/2511.12073",
        "pdf_url": "https://arxiv.org/pdf/2511.12073",
        "title": "Informed Bootstrap Augmentation Improves EEG Decoding",
        "authors": [
            "Woojae Jeong",
            "Wenhui Cui",
            "Kleanthis Avramidis",
            "Takfarinas Medani",
            "Shrikanth Narayanan",
            "Richard Leahy"
        ],
        "comments": "",
        "subjects": "Signal Processing (eess.SP); Machine Learning (cs.LG)",
        "abstract": "Electroencephalography (EEG) offers detailed access to neural dynamics but remains constrained by noise and trial-by-trial variability, limiting decoding performance in data-restricted or complex paradigms. Data augmentation is often employed to enhance feature representations, yet conventional uniform averaging overlooks differences in trial informativeness and can degrade representational quality. We introduce a weighted bootstrapping approach that prioritizes more reliable trials to generate higher-quality augmented samples. In a Sentence Evaluation paradigm, weights were computed from relative ERP differences and applied during probabilistic sampling and averaging. Across conditions, weighted bootstrapping improved decoding accuracy relative to unweighted (from 68.35% to 71.25% at best), demonstrating that emphasizing reliable trials strengthens representational quality. The results demonstrate that reliability-based augmentation yields more robust and discriminative EEG representations. The code is publicly available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 220,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12081",
        "abs_url": "https://arxiv.org/abs/2511.12081",
        "pdf_url": "https://arxiv.org/pdf/2511.12081",
        "title": "From Scaling to Structured Expressivity: Rethinking Transformers for CTR Prediction",
        "authors": [
            "Bencheng Yan",
            "Yuejie Lei",
            "Zhiyuan Zeng",
            "Di Wang",
            "Kaiyi Lin",
            "Pengjie Wang",
            "Jian Xu",
            "Bo Zheng"
        ],
        "comments": "",
        "subjects": "Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "Despite massive investments in scale, deep models for click-through rate (CTR) prediction often exhibit rapidly diminishing returns - a stark contrast to the smooth, predictable gains seen in large language models. We identify the root cause as a structural misalignment: Transformers assume sequential compositionality, while CTR data demand combinatorial reasoning over high-cardinality semantic fields. Unstructured attention spreads capacity indiscriminately, amplifying noise under extreme sparsity and breaking scalable learning. To restore alignment, we introduce the Field-Aware Transformer (FAT), which embeds field-based interaction priors into attention through decomposed content alignment and cross-field modulation. This design ensures model complexity scales with the number of fields F, not the total vocabulary size n >> F, leading to tighter generalization and, critically, observed power-law scaling in AUC as model width increases. We present the first formal scaling law for CTR models, grounded in Rademacher complexity, that explains and predicts this behavior. On large-scale benchmarks, FAT improves AUC by up to +0.51% over state-of-the-art methods. Deployed online, it delivers +2.33% CTR and +0.66% RPM. Our work establishes that effective scaling in recommendation arises not from size, but from structured expressivity-architectural coherence with data semantics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 221,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12101",
        "abs_url": "https://arxiv.org/abs/2511.12101",
        "pdf_url": "https://arxiv.org/pdf/2511.12101",
        "title": "Decoupled Action Head: Confining Task Knowledge to Conditioning Layers",
        "authors": [
            "Jian Zhou",
            "Sihao Lin",
            "Shuai Fu",
            "Qi WU"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Behavior Cloning (BC) is a data-driven supervised learning approach that has gained increasing attention with the success of scaling laws in language and vision domains. Among its implementations in robotic manipulation, Diffusion Policy (DP), with its two variants DP-CNN (DP-C) and DP-Transformer (DP-T), is one of the most effective and widely adopted models, demonstrating the advantages of predicting continuous action sequences. However, both DP and other BC methods remain constrained by the scarcity of paired training data, and the internal mechanisms underlying DP's effectiveness remain insufficiently understood, leading to limited generalization and a lack of principled design in model development. In this work, we propose a decoupled training recipe that leverages nearly cost-free kinematics-generated trajectories as observation-free data to pretrain a general action head (action generator). The pretrained action head is then frozen and adapted to novel tasks through feature modulation. Our experiments demonstrate the feasibility of this approach in both in-distribution and out-of-distribution scenarios. As an additional benefit, decoupling improves training efficiency; for instance, DP-C achieves up to a 41% speedup. Furthermore, the confinement of task-specific knowledge to the conditioning components under decoupling, combined with the near-identical performance of DP-C in both normal and decoupled training, indicates that the action generation backbone plays a limited role in robotic manipulation. Motivated by this observation, we introduce DP-MLP, which replaces the 244M-parameter U-Net backbone of DP-C with only 4M parameters of simple MLP blocks, achieving a 83.9% faster training speed under normal training and 89.1% under decoupling.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 222,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12162",
        "abs_url": "https://arxiv.org/abs/2511.12162",
        "pdf_url": "https://arxiv.org/pdf/2511.12162",
        "title": "Codebook-Centric Deep Hashing: End-to-End Joint Learning of Semantic Hash Centers and Neural Hash Function",
        "authors": [
            "Shuo Yin",
            "Zhiyuan Yin",
            "Yuqing Hou",
            "Rui Liu",
            "Yong Chen",
            "Dell Zhang"
        ],
        "comments": "14 pages",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Hash center-based deep hashing methods improve upon pairwise or triplet-based approaches by assigning fixed hash centers to each class as learning targets, thereby avoiding the inefficiency of local similarity optimization. However, random center initialization often disregards inter-class semantic relationships. While existing two-stage methods mitigate this by first refining hash centers with semantics and then training the hash function, they introduce additional complexity, computational overhead, and suboptimal performance due to stage-wise discrepancies. To address these limitations, we propose $\\textbf{Center-Reassigned Hashing (CRH)}$, an end-to-end framework that $\\textbf{dynamically reassigns hash centers}$ from a preset codebook while jointly optimizing the hash function. Unlike previous methods, CRH adapts hash centers to the data distribution $\\textbf{without explicit center optimization phases}$, enabling seamless integration of semantic relationships into the learning process. Furthermore, $\\textbf{a multi-head mechanism}$ enhances the representational capacity of hash centers, capturing richer semantic structures. Extensive experiments on three benchmarks demonstrate that CRH learns semantically meaningful hash centers and outperforms state-of-the-art deep hashing methods in retrieval tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 223,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12167",
        "abs_url": "https://arxiv.org/abs/2511.12167",
        "pdf_url": "https://arxiv.org/pdf/2511.12167",
        "title": "Rapid Machine Learning-Driven Detection of Pesticides and Dyes Using Raman Spectroscopy",
        "authors": [
            "Quach Thi Thai Binh",
            "Thuan Phuoc",
            "Xuan Hai",
            "Thang Bach Phan",
            "Vu Thi Hanh Thu",
            "Nguyen Tuan Hung"
        ],
        "comments": "25 pages, 9 figures",
        "subjects": "Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG)",
        "abstract": "The extensive use of pesticides and synthetic dyes poses critical threats to food safety, human health, and environmental sustainability, necessitating rapid and reliable detection methods. Raman spectroscopy offers molecularly specific fingerprints but suffers from spectral noise, fluorescence background, and band overlap, limiting its real-world applicability. Here, we propose a deep learning framework based on ResNet-18 feature extraction, combined with advanced classifiers, including XGBoost, SVM, and their hybrid integration, to detect pesticides and dyes from Raman spectroscopy, called MLRaman. The MLRaman with the CNN-XGBoost model achieved a predictive accuracy of 97.4% and a perfect AUC of 1.0, while it with the CNN-SVM model provided competitive results with robust class-wise discrimination. Dimensionality reduction analyses (PCA, t-SNE, UMAP) confirmed the separability of Raman embeddings across 10 analytes, including 7 pesticides and 3 dyes. Finally, we developed a user-friendly Streamlit application for real-time prediction, which successfully identified unseen Raman spectra from our independent experiments and also literature sources, underscoring strong generalization capacity. This study establishes a scalable, practical MLRaman model for multi-residue contaminant monitoring, with significant potential for deployment in food safety and environmental surveillance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 224,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12182",
        "abs_url": "https://arxiv.org/abs/2511.12182",
        "pdf_url": "https://arxiv.org/pdf/2511.12182",
        "title": "Chemistry-Enhanced Diffusion-Based Framework for Small-to-Large Molecular Conformation Generation",
        "authors": [
            "Yifei Zhu",
            "Jiahui Zhang",
            "Jiawei Peng",
            "Mengge Li",
            "Chao Xu",
            "Zhenggang Lan"
        ],
        "comments": "",
        "subjects": "Chemical Physics (physics.chem-ph); Machine Learning (cs.LG)",
        "abstract": "Obtaining 3D conformations of realistic polyatomic molecules at the quantum chemistry level remains challenging, and although recent machine learning advances offer promise, predicting large-molecule structures still requires substantial computational effort. Here, we introduce StoL, a diffusion model-based framework that enables rapid and knowledge-free generation of large molecular structures from small-molecule data. Remarkably, StoL assembles molecules in a LEGO-style fashion from scratch, without seeing the target molecules or any structures of comparable size during training. Given a SMILES input, it decomposes the molecule into chemically valid fragments, generates their 3D structures with a diffusion model trained on small molecules, and assembles them into diverse conformations. This fragment-based strategy eliminates the need for large-molecule training data while maintaining high scalability and transferability. By embedding chemical principles into key steps, StoL ensures faster convergence, chemically rational structures, and broad configurational coverage, as confirmed against DFT calculations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 225,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12236",
        "abs_url": "https://arxiv.org/abs/2511.12236",
        "pdf_url": "https://arxiv.org/pdf/2511.12236",
        "title": "Consistency Is the Key: Detecting Hallucinations in LLM Generated Text By Checking Inconsistencies About Key Facts",
        "authors": [
            "Raavi Gupta",
            "Pranav Hari Panicker",
            "Sumit Bhatia",
            "Ganesh Ramakrishnan"
        ],
        "comments": "To appear at International Joint Conference on Natural Language Processing & Asia-Pacific Chapter of the Association for Computational Linguistics (IJCNLP-AACL), 2025",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Large language models (LLMs), despite their remarkable text generation capabilities, often hallucinate and generate text that is factually incorrect and not grounded in real-world knowledge. This poses serious risks in domains like healthcare, finance, and customer support. A typical way to use LLMs is via the APIs provided by LLM vendors where there is no access to model weights or options to fine-tune the model. Existing methods to detect hallucinations in such settings where the model access is restricted or constrained by resources typically require making multiple LLM API calls, increasing latency and API cost. We introduce CONFACTCHECK, an efficient hallucination detection approach that does not leverage any external knowledge base and works on the simple intuition that responses to factual probes within the generated text should be consistent within a single LLM and across different LLMs. Rigorous empirical evaluation on multiple datasets that cover both the generation of factual texts and the open generation shows that CONFACTCHECK can detect hallucinated facts efficiently using fewer resources and achieves higher accuracy scores compared to existing baselines that operate under similar conditions. Our code is available here.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 226,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12260",
        "abs_url": "https://arxiv.org/abs/2511.12260",
        "pdf_url": "https://arxiv.org/pdf/2511.12260",
        "title": "Reinforcement Learning for Chemical Ordering in Alloy Nanoparticles",
        "authors": [
            "Jonas Elsborg",
            "Arghya Bhowmik"
        ],
        "comments": "15 pages, 7 figures, 1 table",
        "subjects": "Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG); Computational Physics (physics.comp-ph)",
        "abstract": "We approach the search for optimal element ordering in bimetallic alloy nanoparticles (NPs) as a reinforcement learning (RL) problem, and have built an RL agent that learns to perform such global optimisation using the geometric graph representation of the NPs. To demonstrate the effectiveness, we train an RL agent to perform composition-conserving atomic swap actions on the icosahedral nanoparticle structure. Trained once on randomised $Ag_{X}Au_{309-X}$ compositions and orderings, the agent discovers previously established ground state structure. We show that this optimization is robust to differently ordered initialisations of the same NP compositions. We also demonstrate that a trained policy can extrapolate effectively to NPs of unseen size. However, the efficacy is limited when multiple alloying elements are involved. Our results demonstrate that RL with pre-trained equivariant graph encodings can navigate combinatorial ordering spaces at the nanoparticle scale, and offer a transferable optimisation strategy with the potential to generalise across composition and reduce repeated individual search cost.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 227,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12278",
        "abs_url": "https://arxiv.org/abs/2511.12278",
        "pdf_url": "https://arxiv.org/pdf/2511.12278",
        "title": "PCA++: How Uniformity Induces Robustness to Background Noise in Contrastive Learning",
        "authors": [
            "Mingqi Wu",
            "Qiang Sun",
            "Yi Yang"
        ],
        "comments": "14 pages main, 26 pages appendix",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "High-dimensional data often contain low-dimensional signals obscured by structured background noise, which limits the effectiveness of standard PCA. Motivated by contrastive learning, we address the problem of recovering shared signal subspaces from positive pairs, paired observations sharing the same signal but differing in background. Our baseline, PCA+, uses alignment-only contrastive learning and succeeds when background variation is mild, but fails under strong noise or high-dimensional regimes. To address this, we introduce PCA++, a hard uniformity-constrained contrastive PCA that enforces identity covariance on projected features. PCA++ has a closed-form solution via a generalized eigenproblem, remains stable in high dimensions, and provably regularizes against background interference. We provide exact high-dimensional asymptotics in both fixed-aspect-ratio and growing-spike regimes, showing uniformity's role in robust signal recovery. Empirically, PCA++ outperforms standard PCA and alignment-only PCA+ on simulations, corrupted-MNIST, and single-cell transcriptomics, reliably recovering condition-invariant structure. More broadly, we clarify uniformity's role in contrastive learning, showing that explicit feature dispersion defends against structured noise and enhances robustness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 228,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12281",
        "abs_url": "https://arxiv.org/abs/2511.12281",
        "pdf_url": "https://arxiv.org/pdf/2511.12281",
        "title": "Cmprsr: Abstractive Token-Level Question-Agnostic Prompt Compressor",
        "authors": [
            "Ivan Zakazov",
            "Alexander Sharipov",
            "Berke Argin",
            "Oussama Gabouj",
            "Kamel Charaf",
            "Alexi Semiz",
            "Lorenzo Drudi",
            "Nicolas Baldwin",
            "Robert West"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Motivated by the high costs of using black-box Large Language Models (LLMs), we introduce a novel prompt compression paradigm, under which we use smaller LLMs to compress inputs for the larger ones. We present the first comprehensive LLM-as-a-compressor benchmark spanning 25 open- and closed-source models, which reveals significant disparity in models' compression ability in terms of (i) preserving semantically important information (ii) following the user-provided compression rate (CR). We further improve the performance of gpt-4.1-mini, the best overall vanilla compressor, with Textgrad-based compression meta-prompt optimization. We also identify the most promising open-source vanilla LLM - Qwen3-4B - and post-train it with a combination of supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), pursuing the dual objective of CR adherence and maximizing the downstream task performance. We call the resulting model Cmprsr and demonstrate its superiority over both extractive and vanilla abstractive compression across the entire range of compression rates on lengthy inputs from MeetingBank and LongBench as well as short prompts from GSM8k. The latter highlights Cmprsr's generalizability across varying input lengths and domains. Moreover, Cmprsr closely follows the requested compression rate, offering fine control over the cost-quality trade-off.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 229,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12342",
        "abs_url": "https://arxiv.org/abs/2511.12342",
        "pdf_url": "https://arxiv.org/pdf/2511.12342",
        "title": "Ground Plane Projection for Improved Traffic Analytics at Intersections",
        "authors": [
            "Sajjad Pakdamansavoji",
            "Kumar Vaibhav Jha",
            "Baher Abdulhai",
            "James H Elder"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Accurate turning movement counts at intersections are important for signal control, traffic management and urban planning. Computer vision systems for automatic turning movement counts typically rely on visual analysis in the image plane of an infrastructure camera. Here we explore potential advantages of back-projecting vehicles detected in one or more infrastructure cameras to the ground plane for analysis in real-world 3D coordinates. For single-camera systems we find that back-projection yields more accurate trajectory classification and turning movement counts. We further show that even higher accuracy can be achieved through weak fusion of back-projected detections from multiple cameras. These results suggeest that traffic should be analyzed on the ground plane, not the image plane",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 230,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12359",
        "abs_url": "https://arxiv.org/abs/2511.12359",
        "pdf_url": "https://arxiv.org/pdf/2511.12359",
        "title": "More Than Irrational: Modeling Belief-Biased Agents",
        "authors": [
            "Yifan Zhu",
            "Sammie Katt",
            "Samuel Kaski"
        ],
        "comments": "13 pages, 8 figures. Accepted at the 40th Annual AAAI Conference on Artificial Intelligence (AAAI 2026)",
        "subjects": "Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)",
        "abstract": "Despite the explosive growth of AI and the technologies built upon it, predicting and inferring the sub-optimal behavior of users or human collaborators remains a critical challenge. In many cases, such behaviors are not a result of irrationality, but rather a rational decision made given inherent cognitive bounds and biased beliefs about the world. In this paper, we formally introduce a class of computational-rational (CR) user models for cognitively-bounded agents acting optimally under biased beliefs. The key novelty lies in explicitly modeling how a bounded memory process leads to a dynamically inconsistent and biased belief state and, consequently, sub-optimal sequential decision-making. We address the challenge of identifying the latent user-specific bound and inferring biased belief states from passive observations on the fly. We argue that for our formalized CR model family with an explicit and parameterized cognitive process, this challenge is tractable. To support our claim, we propose an efficient online inference method based on nested particle filtering that simultaneously tracks the user's latent belief state and estimates the unknown cognitive bound from a stream of observed actions. We validate our approach in a representative navigation task using memory decay as an example of a cognitive bound. With simulations, we show that (1) our CR model generates intuitively plausible behaviors corresponding to different levels of memory capacity, and (2) our inference method accurately and efficiently recovers the ground-truth cognitive bounds from limited observations ($\\le 100$ steps). We further demonstrate how this approach provides a principled foundation for developing adaptive AI assistants, enabling adaptive assistance that accounts for the user's memory limitations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 231,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12394",
        "abs_url": "https://arxiv.org/abs/2511.12394",
        "pdf_url": "https://arxiv.org/pdf/2511.12394",
        "title": "Multi-Domain EEG Representation Learning with Orthogonal Mapping and Attention-based Fusion for Cognitive Load Classification",
        "authors": [
            "Prithila Angkan",
            "Amin Jalali",
            "Paul Hungler",
            "Ali Etemad"
        ],
        "comments": "This work has been submitted to the Transactions on Human Machine Systems for possible publication",
        "subjects": "Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)",
        "abstract": "We propose a new representation learning solution for the classification of cognitive load based on Electroencephalogram (EEG). Our method integrates both time and frequency domains by first passing the raw EEG signals through the convolutional encoder to obtain the time domain representations. Next, we measure the Power Spectral Density (PSD) for all five EEG frequency bands and generate the channel power values as 2D images referred to as multi-spectral topography maps. These multi-spectral topography maps are then fed to a separate encoder to obtain the representations in frequency domain. Our solution employs a multi-domain attention module that maps these domain-specific embeddings onto a shared embedding space to emphasize more on important inter-domain relationships to enhance the representations for cognitive load classification. Additionally, we incorporate an orthogonal projection constraint during the training of our method to effectively increase the inter-class distances while improving intra-class clustering. This enhancement allows efficient discrimination between different cognitive states and aids in better grouping of similar states within the feature space. We validate the effectiveness of our model through extensive experiments on two public EEG datasets, CL-Drive and CLARE for cognitive load classification. Our results demonstrate the superiority of our multi-domain approach over the traditional single-domain techniques. Moreover, we conduct ablation and sensitivity analyses to assess the impact of various components of our method. Finally, robustness experiments on different amounts of added noise demonstrate the stability of our method compared to other state-of-the-art solutions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 232,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12397",
        "abs_url": "https://arxiv.org/abs/2511.12397",
        "pdf_url": "https://arxiv.org/pdf/2511.12397",
        "title": "Stochastic Predictive Analytics for Stocks in the Newsvendor Problem",
        "authors": [
            "Pedro A. Pury"
        ],
        "comments": "21 pages, 4 figures",
        "subjects": "Applications (stat.AP); Machine Learning (cs.LG)",
        "abstract": "This work addresses a key challenge in inventory management by developing a stochastic model that describes the dynamic distribution of inventory stock over time without assuming a specific demand distribution. Our model provides a flexible and applicable solution for situations with limited historical data and short-term predictions, making it well-suited for the Newsvendor problem. We evaluate our model's performance using real-world data from a large electronic marketplace, demonstrating its effectiveness in a practical forecasting scenario.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 233,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12421",
        "abs_url": "https://arxiv.org/abs/2511.12421",
        "pdf_url": "https://arxiv.org/pdf/2511.12421",
        "title": "From Black Box to Bijection: Interpreting Machine Learning to Build a Zeta Map Algorithm",
        "authors": [
            "Xiaoyu Huang",
            "Blake Jackson",
            "Kyu-Hwan Lee"
        ],
        "comments": "Extended abstract submitted to the 38th FPSAC (2026, Seattle). 12 pages, 1 figure",
        "subjects": "Combinatorics (math.CO); Machine Learning (cs.LG)",
        "abstract": "There is a large class of problems in algebraic combinatorics which can be distilled into the same challenge: construct an explicit combinatorial bijection. Traditionally, researchers have solved challenges like these by visually inspecting the data for patterns, formulating conjectures, and then proving them. But what is to be done if patterns fail to emerge until the data grows beyond human scale? In this paper, we propose a new workflow for discovering combinatorial bijections via machine learning. As a proof of concept, we train a transformer on paired Dyck paths and use its learned attention patterns to derive a new algorithmic description of the zeta map, which we call the \\textit{Scaffolding Map}.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 234,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12423",
        "abs_url": "https://arxiv.org/abs/2511.12423",
        "pdf_url": "https://arxiv.org/pdf/2511.12423",
        "title": "GRAPHTEXTACK: A Realistic Black-Box Node Injection Attack on LLM-Enhanced GNNs",
        "authors": [
            "Jiaji Ma",
            "Puja Trivedi",
            "Danai Koutra"
        ],
        "comments": "AAAI 2026",
        "subjects": "Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
        "abstract": "Text-attributed graphs (TAGs), which combine structural and textual node information, are ubiquitous across many domains. Recent work integrates Large Language Models (LLMs) with Graph Neural Networks (GNNs) to jointly model semantics and structure, resulting in more general and expressive models that achieve state-of-the-art performance on TAG benchmarks. However, this integration introduces dual vulnerabilities: GNNs are sensitive to structural perturbations, while LLM-derived features are vulnerable to prompt injection and adversarial phrasing. While existing adversarial attacks largely perturb structure or text independently, we find that uni-modal attacks cause only modest degradation in LLM-enhanced GNNs. Moreover, many existing attacks assume unrealistic capabilities, such as white-box access or direct modification of graph data. To address these gaps, we propose GRAPHTEXTACK, the first black-box, multi-modal{, poisoning} node injection attack for LLM-enhanced GNNs. GRAPHTEXTACK injects nodes with carefully crafted structure and semantics to degrade model performance, operating under a realistic threat model without relying on model internals or surrogate models. To navigate the combinatorial, non-differentiable search space of connectivity and feature assignments, GRAPHTEXTACK introduces a novel evolutionary optimization framework with a multi-objective fitness function that balances local prediction disruption and global graph influence. Extensive experiments on five datasets and two state-of-the-art LLM-enhanced GNN models show that GRAPHTEXTACK significantly outperforms 12 strong baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 235,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12449",
        "abs_url": "https://arxiv.org/abs/2511.12449",
        "pdf_url": "https://arxiv.org/pdf/2511.12449",
        "title": "MOON2.0: Dynamic Modality-balanced Multimodal Representation Learning for E-commerce Product Understanding",
        "authors": [
            "Zhanheng Nie",
            "Chenghan Fu",
            "Daoze Zhang",
            "Junxian Wu",
            "Wanxian Guan",
            "Pengjie Wang",
            "Jian Xu",
            "Bo Zheng"
        ],
        "comments": "11 pages, 7 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "The rapid growth of e-commerce calls for multimodal models that comprehend rich visual and textual product information. Although recent multimodal large language models (MLLMs) for product understanding exhibit strong capability in representation learning for e-commerce, they still face three challenges: (i) the modality imbalance induced by modality mixed training; (ii) underutilization of the intrinsic alignment relationships among visual and textual information within a product; and (iii) limited handling of noise in e-commerce multimodal data. To address these, we propose MOON2.0, a dynamic modality-balanced multimodal representation learning framework for e-commerce product understanding. MOON2.0 comprises: (1) a Modality-driven Mixture-of-Experts (MoE) module that adaptively processes input samples by their modality composition, enabling Multimodal Joint Learning to mitigate the modality imbalance; (2) a Dual-level Alignment method to better leverage semantic alignment properties inside individual products; and (3) an MLLM-based Image-text Co-augmentation strategy that integrates textual enrichment with visual expansion, coupled with Dynamic Sample Filtering to improve training data quality. We further introduce MBE2.0, a co-augmented multimodal representation benchmark for e-commerce representation learning and evaluation. Experiments show that MOON2.0 delivers state-of-the-art zero-shot performance on MBE2.0 and multiple public datasets. Furthermore, attention-based heatmap visualization provides qualitative evidence of improved multimodal alignment of MOON2.0.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 236,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12451",
        "abs_url": "https://arxiv.org/abs/2511.12451",
        "pdf_url": "https://arxiv.org/pdf/2511.12451",
        "title": "A Multicollinearity-Aware Signal-Processing Framework for Cross-$β$ Identification via X-ray Scattering of Alzheimer's Tissue",
        "authors": [
            "Abdullah Al Bashit",
            "Prakash Nepal",
            "Lee Makowski"
        ],
        "comments": "19 pages, 4 figures, journal paper under review",
        "subjects": "Image and Video Processing (eess.IV); Machine Learning (cs.LG)",
        "abstract": "X-ray scattering measurements of in situ human brain tissue encode structural signatures of pathological cross-$\\beta$ inclusions, yet systematic exploitation of these data for automated detection remains challenging due to substrate contamination, strong inter-feature correlations, and limited sample sizes. This work develops a three-stage classification framework for identifying cross-$\\beta$ structural inclusions-a hallmark of Alzheimer's disease-in X-ray scattering profiles of post-mortem human brain. Stage 1 employs a Bayes-optimal classifier to separate mica substrate from tissue regions on the basis of their distinct scattering signatures. Stage 2 introduces a multicollinearityaware, class-conditional correlation pruning scheme with formal guarantees on the induced Bayes risk and approximation error, thereby reducing redundancy while retaining class-discriminative information. Stage 3 trains a compact neural network on the pruned feature set to detect the presence or absence of cross-$\\beta$ fibrillar ordering. The top-performing model, optimized with a composite loss combining Focal and Dice objectives, attains a test F1-score of 84.30% using 11 of 211 candidate features and 174 trainable parameters. The overall framework yields an interpretable, theory-grounded strategy for data-limited classification problems involving correlated, high-dimensional experimental measurements, exemplified here by X-ray scattering profiles of neurodegenerative tissue.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 237,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12482",
        "abs_url": "https://arxiv.org/abs/2511.12482",
        "pdf_url": "https://arxiv.org/pdf/2511.12482",
        "title": "Discovering autonomous quantum error correction via deep reinforcement learning",
        "authors": [
            "Yue Yin",
            "Tailong Xiao",
            "Xiaoyang Deng",
            "Ming He",
            "Jianping Fan",
            "Guihua Zeng"
        ],
        "comments": "",
        "subjects": "Quantum Physics (quant-ph); Machine Learning (cs.LG)",
        "abstract": "Quantum error correction is essential for fault-tolerant quantum computing. However, standard methods relying on active measurements may introduce additional errors. Autonomous quantum error correction (AQEC) circumvents this by utilizing engineered dissipation and drives in bosonic systems, but identifying practical encoding remains challenging due to stringent Knill-Laflamme conditions. In this work, we utilize curriculum learning enabled deep reinforcement learning to discover Bosonic codes under approximate AQEC framework to resist both single-photon and double-photon losses. We present an analytical solution of solving the master equation under approximation conditions, which can significantly accelerate the training process of reinforcement learning. The agent first identifies an encoded subspace surpassing the breakeven point through rapid exploration within a constrained evolutionary time-frame, then strategically fine-tunes its policy to sustain this performance advantage over extended temporal horizons. We find that the two-phase trained agent can discover the optimal set of codewords, i.e., the Fock states $\\ket{4}$ and $\\ket{7}$ considering the effect of both single-photon and double-photon loss. We identify that the discovered code surpasses the breakeven threshold over a longer evolution time and achieve the state-of-art performance. We also analyze the robustness of the code against the phase damping and amplitude damping noise. Our work highlights the potential of curriculum learning enabled deep reinforcement learning in discovering the optimal quantum error correct code especially in early fault-tolerant quantum systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 238,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12500",
        "abs_url": "https://arxiv.org/abs/2511.12500",
        "pdf_url": "https://arxiv.org/pdf/2511.12500",
        "title": "Iris: First-Class Multi-GPU Programming Experience in Triton",
        "authors": [
            "Muhammad Awad",
            "Muhammad Osama",
            "Brandon Potter"
        ],
        "comments": "",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)",
        "abstract": "Multi-GPU programming traditionally requires developers to navigate complex trade-offs between performance and programmability. High-performance implementations typically rely on low-level HIP/CUDA communication libraries that demand substantial engineering effort for even basic overlap patterns, while simpler abstractions often sacrifice performance. We present Iris, a multi-GPU communication library implemented entirely in Python and Triton that eliminates this trade-off. Iris provides tile-based symmetric memory abstractions that naturally align with Triton's programming model, enabling developers to write single-source kernels that seamlessly interleave computation and communication. We demonstrate a taxonomy of compute-communication overlap patterns--from bulk-synchronous to fine-grained workgroup specialization--that can be implemented with minimal code changes in Iris, often requiring just a few additional lines within the same Triton kernel. Our evaluation shows that Iris achieves near-optimal bandwidth utilization in microbenchmarks and delivers up to 1.79x speedup over PyTorch and RCCL for GEMM+All-Scatter workloads, demonstrating that high-level implementations can match or exceed heavily-optimized libraries while dramatically simplifying multi-GPU programming.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 239,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12544",
        "abs_url": "https://arxiv.org/abs/2511.12544",
        "pdf_url": "https://arxiv.org/pdf/2511.12544",
        "title": "FERMI-ML: A Flexible and Resource-Efficient Memory-In-Situ SRAM Macro for TinyML acceleration",
        "authors": [
            "Mukul Lokhande",
            "Akash Sankhe",
            "S. V. Jaya Chand",
            "Santosh Kumar Vishvakarma"
        ],
        "comments": "",
        "subjects": "Hardware Architecture (cs.AR); Emerging Technologies (cs.ET); Machine Learning (cs.LG); Image and Video Processing (eess.IV)",
        "abstract": "The growing demand for low-power and area-efficient TinyML inference on AIoT devices necessitates memory architectures that minimise data movement while sustaining high computational efficiency. This paper presents FERMI-ML, a Flexible and Resource-Efficient Memory-In-Situ (MIS) SRAM macro designed for TinyML acceleration. The proposed 9T XNOR-based RX9T bit-cell integrates a 5T storage cell with a 4T XNOR compute unit, enabling variable-precision MAC and CAM operations within the same array. A 22-transistor (C22T) compressor-tree-based accumulator facilitates logarithmic 1-64-bit MAC computation with reduced delay and power compared to conventional adder trees. The 4 KB macro achieves dual functionality for in-situ computation and CAM-based lookup operations, supporting Posit-4 or FP-4 precision. Post-layout results at 65 nm show operation at 350 MHz with 0.9 V, delivering a throughput of 1.93 TOPS and an energy efficiency of 364 TOPS/W, while maintaining a Quality-of-Result (QoR) above 97.5% with InceptionV4 and ResNet-18. FERMI-ML thus demonstrates a compact, reconfigurable, and energy-aware digital Memory-In-Situ macro capable of supporting mixed-precision TinyML workloads.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 240,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12556",
        "abs_url": "https://arxiv.org/abs/2511.12556",
        "pdf_url": "https://arxiv.org/pdf/2511.12556",
        "title": "DLMMPR:Deep Learning-based Measurement Matrix for Phase Retrieval",
        "authors": [
            "Jing Liu",
            "Bing Guo",
            "Ren Zhu"
        ],
        "comments": "",
        "subjects": "Optimization and Control (math.OC); Machine Learning (cs.LG)",
        "abstract": "This paper pioneers the integration of learning optimization into measurement matrix design for phase retrieval. We introduce the Deep Learning-based Measurement Matrix for Phase Retrieval (DLMMPR) algorithm, which parameterizes the measurement matrix within an end-to-end deep learning architecture. Synergistically augmented with subgradient descent and proximal mapping modules for robust recovery, DLMMPR's efficacy is decisively confirmed through comprehensive empirical validation across diverse noise regimes. Benchmarked against DeepMMSE and PrComplex, our method yields substantial gains in PSNR and SSIM, underscoring its superiority.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 241,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12614",
        "abs_url": "https://arxiv.org/abs/2511.12614",
        "pdf_url": "https://arxiv.org/pdf/2511.12614",
        "title": "OPFormer: Object Pose Estimation leveraging foundation model with geometric encoding",
        "authors": [
            "Artem Moroz",
            "Vít Zeman",
            "Martin Mikšík",
            "Elizaveta Isianova",
            "Miroslav David",
            "Pavel Burget",
            "Varun Burde"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)",
        "abstract": "We introduce a unified, end-to-end framework that seamlessly integrates object detection and pose estimation with a versatile onboarding process. Our pipeline begins with an onboarding stage that generates object representations from either traditional 3D CAD models or, in their absence, by rapidly reconstructing a high-fidelity neural representation (NeRF) from multi-view images. Given a test image, our system first employs the CNOS detector to localize target objects. For each detection, our novel pose estimation module, OPFormer, infers the precise 6D pose. The core of OPFormer is a transformer-based architecture that leverages a foundation model for robust feature extraction. It uniquely learns a comprehensive object representation by jointly encoding multiple template views and enriches these features with explicit 3D geometric priors using Normalized Object Coordinate Space (NOCS). A decoder then establishes robust 2D-3D correspondences to determine the final pose. Evaluated on the challenging BOP benchmarks, our integrated system demonstrates a strong balance between accuracy and efficiency, showcasing its practical applicability in both model-based and model-free scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 242,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12642",
        "abs_url": "https://arxiv.org/abs/2511.12642",
        "pdf_url": "https://arxiv.org/pdf/2511.12642",
        "title": "Auto-encoder model for faster generation of effective one-body gravitational waveform approximations",
        "authors": [
            "Suyog Garg",
            "Feng-Li Lin",
            "Kipp Cannon"
        ],
        "comments": "Submitting to PRD",
        "subjects": "General Relativity and Quantum Cosmology (gr-qc); Instrumentation and Methods for Astrophysics (astro-ph.IM); Machine Learning (cs.LG)",
        "abstract": "Upgrades to current gravitational wave detectors for the next observation run and upcoming third-generation observatories, like the Einstein telescope, are expected to have enormous improvements in detection sensitivities and compact object merger event rates. Estimation of source parameters for a wider parameter space that these detectable signals will lie in, will be a computational challenge. Thus, it is imperative to have methods to speed-up the likelihood calculations with theoretical waveform predictions, which can ultimately make the parameter estimation faster and aid in rapid multi-messenger follow-ups. Towards this end, we present a conditional variational auto-encoder model, based on the best performing architecture of Liao+2021, for faster generation of aligned-spin SEOBNRv4 inspiral-merger-ringdown waveforms. Our parameter space consists of four parameters, [$m_1$, $m_2$, $\\chi_1(z)$, $\\chi_2(z)$]. The masses are uniformly sampled in $[5,75]\\,M_{\\odot}$ with a mass ratio limit at $10\\,M_{\\odot}$, while the spins are uniform in $[-0.99,0.99]$. We train the model using $\\sim10^5$ input waveforms data with a 70\\%/10\\% train/validation split, while 20\\% data are reserved for testing. The median mismatch for the generated waveforms in the test dataset is $\\sim10^{-2}$, with better performance in a restricted parameter space of $\\chi_{\\rm eff}\\in[-0.80,0.80]$. Our model is able to generate 100 waveforms in 0.1 second at an average speed of about 4.46 ms per waveform. This is 2-3 orders of magnitude faster than the native SEOBNRv4 implementation in lalsimulation. The latent sampling uncertainty of our model can be quantified with a mean mismatch deviation of $2\\times10^{-1}$ for 1000 generations of the same waveform. Our work aims to be the first step towards developing a production-ready machine learning framework for the faster generation of gravitational waveform approximations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 243,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12643",
        "abs_url": "https://arxiv.org/abs/2511.12643",
        "pdf_url": "https://arxiv.org/pdf/2511.12643",
        "title": "Adaptive Dual-Layer Web Application Firewall (ADL-WAF) Leveraging Machine Learning for Enhanced Anomaly and Threat Detection",
        "authors": [
            "Ahmed Sameh",
            "Sahar Selim"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)",
        "abstract": "Web Application Firewalls are crucial for protecting web applications against a wide range of cyber threats. Traditional Web Application Firewalls often struggle to effectively distinguish between malicious and legitimate traffic, leading to limited efficacy in threat detection. To overcome these limitations, this paper proposes an Adaptive Dual-Layer WAF employing a two-layered Machine Learning model designed to enhance the accuracy of anomaly and threat detection. The first layer employs a Decision Tree (DT) algorithm to detect anomalies by identifying traffic deviations from established normal patterns. The second layer employs Support Vector Machine to classify these anomalies as either threat anomalies or benign anomalies. Our Adaptive Dual Layer WAF incorporates comprehensive data pre-processing and feature engineering techniques and has been thoroughly evaluated using five large benchmark datasets. Evaluation using these datasets shows that ADL WAF achieves a detection accuracy of 99.88% and a precision of 100%, significantly enhancing anomaly detection and reducing false positives. These findings suggest that integrating machine learning techniques into WAFs can substantially improve web application security by providing more accurate and efficient threat detection.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 244,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12668",
        "abs_url": "https://arxiv.org/abs/2511.12668",
        "pdf_url": "https://arxiv.org/pdf/2511.12668",
        "title": "AI Bill of Materials and Beyond: Systematizing Security Assurance through the AI Risk Scanning (AIRS) Framework",
        "authors": [
            "Samuel Nathanson",
            "Alexander Lee",
            "Catherine Chen Kieffer",
            "Jared Junkin",
            "Jessica Ye",
            "Amir Saeed",
            "Melanie Lockhart",
            "Russ Fink",
            "Elisha Peterson",
            "Lanier Watkins"
        ],
        "comments": "13 pages, 4 figures, 6 tables",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Assurance for artificial intelligence (AI) systems remains fragmented across software supply-chain security, adversarial machine learning, and governance documentation. Existing transparency mechanisms - including Model Cards, Datasheets, and Software Bills of Materials (SBOMs) - advance provenance reporting but rarely provide verifiable, machine-readable evidence of model security. This paper introduces the AI Risk Scanning (AIRS) Framework, a threat-model-based, evidence-generating framework designed to operationalize AI assurance. The AIRS Framework evolved through three progressive pilot studies - Smurf (AIBOM schema design), OPAL (operational validation), and Pilot C (AIRS) - that reframed AI documentation from descriptive disclosure toward measurable, evidence-bound verification. The framework aligns its assurance fields to the MITRE ATLAS adversarial ML taxonomy and automatically produces structured artifacts capturing model integrity, packaging and serialization safety, structural adapters, and runtime behaviors. Currently, the AIRS Framework is scoped to provide model-level assurances for LLMs, but it could be expanded to include other modalities and cover system-level threats (e.g. application-layer abuses, tool-calling). A proof-of-concept on a quantized GPT-OSS-20B model demonstrates enforcement of safe loader policies, per-shard hash verification, and contamination and backdoor probes executed under controlled runtime conditions. Comparative analysis with SBOM standards of SPDX 3.0 and CycloneDX 1.6 reveals alignment on identity and evaluation metadata, but identifies critical gaps in representing AI-specific assurance fields. The AIRS Framework thus extends SBOM practice to the AI domain by coupling threat modeling with automated, auditable evidence generation, providing a principled foundation for standardized, trustworthy, and machine-verifiable AI risk documentation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 245,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12688",
        "abs_url": "https://arxiv.org/abs/2511.12688",
        "pdf_url": "https://arxiv.org/pdf/2511.12688",
        "title": "Accelerated Distributional Temporal Difference Learning with Linear Function Approximation",
        "authors": [
            "Kaicheng Jin",
            "Yang Peng",
            "Jiansheng Yang",
            "Zhihua Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "In this paper, we study the finite-sample statistical rates of distributional temporal difference (TD) learning with linear function approximation. The purpose of distributional TD learning is to estimate the return distribution of a discounted Markov decision process for a given policy. Previous works on statistical analysis of distributional TD learning focus mainly on the tabular case. We first consider the linear function approximation setting and conduct a fine-grained analysis of the linear-categorical Bellman equation. Building on this analysis, we further incorporate variance reduction techniques in our new algorithms to establish tight sample complexity bounds independent of the support size $K$ when $K$ is large. Our theoretical results imply that, when employing distributional TD learning with linear function approximation, learning the full distribution of the return function from streaming data is no more difficult than learning its expectation. This work provide new insights into the statistical efficiency of distributional reinforcement learning algorithms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 246,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12694",
        "abs_url": "https://arxiv.org/abs/2511.12694",
        "pdf_url": "https://arxiv.org/pdf/2511.12694",
        "title": "X-VMamba: Explainable Vision Mamba",
        "authors": [
            "Mohamed A. Mabrok",
            "Yalda Zafari"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Dynamical Systems (math.DS)",
        "abstract": "State Space Models (SSMs), particularly the Mamba architecture, have recently emerged as powerful alternatives to Transformers for sequence modeling, offering linear computational complexity while achieving competitive performance. Yet, despite their effectiveness, understanding how these Vision SSMs process spatial information remains challenging due to the lack of transparent, attention-like mechanisms. To address this gap, we introduce a controllability-based interpretability framework that quantifies how different parts of the input sequence (tokens or patches) influence the internal state dynamics of SSMs. We propose two complementary formulations: a Jacobian-based method applicable to any SSM architecture that measures influence through the full chain of state propagation, and a Gramian-based approach for diagonal SSMs that achieves superior speed through closed-form analytical solutions. Both methods operate in a single forward pass with linear complexity, requiring no architectural modifications or hyperparameter tuning. We validate our framework through experiments on three diverse medical imaging modalities, demonstrating that SSMs naturally implement hierarchical feature refinement from diffuse low-level textures in early layers to focused, clinically meaningful patterns in deeper layers. Our analysis reveals domain-specific controllability signatures aligned with diagnostic criteria, progressive spatial selectivity across the network hierarchy, and the substantial influence of scanning strategies on attention patterns. Beyond medical imaging, we articulate applications spanning computer vision, natural language processing, and cross-domain tasks. Our framework establishes controllability analysis as a unified, foundational interpretability paradigm for SSMs across all domains. Code and analysis tools will be made available upon publication",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 247,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12743",
        "abs_url": "https://arxiv.org/abs/2511.12743",
        "pdf_url": "https://arxiv.org/pdf/2511.12743",
        "title": "An Evaluation Framework for Network IDS/IPS Datasets: Leveraging MITRE ATT&CK and Industry Relevance Metrics",
        "authors": [
            "Adrita Rahman Tori",
            "Khondokar Fida Hasan"
        ],
        "comments": "32 Pages",
        "subjects": "Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
        "abstract": "The performance of Machine Learning (ML) and Deep Learning (DL)-based Intrusion Detection and Prevention Systems (IDS/IPS) is critically dependent on the relevance and quality of the datasets used for training and evaluation. However, current AI model evaluation practices for developing IDS/IPS focus predominantly on accuracy metrics, often overlooking whether datasets represent industry-specific threats. To address this gap, we introduce a novel multi-dimensional framework that integrates the MITRE ATT&CK knowledge base for threat intelligence and employs five complementary metrics that together provide a comprehensive assessment of dataset suitability. Methodologically, this framework combines threat intelligence, natural language processing, and quantitative analysis to assess the suitability of datasets for specific industry contexts. Applying this framework to nine publicly available IDS/IPS datasets reveals significant gaps in threat coverage, particularly in the healthcare, energy, and financial sectors. In particular, recent datasets (e.g., CIC-IoMT, CIC-UNSW-NB15) align better with sector-specific threats, whereas others, like CICIoV-24, underperform despite their recency. Our findings provide a standardized, interpretable approach for selecting datasets aligned with sector-specific operational requirements, ultimately enhancing the real-world effectiveness of AI-driven IDS/IPS deployments. The efficiency and practicality of the framework are validated through deployment in a real-world case study, underscoring its capacity to inform dataset selection and enhance the effectiveness of AI-driven IDS/IPS in operational environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 248,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12749",
        "abs_url": "https://arxiv.org/abs/2511.12749",
        "pdf_url": "https://arxiv.org/pdf/2511.12749",
        "title": "TSB-HB: A Hierarchical Bayesian Extension of the TSB Model for Intermittent Demand Forecasting",
        "authors": [
            "Zong-Han Bai",
            "Po-Yen Chu"
        ],
        "comments": "Preprint. 11 pages, 1 figure, Equal contribution by the two authors",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Intermittent demand forecasting poses unique challenges due to sparse observations, cold-start items, and obsolescence. Classical models such as Croston, SBA, and the Teunter-Syntetos-Babai (TSB) method provide simple heuristics but lack a principled generative foundation. Deep learning models address these limitations but often require large datasets and sacrifice interpretability. We introduce TSB-HB, a hierarchical Bayesian extension of TSB. Demand occurrence is modeled with a Beta-Binomial distribution, while nonzero demand sizes follow a Log-Normal distribution. Crucially, hierarchical priors enable partial pooling across items, stabilizing estimates for sparse or cold-start series while preserving heterogeneity. This framework yields a fully generative and interpretable model that generalizes classical exponential smoothing. On the UCI Online Retail dataset, TSB-HB achieves lower RMSE and RMSSE than Croston, SBA, TSB, ADIDA, IMAPA, ARIMA and Theta, and on a subset of the M5 dataset it outperforms all classical baselines we evaluate. The model provides calibrated probabilistic forecasts and improved accuracy on intermittent and lumpy items by combining a generative formulation with hierarchical shrinkage, while remaining interpretable and scalable.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 249,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12755",
        "abs_url": "https://arxiv.org/abs/2511.12755",
        "pdf_url": "https://arxiv.org/pdf/2511.12755",
        "title": "Prompt-Driven Domain Adaptation for End-to-End Autonomous Driving via In-Context RL",
        "authors": [
            "Aleesha Khurram",
            "Amir Moeini",
            "Shangtong Zhang",
            "Rohan Chandra"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
        "abstract": "Despite significant progress and advances in autonomous driving, many end-to-end systems still struggle with domain adaptation (DA), such as transferring a policy trained under clear weather to adverse weather conditions. Typical DA strategies in the literature include collecting additional data in the target domain or re-training the model, or both. Both these strategies quickly become impractical as we increase scale and complexity of driving. These limitations have encouraged investigation into few-shot and zero-shot prompt-driven DA at inference time involving LLMs and VLMs. These methods work by adding a few state-action trajectories during inference to the prompt (similar to in-context learning). However, there are two limitations of such an approach: $(i)$ prompt-driven DA methods are currently restricted to perception tasks such as detection and segmentation and $(ii)$ they require expert few-shot data. In this work, we present a new approach to inference-time few-shot prompt-driven DA for closed-loop autonomous driving in adverse weather condition using in-context reinforcement learning (ICRL). Similar to other prompt-driven DA methods, our approach does not require any updates to the model parameters nor does it require additional data collection in adversarial weather regime. Furthermore, our approach advances the state-of-the-art in prompt-driven DA by extending to closed driving using general trajectories observed during inference. Our experiments using the CARLA simulator show that ICRL results in safer, more efficient, and more comfortable driving policies in the target domain compared to state-of-the-art prompt-driven DA baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 250,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12767",
        "abs_url": "https://arxiv.org/abs/2511.12767",
        "pdf_url": "https://arxiv.org/pdf/2511.12767",
        "title": "RoCoISLR: A Romanian Corpus for Isolated Sign Language Recognition",
        "authors": [
            "Cătălin-Alexandru Rîpanu",
            "Andrei-Theodor Hotnog",
            "Giulia-Stefania Imbrea",
            "Dumitru-Clementin Cercel"
        ],
        "comments": "5 pages, 3 figures, 4 tables",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Automatic sign language recognition plays a crucial role in bridging the communication gap between deaf communities and hearing individuals; however, most available datasets focus on American Sign Language. For Romanian Isolated Sign Language Recognition (RoISLR), no large-scale, standardized dataset exists, which limits research progress. In this work, we introduce a new corpus for RoISLR, named RoCoISLR, comprising over 9,000 video samples that span nearly 6,000 standardized glosses from multiple sources. We establish benchmark results by evaluating seven state-of-the-art video recognition models-I3D, SlowFast, Swin Transformer, TimeSformer, Uniformer, VideoMAE, and PoseConv3D-under consistent experimental setups, and compare their performance with that of the widely used WLASL2000 corpus. According to the results, transformer-based architectures outperform convolutional baselines; Swin Transformer achieved a Top-1 accuracy of 34.1%. Our benchmarks highlight the challenges associated with long-tail class distributions in low-resource sign languages, and RoCoISLR provides the initial foundation for systematic RoISLR research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 251,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12783",
        "abs_url": "https://arxiv.org/abs/2511.12783",
        "pdf_url": "https://arxiv.org/pdf/2511.12783",
        "title": "Function-on-Function Bayesian Optimization",
        "authors": [
            "Jingru Huang",
            "Haijie Xu",
            "Manrui Jiang",
            "Chen Zhang"
        ],
        "comments": "13 pages, 4 figures, conference",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Bayesian optimization (BO) has been widely used to optimize expensive and gradient-free objective functions across various domains. However, existing BO methods have not addressed the objective where both inputs and outputs are functions, which increasingly arise in complex systems as advanced sensing technologies. To fill this gap, we propose a novel function-on-function Bayesian optimization (FFBO) framework. Specifically, we first introduce a function-on-function Gaussian process (FFGP) model with a separable operator-valued kernel to capture the correlations between function-valued inputs and outputs. Compared to existing Gaussian process models, FFGP is modeled directly in the function space. Based on FFGP, we define a scalar upper confidence bound (UCB) acquisition function using a weighted operator-based scalarization strategy. Then, a scalable functional gradient ascent algorithm (FGA) is developed to efficiently identify the optimal function-valued input. We further analyze the theoretical properties of the proposed method. Extensive experiments on synthetic and real-world data demonstrate the superior performance of FFBO over existing approaches.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 252,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12805",
        "abs_url": "https://arxiv.org/abs/2511.12805",
        "pdf_url": "https://arxiv.org/pdf/2511.12805",
        "title": "Practical Causal Evaluation Metrics for Biological Networks",
        "authors": [
            "Noriaki Sato",
            "Marco Scutari",
            "Shuichi Kawano",
            "Rui Yamaguchi",
            "Seiya Imoto"
        ],
        "comments": "15 pages, 1 figure",
        "subjects": "Molecular Networks (q-bio.MN); Machine Learning (cs.LG)",
        "abstract": "Estimating causal networks from biological data is a critical step in systems biology. When evaluating the inferred network, assessing the networks based on their intervention effects is particularly important for downstream probabilistic reasoning and the identification of potential drug targets. In the context of gene regulatory network inference, biological databases are often used as reference sources. These databases typically describe relationships in a qualitative rather than quantitative manner. However, few evaluation metrics have been developed that take this qualitative nature into account. To address this, we developed a metric, the sign-augmented Structural Intervention Distance (sSID), and a weighted sSID that incorporates the net effects of the intervention. Through simulations and analyses of real transcriptomic datasets, we found that our proposed metrics could identify a different algorithm as optimal compared to conventional metrics, and the network selected by sSID had a superior performance in the classification task of clinical covariates using transcriptomic data. This suggests that sSID can distinguish networks that are structurally correct but functionally incorrect, highlighting its potential as a more biologically meaningful and practical evaluation metric.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 253,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12823",
        "abs_url": "https://arxiv.org/abs/2511.12823",
        "pdf_url": "https://arxiv.org/pdf/2511.12823",
        "title": "Enhancing LLM Code Generation Capabilities through Test-Driven Development and Code Interpreter",
        "authors": [
            "Sajed Jalil",
            "Shuvo Saha",
            "Hossain Mohammad Seym"
        ],
        "comments": "AACL-IJCNLP 2025 Workshop BLP Shared Task 2, 6 pages, 7 figures, 3 tables",
        "subjects": "Software Engineering (cs.SE); Machine Learning (cs.LG); Programming Languages (cs.PL)",
        "abstract": "Over the past few years, improving LLM code generation capabilities has been a key focus in NLP research. Despite Bengali having 242 million native speakers worldwide, it receives little attention when it comes to training LLMs. More recently, various fine-tuning and augmented generation techniques have been employed to significantly enhance code generation performance. However, they require considerable expertise and resources to utilize effectively as an end user. The goal of our work is to democratize access to powerful code generation tools in resource-constrained emerging markets, enabling users to leverage them in their native language. We introduce a novel approach that combines Test-Driven Development (TDD) and Code Interpreter (CI), utilizing open-weight models, which improves the baseline accuracy for code generation with Bengali prompts and achieves an overall accuracy of 85%. Our approach requires no finetuning and proves that even the smallest models in the same family can attain up to 98% accuracy compared to the largest models. All of our results are publicly shared in GitHub for validation and reproducibility.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 254,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12827",
        "abs_url": "https://arxiv.org/abs/2511.12827",
        "pdf_url": "https://arxiv.org/pdf/2511.12827",
        "title": "Efficient Adversarial Malware Defense via Trust-Based Raw Override and Confidence-Adaptive Bit-Depth Reduction",
        "authors": [
            "Ayush Chaudhary",
            "Sisir Doppalpudi"
        ],
        "comments": "Accepted at IEEE International Conference on Big Data 2025. 10 pages, 2 figures, 8 tables",
        "subjects": "Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
        "abstract": "The deployment of robust malware detection systems in big data environments requires careful consideration of both security effectiveness and computational efficiency. While recent advances in adversarial defenses have demonstrated strong robustness improvements, they often introduce computational overhead ranging from 4x to 22x, which presents significant challenges for production systems processing millions of samples daily. In this work, we propose a novel framework that combines Trust-Raw Override (TRO) with Confidence-Adaptive Bit-Depth Reduction (CABDR) to explicitly optimize the trade-off between adversarial robustness and computational efficiency. Our approach leverages adaptive confidence-based mechanisms to selectively apply defensive measures, achieving 1.76x computational overhead - a 2.3x improvement over state-of-the-art smoothing defenses. Through comprehensive evaluation on the EMBER v2 dataset comprising 800K samples, we demonstrate that our framework maintains 91 percent clean accuracy while reducing attack success rates to 31-37 percent across multiple attack types, with particularly strong performance against optimization-based attacks such as C and W (48.8 percent reduction). The framework achieves throughput of up to 1.26 million samples per second (measured on pre-extracted EMBER features with no runtime feature extraction), validated across 72 production configurations with statistical significance (5 independent runs, 95 percent confidence intervals, p less than 0.01). Our results suggest that practical adversarial robustness in production environments requires explicit optimization of the efficiency-robustness trade-off, providing a viable path for organizations to deploy robust defenses without prohibitive infrastructure costs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 255,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12836",
        "abs_url": "https://arxiv.org/abs/2511.12836",
        "pdf_url": "https://arxiv.org/pdf/2511.12836",
        "title": "DIGing--SGLD: Decentralized and Scalable Langevin Sampling over Time--Varying Networks",
        "authors": [
            "Waheed U. Bajwa",
            "Mert Gurbuzbalaban",
            "Mustafa Ali Kutbay",
            "Lingjiong Zhu",
            "Muhammad Zulqarnain"
        ],
        "comments": "",
        "subjects": "Optimization and Control (math.OC); Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Sampling from a target distribution induced by training data is central to Bayesian learning, with Stochastic Gradient Langevin Dynamics (SGLD) serving as a key tool for scalable posterior sampling and decentralized variants enabling learning when data are distributed across a network of agents. This paper introduces DIGing-SGLD, a decentralized SGLD algorithm designed for scalable Bayesian learning in multi-agent systems operating over time-varying networks. Existing decentralized SGLD methods are restricted to static network topologies, and many exhibit steady-state sampling bias caused by network effects, even when full batches are used. DIGing-SGLD overcomes these limitations by integrating Langevin-based sampling with the gradient-tracking mechanism of the DIGing algorithm, originally developed for decentralized optimization over time-varying networks, thereby enabling efficient and bias-free sampling without a central coordinator. To our knowledge, we provide the first finite-time non-asymptotic Wasserstein convergence guarantees for decentralized SGLD-based sampling over time-varying networks, with explicit constants. Under standard strong convexity and smoothness assumptions, DIGing-SGLD achieves geometric convergence to an $O(\\sqrt{\\eta})$ neighborhood of the target distribution, where $\\eta$ is the stepsize, with dependence on the target accuracy matching the best-known rates for centralized and static-network SGLD algorithms using constant stepsize. Numerical experiments on Bayesian linear and logistic regression validate the theoretical results and demonstrate the strong empirical performance of DIGing-SGLD under dynamically evolving network conditions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 256,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12840",
        "abs_url": "https://arxiv.org/abs/2511.12840",
        "pdf_url": "https://arxiv.org/pdf/2511.12840",
        "title": "Benign Overfitting in Linear Classifiers with a Bias Term",
        "authors": [
            "Yuta Kondo"
        ],
        "comments": "17 pages",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Modern machine learning models with a large number of parameters often generalize well despite perfectly interpolating noisy training data - a phenomenon known as benign overfitting. A foundational explanation for this in linear classification was recently provided by Hashimoto et al. (2025). However, this analysis was limited to the setting of \"homogeneous\" models, which lack a bias (intercept) term - a standard component in practice. This work directly extends Hashimoto et al.'s results to the more realistic inhomogeneous case, which incorporates a bias term. Our analysis proves that benign overfitting persists in these more complex models. We find that the presence of the bias term introduces new constraints on the data's covariance structure required for generalization, an effect that is particularly pronounced when label noise is present. However, we show that in the isotropic case, these new constraints are dominated by the requirements inherited from the homogeneous model. This work provides a more complete picture of benign overfitting, revealing the non-trivial impact of the bias term on the conditions required for good generalization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 257,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12842",
        "abs_url": "https://arxiv.org/abs/2511.12842",
        "pdf_url": "https://arxiv.org/pdf/2511.12842",
        "title": "Scalable learning of macroscopic stochastic dynamics",
        "authors": [
            "Mengyi Chen",
            "Pengru Huang",
            "Kostya S. Novoselov",
            "Qianxiao Li"
        ],
        "comments": "",
        "subjects": "Computational Physics (physics.comp-ph); Machine Learning (cs.LG)",
        "abstract": "Macroscopic dynamical descriptions of complex physical systems are crucial for understanding and controlling material behavior. With the growing availability of data and compute, machine learning has become a promising alternative to first-principles methods to build accurate macroscopic models from microscopic trajectory simulations. However, for spatially extended systems, direct simulations of sufficiently large microscopic systems that inform macroscopic behavior is prohibitive. In this work, we propose a framework that learns the macroscopic dynamics of large stochastic microscopic systems using only small-system simulations. Our framework employs a partial evolution scheme to generate training data pairs by evolving large-system snapshots within local patches. We subsequently identify the closure variables associated with the macroscopic observables and learn the macroscopic dynamics using a custom loss. Furthermore, we introduce a hierarchical upsampling scheme that enables efficient generation of large-system snapshots from small-system trajectory distributions. We empirically demonstrate the accuracy and robustness of our framework through a variety of stochastic spatially extended systems, including those described by stochastic partial differential equations, idealised lattice spin systems, and a more realistic NbMoTa alloy system.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 258,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12848",
        "abs_url": "https://arxiv.org/abs/2511.12848",
        "pdf_url": "https://arxiv.org/pdf/2511.12848",
        "title": "Structured Imitation Learning of Interactive Policies through Inverse Games",
        "authors": [
            "Max M. Sun",
            "Todd Murphey"
        ],
        "comments": "Presented at the \"Workshop on Generative Modeling Meets Human-Robot Interaction\" at Robotics: Science and Systems 2025. Workshop website: this https URL",
        "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
        "abstract": "Generative model-based imitation learning methods have recently achieved strong results in learning high-complexity motor skills from human demonstrations. However, imitation learning of interactive policies that coordinate with humans in shared spaces without explicit communication remains challenging, due to the significantly higher behavioral complexity in multi-agent interactions compared to non-interactive tasks. In this work, we introduce a structured imitation learning framework for interactive policies by combining generative single-agent policy learning with a flexible yet expressive game-theoretic structure. Our method explicitly separates learning into two steps: first, we learn individual behavioral patterns from multi-agent demonstrations using standard imitation learning; then, we structurally learn inter-agent dependencies by solving an inverse game problem. Preliminary results in a synthetic 5-agent social navigation task show that our method significantly improves non-interactive policies and performs comparably to the ground truth interactive policy using only 50 demonstrations. These results highlight the potential of structured imitation learning in interactive settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 259,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12874",
        "abs_url": "https://arxiv.org/abs/2511.12874",
        "pdf_url": "https://arxiv.org/pdf/2511.12874",
        "title": "Classification of Hope in Textual Data using Transformer-Based Models",
        "authors": [
            "Chukwuebuka Fortunate Ijezue",
            "Tania-Amanda Fredrick Eneye",
            "Maaz Amjad"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "This paper presents a transformer-based approach for classifying hope expressions in text. We developed and compared three architectures (BERT, GPT-2, and DeBERTa) for both binary classification (Hope vs. Not Hope) and multiclass categorization (five hope-related categories). Our initial BERT implementation achieved 83.65% binary and 74.87% multiclass accuracy. In the extended comparison, BERT demonstrated superior performance (84.49% binary, 72.03% multiclass accuracy) while requiring significantly fewer computational resources (443s vs. 704s training time) than newer architectures. GPT-2 showed lowest overall accuracy (79.34% binary, 71.29% multiclass), while DeBERTa achieved moderate results (80.70% binary, 71.56% multiclass) but at substantially higher computational cost (947s for multiclass training). Error analysis revealed architecture-specific strengths in detecting nuanced hope expressions, with GPT-2 excelling at sarcasm detection (92.46% recall). This study provides a framework for computational analysis of hope, with applications in mental health and social media analysis, while demonstrating that architectural suitability may outweigh model size for specialized emotion detection tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 260,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12976",
        "abs_url": "https://arxiv.org/abs/2511.12976",
        "pdf_url": "https://arxiv.org/pdf/2511.12976",
        "title": "MCAQ-YOLO: Morphological Complexity-Aware Quantization for Efficient Object Detection with Curriculum Learning",
        "authors": [
            "Yoonjae Seo",
            "Ermal Elbasani",
            "Jaehong Lee"
        ],
        "comments": "9 pages, 2 figures, 7 tables. Preprint",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Most neural network quantization methods apply uniform bit precision across spatial regions, ignoring the heterogeneous structural and textural complexity of visual data. This paper introduces MCAQ-YOLO, a morphological complexity-aware quantization framework for object detection. The framework employs five morphological metrics - fractal dimension, texture entropy, gradient variance, edge density, and contour complexity - to characterize local visual morphology and guide spatially adaptive bit allocation. By correlating these metrics with quantization sensitivity, MCAQ-YOLO dynamically adjusts bit precision according to spatial complexity. In addition, a curriculum-based quantization-aware training scheme progressively increases quantization difficulty to stabilize optimization and accelerate convergence. Experimental results demonstrate a strong correlation between morphological complexity and quantization sensitivity and show that MCAQ-YOLO achieves superior detection accuracy and convergence efficiency compared with uniform quantization. On a safety equipment dataset, MCAQ-YOLO attains 85.6 percent mAP@0.5 with an average of 4.2 bits and a 7.6x compression ratio, yielding 3.5 percentage points higher mAP than uniform 4-bit quantization while introducing only 1.8 ms of additional runtime overhead per image. Cross-dataset validation on COCO and Pascal VOC further confirms consistent performance gains, indicating that morphology-driven spatial quantization can enhance efficiency and robustness for computationally constrained, safety-critical visual recognition tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 261,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.12995",
        "abs_url": "https://arxiv.org/abs/2511.12995",
        "pdf_url": "https://arxiv.org/pdf/2511.12995",
        "title": "Revealing the dynamic responses of Pb under shock loading based on DFT-accuracy machine learning potential",
        "authors": [
            "Enze Hou",
            "Xiaoyang Wang",
            "Han Wang"
        ],
        "comments": "",
        "subjects": "Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG)",
        "abstract": "Lead (Pb) is a typical low-melting-point ductile metal and serves as an important model material in the study of dynamic responses. Under shock-wave loading, its dynamic mechanical behavior comprises two key phenomena: plastic deformation and shock induced phase transitions. The underlying mechanisms of these processes are still poorly understood. Revealing these mechanisms remains challenging for experimental approaches. Non-equilibrium molecular dynamics (NEMD) simulations are an alternative theoretical tool for studying dynamic responses, as they capture atomic-scale mechanisms such as defect evolution and deformation pathways. However, due to the limited accuracy of empirical interatomic potentials, the reliability of previous NEMD studies is questioned. Using our newly developed machine learning potential for Pb-Sn alloys, we revisited the microstructure evolution in response to shock loading under various shock orientations. The results reveal that shock loading along the [001] orientation of Pb exhibits a fast, reversible, and massive phase transition and stacking fault evolution. The behavior of Pb differs from previous studies by the absence of twinning during plastic deformation. Loading along the [011] orientation leads to slow, irreversible plastic deformation, and a localized FCC-BCC phase transition in the Pitsch orientation relationship. This study provides crucial theoretical insights into the dynamic mechanical response of Pb, offering a theoretical input for understanding the microstructure-performance relationship under extreme conditions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 262,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13019",
        "abs_url": "https://arxiv.org/abs/2511.13019",
        "pdf_url": "https://arxiv.org/pdf/2511.13019",
        "title": "MeanFlow Transformers with Representation Autoencoders",
        "authors": [
            "Zheyuan Hu",
            "Chieh-Hsin Lai",
            "Ge Wu",
            "Yuki Mitsufuji",
            "Stefano Ermon"
        ],
        "comments": "Code is available at this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "MeanFlow (MF) is a diffusion-motivated generative model that enables efficient few-step generation by learning long jumps directly from noise to data. In practice, it is often used as a latent MF by leveraging the pre-trained Stable Diffusion variational autoencoder (SD-VAE) for high-dimensional data modeling. However, MF training remains computationally demanding and is often unstable. During inference, the SD-VAE decoder dominates the generation cost, and MF depends on complex guidance hyperparameters for class-conditional generation. In this work, we develop an efficient training and sampling scheme for MF in the latent space of a Representation Autoencoder (RAE), where a pre-trained vision encoder (e.g., DINO) provides semantically rich latents paired with a lightweight decoder. We observe that naive MF training in the RAE latent space suffers from severe gradient explosion. To stabilize and accelerate training, we adopt Consistency Mid-Training for trajectory-aware initialization and use a two-stage scheme: distillation from a pre-trained flow matching teacher to speed convergence and reduce variance, followed by an optional bootstrapping stage with a one-point velocity estimator to further reduce deviation from the oracle mean flow. This design removes the need for guidance, simplifies training configurations, and reduces computation in both training and sampling. Empirically, our method achieves a 1-step FID of 2.03, outperforming vanilla MF's 3.43, while reducing sampling GFLOPS by 38% and total training cost by 83% on ImageNet 256. We further scale our approach to ImageNet 512, achieving a competitive 1-step FID of 3.23 with the lowest GFLOPS among all baselines. Code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 263,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13025",
        "abs_url": "https://arxiv.org/abs/2511.13025",
        "pdf_url": "https://arxiv.org/pdf/2511.13025",
        "title": "Reconstruction of Manifold Distances from Noisy Observations",
        "authors": [
            "Charles Fefferman",
            "Jonathan Marty",
            "Kevin Ren"
        ],
        "comments": "43 pages",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Differential Geometry (math.DG); Probability (math.PR)",
        "abstract": "We consider the problem of reconstructing the intrinsic geometry of a manifold from noisy pairwise distance observations. Specifically, let $M$ denote a diameter 1 d-dimensional manifold and $\\mu$ a probability measure on $M$ that is mutually absolutely continuous with the volume measure. Suppose $X_1,\\dots,X_N$ are i.i.d. samples of $\\mu$ and we observe noisy-distance random variables $d'(X_j, X_k)$ that are related to the true geodesic distances $d(X_j,X_k)$. With mild assumptions on the distributions and independence of the noisy distances, we develop a new framework for recovering all distances between points in a sufficiently dense subsample of $M$. Our framework improves on previous work which assumed i.i.d. additive noise with known moments. Our method is based on a new way to estimate $L_2$-norms of certain expectation-functions $f_x(y)=\\mathbb{E}d'(x,y)$ and use them to build robust clusters centered at points of our sample. Using a new geometric argument, we establish that, under mild geometric assumptions--bounded curvature and positive injectivity radius--these clusters allow one to recover the true distances between points in the sample up to an additive error of $O(\\varepsilon \\log \\varepsilon^{-1})$. We develop two distinct algorithms for producing these clusters. The first achieves a sample complexity $N \\asymp \\varepsilon^{-2d-2}\\log(1/\\varepsilon)$ and runtime $o(N^3)$. The second introduces novel geometric ideas that warrant further investigation. In the presence of missing observations, we show that a quantitative lower bound on sampling probabilities suffices to modify the cluster construction in the first algorithm and extend all recovery guarantees. Our main technical result also elucidates which properties of a manifold are necessary for the distance recovery, which suggests further extension of our techniques to a broader class of metric probability spaces.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 264,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13071",
        "abs_url": "https://arxiv.org/abs/2511.13071",
        "pdf_url": "https://arxiv.org/pdf/2511.13071",
        "title": "Orientation-Free Neural Network-Based Bias Estimation for Low-Cost Stationary Accelerometers",
        "authors": [
            "Michal Levin",
            "Itzik Klein"
        ],
        "comments": "22 pages, 10 figures",
        "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
        "abstract": "Low-cost micro-electromechanical accelerometers are widely used in navigation, robotics, and consumer devices for motion sensing and position estimation. However, their performance is often degraded by bias errors. To eliminate deterministic bias terms a calibration procedure is applied under stationary conditions. It requires accelerom- eter leveling or complex orientation-dependent calibration procedures. To overcome those requirements, in this paper we present a model-free learning-based calibration method that estimates accelerometer bias under stationary conditions, without requiring knowledge of the sensor orientation and without the need to rotate the sensors. The proposed approach provides a fast, practical, and scalable solution suitable for rapid field deployment. Experimental validation on a 13.39-hour dataset collected from six accelerometers shows that the proposed method consistently achieves error levels more than 52% lower than traditional techniques. On a broader scale, this work contributes to the advancement of accurate calibration methods in orientation-free scenarios. As a consequence, it improves the reliability of low-cost inertial sensors in diverse scientific and industrial applications and eliminates the need for leveled calibration.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 265,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13091",
        "abs_url": "https://arxiv.org/abs/2511.13091",
        "pdf_url": "https://arxiv.org/pdf/2511.13091",
        "title": "STEP: Success-Rate-Aware Trajectory-Efficient Policy Optimization",
        "authors": [
            "Yuhan Chen",
            "Yuxuan Liu",
            "Long Zhang",
            "Pengzhi Gao",
            "Jian Luan",
            "Wei Liu"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Multi-turn interaction remains challenging for online reinforcement learning. A common solution is trajectory-level optimization, which treats each trajectory as a single training sample. However, this approach can be inefficient and yield misleading learning signals: it applies uniform sampling across tasks regardless of difficulty, penalizes correct intermediate actions in failed trajectories, and incurs high sample-collection costs. To address these issues, we propose STEP (Success-rate-aware Trajectory-Efficient Policy optimization), a framework that dynamically allocates sampling based on per-task success rates and performs step-level optimization. STEP maintains a smoothed success-rate record to guide adaptive trajectory resampling, allocating more effort to harder tasks. It then computes success-rate-weighted advantages and decomposes trajectories into step-level samples. Finally, it applies a step-level GRPO augmentation to refine updates for low-success tasks. Experiments on OSWorld and AndroidWorld show that STEP substantially improves sample efficiency and training stability over trajectory-level GRPO, converging faster and generalizing better under the same sampling budget.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 266,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13125",
        "abs_url": "https://arxiv.org/abs/2511.13125",
        "pdf_url": "https://arxiv.org/pdf/2511.13125",
        "title": "Region-Point Joint Representation for Effective Trajectory Similarity Learning",
        "authors": [
            "Hao Long",
            "Silin Zhou",
            "Lisi Chen",
            "Shuo Shang"
        ],
        "comments": "This paper is accepted by AAAI2026",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "Recent learning-based methods have reduced the computational complexity of traditional trajectory similarity computation, but state-of-the-art (SOTA) methods still fail to leverage the comprehensive spectrum of trajectory information for similarity modeling. To tackle this problem, we propose \\textbf{RePo}, a novel method that jointly encodes \\textbf{Re}gion-wise and \\textbf{Po}int-wise features to capture both spatial context and fine-grained moving patterns. For region-wise representation, the GPS trajectories are first mapped to grid sequences, and spatial context are captured by structural features and semantic context enriched by visual features. For point-wise representation, three lightweight expert networks extract local, correlation, and continuous movement patterns from dense GPS sequences. Then, a router network adaptively fuses the learned point-wise features, which are subsequently combined with region-wise features using cross-attention to produce the final trajectory embedding. To train RePo, we adopt a contrastive loss with hard negative samples to provide similarity ranking supervision. Experiment results show that RePo achieves an average accuracy improvement of 22.2\\% over SOTA baselines across all evaluation metrics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 267,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13214",
        "abs_url": "https://arxiv.org/abs/2511.13214",
        "pdf_url": "https://arxiv.org/pdf/2511.13214",
        "title": "Learning to Solve Resource-Constrained Project Scheduling Problems with Duration Uncertainty using Graph Neural Networks",
        "authors": [
            "Guillaume Infantes",
            "Stéphanie Roussel",
            "Antoine Jacquet",
            "Emmanuel Benazera"
        ],
        "comments": "Accepted at ICTAI 2025 Conference",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "The Resource-Constrained Project Scheduling Problem (RCPSP) is a classical scheduling problem that has received significant attention due to of its numerous applications in industry. However, in practice, task durations are subject to uncertainty that must be considered in order to propose resilient scheduling. In this paper, we address the RCPSP variant with uncertain tasks duration (modeled using known probabilities) and aim to minimize the overall expected project duration. Our objective is to produce a baseline schedule that can be reused multiple times in an industrial setting regardless of the actual duration scenario. We leverage Graph Neural Networks in conjunction with Deep Reinforcement Learning (DRL) to develop an effective policy for task scheduling. This policy operates similarly to a priority dispatch rule and is paired with a Serial Schedule Generation Scheme to produce a schedule. Our empirical evaluation on standard benchmarks demonstrates the approach's superiority in terms of performance and its ability to generalize. The developed framework, Wheatley, is made publicly available online to facilitate further research and reproducibility.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 268,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13221",
        "abs_url": "https://arxiv.org/abs/2511.13221",
        "pdf_url": "https://arxiv.org/pdf/2511.13221",
        "title": "Likelihood-guided Regularization in Attention Based Models",
        "authors": [
            "Mohamed Salem",
            "Inyoung Kim"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "The transformer architecture has demonstrated strong performance in classification tasks involving structured and high-dimensional data. However, its success often hinges on large- scale training data and careful regularization to prevent overfitting. In this paper, we intro- duce a novel likelihood-guided variational Ising-based regularization framework for Vision Transformers (ViTs), which simultaneously enhances model generalization and dynamically prunes redundant parameters. The proposed variational Ising-based regularization approach leverages Bayesian sparsification techniques to impose structured sparsity on model weights, allowing for adaptive architecture search during training. Unlike traditional dropout-based methods, which enforce fixed sparsity patterns, the variational Ising-based regularization method learns task-adaptive regularization, improving both efficiency and interpretability. We evaluate our approach on benchmark vision datasets, including MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100, demonstrating improved generalization under sparse, complex data and allowing for principled uncertainty quantification on both weights and selection parameters. Additionally, we show that the Ising regularizer leads to better-calibrated probability estimates and structured feature selection through uncertainty-aware attention mechanisms. Our results highlight the effectiveness of structured Bayesian sparsification in enhancing transformer-based architectures, offering a principled alternative to standard regularization techniques.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 269,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13262",
        "abs_url": "https://arxiv.org/abs/2511.13262",
        "pdf_url": "https://arxiv.org/pdf/2511.13262",
        "title": "Case study of a differentiable heterogeneous multiphysics solver for a nuclear fusion application",
        "authors": [
            "Jack B. Coughlin",
            "Archis Joglekar",
            "Jonathan Brodrick",
            "Alexander Lavin"
        ],
        "comments": "",
        "subjects": "Computational Physics (physics.comp-ph); Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG); Mathematical Software (cs.MS); Plasma Physics (physics.plasm-ph)",
        "abstract": "This work presents a case study of a heterogeneous multiphysics solver from the nuclear fusion domain. At the macroscopic scale, an auto-differentiable ODE solver in JAX computes the evolution of the pulsed power circuit and bulk plasma parameters for a compressing Z Pinch. The ODE solver requires a closure for the impedance of the plasma load obtained via root-finding at every timestep, which we solve efficiently using gradient-based Newton iteration. However, incorporating non-differentiable production-grade plasma solvers like Gkeyll (a C/CUDA plasma simulation suite) into a gradient-based workflow is non-trivial. The ''Tesseract'' software addresses this challenge by providing a multi-physics differentiable abstraction layer made fully compatible with JAX (through the `tesseract_jax` adapter). This architecture ensures end-to-end differentiability while allowing seamless interchange between high-fidelity solvers (Gkeyll), neural surrogates, and analytical approximations for rapid, progressive prototyping.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 270,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13295",
        "abs_url": "https://arxiv.org/abs/2511.13295",
        "pdf_url": "https://arxiv.org/pdf/2511.13295",
        "title": "Causal Inference, Biomarker Discovery, Graph Neural Network, Feature Selection",
        "authors": [
            "Chaowang Lan",
            "Jingxin Wu",
            "Yulong Yuan",
            "Chuxun Liu",
            "Huangyi Kang",
            "Caihua Liu"
        ],
        "comments": "",
        "subjects": "Quantitative Methods (q-bio.QM); Machine Learning (cs.LG)",
        "abstract": "Biomarker discovery from high-throughput transcriptomic data is crucial for advancing precision medicine. However, existing methods often neglect gene-gene regulatory relationships and lack stability across datasets, leading to conflation of spurious correlations with genuine causal effects. To address these issues, we develop a causal graph neural network (Causal-GNN) method that integrates causal inference with multi-layer graph neural networks (GNNs). The key innovation is the incorporation of causal effect estimation for identifying stable biomarkers, coupled with a GNN-based propensity scoring mechanism that leverages cross-gene regulatory networks. Experimental results demonstrate that our method achieves consistently high predictive accuracy across four distinct datasets and four independent classifiers. Moreover, it enables the identification of more stable biomarkers compared to traditional methods. Our work provides a robust, efficient, and biologically interpretable tool for biomarker discovery, demonstrating strong potential for broad application across medical disciplines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 271,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13333",
        "abs_url": "https://arxiv.org/abs/2511.13333",
        "pdf_url": "https://arxiv.org/pdf/2511.13333",
        "title": "AutoMalDesc: Large-Scale Script Analysis for Cyber Threat Research",
        "authors": [
            "Alexandru-Mihai Apostu",
            "Andrei Preda",
            "Alexandra Daniela Damir",
            "Diana Bolocan",
            "Radu Tudor Ionescu",
            "Ioana Croitoru",
            "Mihaela Gaman"
        ],
        "comments": "Accepted at AAAI 2026 (oral)",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Generating thorough natural language explanations for threat detections remains an open problem in cybersecurity research, despite significant advances in automated malware detection systems. In this work, we present AutoMalDesc, an automated static analysis summarization framework that, following initial training on a small set of expert-curated examples, operates independently at scale. This approach leverages an iterative self-paced learning pipeline to progressively enhance output quality through synthetic data generation and validation cycles, eliminating the need for extensive manual data annotation. Evaluation across 3,600 diverse samples in five scripting languages demonstrates statistically significant improvements between iterations, showing consistent gains in both summary quality and classification accuracy. Our comprehensive validation approach combines quantitative metrics based on established malware labels with qualitative assessment from both human experts and LLM-based judges, confirming both technical precision and linguistic coherence of generated summaries. To facilitate reproducibility and advance research in this domain, we publish our complete dataset of more than 100K script samples, including annotated seed (0.9K) and test (3.6K) datasets, along with our methodology and evaluation framework.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 272,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13389",
        "abs_url": "https://arxiv.org/abs/2511.13389",
        "pdf_url": "https://arxiv.org/pdf/2511.13389",
        "title": "Uncovering Causal Drivers of Energy Efficiency for Industrial Process in Foundry via Time-Series Causal Inference",
        "authors": [
            "Zhipeng Ma",
            "Bo Nørregaard Jørgensen",
            "Zheng Grace Ma"
        ],
        "comments": "Accepted by the Energy this http URL Conference 2025 (EI.A 2025)",
        "subjects": "Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "Improving energy efficiency in industrial foundry processes is a critical challenge, as these operations are highly energy-intensive and marked by complex interdependencies among process variables. Correlation-based analyses often fail to distinguish true causal drivers from spurious associations, limiting their usefulness for decision-making. This paper applies a time-series causal inference framework to identify the operational factors that directly affect energy efficiency in induction furnace melting. Using production data from a Danish foundry, the study integrates time-series clustering to segment melting cycles into distinct operational modes with the PCMCI+ algorithm, a state-of-the-art causal discovery method, to uncover cause-effect relationships within each mode. Across clusters, robust causal relations among energy consumption, furnace temperature, and material weight define the core drivers of efficiency, while voltage consistently influences cooling water temperature with a delayed response. Cluster-specific differences further distinguish operational regimes: efficient clusters are characterized by stable causal structures, whereas inefficient ones exhibit reinforcing feedback loops and atypical dependencies. The contributions of this study are twofold. First, it introduces an integrated clustering-causal inference pipeline as a methodological innovation for analyzing energy-intensive processes. Second, it provides actionable insights that enable foundry operators to optimize performance, reduce energy consumption, and lower emissions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 273,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13408",
        "abs_url": "https://arxiv.org/abs/2511.13408",
        "pdf_url": "https://arxiv.org/pdf/2511.13408",
        "title": "Taming Barren Plateaus in Arbitrary Parameterized Quantum Circuits Without Sacrificing Expressibility",
        "authors": [
            "Zhenyu Chen",
            "Yuguo Shao",
            "Zhengwei Liu",
            "Zhaohui Wei"
        ],
        "comments": "",
        "subjects": "Quantum Physics (quant-ph); Computational Complexity (cs.CC); Information Theory (cs.IT); Machine Learning (cs.LG)",
        "abstract": "Quantum algorithms based on parameterized quantum circuits (PQCs) have enabled a wide range of applications on near-term quantum devices. However, existing PQC architectures face several challenges, among which the ``barren plateaus\" phenomenon is particularly prominent. In such cases, the loss function concentrates exponentially with increasing system size, thereby hindering effective parameter optimization. To address this challenge, we propose a general and hardware-efficient method for eliminating barren plateaus in an arbitrary PQC. Specifically, our approach achieves this by inserting a layer of easily implementable quantum channels into the original PQC, each channel requiring only one ancilla qubit and four additional gates, yielding a modified PQC (MPQC) that is provably at least as expressive as the original PQC and, under mild assumptions, is guaranteed to be free from barren plateaus. Furthermore, by appropriately adjusting the structure of MPQCs, we rigorously prove that any parameter in the original PQC can be made trainable. Importantly, the absence of barren plateaus in MPQCs is robust against realistic noise, making our approach directly applicable to current noisy intermediate-scale quantum (NISQ) hardware. Numerically, we demonstrate the practicality of our method by modifying a commonly used PQC for thermal-state preparation. The results show that {barren plateaus are effectively eliminated} in this class of circuits with up to 100 qubits and 2400 layers, whereas the original ansatz suffers from severe gradient vanishing.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 274,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13478",
        "abs_url": "https://arxiv.org/abs/2511.13478",
        "pdf_url": "https://arxiv.org/pdf/2511.13478",
        "title": "Semantic Document Derendering: SVG Reconstruction via Vision-Language Modeling",
        "authors": [
            "Adam Hazimeh",
            "Ke Wang",
            "Mark Collier",
            "Gilles Baechler",
            "Efi Kokiopoulou",
            "Pascal Frossard"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Multimedia documents such as slide presentations and posters are designed to be interactive and easy to modify. Yet, they are often distributed in a static raster format, which limits editing and customization. Restoring their editability requires converting these raster images back into structured vector formats. However, existing geometric raster-vectorization methods, which rely on low-level primitives like curves and polygons, fall short at this task. Specifically, when applied to complex documents like slides, they fail to preserve the high-level structure, resulting in a flat collection of shapes where the semantic distinction between image and text elements is lost. To overcome this limitation, we address the problem of semantic document derendering by introducing SliDer, a novel framework that uses Vision-Language Models (VLMs) to derender slide images as compact and editable Scalable Vector Graphic (SVG) representations. SliDer detects and extracts attributes from individual image and text elements in a raster input and organizes them into a coherent SVG format. Crucially, the model iteratively refines its predictions during inference in a process analogous to human design, generating SVG code that more faithfully reconstructs the original raster upon rendering. Furthermore, we introduce Slide2SVG, a novel dataset comprising raster-SVG pairs of slide documents curated from real-world scientific presentations, to facilitate future research in this domain. Our results demonstrate that SliDer achieves a reconstruction LPIPS of 0.069 and is favored by human evaluators in 82.9% of cases compared to the strongest zero-shot VLM baseline.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 275,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13487",
        "abs_url": "https://arxiv.org/abs/2511.13487",
        "pdf_url": "https://arxiv.org/pdf/2511.13487",
        "title": "Systematic evaluation of time-frequency features for binaural sound source localization",
        "authors": [
            "Davoud Shariat Panah",
            "Alessandro Ragano",
            "Dan Barry",
            "Jan Skoglund",
            "Andrew Hines"
        ],
        "comments": "Submitted to ICASSP 2026",
        "subjects": "Audio and Speech Processing (eess.AS); Machine Learning (cs.LG); Sound (cs.SD)",
        "abstract": "This study presents a systematic evaluation of time-frequency feature design for binaural sound source localization (SSL), focusing on how feature selection influences model performance across diverse conditions. We investigate the performance of a convolutional neural network (CNN) model using various combinations of amplitude-based features (magnitude spectrogram, interaural level difference - ILD) and phase-based features (phase spectrogram, interaural phase difference - IPD). Evaluations on in-domain and out-of-domain data with mismatched head-related transfer functions (HRTFs) reveal that carefully chosen feature combinations often outperform increases in model complexity. While two-feature sets such as ILD + IPD are sufficient for in-domain SSL, generalization to diverse content requires richer inputs combining channel spectrograms with both ILD and IPD. Using the optimal feature sets, our low-complexity CNN model achieves competitive performance. Our findings underscore the importance of feature design in binaural SSL and provide practical guidance for both domain-specific and general-purpose localization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 276,
        "date": "2025-11-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-18?abs=True",
        "arxiv_id": "2511.13503",
        "abs_url": "https://arxiv.org/abs/2511.13503",
        "pdf_url": "https://arxiv.org/pdf/2511.13503",
        "title": "The Shape of Data: Topology Meets Analytics. A Practical Introduction to Topological Analytics and the Stability Index (TSI) in Business",
        "authors": [
            "Ioannis Diamantis"
        ],
        "comments": "36 pages, 22 figures",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Econometrics (econ.EM); Applications (stat.AP)",
        "abstract": "Modern business and economic datasets often exhibit nonlinear, multi-scale structures that traditional linear tools under-represent. Topological Data Analysis (TDA) offers a geometric lens for uncovering robust patterns, such as connected components, loops and voids, across scales. This paper provides an intuitive, figure-driven introduction to persistent homology and a practical, reproducible TDA pipeline for applied analysts. Through comparative case studies in consumer behavior, equity markets (SAX/eSAX vs.\\ TDA) and foreign exchange dynamics, we demonstrate how topological features can reveal segmentation patterns and structural relationships beyond classical statistical methods. We discuss methodological choices regarding distance metrics, complex construction and interpretation, and we introduce the \\textit{Topological Stability Index} (TSI), a simple yet interpretable indicator of structural variability derived from persistence lifetimes. We conclude with practical guidelines for TDA implementation, visualization and communication in business and economic analytics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    }
]