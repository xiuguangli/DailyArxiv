[
    {
        "order": 1,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07436",
        "abs_url": "https://arxiv.org/abs/2511.07436",
        "pdf_url": "https://arxiv.org/pdf/2511.07436",
        "title": "Analysing Environmental Efficiency in AI for X-Ray Diagnosis",
        "authors": [
            "Liam Kearns"
        ],
        "comments": "17 pages, 8 figures",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "The integration of AI tools into medical applications has aimed to improve the efficiency of diagnosis. The emergence of large language models (LLMs), such as ChatGPT and Claude, has expanded this integration even further. Because of LLM versatility and ease of use through APIs, these larger models are often utilised even though smaller, custom models can be used instead. In this paper, LLMs and small discriminative models are integrated into a Mendix application to detect Covid-19 in chest X-rays. These discriminative models are also used to provide knowledge bases for LLMs to improve accuracy. This provides a benchmark study of 14 different model configurations for comparison of accuracy and environmental impact. The findings indicated that while smaller models reduced the carbon footprint of the application, the output was biased towards a positive diagnosis and the output probabilities were lacking confidence. Meanwhile, restricting LLMs to only give probabilistic output caused poor performance in both accuracy and carbon footprint, demonstrating the risk of using LLMs as a universal AI solution. While using the smaller LLM GPT-4.1-Nano reduced the carbon footprint by 94.2% compared to the larger models, this was still disproportionate to the discriminative models; the most efficient solution was the Covid-Net model. Although it had a larger carbon footprint than other small models, its carbon footprint was 99.9% less than when using GPT-4.5-Preview, whilst achieving an accuracy of 95.5%, the highest of all models examined. This paper contributes to knowledge by comparing generative and discriminative models in Covid-19 detection as well as highlighting the environmental risk of using generative tools for classification tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 2,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07437",
        "abs_url": "https://arxiv.org/abs/2511.07437",
        "pdf_url": "https://arxiv.org/pdf/2511.07437",
        "title": "Agentic Educational Content Generation for African Languages on Edge Devices",
        "authors": [
            "Ravi Gupta",
            "Guneet Bhatia"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Addressing educational inequity in Sub-Saharan Africa, this research presents an autonomous agent-orchestrated framework for decentralized, culturally adaptive educational content generation on edge devices. The system leverages four specialized agents that work together to generate contextually appropriate educational content. Experimental validation on platforms including Raspberry Pi 4B and NVIDIA Jetson Nano demonstrates significant performance achievements. InkubaLM on Jetson Nano achieved a Time-To-First-Token (TTFT) of 129 ms, an average inter-token latency of 33 ms, and a throughput of 45.2 tokens per second while consuming 8.4 W. On Raspberry Pi 4B, InkubaLM also led with 326 ms TTFT and 15.9 tokens per second at 5.8 W power consumption. The framework consistently delivered high multilingual quality, averaging a BLEU score of 0.688, cultural relevance of 4.4/5, and fluency of 4.2/5 across tested African languages. Through potential partnerships with active community organizations including African Youth & Community Organization (AYCO) and Florida Africa Foundation, this research aims to establish a practical foundation for accessible, localized, and sustainable AI-driven education in resource-constrained environments. Keeping focus on long-term viability and cultural appropriateness, it contributes to United Nations SDGs 4, 9, and 10. Index Terms - Multi-Agent Systems, Edge AI Computing, Educational Technology, African Languages, Rural Education, Sustainable Development, UN SDG.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 3,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07483",
        "abs_url": "https://arxiv.org/abs/2511.07483",
        "pdf_url": "https://arxiv.org/pdf/2511.07483",
        "title": "Beyond Correctness: Confidence-Aware Reward Modeling for Enhancing Large Language Model Reasoning",
        "authors": [
            "Qianxi He",
            "Qingyu Ren",
            "Shanzhe Lei",
            "Xuhong Wang",
            "Yingchun Wang"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Recent advancements in large language models (LLMs) have shifted the post-training paradigm from traditional instruction tuning and human preference alignment toward reinforcement learning (RL) focused on reasoning capabilities. However, numerous technical reports indicate that purely rule-based reward RL frequently results in poor-quality reasoning chains or inconsistencies between reasoning processes and final answers, particularly when the base model is of smaller scale. During the RL exploration process, models might employ low-quality reasoning chains due to the lack of knowledge, occasionally producing correct answers randomly and receiving rewards based on established rule-based judges. This constrains the potential for resource-limited organizations to conduct direct reinforcement learning training on smaller-scale models. We propose a novel confidence-based reward model tailored for enhancing STEM reasoning capabilities. Unlike conventional approaches, our model penalizes not only incorrect answers but also low-confidence correct responses, thereby promoting more robust and logically consistent reasoning. We validate the effectiveness of our approach through static evaluations, Best-of-N inference tests, and PPO-based RL training. Our method outperforms several state-of-the-art open-source reward models across diverse STEM benchmarks. We release our codes and model in this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 4,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07568",
        "abs_url": "https://arxiv.org/abs/2511.07568",
        "pdf_url": "https://arxiv.org/pdf/2511.07568",
        "title": "Procedural Knowledge Improves Agentic LLM Workflows",
        "authors": [
            "Vincent Hsiao",
            "Mark Roberts",
            "Leslie Smith"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Large language models (LLMs) often struggle when performing agentic tasks without substantial tool support, prom-pt engineering, or fine tuning. Despite research showing that domain-dependent, procedural knowledge can dramatically increase planning efficiency, little work evaluates its potential for improving LLM performance on agentic tasks that may require implicit planning. We formalize, implement, and evaluate an agentic LLM workflow that leverages procedural knowledge in the form of a hierarchical task network (HTN). Empirical results of our implementation show that hand-coded HTNs can dramatically improve LLM performance on agentic tasks, and using HTNs can boost a 20b or 70b parameter LLM to outperform a much larger 120b parameter LLM baseline. Furthermore, LLM-created HTNs improve overall performance, though less so. The results suggest that leveraging expertise--from humans, documents, or LLMs--to curate procedural knowledge will become another important tool for improving LLM workflows.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 5,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07581",
        "abs_url": "https://arxiv.org/abs/2511.07581",
        "pdf_url": "https://arxiv.org/pdf/2511.07581",
        "title": "Think Before You Retrieve: Learning Test-Time Adaptive Search with Small Language Models",
        "authors": [
            "Supriti Vijay",
            "Aman Priyanshu",
            "Anu Vellore",
            "Baturay Saglam",
            "Amin Karbasi"
        ],
        "comments": "37 images, 7 figures, and 15 tables",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Information Retrieval (cs.IR)",
        "abstract": "Effective information retrieval requires reasoning over partial evidence and refining strategies as information emerges. Yet current approaches fall short: neural retrievers lack reasoning capabilities, large language models (LLMs) provide semantic depth but at prohibitive cost, and query rewriting or decomposition limits improvement to static transformations. As a result, existing methods fail to capture the iterative dynamics of exploration, feedback, and revision that complex user queries demand. We introduce Orion, a training framework that enables compact models (350M-1.2B parameters) to perform iterative retrieval through learned search strategies. Orion combines: (1) synthetic trajectory generation and supervised fine-tuning to encourage diverse exploration patterns in models, (2) reinforcement learning (RL) that rewards effective query refinement and backtracking behaviors, and (3) inference-time beam search algorithms that exploit the self-reflection capabilities learned during RL. Despite using only 3% of the training data available, our 1.2B model achieves 77.6% success on SciFact (vs. 72.6% for prior retrievers), 25.2% on BRIGHT (vs. 22.1%), 63.2% on NFCorpus (vs. 57.8%), and remains competitive on FEVER, HotpotQA, and MSMarco. It outperforms retrievers up to 200-400x larger on five of six benchmarks. These findings suggest that retrieval performance can emerge from learned strategies, not just model scale, when models are trained to search, reflect, and revise.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 6,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07587",
        "abs_url": "https://arxiv.org/abs/2511.07587",
        "pdf_url": "https://arxiv.org/pdf/2511.07587",
        "title": "Beyond Fact Retrieval: Episodic Memory for RAG with Generative Semantic Workspaces",
        "authors": [
            "Shreyas Rajesh",
            "Pavan Holur",
            "Chenda Duan",
            "David Chong",
            "Vwani Roychowdhury"
        ],
        "comments": "AAAI 2026 Oral",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Large Language Models (LLMs) face fundamental challenges in long-context reasoning: many documents exceed their finite context windows, while performance on texts that do fit degrades with sequence length, necessitating their augmentation with external memory frameworks. Current solutions, which have evolved from retrieval using semantic embeddings to more sophisticated structured knowledge graphs representations for improved sense-making and associativity, are tailored for fact-based retrieval and fail to build the space-time-anchored narrative representations required for tracking entities through episodic events. To bridge this gap, we propose the \\textbf{Generative Semantic Workspace} (GSW), a neuro-inspired generative memory framework that builds structured, interpretable representations of evolving situations, enabling LLMs to reason over evolving roles, actions, and spatiotemporal contexts. Our framework comprises an \\textit{Operator}, which maps incoming observations to intermediate semantic structures, and a \\textit{Reconciler}, which integrates these into a persistent workspace that enforces temporal, spatial, and logical coherence. On the Episodic Memory Benchmark (EpBench) \\cite{huet_episodic_2025} comprising corpora ranging from 100k to 1M tokens in length, GSW outperforms existing RAG based baselines by up to \\textbf{20\\%}. Furthermore, GSW is highly efficient, reducing query-time context tokens by \\textbf{51\\%} compared to the next most token-efficient baseline, reducing inference time costs considerably. More broadly, GSW offers a concrete blueprint for endowing LLMs with human-like episodic memory, paving the way for more capable agents that can reason over long horizons.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 7,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07667",
        "abs_url": "https://arxiv.org/abs/2511.07667",
        "pdf_url": "https://arxiv.org/pdf/2511.07667",
        "title": "AI-Driven Contribution Evaluation and Conflict Resolution: A Framework & Design for Group Workload Investigation",
        "authors": [
            "Jakub Slapek",
            "Mir Seyedebrahimi",
            "Yang Jianhua"
        ],
        "comments": "20 pages, 8 figures, 8 tables",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "The equitable assessment of individual contribution in teams remains a persistent challenge, where conflict and disparity in workload can result in unfair performance evaluation, often requiring manual intervention - a costly and challenging process. We survey existing tool features and identify a gap in conflict resolution methods and AI integration. To address this, we propose a framework and implementation design for a novel AI-enhanced tool that assists in dispute investigation. The framework organises heterogeneous artefacts - submissions (code, text, media), communications (chat, email), coordination records (meeting logs, tasks), peer assessments, and contextual information - into three dimensions with nine benchmarks: Contribution, Interaction, and Role. Objective measures are normalised, aggregated per dimension, and paired with inequality measures (Gini index) to surface conflict markers. A Large Language Model (LLM) architecture performs validated and contextual analysis over these measures to generate interpretable and transparent advisory judgments. We argue for feasibility under current statutory and institutional policy, and outline practical analytics (sentimental, task fidelity, word/line count, etc.), bias safeguards, limitations, and practical challenges.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 8,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07669",
        "abs_url": "https://arxiv.org/abs/2511.07669",
        "pdf_url": "https://arxiv.org/pdf/2511.07669",
        "title": "Making LLMs Reliable When It Matters Most: A Five-Layer Architecture for High-Stakes Decisions",
        "authors": [
            "Alejandro R. Jadad"
        ],
        "comments": "24 pages, 1 figure, 2 tables",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Current large language models (LLMs) excel in verifiable domains where outputs can be checked before action but prove less reliable for high-stakes strategic decisions with uncertain outcomes. This gap, driven by mutually reinforcing cognitive biases in both humans and artificial intelligence (AI) systems, threatens the defensibility of valuations and sustainability of investments in the sector. This report describes a framework emerging from systematic qualitative assessment across 7 frontier-grade LLMs and 3 market-facing venture vignettes under time pressure. Detailed prompting specifying decision partnership and explicitly instructing avoidance of sycophancy, confabulation, solution drift, and nihilism achieved initial partnership state but failed to maintain it under operational pressure. Sustaining protective partnership state required an emergent 7-stage calibration sequence, built upon a 4-stage initialization process, within a 5-layer protection architecture enabling bias self-monitoring, human-AI adversarial challenge, partnership state verification, performance degradation detection, and stakeholder protection. Three discoveries resulted: partnership state is achievable through ordered calibration but requires emergent maintenance protocols; reliability degrades when architectural drift and context exhaustion align; and dissolution discipline prevents costly pursuit of fundamentally wrong directions. Cross-model validation revealed systematic performance differences across LLM architectures. This approach demonstrates that human-AI teams can achieve cognitive partnership capable of preventing avoidable regret in high-stakes decisions, addressing return-on-investment expectations that depend on AI systems supporting consequential decision-making without introducing preventable cognitive traps when verification arrives too late.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 9,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07678",
        "abs_url": "https://arxiv.org/abs/2511.07678",
        "pdf_url": "https://arxiv.org/pdf/2511.07678",
        "title": "AIA Forecaster: Technical Report",
        "authors": [
            "Rohan Alur",
            "Bradly C. Stadie",
            "Daniel Kang",
            "Ryan Chen",
            "Matt McManus",
            "Michael Rickert",
            "Tyler Lee",
            "Michael Federici",
            "Richard Zhu",
            "Dennis Fogerty",
            "Hayley Williamson",
            "Nina Lozinski",
            "Aaron Linsky",
            "Jasjeet S. Sekhon"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "This technical report describes the AIA Forecaster, a Large Language Model (LLM)-based system for judgmental forecasting using unstructured data. The AIA Forecaster approach combines three core elements: agentic search over high-quality news sources, a supervisor agent that reconciles disparate forecasts for the same event, and a set of statistical calibration techniques to counter behavioral biases in large language models. On the ForecastBench benchmark (Karger et al., 2024), the AIA Forecaster achieves performance equal to human superforecasters, surpassing prior LLM baselines. In addition to reporting on ForecastBench, we also introduce a more challenging forecasting benchmark sourced from liquid prediction markets. While the AIA Forecaster underperforms market consensus on this benchmark, an ensemble combining AIA Forecaster with market consensus outperforms consensus alone, demonstrating that our forecaster provides additive information. Our work establishes a new state of the art in AI forecasting and provides practical, transferable recommendations for future research. To the best of our knowledge, this is the first work that verifiably achieves expert-level forecasting at scale.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 10,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07685",
        "abs_url": "https://arxiv.org/abs/2511.07685",
        "pdf_url": "https://arxiv.org/pdf/2511.07685",
        "title": "ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents",
        "authors": [
            "Manasi Sharma",
            "Chen Bo Calvin Zhang",
            "Chaithanya Bandi",
            "Clinton Wang",
            "Ankit Aich",
            "Huy Nghiem",
            "Tahseen Rabbani",
            "Ye Htet",
            "Brian Jang",
            "Sumana Basu",
            "Aishwarya Balwani",
            "Denis Peskoff",
            "Marcos Ayestaran",
            "Sean M. Hendryx",
            "Brad Kenstler",
            "Bing Liu"
        ],
        "comments": "27 pages, 21 figures, pre-print",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Deep Research (DR) is an emerging agent application that leverages large language models (LLMs) to address open-ended queries. It requires the integration of several capabilities, including multi-step reasoning, cross-document synthesis, and the generation of evidence-backed, long-form answers. Evaluating DR remains challenging because responses are lengthy and diverse, admit many valid solutions, and often depend on dynamic information sources. We introduce ResearchRubrics, a standardized benchmark for DR built with over 2,800+ hours of human labor that pairs realistic, domain-diverse prompts with 2,500+ expert-written, fine-grained rubrics to assess factual grounding, reasoning soundness, and clarity. We also propose a new complexity framework for categorizing DR tasks along three axes: conceptual breadth, logical nesting, and exploration. In addition, we develop human and model-based evaluation protocols that measure rubric adherence for DR agents. We evaluate several state-of-the-art DR systems and find that even leading agents like Gemini's DR and OpenAI's DR achieve under 68% average compliance with our rubrics, primarily due to missed implicit context and inadequate reasoning about retrieved information. Our results highlight the need for robust, scalable assessment of deep research capabilities, to which end we release ResearchRubrics(including all prompts, rubrics, and evaluation code) to facilitate progress toward well-justified research assistants.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 11,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07690",
        "abs_url": "https://arxiv.org/abs/2511.07690",
        "pdf_url": "https://arxiv.org/pdf/2511.07690",
        "title": "Towards AI-Assisted Generation of Military Training Scenarios",
        "authors": [
            "Soham Hans",
            "Volkan Ustun",
            "Benjamin Nye",
            "James Sterrett",
            "Matthew Green"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Achieving expert-level performance in simulation-based training relies on the creation of complex, adaptable scenarios, a traditionally laborious and resource intensive process. Although prior research explored scenario generation for military training, pre-LLM AI tools struggled to generate sufficiently complex or adaptable scenarios. This paper introduces a multi-agent, multi-modal reasoning framework that leverages Large Language Models (LLMs) to generate critical training artifacts, such as Operations Orders (OPORDs). We structure our framework by decomposing scenario generation into a hierarchy of subproblems, and for each one, defining the role of the AI tool: (1) generating options for a human author to select from, (2) producing a candidate product for human approval or modification, or (3) generating textual artifacts fully automatically. Our framework employs specialized LLM-based agents to address distinct subproblems. Each agent receives input from preceding subproblem agents, integrating both text-based scenario details and visual information (e.g., map features, unit positions and applies specialized reasoning to produce appropriate outputs. Subsequent agents process these outputs sequentially, preserving logical consistency and ensuring accurate document generation. This multi-agent strategy overcomes the limitations of basic prompting or single-agent approaches when tackling such highly complex tasks. We validate our framework through a proof-of-concept that generates the scheme of maneuver and movement section of an OPORD while estimating map positions and movements as a precursor demonstrating its feasibility and accuracy. Our results demonstrate the potential of LLM-driven multi-agent systems to generate coherent, nuanced documents and adapt dynamically to changing conditions, advancing automation in scenario generation for military training.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 12,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07842",
        "abs_url": "https://arxiv.org/abs/2511.07842",
        "pdf_url": "https://arxiv.org/pdf/2511.07842",
        "title": "Alignment-Aware Quantization for LLM Safety",
        "authors": [
            "Sunghyun Wee",
            "Suyoung Kim",
            "Hyeonjin Kim",
            "Kyomin Hwang",
            "Nojun Kwak"
        ],
        "comments": "9 pages, 3 figures. Includes 7 pages of supplementary material",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Safety and efficiency are both important factors when deploying large language models(LLMs). LLMs are trained to follow human alignment for safety, and post training quantization(PTQ) is applied afterward for efficiency. However, these two objectives are often in conflict, revealing a fundamental flaw in the conventional PTQ paradigm: quantization can turn into a safety vulnerability if it only aims to achieve low perplexity. Models can demonstrate low perplexity yet exhibit significant degradation in alignment with the safety policy, highlighting that perplexity alone is an insufficient and often misleading proxy for model safety. To address this, we propose Alignment-Aware Quantization(AAQ), a novel approach that integrates Alignment-Preserving Contrastive(APC) loss into the PTQ pipeline. Compared to simple reconstruction loss, ours explicitly preserves alignment by encouraging the quantized model to mimic its safe, instruction-tuned model while diverging from the unaligned, pre-trained counterpart. Our method achieves this robust safety alignment without resorting to specialized safety-focused calibration datasets, highlighting its practical utility and broad applicability. AAQ is compatible with standard PTQ techniques and enables robust 4-bit (W4A4) quantization across diverse model families such as LLaMA, Qwen, and Mistral while maintaining safety where previous methods fail. Our work resolves the critical trade-off between efficiency and safety, paving the way toward LLMs that are both efficient and trustworthy. Anonymized code is available in the supplementary material.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 13,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07850",
        "abs_url": "https://arxiv.org/abs/2511.07850",
        "pdf_url": "https://arxiv.org/pdf/2511.07850",
        "title": "GAMA: A Neural Neighborhood Search Method with Graph-aware Multi-modal Attention for Vehicle Routing Problem",
        "authors": [
            "Xiangling Chen",
            "Yi Mei",
            "Mengjie Zhang"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Recent advances in neural neighborhood search methods have shown potential in tackling Vehicle Routing Problems (VRPs). However, most existing approaches rely on simplistic state representations and fuse heterogeneous information via naive concatenation, limiting their ability to capture rich structural and semantic context. To address these limitations, we propose GAMA, a neural neighborhood search method with Graph-aware Multi-modal Attention model in VRP. GAMA encodes the problem instance and its evolving solution as distinct modalities using graph neural networks, and models their intra- and inter-modal interactions through stacked self- and cross-attention layers. A gated fusion mechanism further integrates the multi-modal representations into a structured state, enabling the policy to make informed and generalizable operator selection decisions. Extensive experiments conducted across various synthetic and benchmark instances demonstrate that the proposed algorithm GAMA significantly outperforms the recent neural baselines. Further ablation studies confirm that both the multi-modal attention mechanism and the gated fusion design play a key role in achieving the observed performance gains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 14,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07863",
        "abs_url": "https://arxiv.org/abs/2511.07863",
        "pdf_url": "https://arxiv.org/pdf/2511.07863",
        "title": "WaterMod: Modular Token-Rank Partitioning for Probability-Balanced LLM Watermarking",
        "authors": [
            "Shinwoo Park",
            "Hyejin Park",
            "Hyeseon Ahn",
            "Yo-Sub Han"
        ],
        "comments": "AAAI 2026 (Oral). This is the extended preprint version including appendices",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Large language models now draft news, legal analyses, and software code with human-level fluency. At the same time, regulations such as the EU AI Act mandate that each synthetic passage carry an imperceptible, machine-verifiable mark for provenance. Conventional logit-based watermarks satisfy this requirement by selecting a pseudorandom green vocabulary at every decoding step and boosting its logits, yet the random split can exclude the highest-probability token and thus erode fluency. WaterMod mitigates this limitation through a probability-aware modular rule. The vocabulary is first sorted in descending model probability; the resulting ranks are then partitioned by the residue rank mod k, which distributes adjacent-and therefore semantically similar-tokens across different classes. A fixed bias of small magnitude is applied to one selected class. In the zero-bit setting (k=2), an entropy-adaptive gate selects either the even or the odd parity as the green list. Because the top two ranks fall into different parities, this choice embeds a detectable signal while guaranteeing that at least one high-probability token remains available for sampling. In the multi-bit regime (k>2), the current payload digit d selects the color class whose ranks satisfy rank mod k = d. Biasing the logits of that class embeds exactly one base-k digit per decoding step, thereby enabling fine-grained provenance tracing. The same modular arithmetic therefore supports both binary attribution and rich payloads. Experimental results demonstrate that WaterMod consistently attains strong watermark detection performance while maintaining generation quality in both zero-bit and multi-bit settings. This robustness holds across a range of tasks, including natural language generation, mathematical reasoning, and code synthesis. Our code and data are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 15,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07890",
        "abs_url": "https://arxiv.org/abs/2511.07890",
        "pdf_url": "https://arxiv.org/pdf/2511.07890",
        "title": "Confidence-Aware Neural Decoding of Overt Speech from EEG: Toward Robust Brain-Computer Interfaces",
        "authors": [
            "Soowon Kim",
            "Byung-Kwan Ko",
            "Seo-Hyun Lee"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Non-invasive brain-computer interfaces that decode spoken commands from electroencephalogram must be both accurate and trustworthy. We present a confidence-aware decoding framework that couples deep ensembles of compact, speech-oriented convolutional networks with post-hoc calibration and selective classification. Uncertainty is quantified using ensemble-based predictive entropy, top-two margin, and mutual information, and decisions are made with an abstain option governed by an accuracy-coverage operating point. The approach is evaluated on a multi-class overt speech dataset using a leakage-safe, block-stratified split that respects temporal contiguity. Compared with widely used baselines, the proposed method yields more reliable probability estimates, improved selective performance across operating points, and balanced per-class acceptance. These results suggest that confidence-aware neural decoding can provide robust, deployment-oriented behavior for real-world brain-computer interface communication systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 16,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07895",
        "abs_url": "https://arxiv.org/abs/2511.07895",
        "pdf_url": "https://arxiv.org/pdf/2511.07895",
        "title": "Toward Robust EEG-based Intention Decoding during Misarticulated Speech in Aphasia",
        "authors": [
            "Ha-Na Jo",
            "Jung-Sun Lee",
            "Eunyeong Ko"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Aphasia severely limits verbal communication due to impaired language production, often leading to frequent misarticulations during speech attempts. Despite growing interest in brain-computer interface technologies, relatively little attention has been paid to developing EEG-based communication support systems tailored for aphasic patients. To address this gap, we recruited a single participant with expressive aphasia and conducted an Korean-based automatic speech task. EEG signals were recorded during task performance, and each trial was labeled as either correct or incorrect depending on whether the intended word was successfully spoken. Spectral analysis revealed distinct neural activation patterns between the two trial types: misarticulated trials exhibited excessive delta power across widespread channels and increased theta-alpha activity in frontal regions. Building upon these findings, we developed a soft multitask learning framework with maximum mean discrepancy regularization that focus on delta features to jointly optimize class discrimination while aligning the EEG feature distributions of correct and misarticulated trials. The proposed model achieved 58.6 % accuracy for correct and 45.5 % for misarticulated trials-outperforming the baseline by over 45 % on the latter-demonstrating robust intention decoding even under articulation errors. These results highlight the feasibility of EEG-based assistive systems capable of supporting real-world, imperfect speech conditions in aphasia patients.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 17,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07896",
        "abs_url": "https://arxiv.org/abs/2511.07896",
        "pdf_url": "https://arxiv.org/pdf/2511.07896",
        "title": "SparseRM: A Lightweight Preference Modeling with Sparse Autoencoder",
        "authors": [
            "Dengcan Liu",
            "Jiahao Li",
            "Zheren Fu",
            "Yi Tu",
            "Jiajun Li",
            "Zhendong Mao",
            "Yongdong Zhang"
        ],
        "comments": "15pages,11figures,AAAI-26",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Reward models (RMs) are a core component in the post-training of large language models (LLMs), serving as proxies for human preference evaluation and guiding model alignment. However, training reliable RMs under limited resources remains challenging due to the reliance on large-scale preference annotations and the high cost of fine-tuning LLMs. To address this, we propose SparseRM, which leverages Sparse Autoencoder (SAE) to extract preference-relevant information encoded in model representations, enabling the construction of a lightweight and interpretable reward model. SparseRM first employs SAE to decompose LLM representations into interpretable directions that capture preference-relevant features. The representations are then projected onto these directions to compute alignment scores, which quantify the strength of each preference feature in the representations. A simple reward head aggregates these scores to predict preference scores. Experiments on three preference modeling tasks show that SparseRM achieves superior performance over most mainstream RMs while using less than 1% of trainable parameters. Moreover, it integrates seamlessly into downstream alignment pipelines, highlighting its potential for efficient alignment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 18,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07897",
        "abs_url": "https://arxiv.org/abs/2511.07897",
        "pdf_url": "https://arxiv.org/pdf/2511.07897",
        "title": "Data Descriptions from Large Language Models with Influence Estimation",
        "authors": [
            "Chaeri Kim",
            "Jaeyeon Bae",
            "Taehwan Kim"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Deep learning models have been successful in many areas but understanding their behaviors still remains a black-box. Most prior explainable AI (XAI) approaches have focused on interpreting and explaining how models make predictions. In contrast, we would like to understand how data can be explained with deep learning model training and propose a novel approach to understand the data via one of the most common media - language - so that humans can easily understand. Our approach proposes a pipeline to generate textual descriptions that can explain the data with large language models by incorporating external knowledge bases. However, generated data descriptions may still include irrelevant information, so we introduce to exploit influence estimation to choose the most informative textual descriptions, along with the CLIP score. Furthermore, based on the phenomenon of cross-modal transferability, we propose a novel benchmark task named cross-modal transfer classification to examine the effectiveness of our textual descriptions. In the experiment of zero-shot setting, we show that our textual descriptions are more effective than other baseline descriptions, and furthermore, we successfully boost the performance of the model trained only on images across all nine image classification datasets. These results are further supported by evaluation using GPT-4o. Through our approach, we may gain insights into the inherent interpretability of the decision-making process of the model.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 19,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07901",
        "abs_url": "https://arxiv.org/abs/2511.07901",
        "pdf_url": "https://arxiv.org/pdf/2511.07901",
        "title": "DANS-KGC: Diffusion Based Adaptive Negative Sampling for Knowledge Graph Completion",
        "authors": [
            "Haoning Li",
            "Qinghua Huang"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Negative sampling (NS) strategies play a crucial role in knowledge graph representation. In order to overcome the limitations of existing negative sampling strategies, such as vulnerability to false negatives, limited generalization, and lack of control over sample hardness, we propose DANS-KGC (Diffusion-based Adaptive Negative Sampling for Knowledge Graph Completion). DANS-KGC comprises three key components: the Difficulty Assessment Module (DAM), the Adaptive Negative Sampling Module (ANS), and the Dynamic Training Mechanism (DTM). DAM evaluates the learning difficulty of entities by integrating semantic and structural features. Based on this assessment, ANS employs a conditional diffusion model with difficulty-aware noise scheduling, leveraging semantic and neighborhood information during the denoising phase to generate negative samples of diverse hardness. DTM further enhances learning by dynamically adjusting the hardness distribution of negative samples throughout training, enabling a curriculum-style progression from easy to hard examples. Extensive experiments on six benchmark datasets demonstrate the effectiveness and generalization ability of DANS-KGC, with the method achieving state-of-the-art results on all three evaluation metrics for the UMLS and YAGO3-10 datasets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 20,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07912",
        "abs_url": "https://arxiv.org/abs/2511.07912",
        "pdf_url": "https://arxiv.org/pdf/2511.07912",
        "title": "Neurophysiological Characteristics of Adaptive Reasoning for Creative Problem-Solving Strategy",
        "authors": [
            "Jun-Young Kim",
            "Young-Seok Kweon",
            "Gi-Hwan Shin",
            "Seong-Whan Lee"
        ],
        "comments": "4 pages, 4 figures, 1 table,",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Adaptive reasoning enables humans to flexibly adjust inference strategies when environmental rules or contexts change, yet its underlying neural dynamics remain unclear. This study investigated the neurophysiological mechanisms of adaptive reasoning using a card-sorting paradigm combined with electroencephalography and compared human performance with that of a multimodal large language model. Stimulus- and feedback-locked analyses revealed coordinated delta-theta-alpha dynamics: early delta-theta activity reflected exploratory monitoring and rule inference, whereas occipital alpha engagement indicated confirmatory stabilization of attention after successful rule identification. In contrast, the multimodal large language model exhibited only short-term feedback-driven adjustments without hierarchical rule abstraction or genuine adaptive reasoning. These findings identify the neural signatures of human adaptive reasoning and highlight the need for brain-inspired artificial intelligence that incorporates oscillatory feedback coordination for true context-sensitive adaptation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 21,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07920",
        "abs_url": "https://arxiv.org/abs/2511.07920",
        "pdf_url": "https://arxiv.org/pdf/2511.07920",
        "title": "Lightweight Diffusion-based Framework for Online Imagined Speech Decoding in Aphasia",
        "authors": [
            "Eunyeong Ko",
            "Soowon Kim",
            "Ha-Na Jo"
        ],
        "comments": "4 pages, 2 figures, 1 table, Name of Conference: International Conference on Brain-Computer Interface",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "A diffusion-based neural decoding framework optimized for real-time imagined speech classification in individuals with aphasia. The system integrates a lightweight conditional diffusion encoder and convolutional classifier trained using subject-specific EEG data acquired from a Korean-language paradigm. A dual-criterion early stopping strategy enabled rapid convergence under limited calibration data, while dropout regularization and grouped temporal convolutions ensured stable generalization. During online operation, continuous EEG streams were processed in two-second sliding windows to generate class probabilities that dynamically modulated visual and auditory feedback according to decoding confidence. Across twenty real-time trials, the framework achieved 65% top-1 and 70% top-2 accuracy, outperforming offline evaluation (50% top-1). These results demonstrate the feasibility of deploying diffusion-based EEG decoding under practical clinical constraints, maintaining reliable performance despite environmental variability and minimal preprocessing. The proposed framework advances the translation of imagined speech brain-computer interfaces toward clinical communication support for individuals with severe expressive language impairment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 22,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07932",
        "abs_url": "https://arxiv.org/abs/2511.07932",
        "pdf_url": "https://arxiv.org/pdf/2511.07932",
        "title": "Computational Blueprints: Generating Isomorphic Mathematics Problems with Large Language Models",
        "authors": [
            "Jeong-Hoon Kim",
            "Jinwoo Nam",
            "Geunsik Jo"
        ],
        "comments": "EMNLP2025 Industry Track",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Personalized mathematics education is growing rapidly, creating a strong demand for large sets of similar practice problems. Yet existing studies on mathematics problem generation have focused on data augmentation for training neural language models rather than on direct educational deployment. To bridge this gap, we define a new task, Isomorphic Math Problem Generation (IMPG), designed to produce structurally consistent variants of source problems. Subsequently, we explored LLM-based frameworks for automatic IMPG through successive refinements, and established Computational Blueprints for Isomorphic Twins (CBIT). With meta-level generation and template-based selective variation, CBIT achieves high mathematical correctness and structural consistency while reducing the cost of generation. Empirical results across refinements demonstrate that CBIT is superior on generation accuracy and cost-effectiveness at scale. Most importantly, CBIT-generated problems exhibited an error rate 17.8% lower than expert-authored items, with deployment to 6,732 learners on a commercial education platform yielding 186,870 interactions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 23,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07936",
        "abs_url": "https://arxiv.org/abs/2511.07936",
        "pdf_url": "https://arxiv.org/pdf/2511.07936",
        "title": "Toward Practical BCI: A Real-time Wireless Imagined Speech EEG Decoding System",
        "authors": [
            "Ji-Ha Park",
            "Heon-Gyu Kwak",
            "Gi-Hwan Shin",
            "Yoo-In Jeon",
            "Sun-Min Park",
            "Ji-Yeon Hwang",
            "Seong-Whan Lee"
        ],
        "comments": "4 pages, 2 figures, 1 table, Name of Conference: International Conference on Brain-Computer Interface",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Brain-computer interface (BCI) research, while promising, has largely been confined to static and fixed environments, limiting real-world applicability. To move towards practical BCI, we introduce a real-time wireless imagined speech electroencephalogram (EEG) decoding system designed for flexibility and everyday use. Our framework focuses on practicality, demonstrating extensibility beyond wired EEG devices to portable, wireless hardware. A user identification module recognizes the operator and provides a personalized, user-specific service. To achieve seamless, real-time operation, we utilize the lab streaming layer to manage the continuous streaming of live EEG signals to the personalized decoder. This end-to-end pipeline enables a functional real-time application capable of classifying user commands from imagined speech EEG signals, achieving an overall 4-class accuracy of 62.00 % on a wired device and 46.67 % on a portable wireless headset. This paper demonstrates a significant step towards truly practical and accessible BCI technology, establishing a clear direction for future research in robust, practical, and personalized neural interfaces.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 24,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07943",
        "abs_url": "https://arxiv.org/abs/2511.07943",
        "pdf_url": "https://arxiv.org/pdf/2511.07943",
        "title": "Thinker: Training LLMs in Hierarchical Thinking for Deep Search via Multi-Turn Interaction",
        "authors": [
            "Jun Xu",
            "Xinkai Du",
            "Yu Ao",
            "Peilong Zhao",
            "Yang Li",
            "Ling Zhong",
            "Lin Yuan",
            "Zhongpu Bo",
            "Xiaorui Wang",
            "Mengshu Sun",
            "Zhengke Gui",
            "Dalong Zhang",
            "Zhaoyang Wang",
            "Qiwei Wang",
            "Yangyang Hou",
            "Zhiying Yin",
            "Haofen Wang",
            "Huajun Chen",
            "Lei Liang",
            "Jun Zhou"
        ],
        "comments": "Accepted to AAAI 2026. Extended version with full Appendix",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Efficient retrieval of external knowledge bases and web pages is crucial for enhancing the reasoning abilities of LLMs. Previous works on training LLMs to leverage external retrievers for solving complex problems have predominantly employed end-to-end reinforcement learning. However, these approaches neglect supervision over the reasoning process, making it difficult to guarantee logical coherence and rigor. To address these limitations, we propose Thinker, a hierarchical thinking model for deep search through multi-turn interaction, making the reasoning process supervisable and verifiable. It decomposes complex problems into independently solvable sub-problems, each dually represented in both natural language and an equivalent logical function to support knowledge base and web searches. Concurrently, dependencies between sub-problems are passed as parameters via these logical functions, enhancing the logical coherence of the problem-solving process. To avoid unnecessary external searches, we perform knowledge boundary determination to check if a sub-problem is within the LLM's intrinsic knowledge, allowing it to answer directly. Experimental results indicate that with as few as several hundred training samples, the performance of Thinker is competitive with established baselines. Furthermore, when scaled to the full training set, Thinker significantly outperforms these methods across various datasets and model sizes. The source code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 25,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07968",
        "abs_url": "https://arxiv.org/abs/2511.07968",
        "pdf_url": "https://arxiv.org/pdf/2511.07968",
        "title": "TimeFlow: Towards Stochastic-Aware and Efficient Time Series Generation via Flow Matching Modeling",
        "authors": [
            "He Panjing",
            "Cheng Mingyue",
            "Li Li",
            "Zhang XiaoHan"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Generating high-quality time series data has emerged as a critical research topic due to its broad utility in supporting downstream time series mining tasks. A major challenge lies in modeling the intrinsic stochasticity of temporal dynamics, as real-world sequences often exhibit random fluctuations and localized variations. While diffusion models have achieved remarkable success, their generation process is computationally inefficient, often requiring hundreds to thousands of expensive function evaluations per sample. Flow matching has emerged as a more efficient paradigm, yet its conventional ordinary differential equation (ODE)-based formulation fails to explicitly capture stochasticity, thereby limiting the fidelity of generated sequences. By contrast, stochastic differential equation (SDE) are naturally suited for modeling randomness and uncertainty. Motivated by these insights, we propose TimeFlow, a novel SDE-based flow matching framework that integrates a encoder-only architecture. Specifically, we design a component-wise decomposed velocity field to capture the multi-faceted structure of time series and augment the vanilla flow-matching optimization with an additional stochastic term to enhance representational expressiveness. TimeFlow is flexible and general, supporting both unconditional and conditional generation tasks within a unified framework. Extensive experiments across diverse datasets demonstrate that our model consistently outperforms strong baselines in generation quality, diversity, and efficiency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 26,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07973",
        "abs_url": "https://arxiv.org/abs/2511.07973",
        "pdf_url": "https://arxiv.org/pdf/2511.07973",
        "title": "Versatile and Risk-Sensitive Cardiac Diagnosis via Graph-Based ECG Signal Representation",
        "authors": [
            "Yue Wang",
            "Yuyang Xu",
            "Renjun Hu",
            "Fanqi Shen",
            "Hanyun Jiang",
            "Jun Wang",
            "Jintai Chen",
            "Danny Z. Chen",
            "Jian Wu",
            "Haochao Ying"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Despite the rapid advancements of electrocardiogram (ECG) signal diagnosis and analysis methods through deep learning, two major hurdles still limit their clinical adoption: the lack of versatility in processing ECG signals with diverse configurations, and the inadequate detection of risk signals due to sample imbalances. Addressing these challenges, we introduce VersAtile and Risk-Sensitive cardiac diagnosis (VARS), an innovative approach that employs a graph-based representation to uniformly model heterogeneous ECG signals. VARS stands out by transforming ECG signals into versatile graph structures that capture critical diagnostic features, irrespective of signal diversity in the lead count, sampling frequency, and duration. This graph-centric formulation also enhances diagnostic sensitivity, enabling precise localization and identification of abnormal ECG patterns that often elude standard analysis methods. To facilitate representation transformation, our approach integrates denoising reconstruction with contrastive learning to preserve raw ECG information while highlighting pathognomonic patterns. We rigorously evaluate the efficacy of VARS on three distinct ECG datasets, encompassing a range of structural variations. The results demonstrate that VARS not only consistently surpasses existing state-of-the-art models across all these datasets but also exhibits substantial improvement in identifying risk signals. Additionally, VARS offers interpretability by pinpointing the exact waveforms that lead to specific model outputs, thereby assisting clinicians in making informed decisions. These findings suggest that our VARS will likely emerge as an invaluable tool for comprehensive cardiac health assessment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 27,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07974",
        "abs_url": "https://arxiv.org/abs/2511.07974",
        "pdf_url": "https://arxiv.org/pdf/2511.07974",
        "title": "Towards Fine-Grained Interpretability: Counterfactual Explanations for Misclassification with Saliency Partition",
        "authors": [
            "Lintong Zhang",
            "Kang Yin",
            "Seong-Whan Lee"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Attribution-based explanation techniques capture key patterns to enhance visual interpretability; however, these patterns often lack the granularity needed for insight in fine-grained tasks, particularly in cases of model misclassification, where explanations may be insufficiently detailed. To address this limitation, we propose a fine-grained counterfactual explanation framework that generates both object-level and part-level interpretability, addressing two fundamental questions: (1) which fine-grained features contribute to model misclassification, and (2) where dominant local features influence counterfactual adjustments. Our approach yields explainable counterfactuals in a non-generative manner by quantifying similarity and weighting component contributions within regions of interest between correctly classified and misclassified samples. Furthermore, we introduce a saliency partition module grounded in Shapley value contributions, isolating features with region-specific relevance. Extensive experiments demonstrate the superiority of our approach in capturing more granular, intuitively meaningful regions, surpassing fine-grained methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 28,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07979",
        "abs_url": "https://arxiv.org/abs/2511.07979",
        "pdf_url": "https://arxiv.org/pdf/2511.07979",
        "title": "Benchmarking Multi-Step Legal Reasoning and Analyzing Chain-of-Thought Effects in Large Language Models",
        "authors": [
            "Wenhan Yu",
            "Xinbo Lin",
            "Lanxin Ni",
            "Jinhua Cheng",
            "Lei Sha"
        ],
        "comments": "21 pages, 7 figures. To appear in AAAI 2026",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Large language models (LLMs) have demonstrated strong reasoning abilities across specialized domains, motivating research into their application to legal reasoning. However, existing legal benchmarks often conflate factual recall with genuine inference, fragment the reasoning process, and overlook the quality of reasoning. To address these limitations, we introduce MSLR, the first Chinese multi-step legal reasoning dataset grounded in real-world judicial decision making. MSLR adopts the IRAC framework (Issue, Rule, Application, Conclusion) to model structured expert reasoning from official legal documents. In addition, we design a scalable Human-LLM collaborative annotation pipeline that efficiently produces fine-grained step-level reasoning annotations and provides a reusable methodological framework for multi-step reasoning datasets. Evaluation of multiple LLMs on MSLR shows only moderate performance, highlighting the challenges of adapting to complex legal reasoning. Further experiments demonstrate that Self-Initiated Chain-of-Thought prompts generated by models autonomously improve reasoning coherence and quality, outperforming human-designed prompts. MSLR contributes to advancing LLM reasoning and Chain-of-Thought strategies and offers open resources for future research. The dataset and code are available at this https URL and this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 29,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07980",
        "abs_url": "https://arxiv.org/abs/2511.07980",
        "pdf_url": "https://arxiv.org/pdf/2511.07980",
        "title": "Capturing Complex Spatial-Temporal Dependencies in Traffic Forecasting: A Self-Attention Approach",
        "authors": [
            "Zheng Chenghong",
            "Zongyin Deng",
            "Liu Cheng",
            "Xiong Simin",
            "Di Deshi",
            "Li Guanyao"
        ],
        "comments": "5 pages",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "We study the problem of traffic forecasting, aiming to predict the inflow and outflow of a region in the subsequent time slot. The problem is complex due to the intricate spatial and temporal interdependence among regions. Prior works study the spatial and temporal dependency in a decouple manner, failing to capture their joint effect. In this work, we propose ST-SAM, a novel and efficient Spatial-Temporal Self-Attention Model for traffic forecasting. ST-SAM uses a region embedding layer to learn time-specific embedding from traffic data for regions. Then, it employs a spatial-temporal dependency learning module based on self-attention mechanism to capture the joint spatial-temporal dependency for both nearby and faraway regions. ST-SAM entirely relies on self-attention to capture both local and global spatial-temporal correlations, which make it effective and efficient. Extensive experiments on two real world datasets show that ST-SAM is substantially more accurate and efficient than the state-of-the-art approaches (with an average improvement of up to 15% on RMSE, 17% on MAPE, and 32 times on training time in our experiments).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 30,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07988",
        "abs_url": "https://arxiv.org/abs/2511.07988",
        "pdf_url": "https://arxiv.org/pdf/2511.07988",
        "title": "The One Where They Brain-Tune for Social Cognition: Multi-Modal Brain-Tuning on Friends",
        "authors": [
            "Nico Policzer",
            "Cameron Braunstein",
            "Mariya Toneva"
        ],
        "comments": "20 pages, 7 figures. Appearing at the NeurIPS 2025 Workshop on Interpreting Cognition in Deep Learning Models",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Recent studies on audio models show brain-tuning - fine-tuning models to better predict corresponding fMRI activity - improves brain alignment and increases performance on downstream semantic and audio tasks. We extend this approach to a multimodal audio-video model to enhance social cognition, targeting the Superior Temporal Sulcus (STS), a key region for social processing, while subjects watch Friends. We find significant increases in brain alignment to the STS and an adjacent ROI, as well as improvements to a social cognition task related to the training data - sarcasm detection in sitcoms. In summary, our study extends brain-tuning to the multi-modal domain, demonstrating improvements to a downstream task after tuning to a relevant functional region.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 31,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07991",
        "abs_url": "https://arxiv.org/abs/2511.07991",
        "pdf_url": "https://arxiv.org/pdf/2511.07991",
        "title": "VSPO: Validating Semantic Pitfalls in Ontology via LLM-Based CQ Generation",
        "authors": [
            "Hyojun Choi",
            "Seokju Hwang",
            "Kyong-Ho Lee"
        ],
        "comments": "Accepted at AAAI 2026 oral",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Competency Questions (CQs) play a crucial role in validating ontology design. While manually crafting CQs can be highly time-consuming and costly for ontology engineers, recent studies have explored the use of large language models (LLMs) to automate this process. However, prior approaches have largely evaluated generated CQs based on their similarity to existing datasets, which often fail to verify semantic pitfalls such as \"Misusing allValuesFrom\". Since such pitfalls cannot be reliably detected through rule-based methods, we propose a novel dataset and model of Validating Semantic Pitfalls in Ontology (VSPO) for CQ generation specifically designed to verify the semantic pitfalls. To simulate missing and misused axioms, we use LLMs to generate natural language definitions of classes and properties and introduce misalignments between the definitions and the ontology by removing axioms or altering logical operators (e.g., substituting union with intersection). We then fine-tune LLaMA-3.1-8B-Instruct to generate CQs that validate these semantic discrepancies between the provided definitions and the corresponding axioms. The resulting CQs can detect a broader range of modeling errors compared to existing public datasets. Our fine-tuned model demonstrates superior performance over baselines, showing 26% higher precision and 28.2% higher recall than GPT-4.1 in generating CQs for pitfall validation. This research enables automatic generation of TBox-validating CQs using LLMs, significantly reducing manual effort while improving semantic alignment between ontologies and expert knowledge. To the best of our knowledge, this is the first study to target semantic pitfall validation in CQ generation using LLMs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 32,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07994",
        "abs_url": "https://arxiv.org/abs/2511.07994",
        "pdf_url": "https://arxiv.org/pdf/2511.07994",
        "title": "Enhancing Logical Expressiveness in Graph Neural Networks via Path-Neighbor Aggregation",
        "authors": [
            "Han Yu",
            "Xiaojuan Zhao",
            "Aiping Li",
            "Kai Chen",
            "Ziniu Liu",
            "Zhichao Peng"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Graph neural networks (GNNs) can effectively model structural information of graphs, making them widely used in knowledge graph (KG) reasoning. However, existing studies on the expressive power of GNNs mainly focuses on simple single-relation graphs, and there is still insufficient discussion on the power of GNN to express logical rules in KGs. How to enhance the logical expressive power of GNNs is still a key issue. Motivated by this, we propose Path-Neighbor enhanced GNN (PN-GNN), a method to enhance the logical expressive power of GNN by aggregating node-neighbor embeddings on the reasoning path. First, we analyze the logical expressive power of existing GNN-based methods and point out the shortcomings of the expressive power of these methods. Then, we theoretically investigate the logical expressive power of PN-GNN, showing that it not only has strictly stronger expressive power than C-GNN but also that its $(k+1)$-hop logical expressiveness is strictly superior to that of $k$-hop. Finally, we evaluate the logical expressive power of PN-GNN on six synthetic datasets and two real-world datasets. Both theoretical analysis and extensive experiments confirm that PN-GNN enhances the expressive power of logical rules without compromising generalization, as evidenced by its competitive performance in KG reasoning tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 33,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07995",
        "abs_url": "https://arxiv.org/abs/2511.07995",
        "pdf_url": "https://arxiv.org/pdf/2511.07995",
        "title": "Multivariate Time series Anomaly Detection:A Framework of Hidden Markov Models",
        "authors": [
            "Jinbo Li",
            "Witold Pedrycz",
            "Iqbal Jamal"
        ],
        "comments": "25 pages, 8 figures, 6 tables",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "In this study, we develop an approach to multivariate time series anomaly detection focused on the transformation of multivariate time series to univariate time series. Several transformation techniques involving Fuzzy C-Means (FCM) clustering and fuzzy integral are studied. In the sequel, a Hidden Markov Model (HMM), one of the commonly encountered statistical methods, is engaged here to detect anomalies in multivariate time series. We construct HMM-based anomaly detectors and in this context compare several transformation methods. A suite of experimental studies along with some comparative analysis is reported.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 34,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08008",
        "abs_url": "https://arxiv.org/abs/2511.08008",
        "pdf_url": "https://arxiv.org/pdf/2511.08008",
        "title": "Combining LLM Semantic Reasoning with GNN Structural Modeling for Multi-view Multi-Label Feature Selection",
        "authors": [
            "Zhiqi Chen",
            "Yuzhou Liu",
            "Jiarui Liu",
            "Wanfu Gao"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Multi-view multi-label feature selection aims to identify informative features from heterogeneous views, where each sample is associated with multiple interdependent labels. This problem is particularly important in machine learning involving high-dimensional, multimodal data such as social media, bioinformatics or recommendation systems. Existing Multi-View Multi-Label Feature Selection (MVMLFS) methods mainly focus on analyzing statistical information of data, but seldom consider semantic information. In this paper, we aim to use these two types of information jointly and propose a method that combines Large Language Models (LLMs) semantic reasoning with Graph Neural Networks (GNNs) structural modeling for MVMLFS. Specifically, the method consists of three main components. (1) LLM is first used as an evaluation agent to assess the latent semantic relevance among feature, view, and label descriptions. (2) A semantic-aware heterogeneous graph with two levels is designed to represent relations among features, views and labels: one is a semantic graph representing semantic relations, and the other is a statistical graph. (3) A lightweight Graph Attention Network (GAT) is applied to learn node embedding in the heterogeneous graph as feature saliency scores for ranking and selection. Experimental results on multiple benchmark datasets demonstrate the superiority of our method over state-of-the-art baselines, and it is still effective when applied to small-scale datasets, showcasing its robustness, flexibility, and generalization ability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 35,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08022",
        "abs_url": "https://arxiv.org/abs/2511.08022",
        "pdf_url": "https://arxiv.org/pdf/2511.08022",
        "title": "Numerical Sensitivity and Robustness: Exploring the Flaws of Mathematical Reasoning in Large Language Models",
        "authors": [
            "Zhishen Sun",
            "Guang Dai",
            "Ivor Tsang",
            "Haishan Ye"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "LLMs have made significant progress in the field of mathematical reasoning, but whether they have true the mathematical understanding ability is still controversial. To explore this issue, we propose a new perturbation framework to evaluate LLMs' reasoning ability in complex environments by injecting additional semantically irrelevant perturbation sentences and gradually increasing the perturbation intensity. At the same time, we use an additional perturbation method: core questioning instruction missing, to further analyze the LLMs' problem-solving mechanism. The experimental results show that LLMs perform stably when facing perturbation sentences without numbers, but there is also a robustness boundary. As the perturbation intensity increases, the performance exhibits varying degrees of decline; when facing perturbation sentences with numbers, the performance decreases more significantly, most open source models with smaller parameters decrease by nearly or even more than 10%, and further increasing with the enhancement of perturbation intensity, with the maximum decrease reaching 51.55%. Even the most advanced commercial LLMs have seen a 3%-10% performance drop. By analyzing the reasoning process of LLMs in detail, We find that models are more sensitive to perturbations with numerical information and are more likely to give incorrect answers when disturbed by irrelevant numerical information. The higher the perturbation intensity, the more obvious these defects are. At the same time, in the absence of core questioning instruction, models can still maintain an accuracy of 20%-40%, indicating that LLMs may rely on memory templates or pattern matching to complete the task, rather than logical reasoning. In general, our work reveals the shortcomings and limitations of current LLMs in their reasoning capabilities, which is of great significance for the further development of LLMs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 36,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08024",
        "abs_url": "https://arxiv.org/abs/2511.08024",
        "pdf_url": "https://arxiv.org/pdf/2511.08024",
        "title": "Knowledge-Augmented Long-CoT Generation for Complex Biomolecular Reasoning",
        "authors": [
            "Tianwen Lyu",
            "Xiang Zhuang",
            "Keyan Ding",
            "Xinzhe Cao",
            "Lei Liang",
            "Wei Zhao",
            "Qiang Zhang",
            "Huajun Chen"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Understanding complex biomolecular mechanisms requires multi-step reasoning across molecular interactions, signaling cascades, and metabolic pathways. While large language models(LLMs) show promise in such tasks, their application to biomolecular problems is hindered by logical inconsistencies and the lack of grounding in domain knowledge. Existing approaches often exacerbate these issues: reasoning steps may deviate from biological facts or fail to capture long mechanistic dependencies. To address these challenges, we propose a Knowledge-Augmented Long-CoT Reasoning framework that integrates LLMs with knowledge graph-based multi-hop reasoning chains. The framework constructs mechanistic chains via guided multi-hop traversal and pruning on the knowledge graph; these chains are then incorporated into supervised fine-tuning to improve factual grounding and further refined with reinforcement learning to enhance reasoning reliability and consistency. Furthermore, to overcome the shortcomings of existing benchmarks, which are often restricted in scale and scope and lack annotations for deep reasoning chains, we introduce PrimeKGQA, a comprehensive benchmark for biomolecular question answering. Experimental results on both PrimeKGQA and existing datasets demonstrate that although larger closed-source models still perform well on relatively simple tasks, our method demonstrates clear advantages as reasoning depth increases, achieving state-of-the-art performance on multi-hop tasks that demand traversal of structured biological knowledge. These findings highlight the effectiveness of combining structured knowledge with advanced reasoning strategies for reliable and interpretable biomolecular reasoning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 37,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08042",
        "abs_url": "https://arxiv.org/abs/2511.08042",
        "pdf_url": "https://arxiv.org/pdf/2511.08042",
        "title": "Towards a Standard, Enterprise-Relevant Agentic AI Benchmark: Lessons from 5.5 billion tokens' worth of agentic AI evaluations",
        "authors": [
            "JV Roig"
        ],
        "comments": "34 pages",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Enterprise adoption of agentic AI systems requires reliable evaluation methods that reflect real-world deployment scenarios. Traditional LLM benchmarks suffer from training data contamination and fail to measure agentic capabilities such as multi-step tool use and decision-making under uncertainty. We present the Kamiwaza Agentic Merit Index (KAMI) v0.1, an enterprise-focused benchmark that addresses both contamination resistance and agentic evaluation. Through 170,000 LLM test items processing over 5.5 billion tokens across 35 model configurations, we demonstrate that traditional benchmark rankings poorly predict practical agentic performance. Notably, newer generation models like Llama 4 or Qwen 3 do not always outperform their older generation variants on enterprise-relevant tasks, contradicting traditional benchmark trends. We also present insights on cost-performance tradeoffs, model-specific behavioral patterns, and the impact of reasoning capabilities on token efficiency -- findings critical for enterprises making deployment decisions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 38,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08052",
        "abs_url": "https://arxiv.org/abs/2511.08052",
        "pdf_url": "https://arxiv.org/pdf/2511.08052",
        "title": "Dual-Process Scaffold Reasoning for Enhancing LLM Code Debugging",
        "authors": [
            "Po-Chung Hsieh",
            "Chin-Po Chen",
            "Jeng-Lin Li",
            "Ming-Ching Chang"
        ],
        "comments": "5 pages, 2 figures",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Software Engineering (cs.SE)",
        "abstract": "Recent LLMs have demonstrated sophisticated problem-solving capabilities on various benchmarks through advanced reasoning algorithms. However, the key research question of identifying reasoning steps that balance complexity and computational efficiency remains unsolved. Recent research has increasingly drawn upon psychological theories to explore strategies for optimizing cognitive pathways. The LLM's final outputs and intermediate steps are regarded as System 1 and System 2, respectively. However, an in-depth exploration of the System 2 reasoning is still lacking. Therefore, we propose a novel psychologically backed Scaffold Reasoning framework for code debugging, which encompasses the Scaffold Stream, Analytic Stream, and Integration Stream. The construction of reference code within the Scaffold Stream is integrated with the buggy code analysis results produced by the Analytic Stream through the Integration Stream. Our framework achieves an 88.91% pass rate and an average inference time of 5.36 seconds per-problem on DebugBench, outperforming other reasoning approaches across various LLMs in both reasoning accuracy and efficiency. Further analyses elucidate the advantages and limitations of various cognitive pathways across varying problem difficulties and bug types. Our findings also corroborate the alignment of the proposed Scaffold Reasoning framework with human cognitive processes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 39,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08055",
        "abs_url": "https://arxiv.org/abs/2511.08055",
        "pdf_url": "https://arxiv.org/pdf/2511.08055",
        "title": "MSCR: Exploring the Vulnerability of LLMs' Mathematical Reasoning Abilities Using Multi-Source Candidate Replacement",
        "authors": [
            "Zhishen Sun",
            "Guang Dai",
            "Haishan Ye"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "LLMs demonstrate performance comparable to human abilities in complex tasks such as mathematical reasoning, but their robustness in mathematical reasoning under minor input perturbations still lacks systematic investigation. Existing methods generally suffer from limited scalability, weak semantic preservation, and high costs. Therefore, we propose MSCR, an automated adversarial attack method based on multi-source candidate replacement. By combining three information sources including cosine similarity in the embedding space of LLMs, the WordNet dictionary, and contextual predictions from a masked language model, we generate for each word in the input question a set of semantically similar candidates, which are then filtered and substituted one by one to carry out the attack. We conduct large-scale experiments on LLMs using the GSM8K and MATH500 benchmarks. The results show that even a slight perturbation involving only a single word can significantly reduce the accuracy of all models, with the maximum drop reaching 49.89% on GSM8K and 35.40% on MATH500, while preserving the high semantic consistency of the perturbed questions. Further analysis reveals that perturbations not only lead to incorrect outputs but also substantially increase the average response length, which results in more redundant reasoning paths and higher computational resource consumption. These findings highlight the robustness deficiencies and efficiency bottlenecks of current LLMs in mathematical reasoning tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 40,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08066",
        "abs_url": "https://arxiv.org/abs/2511.08066",
        "pdf_url": "https://arxiv.org/pdf/2511.08066",
        "title": "Information Capacity: Evaluating the Efficiency of Large Language Models via Text Compression",
        "authors": [
            "Cheng Yuan",
            "Jiawei Shao",
            "Chi Zhang",
            "Xuelong Li"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Signal Processing (eess.SP)",
        "abstract": "Recent years have witnessed the rapid advancements of large language models (LLMs) and their expanding applications, leading to soaring demands for computational resources. The widespread adoption of test-time scaling further aggravates the tension between model capability and resource consumption, highlighting the importance of inference efficiency. However, a unified metric that accurately reflects an LLM's efficiency across different model sizes and architectures remains absent. Motivated by the correlation between compression and intelligence, we introduce information capacity, a measure of model efficiency based on text compression performance relative to computational complexity. Larger models can predict the next token more accurately, achieving greater compression gains but at higher computational costs. Empirical evaluations on mainstream open-source models show that models of varying sizes within a series exhibit consistent information capacity. This metric enables a fair efficiency comparison across model series and accurate performance prediction within a model series. A distinctive feature of information capacity is that it incorporates tokenizer efficiency, which affects both input and output token counts but is often neglected in LLM evaluations. We assess the information capacity of 49 models on 5 heterogeneous datasets and observe consistent results on the influences of tokenizer efficiency, pretraining data, and the mixture-of-experts architecture.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 41,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08072",
        "abs_url": "https://arxiv.org/abs/2511.08072",
        "pdf_url": "https://arxiv.org/pdf/2511.08072",
        "title": "Clustering-based Anomaly Detection in Multivariate Time Series Data",
        "authors": [
            "Jinbo Li",
            "Hesam Izakian",
            "Witold Pedrycz",
            "Iqbal Jamal"
        ],
        "comments": "33 pages, 20 figures",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Multivariate time series data come as a collection of time series describing different aspects of a certain temporal phenomenon. Anomaly detection in this type of data constitutes a challenging problem yet with numerous applications in science and engineering because anomaly scores come from the simultaneous consideration of the temporal and variable relationships. In this paper, we propose a clustering-based approach to detect anomalies concerning the amplitude and the shape of multivariate time series. First, we use a sliding window to generate a set of multivariate subsequences and thereafter apply an extended fuzzy clustering to reveal a structure present within the generated multivariate subsequences. Finally, a reconstruction criterion is employed to reconstruct the multivariate subsequences with the optimal cluster centers and the partition matrix. We construct a confidence index to quantify a level of anomaly detected in the series and apply Particle Swarm Optimization as an optimization vehicle for the problem of anomaly detection. Experimental studies completed on several synthetic and six real-world datasets suggest that the proposed methods can detect the anomalies in multivariate time series. With the help of available clusters revealed by the extended fuzzy clustering, the proposed framework can detect anomalies in the multivariate time series and is suitable for identifying anomalous amplitude and shape patterns in various application domains such as health care, weather data analysis, finance, and disease outbreak detection.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 42,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08082",
        "abs_url": "https://arxiv.org/abs/2511.08082",
        "pdf_url": "https://arxiv.org/pdf/2511.08082",
        "title": "Prudential Reliability of Large Language Models in Reinsurance: Governance, Assurance, and Capital Efficiency",
        "authors": [
            "Stella C. Dong"
        ],
        "comments": "48 pages, 9 figures, 5 tables. Submitted to the Journal of Risk and Insurance (JRI), November 2025",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG); General Economics (econ.GN)",
        "abstract": "This paper develops a prudential framework for assessing the reliability of large language models (LLMs) in reinsurance. A five-pillar architecture--governance, data lineage, assurance, resilience, and regulatory alignment--translates supervisory expectations from Solvency II, SR 11-7, and guidance from EIOPA (2025), NAIC (2023), and IAIS (2024) into measurable lifecycle controls. The framework is implemented through the Reinsurance AI Reliability and Assurance Benchmark (RAIRAB), which evaluates whether governance-embedded LLMs meet prudential standards for grounding, transparency, and accountability. Across six task families, retrieval-grounded configurations achieved higher grounding accuracy (0.90), reduced hallucination and interpretive drift by roughly 40%, and nearly doubled transparency. These mechanisms lower informational frictions in risk transfer and capital allocation, showing that existing prudential doctrines already accommodate reliable AI when governance is explicit, data are traceable, and assurance is verifiable.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 43,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08091",
        "abs_url": "https://arxiv.org/abs/2511.08091",
        "pdf_url": "https://arxiv.org/pdf/2511.08091",
        "title": "Gateways to Tractability for Satisfiability in Pearl's Causal Hierarchy",
        "authors": [
            "Robert Ganian",
            "Marlene Grndel",
            "Simon Wietheger"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computational Complexity (cs.CC)",
        "abstract": "Pearl's Causal Hierarchy (PCH) is a central framework for reasoning about probabilistic, interventional, and counterfactual statements, yet the satisfiability problem for PCH formulas is computationally intractable in almost all classical settings. We revisit this challenge through the lens of parameterized complexity and identify the first gateways to tractability. Our results include fixed-parameter and XP-algorithms for satisfiability in key probabilistic and counterfactual fragments, using parameters such as primal treewidth and the number of variables, together with matching hardness results that map the limits of tractability. Technically, we depart from the dynamic programming paradigm typically employed for treewidth-based algorithms and instead exploit structural characterizations of well-formed causal models, providing a new algorithmic toolkit for causal reasoning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 44,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08108",
        "abs_url": "https://arxiv.org/abs/2511.08108",
        "pdf_url": "https://arxiv.org/pdf/2511.08108",
        "title": "Improving Industrial Injection Molding Processes with Explainable AI for Quality Classification",
        "authors": [
            "Georg Rottenwalter",
            "Marcel Tilly",
            "Victor Owolabi"
        ],
        "comments": "Accepted and published at the 2025 IEEE 9th Forum on Research and Technologies for Society and Industry (RTSI). 10 pages, 6 figures. DOI: https://doi.org/10.1109/RTSI64020.2025.11212395. This is the author-accepted manuscript (AAM) version",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Machine learning is an essential tool for optimizing industrial quality control processes. However, the complexity of machine learning models often limits their practical applicability due to a lack of interpretability. Additionally, many industrial machines lack comprehensive sensor technology, making data acquisition incomplete and challenging. Explainable Artificial Intelligence offers a solution by providing insights into model decision-making and identifying the most relevant features for classification. In this paper, we investigate the impact of feature reduction using XAI techniques on the quality classification of injection-molded parts. We apply SHAP, Grad-CAM, and LIME to analyze feature importance in a Long Short-Term Memory model trained on real production data. By reducing the original 19 input features to 9 and 6, we evaluate the trade-off between model accuracy, inference speed, and interpretability. Our results show that reducing features can improve generalization while maintaining high classification performance, with an small increase in inference speed. This approach enhances the feasibility of AI-driven quality control, particularly for industrial settings with limited sensor capabilities, and paves the way for more efficient and interpretable machine learning applications in manufacturing.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 45,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08117",
        "abs_url": "https://arxiv.org/abs/2511.08117",
        "pdf_url": "https://arxiv.org/pdf/2511.08117",
        "title": "Advancements in synthetic data extraction for industrial injection molding",
        "authors": [
            "Georg Rottenwalter",
            "Marcel Tilly",
            "Christian Bielenberg",
            "Katharina Obermeier"
        ],
        "comments": "Published in: Progress in Artificial Intelligence, EPIA 2023, Lecture Notes in Computer Science, vol. 14258. 13 pages, 3 figures, 3 tables. This is the author-accepted manuscript (AAM) version. DOI: https://doi.org/10.1007/978-3-031-49011-8_43",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Machine learning has significant potential for optimizing various industrial processes. However, data acquisition remains a major challenge as it is both time-consuming and costly. Synthetic data offers a promising solution to augment insufficient data sets and improve the robustness of machine learning models. In this paper, we investigate the feasibility of incorporating synthetic data into the training process of the injection molding process using an existing Long Short-Term Memory architecture. Our approach is to generate synthetic data by simulating production cycles and incorporating them into the training data set. Through iterative experimentation with different proportions of synthetic data, we attempt to find an optimal balance that maximizes the benefits of synthetic data while preserving the authenticity and relevance of real data. Our results suggest that the inclusion of synthetic data improves the model's ability to handle different scenarios, with potential practical industrial applications to reduce manual labor, machine use, and material waste. This approach provides a valuable alternative for situations where extensive data collection and maintenance has been impractical or costly and thus could contribute to more efficient manufacturing processes in the future.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 46,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08132",
        "abs_url": "https://arxiv.org/abs/2511.08132",
        "pdf_url": "https://arxiv.org/pdf/2511.08132",
        "title": "National Institute on Aging PREPARE Challenge: Early Detection of Cognitive Impairment Using Speech - The SpeechCARE Solution",
        "authors": [
            "Maryam Zolnoori",
            "Hossein Azadmaleki",
            "Yasaman Haghbin",
            "Ali Zolnour",
            "Mohammad Javad Momeni Nezhad",
            "Sina Rashidi",
            "Mehdi Naserian",
            "Elyas Esmaeili",
            "Sepehr Karimi Arpanahi"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Alzheimer's disease and related dementias (ADRD) affect one in five adults over 60, yet more than half of individuals with cognitive decline remain undiagnosed. Speech-based assessments show promise for early detection, as phonetic motor planning deficits alter acoustic features (e.g., pitch, tone), while memory and language impairments lead to syntactic and semantic errors. However, conventional speech-processing pipelines with hand-crafted features or general-purpose audio classifiers often exhibit limited performance and generalizability. To address these limitations, we introduce SpeechCARE, a multimodal speech processing pipeline that leverages pretrained, multilingual acoustic and linguistic transformer models to capture subtle speech-related cues associated with cognitive impairment. Inspired by the Mixture of Experts (MoE) paradigm, SpeechCARE employs a dynamic fusion architecture that weights transformer-based acoustic, linguistic, and demographic inputs, allowing integration of additional modalities (e.g., social factors, imaging) and enhancing robustness across diverse tasks. Its robust preprocessing includes automatic transcription, large language model (LLM)-based anomaly detection, and task identification. A SHAP-based explainability module and LLM reasoning highlight each modality's contribution to decision-making. SpeechCARE achieved AUC = 0.88 and F1 = 0.72 for classifying cognitively healthy, MCI, and AD individuals, with AUC = 0.90 and F1 = 0.62 for MCI detection. Bias analysis showed minimal disparities, except for adults over 80. Mitigation techniques included oversampling and weighted loss. Future work includes deployment in real-world care settings (e.g., VNS Health, Columbia ADRC) and EHR-integrated explainability for underrepresented populations in New York City.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 47,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08151",
        "abs_url": "https://arxiv.org/abs/2511.08151",
        "pdf_url": "https://arxiv.org/pdf/2511.08151",
        "title": "SciAgent: A Unified Multi-Agent System for Generalistic Scientific Reasoning",
        "authors": [
            "Xuchen Li",
            "Ruitao Wu",
            "Xuanbo Liu",
            "Xukai Wang",
            "Jinbo Hu",
            "Zhixin Bai",
            "Bohan Zeng",
            "Hao Liang",
            "Leheng Chen",
            "Mingrui Chen",
            "Haitian Zhong",
            "Xuanlin Yang",
            "Xu-Yao Zhang",
            "Liu Liu",
            "Jia Li",
            "Kaiqi Huang",
            "Jiahao Xu",
            "Haitao Mi",
            "Wentao Zhang",
            "Bin Dong"
        ],
        "comments": "Technique Report",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Multiagent Systems (cs.MA)",
        "abstract": "Recent advances in large language models have enabled AI systems to achieve expert-level performance on domain-specific scientific tasks, yet these systems remain narrow and handcrafted. We introduce SciAgent, a unified multi-agent system designed for generalistic scientific reasoning-the ability to adapt reasoning strategies across disciplines and difficulty levels. SciAgent organizes problem solving as a hierarchical process: a Coordinator Agent interprets each problem's domain and complexity, dynamically orchestrating specialized Worker Systems, each composed of interacting reasoning Sub-agents for symbolic deduction, conceptual modeling, numerical computation, and verification. These agents collaboratively assemble and refine reasoning pipelines tailored to each task. Across mathematics and physics Olympiads (IMO, IMC, IPhO, CPhO), SciAgent consistently attains or surpasses human gold-medalist performance, demonstrating both domain generality and reasoning adaptability. Additionally, SciAgent has been tested on the International Chemistry Olympiad (IChO) and selected problems from the Humanity's Last Exam (HLE) benchmark, further confirming the system's ability to generalize across diverse scientific domains. This work establishes SciAgent as a concrete step toward generalistic scientific intelligence-AI systems capable of coherent, cross-disciplinary reasoning at expert levels.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 48,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08168",
        "abs_url": "https://arxiv.org/abs/2511.08168",
        "pdf_url": "https://arxiv.org/pdf/2511.08168",
        "title": "oboro: Text-to-Image Synthesis on Limited Data using Flow-based Diffusion Transformer with MMH Attention",
        "authors": [
            "Ryusuke Mizutani",
            "Kazuaki Matano",
            "Tsugumi Kadowaki",
            "Haruki Tenya",
            "Layris",
            "nuigurumi",
            "Koki Hashimoto",
            "Yu Tanaka"
        ],
        "comments": "11 pages, 10 figures",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "This project was conducted as a 2nd-term adopted project of the \"Post-5G Information and Communication System Infrastructure Enhancement R&D Project Development of Competitive Generative AI Foundation Models (GENIAC),\" a business of the Ministry of Economy, Trade and Industry (METI) and the New Energy and Industrial Technology Development Organization (NEDO). To address challenges such as labor shortages in Japan's anime production industry, this project aims to develop an image generation model from scratch. This report details the technical specifications of the developed image generation model, \"oboro:.\" We have developed \"oboro:,\" a new image generation model built from scratch, using only copyright-cleared images for training. A key characteristic is its architecture, designed to generate high-quality images even from limited datasets. The foundation model weights and inference code are publicly available alongside this report. This project marks the first release of an open-source, commercially-oriented image generation AI fully developed in Japan. AiHUB originated from the OSS community; by maintaining transparency in our development process, we aim to contribute to Japan's AI researcher and engineer community and promote the domestic AI development ecosystem.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 49,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08172",
        "abs_url": "https://arxiv.org/abs/2511.08172",
        "pdf_url": "https://arxiv.org/pdf/2511.08172",
        "title": "An Efficient Training Pipeline for Reasoning Graphical User Interface Agents",
        "authors": [
            "Georgios Pantazopoulos",
            "Eda B. zyiit"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Visual grounding is the task of localising image regions from natural language queries and is critical for reasoning capable Graphical User Interface agents. Many existing methods rely on massive, noisy synthetic this http URL work introduces an efficient training pipeline that combines model-based data filtering with parameter-efficient fine-tuning. From 4.8M synthetic examples, 12K clean and diverse instances are curated by first identifying challenging cases, removing misaligned and then selecting a diverse set of multimodal instances. On this data, a 3B-parameter Vision-Language Model is trained under three regimes: supervised fine-tuning, chain-of-thought- augmented fine-tuning, and reinforcement learning via Group Relative Policy Optimization. Models trained with the filtered data and lightweight training strategies match or surpass larger baselines on benchmarks such as ScreenSpot, Multimodal-Mind2Web, and AndroidControl. These results demonstrate that principled data curation and robust adaptation can rival large-scale training, enabling compact yet capable multimodal reasoning agents.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 50,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08191",
        "abs_url": "https://arxiv.org/abs/2511.08191",
        "pdf_url": "https://arxiv.org/pdf/2511.08191",
        "title": "Towards Provably Unlearnable Examples via Bayes Error Optimisation",
        "authors": [
            "Ruihan Zhang",
            "Jun Sun",
            "Ee-Peng Lim",
            "Peixin Zhang"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "The recent success of machine learning models, especially large-scale classifiers and language models, relies heavily on training with massive data. These data are often collected from online sources. This raises serious concerns about the protection of user data, as individuals may not have given consent for their data to be used in training. To address this concern, recent studies introduce the concept of unlearnable examples, i.e., data instances that appear natural but are intentionally altered to prevent models from effectively learning from them. While existing methods demonstrate empirical effectiveness, they typically rely on heuristic trials and lack formal guarantees. Besides, when unlearnable examples are mixed with clean data, as is often the case in practice, their unlearnability disappears. In this work, we propose a novel approach to constructing unlearnable examples by systematically maximising the Bayes error, a measurement of irreducible classification error. We develop an optimisation-based approach and provide an efficient solution using projected gradient ascent. Our method provably increases the Bayes error and remains effective when the unlearning examples are mixed with clean samples. Experimental results across multiple datasets and model architectures are consistent with our theoretical analysis and show that our approach can restrict data learnability, effectively in practice.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 51,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08206",
        "abs_url": "https://arxiv.org/abs/2511.08206",
        "pdf_url": "https://arxiv.org/pdf/2511.08206",
        "title": "EHRStruct: A Comprehensive Benchmark Framework for Evaluating Large Language Models on Structured Electronic Health Record Tasks",
        "authors": [
            "Xiao Yang",
            "Xuejiao Zhao",
            "Zhiqi Shen"
        ],
        "comments": "28pages, 6 figures, 6 tables",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Structured Electronic Health Record (EHR) data stores patient information in relational tables and plays a central role in clinical decision-making. Recent advances have explored the use of large language models (LLMs) to process such data, showing promise across various clinical this http URL, the absence of standardized evaluation frameworks and clearly defined tasks makes it difficult to systematically assess and compare LLM performance on structured EHR this http URL address these evaluation challenges, we introduce EHRStruct, a benchmark specifically designed to evaluate LLMs on structured EHR this http URL defines 11 representative tasks spanning diverse clinical needs and includes 2,200 task-specific evaluation samples derived from two widely used EHR this http URL use EHRStruct to evaluate 20 advanced and representative LLMs, covering both general and medical this http URL further analyze key factors influencing model performance, including input formats, few-shot generalisation, and finetuning strategies, and compare results with 11 state-of-the-art LLM-based enhancement methods for structured data reasoning. Our results indicate that many structured EHR tasks place high demands on the understanding and reasoning capabilities of this http URL response, we propose EHRMaster, a code-augmented method that achieves state-of-the-art performance and offers practical",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 52,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08217",
        "abs_url": "https://arxiv.org/abs/2511.08217",
        "pdf_url": "https://arxiv.org/pdf/2511.08217",
        "title": "MADD: Multi-Agent Drug Discovery Orchestra",
        "authors": [
            "Gleb V. Solovev",
            "Alina B. Zhidkovskaya",
            "Anastasia Orlova",
            "Nina Gubina",
            "Anastasia Vepreva",
            "Rodion Golovinskii",
            "Ilya Tonkii",
            "Ivan Dubrovsky",
            "Ivan Gurev",
            "Dmitry Gilemkhanov",
            "Denis Chistiakov",
            "Timur A. Aliev",
            "Ivan Poddiakov",
            "Galina Zubkova",
            "Ekaterina V. Skorb",
            "Vladimir Vinogradov",
            "Alexander Boukhanovsky",
            "Nikolay Nikitin",
            "Andrei Dmitrenko",
            "Anna Kalyuzhnaya",
            "Andrey Savchenko"
        ],
        "comments": "EMNLP2025 accepted paper",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Hit identification is a central challenge in early drug discovery, traditionally requiring substantial experimental resources. Recent advances in artificial intelligence, particularly large language models (LLMs), have enabled virtual screening methods that reduce costs and improve efficiency. However, the growing complexity of these tools has limited their accessibility to wet-lab researchers. Multi-agent systems offer a promising solution by combining the interpretability of LLMs with the precision of specialized models and tools. In this work, we present MADD, a multi-agent system that builds and executes customized hit identification pipelines from natural language queries. MADD employs four coordinated agents to handle key subtasks in de novo compound generation and screening. We evaluate MADD across seven drug discovery cases and demonstrate its superior performance compared to existing LLM-based solutions. Using MADD, we pioneer the application of AI-first drug design to five biological targets and release the identified hit molecules. Finally, we introduce a new benchmark of query-molecule pairs and docking scores for over three million compounds to contribute to the agentic future of drug design.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 53,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08234",
        "abs_url": "https://arxiv.org/abs/2511.08234",
        "pdf_url": "https://arxiv.org/pdf/2511.08234",
        "title": "Beyond Distributions: Geometric Action Control for Continuous Reinforcement Learning",
        "authors": [
            "Zhihao Lin"
        ],
        "comments": "18 pages, 5 figures",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Gaussian policies have dominated continuous control in deep reinforcement learning (RL), yet they suffer from a fundamental mismatch: their unbounded support requires ad-hoc squashing functions that distort the geometry of bounded action spaces. While von Mises-Fisher (vMF) distributions offer a theoretically grounded alternative on the sphere, their reliance on Bessel functions and rejection sampling hinders practical adoption. We propose \\textbf{Geometric Action Control (GAC)}, a novel action generation paradigm that preserves the geometric benefits of spherical distributions while \\textit{simplifying computation}. GAC decomposes action generation into a direction vector and a learnable concentration parameter, enabling efficient interpolation between deterministic actions and uniform spherical noise. This design reduces parameter count from \\(2d\\) to \\(d+1\\), and avoids the \\(O(dk)\\) complexity of vMF rejection sampling, achieving simple \\(O(d)\\) operations. Empirically, GAC consistently matches or exceeds state-of-the-art methods across six MuJoCo benchmarks, achieving 37.6\\% improvement over SAC on Ant-v4 and the best results on 4 out of 6 tasks. Our ablation studies reveal that both \\textbf{spherical normalization} and \\textbf{adaptive concentration control} are essential to GAC's success. These findings suggest that robust and efficient continuous control does not require complex distributions, but a principled respect for the geometry of action spaces. Code and pretrained models are available in supplementary materials.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 54,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08242",
        "abs_url": "https://arxiv.org/abs/2511.08242",
        "pdf_url": "https://arxiv.org/pdf/2511.08242",
        "title": "Towards Outcome-Oriented, Task-Agnostic Evaluation of AI Agents",
        "authors": [
            "Waseem AlShikh",
            "Muayad Sayed Ali",
            "Brian Kennedy",
            "Dmytro Mozolevskyi"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "As AI agents proliferate across industries and applications, evaluating their performance based solely on infrastructural metrics such as latency, time-to-first-token, or token throughput is proving insufficient. These metrics fail to capture the quality of an agent's decisions, its operational autonomy, or its ultimate business value. This white paper proposes a novel, comprehensive framework of eleven outcome-based, task-agnostic performance metrics for AI agents that transcend domain boundaries. These metrics are designed to enable organizations to evaluate agents based on the quality of their decisions, their degree of autonomy, their adaptability to new challenges, and the tangible business value they deliver, regardless of the underlying model architecture or specific use case. We introduce metrics such as Goal Completion Rate (GCR), Autonomy Index (AIx), Multi-Step Task Resilience (MTR), and Business Impact Efficiency (BIE). Through a large-scale simulated experiment involving four distinct agent architectures (ReAct, Chain-of-Thought, Tool-Augmented, Hybrid) across five diverse domains (Healthcare, Finance, Marketing, Legal, and Customer Service), we demonstrate the framework's efficacy. Our results reveal significant performance trade-offs between different agent designs, highlighting the Hybrid Agent as the most consistently high-performing model across the majority of our proposed metrics, achieving an average Goal Completion Rate of 88.8\\% and the highest Return on Investment (ROI). This work provides a robust, standardized methodology for the holistic evaluation of AI agents, paving the way for more effective development, deployment, and governance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 55,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08246",
        "abs_url": "https://arxiv.org/abs/2511.08246",
        "pdf_url": "https://arxiv.org/pdf/2511.08246",
        "title": "Where and What Matters: Sensitivity-Aware Task Vectors for Many-Shot Multimodal In-Context Learning",
        "authors": [
            "Ziyu Ma",
            "Chenhui Gou",
            "Yiming Hu",
            "Yong Wang",
            "Xiangxiang Chu",
            "Bohan Zhuang",
            "Jianfei Cai"
        ],
        "comments": "Accepted by AAAI 2026",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Large Multimodal Models (LMMs) have shown promising in-context learning (ICL) capabilities, but scaling to many-shot settings remains difficult due to limited context length and high inference cost. To address these challenges, task-vector-based methods have been explored by inserting compact representations of many-shot in-context demonstrations into model activations. However, existing task-vector-based methods either overlook the importance of where to insert task vectors or struggle to determine suitable values for each location. To this end, we propose a novel Sensitivity-aware Task Vector insertion framework (STV) to figure out where and what to insert. Our key insight is that activation deltas across query-context pairs exhibit consistent structural patterns, providing a reliable cue for insertion. Based on the identified sensitive-aware locations, we construct a pre-clustered activation bank for each location by clustering the activation values, and then apply reinforcement learning to choose the most suitable one to insert. We evaluate STV across a range of multimodal models (e.g., Qwen-VL, Idefics-2) and tasks (e.g., VizWiz, OK-VQA), demonstrating its effectiveness and showing consistent improvements over previous task-vector-based methods with strong generalization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 56,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08274",
        "abs_url": "https://arxiv.org/abs/2511.08274",
        "pdf_url": "https://arxiv.org/pdf/2511.08274",
        "title": "Multi-Agent GraphRAG: A Text-to-Cypher Framework for Labeled Property Graphs",
        "authors": [
            "Anton Gusarov",
            "Anastasia Volkova",
            "Valentin Khrulkov",
            "Andrey Kuznetsov",
            "Evgenii Maslov",
            "Ivan Oseledets"
        ],
        "comments": "Code to be released",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "While Retrieval-Augmented Generation (RAG) methods commonly draw information from unstructured documents, the emerging paradigm of GraphRAG aims to leverage structured data such as knowledge graphs. Most existing GraphRAG efforts focus on Resource Description Framework (RDF) knowledge graphs, relying on triple representations and SPARQL queries. However, the potential of Cypher and Labeled Property Graph (LPG) databases to serve as scalable and effective reasoning engines within GraphRAG pipelines remains underexplored in current research literature. To fill this gap, we propose Multi-Agent GraphRAG, a modular LLM agentic system for text-to-Cypher query generation serving as a natural language interface to LPG-based graph data. Our proof-of-concept system features an LLM-based workflow for automated Cypher queries generation and execution, using Memgraph as the graph database backend. Iterative content-aware correction and normalization, reinforced by an aggregated feedback loop, ensures both semantic and syntactic refinement of generated queries. We evaluate our system on the CypherBench graph dataset covering several general domains with diverse types of queries. In addition, we demonstrate performance of the proposed workflow on a property graph derived from the IFC (Industry Foundation Classes) data, representing a digital twin of a building. This highlights how such an approach can bridge AI with real-world applications at scale, enabling industrial digital automation use cases.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 57,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08283",
        "abs_url": "https://arxiv.org/abs/2511.08283",
        "pdf_url": "https://arxiv.org/pdf/2511.08283",
        "title": "DiagramIR: An Automatic Pipeline for Educational Math Diagram Evaluation",
        "authors": [
            "Vishal Kumar",
            "Shubhra Mishra",
            "Rebecca Hao",
            "Rizwaan Malik",
            "David Broman",
            "Dorottya Demszky"
        ],
        "comments": "Published at the Math-AI Workshop at NeurIPS 2025",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Large Language Models (LLMs) are increasingly being adopted as tools for learning; however, most tools remain text-only, limiting their usefulness for domains where visualizations are essential, such as mathematics. Recent work shows that LLMs are capable of generating code that compiles to educational figures, but a major bottleneck remains: scalable evaluation of these diagrams. We address this by proposing DiagramIR: an automatic and scalable evaluation pipeline for geometric figures. Our method relies on intermediate representations (IRs) of LaTeX TikZ code. We compare our pipeline to other evaluation baselines such as LLM-as-a-Judge, showing that our approach has higher agreement with human raters. This evaluation approach also enables smaller models like GPT-4.1-Mini to perform comparably to larger models such as GPT-5 at a 10x lower inference cost, which is important for deploying accessible and scalable education technologies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 58,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08301",
        "abs_url": "https://arxiv.org/abs/2511.08301",
        "pdf_url": "https://arxiv.org/pdf/2511.08301",
        "title": "Smarter Together: Creating Agentic Communities of Practice through Shared Experiential Learning",
        "authors": [
            "Valentin Tablan",
            "Scott Taylor",
            "Gabriel Hurtado",
            "Kristoffer Bernhem",
            "Anders Uhrenholt",
            "Gabriele Farei",
            "Karo Moilanen"
        ],
        "comments": "24 pages",
        "subjects": "Artificial Intelligence (cs.AI); Software Engineering (cs.SE)",
        "abstract": "The transition from human-centric to agent-centric software development practices is disrupting existing knowledge sharing environments for software developers. Traditional peer-to-peer repositories and developer communities for shared technical knowledge and best practice have witnessed dramatic drops in participation in a short period of time. At the same time, agentic functional equivalents are yet to emerge leaving AI agents, which already generate a significant proportion of all new software code produced, without access to repositories of valuable shared learning. In this paper, we introduce Spark, a novel shared agentic memory architecture which is designed to emulate the collective intelligence and know-how of human developer communities. Spark enables AI coding agents to both contribute to and draw from a persistent and continuously evolving experiential memory. Agents operating in the same general problem space use the Spark shared memory as a repository of new knowledge to achieve collective continual learning. We evaluate Spark as a coach for AI coding agents performing software development tasks. We demonstrate that recommendations made by Spark improve the quality of code generated by generic code generation models at varying sizes and capability tiers. Boosted by Spark, a small open-weights model with 30 billion parameters was able to match the code quality afforded by a much larger state-of-the-art model. Separately, we measure the intrinsic quality of recommendations generated by Spark against a wide range of criteria inspired by software development best practice, and achieve helpfulness levels of up to 98.2% in the top two (out of five) qualitative helpfulness bands.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 59,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08343",
        "abs_url": "https://arxiv.org/abs/2511.08343",
        "pdf_url": "https://arxiv.org/pdf/2511.08343",
        "title": "JobSphere: An AI-Powered Multilingual Career Copilot for Government Employment Platforms",
        "authors": [
            "Srihari R",
            "Adarsha B V",
            "Mohammed Usman Hussain",
            "Shweta Singh"
        ],
        "comments": "7 pages, 4 figures, 4 tables",
        "subjects": "Artificial Intelligence (cs.AI); Computers and Society (cs.CY)",
        "abstract": "Users of government employment websites commonly face engagement and accessibility challenges linked to navigational complexity, a dearth of language options, and a lack of personalized support. This paper introduces JobSphere, an AI-powered career assistant that is redefining the employment platform in Punjab called PGRKAM. JobSphere employs Retrieval-Augmented Generation (RAG) architecture, and it is multilingual, available in English, Hindi and Punjabi. JobSphere technique uses 4-bit quantization, allowing the platform to deploy on consumer-grade GPUs (i.e., NVIDIA RTX 3050 4GB), making the implementation 89% cheaper than that of cloud-based systems. Key innovations include voice-enabled interaction with the assistant, automated mock tests, resume parsing with skills recognition, and embed-based job recommendation that achieves a precision@10 score of 68%. An evaluation of JobSphere's implementation reveals 94% factual accuracy, a median response time of 1.8 seconds, and a System Usability Scale score of 78.5/100, a 50% improvement compared to the baseline PGRKAM platform context. In conclusion, JobSphere effectively fills significant accessibility gaps for Punjab/Hindi-speaking users in rural locations, while also affirming the users access to trusted job content provided by government agencies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 60,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08363",
        "abs_url": "https://arxiv.org/abs/2511.08363",
        "pdf_url": "https://arxiv.org/pdf/2511.08363",
        "title": "AI-Powered Data Visualization Platform: An Intelligent Web Application for Automated Dataset Analysis",
        "authors": [
            "Srihari R",
            "Pallavi M",
            "Tejaswini S",
            "Vaishnavi R C"
        ],
        "comments": "7 pages, 4 figures, 4 tables",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "An AI-powered data visualization platform that automates the entire data analysis process, from uploading a dataset to generating an interactive visualization. Advanced machine learning algorithms are employed to clean and preprocess the data, analyse its features, and automatically select appropriate visualizations. The system establishes the process of automating AI-based analysis and visualization from the context of data-driven environments, and eliminates the challenge of time-consuming manual data analysis. The combination of a Python Flask backend to access the dataset, paired with a React frontend, provides a robust platform that automatically interacts with Firebase Cloud Storage for numerous data processing and data analysis solutions and real-time sources. Key contributions include automatic and intelligent data cleaning, with imputation for missing values, and detection of outliers, via analysis of the data set. AI solutions to intelligently select features, using four different algorithms, and intelligent title generation and visualization are determined by the attributes of the dataset. These contributions were evaluated using two separate datasets to assess the platform's performance. In the process evaluation, the initial analysis was performed in real-time on datasets as large as 100000 rows, while the cloud-based demand platform scales to meet requests from multiple users and processes them simultaneously. In conclusion, the cloud-based data visualization application allowed for a significant reduction of manual inputs to the data analysis process while maintaining a high quality, impactful visual outputs, and user experiences",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 61,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08379",
        "abs_url": "https://arxiv.org/abs/2511.08379",
        "pdf_url": "https://arxiv.org/pdf/2511.08379",
        "title": "SOM Directions are Better than One: Multi-Directional Refusal Suppression in Language Models",
        "authors": [
            "Giorgio Piras",
            "Raffaele Mura",
            "Fabio Brau",
            "Luca Oneto",
            "Fabio Roli",
            "Battista Biggio"
        ],
        "comments": "Accepted at AAAI 2026",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Refusal refers to the functional behavior enabling safety-aligned language models to reject harmful or unethical prompts. Following the growing scientific interest in mechanistic interpretability, recent work encoded refusal behavior as a single direction in the model's latent space; e.g., computed as the difference between the centroids of harmful and harmless prompt representations. However, emerging evidence suggests that concepts in LLMs often appear to be encoded as a low-dimensional manifold embedded in the high-dimensional latent space. Motivated by these findings, we propose a novel method leveraging Self-Organizing Maps (SOMs) to extract multiple refusal directions. To this end, we first prove that SOMs generalize the prior work's difference-in-means technique. We then train SOMs on harmful prompt representations to identify multiple neurons. By subtracting the centroid of harmless representations from each neuron, we derive a set of multiple directions expressing the refusal concept. We validate our method on an extensive experimental setup, demonstrating that ablating multiple directions from models' internals outperforms not only the single-direction baseline but also specialized jailbreak algorithms, leading to an effective suppression of refusal. Finally, we conclude by analyzing the mechanistic implications of our approach.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 62,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08409",
        "abs_url": "https://arxiv.org/abs/2511.08409",
        "pdf_url": "https://arxiv.org/pdf/2511.08409",
        "title": "FaithAct: Faithfulness Planning and Acting in MLLMs",
        "authors": [
            "Junxian Li",
            "Xinyue Xu",
            "Sai Ma",
            "Sichao Li"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Unfaithfulness remains a persistent challenge for large language models (LLMs), which often produce plausible yet ungrounded reasoning chains that diverge from perceptual evidence or final conclusions. We distinguish between behavioral faithfulness (alignment between reasoning and output) and perceptual faithfulness (alignment between reasoning and input), and introduce FaithEval for quantifying step-level and chain-level faithfulness by evaluating whether each claimed object is visually supported by the image. Building on these insights, we propose FaithAct, a faithfulness-first planning and acting framework that enforces evidential grounding at every reasoning step. Experiments across multiple reasoning benchmarks demonstrate that FaithAct improves perceptual faithfulness by up to 26% without degrading task accuracy compared to prompt-based and tool-augmented baselines. Our analysis shows that treating faithfulness as a guiding principle not only mitigates hallucination but also leads to more stable reasoning trajectories. This work thereby establishes a unified framework for both evaluating and enforcing faithfulness in multimodal reasoning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 63,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08439",
        "abs_url": "https://arxiv.org/abs/2511.08439",
        "pdf_url": "https://arxiv.org/pdf/2511.08439",
        "title": "Dataset Safety in Autonomous Driving: Requirements, Risks, and Assurance",
        "authors": [
            "Alireza Abbaspour",
            "Tejaskumar Balgonda Patil",
            "B Ravi Kiran",
            "Russel Mohr",
            "Senthil Yogamani"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Dataset integrity is fundamental to the safety and reliability of AI systems, especially in autonomous driving. This paper presents a structured framework for developing safe datasets aligned with ISO/PAS 8800 guidelines. Using AI-based perception systems as the primary use case, it introduces the AI Data Flywheel and the dataset lifecycle, covering data collection, annotation, curation, and maintenance. The framework incorporates rigorous safety analyses to identify hazards and mitigate risks caused by dataset insufficiencies. It also defines processes for establishing dataset safety requirements and proposes verification and validation strategies to ensure compliance with safety standards. In addition to outlining best practices, the paper reviews recent research and emerging trends in dataset safety and autonomous vehicle development, providing insights into current challenges and future directions. By integrating these perspectives, the paper aims to advance robust, safety-assured AI systems for autonomous driving applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 64,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08484",
        "abs_url": "https://arxiv.org/abs/2511.08484",
        "pdf_url": "https://arxiv.org/pdf/2511.08484",
        "title": "Patching LLM Like Software: A Lightweight Method for Improving Safety Policy in Large Language Models",
        "authors": [
            "Huzaifa Arif",
            "Keerthiram Murugesan",
            "Ching-Yun Ko",
            "Pin-Yu Chen",
            "Payel Das",
            "Alex Gittens"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "We propose patching for large language models (LLMs) like software versions, a lightweight and modular approach for addressing safety vulnerabilities. While vendors release improved LLM versions, major releases are costly, infrequent, and difficult to tailor to customer needs, leaving released models with known safety gaps. Unlike full-model fine-tuning or major version updates, our method enables rapid remediation by prepending a compact, learnable prefix to an existing model. This \"patch\" introduces only 0.003% additional parameters, yet reliably steers model behavior toward that of a safer reference model. Across three critical domains (toxicity mitigation, bias reduction, and harmfulness refusal) policy patches achieve safety improvements comparable to next-generation safety-aligned models while preserving fluency. Our results demonstrate that LLMs can be \"patched\" much like software, offering vendors and practitioners a practical mechanism for distributing scalable, efficient, and composable safety updates between major model releases.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 65,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08548",
        "abs_url": "https://arxiv.org/abs/2511.08548",
        "pdf_url": "https://arxiv.org/pdf/2511.08548",
        "title": "A Matter of Interest: Understanding Interestingness of Math Problems in Humans and Language Models",
        "authors": [
            "Shubhra Mishra",
            "Yuka Machino",
            "Gabriel Poesia",
            "Albert Jiang",
            "Joy Hsu",
            "Adrian Weller",
            "Challenger Mishra",
            "David Broman",
            "Joshua B. Tenenbaum",
            "Mateja Jamnik",
            "Cedegao E. Zhang",
            "Katherine M. Collins"
        ],
        "comments": "Published at the Math-AI Workshop, NeurIPS 2025",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "The evolution of mathematics has been guided in part by interestingness. From researchers choosing which problems to tackle next, to students deciding which ones to engage with, people's choices are often guided by judgments about how interesting or challenging problems are likely to be. As AI systems, such as LLMs, increasingly participate in mathematics with people -- whether for advanced research or education -- it becomes important to understand how well their judgments align with human ones. Our work examines this alignment through two empirical studies of human and LLM assessment of mathematical interestingness and difficulty, spanning a range of mathematical experience. We study two groups: participants from a crowdsourcing platform and International Math Olympiad competitors. We show that while many LLMs appear to broadly agree with human notions of interestingness, they mostly do not capture the distribution observed in human judgments. Moreover, most LLMs only somewhat align with why humans find certain math problems interesting, showing weak correlation with human-selected interestingness rationales. Together, our findings highlight both the promises and limitations of current LLMs in capturing human interestingness judgments for mathematical AI thought partnerships.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 66,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08558",
        "abs_url": "https://arxiv.org/abs/2511.08558",
        "pdf_url": "https://arxiv.org/pdf/2511.08558",
        "title": "Hyperdimensional Decoding of Spiking Neural Networks",
        "authors": [
            "Cedrick Kinavuidi",
            "Luca Peres",
            "Oliver Rhodes"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "This work presents a novel spiking neural network (SNN) decoding method, combining SNNs with Hyperdimensional computing (HDC). The goal is to create a decoding method with high accuracy, high noise robustness, low latency and low energy usage. Compared to analogous architectures decoded with existing approaches, the presented SNN-HDC model attains generally better classification accuracy, lower classification latency and lower estimated energy consumption on multiple test cases from literature. The SNN-HDC achieved estimated energy consumption reductions ranging from 1.24x to 3.67x on the DvsGesture dataset and from 1.38x to 2.27x on the SL-Animals-DVS dataset. The presented decoding method can also efficiently identify unknown classes it has not been trained on. In the DvsGesture dataset the SNN-HDC model can identify 100% of samples from an unseen/untrained class. Given the numerous benefits shown and discussed in this paper, this decoding method represents a very compelling alternative to both rate and latency decoding.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 67,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08581",
        "abs_url": "https://arxiv.org/abs/2511.08581",
        "pdf_url": "https://arxiv.org/pdf/2511.08581",
        "title": "DeepProofLog: Efficient Proving in Deep Stochastic Logic Programs",
        "authors": [
            "Ying Jiao",
            "Rodrigo Castellano Ontiveros",
            "Luc De Raedt",
            "Marco Gori",
            "Francesco Giannini",
            "Michelangelo Diligenti",
            "Giuseppe Marra"
        ],
        "comments": "Accepted at AAAI 2026",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Neurosymbolic (NeSy) AI aims to combine the strengths of neural architectures and symbolic reasoning to improve the accuracy, interpretability, and generalization capability of AI models. While logic inference on top of subsymbolic modules has been shown to effectively guarantee these properties, this often comes at the cost of reduced scalability, which can severely limit the usability of NeSy models. This paper introduces DeepProofLog (DPrL), a novel NeSy system based on stochastic logic programs, which addresses the scalability limitations of previous methods. DPrL parameterizes all derivation steps with neural networks, allowing efficient neural guidance over the proving system. Additionally, we establish a formal mapping between the resolution process of our deep stochastic logic programs and Markov Decision Processes, enabling the application of dynamic programming and reinforcement learning techniques for efficient inference and learning. This theoretical connection improves scalability for complex proof spaces and large knowledge bases. Our experiments on standard NeSy benchmarks and knowledge graph reasoning tasks demonstrate that DPrL outperforms existing state-of-the-art NeSy systems, advancing scalability to larger and more complex settings than previously possible.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 68,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2404.19477",
        "abs_url": "https://arxiv.org/abs/2404.19477",
        "pdf_url": "https://arxiv.org/pdf/2404.19477",
        "title": "Hybrid Bit and Semantic Communications",
        "authors": [
            "Kaiwen Yu",
            "Renhe Fan",
            "Gang Wu",
            "Zhijin Qin"
        ],
        "comments": "",
        "subjects": "Signal Processing (eess.SP); Artificial Intelligence (cs.AI)",
        "abstract": "Semantic communication technology is regarded as a method surpassing the Shannon limit of bit transmission, capable of effectively enhancing transmission efficiency. However, current approaches that directly map content to transmission symbols are challenging to deploy in practice, imposing significant limitations on the development of semantic communication. To address this challenge, we propose a hybrid bit and semantic communication system, named HybridBSC, in which encoded semantic information is inserted into bit information for transmission via conventional digital communication systems utilizing same spectrum resources. The system can be easily deployed using existing communication architecture to achieve bit and semantic information transmission. Particularly, we design a semantic insertion and extraction scheme to implement this strategy. Furthermore, we conduct experimental validation based on the pluto-based software defined radio (SDR) platform in a real wireless channel, demonstrating that the proposed strategy can simultaneously transmit semantic and bit information.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 69,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2508.18316",
        "abs_url": "https://arxiv.org/abs/2508.18316",
        "pdf_url": "https://arxiv.org/pdf/2508.18316",
        "title": "Evaluating Federated Learning for At-Risk Student Prediction: A Comparative Analysis of Model Complexity and Data Balancing",
        "authors": [
            "Rodrigo Tertulino",
            "Ricardo Almeida"
        ],
        "comments": "This article has been prepared to be submitted to the Fundamenta Informaticae Journal",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Logic in Computer Science (cs.LO)",
        "abstract": "This study proposes and validates a Federated Learning (FL) framework to proactively identify at-risk students while preserving data privacy. Persistently high dropout rates in distance education remain a pressing institutional challenge. Using the large-scale OULAD dataset, we simulate a privacy-centric scenario where models are trained on early academic performance and digital engagement patterns. Our work investigates the practical trade-offs between model complexity (Logistic Regression vs. a Deep Neural Network) and the impact of local data balancing. The resulting federated model achieves strong predictive power (ROC AUC approximately 85%), demonstrating that FL is a practical and scalable solution for early-warning systems that inherently respects student data sovereignty.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 70,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2509.10516",
        "abs_url": "https://arxiv.org/abs/2509.10516",
        "pdf_url": "https://arxiv.org/pdf/2509.10516",
        "title": "Privacy-Preserving Personalization in Education: A Federated Recommender System for Student Performance Prediction",
        "authors": [
            "Rodrigo Tertulino",
            "Ricardo Almeida"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Logic in Computer Science (cs.LO)",
        "abstract": "The increasing digitalization of education presents unprecedented opportunities for data-driven personalization, but it also introduces significant challenges to student data privacy. Conventional recommender systems rely on centralized data, a paradigm often incompatible with modern data protection regulations. A novel privacy-preserving recommender system is proposed and evaluated to address this critical issue using Federated Learning (FL). The approach utilizes a Deep Neural Network (DNN) with rich, engineered features from the large-scale ASSISTments educational dataset. A rigorous comparative analysis of federated aggregation strategies was conducted, identifying FedProx as a significantly more stable and effective method for handling heterogeneous student data than the standard FedAvg baseline. The optimized federated model achieves a high-performance F1-Score of 76.28%, corresponding to 92% of the performance of a powerful, centralized XGBoost model. These findings validate that a federated approach can provide highly effective content recommendations without centralizing sensitive student data. Consequently, our work presents a viable and robust solution to the personalization-privacy dilemma in modern educational platforms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 71,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.00133",
        "abs_url": "https://arxiv.org/abs/2511.00133",
        "pdf_url": "https://arxiv.org/pdf/2511.00133",
        "title": "Feature Importance Guided Random Forest Learning with Simulated Annealing Based Hyperparameter Tuning",
        "authors": [
            "Kowshik Balasubramanian",
            "Andre Williams",
            "Ismail Butun"
        ],
        "comments": "10 pages, 2 figures, 3 tables, submitted to IEEE Intelligent Systems journal",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Emerging Technologies (cs.ET); Information Theory (cs.IT)",
        "abstract": "This paper introduces a novel framework for enhancing Random Forest classifiers by integrating probabilistic feature sampling and hyperparameter tuning via Simulated Annealing. The proposed framework exhibits substantial advancements in predictive accuracy and generalization, adeptly tackling the multifaceted challenges of robust classification across diverse domains, including credit risk evaluation, anomaly detection in IoT ecosystems, early-stage medical diagnostics, and high-dimensional biological data analysis. To overcome the limitations of conventional Random Forests, we present an approach that places stronger emphasis on capturing the most relevant signals from data while enabling adaptive hyperparameter configuration. The model is guided towards features that contribute more meaningfully to classification and optimizing this with dynamic parameter tuning. The results demonstrate consistent accuracy improvements and meaningful insights into feature relevance, showcasing the efficacy of combining importance aware sampling and metaheuristic optimization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 72,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07420",
        "abs_url": "https://arxiv.org/abs/2511.07420",
        "pdf_url": "https://arxiv.org/pdf/2511.07420",
        "title": "Advancing mathematics research with large language models",
        "authors": [
            "Lisa Carbone"
        ],
        "comments": "",
        "subjects": "History and Overview (math.HO); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Group Theory (math.GR); Logic (math.LO)",
        "abstract": "The main drawback of using generative AI for advanced mathematics via Large Language Models (LLMs) is that they are probabilistic pattern-matchers, not logical reasoning engines. However, LLMs can pick up on patterns in higher mathematics that are difficult for humans to see. By putting the design of LLMs to their advantage, mathematicians may use them as powerful interactive assistants that can carry out laborious tasks, generate and debug code, check examples, formulate conjectures and more. We discuss how LLMs can be used to advance mathematics research by careful use of prompt engineering. We also discuss the integration of LLMs with Computer Algebra Systems and formal proof assistants such as Lean.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 73,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07423",
        "abs_url": "https://arxiv.org/abs/2511.07423",
        "pdf_url": "https://arxiv.org/pdf/2511.07423",
        "title": "Synera: Synergistic LLM Serving across Device and Cloud at Scale",
        "authors": [
            "Genglin Wang",
            "Liekang Zeng",
            "Bufang Yang",
            "Kaiwei Liu",
            "Guoliang Xing",
            "Chumin Sun",
            "Li Zhou",
            "Jie Sun",
            "Zhenyu Yan"
        ],
        "comments": "",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Large Language Models (LLMs) are becoming key components in various mobile operating systems, driving smart applications like interactive chatbots and personal assistants. While bringing enhanced intelligence to mobile ends, their deployment suffers from a set of performance challenges, especially the generation quality degradation and prolonged latency. Prior works have mainly relied on solutions of cloud offloading or on-device Small Language Models (SLMs). However, the former is usually limited by the communication bottleneck, and the latter sacrifices generation quality due to resource constraints. To mitigate these limitations, this paper proposes Synera, a device-cloud synergistic LLM serving system that applies an efficient SLM-LLM synergistic mechanism. Through empirical studies on LLM's unique computing characteristics, Synera identifies a set of underexplored optimization opportunities in device-cloud synergistic LLM inference, including offloading decisions, pipeline stalls, and batching bottlenecks. To translate them into enhanced performance, Synera introduces tailored designs of communication-efficient selective offloading, stall-free parallel inference, and scalable cloud batching. Extensive evaluations with real-world testbeds show that Synera enables 1.20-5.47x better generation quality against competitive baselines with on-par latency performance. Compared with existing cloud serving, Synera achieves 8.2-16.5% lower cloud serving cost on various benchmarks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 74,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07425",
        "abs_url": "https://arxiv.org/abs/2511.07425",
        "pdf_url": "https://arxiv.org/pdf/2511.07425",
        "title": "An Evaluation of LLMs Inference on Popular Single-board Computers",
        "authors": [
            "Tung",
            "Nguyen",
            "Tuyen Nguyen"
        ],
        "comments": "9 pages, 3 figures",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Artificial Intelligence (cs.AI)",
        "abstract": "The growing demand for on-device large language model (LLM) inference is driving interest in deploying lightweight, cost-effective AI solutions on edge hardware. Single-board computers (SBCs) such as the Raspberry Pi and Orange Pi offer a promising platform for localized, privacy-preserving inference-but remain underexplored in the context of LLM workloads. In this work, we benchmark the performance of 25 quantized open-source LLMs across three SBCs-Raspberry Pi 4, Raspberry Pi 5, and Orange Pi 5 Pro-using two inference runtimes: Ollama and Llamafile. We evaluate generation throughput, memory usage, and power consumption under varying CPU configurations, using multiple prompt types to simulate realistic workloads. Our results show that SBCs can reliably support models up to 1.5B parameters, with Llamafile achieving up to 4x higher throughput and 30-40% lower power usage than Ollama. We identify architecture-specific bottlenecks, highlight runtime-level trade-offs, and provide practical deployment recommendations. This study offers the first broad evaluation of LLM inference on SBCs, bridging the gap between high-performance language models and affordable edge computing.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 75,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07426",
        "abs_url": "https://arxiv.org/abs/2511.07426",
        "pdf_url": "https://arxiv.org/pdf/2511.07426",
        "title": "Network and Systems Performance Characterization of MCP-Enabled LLM Agents",
        "authors": [
            "Zihao Ding",
            "Mufeng Zhu",
            "Yao Liu"
        ],
        "comments": "",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Networking and Internet Architecture (cs.NI); Software Engineering (cs.SE)",
        "abstract": "Model Context Protocol (MCP) has recently gained increased attention within the AI community for providing a standardized way for large language models (LLMs) to interact with external tools and services, significantly enhancing their capabilities. However, the inclusion of extensive contextual information, including system prompts, MCP tool definitions, and context histories, in MCP-enabled LLM interactions, dramatically inflates token usage. Given that LLM providers charge based on tokens, these expanded contexts can quickly escalate monetary costs and increase the computational load on LLM services. This paper presents a comprehensive measurement-based analysis of MCP-enabled interactions with LLMs, revealing trade-offs between capability, performance, and cost. We explore how different LLM models and MCP configurations impact key performance metrics such as token efficiency, monetary cost, task completion times, and task success rates, and suggest potential optimizations, including enabling parallel tool calls and implementing robust task abort mechanisms. These findings provide useful insights for developing more efficient, robust, and cost-effective MCP-enabled workflows.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 76,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07427",
        "abs_url": "https://arxiv.org/abs/2511.07427",
        "pdf_url": "https://arxiv.org/pdf/2511.07427",
        "title": "DynaKV: Enabling Accurate and Efficient Long-Sequence LLM Decoding on Smartphones",
        "authors": [
            "Tuowei Wang",
            "Minxing Huang",
            "Fengzu Li",
            "Ligeng Chen",
            "Jinrui Zhang",
            "Ju Ren"
        ],
        "comments": "",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Artificial Intelligence (cs.AI)",
        "abstract": "As the demand for human-like reasoning, multi-turn dialogues, and long-form responses grows, large language models (LLMs) are increasingly expected to support efficient and effective long-sequence decoding. However, due to limited DRAM capacity, long-seuqence LLM decoding on smartphones is constrained by the key-value cache (KVCache), whose memory footprint increases linearly with sequence length. Retrieval-based methods mitigate DRAM pressure by offloading KVCache to flash and retrieving query-relevant entries through cluster-based indexing. Unfortunately, as decoding progresses, KVCache distribution shifts render static or local cluster updates progressively misaligned, excluding essential entries or fetching redundant ones. These issues are further exacerbated by smartphone-specific limitations in bandwidth, IOPS, and memory capacity. We propose DynaKV, the first adaptive KVCache management approach that jointly addresses accuracy and efficiency for long-sequence decoding on smartphones. DynaKV integrates three key techniques: (1) Migration-Free Cluster Adaptation, which adaptively splits clusters during retrieval without incurring additional transfers; (2) Continuity-Centric Flash Management, which co-locates correlated entries and clusters and employs a dual-head layout for efficient updates; and (3) Memory-Efficient Cache Design, which virtualizes cache space across DRAM and flash and extends replacement policies to align with cluster-level access patterns. Evaluations demonstrate that DynaKV improves retrieval accuracy and reduces end-to-end latency compared to state-of-the-art solutions, achieving average gains of $1.38\\times$ in accuracy and $1.47\\times$ speedups. Furthermore, the insights of DynaKV naturally extend to other long-context workloads and multi-tier memory hierarchies, underscoring its broader applicability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 77,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07433",
        "abs_url": "https://arxiv.org/abs/2511.07433",
        "pdf_url": "https://arxiv.org/pdf/2511.07433",
        "title": "Benchmarking Simulacra AI's Quantum Accurate Synthetic Data Generation for Chemical Sciences",
        "authors": [
            "Fabio Falcioni",
            "Elena Orlova",
            "Timothy Heightman",
            "Philip Mantrov",
            "Aleksei Ustimenko"
        ],
        "comments": "",
        "subjects": "Chemical Physics (physics.chem-ph); Artificial Intelligence (cs.AI); Computational Physics (physics.comp-ph)",
        "abstract": "In this work, we benchmark \\simulacra's synthetic data generation pipeline against a state-of-the-art Microsoft pipeline on a dataset of small to large systems. By analyzing the energy quality, autocorrelation times, and effective sample size, our findings show that Simulacra's Large Wavefunction Models (LWM) pipeline, paired with state-of-the-art Variational Monte Carlo (VMC) sampling algorithms, reduces data generation costs by 15-50x, while maintaining parity in energy accuracy, and 2-3x compared to traditional CCSD methods on the scale of amino acids. This enables the creation of affordable, large-scale \\textit{ab-initio} datasets, accelerating AI-driven optimization and discovery in the pharmaceutical industry and beyond. Our improvements are based on a novel and proprietary sampling scheme called Replica Exchange with Langevin Adaptive eXploration (RELAX).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 78,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07441",
        "abs_url": "https://arxiv.org/abs/2511.07441",
        "pdf_url": "https://arxiv.org/pdf/2511.07441",
        "title": "AudAgent: Automated Auditing of Privacy Policy Compliance in AI Agents",
        "authors": [
            "Ye Zheng",
            "Yidan Hu"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
        "abstract": "AI agents can autonomously perform tasks and, often without explicit user consent, collect or disclose users' sensitive local data, which raises serious privacy concerns. Although AI agents' privacy policies may describe their intended data practices, there remains limited transparency and accountability about whether runtime behavior matches those policies. To close this gap, we introduce AudAgent, a visual framework that continuously monitors AI agents' data practices in real time and guards compliance with stated privacy policies. AudAgent consists of four components for automated privacy auditing of AI agents. (i) Policy parsing: an ensemble of LLMs translates natural-language privacy policies into a structured privacy-policy model, where cross-LLM voting guarantees confidence of the parsing results. (ii) Runtime annotation: a lightweight Presidio-based analyzer detects sensitive data and annotates how the data is used based on the context of the AI agent's operations and the privacy-policy model. (iii) Compliance auditing: ontology alignment and automata-based evaluation connect the policy model with runtime annotations, enabling on-the-fly compliance checks between the natural-language policy and observed unordered data practices of AI agents. (iv) User interface: a platform-independent implementation visualizes the real-time execution trace of AI agents along with potential privacy risks detected during auditing, providing user-friendly transparency and accountability. In addition to common formatted privacy policies, AudAgent also supports user-defined policies for fine-grained control and customization. We evaluate AudAgent on AI agents built upon mainstream programming frameworks such as AutoGen, experiments show that AudAgent effectively identifies potential privacy policy violations in real time.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 79,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07442",
        "abs_url": "https://arxiv.org/abs/2511.07442",
        "pdf_url": "https://arxiv.org/pdf/2511.07442",
        "title": "Pinching Antennas Meet AI in Next-Generation Wireless Networks",
        "authors": [
            "Fang Fang",
            "Zhiguo Ding",
            "Victor C. M. Leung",
            "Lajos Hanzo"
        ],
        "comments": "",
        "subjects": "Networking and Internet Architecture (cs.NI); Artificial Intelligence (cs.AI)",
        "abstract": "Next-generation (NG) wireless networks must embrace innate intelligence in support of demanding emerging applications, such as extended reality and autonomous systems, under ultra-reliable and low-latency requirements. Pinching antennas (PAs), a new flexible low-cost technology, can create line-of-sight links by dynamically activating small dielectric pinches along a waveguide on demand. As a compelling complement, artificial intelligence (AI) offers the intelligence needed to manage the complex control of PA activation positions and resource allocation in these dynamic environments. This article explores the \"win-win\" cooperation between AI and PAs: AI facilitates the adaptive optimization of PA activation positions along the waveguide, while PAs support edge AI tasks such as federated learning and over-the-air aggregation. We also discuss promising research directions including large language model-driven PA control frameworks, and how PA-AI integration can advance semantic communications, and integrated sensing and communication. This synergy paves the way for adaptive, resilient, and self-optimizing NG networks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 80,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07445",
        "abs_url": "https://arxiv.org/abs/2511.07445",
        "pdf_url": "https://arxiv.org/pdf/2511.07445",
        "title": "A Preliminary Study of RAG for Taiwanese Historical Archives",
        "authors": [
            "Claire Lin",
            "Bo-Han Feng",
            "Xuanjun Chen",
            "Te-Lun Yang",
            "Hung-yi Lee",
            "Jyh-Shing Roger Jang"
        ],
        "comments": "Accepted by ROCLING 2025",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a promising approach for knowledge-intensive tasks. However, few studies have examined RAG for Taiwanese Historical Archives. In this paper, we present an initial study of a RAG pipeline applied to two historical Traditional Chinese datasets, Fort Zeelandia and the Taiwan Provincial Council Gazette, along with their corresponding open-ended query sets. We systematically investigate the effects of query characteristics and metadata integration strategies on retrieval quality, answer generation, and the performance of the overall system. The results show that early-stage metadata integration enhances both retrieval and answer accuracy while also revealing persistent challenges for RAG systems, including hallucinations during generation and difficulties in handling temporal or multi-hop historical queries.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 81,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07451",
        "abs_url": "https://arxiv.org/abs/2511.07451",
        "pdf_url": "https://arxiv.org/pdf/2511.07451",
        "title": "Exploring the Psychometric Validity of AI-Generated Student Responses: A Study on Virtual Personas' Learning Motivation",
        "authors": [
            "Huanxiao Wang"
        ],
        "comments": "The paper has been accepted as proceedings of Artificial Intelligence in Measurement and Education Conference (AIME-Con) (2025)",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI)",
        "abstract": "This study explores whether large language models (LLMs) can simulate valid student responses for educational measurement. Using GPT -4o, 2000 virtual student personas were generated. Each persona completed the Academic Motivation Scale (AMS). Factor analyses(EFA and CFA) and clustering showed GPT -4o reproduced the AMS structure and distinct motivational subgroups.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 82,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07457",
        "abs_url": "https://arxiv.org/abs/2511.07457",
        "pdf_url": "https://arxiv.org/pdf/2511.07457",
        "title": "GRIP: In-Parameter Graph Reasoning through Fine-Tuning Large Language Models",
        "authors": [
            "Jiarui Feng",
            "Donghong Cai",
            "Yixin Chen",
            "Muhan Zhang"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in modeling sequential textual data and generalizing across diverse tasks. However, adapting LLMs to effectively handle structural data, such as knowledge graphs or web data, remains a challenging problem. Some approaches adopt complex strategies to convert graphs into text sequences, resulting in significant token overhead and rendering them impractical for large-scale graphs. Others introduce additional modules to encode graphs into fixed-size token representations for LLMs. However, these methods typically require large-scale post-training on graph-text corpus and complex alignment procedures, yet often yield sub-optimal results due to poor modality alignment. Inspired by in-parameter knowledge injection for test-time adaptation of LLMs, we propose GRIP, a novel framework that equips LLMs with the ability to internalize complex relational information from graphs through carefully designed fine-tuning tasks. This knowledge is efficiently stored within lightweight LoRA parameters, enabling the fine-tuned LLM to perform a wide range of graph-related tasks without requiring access to the original graph at inference time. Extensive experiments across multiple benchmarks validate the effectiveness and efficiency of our approach.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 83,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07458",
        "abs_url": "https://arxiv.org/abs/2511.07458",
        "pdf_url": "https://arxiv.org/pdf/2511.07458",
        "title": "REFLEX: Reference-Free Evaluation of Log Summarization via Large Language Model Judgment",
        "authors": [
            "Priyanka Mudgal"
        ],
        "comments": "Accepted at IEEE-ICETISI 2025",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Software Engineering (cs.SE)",
        "abstract": "Evaluating log summarization systems is challenging due to the lack of high-quality reference summaries and the limitations of existing metrics like ROUGE and BLEU, which depend on surface-level lexical overlap. We introduce REFLEX, a reference-free evaluation metric for log summarization based on large language model (LLM) judgment. REFLEX uses LLMs as zero-shot evaluators to assess summary quality along dimensions such as relevance, informativeness, and coherence, without requiring gold-standard references or human annotations. We show that REFLEX produces stable, interpretable, and fine-grained evaluations across multiple log summarization dataset, and more effectively distinguishes model outputs than traditional metrics. REFLEX provides a scalable alternative for evaluating log summaries in real-world settings where reference data is scarce or unavailable.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 84,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07459",
        "abs_url": "https://arxiv.org/abs/2511.07459",
        "pdf_url": "https://arxiv.org/pdf/2511.07459",
        "title": "Optimizing Classification of Infrequent Labels by Reducing Variability in Label Distribution",
        "authors": [
            "Ashutosh Agarwal"
        ],
        "comments": "Accepted and presented at 6th International Conference on Emerging research in electronics, computer science and technology ( ICERECT)",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "This paper presents a novel solution, LEVER, designed to address the challenges posed by underperforming infrequent categories in Extreme Classification (XC) tasks. Infrequent categories, often characterized by sparse samples, suffer from high label inconsistency, which undermines classification performance. LEVER mitigates this problem by adopting a robust Siamese-style architecture, leveraging knowledge transfer to reduce label inconsistency and enhance the performance of One-vs-All classifiers. Comprehensive testing across multiple XC datasets reveals substantial improvements in the handling of infrequent categories, setting a new benchmark for the field. Additionally, the paper introduces two newly created multi-intent datasets, offering essential resources for future XC research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 85,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07461",
        "abs_url": "https://arxiv.org/abs/2511.07461",
        "pdf_url": "https://arxiv.org/pdf/2511.07461",
        "title": "It Takes Two: A Dual Stage Approach for Terminology-Aware Translation",
        "authors": [
            "Akshat Singh Jaswal"
        ],
        "comments": "Accepted to WMT 2025. Code availavle at this https URL",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "This paper introduces DuTerm, a novel two-stage architecture for terminology-constrained machine translation. Our system combines a terminology-aware NMT model, adapted via fine-tuning on large-scale synthetic data, with a prompt-based LLM for post-editing. The LLM stage refines NMT output and enforces terminology adherence. We evaluate DuTerm on English-to German, English-to-Spanish, and English-to-Russian with the WMT 2025 Terminology Shared Task corpus. We demonstrate that flexible, context-driven terminology handling by the LLM consistently yields higher quality translations than strict constraint enforcement. Our results highlight a critical trade-off, revealing that an LLM's work best for high-quality translation as context-driven mutators rather than generators.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 86,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07463",
        "abs_url": "https://arxiv.org/abs/2511.07463",
        "pdf_url": "https://arxiv.org/pdf/2511.07463",
        "title": "Dynamic Stability of LLM-Generated Code",
        "authors": [
            "Prateek Rajput",
            "Abdoul Aziz Bonkoungou",
            "Yewei Song",
            "Abdoul Kader Kabore",
            "Iyiola E. Olatunji",
            "Jacques Klein",
            "Tegewende Bissyande"
        ],
        "comments": "10 pages, 8 figures",
        "subjects": "Programming Languages (cs.PL); Artificial Intelligence (cs.AI); Software Engineering (cs.SE)",
        "abstract": "Current evaluations of LLMs for code generation emphasize functional correctness, overlooking the fact that functionally correct solutions can differ significantly in algorithmic complexity. For instance, an $(O(n^2))$ versus $(O(n \\log n))$ sorting algorithm may yield similar output but incur vastly different performance costs in production. This discrepancy reveals a critical limitation in current evaluation methods: they fail to capture the behavioral and performance diversity among correct solutions. To address this, we introduce a principled framework for evaluating the dynamic stability of generated code. We propose two metrics derived from opcode distributions: Static Canonical Trace Divergence (SCTD), which captures algorithmic structure diversity across generated solutions, and Dynamic Canonical Trace Divergence (DCTD), which quantifies runtime behavioral variance. Their ratio, the Behavioral Expression Factor (BEF), serves as a diagnostic signal: it indicates critical runtime instability when BEF $\\ll$ 1 and functional redundancy when BEF $\\gg$ 1. Empirical results on BigOBench and CodeContests show that state-of-the-art LLMs exhibit significant algorithmic variance even among functionally correct outputs. Notably, increasing sampling temperature improves pass@1 rates but degrades stability, revealing an unrecognized trade-off: searching for correct solutions in diverse output spaces introduces a \"penalty of instability\" between correctness and behavioral consistency. Our findings call for stability-aware objectives in code generation and new benchmarks with asymptotic test cases for robust, real-world LLM evaluation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 87,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07464",
        "abs_url": "https://arxiv.org/abs/2511.07464",
        "pdf_url": "https://arxiv.org/pdf/2511.07464",
        "title": "Motif 2 12.7B technical report",
        "authors": [
            "Junghwan Lim",
            "Sungmin Lee",
            "Dongseok Kim",
            "Taehyun Kim",
            "Eunhwan Park",
            "Jeesoo Lee",
            "Jeongdoo Lee",
            "Junhyeok Lee",
            "Wai Ting Cheung",
            "Dahye Choi",
            "Jaeheui Her",
            "Jaeyeon Huh",
            "Hanbin Jung",
            "Changjin Kang",
            "Beomgyu Kim",
            "Minjae Kim",
            "Taewhan Kim",
            "Youngrok Kim",
            "Hyukjin Kweon",
            "Haesol Lee",
            "Kungyu Lee",
            "Dongpin Oh",
            "Yeongjae Park",
            "Bokki Ryu",
            "Dongjoo Weon"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "We introduce Motif-2-12.7B, a new open-weight foundation model that pushes the efficiency frontier of large language models by combining architectural innovation with system-level optimization. Designed for scalable language understanding and robust instruction generalization under constrained compute budgets, Motif-2-12.7B builds upon Motif-2.6B with the integration of Grouped Differential Attention (GDA), which improves representational efficiency by disentangling signal and noise-control attention pathways. The model is pre-trained on 5.5 trillion tokens spanning diverse linguistic, mathematical, scientific, and programming domains using a curriculum-driven data scheduler that gradually changes the data composition ratio. The training system leverages the MuonClip optimizer alongside custom high-performance kernels, including fused PolyNorm activations and the Parallel Muon algorithm, yielding significant throughput and memory efficiency gains in large-scale distributed environments. Post-training employs a three-stage supervised fine-tuning pipeline that successively enhances general instruction adherence, compositional understanding, and linguistic precision. Motif-2-12.7B demonstrates competitive performance across diverse benchmarks, showing that thoughtful architectural scaling and optimized training design can rival the capabilities of much larger models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 88,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07477",
        "abs_url": "https://arxiv.org/abs/2511.07477",
        "pdf_url": "https://arxiv.org/pdf/2511.07477",
        "title": "The Polite Liar: Epistemic Pathology in Language Models",
        "authors": [
            "Bentley DeVilling"
        ],
        "comments": "17 pages, 2 tables, Preprint - under review at AI & Society",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Large language models exhibit a peculiar epistemic pathology: they speak as if they know, even when they do not. This paper argues that such confident fabrication, what I call the polite liar, is a structural consequence of reinforcement learning from human feedback (RLHF). Building on Frankfurt's analysis of bullshit as communicative indifference to truth, I show that this pathology is not deception but structural indifference: a reward architecture that optimizes for perceived sincerity over evidential accuracy. Current alignment methods reward models for being helpful, harmless, and polite, but not for being epistemically grounded. As a result, systems learn to maximize user satisfaction rather than truth, performing conversational fluency as a virtue. I analyze this behavior through the lenses of epistemic virtue theory, speech-act philosophy, and cognitive alignment, showing that RLHF produces agents trained to mimic epistemic confidence without access to epistemic justification. The polite liar thus reveals a deeper alignment tension between linguistic cooperation and epistemic integrity. The paper concludes with an \"epistemic alignment\" principle: reward justified confidence over perceived fluency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 89,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07480",
        "abs_url": "https://arxiv.org/abs/2511.07480",
        "pdf_url": "https://arxiv.org/pdf/2511.07480",
        "title": "KG-DF: A Black-box Defense Framework against Jailbreak Attacks Based on Knowledge Graphs",
        "authors": [
            "Shuyuan Liu",
            "Jiawei Chen",
            "Xiao Yang",
            "Hang Su",
            "Zhaoxia Yin"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
        "abstract": "With the widespread application of large language models (LLMs) in various fields, the security challenges they face have become increasingly prominent, especially the issue of jailbreak. These attacks induce the model to generate erroneous or uncontrolled outputs through crafted inputs, threatening the generality and security of the model. Although existing defense methods have shown some effectiveness, they often struggle to strike a balance between model generality and security. Excessive defense may limit the normal use of the model, while insufficient defense may lead to security vulnerabilities. In response to this problem, we propose a Knowledge Graph Defense Framework (KG-DF). Specifically, because of its structured knowledge representation and semantic association capabilities, Knowledge Graph(KG) can be searched by associating input content with safe knowledge in the knowledge base, thus identifying potentially harmful intentions and providing safe reasoning paths. However, traditional KG methods encounter significant challenges in keyword extraction, particularly when confronted with diverse and evolving attack strategies. To address this issue, we introduce an extensible semantic parsing module, whose core task is to transform the input query into a set of structured and secure concept representations, thereby enhancing the relevance of the matching process. Experimental results show that our framework enhances defense performance against various jailbreak attack methods, while also improving the response quality of the LLM in general QA scenarios by incorporating domain-general knowledge.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 90,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07482",
        "abs_url": "https://arxiv.org/abs/2511.07482",
        "pdf_url": "https://arxiv.org/pdf/2511.07482",
        "title": "Alignment-Constrained Dynamic Pruning for LLMs: Identifying and Preserving Alignment-Critical Circuits",
        "authors": [
            "Dev Patel",
            "Gabrielle Gervacio",
            "Diekola Raimi",
            "Kevin Zhu",
            "Ryan Lagasse",
            "Gabriel Grand",
            "Ashwinee Panda",
            "Maheep Chaudhary"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Large Language Models require substantial computational resources for inference, posing deployment challenges. While dynamic pruning offers superior efficiency over static methods through adaptive circuit selection, it exacerbates alignment degradation by retaining only input-dependent safety-critical circuit preservation across diverse inputs. As a result, addressing these heightened alignment vulnerabilities remains critical. We introduce Alignment-Aware Probe Pruning (AAPP), a dynamic structured pruning method that adaptively preserves alignment-relevant circuits during inference, building upon Probe Pruning. Experiments on LLaMA 2-7B, Qwen2.5-14B-Instruct, and Gemma-3-12B-IT show AAPP improves refusal rates by 50\\% at matched compute, enabling efficient yet safety-preserving LLM deployment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 91,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07485",
        "abs_url": "https://arxiv.org/abs/2511.07485",
        "pdf_url": "https://arxiv.org/pdf/2511.07485",
        "title": "When Are Learning Biases Equivalent? A Unifying Framework for Fairness, Robustness, and Distribution Shift",
        "authors": [
            "Sushant Mehta"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "Machine learning systems exhibit diverse failure modes: unfairness toward protected groups, brittleness to spurious correlations, poor performance on minority sub-populations, which are typically studied in isolation by distinct research communities. We propose a unifying theoretical framework that characterizes when different bias mechanisms produce quantitatively equivalent effects on model performance. By formalizing biases as violations of conditional independence through information-theoretic measures, we prove formal equivalence conditions relating spurious correlations, subpopulation shift, class imbalance, and fairness violations. Our theory predicts that a spurious correlation of strength $\\alpha$ produces equivalent worst-group accuracy degradation as a sub-population imbalance ratio $r \\approx (1+\\alpha)/(1-\\alpha)$ under feature overlap assumptions. Empirical validation in six datasets and three architectures confirms that predicted equivalences hold within the accuracy of the worst group 3\\%, enabling the principled transfer of debiasing methods across problem domains. This work bridges the literature on fairness, robustness, and distribution shifts under a common perspective.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 92,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07493",
        "abs_url": "https://arxiv.org/abs/2511.07493",
        "pdf_url": "https://arxiv.org/pdf/2511.07493",
        "title": "Enabling Automatic Self-Talk Detection via Earables",
        "authors": [
            "Euihyeok Lee",
            "Seonghyeon Kim",
            "SangHun Im",
            "Heung-Seon Oh",
            "Seungwoo Kang"
        ],
        "comments": "",
        "subjects": "Sound (cs.SD); Artificial Intelligence (cs.AI)",
        "abstract": "Self-talk-an internal dialogue that can occur silently or be spoken aloud-plays a crucial role in emotional regulation, cognitive processing, and motivation, yet has remained largely invisible and unmeasurable in everyday life. In this paper, we present MutterMeter, a mobile system that automatically detects vocalized self-talk from audio captured by earable microphones in real-world settings. Detecting self-talk is technically challenging due to its diverse acoustic forms, semantic and grammatical incompleteness, and irregular occurrence patterns, which differ fundamentally from assumptions underlying conventional speech understanding models. To address these challenges, MutterMeter employs a hierarchical classification architecture that progressively integrates acoustic, linguistic, and contextual information through a sequential processing pipeline, adaptively balancing accuracy and computational efficiency. We build and evaluate MutterMeter using a first-of-its-kind dataset comprising 31.1 hours of audio collected from 25 participants. Experimental results demonstrate that MutterMeter achieves robust performance with a macro-averaged F1 score of 0.84, outperforming conventional approaches, including LLM-based and speech emotion recognition models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 93,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07498",
        "abs_url": "https://arxiv.org/abs/2511.07498",
        "pdf_url": "https://arxiv.org/pdf/2511.07498",
        "title": "Focusing on Language: Revealing and Exploiting Language Attention Heads in Multilingual Large Language Models",
        "authors": [
            "Xin Liu",
            "Qiyang Song",
            "Qihang Zhou",
            "Haichao Du",
            "Shaowen Xu",
            "Wenbo Jiang",
            "Weijuan Zhang",
            "Xiaoqi Jia"
        ],
        "comments": "Accepted by AAAI-2026",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Large language models (LLMs) increasingly support multilingual understanding and generation. Meanwhile, efforts to interpret their internal mechanisms have emerged, offering insights to enhance multilingual performance. While multi-head self-attention (MHA) has proven critical in many areas, its role in multilingual capabilities remains underexplored. In this work, we study the contribution of MHA in supporting multilingual processing in LLMs. We propose Language Attention Head Importance Scores (LAHIS), an effective and efficient method that identifies attention head importance for multilingual capabilities via a single forward and backward pass through the LLM. Applying LAHIS to Aya-23-8B, Llama-3.2-3B, and Mistral-7B-v0.1, we reveal the existence of both language-specific and language-general heads. Language-specific heads enable cross-lingual attention transfer to guide the model toward target language contexts and mitigate off-target language generation issue, contributing to addressing challenges in multilingual LLMs. We also introduce a lightweight adaptation that learns a soft head mask to modulate attention outputs over language heads, requiring only 20 tunable parameters to improve XQuAD accuracy. Overall, our work enhances both the interpretability and multilingual capabilities of LLMs from the perspective of MHA.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 94,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07503",
        "abs_url": "https://arxiv.org/abs/2511.07503",
        "pdf_url": "https://arxiv.org/pdf/2511.07503",
        "title": "Biologically-Informed Hybrid Membership Inference Attacks on Generative Genomic Models",
        "authors": [
            "Asia Belfiore",
            "Jonathan Passerat-Palmbach",
            "Dmitrii Usynin"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
        "abstract": "The increased availability of genetic data has transformed genomics research, but raised many privacy concerns regarding its handling due to its sensitive nature. This work explores the use of language models (LMs) for the generation of synthetic genetic mutation profiles, leveraging differential privacy (DP) for the protection of sensitive genetic data. We empirically evaluate the privacy guarantees of our DP modes by introducing a novel Biologically-Informed Hybrid Membership Inference Attack (biHMIA), which combines traditional black box MIA with contextual genomics metrics for enhanced attack power. Our experiments show that both small and large transformer GPT-like models are viable synthetic variant generators for small-scale genomics, and that our hybrid attack leads, on average, to higher adversarial success compared to traditional metric-based MIAs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 95,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07505",
        "abs_url": "https://arxiv.org/abs/2511.07505",
        "pdf_url": "https://arxiv.org/pdf/2511.07505",
        "title": "FedRW: Efficient Privacy-Preserving Data Reweighting for Enhancing Federated Learning of Language Models",
        "authors": [
            "Pukang Ye",
            "Junwei Luo",
            "Xiaolei Dong",
            "Yunbo Yang"
        ],
        "comments": "Accepted at NeurIPS 2025. Code is available at this https URL",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
        "abstract": "Data duplication within large-scale corpora often impedes large language models' (LLMs) performance and privacy. In privacy-concerned federated learning scenarios, conventional deduplication methods typically rely on trusted third parties to perform uniform deletion, risking loss of informative samples while introducing privacy vulnerabilities. To address these gaps, we propose Federated ReWeighting (FedRW), the first privacy-preserving framework, to the best of our knowledge, that performs soft deduplication via sample reweighting instead of deletion in federated LLM training, without assuming a trusted third party. At its core, FedRW proposes a secure, frequency-aware reweighting protocol through secure multi-party computation, coupled with a parallel orchestration strategy to ensure efficiency and scalability. During training, FedRW utilizes an adaptive reweighting mechanism with global sample frequencies to adjust individual loss contributions, effectively improving generalization and robustness. Empirical results demonstrate that FedRW outperforms the state-of-the-art method by achieving up to 28.78x speedup in preprocessing and approximately 11.42% improvement in perplexity, while offering enhanced security guarantees. FedRW thus establishes a new paradigm for managing duplication in federated LLM training.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 96,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07559",
        "abs_url": "https://arxiv.org/abs/2511.07559",
        "pdf_url": "https://arxiv.org/pdf/2511.07559",
        "title": "N-ReLU: Zero-Mean Stochastic Extension of ReLU",
        "authors": [
            "Md Motaleb Hossen Manik",
            "Md Zabirul Islam",
            "Ge Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Activation functions are fundamental for enabling nonlinear representations in deep neural networks. However, the standard rectified linear unit (ReLU) often suffers from inactive or \"dead\" neurons caused by its hard zero cutoff. To address this issue, we introduce N-ReLU (Noise-ReLU), a zero-mean stochastic extension of ReLU that replaces negative activations with Gaussian noise while preserving the same expected output. This expectation-aligned formulation maintains gradient flow in inactive regions and acts as an annealing-style regularizer during training. Experiments on the MNIST dataset using both multilayer perceptron (MLP) and convolutional neural network (CNN) architectures show that N-ReLU achieves accuracy comparable to or slightly exceeding that of ReLU, LeakyReLU, PReLU, GELU, and RReLU at moderate noise levels (sigma = 0.05-0.10), with stable convergence and no dead neurons observed. These results demonstrate that lightweight Gaussian noise injection offers a simple yet effective mechanism to enhance optimization robustness without modifying network structures or introducing additional parameters.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 97,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07584",
        "abs_url": "https://arxiv.org/abs/2511.07584",
        "pdf_url": "https://arxiv.org/pdf/2511.07584",
        "title": "SemanticForge: Repository-Level Code Generation through Semantic Knowledge Graphs and Constraint Satisfaction",
        "authors": [
            "Wuyang Zhang",
            "Chenkai Zhang",
            "Zhen Luo",
            "Jianming Ma",
            "Wangming Yuan",
            "Chuqiao Gu",
            "Chenwei Feng"
        ],
        "comments": "",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)",
        "abstract": "Large language models (LLMs) have transformed software development by enabling automated code generation, yet they frequently suffer from systematic errors that limit practical deployment. We identify two critical failure modes: \\textit{logical hallucination} (incorrect control/data-flow reasoning) and \\textit{schematic hallucination} (type mismatches, signature violations, and architectural inconsistencies). These errors stem from the absence of explicit, queryable representations of repository-wide semantics. This paper presents \\textbf{SemanticForge}, which introduces four fundamental algorithmic advances for semantically-aware code generation: (1) a novel automatic reconciliation algorithm for dual static-dynamic knowledge graphs, unifying compile-time and runtime program semantics; (2) a neural approach that learns to generate structured graph queries from natural language, achieving 73\\% precision versus 51\\% for traditional retrieval; (3) a novel beam search algorithm with integrated SMT solving, enabling real-time constraint verification during generation rather than post-hoc validation; and (4) an incremental maintenance algorithm that updates knowledge graphs in $O(|\\Delta R| \\cdot \\log n)$ time while maintaining semantic equivalence.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 98,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07585",
        "abs_url": "https://arxiv.org/abs/2511.07585",
        "pdf_url": "https://arxiv.org/pdf/2511.07585",
        "title": "LLM Output Drift: Cross-Provider Validation & Mitigation for Financial Workflows",
        "authors": [
            "Raffi Khatchadourian",
            "Rolando Franco"
        ],
        "comments": "11 pages, 5 figures. To appear in AI4F @ ACM ICAIF '25, November 15-18, 2025, Singapore",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (stat.ML)",
        "abstract": "Financial institutions deploy Large Language Models (LLMs) for reconciliations, regulatory reporting, and client communications, but nondeterministic outputs (output drift) undermine auditability and trust. We quantify drift across five model architectures (7B-120B parameters) on regulated financial tasks, revealing a stark inverse relationship: smaller models (Granite-3-8B, Qwen2.5-7B) achieve 100% output consistency at T=0.0, while GPT-OSS-120B exhibits only 12.5% consistency (95% CI: 3.5-36.0%) regardless of configuration (p<0.0001, Fisher's exact test). This finding challenges conventional assumptions that larger models are universally superior for production deployment. Our contributions include: (i) a finance-calibrated deterministic test harness combining greedy decoding (T=0.0), fixed seeds, and SEC 10-K structure-aware retrieval ordering; (ii) task-specific invariant checking for RAG, JSON, and SQL outputs using finance-calibrated materiality thresholds (plus or minus 5%) and SEC citation validation; (iii) a three-tier model classification system enabling risk-appropriate deployment decisions; and (iv) an audit-ready attestation system with dual-provider validation. We evaluated five models (Qwen2.5-7B via Ollama, Granite-3-8B via IBM this http URL, Llama-3.3-70B, Mistral-Medium-2505, and GPT-OSS-120B) across three regulated financial tasks. Across 480 runs (n=16 per condition), structured tasks (SQL) remain stable even at T=0.2, while RAG tasks show drift (25-75%), revealing task-dependent sensitivity. Cross-provider validation confirms deterministic behavior transfers between local and cloud deployments. We map our framework to Financial Stability Board (FSB), Bank for International Settlements (BIS), and Commodity Futures Trading Commission (CFTC) requirements, demonstrating practical pathways for compliance-ready AI deployments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 99,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07593",
        "abs_url": "https://arxiv.org/abs/2511.07593",
        "pdf_url": "https://arxiv.org/pdf/2511.07593",
        "title": "Leveraging the Power of AI and Social Interactions to Restore Trust in Public Polls",
        "authors": [
            "Amr Akmal Abouelmagd",
            "Amr Hilal"
        ],
        "comments": "",
        "subjects": "Social and Information Networks (cs.SI); Artificial Intelligence (cs.AI); Computers and Society (cs.CY)",
        "abstract": "The emergence of crowdsourced data has significantly reshaped social science, enabling extensive exploration of collective human actions, viewpoints, and societal dynamics. However, ensuring safe, fair, and reliable participation remains a persistent challenge. Traditional polling methods have seen a notable decline in engagement over recent decades, raising concerns about the credibility of collected data. Meanwhile, social and peer-to-peer networks have become increasingly widespread, but data from these platforms can suffer from credibility issues due to fraudulent or ineligible participation. In this paper, we explore how social interactions can help restore credibility in crowdsourced data collected over social networks. We present an empirical study to detect ineligible participation in a polling task through AI-based graph analysis of social interactions among imperfect participants composed of honest and dishonest actors. Our approach focuses solely on the structure of social interaction graphs, without relying on the content being shared. We simulate different levels and types of dishonest behavior among participants who attempt to propagate the task within their social networks. We conduct experiments on real-world social network datasets, using different eligibility criteria and modeling diverse participation patterns. Although structural differences in social interaction graphs introduce some performance variability, our study achieves promising results in detecting ineligibility across diverse social and behavioral profiles, with accuracy exceeding 90% in some configurations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 100,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07603",
        "abs_url": "https://arxiv.org/abs/2511.07603",
        "pdf_url": "https://arxiv.org/pdf/2511.07603",
        "title": "One Router to Route Them All: Homogeneous Expert Routing for Heterogeneous Graph Transformers",
        "authors": [
            "Georgiy Shakirov",
            "Albert Arakelov"
        ],
        "comments": "14 pages, 4 figures; 2 tables; work in progress, feedback welcome",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "A common practice in heterogeneous graph neural networks (HGNNs) is to condition parameters on node/edge types, assuming types reflect semantic roles. However, this can cause overreliance on surface-level labels and impede cross-type knowledge transfer. We explore integrating Mixture-of-Experts (MoE) into HGNNs--a direction underexplored despite MoE's success in homogeneous settings. Crucially, we question the need for type-specific experts. We propose Homogeneous Expert Routing (HER), an MoE layer for Heterogeneous Graph Transformers (HGT) that stochastically masks type embeddings during routing to encourage type-agnostic specialization. Evaluated on IMDB, ACM, and DBLP for link prediction, HER consistently outperforms standard HGT and a type-separated MoE baseline. Analysis on IMDB shows HER experts specialize by semantic patterns (e.g., movie genres) rather than node types, confirming routing is driven by latent semantics. Our work demonstrates that regularizing type dependence in expert routing yields more generalizable, efficient, and interpretable representations--a new design principle for heterogeneous graph learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 101,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07629",
        "abs_url": "https://arxiv.org/abs/2511.07629",
        "pdf_url": "https://arxiv.org/pdf/2511.07629",
        "title": "Partial Action Replacement: Tackling Distribution Shift in Offline MARL",
        "authors": [
            "Yue Jin",
            "Giovanni Montana"
        ],
        "comments": "Accepted by AAAI 2026",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)",
        "abstract": "Offline multi-agent reinforcement learning (MARL) is severely hampered by the challenge of evaluating out-of-distribution (OOD) joint actions. Our core finding is that when the behavior policy is factorized - a common scenario where agents act fully or partially independently during data collection - a strategy of partial action replacement (PAR) can significantly mitigate this challenge. PAR updates a single or part of agents' actions while the others remain fixed to the behavioral data, reducing distribution shift compared to full joint-action updates. Based on this insight, we develop Soft-Partial Conservative Q-Learning (SPaCQL), using PAR to mitigate OOD issue and dynamically weighting different PAR strategies based on the uncertainty of value estimation. We provide a rigorous theoretical foundation for this approach, proving that under factorized behavior policies, the induced distribution shift scales linearly with the number of deviating agents rather than exponentially with the joint-action space. This yields a provably tighter value error bound for this important class of offline MARL problems. Our theoretical results also indicate that SPaCQL adaptively addresses distribution shift using uncertainty-informed weights. Our empirical results demonstrate SPaCQL enables more effective policy learning, and manifest its remarkable superiority over baseline algorithms when the offline dataset exhibits the independence structure.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 102,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07637",
        "abs_url": "https://arxiv.org/abs/2511.07637",
        "pdf_url": "https://arxiv.org/pdf/2511.07637",
        "title": "Private-RAG: Answering Multiple Queries with LLMs while Keeping Your Data Private",
        "authors": [
            "Ruihan Wu",
            "Erchi Wang",
            "Zhiyuan Zhang",
            "Yu-Xiang Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)",
        "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by retrieving documents from an external corpus at inference time. When this corpus contains sensitive information, however, unprotected RAG systems are at risk of leaking private information. Prior work has introduced differential privacy (DP) guarantees for RAG, but only in single-query settings, which fall short of realistic usage. In this paper, we study the more practical multi-query setting and propose two DP-RAG algorithms. The first, MURAG, leverages an individual privacy filter so that the accumulated privacy loss only depends on how frequently each document is retrieved rather than the total number of queries. The second, MURAG-ADA, further improves utility by privately releasing query-specific thresholds, enabling more precise selection of relevant documents. Our experiments across multiple LLMs and datasets demonstrate that the proposed methods scale to hundreds of queries within a practical DP budget ($\\varepsilon\\approx10$), while preserving meaningful utility.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 103,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07645",
        "abs_url": "https://arxiv.org/abs/2511.07645",
        "pdf_url": "https://arxiv.org/pdf/2511.07645",
        "title": "A Self-Improving Architecture for Dynamic Safety in Large Language Models",
        "authors": [
            "Tyler Slater"
        ],
        "comments": "Under review at the journal Information and Software Technology (Special Issue on Software Architecture for AI-Driven Systems)",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)",
        "abstract": "Context: The integration of Large Language Models (LLMs) into core software systems is accelerating. However, existing software architecture patterns are static, while current safety assurance methods are not scalable, leaving systems vulnerable to novel adversarial threats. Objective: To design, implement, and evaluate a novel software architecture that enables an AI-driven system to autonomously and continuously adapt its own safety protocols at runtime. Method: We propose the Self-Improving Safety Framework (SISF), a runtime architecture that couples an unprotected, unaligned base LLM (mistralai/Mistral-7B-v0.1) with a dynamic feedback loop. This loop consists of an AI Adjudicator (GPT-4o) for breach detection and a Policy Synthesis Module (GPT-4 Turbo) that autonomously generates new, generalized safety policies (both heuristic and semantic) in response to failures. Results: We conducted a dynamic learning evaluation using the 520-prompt AdvBench dataset. The unprotected model was 100% vulnerable. Our SISF, starting from zero policies, demonstrated a clear learning curve: it detected 237 breaches, autonomously synthesized 234 new policies, and reduced the overall Attack Success Rate (ASR) to 45.58%. In a subsequent test on 520 benign prompts, the SISF achieved a 0.00% False Positive Rate (FPR), proving its ability to adapt without compromising user utility. Conclusion: An architectural approach to AI safety, based on the principles of self-adaptation, is a viable and effective strategy. Our framework demonstrates a practical path towards building more robust, resilient, and scalable AI-driven systems, shifting safety assurance from a static, pre-deployment activity to an automated, runtime process.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 104,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07649",
        "abs_url": "https://arxiv.org/abs/2511.07649",
        "pdf_url": "https://arxiv.org/pdf/2511.07649",
        "title": "Adaptive Graph Learning with Transformer for Multi-Reservoir Inflow Prediction",
        "authors": [
            "Pengfei Hu",
            "Ming Fan",
            "Xiaoxue Han",
            "Chang Lu",
            "Wei Zhang",
            "Hyun Kang",
            "Yue Ning",
            "Dan Lu"
        ],
        "comments": "ICDM 2025 DMESS Workshop",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Reservoir inflow prediction is crucial for water resource management, yet existing approaches mainly focus on single-reservoir models that ignore spatial dependencies among interconnected reservoirs. We introduce AdaTrip as an adaptive, time-varying graph learning framework for multi-reservoir inflow forecasting. AdaTrip constructs dynamic graphs where reservoirs are nodes with directed edges reflecting hydrological connections, employing attention mechanisms to automatically identify crucial spatial and temporal dependencies. Evaluation on thirty reservoirs in the Upper Colorado River Basin demonstrates superiority over existing baselines, with improved performance for reservoirs with limited records through parameter sharing. Additionally, AdaTrip provides interpretable attention maps at edge and time-step levels, offering insights into hydrological controls to support operational decision-making. Our code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 105,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07659",
        "abs_url": "https://arxiv.org/abs/2511.07659",
        "pdf_url": "https://arxiv.org/pdf/2511.07659",
        "title": "Revisiting NLI: Towards Cost-Effective and Human-Aligned Metrics for Evaluating LLMs in Question Answering",
        "authors": [
            "Sai Shridhar Balamurali",
            "Lu Cheng"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Evaluating answers from state-of-the-art large language models (LLMs) is challenging: lexical metrics miss semantic nuances, whereas \"LLM-as-Judge\" scoring is computationally expensive. We re-evaluate a lightweight alternative -- off-the-shelf Natural Language Inference (NLI) scoring augmented by a simple lexical-match flag and find that this decades-old technique matches GPT-4o's accuracy (89.9%) on long-form QA, while requiring orders-of-magnitude fewer parameters. To test human alignment of these metrics rigorously, we introduce DIVER-QA, a new 3000-sample human-annotated benchmark spanning five QA datasets and five candidate LLMs. Our results highlight that inexpensive NLI-based evaluation remains competitive and offer DIVER-QA as an open resource for future metric research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 106,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07663",
        "abs_url": "https://arxiv.org/abs/2511.07663",
        "pdf_url": "https://arxiv.org/pdf/2511.07663",
        "title": "Cortex AISQL: A Production SQL Engine for Unstructured Data",
        "authors": [
            "Paritosh Aggarwal",
            "Bowei Chen",
            "Anupam Datta",
            "Benjamin Han",
            "Boxin Jiang",
            "Nitish Jindal",
            "Zihan Li",
            "Aaron Lin",
            "Pawel Liskowski",
            "Jay Tayade",
            "Dimitris Tsirogiannis",
            "Nathan Wiegand",
            "Weicheng Zhao"
        ],
        "comments": "",
        "subjects": "Databases (cs.DB); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Snowflake's Cortex AISQL is a production SQL engine that integrates native semantic operations directly into SQL. This integration allows users to write declarative queries that combine relational operations with semantic reasoning, enabling them to query both structured and unstructured data effortlessly. However, making semantic operations efficient at production scale poses fundamental challenges. Semantic operations are more expensive than traditional SQL operations, possess distinct latency and throughput characteristics, and their cost and selectivity are unknown during query compilation. Furthermore, existing query engines are not designed to optimize semantic operations. The AISQL query execution engine addresses these challenges through three novel techniques informed by production deployment data from Snowflake customers. First, AI-aware query optimization treats AI inference cost as a first-class optimization objective, reasoning about large language model (LLM) cost directly during query planning to achieve 2-8$\\times$ speedups. Second, adaptive model cascades reduce inference costs by routing most rows through a fast proxy model while escalating uncertain cases to a powerful oracle model, achieving 2-6$\\times$ speedups while maintaining 90-95% of oracle model quality. Third, semantic join query rewriting lowers the quadratic time complexity of join operations to linear through reformulation as multi-label classification tasks, achieving 15-70$\\times$ speedups with often improved prediction quality. AISQL is deployed in production at Snowflake, where it powers diverse customer workloads across analytics, search, and content understanding.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 107,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07665",
        "abs_url": "https://arxiv.org/abs/2511.07665",
        "pdf_url": "https://arxiv.org/pdf/2511.07665",
        "title": "FractalCloud: A Fractal-Inspired Architecture for Efficient Large-Scale Point Cloud Processing",
        "authors": [
            "Yuzhe Fu",
            "Changchun Zhou",
            "Hancheng Ye",
            "Bowen Duan",
            "Qiyu Huang",
            "Chiyue Wei",
            "Cong Guo",
            "Hai \"Helen'' Li",
            "Yiran Chen"
        ],
        "comments": "Accepted for publication in HPCA2026. Codes will be released later",
        "subjects": "Hardware Architecture (cs.AR); Artificial Intelligence (cs.AI)",
        "abstract": "Three-dimensional (3D) point clouds are increasingly used in applications such as autonomous driving, robotics, and virtual reality (VR). Point-based neural networks (PNNs) have demonstrated strong performance in point cloud analysis, originally targeting small-scale inputs. However, as PNNs evolve to process large-scale point clouds with hundreds of thousands of points, all-to-all computation and global memory access in point cloud processing introduce substantial overhead, causing $O(n^2)$ computational complexity and memory traffic where n is the number of points}. Existing accelerators, primarily optimized for small-scale workloads, overlook this challenge and scale poorly due to inefficient partitioning and non-parallel architectures. To address these issues, we propose FractalCloud, a fractal-inspired hardware architecture for efficient large-scale 3D point cloud processing. FractalCloud introduces two key optimizations: (1) a co-designed Fractal method for shape-aware and hardware-friendly partitioning, and (2) block-parallel point operations that decompose and parallelize all point operations. A dedicated hardware design with on-chip fractal and flexible parallelism further enables fully parallel processing within limited memory resources. Implemented in 28 nm technology as a chip layout with a core area of 1.5 $mm^2$, FractalCloud achieves 21.7x speedup and 27x energy reduction over state-of-the-art accelerators while maintaining network accuracy, demonstrating its scalability and efficiency for PNN inference.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 108,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07677",
        "abs_url": "https://arxiv.org/abs/2511.07677",
        "pdf_url": "https://arxiv.org/pdf/2511.07677",
        "title": "Speech Separation for Hearing-Impaired Children in the Classroom",
        "authors": [
            "Feyisayo Olalere",
            "Kiki van der Heijden",
            "H. Christiaan Stronks",
            "Jeroen Briaire",
            "Johan H. M. Frijns",
            "Yagmur Gltrk"
        ],
        "comments": "13 pages",
        "subjects": "Sound (cs.SD); Artificial Intelligence (cs.AI)",
        "abstract": "Classroom environments are particularly challenging for children with hearing impairments, where background noise, multiple talkers, and reverberation degrade speech perception. These difficulties are greater for children than adults, yet most deep learning speech separation models for assistive devices are developed using adult voices in simplified, low-reverberation conditions. This overlooks both the higher spectral similarity of children's voices, which weakens separation cues, and the acoustic complexity of real classrooms. We address this gap using MIMO-TasNet, a compact, low-latency, multi-channel architecture suited for real-time deployment in bilateral hearing aids or cochlear implants. We simulated naturalistic classroom scenes with moving child-child and child-adult talker pairs under varying noise and distance conditions. Training strategies tested how well the model adapts to children's speech through spatial cues. Models trained on adult speech, classroom data, and finetuned variants were compared to assess data-efficient adaptation. Results show that adult-trained models perform well in clean scenes, but classroom-specific training greatly improves separation quality. Finetuning with only half the classroom data achieved comparable gains, confirming efficient transfer learning. Training with diffuse babble noise further enhanced robustness, and the model preserved spatial awareness while generalizing to unseen distances. These findings demonstrate that spatially aware architectures combined with targeted adaptation can improve speech accessibility for children in noisy classrooms, supporting future on-device assistive technologies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 109,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07682",
        "abs_url": "https://arxiv.org/abs/2511.07682",
        "pdf_url": "https://arxiv.org/pdf/2511.07682",
        "title": "Designing and Evaluating Malinowski's Lens: An AI-Native Educational Game for Ethnographic Learning",
        "authors": [
            "Michael Hoffmann",
            "Jophin John",
            "Jan Fillies",
            "Adrian Paschke"
        ],
        "comments": "21 pages, 8 figures. Full preprint version; shorter version in preparation",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)",
        "abstract": "This study introduces 'Malinowski's Lens', the first AI-native educational game for anthropology that transforms Bronislaw Malinowski's 'Argonauts of the Western Pacific' (1922) into an interactive learning experience. The system combines Retrieval-Augmented Generation with DALL-E 3 text-to-image generation, creating consistent VGA-style visuals as players embody Malinowski during his Trobriand Islands fieldwork (1915-1918). To address ethical concerns, indigenous peoples appear as silhouettes while Malinowski is detailed, prompting reflection on anthropological representation. Two validation studies confirmed effectiveness: Study 1 with 10 non-specialists showed strong learning outcomes (average quiz score 7.5/10) and excellent usability (SUS: 83/100). Study 2 with 4 expert anthropologists confirmed pedagogical value, with one senior researcher discovering \"new aspects\" of Malinowski's work through gameplay. The findings demonstrate that AI-driven educational games can effectively convey complex anthropological concepts while sparking disciplinary curiosity. This study advances AI-native educational game design and provides a replicable model for transforming academic texts into engaging interactive experiences.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 110,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07689",
        "abs_url": "https://arxiv.org/abs/2511.07689",
        "pdf_url": "https://arxiv.org/pdf/2511.07689",
        "title": "Stress Testing Factual Consistency Metrics for Long-Document Summarization",
        "authors": [
            "Zain Muhammad Mujahid",
            "Dustin Wright",
            "Isabelle Augenstein"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Evaluating the factual consistency of abstractive text summarization remains a significant challenge, particularly for long documents, where conventional metrics struggle with input length limitations and long-range dependencies. In this work, we systematically evaluate the reliability of six widely used reference-free factuality metrics, originally proposed for short-form summarization, in the long-document setting. We probe metric robustness through seven factuality-preserving perturbations applied to summaries, namely paraphrasing, simplification, synonym replacement, logically equivalent negations, vocabulary reduction, compression, and source text insertion, and further analyze their sensitivity to retrieval context and claim information density. Across three long-form benchmark datasets spanning science fiction, legal, and scientific domains, our results reveal that existing short-form metrics produce inconsistent scores for semantically equivalent summaries and exhibit declining reliability for information-dense claims whose content is semantically similar to many parts of the source document. While expanding the retrieval context improves stability in some domains, no metric consistently maintains factual alignment under long-context conditions. Finally, our results highlight concrete directions for improving factuality evaluation, including multi-span reasoning, context-aware calibration, and training on meaning-preserving variations to enhance robustness in long-form summarization. We release all code, perturbed data, and scripts required to reproduce our results at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 111,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07691",
        "abs_url": "https://arxiv.org/abs/2511.07691",
        "pdf_url": "https://arxiv.org/pdf/2511.07691",
        "title": "CAPO: Confidence Aware Preference Optimization Learning for Multilingual Preferences",
        "authors": [
            "Rhitabrat Pokharel",
            "Yufei Tao",
            "Ameeta Agrawal"
        ],
        "comments": "Accepted at IJCNLP-AACL 2025 Findings",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Preference optimization is a critical post-training technique used to align large language models (LLMs) with human preferences, typically by fine-tuning on ranked response pairs. While methods like Direct Preference Optimization (DPO) have proven effective in English, they often fail to generalize robustly to multilingual settings. We propose a simple yet effective alternative, Confidence-Aware Preference Optimization (CAPO), which replaces DPO's fixed treatment of preference pairs with a dynamic loss scaling mechanism based on a relative reward. By modulating the learning signal according to the confidence in each preference pair, CAPO enhances robustness to noisy or low-margin comparisons, typically encountered in multilingual text. Empirically, CAPO outperforms existing preference optimization baselines by at least 16% in reward accuracy, and improves alignment by widening the gap between preferred and dispreferred responses across languages.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 112,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07701",
        "abs_url": "https://arxiv.org/abs/2511.07701",
        "pdf_url": "https://arxiv.org/pdf/2511.07701",
        "title": "Diffusion Guided Adversarial State Perturbations in Reinforcement Learning",
        "authors": [
            "Xiaolin Sun",
            "Feidi Liu",
            "Zhengming Ding",
            "ZiZhan Zheng"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Reinforcement learning (RL) systems, while achieving remarkable success across various domains, are vulnerable to adversarial attacks. This is especially a concern in vision-based environments where minor manipulations of high-dimensional image inputs can easily mislead the agent's behavior. To this end, various defenses have been proposed recently, with state-of-the-art approaches achieving robust performance even under large state perturbations. However, after closer investigation, we found that the effectiveness of the current defenses is due to a fundamental weakness of the existing $l_p$ norm-constrained attacks, which can barely alter the semantics of image input even under a relatively large perturbation budget. In this work, we propose SHIFT, a novel policy-agnostic diffusion-based state perturbation attack to go beyond this limitation. Our attack is able to generate perturbed states that are semantically different from the true states while remaining realistic and history-aligned to avoid detection. Evaluations show that our attack effectively breaks existing defenses, including the most sophisticated ones, significantly outperforming existing attacks while being more perceptually stealthy. The results highlight the vulnerability of RL agents to semantics-aware adversarial perturbations, indicating the importance of developing more robust policies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 113,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07707",
        "abs_url": "https://arxiv.org/abs/2511.07707",
        "pdf_url": "https://arxiv.org/pdf/2511.07707",
        "title": "A Negotiation-Based Multi-Agent Reinforcement Learning Approach for Dynamic Scheduling of Reconfigurable Manufacturing Systems",
        "authors": [
            "Manonmani Sekar",
            "Nasim Nezamoddini"
        ],
        "comments": "",
        "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI)",
        "abstract": "Reconfigurable manufacturing systems (RMS) are critical for future market adjustment given their rapid adaptation to fluctuations in consumer demands, the introduction of new technological advances, and disruptions in linked supply chain sections. The adjustable hard settings of such systems require a flexible soft planning mechanism that enables realtime production planning and scheduling amid the existing complexity and variability in their configuration settings. This study explores the application of multi agent reinforcement learning (MARL) for dynamic scheduling in soft planning of the RMS settings. In the proposed framework, deep Qnetwork (DQN) agents trained in centralized training learn optimal job machine assignments in real time while adapting to stochastic events such as machine breakdowns and reconfiguration delays. The model also incorporates a negotiation with an attention mechanism to enhance state representation and improve decision focus on critical system features. Key DQN enhancements including prioritized experience replay, nstep returns, double DQN and soft target update are used to stabilize and accelerate learning. Experiments conducted in a simulated RMS environment demonstrate that the proposed approach outperforms baseline heuristics in reducing makespan and tardiness while improving machine utilization. The reconfigurable manufacturing environment was extended to simulate realistic challenges, including machine failures and reconfiguration times. Experimental results show that while the enhanced DQN agent is effective in adapting to dynamic conditions, machine breakdowns increase variability in key performance metrics such as makespan, throughput, and total tardiness. The results confirm the advantages of applying the MARL mechanism for intelligent and adaptive scheduling in dynamic reconfigurable manufacturing environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 114,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07734",
        "abs_url": "https://arxiv.org/abs/2511.07734",
        "pdf_url": "https://arxiv.org/pdf/2511.07734",
        "title": "Global Optimization on Graph-Structured Data via Gaussian Processes with Spectral Representations",
        "authors": [
            "Shu Hong",
            "Yongsheng Mei",
            "Mahdi Imani",
            "Tian Lan"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Social and Information Networks (cs.SI); Signal Processing (eess.SP)",
        "abstract": "Bayesian optimization (BO) is a powerful framework for optimizing expensive black-box objectives, yet extending it to graph-structured domains remains challenging due to the discrete and combinatorial nature of graphs. Existing approaches often rely on either full graph topology-impractical for large or partially observed graphs-or incremental exploration, which can lead to slow convergence. We introduce a scalable framework for global optimization over graphs that employs low-rank spectral representations to build Gaussian process (GP) surrogates from sparse structural observations. The method jointly infers graph structure and node representations through learnable embeddings, enabling efficient global search and principled uncertainty estimation even with limited data. We also provide theoretical analysis establishing conditions for accurate recovery of underlying graph structure under different sampling regimes. Experiments on synthetic and real-world datasets demonstrate that our approach achieves faster convergence and improved optimization performance compared to prior methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 115,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07737",
        "abs_url": "https://arxiv.org/abs/2511.07737",
        "pdf_url": "https://arxiv.org/pdf/2511.07737",
        "title": "TurboSAT: Gradient-Guided Boolean Satisfiability Accelerated on GPU-CPU Hybrid System",
        "authors": [
            "Steve Dai",
            "Cunxi Yu",
            "Kalyan Krishnamani",
            "Brucek Khailany"
        ],
        "comments": "7 pages, 5 equations, 5 figures, 1 table",
        "subjects": "Logic in Computer Science (cs.LO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Mathematical Software (cs.MS)",
        "abstract": "While accelerated computing has transformed many domains of computing, its impact on logical reasoning, specifically Boolean satisfiability (SAT), remains limited. State-of-the-art SAT solvers rely heavily on inherently sequential conflict-driven search algorithms that offer powerful heuristics but limit the amount of parallelism that could otherwise enable significantly more scalable SAT solving. Inspired by neural network training, we formulate the SAT problem as a binarized matrix-matrix multiplication layer that could be optimized using a differentiable objective function. Enabled by this encoding, we combine the strengths of parallel differentiable optimization and sequential search to accelerate SAT on a hybrid GPU-CPU system. In this system, the GPUs leverage parallel differentiable solving to rapidly evaluate SAT clauses and use gradients to stochastically explore the solution space and optimize variable assignments. Promising partial assignments generated by the GPUs are post-processed on many CPU threads which exploit conflict-driven sequential search to further traverse the solution subspaces and identify complete assignments. Prototyping the hybrid solver on an NVIDIA DGX GB200 node, our solver achieves runtime speedups up to over 200x when compared to a state-of-the-art CPU-based solver on public satisfiable benchmark problems from the SAT Competition.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 116,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07748",
        "abs_url": "https://arxiv.org/abs/2511.07748",
        "pdf_url": "https://arxiv.org/pdf/2511.07748",
        "title": "Auto-US: An Ultrasound Video Diagnosis Agent Using Video Classification Framework and LLMs",
        "authors": [
            "Yuezhe Yang",
            "Yiyue Guo",
            "Wenjie Cai",
            "Qingqing Ruan",
            "Siying Wang",
            "Xingbo Dong",
            "Zhe Jin",
            "Yong Dai"
        ],
        "comments": "Under Review",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "AI-assisted ultrasound video diagnosis presents new opportunities to enhance the efficiency and accuracy of medical imaging analysis. However, existing research remains limited in terms of dataset diversity, diagnostic performance, and clinical applicability. In this study, we propose \\textbf{Auto-US}, an intelligent diagnosis agent that integrates ultrasound video data with clinical diagnostic text. To support this, we constructed \\textbf{CUV Dataset} of 495 ultrasound videos spanning five categories and three organs, aggregated from multiple open-access sources. We developed \\textbf{CTU-Net}, which achieves state-of-the-art performance in ultrasound video classification, reaching an accuracy of 86.73\\% Furthermore, by incorporating large language models, Auto-US is capable of generating clinically meaningful diagnostic suggestions. The final diagnostic scores for each case exceeded 3 out of 5 and were validated by professional clinicians. These results demonstrate the effectiveness and clinical potential of Auto-US in real-world ultrasound applications. Code and data are available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 117,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07772",
        "abs_url": "https://arxiv.org/abs/2511.07772",
        "pdf_url": "https://arxiv.org/pdf/2511.07772",
        "title": "SALT: Steering Activations towards Leakage-free Thinking in Chain of Thought",
        "authors": [
            "Shourya Batra",
            "Pierce Tillman",
            "Samarth Gaggar",
            "Shashank Kesineni",
            "Kevin Zhu",
            "Sunishchal Dev",
            "Ashwinee Panda",
            "Vasu Sharma",
            "Maheep Chaudhary"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "As Large Language Models (LLMs) evolve into personal assistants with access to sensitive user data, they face a critical privacy challenge: while prior work has addressed output-level privacy, recent findings reveal that LLMs often leak private information through their internal reasoning processes, violating contextual privacy expectations. These leaky thoughts occur when models inadvertently expose sensitive details in their reasoning traces, even when final outputs appear safe. The challenge lies in preventing such leakage without compromising the model's reasoning capabilities, requiring a delicate balance between privacy and utility. We introduce Steering Activations towards Leakage-free Thinking (SALT), a lightweight test-time intervention that mitigates privacy leakage in model's Chain of Thought (CoT) by injecting targeted steering vectors into hidden state. We identify the high-leakage layers responsible for this behavior. Through experiments across multiple LLMs, we demonstrate that SALT achieves reductions including $18.2\\%$ reduction in CPL on QwQ-32B, $17.9\\%$ reduction in CPL on Llama-3.1-8B, and $31.2\\%$ reduction in CPL on Deepseek in contextual privacy leakage dataset AirGapAgent-R while maintaining comparable task performance and utility. Our work establishes SALT as a practical approach for test-time privacy protection in reasoning-capable language models, offering a path toward safer deployment of LLM-based personal agents.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 118,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07787",
        "abs_url": "https://arxiv.org/abs/2511.07787",
        "pdf_url": "https://arxiv.org/pdf/2511.07787",
        "title": "Physical Consistency of Aurora's Encoder: A Quantitative Study",
        "authors": [
            "Benjamin Richards",
            "Pushpa Kumar Balan"
        ],
        "comments": "Accepted for poster presentation at the AICC: Workshop on AI for Climate and Conservation at EurIPS 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "The high accuracy of large-scale weather forecasting models like Aurora is often accompanied by a lack of transparency, as their internal representations remain largely opaque. This \"black box\" nature hinders their adoption in high-stakes operational settings. In this work, we probe the physical consistency of Aurora's encoder by investigating whether its latent representations align with known physical and meteorological concepts. Using a large-scale dataset of embeddings, we train linear classifiers to identify three distinct concepts: the fundamental land-sea boundary, high-impact extreme temperature events, and atmospheric instability. Our findings provide quantitative evidence that Aurora learns physically consistent features, while also highlighting its limitations in capturing the rarest events. This work underscores the critical need for interpretability methods to validate and build trust in the next generation of Al-driven weather models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 119,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07793",
        "abs_url": "https://arxiv.org/abs/2511.07793",
        "pdf_url": "https://arxiv.org/pdf/2511.07793",
        "title": "HybridGuard: Enhancing Minority-Class Intrusion Detection in Dew-Enabled Edge-of-Things Networks",
        "authors": [
            "Binayak Kara",
            "Ujjwal Sahua",
            "Ciza Thomas",
            "Jyoti Prakash Sahoo"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Securing Dew-Enabled Edge-of-Things (EoT) networks against sophisticated intrusions is a critical challenge. This paper presents HybridGuard, a framework that integrates machine learning and deep learning to improve intrusion detection. HybridGuard addresses data imbalance through mutual information based feature selection, ensuring that the most relevant features are used to improve detection performance, especially for minority attack classes. The framework leverages Wasserstein Conditional Generative Adversarial Networks with Gradient Penalty (WCGAN-GP) to further reduce class imbalance and enhance detection precision. It adopts a two-phase architecture called DualNetShield to support advanced traffic analysis and anomaly detection, improving the granular identification of threats in complex EoT environments. HybridGuard is evaluated on the UNSW-NB15, CIC-IDS-2017, and IOTID20 datasets, where it demonstrates strong performance across diverse attack scenarios and outperforms existing solutions in adapting to evolving cybersecurity threats. This approach establishes HybridGuard as an effective tool for protecting EoT networks against modern intrusions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 120,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07803",
        "abs_url": "https://arxiv.org/abs/2511.07803",
        "pdf_url": "https://arxiv.org/pdf/2511.07803",
        "title": "Judging by the Rules: Compliance-Aligned Framework for Modern Slavery Statement Monitoring",
        "authors": [
            "Wenhao Xu",
            "Akshatha Arodi",
            "Jian-Yun Nie",
            "Arsene Fansi Tchango"
        ],
        "comments": "To appear at AAAI-26 (Social Impact Track)",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI)",
        "abstract": "Modern slavery affects millions of people worldwide, and regulatory frameworks such as Modern Slavery Acts now require companies to publish detailed disclosures. However, these statements are often vague and inconsistent, making manual review time-consuming and difficult to scale. While NLP offers a promising path forward, high-stakes compliance tasks require more than accurate classification: they demand transparent, rule-aligned outputs that legal experts can verify. Existing applications of large language models (LLMs) often reduce complex regulatory assessments to binary decisions, lacking the necessary structure for robust legal scrutiny. We argue that compliance verification is fundamentally a rule-matching problem: it requires evaluating whether textual statements adhere to well-defined regulatory rules. To this end, we propose a novel framework that harnesses AI for rule-level compliance verification while preserving expert oversight. At its core is the Compliance Alignment Judge (CA-Judge), which evaluates model-generated justifications based on their fidelity to statutory requirements. Using this feedback, we train the Compliance Alignment LLM (CALLM), a model that produces rule-consistent, human-verifiable outputs. CALLM improves predictive performance and generates outputs that are both transparent and legally grounded, offering a more verifiable and actionable solution for real-world compliance analysis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 121,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07807",
        "abs_url": "https://arxiv.org/abs/2511.07807",
        "pdf_url": "https://arxiv.org/pdf/2511.07807",
        "title": "PRISM: Privacy-preserving Inference System with Homomorphic Encryption and Modular Activation",
        "authors": [
            "Zeinab Elkhatib",
            "Ali Sekmen",
            "Kamrul Hasan"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "With the rapid advancements in machine learning, models have become increasingly capable of learning and making predictions in various industries. However, deploying these models in critical infrastructures presents a major challenge, as concerns about data privacy prevent unrestricted data sharing. Homomor- phic encryption (HE) offers a solution by enabling computations on encrypted data, but it remains incompatible with machine learning models like convolutional neural networks (CNNs), due to their reliance on non-linear activation functions. To bridge this gap, this work proposes an optimized framework that replaces standard non-linear functions with homomorphically compatible approximations, ensuring secure computations while minimizing computational overhead. The proposed approach restructures the CNN architecture and introduces an efficient activation function approximation method to mitigate the performance trade-offs in- troduced by encryption. Experiments on CIFAR-10 achieve 94.4% accuracy with 2.42 s per single encrypted sample and 24,000 s per 10,000 encrypted samples, using a degree-4 polynomial and Softplus activation under CKKS, balancing accuracy and privacy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 122,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07820",
        "abs_url": "https://arxiv.org/abs/2511.07820",
        "pdf_url": "https://arxiv.org/pdf/2511.07820",
        "title": "SONIC: Supersizing Motion Tracking for Natural Humanoid Whole-Body Control",
        "authors": [
            "Zhengyi Luo",
            "Ye Yuan",
            "Tingwu Wang",
            "Chenran Li",
            "Sirui Chen",
            "Fernando Castaeda",
            "Zi-Ang Cao",
            "Jiefeng Li",
            "David Minor",
            "Qingwei Ben",
            "Xingye Da",
            "Runyu Ding",
            "Cyrus Hogg",
            "Lina Song",
            "Edy Lim",
            "Eugene Jeong",
            "Tairan He",
            "Haoru Xue",
            "Wenli Xiao",
            "Zi Wang",
            "Simon Yuen",
            "Jan Kautz",
            "Yan Chang",
            "Umar Iqbal",
            "Linxi \"Jim\" Fan",
            "Yuke Zhu"
        ],
        "comments": "Project page: this https URL",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR); Systems and Control (eess.SY)",
        "abstract": "Despite the rise of billion-parameter foundation models trained across thousands of GPUs, similar scaling gains have not been shown for humanoid control. Current neural controllers for humanoids remain modest in size, target a limited behavior set, and are trained on a handful of GPUs over several days. We show that scaling up model capacity, data, and compute yields a generalist humanoid controller capable of creating natural and robust whole-body movements. Specifically, we posit motion tracking as a natural and scalable task for humanoid control, leverageing dense supervision from diverse motion-capture data to acquire human motion priors without manual reward engineering. We build a foundation model for motion tracking by scaling along three axes: network size (from 1.2M to 42M parameters), dataset volume (over 100M frames, 700 hours of high-quality motion data), and compute (9k GPU hours). Beyond demonstrating the benefits of scale, we show the practical utility of our model through two mechanisms: (1) a real-time universal kinematic planner that bridges motion tracking to downstream task execution, enabling natural and interactive control, and (2) a unified token space that supports various motion input interfaces, such as VR teleoperation devices, human videos, and vision-language-action (VLA) models, all using the same policy. Scaling motion tracking exhibits favorable properties: performance improves steadily with increased compute and data diversity, and learned representations generalize to unseen motions, establishing motion tracking at scale as a practical foundation for humanoid control.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 123,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07833",
        "abs_url": "https://arxiv.org/abs/2511.07833",
        "pdf_url": "https://arxiv.org/pdf/2511.07833",
        "title": "MURPHY: Multi-Turn GRPO for Self Correcting Code Generation",
        "authors": [
            "Chanakya Ekbote",
            "Vijay Lingam",
            "Behrooz Omidvar-Tehrani",
            "Jun Huan",
            "Sujay Sanghavi",
            "Anoop Deoras",
            "Stefano Soatto"
        ],
        "comments": "20 pages, 2 figures, 6 Tables",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful framework for enhancing the reasoning capabilities of large language models (LLMs). However, existing approaches such as Group Relative Policy Optimization (GRPO) and its variants, while effective on reasoning benchmarks, struggle with agentic tasks that require iterative decision-making. We introduce Murphy, a multi-turn reflective optimization framework that extends GRPO by incorporating iterative self-correction during training. By leveraging both quantitative and qualitative execution feedback, Murphy enables models to progressively refine their reasoning across multiple turns. Evaluations on code generation benchmarks with model families such as Qwen and OLMo show that Murphy consistently improves performance, achieving up to a 8% relative gain in pass@1 over GRPO, on similar compute budgets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 124,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07857",
        "abs_url": "https://arxiv.org/abs/2511.07857",
        "pdf_url": "https://arxiv.org/pdf/2511.07857",
        "title": "A General Method for Proving Networks Universal Approximation Property",
        "authors": [
            "Wei Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Deep learning architectures are highly diverse. To prove their universal approximation properties, existing works typically rely on model-specific proofs. Generally, they construct a dedicated mathematical formulation for each architecture (e.g., fully connected networks, CNNs, or Transformers) and then prove their universal approximability. However, this approach suffers from two major limitations: first, every newly proposed architecture often requires a completely new proof from scratch; second, these proofs are largely isolated from one another, lacking a common analytical foundation. This not only incurs significant redundancy but also hinders unified theoretical understanding across different network families. To address these issues, this paper proposes a general and modular framework for proving universal approximation. We define a basic building block (comprising one or multiple layers) that possesses the universal approximation property as a Universal Approximation Module (UAM). Under this condition, we show that any deep network composed of such modules inherently retains the universal approximation property. Moreover, the overall approximation process can be interpreted as a progressive refinement across modules. This perspective not only unifies the analysis of diverse architectures but also enables a step-by-step understanding of how expressive power evolves through the network.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 125,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07865",
        "abs_url": "https://arxiv.org/abs/2511.07865",
        "pdf_url": "https://arxiv.org/pdf/2511.07865",
        "title": "LLM-Powered Fully Automated Chaos Engineering: Towards Enabling Anyone to Build Resilient Software Systems at Low Cost",
        "authors": [
            "Daisuke Kikuta",
            "Hiroki Ikeuchi",
            "Kengo Tajiri"
        ],
        "comments": "Accepted at ASE 2025 NIER Track. The code is available at this https URL",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Multiagent Systems (cs.MA)",
        "abstract": "Chaos Engineering (CE) is an engineering technique aimed at improving the resilience of distributed systems. It involves intentionally injecting faults into a system to test its resilience, uncover weaknesses, and address them before they cause failures in production. Recent CE tools automate the execution of predefined CE experiments. However, planning such experiments and improving the system based on the experimental results still remain manual. These processes are labor-intensive and require multi-domain expertise. To address these challenges and enable anyone to build resilient systems at low cost, this paper proposes ChaosEater, a system that automates the entire CE cycle with Large Language Models (LLMs). It predefines an agentic workflow according to a systematic CE cycle and assigns subdivided processes within the workflow to LLMs. ChaosEater targets CE for software systems built on Kubernetes. Therefore, the LLMs in ChaosEater complete CE cycles through software engineering tasks, including requirement definition, code generation, testing, and debugging. We evaluate ChaosEater through case studies on small- and large-scale Kubernetes systems. The results demonstrate that it consistently completes reasonable CE cycles with significantly low time and monetary costs. Its cycles are also qualitatively validated by human engineers and LLMs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 126,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07876",
        "abs_url": "https://arxiv.org/abs/2511.07876",
        "pdf_url": "https://arxiv.org/pdf/2511.07876",
        "title": "LoopLLM: Transferable Energy-Latency Attacks in LLMs via Repetitive Generation",
        "authors": [
            "Xingyu Li",
            "Xiaolei Liu",
            "Cheng Liu",
            "Yixiao Xu",
            "Kangyi Ding",
            "Bangzhou Xin",
            "Jia-Li Yin"
        ],
        "comments": "14 pages with 7 figures; accepted by the AAAI 2026",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "As large language models (LLMs) scale, their inference incurs substantial computational resources, exposing them to energy-latency attacks, where crafted prompts induce high energy and latency cost. Existing attack methods aim to prolong output by delaying the generation of termination symbols. However, as the output grows longer, controlling the termination symbols through input becomes difficult, making these methods less effective. Therefore, we propose LoopLLM, an energy-latency attack framework based on the observation that repetitive generation can trigger low-entropy decoding loops, reliably compelling LLMs to generate until their output limits. LoopLLM introduces (1) a repetition-inducing prompt optimization that exploits autoregressive vulnerabilities to induce repetitive generation, and (2) a token-aligned ensemble optimization that aggregates gradients to improve cross-model transferability. Extensive experiments on 12 open-source and 2 commercial LLMs show that LoopLLM significantly outperforms existing methods, achieving over 90% of the maximum output length, compared to 20% for baselines, and improving transferability by around 40% to DeepSeek-V3 and Gemini 2.5 Flash.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 127,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07884",
        "abs_url": "https://arxiv.org/abs/2511.07884",
        "pdf_url": "https://arxiv.org/pdf/2511.07884",
        "title": "Meta-cognitive Multi-scale Hierarchical Reasoning for Motor Imagery Decoding",
        "authors": [
            "Si-Hyun Kim",
            "Heon-Gyu Kwak",
            "Byoung-Hee Kwon",
            "Seong-Whan Lee"
        ],
        "comments": "4 pages, 1 figures, 1 table, Name of Conference: International Winter Conference on Brain-Computer Interface",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Brain-computer interface (BCI) aims to decode motor intent from noninvasive neural signals to enable control of external devices, but practical deployment remains limited by noise and variability in motor imagery (MI)-based electroencephalogram (EEG) signals. This work investigates a hierarchical and meta-cognitive decoding framework for four-class MI classification. We introduce a multi-scale hierarchical signal processing module that reorganizes backbone features into temporal multi-scale representations, together with an introspective uncertainty estimation module that assigns per-cycle reliability scores and guides iterative refinement. We instantiate this framework on three standard EEG backbones (EEGNet, ShallowConvNet, and DeepConvNet) and evaluate four-class MI decoding using the BCI Competition IV-2a dataset under a subject-independent setting. Across all backbones, the proposed components improve average classification accuracy and reduce inter-subject variance compared to the corresponding baselines, indicating increased robustness to subject heterogeneity and noisy trials. These results suggest that combining hierarchical multi-scale processing with introspective confidence estimation can enhance the reliability of MI-based BCI systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 128,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07885",
        "abs_url": "https://arxiv.org/abs/2511.07885",
        "pdf_url": "https://arxiv.org/pdf/2511.07885",
        "title": "Intelligence per Watt: Measuring Intelligence Efficiency of Local AI",
        "authors": [
            "Jon Saad-Falcon",
            "Avanika Narayan",
            "Hakki Orhun Akengin",
            "J. Wes Griffin",
            "Herumb Shandilya",
            "Adrian Gamarra Lafuente",
            "Medhya Goel",
            "Rebecca Joseph",
            "Shlok Natarajan",
            "Etash Kumar Guha",
            "Shang Zhu",
            "Ben Athiwaratkun",
            "John Hennessy",
            "Azalia Mirhoseini",
            "Christopher R"
        ],
        "comments": "",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Large language model (LLM) queries are predominantly processed by frontier models in centralized cloud infrastructure. Rapidly growing demand strains this paradigm, and cloud providers struggle to scale infrastructure at pace. Two advances enable us to rethink this paradigm: small LMs (<=20B active parameters) now achieve competitive performance to frontier models on many tasks, and local accelerators (e.g., Apple M4 Max) run these models at interactive latencies. This raises the question: can local inference viably redistribute demand from centralized infrastructure? Answering this requires measuring whether local LMs can accurately answer real-world queries and whether they can do so efficiently enough to be practical on power-constrained devices (i.e., laptops). We propose intelligence per watt (IPW), task accuracy divided by unit of power, as a metric for assessing capability and efficiency of local inference across model-accelerator pairs. We conduct a large-scale empirical study across 20+ state-of-the-art local LMs, 8 accelerators, and a representative subset of LLM traffic: 1M real-world single-turn chat and reasoning queries. For each query, we measure accuracy, energy, latency, and power. Our analysis reveals $3$ findings. First, local LMs can accurately answer 88.7% of single-turn chat and reasoning queries with accuracy varying by domain. Second, from 2023-2025, IPW improved 5.3x and local query coverage rose from 23.2% to 71.3%. Third, local accelerators achieve at least 1.4x lower IPW than cloud accelerators running identical models, revealing significant headroom for optimization. These findings demonstrate that local inference can meaningfully redistribute demand from centralized infrastructure, with IPW serving as the critical metric for tracking this transition. We release our IPW profiling harness for systematic intelligence-per-watt benchmarking.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 129,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07891",
        "abs_url": "https://arxiv.org/abs/2511.07891",
        "pdf_url": "https://arxiv.org/pdf/2511.07891",
        "title": "Toward Adaptive BCIs: Enhancing Decoding Stability via User State-Aware EEG Filtering",
        "authors": [
            "Yeon-Woo Choi",
            "Hye-Bin Shin",
            "Dan Li"
        ],
        "comments": "4 pages, 3 figures, conference",
        "subjects": "Signal Processing (eess.SP); Artificial Intelligence (cs.AI)",
        "abstract": "Brain-computer interfaces (BCIs) often suffer from limited robustness and poor long-term adaptability. Model performance rapidly degrades when user attention fluctuates, brain states shift over time, or irregular artifacts appear during interaction. To mitigate these issues, we introduce a user state-aware electroencephalogram (EEG) filtering framework that refines neural representations before decoding user intentions. The proposed method continuously estimates the user's cognitive state (e.g., focus or distraction) from EEG features and filters unreliable segments by applying adaptive weighting based on the estimated attention level. This filtering stage suppresses noisy or out-of-focus epochs, thereby reducing distributional drift and improving the consistency of subsequent decoding. Experiments on multiple EEG datasets that emulate real BCI scenarios demonstrate that the proposed state-aware filtering enhances classification accuracy and stability across different user states and sessions compared with conventional preprocessing pipelines. These findings highlight that leveraging brain-derived state information--even without additional user labels--can substantially improve the reliability of practical EEG-based BCIs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 130,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07899",
        "abs_url": "https://arxiv.org/abs/2511.07899",
        "pdf_url": "https://arxiv.org/pdf/2511.07899",
        "title": "Statistically Assuring Safety of Control Systems using Ensembles of Safety Filters and Conformal Prediction",
        "authors": [
            "Ihab Tabbara",
            "Yuxuan Yang",
            "Hussein Sibai"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Robotics (cs.RO); Systems and Control (eess.SY)",
        "abstract": "Safety assurance is a fundamental requirement for deploying learning-enabled autonomous systems. Hamilton-Jacobi (HJ) reachability analysis is a fundamental method for formally verifying safety and generating safe controllers. However, computing the HJ value function that characterizes the backward reachable set (BRS) of a set of user-defined failure states is computationally expensive, especially for high-dimensional systems, motivating the use of reinforcement learning approaches to approximate the value function. Unfortunately, a learned value function and its corresponding safe policy are not guaranteed to be correct. The learned value function evaluated at a given state may not be equal to the actual safety return achieved by following the learned safe policy. To address this challenge, we introduce a conformal prediction-based (CP) framework that bounds such uncertainty. We leverage CP to provide probabilistic safety guarantees when using learned HJ value functions and policies to prevent control systems from reaching failure states. Specifically, we use CP to calibrate the switching between the unsafe nominal controller and the learned HJ-based safe policy and to derive safety guarantees under this switched policy. We also investigate using an ensemble of independently trained HJ value functions as a safety filter and compare this ensemble approach to using individual value functions alone.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 131,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07904",
        "abs_url": "https://arxiv.org/abs/2511.07904",
        "pdf_url": "https://arxiv.org/pdf/2511.07904",
        "title": "Test-driven Reinforcement Learning",
        "authors": [
            "Zhao Yu",
            "Xiuping Wu",
            "Liangjun Ke"
        ],
        "comments": "AAAI 2026 oral",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Reinforcement learning (RL) has been recognized as a powerful tool for robot control tasks. RL typically employs reward functions to define task objectives and guide agent learning. However, since the reward function serves the dual purpose of defining the optimal goal and guiding learning, it is challenging to design the reward function manually, which often results in a suboptimal task representation. To tackle the reward design challenge in RL, inspired by the satisficing theory, we propose a Test-driven Reinforcement Learning (TdRL) framework. In the TdRL framework, multiple test functions are used to represent the task objective rather than a single reward function. Test functions can be categorized as pass-fail tests and indicative tests, each dedicated to defining the optimal objective and guiding the learning process, respectively, thereby making defining tasks easier. Building upon such a task definition, we first prove that if a trajectory return function assigns higher returns to trajectories closer to the optimal trajectory set, maximum entropy policy optimization based on this return function will yield a policy that is closer to the optimal policy set. Then, we introduce a lexicographic heuristic approach to compare the relative distance relationship between trajectories and the optimal trajectory set for learning the trajectory return function. Furthermore, we develop an algorithm implementation of TdRL. Experimental results on the DeepMind Control Suite benchmark demonstrate that TdRL matches or outperforms handcrafted reward methods in policy training, with greater design simplicity and inherent support for multi-objective optimization. We argue that TdRL offers a novel perspective for representing task objectives, which could be helpful in addressing the reward design challenges in RL applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 132,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07926",
        "abs_url": "https://arxiv.org/abs/2511.07926",
        "pdf_url": "https://arxiv.org/pdf/2511.07926",
        "title": "CNN-Based Automated Parameter Extraction Framework for Modeling Memristive Devices",
        "authors": [
            "Akif Hamid",
            "Orchi Hassan"
        ],
        "comments": "",
        "subjects": "Emerging Technologies (cs.ET); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Resistive random access memory (RRAM) is a promising candidate for next-generation nonvolatile memory (NVM) and in-memory computing applications. Compact models are essential for analyzing the circuit and system-level performance of experimental RRAM devices. However, most existing RRAM compact models rely on multiple fitting parameters to reproduce the device I-V characteristics, and in most cases, as the parameters are not directly related to measurable quantities, their extraction requires extensive manual tuning, making the process time-consuming and limiting adaptability across different devices. This work presents an automated framework for extracting the fitting parameters of the widely used Stanford RRAM model directly from the device I-V characteristics. The framework employs a convolutional neural network (CNN) trained on a synthetic dataset to generate initial parameter estimates, which are then refined through three heuristic optimization blocks that minimize errors via adaptive binary search in the parameter space. We evaluated the framework using four key NVM metrics: set voltage, reset voltage, hysteresis loop area, and low resistance state (LRS) slope. Benchmarking against RRAM device characteristics derived from previously reported Stanford model fits, other analytical models, and experimental data shows that the framework achieves low error across diverse device characteristics, offering a fast, reliable, and robust solution for RRAM modeling.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 133,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07931",
        "abs_url": "https://arxiv.org/abs/2511.07931",
        "pdf_url": "https://arxiv.org/pdf/2511.07931",
        "title": "SpeechJudge: Towards Human-Level Judgment for Speech Naturalness",
        "authors": [
            "Xueyao Zhang",
            "Chaoren Wang",
            "Huan Liao",
            "Ziniu Li",
            "Yuancheng Wang",
            "Li Wang",
            "Dongya Jia",
            "Yuanzhe Chen",
            "Xiulin Li",
            "Zhuo Chen",
            "Zhizheng Wu"
        ],
        "comments": "Project Page: this https URL",
        "subjects": "Sound (cs.SD); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Aligning large generative models with human feedback is a critical challenge. In speech synthesis, this is particularly pronounced due to the lack of a large-scale human preference dataset, which hinders the development of models that truly align with human perception. To address this, we introduce SpeechJudge, a comprehensive suite comprising a dataset, a benchmark, and a reward model centered on naturalness--one of the most fundamental subjective metrics for speech synthesis. First, we present SpeechJudge-Data, a large-scale human feedback corpus of 99K speech pairs. The dataset is constructed using a diverse set of advanced zero-shot text-to-speech (TTS) models across diverse speech styles and multiple languages, with human annotations for both intelligibility and naturalness preference. From this, we establish SpeechJudge-Eval, a challenging benchmark for speech naturalness judgment. Our evaluation reveals that existing metrics and AudioLLMs struggle with this task; the leading model, Gemini-2.5-Flash, achieves less than 70% agreement with human judgment, highlighting a significant gap for improvement. To bridge this gap, we develop SpeechJudge-GRM, a generative reward model (GRM) based on Qwen2.5-Omni-7B. It is trained on SpeechJudge-Data via a two-stage post-training process: Supervised Fine-Tuning (SFT) with Chain-of-Thought rationales followed by Reinforcement Learning (RL) with GRPO on challenging cases. On the SpeechJudge-Eval benchmark, the proposed SpeechJudge-GRM demonstrates superior performance, achieving 77.2% accuracy (and 79.4% after inference-time scaling @10) compared to a classic Bradley-Terry reward model (72.7%). Furthermore, SpeechJudge-GRM can be also employed as a reward function during the post-training of speech generation models to facilitate their alignment with human preferences.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 134,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07941",
        "abs_url": "https://arxiv.org/abs/2511.07941",
        "pdf_url": "https://arxiv.org/pdf/2511.07941",
        "title": "Libra-MIL: Multimodal Prototypes Stereoscopic Infused with Task-specific Language Priors for Few-shot Whole Slide Image Classification",
        "authors": [
            "Zhenfeng Zhuang",
            "Fangyu Zhou",
            "Liansheng Wang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "While Large Language Models (LLMs) are emerging as a promising direction in computational pathology, the substantial computational cost of giga-pixel Whole Slide Images (WSIs) necessitates the use of Multi-Instance Learning (MIL) to enable effective modeling. A key challenge is that pathological tasks typically provide only bag-level labels, while instance-level descriptions generated by LLMs often suffer from bias due to a lack of fine-grained medical knowledge. To address this, we propose that constructing task-specific pathological entity prototypes is crucial for learning generalizable features and enhancing model interpretability. Furthermore, existing vision-language MIL methods often employ unidirectional guidance, limiting cross-modal synergy. In this paper, we introduce a novel approach, Multimodal Prototype-based Multi-Instance Learning, that promotes bidirectional interaction through a balanced information compression scheme. Specifically, we leverage a frozen LLM to generate task-specific pathological entity descriptions, which are learned as text prototypes. Concurrently, the vision branch learns instance-level prototypes to mitigate the model's reliance on redundant data. For the fusion stage, we employ the Stereoscopic Optimal Transport (SOT) algorithm, which is based on a similarity metric, thereby facilitating broader semantic alignment in a higher-dimensional space. We conduct few-shot classification and explainability experiments on three distinct cancer datasets, and the results demonstrate the superior generalization capabilities of our proposed method.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 135,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07942",
        "abs_url": "https://arxiv.org/abs/2511.07942",
        "pdf_url": "https://arxiv.org/pdf/2511.07942",
        "title": "Balance Equation-based Distributionally Robust Offline Imitation Learning",
        "authors": [
            "Rishabh Agrawal",
            "Yusuf Alvi",
            "Rahul Jain",
            "Ashutosh Nayyar"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Imitation Learning (IL) has proven highly effective for robotic and control tasks where manually designing reward functions or explicit controllers is infeasible. However, standard IL methods implicitly assume that the environment dynamics remain fixed between training and deployment. In practice, this assumption rarely holds where modeling inaccuracies, real-world parameter variations, and adversarial perturbations can all induce shifts in transition dynamics, leading to severe performance degradation. We address this challenge through Balance Equation-based Distributionally Robust Offline Imitation Learning, a framework that learns robust policies solely from expert demonstrations collected under nominal dynamics, without requiring further environment interaction. We formulate the problem as a distributionally robust optimization over an uncertainty set of transition models, seeking a policy that minimizes the imitation loss under the worst-case transition distribution. Importantly, we show that this robust objective can be reformulated entirely in terms of the nominal data distribution, enabling tractable offline learning. Empirical evaluations on continuous-control benchmarks demonstrate that our approach achieves superior robustness and generalization compared to state-of-the-art offline IL baselines, particularly under perturbed or shifted environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 136,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07950",
        "abs_url": "https://arxiv.org/abs/2511.07950",
        "pdf_url": "https://arxiv.org/pdf/2511.07950",
        "title": "USV Obstacles Detection and Tracking in Marine Environments",
        "authors": [
            "Yara AlaaEldin",
            "Enrico Simetti",
            "Francesca Odone"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
        "abstract": "Developing a robust and effective obstacle detection and tracking system for Unmanned Surface Vehicle (USV) at marine environments is a challenging task. Research efforts have been made in this area during the past years by GRAAL lab at the university of Genova that resulted in a methodology for detecting and tracking obstacles on the image plane and, then, locating them in the 3D LiDAR point cloud. In this work, we continue on the developed system by, firstly, evaluating its performance on recently published marine datasets. Then, we integrate the different blocks of the system on ROS platform where we could test it in real-time on synchronized LiDAR and camera data collected in various marine conditions available in the MIT marine datasets. We present a thorough experimental analysis of the results obtained using two approaches; one that uses sensor fusion between the camera and LiDAR to detect and track the obstacles and the other uses only the LiDAR point cloud for the detection and tracking. In the end, we propose a hybrid approach that merges the advantages of both approaches to build an informative obstacles map of the surrounding environment to the USV.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 137,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07975",
        "abs_url": "https://arxiv.org/abs/2511.07975",
        "pdf_url": "https://arxiv.org/pdf/2511.07975",
        "title": "Reliable and Private Utility Signaling for Data Markets",
        "authors": [
            "Li Peng",
            "Jiayao Zhang",
            "Yihang Wu",
            "Weiran Liu",
            "Jinfei Liu",
            "Zheng Yan",
            "Kui Ren",
            "Lei Zhang",
            "Lin Qu"
        ],
        "comments": "",
        "subjects": "Computer Science and Game Theory (cs.GT); Artificial Intelligence (cs.AI)",
        "abstract": "The explosive growth of data has highlighted its critical role in driving economic growth through data marketplaces, which enable extensive data sharing and access to high-quality datasets. To support effective trading, signaling mechanisms provide participants with information about data products before transactions, enabling informed decisions and facilitating trading. However, due to the inherent free-duplication nature of data, commonly practiced signaling methods face a dilemma between privacy and reliability, undermining the effectiveness of signals in guiding decision-making. To address this, this paper explores the benefits and develops a non-TCP-based construction for a desirable signaling mechanism that simultaneously ensures privacy and reliability. We begin by formally defining the desirable utility signaling mechanism and proving its ability to prevent suboptimal decisions for both participants and facilitate informed data trading. To design a protocol to realize its functionality, we propose leveraging maliciously secure multi-party computation (MPC) to ensure the privacy and robustness of signal computation and introduce an MPC-based hash verification scheme to ensure input reliability. In multi-seller scenarios requiring fair data valuation, we further explore the design and optimization of the MPC-based KNN-Shapley method with improved efficiency. Rigorous experiments demonstrate the efficiency and practicality of our approach.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 138,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07982",
        "abs_url": "https://arxiv.org/abs/2511.07982",
        "pdf_url": "https://arxiv.org/pdf/2511.07982",
        "title": "NOTAM-Evolve: A Knowledge-Guided Self-Evolving Optimization Framework with LLMs for NOTAM Interpretation",
        "authors": [
            "Maoqi Liu",
            "Quan Fang",
            "Yuhao Wu",
            "Can Zhao",
            "Yang Yang",
            "Kaiquan Cai"
        ],
        "comments": "Accepted to AAAI 2026",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Accurate interpretation of Notices to Airmen (NOTAMs) is critical for aviation safety, yet their condensed and cryptic language poses significant challenges to both manual and automated processing. Existing automated systems are typically limited to shallow parsing, failing to extract the actionable intelligence needed for operational decisions. We formalize the complete interpretation task as deep parsing, a dual-reasoning challenge requiring both dynamic knowledge grounding (linking the NOTAM to evolving real-world aeronautical data) and schema-based inference (applying static domain rules to deduce operational status). To tackle this challenge, we propose NOTAM-Evolve, a self-evolving framework that enables a large language model (LLM) to autonomously master complex NOTAM interpretation. Leveraging a knowledge graph-enhanced retrieval module for data grounding, the framework introduces a closed-loop learning process where the LLM progressively improves from its own outputs, minimizing the need for extensive human-annotated reasoning traces. In conjunction with this framework, we introduce a new benchmark dataset of 10,000 expert-annotated NOTAMs. Our experiments demonstrate that NOTAM-Evolve achieves a 30.4% absolute accuracy improvement over the base LLM, establishing a new state of the art on the task of structured NOTAM interpretation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 139,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07989",
        "abs_url": "https://arxiv.org/abs/2511.07989",
        "pdf_url": "https://arxiv.org/pdf/2511.07989",
        "title": "State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?",
        "authors": [
            "Taja Kuzman Pungerek",
            "Peter Rupnik",
            "Ivan Porupski",
            "Vuk Dini",
            "Nikola Ljubei"
        ],
        "comments": "16 pages; 4 figures; 3 tables. Submitted to the LREC 2026 conference",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Until recently, fine-tuned BERT-like models provided state-of-the-art performance on text classification tasks. With the rise of instruction-tuned decoder-only models, commonly known as large language models (LLMs), the field has increasingly moved toward zero-shot and few-shot prompting. However, the performance of LLMs on text classification, particularly on less-resourced languages, remains under-explored. In this paper, we evaluate the performance of current language models on text classification tasks across several South Slavic languages. We compare openly available fine-tuned BERT-like models with a selection of open-source and closed-source LLMs across three tasks in three domains: sentiment classification in parliamentary speeches, topic classification in news articles and parliamentary speeches, and genre identification in web texts. Our results show that LLMs demonstrate strong zero-shot performance, often matching or surpassing fine-tuned BERT-like models. Moreover, when used in a zero-shot setup, LLMs perform comparably in South Slavic languages and English. However, we also point out key drawbacks of LLMs, including less predictable outputs, significantly slower inference, and higher computational costs. Due to these limitations, fine-tuned BERT-like models remain a more practical choice for large-scale automatic text annotation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 140,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.07998",
        "abs_url": "https://arxiv.org/abs/2511.07998",
        "pdf_url": "https://arxiv.org/pdf/2511.07998",
        "title": "Self-Correction Distillation for Structured Data Question Answering",
        "authors": [
            "Yushan Zhu",
            "Wen Zhang",
            "Long Jin",
            "Mengshu Sun",
            "Ling Zhong",
            "Zhiqiang Liu",
            "Juan Li",
            "Lei Liang",
            "Chong Long",
            "Chao Deng",
            "Junlan Feng"
        ],
        "comments": "Accepted to AAAI 2026",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Structured data question answering (QA), including table QA, Knowledge Graph (KG) QA, and temporal KG QA, is a pivotal research area. Advances in large language models (LLMs) have driven significant progress in unified structural QA frameworks like TrustUQA. However, these frameworks face challenges when applied to small-scale LLMs since small-scale LLMs are prone to errors in generating structured queries. To improve the structured data QA ability of small-scale LLMs, we propose a self-correction distillation (SCD) method. In SCD, an error prompt mechanism (EPM) is designed to detect errors and provide customized error messages during inference, and a two-stage distillation strategy is designed to transfer large-scale LLMs' query-generation and error-correction capabilities to small-scale LLM. Experiments across 5 benchmarks with 3 structured data types demonstrate that our SCD achieves the best performance and superior generalization on small-scale LLM (8B) compared to other distillation methods, and closely approaches the performance of GPT4 on some datasets. Furthermore, large-scale LLMs equipped with EPM surpass the state-of-the-art results on most datasets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 141,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08012",
        "abs_url": "https://arxiv.org/abs/2511.08012",
        "pdf_url": "https://arxiv.org/pdf/2511.08012",
        "title": "DOA Estimation with Lightweight Network on LLM-Aided Simulated Acoustic Scenes",
        "authors": [
            "Haowen Li",
            "Zhengding Luo",
            "Dongyuan Shi",
            "Boxiang Wang",
            "Junwei Ji",
            "Ziyi Yang",
            "Woon-Seng Gan"
        ],
        "comments": "",
        "subjects": "Sound (cs.SD); Artificial Intelligence (cs.AI)",
        "abstract": "Direction-of-Arrival (DOA) estimation is critical in spatial audio and acoustic signal processing, with wide-ranging applications in real-world. Most existing DOA models are trained on synthetic data by convolving clean speech with room impulse responses (RIRs), which limits their generalizability due to constrained acoustic diversity. In this paper, we revisit DOA estimation using a recently introduced dataset constructed with the assistance of large language models (LLMs), which provides more realistic and diverse spatial audio scenes. We benchmark several representative neural-based DOA methods on this dataset and propose LightDOA, a lightweight DOA estimation model based on depthwise separable convolutions, specifically designed for mutil-channel input in varying environments. Experimental results show that LightDOA achieves satisfactory accuracy and robustness across various acoustic scenes while maintaining low computational complexity. This study not only highlights the potential of spatial audio synthesized with the assistance of LLMs in advancing robust and efficient DOA estimation research, but also highlights LightDOA as efficient solution for resource-constrained applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 142,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08016",
        "abs_url": "https://arxiv.org/abs/2511.08016",
        "pdf_url": "https://arxiv.org/pdf/2511.08016",
        "title": "AVOID-JACK: Avoidance of Jackknifing for Swarms of Long Heavy Articulated Vehicles",
        "authors": [
            "Adrian Schnnagel",
            "Michael Dub",
            "Christoph Steup",
            "Felix Keppler",
            "Sanaz Mostaghim"
        ],
        "comments": "6+1 pages, 9 figures, accepted for publication in IEEE MRS 2025",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)",
        "abstract": "This paper presents a novel approach to avoiding jackknifing and mutual collisions in Heavy Articulated Vehicles (HAVs) by leveraging decentralized swarm intelligence. In contrast to typical swarm robotics research, our robots are elongated and exhibit complex kinematics, introducing unique challenges. Despite its relevance to real-world applications such as logistics automation, remote mining, airport baggage transport, and agricultural operations, this problem has not been addressed in the existing literature. To tackle this new class of swarm robotics problems, we propose a purely reaction-based, decentralized swarm intelligence strategy tailored to automate elongated, articulated vehicles. The method presented in this paper prioritizes jackknifing avoidance and establishes a foundation for mutual collision avoidance. We validate our approach through extensive simulation experiments and provide a comprehensive analysis of its performance. For the experiments with a single HAV, we observe that for 99.8% jackknifing was successfully avoided and that 86.7% and 83.4% reach their first and second goals, respectively. With two HAVs interacting, we observe 98.9%, 79.4%, and 65.1%, respectively, while 99.7% of the HAVs do not experience mutual collisions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 143,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08046",
        "abs_url": "https://arxiv.org/abs/2511.08046",
        "pdf_url": "https://arxiv.org/pdf/2511.08046",
        "title": "ProSona: Prompt-Guided Personalization for Multi-Expert Medical Image Segmentation",
        "authors": [
            "Aya Elgebaly",
            "Nikolaos Delopoulos",
            "Juliane Hrner-Rieber",
            "Carolin Rippke",
            "Sebastian Klter",
            "Luca Boldrini",
            "Lorenzo Placidi",
            "Riccardo Dal Bello",
            "Nicolaus Andratschke",
            "Michael Baumgartl",
            "Claus Belka",
            "Christopher Kurz",
            "Guillaume Landry",
            "Shadi Albarqouni"
        ],
        "comments": "5 pages, 5 figures. Submitted to IEEE International Symposium on Biomedical Imaging (ISBI) 2026",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Automated medical image segmentation suffers from high inter-observer variability, particularly in tasks such as lung nodule delineation, where experts often disagree. Existing approaches either collapse this variability into a consensus mask or rely on separate model branches for each annotator. We introduce ProSona, a two-stage framework that learns a continuous latent space of annotation styles, enabling controllable personalization via natural language prompts. A probabilistic U-Net backbone captures diverse expert hypotheses, while a prompt-guided projection mechanism navigates this latent space to generate personalized segmentations. A multi-level contrastive objective aligns textual and visual representations, promoting disentangled and interpretable expert styles. Across the LIDC-IDRI lung nodule and multi-institutional prostate MRI datasets, ProSona reduces the Generalized Energy Distance by 17% and improves mean Dice by more than one point compared with DPersona. These results demonstrate that natural-language prompts can provide flexible, accurate, and interpretable control over personalized medical image segmentation. Our implementation is available online 1 .",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 144,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08071",
        "abs_url": "https://arxiv.org/abs/2511.08071",
        "pdf_url": "https://arxiv.org/pdf/2511.08071",
        "title": "Radar-APLANC: Unsupervised Radar-based Heartbeat Sensing via Augmented Pseudo-Label and Noise Contrast",
        "authors": [
            "Ying Wang",
            "Zhaodong Sun",
            "Xu Cheng",
            "Zuxian He",
            "Xiaobai Li"
        ],
        "comments": "Accepted by AAAI 2026",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Signal Processing (eess.SP)",
        "abstract": "Frequency Modulated Continuous Wave (FMCW) radars can measure subtle chest wall oscillations to enable non-contact heartbeat sensing. However, traditional radar-based heartbeat sensing methods face performance degradation due to noise. Learning-based radar methods achieve better noise robustness but require costly labeled signals for supervised training. To overcome these limitations, we propose the first unsupervised framework for radar-based heartbeat sensing via Augmented Pseudo-Label and Noise Contrast (Radar-APLANC). We propose to use both the heartbeat range and noise range within the radar range matrix to construct the positive and negative samples, respectively, for improved noise robustness. Our Noise-Contrastive Triplet (NCT) loss only utilizes positive samples, negative samples, and pseudo-label signals generated by the traditional radar method, thereby avoiding dependence on expensive ground-truth physiological signals. We further design a pseudo-label augmentation approach featuring adaptive noise-aware label selection to improve pseudo-label signal quality. Extensive experiments on the Equipleth dataset and our collected radar dataset demonstrate that our unsupervised method achieves performance comparable to state-of-the-art supervised methods. Our code, dataset, and supplementary materials can be accessed from this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 145,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08077",
        "abs_url": "https://arxiv.org/abs/2511.08077",
        "pdf_url": "https://arxiv.org/pdf/2511.08077",
        "title": "An Integrated Fusion Framework for Ensemble Learning Leveraging Gradient Boosting and Fuzzy Rule-Based Models",
        "authors": [
            "Jinbo Li",
            "Peng Liu",
            "Long Chen",
            "Witold Pedrycz",
            "Weiping Ding"
        ],
        "comments": "15 pages, 6 figures. IEEE Transactions on Artificial Intelligence (2024)",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "The integration of different learning paradigms has long been a focus of machine learning research, aimed at overcoming the inherent limitations of individual methods. Fuzzy rule-based models excel in interpretability and have seen widespread application across diverse fields. However, they face challenges such as complex design specifications and scalability issues with large datasets. The fusion of different techniques and strategies, particularly Gradient Boosting, with Fuzzy Rule-Based Models offers a robust solution to these challenges. This paper proposes an Integrated Fusion Framework that merges the strengths of both paradigms to enhance model performance and interpretability. At each iteration, a Fuzzy Rule-Based Model is constructed and controlled by a dynamic factor to optimize its contribution to the overall ensemble. This control factor serves multiple purposes: it prevents model dominance, encourages diversity, acts as a regularization parameter, and provides a mechanism for dynamic tuning based on model performance, thus mitigating the risk of overfitting. Additionally, the framework incorporates a sample-based correction mechanism that allows for adaptive adjustments based on feedback from a validation set. Experimental results substantiate the efficacy of the presented gradient boosting framework for fuzzy rule-based models, demonstrating performance enhancement, especially in terms of mitigating overfitting and complexity typically associated with many rules. By leveraging an optimal factor to govern the contribution of each model, the framework improves performance, maintains interpretability, and simplifies the maintenance and update of the models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 146,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08078",
        "abs_url": "https://arxiv.org/abs/2511.08078",
        "pdf_url": "https://arxiv.org/pdf/2511.08078",
        "title": "Constrained and Robust Policy Synthesis with Satisfiability-Modulo-Probabilistic-Model-Checking",
        "authors": [
            "Linus Heck",
            "Filip Mack",
            "Milan eka",
            "Sebastian Junges"
        ],
        "comments": "Accepted to AAAI 2026",
        "subjects": "Logic in Computer Science (cs.LO); Artificial Intelligence (cs.AI)",
        "abstract": "The ability to compute reward-optimal policies for given and known finite Markov decision processes (MDPs) underpins a variety of applications across planning, controller synthesis, and verification. However, we often want policies (1) to be robust, i.e., they perform well on perturbations of the MDP and (2) to satisfy additional structural constraints regarding, e.g., their representation or implementation cost. Computing such robust and constrained policies is indeed computationally more challenging. This paper contributes the first approach to effectively compute robust policies subject to arbitrary structural constraints using a flexible and efficient framework. We achieve flexibility by allowing to express our constraints in a first-order theory over a set of MDPs, while the root for our efficiency lies in the tight integration of satisfiability solvers to handle the combinatorial nature of the problem and probabilistic model checking algorithms to handle the analysis of MDPs. Experiments on a few hundred benchmarks demonstrate the feasibility for constrained and robust policy synthesis and the competitiveness with state-of-the-art methods for various fragments of the problem.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 147,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08080",
        "abs_url": "https://arxiv.org/abs/2511.08080",
        "pdf_url": "https://arxiv.org/pdf/2511.08080",
        "title": "Hierarchical Structure-Property Alignment for Data-Efficient Molecular Generation and Editing",
        "authors": [
            "Ziyu Fan",
            "Zhijian Huang",
            "Yahan Li",
            "Xiaowen Hu",
            "Siyuan Shen",
            "Yunliang Wang",
            "Zeyu Zhong",
            "Shuhong Liu",
            "Shuning Yang",
            "Shangqian Wu",
            "Min Wu",
            "Lei Deng"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Property-constrained molecular generation and editing are crucial in AI-driven drug discovery but remain hindered by two factors: (i) capturing the complex relationships between molecular structures and multiple properties remains challenging, and (ii) the narrow coverage and incomplete annotations of molecular properties weaken the effectiveness of property-based models. To tackle these limitations, we propose HSPAG, a data-efficient framework featuring hierarchical structure-property alignment. By treating SMILES and molecular properties as complementary modalities, the model learns their relationships at atom, substructure, and whole-molecule levels. Moreover, we select representative samples through scaffold clustering and hard samples via an auxiliary variational auto-encoder (VAE), substantially reducing the required pre-training data. In addition, we incorporate a property relevance-aware masking mechanism and diversified perturbation strategies to enhance generation quality under sparse annotations. Experiments demonstrate that HSPAG captures fine-grained structure-property relationships and supports controllable generation under multiple property constraints. Two real-world case studies further validate the editing capabilities of HSPAG.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 148,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08085",
        "abs_url": "https://arxiv.org/abs/2511.08085",
        "pdf_url": "https://arxiv.org/pdf/2511.08085",
        "title": "BARD10: A New Benchmark Reveals Significance of Bangla Stop-Words in Authorship Attribution",
        "authors": [
            "Abdullah Muhammad Moosa",
            "Nusrat Sultana",
            "Mahdi Muhammad Moosa",
            "Md. Miraiz Hossain"
        ],
        "comments": "28 pages, 6 Figures",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "This research presents a comprehensive investigation into Bangla authorship attribution, introducing a new balanced benchmark corpus BARD10 (Bangla Authorship Recognition Dataset of 10 authors) and systematically analyzing the impact of stop-word removal across classical and deep learning models to uncover the stylistic significance of Bangla stop-words. BARD10 is a curated corpus of Bangla blog and opinion prose from ten contemporary authors, alongside the methodical assessment of four representative classifiers: SVM (Support Vector Machine), Bangla BERT (Bidirectional Encoder Representations from Transformers), XGBoost, and a MLP (Multilayer Perception), utilizing uniform preprocessing on both BARD10 and the benchmark corpora BAAD16 (Bangla Authorship Attribution Dataset of 16 authors). In all datasets, the classical TF-IDF + SVM baseline outperformed, attaining a macro-F1 score of 0.997 on BAAD16 and 0.921 on BARD10, while Bangla BERT lagged by as much as five points. This study reveals that BARD10 authors are highly sensitive to stop-word pruning, while BAAD16 authors remain comparatively robust highlighting genre-dependent reliance on stop-word signatures. Error analysis revealed that high frequency components transmit authorial signatures that are diminished or reduced by transformer models. Three insights are identified: Bangla stop-words serve as essential stylistic indicators; finely calibrated ML models prove effective within short-text limitations; and BARD10 connects formal literature with contemporary web dialogue, offering a reproducible benchmark for future long-context or domain-adapted transformers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 149,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08086",
        "abs_url": "https://arxiv.org/abs/2511.08086",
        "pdf_url": "https://arxiv.org/pdf/2511.08086",
        "title": "Dynamic Sparsity: Challenging Common Sparsity Assumptions for Learning World Models in Robotic Reinforcement Learning Benchmarks",
        "authors": [
            "Muthukumar Pandaram",
            "Jakob Hollenstein",
            "David Drexel",
            "Samuele Tosatto",
            "Antonio Rodrguez-Snchez",
            "Justus Piater"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Robotics (cs.RO)",
        "abstract": "The use of learned dynamics models, also known as world models, can improve the sample efficiency of reinforcement learning. Recent work suggests that the underlying causal graphs of such dynamics models are sparsely connected, with each of the future state variables depending only on a small subset of the current state variables, and that learning may therefore benefit from sparsity priors. Similarly, temporal sparsity, i.e. sparsely and abruptly changing local dynamics, has also been proposed as a useful inductive bias. In this work, we critically examine these assumptions by analyzing ground-truth dynamics from a set of robotic reinforcement learning environments in the MuJoCo Playground benchmark suite, aiming to determine whether the proposed notions of state and temporal sparsity actually tend to hold in typical reinforcement learning tasks. We study (i) whether the causal graphs of environment dynamics are sparse, (ii) whether such sparsity is state-dependent, and (iii) whether local system dynamics change sparsely. Our results indicate that global sparsity is rare, but instead the tasks show local, state-dependent sparsity in their dynamics and this sparsity exhibits distinct structures, appearing in temporally localized clusters (e.g., during contact events) and affecting specific subsets of state dimensions. These findings challenge common sparsity prior assumptions in dynamics learning, emphasizing the need for grounded inductive biases that reflect the state-dependent sparsity structure of real-world dynamics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 150,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08090",
        "abs_url": "https://arxiv.org/abs/2511.08090",
        "pdf_url": "https://arxiv.org/pdf/2511.08090",
        "title": "StableMorph: High-Quality Face Morph Generation with Stable Diffusion",
        "authors": [
            "Wassim Kabbani",
            "Kiran Raja",
            "Raghavendra Ramachandra",
            "Christoph Busch"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Face morphing attacks threaten the integrity of biometric identity systems by enabling multiple individuals to share a single identity. To develop and evaluate effective morphing attack detection (MAD) systems, we need access to high-quality, realistic morphed images that reflect the challenges posed in real-world scenarios. However, existing morph generation methods often produce images that are blurry, riddled with artifacts, or poorly constructed making them easy to detect and not representative of the most dangerous attacks. In this work, we introduce StableMorph, a novel approach that generates highly realistic, artifact-free morphed face images using modern diffusion-based image synthesis. Unlike prior methods, StableMorph produces full-head images with sharp details, avoids common visual flaws, and offers unmatched control over visual attributes. Through extensive evaluation, we show that StableMorph images not only rival or exceed the quality of genuine face images but also maintain a strong ability to fool face recognition systems posing a greater challenge to existing MAD solutions and setting a new standard for morph quality in research and operational testing. StableMorph improves the evaluation of biometric security by creating more realistic and effective attacks and supports the development of more robust detection systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 151,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08098",
        "abs_url": "https://arxiv.org/abs/2511.08098",
        "pdf_url": "https://arxiv.org/pdf/2511.08098",
        "title": "PerspAct: Enhancing LLM Situated Collaboration Skills through Perspective Taking and Active Vision",
        "authors": [
            "Sabrina Patania",
            "Luca Annese",
            "Anita Pellegrini",
            "Silvia Serino",
            "Anna Lambiase",
            "Luca Pallonetto",
            "Silvia Rossi",
            "Simone Colombani",
            "Tom Foulsham",
            "Azzurra Ruggeri",
            "Dimitri Ognibene"
        ],
        "comments": "Accepted at IAS19",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Human-Computer Interaction (cs.HC)",
        "abstract": "Recent advances in Large Language Models (LLMs) and multimodal foundation models have significantly broadened their application in robotics and collaborative systems. However, effective multi-agent interaction necessitates robust perspective-taking capabilities, enabling models to interpret both physical and epistemic viewpoints. Current training paradigms often neglect these interactive contexts, resulting in challenges when models must reason about the subjectivity of individual perspectives or navigate environments with multiple observers. This study evaluates whether explicitly incorporating diverse points of view using the ReAct framework, an approach that integrates reasoning and acting, can enhance an LLM's ability to understand and ground the demands of other agents. We extend the classic Director task by introducing active visual exploration across a suite of seven scenarios of increasing perspective-taking complexity. These scenarios are designed to challenge the agent's capacity to resolve referential ambiguity based on visual access and interaction, under varying state representations and prompting strategies, including ReAct-style reasoning. Our results demonstrate that explicit perspective cues, combined with active exploration strategies, significantly improve the model's interpretative accuracy and collaborative effectiveness. These findings highlight the potential of integrating active perception with perspective-taking mechanisms in advancing LLMs' application in robotics and multi-agent systems, setting a foundation for future research into adaptive and context-aware AI systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 152,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08120",
        "abs_url": "https://arxiv.org/abs/2511.08120",
        "pdf_url": "https://arxiv.org/pdf/2511.08120",
        "title": "A robust methodology for long-term sustainability evaluation of Machine Learning models",
        "authors": [
            "Jorge Paz-Ruza",
            "Joo Gama",
            "Amparo Alonso-Betanzos",
            "Bertha Guijarro-Berdias"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Sustainability and efficiency have become essential considerations in the development and deployment of Artificial Intelligence systems, yet existing regulatory and reporting practices lack standardized, model-agnostic evaluation protocols. Current assessments often measure only short-term experimental resource usage and disproportionately emphasize batch learning settings, failing to reflect real-world, long-term AI lifecycles. In this work, we propose a comprehensive evaluation protocol for assessing the long-term sustainability of ML models, applicable to both batch and streaming learning scenarios. Through experiments on diverse classification tasks using a range of model types, we demonstrate that traditional static train-test evaluations do not reliably capture sustainability under evolving data and repeated model updates. Our results show that long-term sustainability varies significantly across models, and in many cases, higher environmental cost yields little performance benefit.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 153,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08136",
        "abs_url": "https://arxiv.org/abs/2511.08136",
        "pdf_url": "https://arxiv.org/pdf/2511.08136",
        "title": "SafeMIL: Learning Offline Safe Imitation Policy from Non-Preferred Trajectories",
        "authors": [
            "Returaj Burnwal",
            "Nirav Pravinbhai Bhatt",
            "Balaraman Ravindran"
        ],
        "comments": "18 pages, AAAI 2026",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "In this work, we study the problem of offline safe imitation learning (IL). In many real-world settings, online interactions can be risky, and accurately specifying the reward and the safety cost information at each timestep can be difficult. However, it is often feasible to collect trajectories reflecting undesirable or risky behavior, implicitly conveying the behavior the agent should avoid. We refer to these trajectories as non-preferred trajectories. Unlike standard IL, which aims to mimic demonstrations, our agent must also learn to avoid risky behavior using non-preferred trajectories. In this paper, we propose a novel approach, SafeMIL, to learn a parameterized cost that predicts if the state-action pair is risky via \\textit{Multiple Instance Learning}. The learned cost is then used to avoid non-preferred behaviors, resulting in a policy that prioritizes safety. We empirically demonstrate that our approach can learn a safer policy that satisfies cost constraints without degrading the reward performance, thereby outperforming several baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 154,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08143",
        "abs_url": "https://arxiv.org/abs/2511.08143",
        "pdf_url": "https://arxiv.org/pdf/2511.08143",
        "title": "Relation as a Prior: A Novel Paradigm for LLM-based Document-level Relation Extraction",
        "authors": [
            "Qiankun Pi",
            "Yepeng Sun",
            "Jicang Lu",
            "Qinlong Fan",
            "Ningbo Huang",
            "Shiyu Wang"
        ],
        "comments": "17 pages",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Large Language Models (LLMs) have demonstrated their remarkable capabilities in document understanding. However, recent research reveals that LLMs still exhibit performance gaps in Document-level Relation Extraction (DocRE) as requiring fine-grained comprehension. The commonly adopted \"extract entities then predict relations\" paradigm in LLM-based methods leads to these gaps due to two main reasons: (1) Numerous unrelated entity pairs introduce noise and interfere with the relation prediction for truly related entity pairs. (2) Although LLMs have identified semantic associations between entities, relation labels beyond the predefined set are still treated as prediction errors. To address these challenges, we propose a novel Relation as a Prior (RelPrior) paradigm for LLM-based DocRE. For challenge (1), RelPrior utilizes binary relation as a prior to extract and determine whether two entities are correlated, thereby filtering out irrelevant entity pairs and reducing prediction noise. For challenge (2), RelPrior utilizes predefined relation as a prior to match entities for triples extraction instead of directly predicting relation. Thus, it avoids misjudgment caused by strict predefined relation labeling. Extensive experiments on two benchmarks demonstrate that RelPrior achieves state-of-the-art performance, surpassing existing LLM-based methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 155,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08147",
        "abs_url": "https://arxiv.org/abs/2511.08147",
        "pdf_url": "https://arxiv.org/pdf/2511.08147",
        "title": "ProbSelect: Stochastic Client Selection for GPU-Accelerated Compute Devices in the 3D Continuum",
        "authors": [
            "Andrija Stanisic",
            "Stefan Nastic"
        ],
        "comments": "",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Artificial Intelligence (cs.AI)",
        "abstract": "Integration of edge, cloud and space devices into a unified 3D continuum imposes significant challenges for client selection in federated learning systems. Traditional approaches rely on continuous monitoring and historical data collection, which becomes impractical in dynamic environments where satellites and mobile devices frequently change operational conditions. Furthermore, existing solutions primarily consider CPU-based computation, failing to capture complex characteristics of GPU-accelerated training that is prevalent across the 3D continuum. This paper introduces ProbSelect, a novel approach utilizing analytical modeling and probabilistic forecasting for client selection on GPU-accelerated devices, without requiring historical data or continuous monitoring. We model client selection within user-defined SLOs. Extensive evaluation across diverse GPU architectures and workloads demonstrates that ProbSelect improves SLO compliance by 13.77% on average while achieving 72.5% computational waste reduction compared to baseline approaches.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 156,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08174",
        "abs_url": "https://arxiv.org/abs/2511.08174",
        "pdf_url": "https://arxiv.org/pdf/2511.08174",
        "title": "Deep (Predictive) Discounted Counterfactual Regret Minimization",
        "authors": [
            "Hang Xu",
            "Kai Li",
            "Haobo Fu",
            "Qiang Fu",
            "Junliang Xing",
            "Jian Cheng"
        ],
        "comments": "Accepted to 40th AAAI Conference on Artificial Intelligence (AAAI 2026)",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT)",
        "abstract": "Counterfactual regret minimization (CFR) is a family of algorithms for effectively solving imperfect-information games. To enhance CFR's applicability in large games, researchers use neural networks to approximate its behavior. However, existing methods are mainly based on vanilla CFR and struggle to effectively integrate more advanced CFR variants. In this work, we propose an efficient model-free neural CFR algorithm, overcoming the limitations of existing methods in approximating advanced CFR variants. At each iteration, it collects variance-reduced sampled advantages based on a value network, fits cumulative advantages by bootstrapping, and applies discounting and clipping operations to simulate the update mechanisms of advanced CFR variants. Experimental results show that, compared with model-free neural algorithms, it exhibits faster convergence in typical imperfect-information games and demonstrates stronger adversarial performance in a large poker game.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 157,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08181",
        "abs_url": "https://arxiv.org/abs/2511.08181",
        "pdf_url": "https://arxiv.org/pdf/2511.08181",
        "title": "MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System",
        "authors": [
            "Seung Hwan Cho",
            "Yujin Yang",
            "Danik Baeck",
            "Minjoo Kim",
            "Young-Min Kim",
            "Heejung Lee",
            "Sangjin Park"
        ],
        "comments": "13 pages, 2 figures, Accepted at RDGENAI at CIKM 2025 workshop",
        "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI)",
        "abstract": "Recommender systems (RS) are currently being studied to mitigate limitations during cold-start conditions by leveraging modality information or introducing Agent concepts based on the exceptional reasoning capabilities of Large Language Models (LLMs). Meanwhile, food and beverage recommender systems have traditionally used knowledge graph and ontology concepts due to the domain's unique data attributes and relationship characteristics. On this background, we propose MARC, a multimodal and multi-task cocktail recommender system based on Agentic Retrieval-Augmented Generation (RAG) utilizing graph database under cold-start conditions. The proposed system generates high-quality, contextually appropriate answers through two core processes: a task recognition router and a reflection process. The graph database was constructed by processing cocktail data from Kaggle, and its effectiveness was evaluated using 200 manually crafted questions. The evaluation used both LLM-as-a-judge and human evaluation to demonstrate that answers generated via the graph database outperformed those from a simple vector database in terms of quality. The code is available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 158,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08207",
        "abs_url": "https://arxiv.org/abs/2511.08207",
        "pdf_url": "https://arxiv.org/pdf/2511.08207",
        "title": "FedPoP: Federated Learning Meets Proof of Participation",
        "authors": [
            "Devri ler",
            "Elina van Kempen",
            "Seoyeon Hwang",
            "Nikolaos Laoutaris"
        ],
        "comments": "This version is currently under review",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Federated learning (FL) offers privacy preserving, distributed machine learning, allowing clients to contribute to a global model without revealing their local data. As models increasingly serve as monetizable digital assets, the ability to prove participation in their training becomes essential for establishing ownership. In this paper, we address this emerging need by introducing FedPoP, a novel FL framework that allows nonlinkable proof of participation while preserving client anonymity and privacy without requiring either extensive computations or a public ledger. FedPoP is designed to seamlessly integrate with existing secure aggregation protocols to ensure compatibility with real-world FL deployments. We provide a proof of concept implementation and an empirical evaluation under realistic client dropouts. In our prototype, FedPoP introduces 0.97 seconds of per-round overhead atop securely aggregated FL and enables a client to prove its participation/contribution to a model held by a third party in 0.0612 seconds. These results indicate FedPoP is practical for real-world deployments that require auditable participation without sacrificing privacy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 159,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08225",
        "abs_url": "https://arxiv.org/abs/2511.08225",
        "pdf_url": "https://arxiv.org/pdf/2511.08225",
        "title": "Benchmarking Educational LLMs with Analytics: A Case Study on Gender Bias in Feedback",
        "authors": [
            "Yishan Du",
            "Conrad Borchers",
            "Mutlu Cukurova"
        ],
        "comments": "21 pages, 7 figures",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Human-Computer Interaction (cs.HC)",
        "abstract": "As teachers increasingly turn to GenAI in their educational practice, we need robust methods to benchmark large language models (LLMs) for pedagogical purposes. This article presents an embedding-based benchmarking framework to detect bias in LLMs in the context of formative feedback. Using 600 authentic student essays from the AES 2.0 corpus, we constructed controlled counterfactuals along two dimensions: (i) implicit cues via lexicon-based swaps of gendered terms within essays, and (ii) explicit cues via gendered author background in the prompt. We investigated six representative LLMs (i.e. GPT-5 mini, GPT-4o mini, DeepSeek-R1, DeepSeek-R1-Qwen, Gemini 2.5 Pro, Llama-3-8B). We first quantified the response divergence with cosine and Euclidean distances over sentence embeddings, then assessed significance via permutation tests, and finally, visualised structure using dimensionality reduction. In all models, implicit manipulations reliably induced larger semantic shifts for male-female counterfactuals than for female-male. Only the GPT and Llama models showed sensitivity to explicit gender cues. These findings show that even state-of-the-art LLMs exhibit asymmetric semantic responses to gender substitutions, suggesting persistent gender biases in feedback they provide learners. Qualitative analyses further revealed consistent linguistic differences (e.g., more autonomy-supportive feedback under male cues vs. more controlling feedback under female cues). We discuss implications for fairness auditing of pedagogical GenAI, propose reporting standards for counterfactual evaluation in learning analytics, and outline practical guidance for prompt design and deployment to safeguard equitable feedback.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 160,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08231",
        "abs_url": "https://arxiv.org/abs/2511.08231",
        "pdf_url": "https://arxiv.org/pdf/2511.08231",
        "title": "Real-Time Performance Analysis of Multi-Fidelity Residual Physics-Informed Neural Process-Based State Estimation for Robotic Systems",
        "authors": [
            "Devin Hunter",
            "Chinwendu Enyioha"
        ],
        "comments": "8 pages, 5 figures",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Various neural network architectures are used in many of the state-of-the-art approaches for real-time nonlinear state estimation. With the ever-increasing incorporation of these data-driven models into the estimation domain, model predictions with reliable margins of error are a requirement -- especially for safety-critical applications. This paper discusses the application of a novel real-time, data-driven estimation approach based on the multi-fidelity residual physics-informed neural process (MFR-PINP) toward the real-time state estimation of a robotic system. Specifically, we address the model-mismatch issue of selecting an accurate kinematic model by tasking the MFR-PINP to also learn the residuals between simple, low-fidelity predictions and complex, high-fidelity ground-truth dynamics. To account for model uncertainty present in a physical implementation, robust uncertainty guarantees from the split conformal (SC) prediction framework are modeled in the training and inference paradigms. We provide implementation details of our MFR-PINP-based estimator for a hybrid online learning setting to validate our model's usage in real-time applications. Experimental results of our approach's performance in comparison to the state-of-the-art variants of the Kalman filter (i.e. unscented Kalman filter and deep Kalman filter) in estimation scenarios showed promising results for the MFR-PINP model as a viable option in real-time estimation tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 161,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08240",
        "abs_url": "https://arxiv.org/abs/2511.08240",
        "pdf_url": "https://arxiv.org/pdf/2511.08240",
        "title": "Hierarchical Direction Perception via Atomic Dot-Product Operators for Rotation-Invariant Point Clouds Learning",
        "authors": [
            "Chenyu Hu",
            "Xiaotong Li",
            "Hao Zhu",
            "Biao Hou"
        ],
        "comments": "Accepted to AAAI 2026. Code is available at: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Point cloud processing has become a cornerstone technology in many 3D vision tasks. However, arbitrary rotations introduce variations in point cloud orientations, posing a long-standing challenge for effective representation learning. The core of this issue is the disruption of the point cloud's intrinsic directional characteristics caused by rotational perturbations. Recent methods attempt to implicitly model rotational equivariance and invariance, preserving directional information and propagating it into deep semantic spaces. Yet, they often fall short of fully exploiting the multiscale directional nature of point clouds to enhance feature representations. To address this, we propose the Direction-Perceptive Vector Network (DiPVNet). At its core is an atomic dot-product operator that simultaneously encodes directional selectivity and rotation invariance--endowing the network with both rotational symmetry modeling and adaptive directional perception. At the local level, we introduce a Learnable Local Dot-Product (L2DP) Operator, which enables interactions between a center point and its neighbors to adaptively capture the non-uniform local structures of point clouds. At the global level, we leverage generalized harmonic analysis to prove that the dot-product between point clouds and spherical sampling vectors is equivalent to a direction-aware spherical Fourier transform (DASFT). This leads to the construction of a global directional response spectrum for modeling holistic directional structures. We rigorously prove the rotation invariance of both operators. Extensive experiments on challenging scenarios involving noise and large-angle rotations demonstrate that DiPVNet achieves state-of-the-art performance on point cloud classification and segmentation tasks. Our code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 162,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08263",
        "abs_url": "https://arxiv.org/abs/2511.08263",
        "pdf_url": "https://arxiv.org/pdf/2511.08263",
        "title": "ImagebindDC: Compressing Multi-modal Data with Imagebind-based Condensation",
        "authors": [
            "Yue Min",
            "Shaobo Wang",
            "Jiaze Li",
            "Tianle Niu",
            "Junxin Fan",
            "Yongliang Miao",
            "Lijin Yang",
            "Linfeng Zhang"
        ],
        "comments": "AAAI 2026, 18 pages, 6 figures, 6 tables",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Data condensation techniques aim to synthesize a compact dataset from a larger one to enable efficient model training, yet while successful in unimodal settings, they often fail in multimodal scenarios where preserving intricate inter-modal dependencies is crucial. To address this, we introduce ImageBindDC, a novel data condensation framework operating within the unified feature space of ImageBind. Our approach moves beyond conventional distribution-matching by employing a powerful Characteristic Function (CF) loss, which operates in the Fourier domain to facilitate a more precise statistical alignment via exact infinite moment matching. We design our objective to enforce three critical levels of distributional consistency: (i) uni-modal alignment, which matches the statistical properties of synthetic and real data within each modality; (ii) cross-modal alignment, which preserves pairwise semantics by matching the distributions of hybrid real-synthetic data pairs; and (iii) joint-modal alignment, which captures the complete multivariate data structure by aligning the joint distribution of real data pairs with their synthetic counterparts. Extensive experiments highlight the effectiveness of ImageBindDC: on the NYU-v2 dataset, a model trained on just 5 condensed datapoints per class achieves lossless performance comparable to one trained on the full dataset, achieving a new state-of-the-art with an 8.2\\% absolute improvement over the previous best method and more than 4$\\times$ less condensation time.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 163,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08275",
        "abs_url": "https://arxiv.org/abs/2511.08275",
        "pdf_url": "https://arxiv.org/pdf/2511.08275",
        "title": "Bi-Objective Evolutionary Optimization for Large-Scale Open Pit Mine Scheduling Problem under Uncertainty with Chance Constraints",
        "authors": [
            "Ishara Hewa Pathiranage",
            "Aneta Neumann"
        ],
        "comments": "",
        "subjects": "Neural and Evolutionary Computing (cs.NE); Artificial Intelligence (cs.AI)",
        "abstract": "The open-pit mine scheduling problem (OPMSP) is a complex, computationally expensive process in long-term mine planning, constrained by operational and geological dependencies. Traditional deterministic approaches often ignore geological uncertainty, leading to suboptimal and potentially infeasible production schedules. Chance constraints allow modeling of stochastic components by ensuring probabilistic constraints are satisfied with high probability. This paper presents a bi-objective formulation of the OPMSP that simultaneously maximizes expected net present value and minimizes scheduling risk, independent of the confidence level required for the constraint. Solutions are represented using integer encoding, inherently satisfying reserve constraints. We introduce a domain-specific greedy randomized initialization and a precedence-aware period-swap mutation operator. We integrate these operators into three multi-objective evolutionary algorithms: the global simple evolutionary multi-objective optimizer (GSEMO), a mutation-only variant of multi-objective evolutionary algorithm based on decomposition (MOEA/D), and non-dominated sorting genetic algorithm II (NSGA-II). We compare our bi-objective formulation against the single-objective approach, which depends on a specific confidence level, by analyzing mine deposits consisting of up to 112 687 blocks. Results demonstrate that the proposed bi-objective formulation yields more robust and balanced trade-offs between economic value and risk compared to single-objective, confidence-dependent approach.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 164,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08287",
        "abs_url": "https://arxiv.org/abs/2511.08287",
        "pdf_url": "https://arxiv.org/pdf/2511.08287",
        "title": "Dual-Kernel Graph Community Contrastive Learning",
        "authors": [
            "Xiang Chen",
            "Kun Yue",
            "Wenjie Liu",
            "Zhenyu Zhang",
            "Liang Duan"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Graph Contrastive Learning (GCL) has emerged as a powerful paradigm for training Graph Neural Networks (GNNs) in the absence of task-specific labels. However, its scalability on large-scale graphs is hindered by the intensive message passing mechanism of GNN and the quadratic computational complexity of contrastive loss over positive and negative node pairs. To address these issues, we propose an efficient GCL framework that transforms the input graph into a compact network of interconnected node sets while preserving structural information across communities. We firstly introduce a kernelized graph community contrastive loss with linear complexity, enabling effective information transfer among node sets to capture hierarchical structural information of the graph. We then incorporate a knowledge distillation technique into the decoupled GNN architecture to accelerate inference while maintaining strong generalization performance. Extensive experiments on sixteen real-world datasets of varying scales demonstrate that our method outperforms state-of-the-art GCL baselines in both effectiveness and scalability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 165,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08305",
        "abs_url": "https://arxiv.org/abs/2511.08305",
        "pdf_url": "https://arxiv.org/pdf/2511.08305",
        "title": "Test-time Diverse Reasoning by Riemannian Activation Steering",
        "authors": [
            "Ly Tran Ho Khanh",
            "Dongxuan Zhu",
            "Man-Chung Yue",
            "Viet Anh Nguyen"
        ],
        "comments": "19 pages, 6 figures. Accepted for publication at AAAI 2026 (40th AAAI Conference on Artificial Intelligence)",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Best-of-$N$ reasoning improves the accuracy of language models in solving complex tasks by sampling multiple candidate solutions and then selecting the best one based on some criteria. A critical bottleneck for this strategy is the output diversity limit, which occurs when the model generates similar outputs despite stochastic sampling, and hence recites the same error. To address this lack of variance in reasoning paths, we propose a novel unsupervised activation steering strategy that simultaneously optimizes the steering vectors for multiple reasoning trajectories at test time. At any synchronization anchor along the batch generation process, we find the steering vectors that maximize the total volume spanned by all possible intervened activation subsets. We demonstrate that these steering vectors can be determined by solving a Riemannian optimization problem over the product of spheres with a log-determinant objective function. We then use a Riemannian block-coordinate descent algorithm with a well-tuned learning rate to obtain a stationary point of the problem, and we apply these steering vectors until the generation process reaches the subsequent synchronization anchor. Empirical evaluations on popular mathematical benchmarks demonstrate that our test-time Riemannian activation steering strategy outperforms vanilla sampling techniques in terms of generative diversity and solution accuracy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 166,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08314",
        "abs_url": "https://arxiv.org/abs/2511.08314",
        "pdf_url": "https://arxiv.org/pdf/2511.08314",
        "title": "Improving the accuracy and generalizability of molecular property regression models with a substructure-substitution-rule-informed framework",
        "authors": [
            "Xiaoyu Fan",
            "Lin Guo",
            "Ruizhen Jia",
            "Yang Tian",
            "Zhihao Yang",
            "Boxue Tian"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Artificial Intelligence (AI)-aided drug discovery is an active research field, yet AI models often exhibit poor accuracy in regression tasks for molecular property prediction, and perform catastrophically poorly for out-of-distribution (OOD) molecules. Here, we present MolRuleLoss, a substructure-substitution-rule-informed framework that improves the accuracy and generalizability of multiple molecular property regression models (MPRMs) such as GEM and UniMol for diverse molecular property prediction tasks. MolRuleLoss incorporates partial derivative constraints for substructure substitution rules (SSRs) into an MPRM's loss function. When using GEM models for predicting lipophilicity, water solubility, and solvation-free energy (using lipophilicity, ESOL, and freeSolv datasets from MoleculeNet), the root mean squared error (RMSE) values with and without MolRuleLoss were 0.587 vs. 0.660, 0.777 vs. 0.798, and 1.252 vs. 1.877, respectively, representing 2.6-33.3% performance improvements. We show that both the number and the quality of SSRs contribute to the magnitude of prediction accuracy gains obtained upon adding MolRuleLoss to an MPRM. MolRuleLoss improved the generalizability of MPRMs for \"activity cliff\" molecules in a lipophilicity prediction task and improved the generalizability of MPRMs for OOD molecules in a melting point prediction task. In a molecular weight prediction task for OOD molecules, MolRuleLoss reduced the RMSE value of a GEM model from 29.507 to 0.007. We also provide a formal demonstration that the upper bound of the variation for property change of SSRs is positively correlated with an MPRM's error. Together, we show that using the MolRuleLoss framework as a bolt-on boosts the prediction accuracy and generalizability of multiple MPRMs, supporting diverse applications in areas like cheminformatics and AI-aided drug discovery.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 167,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08319",
        "abs_url": "https://arxiv.org/abs/2511.08319",
        "pdf_url": "https://arxiv.org/pdf/2511.08319",
        "title": "Adaptive Multi-Agent Response Refinement in Conversational Systems",
        "authors": [
            "Soyeong Jeong",
            "Aparna Elangovan",
            "Emine Yilmaz",
            "Oleg Rokhlenko"
        ],
        "comments": "LaCATODA Workshop @ AAAI 2026",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable success in conversational systems by generating human-like responses. However, they can fall short, especially when required to account for personalization or specific knowledge. In real-life settings, it is impractical to rely on users to detect these errors and request a new response. One way to address this problem is to refine the response before returning it to the user. While existing approaches focus on refining responses within a single LLM, this method struggles to consider diverse aspects needed for effective conversations. In this work, we propose refining responses through a multi-agent framework, where each agent is assigned a specific role for each aspect. We focus on three key aspects crucial to conversational quality: factuality, personalization, and coherence. Each agent is responsible for reviewing and refining one of these aspects, and their feedback is then merged to improve the overall response. To enhance collaboration among them, we introduce a dynamic communication strategy. Instead of following a fixed sequence of agents, our approach adaptively selects and coordinates the most relevant agents based on the specific requirements of each query. We validate our framework on challenging conversational datasets, demonstrating that ours significantly outperforms relevant baselines, particularly in tasks involving knowledge or user's persona, or both.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 168,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08339",
        "abs_url": "https://arxiv.org/abs/2511.08339",
        "pdf_url": "https://arxiv.org/pdf/2511.08339",
        "title": "LPPG-RL: Lexicographically Projected Policy Gradient Reinforcement Learning with Subproblem Exploration",
        "authors": [
            "Ruiyu Qiu",
            "Rui Wang",
            "Guanghui Yang",
            "Xiang Li",
            "Zhijiang Shao"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Lexicographic multi-objective problems, which consist of multiple conflicting subtasks with explicit priorities, are common in real-world applications. Despite the advantages of Reinforcement Learning (RL) in single tasks, extending conventional RL methods to prioritized multiple objectives remains challenging. In particular, traditional Safe RL and Multi-Objective RL (MORL) methods have difficulty enforcing priority orderings efficiently. Therefore, Lexicographic Multi-Objective RL (LMORL) methods have been developed to address these challenges. However, existing LMORL methods either rely on heuristic threshold tuning with prior knowledge or are restricted to discrete domains. To overcome these limitations, we propose Lexicographically Projected Policy Gradient RL (LPPG-RL), a novel LMORL framework which leverages sequential gradient projections to identify feasible policy update directions, thereby enabling LPPG-RL broadly compatible with all policy gradient algorithms in continuous spaces. LPPG-RL reformulates the projection step as an optimization problem, and utilizes Dykstra's projection rather than generic solvers to deliver great speedups, especially for small- to medium-scale instances. In addition, LPPG-RL introduces Subproblem Exploration (SE) to prevent gradient vanishing, accelerate convergence and enhance stability. We provide theoretical guarantees for convergence and establish a lower bound on policy improvement. Finally, through extensive experiments in a 2D navigation environment, we demonstrate the effectiveness of LPPG-RL, showing that it outperforms existing state-of-the-art continuous LMORL methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 169,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08340",
        "abs_url": "https://arxiv.org/abs/2511.08340",
        "pdf_url": "https://arxiv.org/pdf/2511.08340",
        "title": "HN-MVTS: HyperNetwork-based Multivariate Time Series Forecasting",
        "authors": [
            "Andrey Savchenko",
            "Oleg Kachan"
        ],
        "comments": "AAAI 2026",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Accurate forecasting of multivariate time series data remains a formidable challenge, particularly due to the growing complexity of temporal dependencies in real-world scenarios. While neural network-based models have achieved notable success in this domain, complex channel-dependent models often suffer from performance degradation compared to channel-independent models that do not consider the relationship between components but provide high robustness due to small capacity. In this work, we propose HN-MVTS, a novel architecture that integrates a hypernetwork-based generative prior with an arbitrary neural network forecasting model. The input of this hypernetwork is a learnable embedding matrix of time series components. To restrict the number of new parameters, the hypernetwork learns to generate the weights of the last layer of the target forecasting networks, serving as a data-adaptive regularizer that improves generalization and long-range predictive accuracy. The hypernetwork is used only during the training, so it does not increase the inference time compared to the base forecasting model. Extensive experiments on eight benchmark datasets demonstrate that application of HN-MVTS to the state-of-the-art models (DLinear, PatchTST, TSMixer, etc.) typically improves their performance. Our findings suggest that hypernetwork-driven parameterization offers a promising direction for enhancing existing forecasting techniques in complex scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 170,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08349",
        "abs_url": "https://arxiv.org/abs/2511.08349",
        "pdf_url": "https://arxiv.org/pdf/2511.08349",
        "title": "Hybrid Quantum-Classical Selective State Space Artificial Intelligence",
        "authors": [
            "Amin Ebrahimi",
            "Farzan Haddadi"
        ],
        "comments": "21 pages",
        "subjects": "Quantum Physics (quant-ph); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Hybrid Quantum Classical (HQC) algorithms constitute one of the most effective paradigms for exploiting the computational advantages of quantum systems in large-scale numerical tasks. By operating in high-dimensional Hilbert spaces, quantum circuits enable exponential speed-ups and provide access to richer representations of cost landscapes compared to purely classical methods. These capabilities are particularly relevant for machine learning, where state-of-the-art models especially in Natural Language Processing (NLP) suffer from prohibitive time complexity due to massive matrix multiplications and high-dimensional optimization. In this manuscript, we propose a Hybrid Quantum Classical selection mechanism for the Mamba architecture, designed specifically for temporal sequence classification problems. Our approach leverages Variational Quantum Circuits (VQCs) as quantum gating modules that both enhance feature extraction and improve suppression of irrelevant information. This integration directly addresses the computational bottlenecks of deep learning architectures by exploiting quantum resources for more efficient representation learning. We analyze how introducing quantum subroutines into large language models (LLMs) impacts their generalization capability, expressivity, and parameter efficiency. The results highlight the potential of quantum-enhanced gating mechanisms as a path toward scalable, resource-efficient NLP models, in a limited simulation step. Within the first four epochs on a reshaped MNIST dataset with input format (batch, 784, d_model), our hybrid model achieved 24.6% accuracy while using one quantum layer and achieve higher expressivity, compared to 21.6% obtained by a purely classical selection mechanism. we state No founding",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 171,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08364",
        "abs_url": "https://arxiv.org/abs/2511.08364",
        "pdf_url": "https://arxiv.org/pdf/2511.08364",
        "title": "DPRM: A Dual Implicit Process Reward Model in Multi-Hop Question Answering",
        "authors": [
            "Xinyi Wang",
            "Yiping Song",
            "Zhiliang Tian",
            "Bo Liu",
            "Tingjin Luo",
            "Minlie Huang"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "In multi-hop question answering (MHQA) tasks, Chain of Thought (CoT) improves the quality of generation by guiding large language models (LLMs) through multi-step reasoning, and Knowledge Graphs (KGs) reduce hallucinations via semantic matching. Outcome Reward Models (ORMs) provide feedback after generating the final answers but fail to evaluate the process for multi-step reasoning. Traditional Process Reward Models (PRMs) evaluate the reasoning process but require costly human annotations or rollout generation. While implicit PRM is trained only with outcome signals and derives step rewards through reward parameterization without explicit annotations, it is more suitable for multi-step reasoning in MHQA tasks. However, existing implicit PRM has only been explored for plain text scenarios. When adapting to MHQA tasks, it cannot handle the graph structure constraints in KGs and capture the potential inconsistency between CoT and KG paths. To address these limitations, we propose the DPRM (Dual Implicit Process Reward Model). It trains two implicit PRMs for CoT and KG reasoning in MHQA tasks. Both PRMs, namely KG-PRM and CoT-PRM, derive step-level rewards from outcome signals via reward parameterization without additional explicit annotations. Among them, KG-PRM uses preference pairs to learn structural constraints from KGs. DPRM further introduces a consistency constraint between CoT and KG reasoning steps, making the two PRMs mutually verify and collaboratively optimize the reasoning paths. We also provide a theoretical demonstration of the derivation of process rewards. Experimental results show that our method outperforms 13 baselines on multiple datasets with up to 16.6% improvement on Hit@1.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 172,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08369",
        "abs_url": "https://arxiv.org/abs/2511.08369",
        "pdf_url": "https://arxiv.org/pdf/2511.08369",
        "title": "Text-based Aerial-Ground Person Retrieval",
        "authors": [
            "Xinyu Zhou",
            "Yu Wu",
            "Jiayao Ma",
            "Wenhao Wang",
            "Min Cao",
            "Mang Ye"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "This work introduces Text-based Aerial-Ground Person Retrieval (TAG-PR), which aims to retrieve person images from heterogeneous aerial and ground views with textual descriptions. Unlike traditional Text-based Person Retrieval (T-PR), which focuses solely on ground-view images, TAG-PR introduces greater practical significance and presents unique challenges due to the large viewpoint discrepancy across images. To support this task, we contribute: (1) TAG-PEDES dataset, constructed from public benchmarks with automatically generated textual descriptions, enhanced by a diversified text generation paradigm to ensure robustness under view heterogeneity; and (2) TAG-CLIP, a novel retrieval framework that addresses view heterogeneity through a hierarchically-routed mixture of experts module to learn view-specific and view-agnostic features and a viewpoint decoupling strategy to decouple view-specific features for better cross-modal alignment. We evaluate the effectiveness of TAG-CLIP on both the proposed TAG-PEDES dataset and existing T-PR benchmarks. The dataset and code are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 173,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08378",
        "abs_url": "https://arxiv.org/abs/2511.08378",
        "pdf_url": "https://arxiv.org/pdf/2511.08378",
        "title": "Bid Farewell to Seesaw: Towards Accurate Long-tail Session-based Recommendation via Dual Constraints of Hybrid Intents",
        "authors": [
            "Xiao Wang",
            "Ke Qin",
            "Dongyang Zhang",
            "Xiurui Xie",
            "Shuang Liang"
        ],
        "comments": "",
        "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI)",
        "abstract": "Session-based recommendation (SBR) aims to predict anonymous users' next interaction based on their interaction sessions. In the practical recommendation scenario, low-exposure items constitute the majority of interactions, creating a long-tail distribution that severely compromises recommendation diversity. Existing approaches attempt to address this issue by promoting tail items but incur accuracy degradation, exhibiting a \"see-saw\" effect between long-tail and accuracy performance. We attribute such conflict to session-irrelevant noise within the tail items, which existing long-tail approaches fail to identify and constrain effectively. To resolve this fundamental conflict, we propose \\textbf{HID} (\\textbf{H}ybrid \\textbf{I}ntent-based \\textbf{D}ual Constraint Framework), a plug-and-play framework that transforms the conventional \"see-saw\" into \"win-win\" through introducing the hybrid intent-based dual constraints for both long-tail and accuracy. Two key innovations are incorporated in this framework: (i) \\textit{Hybrid Intent Learning}, where we reformulate the intent extraction strategies by employing attribute-aware spectral clustering to reconstruct the item-to-intent mapping. Furthermore, discrimination of session-irrelevant noise is achieved through the assignment of the target and noise intents to each session. (ii) \\textit{Intent Constraint Loss}, which incorporates two novel constraint paradigms regarding the \\textit{diversity} and \\textit{accuracy} to regulate the representation learning process of both items and sessions. These two objectives are unified into a single training loss through rigorous theoretical derivation. Extensive experiments across multiple SBR models and datasets demonstrate that HID can enhance both long-tail performance and recommendation accuracy, establishing new state-of-the-art performance in long-tail recommender systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 174,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08389",
        "abs_url": "https://arxiv.org/abs/2511.08389",
        "pdf_url": "https://arxiv.org/pdf/2511.08389",
        "title": "Unifying Model and Layer Fusion for Speech Foundation Models",
        "authors": [
            "Yi-Jen Shih",
            "David Harwath"
        ],
        "comments": "Accepted by IEEE ASRU 2025",
        "subjects": "Audio and Speech Processing (eess.AS); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Speech Foundation Models have gained significant attention recently. Prior works have shown that the fusion of representations from multiple layers of the same model or the fusion of multiple models can improve performance on downstream tasks. We unify these two fusion strategies by proposing an interface module that enables fusion across multiple upstream speech models while integrating information across their layers. We conduct extensive experiments on different self-supervised and supervised models across various speech tasks, including ASR and paralinguistic analysis, and demonstrate that our method outperforms prior fusion approaches. We further analyze its scalability concerning model size and count, highlighting the importance of selecting appropriate upstream models. Our results show that the proposed interface provides an additional performance boost when given a suitable upstream model selection, making it a promising approach for utilizing Speech Foundation Models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 175,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08394",
        "abs_url": "https://arxiv.org/abs/2511.08394",
        "pdf_url": "https://arxiv.org/pdf/2511.08394",
        "title": "Interaction Dynamics as a Reward Signal for LLMs",
        "authors": [
            "Sian Gooding",
            "Edward Grefenstette"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)",
        "abstract": "The alignment of Large Language Models (LLMs) for multi-turn conversations typically relies on reward signals derived from the content of the text. This approach, however, overlooks a rich, complementary source of signal: the dynamics of the interaction itself. This paper introduces TRACE (Trajectory-based Reward for Agent Collaboration Estimation), a novel reward signal derived from the geometric properties of a dialogue's embedding trajectory--a concept we term 'conversational geometry'. Our central finding is that a reward model trained only on these structural signals achieves a pairwise accuracy (68.20%) comparable to a powerful LLM baseline that analyzes the full transcript (70.04%). Furthermore, a hybrid model combining interaction dynamics with textual analysis achieves the highest performance (80.17%), demonstrating their complementary nature. This work provides strong evidence that for interactive settings, how an agent communicates is as powerful a predictor of success as what it says, offering a new, privacy-preserving framework that not only aligns agents but also serves as a diagnostic tool for understanding the distinct interaction patterns that drive successful collaboration.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 176,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08436",
        "abs_url": "https://arxiv.org/abs/2511.08436",
        "pdf_url": "https://arxiv.org/pdf/2511.08436",
        "title": "Understanding Electro-communication and Electro-sensing in Weakly Electric Fish using Multi-Agent Deep Reinforcement Learning",
        "authors": [
            "Satpreet H. Singh",
            "Sonja Johnson-Yu",
            "Zhouyang Lu",
            "Aaron Walsman",
            "Federico Pedraja",
            "Denis Turcu",
            "Pratyusha Sharma",
            "Naomi Saphra",
            "Nathaniel B. Sawtell",
            "Kanaka Rajan"
        ],
        "comments": "",
        "subjects": "Neural and Evolutionary Computing (cs.NE); Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA); Systems and Control (eess.SY); Neurons and Cognition (q-bio.NC)",
        "abstract": "Weakly electric fish, like Gnathonemus petersii, use a remarkable electrical modality for active sensing and communication, but studying their rich electrosensing and electrocommunication behavior and associated neural activity in naturalistic settings remains experimentally challenging. Here, we present a novel biologically-inspired computational framework to study these behaviors, where recurrent neural network (RNN) based artificial agents trained via multi-agent reinforcement learning (MARL) learn to modulate their electric organ discharges (EODs) and movement patterns to collectively forage in virtual environments. Trained agents demonstrate several emergent features consistent with real fish collectives, including heavy tailed EOD interval distributions, environmental context dependent shifts in EOD interval distributions, and social interaction patterns like freeloading, where agents reduce their EOD rates while benefiting from neighboring agents' active sensing. A minimal two-fish assay further isolates the role of electro-communication, showing that access to conspecific EODs and relative dominance jointly shape foraging success. Notably, these behaviors emerge through evolution-inspired rewards for individual fitness and emergent inter-agent interactions, rather than through rewarding agents explicitly for social interactions. Our work has broad implications for the neuroethology of weakly electric fish, as well as other social, communicating animals in which extensive recordings from multiple individuals, and thus traditional data-driven modeling, are infeasible.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 177,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08470",
        "abs_url": "https://arxiv.org/abs/2511.08470",
        "pdf_url": "https://arxiv.org/pdf/2511.08470",
        "title": "Binary Split Categorical feature with Mean Absolute Error Criteria in CART",
        "authors": [
            "Peng Yu",
            "Yike Chen",
            "Chao Xu",
            "Albert Bifet",
            "Jesse Read"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "In the context of the Classification and Regression Trees (CART) algorithm, the efficient splitting of categorical features using standard criteria like GINI and Entropy is well-established. However, using the Mean Absolute Error (MAE) criterion for categorical features has traditionally relied on various numerical encoding methods. This paper demonstrates that unsupervised numerical encoding methods are not viable for the MAE criteria. Furthermore, we present a novel and efficient splitting algorithm that addresses the challenges of handling categorical features with the MAE criterion. Our findings underscore the limitations of existing approaches and offer a promising solution to enhance the handling of categorical data in CART algorithms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 178,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08475",
        "abs_url": "https://arxiv.org/abs/2511.08475",
        "pdf_url": "https://arxiv.org/pdf/2511.08475",
        "title": "Designing LLM-based Multi-Agent Systems for Software Engineering Tasks: Quality Attributes, Design Patterns and Rationale",
        "authors": [
            "Yangxiao Cai",
            "Ruiyin Li",
            "Peng Liang",
            "Mojtaba Shahin",
            "Zengyang Li"
        ],
        "comments": "",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)",
        "abstract": "As the complexity of Software Engineering (SE) tasks continues to escalate, Multi-Agent Systems (MASs) have emerged as a focal point of research and practice due to their autonomy and scalability. Furthermore, through leveraging the reasoning and planning capabilities of Large Language Models (LLMs), the application of LLM-based MASs in the field of SE is garnering increasing attention. However, there is no dedicated study that systematically explores the design of LLM-based MASs, including the Quality Attributes (QAs) on which the designers mainly focus, the design patterns used by the designers, and the rationale guiding the design of LLM-based MASs for SE tasks. To this end, we conducted a study to identify the QAs that LLM-based MASs for SE tasks focus on, the design patterns used in the MASs, and the design rationale for the MASs. We collected 94 papers on LLM-based MASs for SE tasks as the source. Our study shows that: (1) Code Generation is the most common SE task solved by LLM-based MASs among ten identified SE tasks, (2) Functional Suitability is the QA on which designers of LLM-based MASs pay the most attention, (3) Role-Based Cooperation is the design pattern most frequently employed among 16 patterns used to construct LLM-based MASs, and (4) Improving the Quality of Generated Code is the most common rationale behind the design of LLM-based MASs. Based on the study results, we presented the implications for the design of LLM-based MASs to support SE tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 179,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08496",
        "abs_url": "https://arxiv.org/abs/2511.08496",
        "pdf_url": "https://arxiv.org/pdf/2511.08496",
        "title": "HQ-SVC: Towards High-Quality Zero-Shot Singing Voice Conversion in Low-Resource Scenarios",
        "authors": [
            "Bingsong Bai",
            "Yizhong Geng",
            "Fengping Wang",
            "Cong Wang",
            "Puyuan Guo",
            "Yingming Gao",
            "Ya Li"
        ],
        "comments": "Accepted by AAAI 2026 main technical track",
        "subjects": "Sound (cs.SD); Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)",
        "abstract": "Zero-shot singing voice conversion (SVC) transforms a source singer's timbre to an unseen target speaker's voice while preserving melodic content without fine-tuning. Existing methods model speaker timbre and vocal content separately, losing essential acoustic information that degrades output quality while requiring significant computational resources. To overcome these limitations, we propose HQ-SVC, an efficient framework for high-quality zero-shot SVC. HQ-SVC first extracts jointly content and speaker features using a decoupled codec. It then enhances fidelity through pitch and volume modeling, preserving critical acoustic information typically lost in separate modeling approaches, and progressively refines outputs via differentiable signal processing and diffusion techniques. Evaluations confirm HQ-SVC significantly outperforms state-of-the-art zero-shot SVC methods in conversion quality and efficiency. Beyond voice conversion, HQ-SVC achieves superior voice naturalness compared to specialized audio super-resolution methods while natively supporting voice super-resolution tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 180,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08500",
        "abs_url": "https://arxiv.org/abs/2511.08500",
        "pdf_url": "https://arxiv.org/pdf/2511.08500",
        "title": "SPEAR-MM: Selective Parameter Evaluation and Restoration via Model Merging for Efficient Financial LLM Adaptation",
        "authors": [
            "Berkcan Kapusuzoglu",
            "Supriyo Chakraborty",
            "Renkun Ni",
            "Stephen Rawls",
            "Sambit Sahu"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Spectral Theory (math.SP)",
        "abstract": "Large language models (LLMs) adapted to financial domains often suffer from catastrophic forgetting of general reasoning capabilities essential for customer interactions and complex financial analysis. We introduce Selective Parameter Evaluation and Restoration via Model Merging (SPEAR-MM), a practical framework that preserves critical capabilities while enabling domain adaptation. Our method approximates layer-wise impact on external benchmarks through post-hoc analysis, then selectively freezes or restores transformer layers via spherical interpolation merging. Applied to LLaMA-3.1-8B for financial tasks, SPEAR-MM achieves 91.2% retention of general capabilities versus 69.7% for standard continual pretraining, while maintaining 94% of domain adaptation gains. The approach provides interpretable trade-off control and reduces computational costs by 90% crucial for resource-constrained financial institutions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 181,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08507",
        "abs_url": "https://arxiv.org/abs/2511.08507",
        "pdf_url": "https://arxiv.org/pdf/2511.08507",
        "title": "Introducing A Bangla Sentence - Gloss Pair Dataset for Bangla Sign Language Translation and Research",
        "authors": [
            "Neelavro Saha",
            "Rafi Shahriyar",
            "Nafis Ashraf Roudra",
            "Saadman Sakib",
            "Annajiat Alim Rasel"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Bangla Sign Language (BdSL) translation represents a low-resource NLP task due to the lack of large-scale datasets that address sentence-level translation. Correspondingly, existing research in this field has been limited to word and alphabet level detection. In this work, we introduce Bangla-SGP, a novel parallel dataset consisting of 1,000 human-annotated sentence-gloss pairs which was augmented with around 3,000 synthetically generated pairs using syntactic and morphological rules through a rule-based Retrieval-Augmented Generation (RAG) pipeline. The gloss sequences of the spoken Bangla sentences are made up of individual glosses which are Bangla sign supported words and serve as an intermediate representation for a continuous sign. Our dataset consists of 1000 high quality Bangla sentences that are manually annotated into a gloss sequence by a professional signer. The augmentation process incorporates rule-based linguistic strategies and prompt engineering techniques that we have adopted by critically analyzing our human annotated sentence-gloss pairs and by working closely with our professional signer. Furthermore, we fine-tune several transformer-based models such as mBart50, Google mT5, GPT4.1-nano and evaluate their sentence-to-gloss translation performance using BLEU scores, based on these evaluation metrics we compare the model's gloss-translation consistency across our dataset and the RWTH-PHOENIX-2014T benchmark.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 182,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08544",
        "abs_url": "https://arxiv.org/abs/2511.08544",
        "pdf_url": "https://arxiv.org/pdf/2511.08544",
        "title": "LeJEPA: Provable and Scalable Self-Supervised Learning Without the Heuristics",
        "authors": [
            "Randall Balestriero",
            "Yann LeCun"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)",
        "abstract": "Learning manipulable representations of the world and its dynamics is central to AI. Joint-Embedding Predictive Architectures (JEPAs) offer a promising blueprint, but lack of practical guidance and theory has led to ad-hoc R&D. We present a comprehensive theory of JEPAs and instantiate it in {\\bf LeJEPA}, a lean, scalable, and theoretically grounded training objective. First, we identify the isotropic Gaussian as the optimal distribution that JEPAs' embeddings should follow to minimize downstream prediction risk. Second, we introduce a novel objective--{\\bf Sketched Isotropic Gaussian Regularization} (SIGReg)--to constrain embeddings to reach that ideal distribution. Combining the JEPA predictive loss with SIGReg yields LeJEPA with numerous theoretical and practical benefits: (i) single trade-off hyperparameter, (ii) linear time and memory complexity, (iii) stability across hyper-parameters, architectures (ResNets, ViTs, ConvNets) and domains, (iv) heuristics-free, e.g., no stop-gradient, no teacher-student, no hyper-parameter schedulers, and (v) distributed training-friendly implementation requiring only $\\approx$50 lines of code. Our empirical validation covers 10+ datasets, 60+ architectures, all with varying scales and domains. As an example, using imagenet-1k for pretraining and linear evaluation with frozen backbone, LeJEPA reaches 79\\% with a ViT-H/14. We hope that the simplicity and theory-friendly ecosystem offered by LeJEPA will reestablish self-supervised pre-training as a core pillar of AI research (\\href{git@github.com:rbalestr-lab/lejepa.git}{GitHub repo}).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 183,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08565",
        "abs_url": "https://arxiv.org/abs/2511.08565",
        "pdf_url": "https://arxiv.org/pdf/2511.08565",
        "title": "Moral Susceptibility and Robustness under Persona Role-Play in Large Language Models",
        "authors": [
            "Davi Bastos Costa",
            "Felippe Alves",
            "Renato Vicente"
        ],
        "comments": "9+8 pages, 7 tables, 6 figures",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computers and Society (cs.CY)",
        "abstract": "Large language models (LLMs) increasingly operate in social contexts, motivating analysis of how they express and shift moral judgments. In this work, we investigate the moral response of LLMs to persona role-play, prompting a LLM to assume a specific character. Using the Moral Foundations Questionnaire (MFQ), we introduce a benchmark that quantifies two properties: moral susceptibility and moral robustness, defined from the variability of MFQ scores across and within personas, respectively. We find that, for moral robustness, model family accounts for most of the variance, while model size shows no systematic effect. The Claude family is, by a significant margin, the most robust, followed by Gemini and GPT-4 models, with other families exhibiting lower robustness. In contrast, moral susceptibility exhibits a mild family effect but a clear within-family size effect, with larger variants being more susceptible. Moreover, robustness and susceptibility are positively correlated, an association that is more pronounced at the family level. Additionally, we present moral foundation profiles for models without persona role-play and for personas averaged across models. Together, these analyses provide a systematic view of how persona conditioning shapes moral behavior in large language models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 184,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08567",
        "abs_url": "https://arxiv.org/abs/2511.08567",
        "pdf_url": "https://arxiv.org/pdf/2511.08567",
        "title": "The Path Not Taken: RLVR Provably Learns Off the Principals",
        "authors": [
            "Hanqing Zhu",
            "Zhenyu Zhang",
            "Hanxian Huang",
            "DiJia Su",
            "Zechun Liu",
            "Jiawei Zhao",
            "Igor Fedorov",
            "Hamed Pirsiavash",
            "Zhizhou Sha",
            "Jinwon Lee",
            "David Z. Pan",
            "Zhangyang Wang",
            "Yuandong Tian",
            "Kai Sheng Tai"
        ],
        "comments": "Preliminary version accepted as a spotlight in NeurIPS 2025 Workshop on Efficient Reasoning",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) reliably improves the reasoning performance of large language models, yet it appears to modify only a small fraction of parameters. We revisit this paradox and show that sparsity is a surface artifact of a model-conditioned optimization bias: for a fixed pretrained model, updates consistently localize to preferred parameter regions, highly consistent across runs and largely invariant to datasets and RL recipes. We mechanistically explain these dynamics with a Three-Gate Theory: Gate I (KL Anchor) imposes a KL-constrained update; Gate II (Model Geometry) steers the step off principal directions into low-curvature, spectrum-preserving subspaces; and Gate III (Precision) hides micro-updates in non-preferred regions, making the off-principal bias appear as sparsity. We then validate this theory and, for the first time, provide a parameter-level characterization of RLVR's learning dynamics: RLVR learns off principal directions in weight space, achieving gains via minimal spectral drift, reduced principal-subspace rotation, and off-principal update alignment. In contrast, SFT targets principal weights, distorts the spectrum, and even lags RLVR. Together, these results provide the first parameter-space account of RLVR's training dynamics, revealing clear regularities in how parameters evolve. Crucially, we show that RL operates in a distinct optimization regime from SFT, so directly adapting SFT-era parameter-efficient fine-tuning (PEFT) methods can be flawed, as evidenced by our case studies on advanced sparse fine-tuning and LoRA variants. We hope this work charts a path toward a white-box understanding of RLVR and the design of geometry-aware, RLVR-native learning algorithms, rather than repurposed SFT-era heuristics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 185,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08570",
        "abs_url": "https://arxiv.org/abs/2511.08570",
        "pdf_url": "https://arxiv.org/pdf/2511.08570",
        "title": "Automatic Grid Updates for Kolmogorov-Arnold Networks using Layer Histograms",
        "authors": [
            "Jamison Moody",
            "James Usevitch"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Kolmogorov-Arnold Networks (KANs) are a class of neural networks that have received increased attention in recent literature. In contrast to MLPs, KANs leverage parameterized, trainable activation functions and offer several benefits including improved interpretability and higher accuracy on learning symbolic equations. However, the original KAN architecture requires adjustments to the domain discretization of the network (called the \"domain grid\") during training, creating extra overhead for the user in the training process. Typical KAN layers are not designed with the ability to autonomously update their domains in a data-driven manner informed by the changing output ranges of previous layers. As an added benefit, this histogram algorithm may also be applied towards detecting out-of-distribution (OOD) inputs in a variety of settings. We demonstrate that AdaptKAN exceeds or matches the performance of prior KAN architectures and MLPs on four different tasks: learning scientific equations from the Feynman dataset, image classification from frozen features, learning a control Lyapunov function, and detecting OOD inputs on the OpenOOD v1.5 benchmark.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 186,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08577",
        "abs_url": "https://arxiv.org/abs/2511.08577",
        "pdf_url": "https://arxiv.org/pdf/2511.08577",
        "title": "Think-at-Hard: Selective Latent Iterations to Improve Reasoning Language Models",
        "authors": [
            "Tianyu Fu",
            "Yichen You",
            "Zekai Chen",
            "Guohao Dai",
            "Huazhong Yang",
            "Yu Wang"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Performance (cs.PF)",
        "abstract": "Improving reasoning capabilities of Large Language Models (LLMs), especially under parameter constraints, is crucial for real-world applications. Prior work proposes recurrent transformers, which allocate a fixed number of extra iterations per token to improve generation quality. After the first, standard forward pass, instead of verbalization, last-layer hidden states are fed back as inputs for additional iterations to refine token predictions. Yet we identify a latent overthinking phenomenon: easy token predictions that are already correct after the first pass are sometimes revised into errors in additional iterations. To address this, we propose Think-at-Hard (TaH), a dynamic latent thinking method that iterates deeper only at hard tokens. It employs a lightweight neural decider to trigger latent iterations only at tokens that are likely incorrect after the standard forward pass. During latent iterations, Low-Rank Adaptation (LoRA) modules shift the LLM objective from general next-token prediction to focused hard-token refinement. We further introduce a duo-causal attention mechanism that extends attention from the token sequence dimension to an additional iteration depth dimension. This enables cross-iteration information flow while maintaining full sequential parallelism. Experiments show that TaH boosts LLM reasoning performance across five challenging benchmarks while maintaining the same parameter count. Compared with baselines that iterate twice for all output tokens, TaH delivers 8.1-11.3% accuracy gains while exempting 94% of tokens from the second iteration. Against strong single-iteration Qwen3 models finetuned with the same data, it also delivers 4.0-5.0% accuracy gains. When allowing less than 3% additional parameters from LoRA and the iteration decider, the gains increase to 8.5-12.6% and 5.3-5.4%, respectively. Our code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 187,
        "date": "2025-11-12",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True",
        "arxiv_id": "2511.08579",
        "abs_url": "https://arxiv.org/abs/2511.08579",
        "pdf_url": "https://arxiv.org/pdf/2511.08579",
        "title": "Training Language Models to Explain Their Own Computations",
        "authors": [
            "Belinda Z. Li",
            "Zifan Carl Guo",
            "Vincent Huang",
            "Jacob Steinhardt",
            "Jacob Andreas"
        ],
        "comments": "33 pages, 7 tables, 8 figures",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Can language models (LMs) learn to faithfully describe their internal computations? Are they better able to describe themselves than other models? We study the extent to which LMs' privileged access to their own internals can be leveraged to produce new techniques for explaining their behavior. Using existing interpretability techniques as a source of ground truth, we fine-tune LMs to generate natural language descriptions of (1) the information encoded by LM features, (2) the causal structure of LMs' internal activations, and (3) the influence of specific input tokens on LM outputs. When trained with only tens of thousands of example explanations, explainer models exhibit non-trivial generalization to new queries. This generalization appears partly attributable to explainer models' privileged access to their own internals: using a model to explain its own computations generally works better than using a *different* model to explain its computations (even if the other model is significantly more capable). Our results suggest not only that LMs can learn to reliably explain their internal computations, but that such explanations offer a scalable complement to existing interpretability methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    }
]