[
    {
        "order": 1,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.17801",
        "abs_url": "https://arxiv.org/abs/2507.17801",
        "pdf_url": "https://arxiv.org/pdf/2507.17801",
        "title": "Lumina-mGPT 2.0: Stand-Alone AutoRegressive Image Modeling",
        "authors": [
            "Yi Xin",
            "Juncheng Yan",
            "Qi Qin",
            "Zhen Li",
            "Dongyang Liu",
            "Shicheng Li",
            "Victor Shea-Jay Huang",
            "Yupeng Zhou",
            "Renrui Zhang",
            "Le Zhuo",
            "Tiancheng Han",
            "Xiaoqing Sun",
            "Siqi Luo",
            "Mengmeng Wang",
            "Bin Fu",
            "Yuewen Cao",
            "Hongsheng Li",
            "Guangtao Zhai",
            "Xiaohong Liu",
            "Yu Qiao",
            "Peng Gao"
        ],
        "comments": "Tech Report, 23 pages, 11 figures, 7 tables",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We present Lumina-mGPT 2.0, a stand-alone, decoder-only autoregressive model that revisits and revitalizes the autoregressive paradigm for high-quality image generation and beyond. Unlike existing approaches that rely on pretrained components or hybrid architectures, Lumina-mGPT 2.0 is trained entirely from scratch, enabling unrestricted architectural design and licensing freedom. It achieves generation quality on par with state-of-the-art diffusion models such as DALL-E 3 and SANA, while preserving the inherent flexibility and compositionality of autoregressive modeling. Our unified tokenization scheme allows the model to seamlessly handle a wide spectrum of tasks-including subject-driven generation, image editing, controllable synthesis, and dense prediction-within a single generative framework. To further boost usability, we incorporate efficient decoding strategies like inference-time scaling and speculative Jacobi sampling to improve quality and speed, respectively. Extensive evaluations on standard text-to-image benchmarks (e.g., GenEval, DPG) demonstrate that Lumina-mGPT 2.0 not only matches but in some cases surpasses diffusion-based models. Moreover, we confirm its multi-task capabilities on the Graph200K benchmark, with the native Lumina-mGPT 2.0 performing exceptionally well. These results position Lumina-mGPT 2.0 as a strong, flexible foundation model for unified multimodal generation. We have released our training details, code, and models at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 2,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.17844",
        "abs_url": "https://arxiv.org/abs/2507.17844",
        "pdf_url": "https://arxiv.org/pdf/2507.17844",
        "title": "SV3.3B: A Sports Video Understanding Model for Action Recognition",
        "authors": [
            "Sai Varun Kodathala",
            "Yashwanth Reddy Vutukoori",
            "Rakesh Vunnam"
        ],
        "comments": "8 pages, 6 figures, 4 tables. Submitted to AIxSET 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "This paper addresses the challenge of automated sports video analysis, which has traditionally been limited by computationally intensive models requiring server-side processing and lacking fine-grained understanding of athletic movements. Current approaches struggle to capture the nuanced biomechanical transitions essential for meaningful sports analysis, often missing critical phases like preparation, execution, and follow-through that occur within seconds. To address these limitations, we introduce SV3.3B, a lightweight 3.3B parameter video understanding model that combines novel temporal motion difference sampling with self-supervised learning for efficient on-device deployment. Our approach employs a DWT-VGG16-LDA based keyframe extraction mechanism that intelligently identifies the 16 most representative frames from sports sequences, followed by a V-DWT-JEPA2 encoder pretrained through mask-denoising objectives and an LLM decoder fine-tuned for sports action description generation. Evaluated on a subset of the NSVA basketball dataset, SV3.3B achieves superior performance across both traditional text generation metrics and sports-specific evaluation criteria, outperforming larger closed-source models including GPT-4o variants while maintaining significantly lower computational requirements. Our model demonstrates exceptional capability in generating technically detailed and analytically rich sports descriptions, achieving 29.2% improvement over GPT-4o in ground truth validation metrics, with substantial improvements in information density, action complexity, and measurement precision metrics essential for comprehensive athletic analysis. Model Available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 3,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.17853",
        "abs_url": "https://arxiv.org/abs/2507.17853",
        "pdf_url": "https://arxiv.org/pdf/2507.17853",
        "title": "Detail++: Training-Free Detail Enhancer for Text-to-Image Diffusion Models",
        "authors": [
            "Lifeng Chen",
            "Jiner Wang",
            "Zihao Pan",
            "Beier Zhu",
            "Xiaofeng Yang",
            "Chi Zhang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Recent advances in text-to-image (T2I) generation have led to impressive visual results. However, these models still face significant challenges when handling complex prompt, particularly those involving multiple subjects with distinct attributes. Inspired by the human drawing process, which first outlines the composition and then incrementally adds details, we propose Detail++, a training-free framework that introduces a novel Progressive Detail Injection (PDI) strategy to address this limitation. Specifically, we decompose a complex prompt into a sequence of simplified sub-prompts, guiding the generation process in stages. This staged generation leverages the inherent layout-controlling capacity of self-attention to first ensure global composition, followed by precise refinement. To achieve accurate binding between attributes and corresponding subjects, we exploit cross-attention mechanisms and further introduce a Centroid Alignment Loss at test time to reduce binding noise and enhance attribute consistency. Extensive experiments on T2I-CompBench and a newly constructed style composition benchmark demonstrate that Detail++ significantly outperforms existing methods, particularly in scenarios involving multiple objects and complex stylistic conditions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 4,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.17859",
        "abs_url": "https://arxiv.org/abs/2507.17859",
        "pdf_url": "https://arxiv.org/pdf/2507.17859",
        "title": "FishDet-M: A Unified Large-Scale Benchmark for Robust Fish Detection and CLIP-Guided Model Selection in Diverse Aquatic Visual Domains",
        "authors": [
            "Muayad Abujabal",
            "Lyes Saad Saoud",
            "Irfan Hussain"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
        "abstract": "Accurate fish detection in underwater imagery is essential for ecological monitoring, aquaculture automation, and robotic perception. However, practical deployment remains limited by fragmented datasets, heterogeneous imaging conditions, and inconsistent evaluation protocols. To address these gaps, we present \\textit{FishDet-M}, the largest unified benchmark for fish detection, comprising 13 publicly available datasets spanning diverse aquatic environments including marine, brackish, occluded, and aquarium scenes. All data are harmonized using COCO-style annotations with both bounding boxes and segmentation masks, enabling consistent and scalable cross-domain evaluation. We systematically benchmark 28 contemporary object detection models, covering the YOLOv8 to YOLOv12 series, R-CNN based detectors, and DETR based models. Evaluations are conducted using standard metrics including mAP, mAP@50, and mAP@75, along with scale-specific analyses (AP$_S$, AP$_M$, AP$_L$) and inference profiling in terms of latency and parameter count. The results highlight the varying detection performance across models trained on FishDet-M, as well as the trade-off between accuracy and efficiency across models of different architectures. To support adaptive deployment, we introduce a CLIP-based model selection framework that leverages vision-language alignment to dynamically identify the most semantically appropriate detector for each input image. This zero-shot selection strategy achieves high performance without requiring ensemble computation, offering a scalable solution for real-time applications. FishDet-M establishes a standardized and reproducible platform for evaluating object detection in complex aquatic scenes. All datasets, pretrained models, and evaluation tools are publicly available to facilitate future research in underwater computer vision and intelligent marine systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 5,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.17860",
        "abs_url": "https://arxiv.org/abs/2507.17860",
        "pdf_url": "https://arxiv.org/pdf/2507.17860",
        "title": "Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis",
        "authors": [
            "Ko Watanabe. Stanislav Frolov. Adriano Lucieri. Andreas Dengel"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Recent advancements in Deep Learning and its application on the edge hold great potential for the revolution of routine screenings for skin cancers like Melanoma. Along with the anticipated benefits of this technology, potential dangers arise from unforseen and inherent biases. Thus, assessing and improving the fairness of such systems is of utmost importance. A key challenge in fairness assessment is to ensure that the evaluation dataset is sufficiently representative of different Personal Identifiable Information (PII) (sex, age, and race) and other minority groups. Against the backdrop of this challenge, this study leverages the state-of-the-art Generative AI (GenAI) LightningDiT model to assess the fairness of publicly available melanoma classifiers. The results suggest that fairness assessment using highly realistic synthetic data is a promising direction. Yet, our findings indicate that verifying fairness becomes difficult when the melanoma-detection model used for evaluation is trained on data that differ from the dataset underpinning the synthetic images. Nonetheless, we propose that our approach offers a valuable new avenue for employing synthetic data to gauge and enhance fairness in medical-imaging GenAI systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 6,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.17892",
        "abs_url": "https://arxiv.org/abs/2507.17892",
        "pdf_url": "https://arxiv.org/pdf/2507.17892",
        "title": "DiNAT-IR: Exploring Dilated Neighborhood Attention for High-Quality Image Restoration",
        "authors": [
            "Hanzhou Liu",
            "Binghan Li",
            "Chengkai Liu",
            "Mi Lu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Transformers, with their self-attention mechanisms for modeling long-range dependencies, have become a dominant paradigm in image restoration tasks. However, the high computational cost of self-attention limits scalability to high-resolution images, making efficiency-quality trade-offs a key research focus. To address this, Restormer employs channel-wise self-attention, which computes attention across channels instead of spatial dimensions. While effective, this approach may overlook localized artifacts that are crucial for high-quality image restoration. To bridge this gap, we explore Dilated Neighborhood Attention (DiNA) as a promising alternative, inspired by its success in high-level vision tasks. DiNA balances global context and local precision by integrating sliding-window attention with mixed dilation factors, effectively expanding the receptive field without excessive overhead. However, our preliminary experiments indicate that directly applying this global-local design to the classic deblurring task hinders accurate visual restoration, primarily due to the constrained global context understanding within local attention. To address this, we introduce a channel-aware module that complements local attention, effectively integrating global context without sacrificing pixel-level precision. The proposed DiNAT-IR, a Transformer-based architecture specifically designed for image restoration, achieves competitive results across multiple benchmarks, offering a high-quality solution for diverse low-level computer vision problems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 7,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.17957",
        "abs_url": "https://arxiv.org/abs/2507.17957",
        "pdf_url": "https://arxiv.org/pdf/2507.17957",
        "title": "AFRDA: Attentive Feature Refinement for Domain Adaptive Semantic Segmentation",
        "authors": [
            "Md. Al-Masrur Khan",
            "Durgakant Pushp",
            "Lantao Liu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In Unsupervised Domain Adaptive Semantic Segmentation (UDA-SS), a model is trained on labeled source domain data (e.g., synthetic images) and adapted to an unlabeled target domain (e.g., real-world images) without access to target annotations. Existing UDA-SS methods often struggle to balance fine-grained local details with global contextual information, leading to segmentation errors in complex regions. To address this, we introduce the Adaptive Feature Refinement (AFR) module, which enhances segmentation accuracy by refining highresolution features using semantic priors from low-resolution logits. AFR also integrates high-frequency components, which capture fine-grained structures and provide crucial boundary information, improving object delineation. Additionally, AFR adaptively balances local and global information through uncertaintydriven attention, reducing misclassifications. Its lightweight design allows seamless integration into HRDA-based UDA methods, leading to state-of-the-art segmentation performance. Our approach improves existing UDA-SS methods by 1.05% mIoU on GTA V --> Cityscapes and 1.04% mIoU on Synthia-->Cityscapes. The implementation of our framework is available at: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 8,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.17959",
        "abs_url": "https://arxiv.org/abs/2507.17959",
        "pdf_url": "https://arxiv.org/pdf/2507.17959",
        "title": "OPEN: A Benchmark Dataset and Baseline for Older Adult Patient Engagement Recognition in Virtual Rehabilitation Learning Environments",
        "authors": [
            "Ali Abedi",
            "Sadaf Safa",
            "Tracey J.F. Colella",
            "Shehroz S. Khan"
        ],
        "comments": "14 pages, 3 figures, 7 tables",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Engagement in virtual learning is essential for participant satisfaction, performance, and adherence, particularly in online education and virtual rehabilitation, where interactive communication plays a key role. Yet, accurately measuring engagement in virtual group settings remains a challenge. There is increasing interest in using artificial intelligence (AI) for large-scale, real-world, automated engagement recognition. While engagement has been widely studied in younger academic populations, research and datasets focused on older adults in virtual and telehealth learning settings remain limited. Existing methods often neglect contextual relevance and the longitudinal nature of engagement across sessions. This paper introduces OPEN (Older adult Patient ENgagement), a novel dataset supporting AI-driven engagement recognition. It was collected from eleven older adults participating in weekly virtual group learning sessions over six weeks as part of cardiac rehabilitation, producing over 35 hours of data, making it the largest dataset of its kind. To protect privacy, raw video is withheld; instead, the released data include facial, hand, and body joint landmarks, along with affective and behavioral features extracted from video. Annotations include binary engagement states, affective and behavioral labels, and context-type indicators, such as whether the instructor addressed the group or an individual. The dataset offers versions with 5-, 10-, 30-second, and variable-length samples. To demonstrate utility, multiple machine learning and deep learning models were trained, achieving engagement recognition accuracy of up to 81 percent. OPEN provides a scalable foundation for personalized engagement modeling in aging populations and contributes to broader engagement recognition research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 9,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.17987",
        "abs_url": "https://arxiv.org/abs/2507.17987",
        "pdf_url": "https://arxiv.org/pdf/2507.17987",
        "title": "Bearded Dragon Activity Recognition Pipeline: An AI-Based Approach to Behavioural Monitoring",
        "authors": [
            "Arsen Yermukan",
            "Pedro Machado",
            "Feliciano Domingos",
            "Isibor Kennedy Ihianle",
            "Jordan J. Bird",
            "Stefano S. K. Kaburu",
            "Samantha J. Ward"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Traditional monitoring of bearded dragon (Pogona Viticeps) behaviour is time-consuming and prone to errors. This project introduces an automated system for real-time video analysis, using You Only Look Once (YOLO) object detection models to identify two key behaviours: basking and hunting. We trained five YOLO variants (v5, v7, v8, v11, v12) on a custom, publicly available dataset of 1200 images, encompassing bearded dragons (600), heating lamps (500), and crickets (100). YOLOv8s was selected as the optimal model due to its superior balance of accuracy (mAP@0.5:0.95 = 0.855) and speed. The system processes video footage by extracting per-frame object coordinates, applying temporal interpolation for continuity, and using rule-based logic to classify specific behaviours. Basking detection proved reliable. However, hunting detection was less accurate, primarily due to weak cricket detection (mAP@0.5 = 0.392). Future improvements will focus on enhancing cricket detection through expanded datasets or specialised small-object detectors. This automated system offers a scalable solution for monitoring reptile behaviour in controlled environments, significantly improving research efficiency and data quality.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 10,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.17995",
        "abs_url": "https://arxiv.org/abs/2507.17995",
        "pdf_url": "https://arxiv.org/pdf/2507.17995",
        "title": "AG-VPReID.VIR: Bridging Aerial and Ground Platforms for Video-based Visible-Infrared Person Re-ID",
        "authors": [
            "Huy Nguyen",
            "Kien Nguyen",
            "Akila Pemasiri",
            "Akmal Jahan",
            "Clinton Fookes",
            "Sridha Sridharan"
        ],
        "comments": "Accepted atIEEE International Joint Conference on Biometrics (IJCB) 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Person re-identification (Re-ID) across visible and infrared modalities is crucial for 24-hour surveillance systems, but existing datasets primarily focus on ground-level perspectives. While ground-based IR systems offer nighttime capabilities, they suffer from occlusions, limited coverage, and vulnerability to obstructions--problems that aerial perspectives uniquely solve. To address these limitations, we introduce this http URL, the first aerial-ground cross-modality video-based person Re-ID dataset. This dataset captures 1,837 identities across 4,861 tracklets (124,855 frames) using both UAV-mounted and fixed CCTV cameras in RGB and infrared modalities. this http URL presents unique challenges including cross-viewpoint variations, modality discrepancies, and temporal dynamics. Additionally, we propose TCC-VPReID, a novel three-stream architecture designed to address the joint challenges of cross-platform and cross-modality person Re-ID. Our approach bridges the domain gaps between aerial-ground perspectives and RGB-IR modalities, through style-robust feature learning, memory-based cross-view adaptation, and intermediary-guided temporal modeling. Experiments show that this http URL presents distinctive challenges compared to existing datasets, with our TCC-VPReID framework achieving significant performance gains across multiple evaluation protocols. Dataset and code are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 11,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.17996",
        "abs_url": "https://arxiv.org/abs/2507.17996",
        "pdf_url": "https://arxiv.org/pdf/2507.17996",
        "title": "Exploring the interplay of label bias with subgroup size and separability: A case study in mammographic density classification",
        "authors": [
            "Emma A.M. Stanley",
            "Raghav Mehta",
            "MÃ©lanie Roschewitz",
            "Nils D. Forkert",
            "Ben Glocker"
        ],
        "comments": "Accepted at MICCAI Workshop on Fairness of AI in Medical Imaging (FAIMI) 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Systematic mislabelling affecting specific subgroups (i.e., label bias) in medical imaging datasets represents an understudied issue concerning the fairness of medical AI systems. In this work, we investigated how size and separability of subgroups affected by label bias influence the learned features and performance of a deep learning model. Therefore, we trained deep learning models for binary tissue density classification using the EMory BrEast imaging Dataset (EMBED), where label bias affected separable subgroups (based on imaging manufacturer) or non-separable \"pseudo-subgroups\". We found that simulated subgroup label bias led to prominent shifts in the learned feature representations of the models. Importantly, these shifts within the feature space were dependent on both the relative size and the separability of the subgroup affected by label bias. We also observed notable differences in subgroup performance depending on whether a validation set with clean labels was used to define the classification threshold for the model. For instance, with label bias affecting the majority separable subgroup, the true positive rate for that subgroup fell from 0.898, when the validation set had clean labels, to 0.518, when the validation set had biased labels. Our work represents a key contribution toward understanding the consequences of label bias on subgroup fairness in medical imaging AI.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 12,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.17998",
        "abs_url": "https://arxiv.org/abs/2507.17998",
        "pdf_url": "https://arxiv.org/pdf/2507.17998",
        "title": "Registration beyond Points: General Affine Subspace Alignment via Geodesic Distance on Grassmann Manifold",
        "authors": [
            "Jaeho Shin",
            "Hyeonjae Gil",
            "Junwoo Jang",
            "Maani Ghaffari",
            "Ayoung Kim"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Affine Grassmannian has been favored for expressing proximity between lines and planes due to its theoretical exactness in measuring distances among features. Despite this advantage, the existing method can only measure the proximity without yielding the distance as an explicit function of rigid body transformation. Thus, an optimizable distance function on the manifold has remained underdeveloped, stifling its application in registration problems. This paper is the first to explicitly derive an optimizable cost function between two Grassmannian features with respect to rigid body transformation ($\\mathbf{R}$ and $\\mathbf{t}$). Specifically, we present a rigorous mathematical proof demonstrating that the bases of high-dimensional linear subspaces can serve as an explicit representation of the cost. Finally, we propose an optimizable cost function based on the transformed bases that can be applied to the registration problem of any affine subspace. Compared to vector parameter-based approaches, our method is able to find a globally optimal solution by directly minimizing the geodesic distance which is agnostic to representation ambiguity. The resulting cost function and its extension to the inlier-set maximizing \\ac{BnB} solver have been demonstrated to improve the convergence of existing solutions or outperform them in various computer vision tasks. The code is available on this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 13,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18009",
        "abs_url": "https://arxiv.org/abs/2507.18009",
        "pdf_url": "https://arxiv.org/pdf/2507.18009",
        "title": "GRR-CoCa: Leveraging LLM Mechanisms in Multimodal Model Architectures",
        "authors": [
            "Jake R. Patock",
            "Nicole Catherine Lewis",
            "Kevin McCoy",
            "Christina Gomez",
            "Canling Chen",
            "Lorenzo Luzi"
        ],
        "comments": "12 pages, 2 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "State-of-the-art (SOTA) image and text generation models are multimodal models that have many similarities to large language models (LLMs). Despite achieving strong performances, leading foundational multimodal model architectures frequently lag behind the architectural sophistication of contemporary LLMs. We propose GRR-CoCa, an improved SOTA Contrastive Captioner (CoCa) model that incorporates Gaussian error gated linear units, root mean squared normalization, and rotary positional embedding into the textual decoders and the vision transformer (ViT) encoder. Each architectural modification has been shown to improve model performance in LLMs, but has yet to be adopted in CoCa. We benchmarked GRR-CoCa against Baseline CoCa, a model with the same modified textual decoders but with CoCa's original ViT encoder. We used standard pretraining and fine-tuning workflows to benchmark the models on contrastive and generative tasks. Our GRR-CoCa significantly outperformed Baseline CoCa on the pretraining dataset and three diverse fine-tuning datasets. Pretraining improvements were 27.25% in contrastive loss, 3.71% in perplexity, and 7.15% in CoCa loss. The average fine-tuning improvements were 13.66% in contrastive loss, 5.18% in perplexity, and 5.55% in CoCa loss. We show that GRR-CoCa's modified architecture improves performance and generalization across vision-language domains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 14,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18015",
        "abs_url": "https://arxiv.org/abs/2507.18015",
        "pdf_url": "https://arxiv.org/pdf/2507.18015",
        "title": "Celeb-DF++: A Large-scale Challenging Video DeepFake Benchmark for Generalizable Forensics",
        "authors": [
            "Yuezun Li",
            "Delong Zhu",
            "Xinjie Cui",
            "Siwei Lyu"
        ],
        "comments": "this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The rapid advancement of AI technologies has significantly increased the diversity of DeepFake videos circulating online, posing a pressing challenge for \\textit{generalizable forensics}, \\ie, detecting a wide range of unseen DeepFake types using a single model. Addressing this challenge requires datasets that are not only large-scale but also rich in forgery diversity. However, most existing datasets, despite their scale, include only a limited variety of forgery types, making them insufficient for developing generalizable detection methods. Therefore, we build upon our earlier Celeb-DF dataset and introduce {Celeb-DF++}, a new large-scale and challenging video DeepFake benchmark dedicated to the generalizable forensics challenge. Celeb-DF++ covers three commonly encountered forgery scenarios: Face-swap (FS), Face-reenactment (FR), and Talking-face (TF). Each scenario contains a substantial number of high-quality forged videos, generated using a total of 22 various recent DeepFake methods. These methods differ in terms of architectures, generation pipelines, and targeted facial regions, covering the most prevalent DeepFake cases witnessed in the wild. We also introduce evaluation protocols for measuring the generalizability of 24 recent detection methods, highlighting the limitations of existing detection methods and the difficulty of our new dataset.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 15,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18023",
        "abs_url": "https://arxiv.org/abs/2507.18023",
        "pdf_url": "https://arxiv.org/pdf/2507.18023",
        "title": "High-fidelity 3D Gaussian Inpainting: preserving multi-view consistency and photorealistic details",
        "authors": [
            "Jun Zhou",
            "Dinghao Li",
            "Nannan Li",
            "Mingjie Wang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Recent advancements in multi-view 3D reconstruction and novel-view synthesis, particularly through Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have greatly enhanced the fidelity and efficiency of 3D content creation. However, inpainting 3D scenes remains a challenging task due to the inherent irregularity of 3D structures and the critical need for maintaining multi-view consistency. In this work, we propose a novel 3D Gaussian inpainting framework that reconstructs complete 3D scenes by leveraging sparse inpainted views. Our framework incorporates an automatic Mask Refinement Process and region-wise Uncertainty-guided Optimization. Specifically, we refine the inpainting mask using a series of operations, including Gaussian scene filtering and back-projection, enabling more accurate localization of occluded regions and realistic boundary restoration. Furthermore, our Uncertainty-guided Fine-grained Optimization strategy, which estimates the importance of each region across multi-view images during training, alleviates multi-view inconsistencies and enhances the fidelity of fine details in the inpainted results. Comprehensive experiments conducted on diverse datasets demonstrate that our approach outperforms existing state-of-the-art methods in both visual quality and view consistency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 16,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18026",
        "abs_url": "https://arxiv.org/abs/2507.18026",
        "pdf_url": "https://arxiv.org/pdf/2507.18026",
        "title": "Emotion Recognition from Skeleton Data: A Comprehensive Survey",
        "authors": [
            "Haifeng Lu",
            "Jiuyi Chen",
            "Zhen Zhang",
            "Ruida Liu",
            "Runhao Zeng",
            "Xiping Hu"
        ],
        "comments": "34 pages, 5 figures, 13 tables",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Emotion recognition through body movements has emerged as a compelling and privacy-preserving alternative to traditional methods that rely on facial expressions or physiological signals. Recent advancements in 3D skeleton acquisition technologies and pose estimation algorithms have significantly enhanced the feasibility of emotion recognition based on full-body motion. This survey provides a comprehensive and systematic review of skeleton-based emotion recognition techniques. First, we introduce psychological models of emotion and examine the relationship between bodily movements and emotional expression. Next, we summarize publicly available datasets, highlighting the differences in data acquisition methods and emotion labeling strategies. We then categorize existing methods into posture-based and gait-based approaches, analyzing them from both data-driven and technical perspectives. In particular, we propose a unified taxonomy that encompasses four primary technical paradigms: Traditional approaches, Feat2Net, FeatFusionNet, and End2EndNet. Representative works within each category are reviewed and compared, with benchmarking results across commonly used datasets. Finally, we explore the extended applications of emotion recognition in mental health assessment, such as detecting depression and autism, and discuss the open challenges and future research directions in this rapidly evolving field.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 17,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18031",
        "abs_url": "https://arxiv.org/abs/2507.18031",
        "pdf_url": "https://arxiv.org/pdf/2507.18031",
        "title": "ViGText: Deepfake Image Detection with Vision-Language Model Explanations and Graph Neural Networks",
        "authors": [
            "Ahmad ALBarqawi",
            "Mahmoud Nazzal",
            "Issa Khalil",
            "Abdallah Khreishah",
            "NhatHai Phan"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "The rapid rise of deepfake technology, which produces realistic but fraudulent digital content, threatens the authenticity of media. Traditional deepfake detection approaches often struggle with sophisticated, customized deepfakes, especially in terms of generalization and robustness against malicious attacks. This paper introduces ViGText, a novel approach that integrates images with Vision Large Language Model (VLLM) Text explanations within a Graph-based framework to improve deepfake detection. The novelty of ViGText lies in its integration of detailed explanations with visual data, as it provides a more context-aware analysis than captions, which often lack specificity and fail to reveal subtle inconsistencies. ViGText systematically divides images into patches, constructs image and text graphs, and integrates them for analysis using Graph Neural Networks (GNNs) to identify deepfakes. Through the use of multi-level feature extraction across spatial and frequency domains, ViGText captures details that enhance its robustness and accuracy to detect sophisticated deepfakes. Extensive experiments demonstrate that ViGText significantly enhances generalization and achieves a notable performance boost when it detects user-customized deepfakes. Specifically, average F1 scores rise from 72.45% to 98.32% under generalization evaluation, and reflects the model's superior ability to generalize to unseen, fine-tuned variations of stable diffusion models. As for robustness, ViGText achieves an increase of 11.1% in recall compared to other deepfake detection approaches. When facing targeted attacks that exploit its graph-based architecture, ViGText limits classification performance degradation to less than 4%. ViGText uses detailed visual and textual analysis to set a new standard for detecting deepfakes, helping ensure media authenticity and information integrity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 18,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18046",
        "abs_url": "https://arxiv.org/abs/2507.18046",
        "pdf_url": "https://arxiv.org/pdf/2507.18046",
        "title": "Enhancing Scene Transition Awareness in Video Generation via Post-Training",
        "authors": [
            "Hanwen Shen",
            "Jiajie Lu",
            "Yupeng Cao",
            "Xiaonan Yang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Recent advances in AI-generated video have shown strong performance on \\emph{text-to-video} tasks, particularly for short clips depicting a single scene. However, current models struggle to generate longer videos with coherent scene transitions, primarily because they cannot infer when a transition is needed from the prompt. Most open-source models are trained on datasets consisting of single-scene video clips, which limits their capacity to learn and respond to prompts requiring multiple scenes. Developing scene transition awareness is essential for multi-scene generation, as it allows models to identify and segment videos into distinct clips by accurately detecting transitions. To address this, we propose the \\textbf{Transition-Aware Video} (TAV) dataset, which consists of preprocessed video clips with multiple scene transitions. Our experiment shows that post-training on the \\textbf{TAV} dataset improves prompt-based scene transition understanding, narrows the gap between required and generated scenes, and maintains image quality.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 19,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18060",
        "abs_url": "https://arxiv.org/abs/2507.18060",
        "pdf_url": "https://arxiv.org/pdf/2507.18060",
        "title": "BokehDiff: Neural Lens Blur with One-Step Diffusion",
        "authors": [
            "Chengxuan Zhu",
            "Qingnan Fan",
            "Qi Zhang",
            "Jinwei Chen",
            "Huaqi Zhang",
            "Chao Xu",
            "Boxin Shi"
        ],
        "comments": "Accepted by ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We introduce BokehDiff, a novel lens blur rendering method that achieves physically accurate and visually appealing outcomes, with the help of generative diffusion prior. Previous methods are bounded by the accuracy of depth estimation, generating artifacts in depth discontinuities. Our method employs a physics-inspired self-attention module that aligns with the image formation process, incorporating depth-dependent circle of confusion constraint and self-occlusion effects. We adapt the diffusion model to the one-step inference scheme without introducing additional noise, and achieve results of high quality and fidelity. To address the lack of scalable paired data, we propose to synthesize photorealistic foregrounds with transparency with diffusion models, balancing authenticity and scene diversity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 20,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18064",
        "abs_url": "https://arxiv.org/abs/2507.18064",
        "pdf_url": "https://arxiv.org/pdf/2507.18064",
        "title": "Adapting Large VLMs with Iterative and Manual Instructions for Generative Low-light Enhancement",
        "authors": [
            "Xiaoran Sun",
            "Liyan Wang",
            "Cong Wang",
            "Yeying Jin",
            "Kin-man Lam",
            "Zhixun Su",
            "Yang Yang",
            "Jinshan Pan"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Most existing low-light image enhancement (LLIE) methods rely on pre-trained model priors, low-light inputs, or both, while neglecting the semantic guidance available from normal-light images. This limitation hinders their effectiveness in complex lighting conditions. In this paper, we propose VLM-IMI, a novel framework that leverages large vision-language models (VLMs) with iterative and manual instructions (IMIs) for LLIE. VLM-IMI incorporates textual descriptions of the desired normal-light content as enhancement cues, enabling semantically informed restoration. To effectively integrate cross-modal priors, we introduce an instruction prior fusion module, which dynamically aligns and fuses image and text features, promoting the generation of detailed and semantically coherent outputs. During inference, we adopt an iterative and manual instruction strategy to refine textual instructions, progressively improving visual quality. This refinement enhances structural fidelity, semantic alignment, and the recovery of fine details under extremely low-light conditions. Extensive experiments across diverse scenarios demonstrate that VLM-IMI outperforms state-of-the-art methods in both quantitative metrics and perceptual quality. The source code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 21,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18082",
        "abs_url": "https://arxiv.org/abs/2507.18082",
        "pdf_url": "https://arxiv.org/pdf/2507.18082",
        "title": "TextSAM-EUS: Text Prompt Learning for SAM to Accurately Segment Pancreatic Tumor in Endoscopic Ultrasound",
        "authors": [
            "Pascal Spiegler",
            "Taha Koleilat",
            "Arash Harirpoush",
            "Corey S. Miller",
            "Hassan Rivaz",
            "Marta Kersten-Oertel",
            "Yiming Xiao"
        ],
        "comments": "Accepted to ICCV 2025 Workshop CVAMD",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Pancreatic cancer carries a poor prognosis and relies on endoscopic ultrasound (EUS) for targeted biopsy and radiotherapy. However, the speckle noise, low contrast, and unintuitive appearance of EUS make segmentation of pancreatic tumors with fully supervised deep learning (DL) models both error-prone and dependent on large, expert-curated annotation datasets. To address these challenges, we present TextSAM-EUS, a novel, lightweight, text-driven adaptation of the Segment Anything Model (SAM) that requires no manual geometric prompts at inference. Our approach leverages text prompt learning (context optimization) through the BiomedCLIP text encoder in conjunction with a LoRA-based adaptation of SAM's architecture to enable automatic pancreatic tumor segmentation in EUS, tuning only 0.86% of the total parameters. On the public Endoscopic Ultrasound Database of the Pancreas, TextSAM-EUS with automatic prompts attains 82.69% Dice and 85.28% normalized surface distance (NSD), and with manual geometric prompts reaches 83.10% Dice and 85.70% NSD, outperforming both existing state-of-the-art (SOTA) supervised DL models and foundation models (e.g., SAM and its variants). As the first attempt to incorporate prompt learning in SAM-based medical image segmentation, TextSAM-EUS offers a practical option for efficient and robust automatic EUS segmentation. Our code will be publicly available upon acceptance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 22,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18099",
        "abs_url": "https://arxiv.org/abs/2507.18099",
        "pdf_url": "https://arxiv.org/pdf/2507.18099",
        "title": "Comparison of Segmentation Methods in Remote Sensing for Land Use Land Cover",
        "authors": [
            "Naman Srivastava",
            "Joel D Joy",
            "Yash Dixit",
            "Swarup E",
            "Rakshit Ramesh"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Land Use Land Cover (LULC) mapping is essential for urban and resource planning, and is one of the key elements in developing smart and sustainable this http URL study evaluates advanced LULC mapping techniques, focusing on Look-Up Table (LUT)-based Atmospheric Correction applied to Cartosat Multispectral (MX) sensor images, followed by supervised and semi-supervised learning models for LULC prediction. We explore DeeplabV3+ and Cross-Pseudo Supervision (CPS). The CPS model is further refined with dynamic weighting, enhancing pseudo-label reliability during training. This comprehensive approach analyses the accuracy and utility of LULC mapping techniques for various urban planning applications. A case study of Hyderabad, India, illustrates significant land use changes due to rapid urbanization. By analyzing Cartosat MX images over time, we highlight shifts such as urban sprawl, shrinking green spaces, and expanding industrial areas. This demonstrates the practical utility of these techniques for urban planners and policymakers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 23,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18100",
        "abs_url": "https://arxiv.org/abs/2507.18100",
        "pdf_url": "https://arxiv.org/pdf/2507.18100",
        "title": "Datasets and Recipes for Video Temporal Grounding via Reinforcement Learning",
        "authors": [
            "Ruizhe Chen",
            "Zhiting Fan",
            "Tianze Luo",
            "Heqing Zou",
            "Zhaopeng Feng",
            "Guiyang Xie",
            "Hansheng Zhang",
            "Zhuochen Wang",
            "Zuozhu Liu",
            "Huaijian Zhang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Video Temporal Grounding (VTG) aims to localize relevant temporal segments in videos given natural language queries. Despite recent progress with large vision-language models (LVLMs) and instruction-tuning, existing approaches often suffer from limited temporal awareness and poor generalization. In this work, we introduce a two-stage training framework that integrates supervised fine-tuning with reinforcement learning (RL) to improve both the accuracy and robustness of VTG models. Our approach first leverages high-quality curated cold start data for SFT initialization, followed by difficulty-controlled RL to further enhance temporal localization and reasoning abilities. Comprehensive experiments on multiple VTG benchmarks demonstrate that our method consistently outperforms existing models, particularly in challenging and open-domain scenarios. We conduct an in-depth analysis of training strategies and dataset curation, highlighting the importance of both high-quality cold start data and difficulty-controlled RL. To facilitate further research and industrial adoption, we release all intermediate datasets, models, and code to the community.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 24,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18104",
        "abs_url": "https://arxiv.org/abs/2507.18104",
        "pdf_url": "https://arxiv.org/pdf/2507.18104",
        "title": "A Multimodal Seq2Seq Transformer for Predicting Brain Responses to Naturalistic Stimuli",
        "authors": [
            "Qianyi He",
            "Yuan Chang Leong"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Neurons and Cognition (q-bio.NC)",
        "abstract": "The Algonauts 2025 Challenge called on the community to develop encoding models that predict whole-brain fMRI responses to naturalistic multimodal movies. In this submission, we propose a sequence-to-sequence Transformer that autoregressively predicts fMRI activity from visual, auditory, and language inputs. Stimulus features were extracted using pretrained models including VideoMAE, HuBERT, Qwen, and BridgeTower. The decoder integrates information from prior brain states, current stimuli, and episode-level summaries via dual cross-attention mechanisms that attend to both perceptual information extracted from the stimulus as well as narrative information provided by high-level summaries of narrative content. One core innovation of our approach is the use of sequences of multimodal context to predict sequences of brain activity, enabling the model to capture long-range temporal structure in both stimuli and neural responses. Another is the combination of a shared encoder with partial subject-specific decoder, which leverages common structure across subjects while accounting for individual variability. Our model achieves strong performance on both in-distribution and out-of-distribution data, demonstrating the effectiveness of temporally-aware, multimodal sequence modeling for brain activity prediction. The code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 25,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18106",
        "abs_url": "https://arxiv.org/abs/2507.18106",
        "pdf_url": "https://arxiv.org/pdf/2507.18106",
        "title": "Distributional Uncertainty for Out-of-Distribution Detection",
        "authors": [
            "JinYoung Kim",
            "DaeUng Jo",
            "Kimin Yun",
            "Jeonghyo Song",
            "Youngjoon Yoo"
        ],
        "comments": "6 pages , 3 figures , IEEE International Conference on Advanced Visual and Signal-Based Systems",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Estimating uncertainty from deep neural networks is a widely used approach for detecting out-of-distribution (OoD) samples, which typically exhibit high predictive uncertainty. However, conventional methods such as Monte Carlo (MC) Dropout often focus solely on either model or data uncertainty, failing to align with the semantic objective of OoD detection. To address this, we propose the Free-Energy Posterior Network, a novel framework that jointly models distributional uncertainty and identifying OoD and misclassified regions using free energy. Our method introduces two key contributions: (1) a free-energy-based density estimator parameterized by a Beta distribution, which enables fine-grained uncertainty estimation near ambiguous or unseen regions; and (2) a loss integrated within a posterior network, allowing direct uncertainty estimation from learned parameters without requiring stochastic sampling. By integrating our approach with the residual prediction branch (RPL) framework, the proposed method goes beyond post-hoc energy thresholding and enables the network to learn OoD regions by leveraging the variance of the Beta distribution, resulting in a semantically meaningful and computationally efficient solution for uncertainty-aware segmentation. We validate the effectiveness of our method on challenging real-world benchmarks, including Fishyscapes, RoadAnomaly, and Segment-Me-If-You-Can.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 26,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18107",
        "abs_url": "https://arxiv.org/abs/2507.18107",
        "pdf_url": "https://arxiv.org/pdf/2507.18107",
        "title": "T2VWorldBench: A Benchmark for Evaluating World Knowledge in Text-to-Video Generation",
        "authors": [
            "Yubin Chen",
            "Xuyang Guo",
            "Zhenmei Shi",
            "Zhao Song",
            "Jiahao Zhang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Text-to-video (T2V) models have shown remarkable performance in generating visually reasonable scenes, while their capability to leverage world knowledge for ensuring semantic consistency and factual accuracy remains largely understudied. In response to this challenge, we propose T2VWorldBench, the first systematic evaluation framework for evaluating the world knowledge generation abilities of text-to-video models, covering 6 major categories, 60 subcategories, and 1,200 prompts across a wide range of domains, including physics, nature, activity, culture, causality, and object. To address both human preference and scalable evaluation, our benchmark incorporates both human evaluation and automated evaluation using vision-language models (VLMs). We evaluated the 10 most advanced text-to-video models currently available, ranging from open source to commercial models, and found that most models are unable to understand world knowledge and generate truly correct videos. These findings point out a critical gap in the capability of current text-to-video models to leverage world knowledge, providing valuable research opportunities and entry points for constructing models with robust capabilities for commonsense reasoning and factual generation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 27,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18135",
        "abs_url": "https://arxiv.org/abs/2507.18135",
        "pdf_url": "https://arxiv.org/pdf/2507.18135",
        "title": "Information Entropy-Based Framework for Quantifying Tortuosity in Meibomian Gland Uneven Atrophy",
        "authors": [
            "Kesheng Wang",
            "Xiaoyu Chen",
            "Chunlei He",
            "Fenfen Li",
            "Xinxin Yu",
            "Dexing Kong",
            "Shoujun Huang",
            "Qi Dai"
        ],
        "comments": "This manuscript contains 7 figures. All comments are welcome",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Information Theory (cs.IT)",
        "abstract": "In the medical image analysis field, precise quantification of curve tortuosity plays a critical role in the auxiliary diagnosis and pathological assessment of various diseases. In this study, we propose a novel framework for tortuosity quantification and demonstrate its effectiveness through the evaluation of meibomian gland atrophy uniformity,serving as a representative application scenario. We introduce an information entropy-based tortuosity quantification framework that integrates probability modeling with entropy theory and incorporates domain transformation of curve data. Unlike traditional methods such as curvature or arc-chord ratio, this approach evaluates the tortuosity of a target curve by comparing it to a designated reference curve. Consequently, it is more suitable for tortuosity assessment tasks in medical data where biologically plausible reference curves are available, providing a more robust and objective evaluation metric without relying on idealized straight-line comparisons. First, we conducted numerical simulation experiments to preliminarily assess the stability and validity of the method. Subsequently, the framework was applied to quantify the spatial uniformity of meibomian gland atrophy and to analyze the difference in this uniformity between \\textit{Demodex}-negative and \\textit{Demodex}-positive patient groups. The results demonstrated a significant difference in tortuosity-based uniformity between the two groups, with an area under the curve of 0.8768, sensitivity of 0.75, and specificity of 0.93. These findings highlight the clinical utility of the proposed framework in curve tortuosity analysis and its potential as a generalizable tool for quantitative morphological evaluation in medical diagnostics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 28,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18144",
        "abs_url": "https://arxiv.org/abs/2507.18144",
        "pdf_url": "https://arxiv.org/pdf/2507.18144",
        "title": "Degradation-Consistent Learning via Bidirectional Diffusion for Low-Light Image Enhancement",
        "authors": [
            "Jinhong He",
            "Minglong Xue",
            "Zhipu Liu",
            "Mingliang Zhou",
            "Aoxiang Ning",
            "Palaiahnakote Shivakumara"
        ],
        "comments": "10page",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)",
        "abstract": "Low-light image enhancement aims to improve the visibility of degraded images to better align with human visual perception. While diffusion-based methods have shown promising performance due to their strong generative capabilities. However, their unidirectional modelling of degradation often struggles to capture the complexity of real-world degradation patterns, leading to structural inconsistencies and pixel misalignments. To address these challenges, we propose a bidirectional diffusion optimization mechanism that jointly models the degradation processes of both low-light and normal-light images, enabling more precise degradation parameter matching and enhancing generation quality. Specifically, we perform bidirectional diffusion-from low-to-normal light and from normal-to-low light during training and introduce an adaptive feature interaction block (AFI) to refine feature representation. By leveraging the complementarity between these two paths, our approach imposes an implicit symmetry constraint on illumination attenuation and noise distribution, facilitating consistent degradation learning and improving the models ability to perceive illumination and detail degradation. Additionally, we design a reflection-aware correction module (RACM) to guide color restoration post-denoising and suppress overexposed regions, ensuring content consistency and generating high-quality images that align with human visual perception. Extensive experiments on multiple benchmark datasets demonstrate that our method outperforms state-of-the-art methods in both quantitative and qualitative evaluations while generalizing effectively to diverse degradation scenarios. Code at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 29,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18173",
        "abs_url": "https://arxiv.org/abs/2507.18173",
        "pdf_url": "https://arxiv.org/pdf/2507.18173",
        "title": "WaveMamba: Wavelet-Driven Mamba Fusion for RGB-Infrared Object Detection",
        "authors": [
            "Haodong Zhu",
            "Wenhao Dong",
            "Linlin Yang",
            "Hong Li",
            "Yuguang Yang",
            "Yangyang Ren",
            "Qingcheng Zhu",
            "Zichao Feng",
            "Changbai Li",
            "Shaohui Lin",
            "Runqi Wang",
            "Xiaoyan Luo",
            "Baochang Zhang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM)",
        "abstract": "Leveraging the complementary characteristics of visible (RGB) and infrared (IR) imagery offers significant potential for improving object detection. In this paper, we propose WaveMamba, a cross-modality fusion method that efficiently integrates the unique and complementary frequency features of RGB and IR decomposed by Discrete Wavelet Transform (DWT). An improved detection head incorporating the Inverse Discrete Wavelet Transform (IDWT) is also proposed to reduce information loss and produce the final detection results. The core of our approach is the introduction of WaveMamba Fusion Block (WMFB), which facilitates comprehensive fusion across low-/high-frequency sub-bands. Within WMFB, the Low-frequency Mamba Fusion Block (LMFB), built upon the Mamba framework, first performs initial low-frequency feature fusion with channel swapping, followed by deep fusion with an advanced gated attention mechanism for enhanced integration. High-frequency features are enhanced using a strategy that applies an ``absolute maximum\" fusion approach. These advancements lead to significant performance gains, with our method surpassing state-of-the-art approaches and achieving average mAP improvements of 4.5% on four benchmarks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 30,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18174",
        "abs_url": "https://arxiv.org/abs/2507.18174",
        "pdf_url": "https://arxiv.org/pdf/2507.18174",
        "title": "Real-Time Object Detection and Classification using YOLO for Edge FPGAs",
        "authors": [
            "Rashed Al Amin",
            "Roman Obermaisser"
        ],
        "comments": "This paper has been accepted for the 67th International Symposium on ELMAR 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Hardware Architecture (cs.AR)",
        "abstract": "Object detection and classification are crucial tasks across various application domains, particularly in the development of safe and reliable Advanced Driver Assistance Systems (ADAS). Existing deep learning-based methods such as Convolutional Neural Networks (CNNs), Single Shot Detectors (SSDs), and You Only Look Once (YOLO) have demonstrated high performance in terms of accuracy and computational speed when deployed on Field-Programmable Gate Arrays (FPGAs). However, despite these advances, state-of-the-art YOLO-based object detection and classification systems continue to face challenges in achieving resource efficiency suitable for edge FPGA platforms. To address this limitation, this paper presents a resource-efficient real-time object detection and classification system based on YOLOv5 optimized for FPGA deployment. The proposed system is trained on the COCO and GTSRD datasets and implemented on the Xilinx Kria KV260 FPGA board. Experimental results demonstrate a classification accuracy of 99%, with a power consumption of 3.5W and a processing speed of 9 frames per second (FPS). These findings highlight the effectiveness of the proposed approach in enabling real-time, resource-efficient object detection and classification for edge computing applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 31,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18176",
        "abs_url": "https://arxiv.org/abs/2507.18176",
        "pdf_url": "https://arxiv.org/pdf/2507.18176",
        "title": "Unsupervised Domain Adaptation for 3D LiDAR Semantic Segmentation Using Contrastive Learning and Multi-Model Pseudo Labeling",
        "authors": [
            "Abhishek Kaushik",
            "Norbert Haala",
            "Uwe Soergel"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Addressing performance degradation in 3D LiDAR semantic segmentation due to domain shifts (e.g., sensor type, geographical location) is crucial for autonomous systems, yet manual annotation of target data is prohibitive. This study addresses the challenge using Unsupervised Domain Adaptation (UDA) and introduces a novel two-stage framework to tackle it. Initially, unsupervised contrastive learning at the segment level is used to pre-train a backbone network, enabling it to learn robust, domain-invariant features without labels. Subsequently, a multi-model pseudo-labeling strategy is introduced, utilizing an ensemble of diverse state-of-the-art architectures (including projection, voxel, hybrid, and cylinder-based methods). Predictions from these models are aggregated via hard voting to generate high-quality, refined pseudo-labels for the unlabeled target domain, mitigating single-model biases. The contrastively pre-trained network is then fine-tuned using these robust pseudo-labels. Experiments adapting from SemanticKITTI to unlabeled target datasets (SemanticPOSS, SemanticSlamantic) demonstrate significant improvements in segmentation accuracy compared to direct transfer and single-model UDA approaches. These results highlight the effectiveness of combining contrastive pre-training with refined ensemble pseudo-labeling for bridging complex domain gaps without requiring target domain annotations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 32,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18177",
        "abs_url": "https://arxiv.org/abs/2507.18177",
        "pdf_url": "https://arxiv.org/pdf/2507.18177",
        "title": "Differential-UMamba: Rethinking Tumor Segmentation Under Limited Data Scenarios",
        "authors": [
            "Dhruv Jain",
            "Romain Modzelewski",
            "Romain HÃ©rault",
            "Clement Chatelain",
            "Eva Torfeh",
            "Sebastien Thureau"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "In data-scarce scenarios, deep learning models often overfit to noise and irrelevant patterns, which limits their ability to generalize to unseen samples. To address these challenges in medical image segmentation, we introduce Diff-UMamba, a novel architecture that combines the UNet framework with the mamba mechanism for modeling long-range dependencies. At the heart of Diff-UMamba is a Noise Reduction Module (NRM), which employs a signal differencing strategy to suppress noisy or irrelevant activations within the encoder. This encourages the model to filter out spurious features and enhance task-relevant representations, thereby improving its focus on clinically meaningful regions. As a result, the architecture achieves improved segmentation accuracy and robustness, particularly in low-data settings. Diff-UMamba is evaluated on multiple public datasets, including MSD (lung and pancreas) and AIIB23, demonstrating consistent performance gains of 1-3% over baseline methods across diverse segmentation tasks. To further assess performance under limited-data conditions, additional experiments are conducted on the BraTS-21 dataset by varying the proportion of available training samples. The approach is also validated on a small internal non-small cell lung cancer (NSCLC) dataset for gross tumor volume (GTV) segmentation in cone beam CT (CBCT), where it achieves a 4-5% improvement over the baseline.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 33,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18184",
        "abs_url": "https://arxiv.org/abs/2507.18184",
        "pdf_url": "https://arxiv.org/pdf/2507.18184",
        "title": "MatSSL: Robust Self-Supervised Representation Learning for Metallographic Image Segmentation",
        "authors": [
            "Hoang Hai Nam Nguyen",
            "Phan Nguyen Duc Hieu",
            "Ho Won Lee"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "MatSSL is a streamlined self-supervised learning (SSL) architecture that employs Gated Feature Fusion at each stage of the backbone to integrate multi-level representations effectively. Current micrograph analysis of metallic materials relies on supervised methods, which require retraining for each new dataset and often perform inconsistently with only a few labeled samples. While SSL offers a promising alternative by leveraging unlabeled data, most existing methods still depend on large-scale datasets to be effective. MatSSL is designed to overcome this limitation. We first perform self-supervised pretraining on a small-scale, unlabeled dataset and then fine-tune the model on multiple benchmark datasets. The resulting segmentation models achieve 69.13% mIoU on MetalDAM, outperforming the 66.73% achieved by an ImageNet-pretrained encoder, and delivers consistently up to nearly 40% improvement in average mIoU on the Environmental Barrier Coating benchmark dataset (EBC) compared to models pretrained with MicroNet. This suggests that MatSSL enables effective adaptation to the metallographic domain using only a small amount of unlabeled data, while preserving the rich and transferable features learned from large-scale pretraining on natural images.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 34,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18192",
        "abs_url": "https://arxiv.org/abs/2507.18192",
        "pdf_url": "https://arxiv.org/pdf/2507.18192",
        "title": "TeEFusion: Blending Text Embeddings to Distill Classifier-Free Guidance",
        "authors": [
            "Minghao Fu",
            "Guo-Hua Wang",
            "Xiaohao Chen",
            "Qing-Guo Chen",
            "Zhao Xu",
            "Weihua Luo",
            "Kaifu Zhang"
        ],
        "comments": "Accepted by ICCV 2025. The code is publicly available at this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Recent advances in text-to-image synthesis largely benefit from sophisticated sampling strategies and classifier-free guidance (CFG) to ensure high-quality generation. However, CFG's reliance on two forward passes, especially when combined with intricate sampling algorithms, results in prohibitively high inference costs. To address this, we introduce TeEFusion (\\textbf{Te}xt \\textbf{E}mbeddings \\textbf{Fusion}), a novel and efficient distillation method that directly incorporates the guidance magnitude into the text embeddings and distills the teacher model's complex sampling strategy. By simply fusing conditional and unconditional text embeddings using linear operations, TeEFusion reconstructs the desired guidance without adding extra parameters, simultaneously enabling the student model to learn from the teacher's output produced via its sophisticated sampling approach. Extensive experiments on state-of-the-art models such as SD3 demonstrate that our method allows the student to closely mimic the teacher's performance with a far simpler and more efficient sampling strategy. Consequently, the student model achieves inference speeds up to 6$\\times$ faster than the teacher model, while maintaining image quality at levels comparable to those obtained through the teacher's complex sampling approach. The code is publicly available at \\href{this https URL}{this http URL}.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 35,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18214",
        "abs_url": "https://arxiv.org/abs/2507.18214",
        "pdf_url": "https://arxiv.org/pdf/2507.18214",
        "title": "LEAF: Latent Diffusion with Efficient Encoder Distillation for Aligned Features in Medical Image Segmentation",
        "authors": [
            "Qilin Huang",
            "Tianyu Lin",
            "Zhiguang Chen",
            "Fudan Zheng"
        ],
        "comments": "Accepted at MICCAI 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Leveraging the powerful capabilities of diffusion models has yielded quite effective results in medical image segmentation tasks. However, existing methods typically transfer the original training process directly without specific adjustments for segmentation tasks. Furthermore, the commonly used pre-trained diffusion models still have deficiencies in feature extraction. Based on these considerations, we propose LEAF, a medical image segmentation model grounded in latent diffusion models. During the fine-tuning process, we replace the original noise prediction pattern with a direct prediction of the segmentation map, thereby reducing the variance of segmentation results. We also employ a feature distillation method to align the hidden states of the convolutional layers with the features from a transformer-based vision encoder. Experimental results demonstrate that our method enhances the performance of the original diffusion model across multiple segmentation datasets for different disease types. Notably, our approach does not alter the model architecture, nor does it increase the number of parameters or computation during the inference phase, making it highly efficient.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 36,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18225",
        "abs_url": "https://arxiv.org/abs/2507.18225",
        "pdf_url": "https://arxiv.org/pdf/2507.18225",
        "title": "3D Test-time Adaptation via Graph Spectral Driven Point Shift",
        "authors": [
            "Xin Wei",
            "Qin Yang",
            "Yijie Fang",
            "Mingrui Zhu",
            "Nannan Wang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "While test-time adaptation (TTA) methods effectively address domain shifts by dynamically adapting pre-trained models to target domain data during online inference, their application to 3D point clouds is hindered by their irregular and unordered structure. Current 3D TTA methods often rely on computationally expensive spatial-domain optimizations and may require additional training data. In contrast, we propose Graph Spectral Domain Test-Time Adaptation (GSDTTA), a novel approach for 3D point cloud classification that shifts adaptation to the graph spectral domain, enabling more efficient adaptation by capturing global structural properties with fewer parameters. Point clouds in target domain are represented as outlier-aware graphs and transformed into graph spectral domain by Graph Fourier Transform (GFT). For efficiency, adaptation is performed by optimizing only the lowest 10% of frequency components, which capture the majority of the point cloud's energy. An inverse GFT (IGFT) is then applied to reconstruct the adapted point cloud with the graph spectral-driven point shift. This process is enhanced by an eigenmap-guided self-training strategy that iteratively refines both the spectral adjustments and the model parameters. Experimental results and ablation studies on benchmark datasets demonstrate the effectiveness of GSDTTA, outperforming existing TTA methods for 3D point cloud classification.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 37,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18237",
        "abs_url": "https://arxiv.org/abs/2507.18237",
        "pdf_url": "https://arxiv.org/pdf/2507.18237",
        "title": "DATA: Domain-And-Time Alignment for High-Quality Feature Fusion in Collaborative Perception",
        "authors": [
            "Chengchang Tian",
            "Jianwei Ma",
            "Yan Huang",
            "Zhanye Chen",
            "Honghao Wei",
            "Hui Zhang",
            "Wei Hong"
        ],
        "comments": "ICCV 2025, accepted as poster. 22 pages including supplementary materials",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Feature-level fusion shows promise in collaborative perception (CP) through balanced performance and communication bandwidth trade-off. However, its effectiveness critically relies on input feature quality. The acquisition of high-quality features faces domain gaps from hardware diversity and deployment conditions, alongside temporal misalignment from transmission delays. These challenges degrade feature quality with cumulative effects throughout the collaborative network. In this paper, we present the Domain-And-Time Alignment (DATA) network, designed to systematically align features while maximizing their semantic representations for fusion. Specifically, we propose a Consistency-preserving Domain Alignment Module (CDAM) that reduces domain gaps through proximal-region hierarchical downsampling and observability-constrained discriminator. We further propose a Progressive Temporal Alignment Module (PTAM) to handle transmission delays via multi-scale motion modeling and two-stage compensation. Building upon the aligned features, an Instance-focused Feature Aggregation Module (IFAM) is developed to enhance semantic representations. Extensive experiments demonstrate that DATA achieves state-of-the-art performance on three typical datasets, maintaining robustness with severe communication delays and pose errors. The code will be released at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 38,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18243",
        "abs_url": "https://arxiv.org/abs/2507.18243",
        "pdf_url": "https://arxiv.org/pdf/2507.18243",
        "title": "DepthDark: Robust Monocular Depth Estimation for Low-Light Environments",
        "authors": [
            "Longjian Zeng",
            "Zunjie Zhu",
            "Rongfeng Lu",
            "Ming Lu",
            "Bolun Zheng",
            "Chenggang Yan",
            "Anke Xue"
        ],
        "comments": "Accepted by ACM MM 2025 conference",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "In recent years, foundation models for monocular depth estimation have received increasing attention. Current methods mainly address typical daylight conditions, but their effectiveness notably decreases in low-light environments. There is a lack of robust foundational models for monocular depth estimation specifically designed for low-light scenarios. This largely stems from the absence of large-scale, high-quality paired depth datasets for low-light conditions and the effective parameter-efficient fine-tuning (PEFT) strategy. To address these challenges, we propose DepthDark, a robust foundation model for low-light monocular depth estimation. We first introduce a flare-simulation module and a noise-simulation module to accurately simulate the imaging process under nighttime conditions, producing high-quality paired depth datasets for low-light conditions. Additionally, we present an effective low-light PEFT strategy that utilizes illumination guidance and multiscale feature fusion to enhance the model's capability in low-light environments. Our method achieves state-of-the-art depth estimation performance on the challenging nuScenes-Night and RobotCar-Night datasets, validating its effectiveness using limited training data and computing resources.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 39,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18255",
        "abs_url": "https://arxiv.org/abs/2507.18255",
        "pdf_url": "https://arxiv.org/pdf/2507.18255",
        "title": "LONG3R: Long Sequence Streaming 3D Reconstruction",
        "authors": [
            "Zhuoguang Chen",
            "Minghui Qin",
            "Tianyuan Yuan",
            "Zhe Liu",
            "Hang Zhao"
        ],
        "comments": "Accepted by ICCV 2025. Project page: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Recent advancements in multi-view scene reconstruction have been significant, yet existing methods face limitations when processing streams of input images. These methods either rely on time-consuming offline optimization or are restricted to shorter sequences, hindering their applicability in real-time scenarios. In this work, we propose LONG3R (LOng sequence streaming 3D Reconstruction), a novel model designed for streaming multi-view 3D scene reconstruction over longer sequences. Our model achieves real-time processing by operating recurrently, maintaining and updating memory with each new observation. We first employ a memory gating mechanism to filter relevant memory, which, together with a new observation, is fed into a dual-source refined decoder for coarse-to-fine interaction. To effectively capture long-sequence memory, we propose a 3D spatio-temporal memory that dynamically prunes redundant spatial information while adaptively adjusting resolution along the scene. To enhance our model's performance on long sequences while maintaining training efficiency, we employ a two-stage curriculum training strategy, each stage targeting specific capabilities. Experiments demonstrate that LONG3R outperforms state-of-the-art streaming methods, particularly for longer sequences, while maintaining real-time inference speed. Project page: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 40,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18260",
        "abs_url": "https://arxiv.org/abs/2507.18260",
        "pdf_url": "https://arxiv.org/pdf/2507.18260",
        "title": "Exploiting Gaussian Agnostic Representation Learning with Diffusion Priors for Enhanced Infrared Small Target Detection",
        "authors": [
            "Junyao Li",
            "Yahao Lu",
            "Xingyuan Guo",
            "Xiaoyu Xian",
            "Tiantian Wang",
            "Yukai Shi"
        ],
        "comments": "Submitted to Neural Networks. We propose the Gaussian Group Squeezer, leveraging Gaussian sampling and compression with diffusion models for channel-based data augmentation",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Infrared small target detection (ISTD) plays a vital role in numerous practical applications. In pursuit of determining the performance boundaries, researchers employ large and expensive manual-labeling data for representation learning. Nevertheless, this approach renders the state-of-the-art ISTD methods highly fragile in real-world challenges. In this paper, we first study the variation in detection performance across several mainstream methods under various scarcity -- namely, the absence of high-quality infrared data -- that challenge the prevailing theories about practical ISTD. To address this concern, we introduce the Gaussian Agnostic Representation Learning. Specifically, we propose the Gaussian Group Squeezer, leveraging Gaussian sampling and compression for non-uniform quantization. By exploiting a diverse array of training samples, we enhance the resilience of ISTD models against various challenges. Then, we introduce two-stage diffusion models for real-world reconstruction. By aligning quantized signals closely with real-world distributions, we significantly elevate the quality and fidelity of the synthetic samples. Comparative evaluations against state-of-the-art detection methods in various scarcity scenarios demonstrate the efficacy of the proposed approach.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 41,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18287",
        "abs_url": "https://arxiv.org/abs/2507.18287",
        "pdf_url": "https://arxiv.org/pdf/2507.18287",
        "title": "Dissecting the Dental Lung Cancer Axis via Mendelian Randomization and Mediation Analysis",
        "authors": [
            "Wenran Zhang",
            "Huihuan Luo",
            "Linda Wei",
            "Ping Nie",
            "Yiqun Wu",
            "Dedong Yu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Periodontitis and dental caries are common oral diseases affecting billions globally. While observational studies suggest links between these conditions and lung cancer, causality remains uncertain. This study used two sample Mendelian randomization (MR) to explore causal relationships between dental traits (periodontitis, dental caries) and lung cancer subtypes, and to assess mediation by pulmonary function. Genetic instruments were derived from the largest available genome wide association studies, including data from 487,823 dental caries and 506,594 periodontitis cases, as well as lung cancer data from the Transdisciplinary Research of Cancer in Lung consortium. Inverse variance weighting was the main analytical method; lung function mediation was assessed using the delta method. The results showed a significant positive causal effect of dental caries on overall lung cancer and its subtypes. Specifically, a one standard deviation increase in dental caries incidence was associated with a 188.0% higher risk of squamous cell lung carcinoma (OR = 2.880, 95% CI = 1.236--6.713, p = 0.014), partially mediated by declines in forced vital capacity (FVC) and forced expiratory volume in one second (FEV1), accounting for 5.124% and 5.890% of the total effect. No causal effect was found for periodontitis. These findings highlight a causal role of dental caries in lung cancer risk and support integrating dental care and pulmonary function monitoring into cancer prevention strategies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 42,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18300",
        "abs_url": "https://arxiv.org/abs/2507.18300",
        "pdf_url": "https://arxiv.org/pdf/2507.18300",
        "title": "LMM-Det: Make Large Multimodal Models Excel in Object Detection",
        "authors": [
            "Jincheng Li",
            "Chunyu Xie",
            "Ji Ao",
            "Dawei Leng",
            "Yuhui Yin"
        ],
        "comments": "Accepted at ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Large multimodal models (LMMs) have garnered wide-spread attention and interest within the artificial intelligence research and industrial communities, owing to their remarkable capability in multimodal understanding, reasoning, and in-context learning, among others. While LMMs have demonstrated promising results in tackling multimodal tasks like image captioning, visual question answering, and visual grounding, the object detection capabilities of LMMs exhibit a significant gap compared to specialist detectors. To bridge the gap, we depart from the conventional methods of integrating heavy detectors with LMMs and propose LMM-Det, a simple yet effective approach that leverages a Large Multimodal Model for vanilla object Detection without relying on specialized detection modules. Specifically, we conduct a comprehensive exploratory analysis when a large multimodal model meets with object detection, revealing that the recall rate degrades significantly compared with specialist detection models. To mitigate this, we propose to increase the recall rate by introducing data distribution adjustment and inference optimization tailored for object detection. We re-organize the instruction conversations to enhance the object detection capabilities of large multimodal models. We claim that a large multimodal model possesses detection capability without any extra detection modules. Extensive experiments support our claim and show the effectiveness of the versatile LMM-Det. The datasets, models, and codes are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 43,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18311",
        "abs_url": "https://arxiv.org/abs/2507.18311",
        "pdf_url": "https://arxiv.org/pdf/2507.18311",
        "title": "Improving Large Vision-Language Models' Understanding for Field Data",
        "authors": [
            "Xiaomei Zhang",
            "Hanyu Zheng",
            "Xiangyu Zhu",
            "Jinghuan Wei",
            "Junhong Zou",
            "Zhen Lei",
            "Zhaoxiang Zhang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Large Vision-Language Models (LVLMs) have shown impressive capabilities across a range of tasks that integrate visual and textual understanding, such as image captioning and visual question answering. These models are trained on large-scale image and video datasets paired with text, enabling them to bridge visual perception and natural language processing. However, their application to scientific domains, especially in interpreting complex field data commonly used in the natural sciences, remains underexplored. In this work, we introduce FieldLVLM, a novel framework designed to improve large vision-language models' understanding of field data. FieldLVLM consists of two main components: a field-aware language generation strategy and a data-compressed multimodal model tuning. The field-aware language generation strategy leverages a special-purpose machine learning pipeline to extract key physical features from field data, such as flow classification, Reynolds number, and vortex patterns. This information is then converted into structured textual descriptions that serve as a dataset. The data-compressed multimodal model tuning focuses on LVLMs with these generated datasets, using a data compression strategy to reduce the complexity of field inputs and retain only the most informative values. This ensures compatibility with the models language decoder and guides its learning more effectively. Experimental results on newly proposed benchmark datasets demonstrate that FieldLVLM significantly outperforms existing methods in tasks involving scientific field data. Our findings suggest that this approach opens up new possibilities for applying large vision-language models to scientific research, helping bridge the gap between large models and domain-specific discovery.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 44,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18323",
        "abs_url": "https://arxiv.org/abs/2507.18323",
        "pdf_url": "https://arxiv.org/pdf/2507.18323",
        "title": "A Multi-Dataset Benchmark for Semi-Supervised Semantic Segmentation in ECG Delineation",
        "authors": [
            "Minje Park",
            "Jeonghwa Lim",
            "Taehyung Yu",
            "Sunghoon Joo"
        ],
        "comments": "6 pages, 2 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Signal Processing (eess.SP)",
        "abstract": "Electrocardiogram (ECG) delineation, the segmentation of meaningful waveform features, is critical for clinical diagnosis. Despite recent advances using deep learning, progress has been limited by the scarcity of publicly available annotated datasets. Semi-supervised learning presents a promising solution by leveraging abundant unlabeled ECG data. In this study, we present the first systematic benchmark for semi-supervised semantic segmentation (SemiSeg) in ECG delineation. We curated and unified multiple public datasets, including previously underused sources, to support robust and diverse evaluation. We adopted five representative SemiSeg algorithms from computer vision, implemented them on two different architectures: the convolutional network and the transformer, and evaluated them in two different settings: in-domain and cross-domain. Additionally, we propose ECG-specific training configurations and augmentation strategies and introduce a standardized evaluation framework. Our results show that the transformer outperforms the convolutional network in semi-supervised ECG delineation. We anticipate that our benchmark will serve as a foundation for advancing semi-supervised ECG delineation methods and will facilitate further research in this domain.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 45,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18327",
        "abs_url": "https://arxiv.org/abs/2507.18327",
        "pdf_url": "https://arxiv.org/pdf/2507.18327",
        "title": "Beyond Low-rankness: Guaranteed Matrix Recovery via Modified Nuclear Norm",
        "authors": [
            "Jiangjun Peng",
            "Yisi Luo",
            "Xiangyong Cao",
            "Shuang Xu",
            "Deyu Meng"
        ],
        "comments": "15 pages, 14 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The nuclear norm (NN) has been widely explored in matrix recovery problems, such as Robust PCA and matrix completion, leveraging the inherent global low-rank structure of the data. In this study, we introduce a new modified nuclear norm (MNN) framework, where the MNN family norms are defined by adopting suitable transformations and performing the NN on the transformed matrix. The MNN framework offers two main advantages: (1) it jointly captures both local information and global low-rankness without requiring trade-off parameter tuning; (2) Under mild assumptions on the transformation, we provided exact theoretical recovery guarantees for both Robust PCA and MC tasks-an achievement not shared by existing methods that combine local and global information. Thanks to its general and flexible design, MNN can accommodate various proven transformations, enabling a unified and effective approach to structured low-rank recovery. Extensive experiments demonstrate the effectiveness of our method. Code and supplementary material are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 46,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18330",
        "abs_url": "https://arxiv.org/abs/2507.18330",
        "pdf_url": "https://arxiv.org/pdf/2507.18330",
        "title": "GVCCS: A Dataset for Contrail Identification and Tracking on Visible Whole Sky Camera Sequences",
        "authors": [
            "Gabriel Jarry",
            "Ramon Dalmau",
            "Philippe Very",
            "Franck Ballerini",
            "Stephania-Denisa Bocu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Aviation's climate impact includes not only CO2 emissions but also significant non-CO2 effects, especially from contrails. These ice clouds can alter Earth's radiative balance, potentially rivaling the warming effect of aviation CO2. Physics-based models provide useful estimates of contrail formation and climate impact, but their accuracy depends heavily on the quality of atmospheric input data and on assumptions used to represent complex processes like ice particle formation and humidity-driven persistence. Observational data from remote sensors, such as satellites and ground cameras, could be used to validate and calibrate these models. However, existing datasets don't explore all aspect of contrail dynamics and formation: they typically lack temporal tracking, and do not attribute contrails to their source flights. To address these limitations, we present the Ground Visible Camera Contrail Sequences (GVCCS), a new open data set of contrails recorded with a ground-based all-sky camera in the visible range. Each contrail is individually labeled and tracked over time, allowing a detailed analysis of its lifecycle. The dataset contains 122 video sequences (24,228 frames) and includes flight identifiers for contrails that form above the camera. As reference, we also propose a unified deep learning framework for contrail analysis using a panoptic segmentation model that performs semantic segmentation (contrail pixel identification), instance segmentation (individual contrail separation), and temporal tracking in a single architecture. By providing high-quality, temporally resolved annotations and a benchmark for model evaluation, our work supports improved contrail monitoring and will facilitate better calibration of physical models. This sets the groundwork for more accurate climate impact understanding and assessments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 47,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18331",
        "abs_url": "https://arxiv.org/abs/2507.18331",
        "pdf_url": "https://arxiv.org/pdf/2507.18331",
        "title": "Boosting Multi-View Indoor 3D Object Detection via Adaptive 3D Volume Construction",
        "authors": [
            "Runmin Zhang",
            "Zhu Yu",
            "Si-Yuan Cao",
            "Lingyu Zhu",
            "Guangyi Zhang",
            "Xiaokai Bai",
            "Hui-Liang Shen"
        ],
        "comments": "Accepted by ICCV2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "This work presents SGCDet, a novel multi-view indoor 3D object detection framework based on adaptive 3D volume construction. Unlike previous approaches that restrict the receptive field of voxels to fixed locations on images, we introduce a geometry and context aware aggregation module to integrate geometric and contextual information within adaptive regions in each image and dynamically adjust the contributions from different views, enhancing the representation capability of voxel features. Furthermore, we propose a sparse volume construction strategy that adaptively identifies and selects voxels with high occupancy probabilities for feature refinement, minimizing redundant computation in free space. Benefiting from the above designs, our framework achieves effective and efficient volume construction in an adaptive way. Better still, our network can be supervised using only 3D bounding boxes, eliminating the dependence on ground-truth scene geometry. Experimental results demonstrate that SGCDet achieves state-of-the-art performance on the ScanNet, ScanNet200 and ARKitScenes datasets. The source code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 48,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18334",
        "abs_url": "https://arxiv.org/abs/2507.18334",
        "pdf_url": "https://arxiv.org/pdf/2507.18334",
        "title": "Improving Bird Classification with Primary Color Additives",
        "authors": [
            "Ezhini Rasendiran R",
            "Chandresh Kumar Maurya"
        ],
        "comments": "5 pages (Accepted to Interspeech 2025)",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Sound (cs.SD); Audio and Speech Processing (eess.AS)",
        "abstract": "We address the problem of classifying bird species using their song recordings, a challenging task due to environmental noise, overlapping vocalizations, and missing labels. Existing models struggle with low-SNR or multi-species recordings. We hypothesize that birds can be classified by visualizing their pitch pattern, speed, and repetition, collectively called motifs. Deep learning models applied to spectrogram images help, but similar motifs across species cause confusion. To mitigate this, we embed frequency information into spectrograms using primary color additives. This enhances species distinction and improves classification accuracy. Our experiments show that the proposed approach achieves statistically significant gains over models without colorization and surpasses the BirdCLEF 2024 winner, improving F1 by 7.3%, ROC-AUC by 6.2%, and CMAP by 6.6%. These results demonstrate the effectiveness of incorporating frequency information via colorization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 49,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18342",
        "abs_url": "https://arxiv.org/abs/2507.18342",
        "pdf_url": "https://arxiv.org/pdf/2507.18342",
        "title": "EgoExoBench: A Benchmark for First- and Third-person View Video Understanding in MLLMs",
        "authors": [
            "Yuping He",
            "Yifei Huang",
            "Guo Chen",
            "Baoqi Pei",
            "Jilan Xu",
            "Tong Lu",
            "Jiangmiao Pang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Transferring and integrating knowledge across first-person (egocentric) and third-person (exocentric) viewpoints is intrinsic to human intelligence, enabling humans to learn from others and convey insights from their own experiences. Despite rapid progress in multimodal large language models (MLLMs), their ability to perform such cross-view reasoning remains unexplored. To address this, we introduce EgoExoBench, the first benchmark for egocentric-exocentric video understanding and reasoning. Built from publicly available datasets, EgoExoBench comprises over 7,300 question-answer pairs spanning eleven sub-tasks organized into three core challenges: semantic alignment, viewpoint association, and temporal reasoning. We evaluate 13 state-of-the-art MLLMs and find that while these models excel on single-view tasks, they struggle to align semantics across perspectives, accurately associate views, and infer temporal dynamics in the ego-exo context. We hope EgoExoBench can serve as a valuable resource for research on embodied agents and intelligent assistants seeking human-like cross-view intelligence.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 50,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18348",
        "abs_url": "https://arxiv.org/abs/2507.18348",
        "pdf_url": "https://arxiv.org/pdf/2507.18348",
        "title": "VB-Mitigator: An Open-source Framework for Evaluating and Advancing Visual Bias Mitigation",
        "authors": [
            "Ioannis Sarridis",
            "Christos Koutlis",
            "Symeon Papadopoulos",
            "Christos Diou"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Bias in computer vision models remains a significant challenge, often resulting in unfair, unreliable, and non-generalizable AI systems. Although research into bias mitigation has intensified, progress continues to be hindered by fragmented implementations and inconsistent evaluation practices. Disparate datasets and metrics used across studies complicate reproducibility, making it difficult to fairly assess and compare the effectiveness of various approaches. To overcome these limitations, we introduce the Visual Bias Mitigator (VB-Mitigator), an open-source framework designed to streamline the development, evaluation, and comparative analysis of visual bias mitigation techniques. VB-Mitigator offers a unified research environment encompassing 12 established mitigation methods, 7 diverse benchmark datasets. A key strength of VB-Mitigator is its extensibility, allowing for seamless integration of additional methods, datasets, metrics, and models. VB-Mitigator aims to accelerate research toward fairness-aware computer vision models by serving as a foundational codebase for the research community to develop and assess their approaches. To this end, we also recommend best evaluation practices and provide a comprehensive performance comparison among state-of-the-art methodologies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 51,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18354",
        "abs_url": "https://arxiv.org/abs/2507.18354",
        "pdf_url": "https://arxiv.org/pdf/2507.18354",
        "title": "Deformable Convolution Module with Globally Learned Relative Offsets for Fundus Vessel Segmentation",
        "authors": [
            "Lexuan Zhu",
            "Yuxuan Li",
            "Yuning Ren"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Deformable convolution can adaptively change the shape of convolution kernel by learning offsets to deal with complex shape features. We propose a novel plug and play deformable convolutional module that uses attention and feedforward networks to learn offsets, so that the deformable patterns can capture long-distance global features. Compared with previously existing deformable convolutions, the proposed module learns the sub pixel displacement field and adaptively warps the feature maps across all channels rather than directly deforms the convolution kernel , which is equivalent to a relative deformation of the kernel sampling grids, achieving global feature deformation and the decoupling of kernel size and learning network. Considering that the fundus blood vessels have globally self similar complex edges, we design a deep learning model for fundus blood vessel segmentation, GDCUnet, based on the proposed convolutional module. Empirical evaluations under the same configuration and unified framework show that GDCUnet has achieved state of the art performance on public datasets. Further ablation experiments demonstrated that the proposed deformable convolutional module could more significantly learn the complex features of fundus blood vessels, enhancing the model representation and generalization this http URL proposed module is similar to the interface of conventional convolution, we suggest applying it to more machine vision tasks with complex global self similar features.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 52,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18371",
        "abs_url": "https://arxiv.org/abs/2507.18371",
        "pdf_url": "https://arxiv.org/pdf/2507.18371",
        "title": "MVG4D: Image Matrix-Based Multi-View and Motion Generation for 4D Content Creation from a Single Image",
        "authors": [
            "Xiaotian Chen",
            "DongFu Yin",
            "Fei Richard Yu",
            "Xuanchen Li",
            "Xinhao Zhang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Advances in generative modeling have significantly enhanced digital content creation, extending from 2D images to complex 3D and 4D scenes. Despite substantial progress, producing high-fidelity and temporally consistent dynamic 4D content remains a challenge. In this paper, we propose MVG4D, a novel framework that generates dynamic 4D content from a single still image by combining multi-view synthesis with 4D Gaussian Splatting (4D GS). At its core, MVG4D employs an image matrix module that synthesizes temporally coherent and spatially diverse multi-view images, providing rich supervisory signals for downstream 3D and 4D reconstruction. These multi-view images are used to optimize a 3D Gaussian point cloud, which is further extended into the temporal domain via a lightweight deformation network. Our method effectively enhances temporal consistency, geometric fidelity, and visual realism, addressing key challenges in motion discontinuity and background degradation that affect prior 4D GS-based methods. Extensive experiments on the Objaverse dataset demonstrate that MVG4D outperforms state-of-the-art baselines in CLIP-I, PSNR, FVD, and time efficiency. Notably, it reduces flickering artifacts and sharpens structural details across views and time, enabling more immersive AR/VR experiences. MVG4D sets a new direction for efficient and controllable 4D generation from minimal inputs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 53,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18374",
        "abs_url": "https://arxiv.org/abs/2507.18374",
        "pdf_url": "https://arxiv.org/pdf/2507.18374",
        "title": "Towards Effective Human-in-the-Loop Assistive AI Agents",
        "authors": [
            "Filippos Bellos",
            "Yayuan Li",
            "Cary Shu",
            "Ruey Day",
            "Jeffrey M. Siskind",
            "Jason J. Corso"
        ],
        "comments": "10 pages, 5 figures, 2 tables",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Effective human-AI collaboration for physical task completion has significant potential in both everyday activities and professional domains. AI agents equipped with informative guidance can enhance human performance, but evaluating such collaboration remains challenging due to the complexity of human-in-the-loop interactions. In this work, we introduce an evaluation framework and a multimodal dataset of human-AI interactions designed to assess how AI guidance affects procedural task performance, error reduction and learning outcomes. Besides, we develop an augmented reality (AR)-equipped AI agent that provides interactive guidance in real-world tasks, from cooking to battlefield medicine. Through human studies, we share empirical insights into AI-assisted human performance and demonstrate that AI-assisted collaboration improves task completion.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 54,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18382",
        "abs_url": "https://arxiv.org/abs/2507.18382",
        "pdf_url": "https://arxiv.org/pdf/2507.18382",
        "title": "Towards Consistent Long-Term Pose Generation",
        "authors": [
            "Yayuan Li",
            "Filippos Bellos",
            "Jason Corso"
        ],
        "comments": "10 pages, 5 figures, 4 tables",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Current approaches to pose generation rely heavily on intermediate representations, either through two-stage pipelines with quantization or autoregressive models that accumulate errors during inference. This fundamental limitation leads to degraded performance, particularly in long-term pose generation where maintaining temporal coherence is crucial. We propose a novel one-stage architecture that directly generates poses in continuous coordinate space from minimal context - a single RGB image and text description - while maintaining consistent distributions between training and inference. Our key innovation is eliminating the need for intermediate representations or token-based generation by operating directly on pose coordinates through a relative movement prediction mechanism that preserves spatial relationships, and a unified placeholder token approach that enables single-forward generation with identical behavior during training and inference. Through extensive experiments on Penn Action and First-Person Hand Action Benchmark (F-PHAB) datasets, we demonstrate that our approach significantly outperforms existing quantization-based and autoregressive methods, especially in long-term generation scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 55,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18385",
        "abs_url": "https://arxiv.org/abs/2507.18385",
        "pdf_url": "https://arxiv.org/pdf/2507.18385",
        "title": "HumanMaterial: Human Material Estimation from a Single Image via Progressive Training",
        "authors": [
            "Yu Jiang",
            "Jiahao Xia",
            "Jiongming Qin",
            "Yusen Wang",
            "Tuo Cao",
            "Chunxia Xiao"
        ],
        "comments": "14",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Full-body Human inverse rendering based on physically-based rendering aims to acquire high-quality materials, which helps achieve photo-realistic rendering under arbitrary illuminations. This task requires estimating multiple material maps and usually relies on the constraint of rendering result. The absence of constraints on the material maps makes inverse rendering an ill-posed task. Previous works alleviated this problem by building material dataset for training, but their simplified material data and rendering equation lead to rendering results with limited realism, especially that of skin. To further alleviate this problem, we construct a higher-quality dataset (OpenHumanBRDF) based on scanned real data and statistical material data. In addition to the normal, diffuse albedo, roughness, specular albedo, we produce displacement and subsurface scattering to enhance the realism of rendering results, especially for the skin. With the increase in prediction tasks for more materials, using an end-to-end model as in the previous work struggles to balance the importance among various material maps, and leads to model underfitting. Therefore, we design a model (HumanMaterial) with progressive training strategy to make full use of the supervision information of the material maps and improve the performance of material estimation. HumanMaterial first obtain the initial material results via three prior models, and then refine the results by a finetuning model. Prior models estimate different material maps, and each map has different significance for rendering results. Thus, we design a Controlled PBR Rendering (CPR) loss, which enhances the importance of the materials to be optimized during the training of prior models. Extensive experiments on OpenHumanBRDF dataset and real data demonstrate that our method achieves state-of-the-art performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 56,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18405",
        "abs_url": "https://arxiv.org/abs/2507.18405",
        "pdf_url": "https://arxiv.org/pdf/2507.18405",
        "title": "Iwin Transformer: Hierarchical Vision Transformer using Interleaved Windows",
        "authors": [
            "Simin Huo",
            "Ning Li"
        ],
        "comments": "14 pages, 10 figures, Submitted to IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "We introduce Iwin Transformer, a novel position-embedding-free hierarchical vision transformer, which can be fine-tuned directly from low to high resolution, through the collaboration of innovative interleaved window attention and depthwise separable convolution. This approach uses attention to connect distant tokens and applies convolution to link neighboring tokens, enabling global information exchange within a single module, overcoming Swin Transformer's limitation of requiring two consecutive blocks to approximate global attention. Extensive experiments on visual benchmarks demonstrate that Iwin Transformer exhibits strong competitiveness in tasks such as image classification (87.4 top-1 accuracy on ImageNet-1K), semantic segmentation and video action recognition. We also validate the effectiveness of the core component in Iwin as a standalone module that can seamlessly replace the self-attention module in class-conditional image generation. The concepts and methods introduced by the Iwin Transformer have the potential to inspire future research, like Iwin 3D Attention in video generation. The code and models are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 57,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18407",
        "abs_url": "https://arxiv.org/abs/2507.18407",
        "pdf_url": "https://arxiv.org/pdf/2507.18407",
        "title": "DCFFSNet: Deep Connectivity Feature Fusion Separation Network for Medical Image Segmentation",
        "authors": [
            "Xun Ye",
            "Ruixiang Tang",
            "Mingda Zhang",
            "Jianglong Qin"
        ],
        "comments": "16 pages , 11 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Medical image segmentation leverages topological connectivity theory to enhance edge precision and regional consistency. However, existing deep networks integrating connectivity often forcibly inject it as an additional feature module, resulting in coupled feature spaces with no standardized mechanism to quantify different feature strengths. To address these issues, we propose DCFFSNet (Dual-Connectivity Feature Fusion-Separation Network). It introduces an innovative feature space decoupling strategy. This strategy quantifies the relative strength between connectivity features and other features. It then builds a deep connectivity feature fusion-separation architecture. This architecture dynamically balances multi-scale feature expression. Experiments were conducted on the ISIC2018, DSB2018, and MoNuSeg datasets. On ISIC2018, DCFFSNet outperformed the next best model (CMUNet) by 1.3% (Dice) and 1.2% (IoU). On DSB2018, it surpassed TransUNet by 0.7% (Dice) and 0.9% (IoU). On MoNuSeg, it exceeded CSCAUNet by 0.8% (Dice) and 0.9% (IoU). The results demonstrate that DCFFSNet exceeds existing mainstream methods across all metrics. It effectively resolves segmentation fragmentation and achieves smooth edge transitions. This significantly enhances clinical usability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 58,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18424",
        "abs_url": "https://arxiv.org/abs/2507.18424",
        "pdf_url": "https://arxiv.org/pdf/2507.18424",
        "title": "Self-Supervised Ultrasound-Video Segmentation with Feature Prediction and 3D Localised Loss",
        "authors": [
            "Edward Ellis",
            "Robert Mendel",
            "Andrew Bulpitt",
            "Nasim Parsa",
            "Michael F Byrne",
            "Sharib Ali"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Acquiring and annotating large datasets in ultrasound imaging is challenging due to low contrast, high noise, and susceptibility to artefacts. This process requires significant time and clinical expertise. Self-supervised learning (SSL) offers a promising solution by leveraging unlabelled data to learn useful representations, enabling improved segmentation performance when annotated data is limited. Recent state-of-the-art developments in SSL for video data include V-JEPA, a framework solely based on feature prediction, avoiding pixel level reconstruction or negative samples. We hypothesise that V-JEPA is well-suited to ultrasound imaging, as it is less sensitive to noisy pixel-level detail while effectively leveraging temporal information. To the best of our knowledge, this is the first study to adopt V-JEPA for ultrasound video data. Similar to other patch-based masking SSL techniques such as VideoMAE, V-JEPA is well-suited to ViT-based models. However, ViTs can underperform on small medical datasets due to lack of inductive biases, limited spatial locality and absence of hierarchical feature learning. To improve locality understanding, we propose a novel 3D localisation auxiliary task to improve locality in ViT representations during V-JEPA pre-training. Our results show V-JEPA with our auxiliary task improves segmentation performance significantly across various frozen encoder configurations, with gains up to 3.4\\% using 100\\% and up to 8.35\\% using only 10\\% of the training data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 59,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18429",
        "abs_url": "https://arxiv.org/abs/2507.18429",
        "pdf_url": "https://arxiv.org/pdf/2507.18429",
        "title": "NLML-HPE: Head Pose Estimation with Limited Data via Manifold Learning",
        "authors": [
            "Mahdi Ghafourian",
            "Federico M. Sukno"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Head pose estimation (HPE) plays a critical role in various computer vision applications such as human-computer interaction and facial recognition. In this paper, we propose a novel deep learning approach for head pose estimation with limited training data via non-linear manifold learning called NLML-HPE. This method is based on the combination of tensor decomposition (i.e., Tucker decomposition) and feed forward neural networks. Unlike traditional classification-based approaches, our method formulates head pose estimation as a regression problem, mapping input landmarks into a continuous representation of pose angles. To this end, our method uses tensor decomposition to split each Euler angle (yaw, pitch, roll) to separate subspaces and models each dimension of the underlying manifold as a cosine curve. We address two key challenges: 1. Almost all HPE datasets suffer from incorrect and inaccurate pose annotations. Hence, we generated a precise and consistent 2D head pose dataset for our training set by rotating 3D head models for a fixed set of poses and rendering the corresponding 2D images. 2. We achieved real-time performance with limited training data as our method accurately captures the nature of rotation of an object from facial landmarks. Once the underlying manifold for rotation around each axis is learned, the model is very fast in predicting unseen data. Our training and testing code is available online along with our trained models: https: //github.com/MahdiGhafoorian/NLML_HPE.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 60,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18444",
        "abs_url": "https://arxiv.org/abs/2507.18444",
        "pdf_url": "https://arxiv.org/pdf/2507.18444",
        "title": "DSFormer: A Dual-Scale Cross-Learning Transformer for Visual Place Recognition",
        "authors": [
            "Haiyang Jiang",
            "Songhao Piao",
            "Chao Gao",
            "Lei Yu",
            "Liguo Chen"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
        "abstract": "Visual Place Recognition (VPR) is crucial for robust mobile robot localization, yet it faces significant challenges in maintaining reliable performance under varying environmental conditions and viewpoints. To address this, we propose a novel framework that integrates Dual-Scale-Former (DSFormer), a Transformer-based cross-learning module, with an innovative block clustering strategy. DSFormer enhances feature representation by enabling bidirectional information transfer between dual-scale features extracted from the final two CNN layers, capturing both semantic richness and spatial details through self-attention for long-range dependencies within each scale and shared cross-attention for cross-scale learning. Complementing this, our block clustering strategy repartitions the widely used San Francisco eXtra Large (SF-XL) training dataset from multiple distinct perspectives, optimizing data organization to further bolster robustness against viewpoint variations. Together, these innovations not only yield a robust global embedding adaptable to environmental changes but also reduce the required training data volume by approximately 30\\% compared to previous partitioning methods. Comprehensive experiments demonstrate that our approach achieves state-of-the-art performance across most benchmark datasets, surpassing advanced reranking methods like DELG, Patch-NetVLAD, TransVPR, and R2Former as a global retrieval solution using 512-dim global descriptors, while significantly improving computational efficiency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 61,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18447",
        "abs_url": "https://arxiv.org/abs/2507.18447",
        "pdf_url": "https://arxiv.org/pdf/2507.18447",
        "title": "PDB-Eval: An Evaluation of Large Multimodal Models for Description and Explanation of Personalized Driving Behavior",
        "authors": [
            "Junda Wu",
            "Jessica Echterhoff",
            "Kyungtae Han",
            "Amr Abdelraouf",
            "Rohit Gupta",
            "Julian McAuley"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Understanding a driver's behavior and intentions is important for potential risk assessment and early accident prevention. Safety and driver assistance systems can be tailored to individual drivers' behavior, significantly enhancing their effectiveness. However, existing datasets are limited in describing and explaining general vehicle movements based on external visual evidence. This paper introduces a benchmark, PDB-Eval, for a detailed understanding of Personalized Driver Behavior, and aligning Large Multimodal Models (MLLMs) with driving comprehension and reasoning. Our benchmark consists of two main components, PDB-X and PDB-QA. PDB-X can evaluate MLLMs' understanding of temporal driving scenes. Our dataset is designed to find valid visual evidence from the external view to explain the driver's behavior from the internal view. To align MLLMs' reasoning abilities with driving tasks, we propose PDB-QA as a visual explanation question-answering task for MLLM instruction fine-tuning. As a generic learning task for generative models like MLLMs, PDB-QA can bridge the domain gap without harming MLLMs' generalizability. Our evaluation indicates that fine-tuning MLLMs on fine-grained descriptions and explanations can effectively bridge the gap between MLLMs and the driving domain, which improves zero-shot performance on question-answering tasks by up to 73.2%. We further evaluate the MLLMs fine-tuned on PDB-X in Brain4Cars' intention prediction and AIDE's recognition tasks. We observe up to 12.5% performance improvements on the turn intention prediction task in Brain4Cars, and consistent performance improvements up to 11.0% on all tasks in AIDE.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 62,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18457",
        "abs_url": "https://arxiv.org/abs/2507.18457",
        "pdf_url": "https://arxiv.org/pdf/2507.18457",
        "title": "Revisiting Physically Realizable Adversarial Object Attack against LiDAR-based Detection: Clarifying Problem Formulation and Experimental Protocols",
        "authors": [
            "Luo Cheng",
            "Hanwei Zhang",
            "Lijun Zhang",
            "Holger Hermanns"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Adversarial robustness in LiDAR-based 3D object detection is a critical research area due to its widespread application in real-world scenarios. While many digital attacks manipulate point clouds or meshes, they often lack physical realizability, limiting their practical impact. Physical adversarial object attacks remain underexplored and suffer from poor reproducibility due to inconsistent setups and hardware differences. To address this, we propose a device-agnostic, standardized framework that abstracts key elements of physical adversarial object attacks, supports diverse methods, and provides open-source code with benchmarking protocols in simulation and real-world settings. Our framework enables fair comparison, accelerates research, and is validated by successfully transferring simulated attacks to a physical LiDAR system. Beyond the framework, we offer insights into factors influencing attack success and advance understanding of adversarial robustness in real-world LiDAR perception.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 63,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18473",
        "abs_url": "https://arxiv.org/abs/2507.18473",
        "pdf_url": "https://arxiv.org/pdf/2507.18473",
        "title": "CRUISE: Cooperative Reconstruction and Editing in V2X Scenarios using Gaussian Splatting",
        "authors": [
            "Haoran Xu",
            "Saining Zhang",
            "Peishuo Li",
            "Baijun Ye",
            "Xiaoxue Chen",
            "Huan-ang Gao",
            "Jv Zheng",
            "Xiaowei Song",
            "Ziqiao Peng",
            "Run Miao",
            "Jinrang Jia",
            "Yifeng Shi",
            "Guangqi Yi",
            "Hang Zhao",
            "Hao Tang",
            "Hongyang Li",
            "Kaicheng Yu",
            "Hao Zhao"
        ],
        "comments": "IROS 2025, Code: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Vehicle-to-everything (V2X) communication plays a crucial role in autonomous driving, enabling cooperation between vehicles and infrastructure. While simulation has significantly contributed to various autonomous driving tasks, its potential for data generation and augmentation in V2X scenarios remains underexplored. In this paper, we introduce CRUISE, a comprehensive reconstruction-and-synthesis framework designed for V2X driving environments. CRUISE employs decomposed Gaussian Splatting to accurately reconstruct real-world scenes while supporting flexible editing. By decomposing dynamic traffic participants into editable Gaussian representations, CRUISE allows for seamless modification and augmentation of driving scenes. Furthermore, the framework renders images from both ego-vehicle and infrastructure views, enabling large-scale V2X dataset augmentation for training and evaluation. Our experimental results demonstrate that: 1) CRUISE reconstructs real-world V2X driving scenes with high fidelity; 2) using CRUISE improves 3D detection across ego-vehicle, infrastructure, and cooperative views, as well as cooperative 3D tracking on the V2X-Seq benchmark; and 3) CRUISE effectively generates challenging corner cases.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 64,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18481",
        "abs_url": "https://arxiv.org/abs/2507.18481",
        "pdf_url": "https://arxiv.org/pdf/2507.18481",
        "title": "Q-Former Autoencoder: A Modern Framework for Medical Anomaly Detection",
        "authors": [
            "Francesco Dalmonte",
            "Emirhan Bayar",
            "Emre Akbas",
            "Mariana-Iuliana Georgescu"
        ],
        "comments": "15 pages",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Anomaly detection in medical images is an important yet challenging task due to the diversity of possible anomalies and the practical impossibility of collecting comprehensively annotated data sets. In this work, we tackle unsupervised medical anomaly detection proposing a modernized autoencoder-based framework, the Q-Former Autoencoder, that leverages state-of-the-art pretrained vision foundation models, such as DINO, DINOv2 and Masked Autoencoder. Instead of training encoders from scratch, we directly utilize frozen vision foundation models as feature extractors, enabling rich, multi-stage, high-level representations without domain-specific fine-tuning. We propose the usage of the Q-Former architecture as the bottleneck, which enables the control of the length of the reconstruction sequence, while efficiently aggregating multiscale features. Additionally, we incorporate a perceptual loss computed using features from a pretrained Masked Autoencoder, guiding the reconstruction towards semantically meaningful structures. Our framework is evaluated on four diverse medical anomaly detection benchmarks, achieving state-of-the-art results on BraTS2021, RESC, and RSNA. Our results highlight the potential of vision foundation model encoders, pretrained on natural images, to generalize effectively to medical image analysis tasks without further fine-tuning. We release the code and models at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 65,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18483",
        "abs_url": "https://arxiv.org/abs/2507.18483",
        "pdf_url": "https://arxiv.org/pdf/2507.18483",
        "title": "A COCO-Formatted Instance-Level Dataset for Plasmodium Falciparum Detection in Giemsa-Stained Blood Smears",
        "authors": [
            "Frauke Wilm",
            "Luis Carlos Rivera Monroy",
            "Mathias Ãttl",
            "Lukas MÃ¼rdter",
            "Leonid Mill",
            "Andreas Maier"
        ],
        "comments": "7 pages, 4 figures, 2 tables, accepted at MICCAI 2025 Open Data",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Accurate detection of Plasmodium falciparum in Giemsa-stained blood smears is an essential component of reliable malaria diagnosis, especially in developing countries. Deep learning-based object detection methods have demonstrated strong potential for automated Malaria diagnosis, but their adoption is limited by the scarcity of datasets with detailed instance-level annotations. In this work, we present an enhanced version of the publicly available NIH malaria dataset, with detailed bounding box annotations in COCO format to support object detection training. We validated the revised annotations by training a Faster R-CNN model to detect infected and non-infected red blood cells, as well as white blood cells. Cross-validation on the original dataset yielded F1 scores of up to 0.88 for infected cell detection. These results underscore the importance of annotation volume and consistency, and demonstrate that automated annotation refinement combined with targeted manual correction can produce training data of sufficient quality for robust detection performance. The updated annotations set is publicly available via GitHub: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 66,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18484",
        "abs_url": "https://arxiv.org/abs/2507.18484",
        "pdf_url": "https://arxiv.org/pdf/2507.18484",
        "title": "Reinforced Embodied Active Defense: Exploiting Adaptive Interaction for Robust Visual Perception in Adversarial 3D Environments",
        "authors": [
            "Xiao Yang",
            "Lingxuan Wu",
            "Lizhong Wang",
            "Chengyang Ying",
            "Hang Su",
            "Jun Zhu"
        ],
        "comments": "arXiv admin note: text overlap with arXiv:2404.00540",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Adversarial attacks in 3D environments have emerged as a critical threat to the reliability of visual perception systems, particularly in safety-sensitive applications such as identity verification and autonomous driving. These attacks employ adversarial patches and 3D objects to manipulate deep neural network (DNN) predictions by exploiting vulnerabilities within complex scenes. Existing defense mechanisms, such as adversarial training and purification, primarily employ passive strategies to enhance robustness. However, these approaches often rely on pre-defined assumptions about adversarial tactics, limiting their adaptability in dynamic 3D settings. To address these challenges, we introduce Reinforced Embodied Active Defense (Rein-EAD), a proactive defense framework that leverages adaptive exploration and interaction with the environment to improve perception robustness in 3D adversarial contexts. By implementing a multi-step objective that balances immediate prediction accuracy with predictive entropy minimization, Rein-EAD optimizes defense strategies over a multi-step horizon. Additionally, Rein-EAD involves an uncertainty-oriented reward-shaping mechanism that facilitates efficient policy updates, thereby reducing computational overhead and supporting real-world applicability without the need for differentiable environments. Comprehensive experiments validate the effectiveness of Rein-EAD, demonstrating a substantial reduction in attack success rates while preserving standard accuracy across diverse tasks. Notably, Rein-EAD exhibits robust generalization to unseen and adaptive attacks, making it suitable for real-world complex tasks, including 3D object classification, face recognition and autonomous driving.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 67,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18498",
        "abs_url": "https://arxiv.org/abs/2507.18498",
        "pdf_url": "https://arxiv.org/pdf/2507.18498",
        "title": "Delving into Mapping Uncertainty for Mapless Trajectory Prediction",
        "authors": [
            "Zongzheng Zhang",
            "Xuchong Qiu",
            "Boran Zhang",
            "Guantian Zheng",
            "Xunjiang Gu",
            "Guoxuan Chi",
            "Huan-ang Gao",
            "Leichen Wang",
            "Ziming Liu",
            "Xinrun Li",
            "Igor Gilitschenski",
            "Hongyang Li",
            "Hang Zhao",
            "Hao Zhao"
        ],
        "comments": "Accepted to IROS 2025, Project Page: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Recent advances in autonomous driving are moving towards mapless approaches, where High-Definition (HD) maps are generated online directly from sensor data, reducing the need for expensive labeling and maintenance. However, the reliability of these online-generated maps remains uncertain. While incorporating map uncertainty into downstream trajectory prediction tasks has shown potential for performance improvements, current strategies provide limited insights into the specific scenarios where this uncertainty is beneficial. In this work, we first analyze the driving scenarios in which mapping uncertainty has the greatest positive impact on trajectory prediction and identify a critical, previously overlooked factor: the agent's kinematic state. Building on these insights, we propose a novel Proprioceptive Scenario Gating that adaptively integrates map uncertainty into trajectory prediction based on forecasts of the ego vehicle's future kinematics. This lightweight, self-supervised approach enhances the synergy between online mapping and trajectory prediction, providing interpretability around where uncertainty is advantageous and outperforming previous integration methods. Additionally, we introduce a Covariance-based Map Uncertainty approach that better aligns with map geometry, further improving trajectory prediction. Extensive ablation studies confirm the effectiveness of our approach, achieving up to 23.6% improvement in mapless trajectory prediction performance over the state-of-the-art method using the real-world nuScenes driving dataset. Our code, data, and models are publicly available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 68,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18503",
        "abs_url": "https://arxiv.org/abs/2507.18503",
        "pdf_url": "https://arxiv.org/pdf/2507.18503",
        "title": "Human Scanpath Prediction in Target-Present Visual Search with Semantic-Foveal Bayesian Attention",
        "authors": [
            "JoÃ£o Luzio",
            "Alexandre Bernardino",
            "Plinio Moreno"
        ],
        "comments": "To be published in the 2025 IEEE International Conference on Development and Learning (ICDL)",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In goal-directed visual tasks, human perception is guided by both top-down and bottom-up cues. At the same time, foveal vision plays a crucial role in directing attention efficiently. Modern research on bio-inspired computational attention models has taken advantage of advancements in deep learning by utilizing human scanpath data to achieve new state-of-the-art performance. In this work, we assess the performance of SemBA-FAST, i.e. Semantic-based Bayesian Attention for Foveal Active visual Search Tasks, a top-down framework designed for predicting human visual attention in target-present visual search. SemBA-FAST integrates deep object detection with a probabilistic semantic fusion mechanism to generate attention maps dynamically, leveraging pre-trained detectors and artificial foveation to update top-down knowledge and improve fixation prediction sequentially. We evaluate SemBA-FAST on the COCO-Search18 benchmark dataset, comparing its performance against other scanpath prediction models. Our methodology achieves fixation sequences that closely match human ground-truth scanpaths. Notably, it surpasses baseline and other top-down approaches and competes, in some cases, with scanpath-informed models. These findings provide valuable insights into the capabilities of semantic-foveal probabilistic frameworks for human-like attention modelling, with implications for real-time cognitive computing and robotics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 69,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18512",
        "abs_url": "https://arxiv.org/abs/2507.18512",
        "pdf_url": "https://arxiv.org/pdf/2507.18512",
        "title": "Explaining How Visual, Textual and Multimodal Encoders Share Concepts",
        "authors": [
            "ClÃ©ment Cornet",
            "Romaric BesanÃ§on",
            "HervÃ© Le Borgne"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Sparse autoencoders (SAEs) have emerged as a powerful technique for extracting human-interpretable features from neural networks activations. Previous works compared different models based on SAE-derived features but those comparisons have been restricted to models within the same modality. We propose a novel indicator allowing quantitative comparison of models across SAE features, and use it to conduct a comparative study of visual, textual and multimodal encoders. We also propose to quantify the Comparative Sharedness of individual features between different classes of models. With these two new tools, we conduct several studies on 21 encoders of the three types, with two significantly different sizes, and considering generalist and domain specific datasets. The results allow to revisit previous studies at the light of encoders trained in a multimodal context and to quantify to which extent all these models share some representations or features. They also suggest that visual features that are specific to VLMs among vision encoders are shared with text encoders, highlighting the impact of text pretraining. The code is available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 70,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18513",
        "abs_url": "https://arxiv.org/abs/2507.18513",
        "pdf_url": "https://arxiv.org/pdf/2507.18513",
        "title": "Towards Large Scale Geostatistical Methane Monitoring with Part-based Object Detection",
        "authors": [
            "Adhemar de Senneville",
            "Xavier Bou",
            "Thibaud Ehret",
            "Rafael Grompone",
            "Jean Louis Bonne",
            "Nicolas Dumelie",
            "Thomas Lauvaux",
            "Gabriele Facciolo"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Object detection is one of the main applications of computer vision in remote sensing imagery. Despite its increasing availability, the sheer volume of remote sensing data poses a challenge when detecting rare objects across large geographic areas. Paradoxically, this common challenge is crucial to many applications, such as estimating environmental impact of certain human activities at scale. In this paper, we propose to address the problem by investigating the methane production and emissions of bio-digesters in France. We first introduce a novel dataset containing bio-digesters, with small training and validation sets, and a large test set with a high imbalance towards observations without objects since such sites are rare. We develop a part-based method that considers essential bio-digester sub-elements to boost initial detections. To this end, we apply our method to new, unseen regions to build an inventory of bio-digesters. We then compute geostatistical estimates of the quantity of methane produced that can be attributed to these infrastructures in a given area at a given time.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 71,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18517",
        "abs_url": "https://arxiv.org/abs/2507.18517",
        "pdf_url": "https://arxiv.org/pdf/2507.18517",
        "title": "Object segmentation in the wild with foundation models: application to vision assisted neuro-prostheses for upper limbs",
        "authors": [
            "Bolutife Atoki",
            "Jenny Benois-Pineau",
            "Renaud PÃ©teri",
            "Fabien Baldacci",
            "Aymar de Rugy"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In this work, we address the problem of semantic object segmentation using foundation models. We investigate whether foundation models, trained on a large number and variety of objects, can perform object segmentation without fine-tuning on specific images containing everyday objects, but in highly cluttered visual scenes. The ''in the wild'' context is driven by the target application of vision guided upper limb neuroprostheses. We propose a method for generating prompts based on gaze fixations to guide the Segment Anything Model (SAM) in our segmentation scenario, and fine-tune it on egocentric visual data. Evaluation results of our approach show an improvement of the IoU segmentation quality metric by up to 0.51 points on real-world challenging data of Grasping-in-the-Wild corpus which is made available on the RoboFlow Platform (this https URL)",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 72,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18522",
        "abs_url": "https://arxiv.org/abs/2507.18522",
        "pdf_url": "https://arxiv.org/pdf/2507.18522",
        "title": "GaussianFusionOcc: A Seamless Sensor Fusion Approach for 3D Occupancy Prediction Using 3D Gaussians",
        "authors": [
            "Tomislav PavkoviÄ",
            "Mohammad-Ali Nikouei Mahani",
            "Johannes Niedermayer",
            "Johannes Betz"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "3D semantic occupancy prediction is one of the crucial tasks of autonomous driving. It enables precise and safe interpretation and navigation in complex environments. Reliable predictions rely on effective sensor fusion, as different modalities can contain complementary information. Unlike conventional methods that depend on dense grid representations, our approach, GaussianFusionOcc, uses semantic 3D Gaussians alongside an innovative sensor fusion mechanism. Seamless integration of data from camera, LiDAR, and radar sensors enables more precise and scalable occupancy prediction, while 3D Gaussian representation significantly improves memory efficiency and inference speed. GaussianFusionOcc employs modality-agnostic deformable attention to extract essential features from each sensor type, which are then used to refine Gaussian properties, resulting in a more accurate representation of the environment. Extensive testing with various sensor combinations demonstrates the versatility of our approach. By leveraging the robustness of multi-modal fusion and the efficiency of Gaussian representation, GaussianFusionOcc outperforms current state-of-the-art models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 73,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18531",
        "abs_url": "https://arxiv.org/abs/2507.18531",
        "pdf_url": "https://arxiv.org/pdf/2507.18531",
        "title": "IntentVCNet: Bridging Spatio-Temporal Gaps for Intention-Oriented Controllable Video Captioning",
        "authors": [
            "Tianheng Qiu",
            "Jingchun Gao",
            "Jingyu Li",
            "Huiyi Leong",
            "Xuan Huang",
            "Xi Wang",
            "Xiaocheng Zhang",
            "Kele Xu",
            "Lan Zhang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Intent-oriented controlled video captioning aims to generate targeted descriptions for specific targets in a video based on customized user intent. Current Large Visual Language Models (LVLMs) have gained strong instruction following and visual comprehension capabilities. Although the LVLMs demonstrated proficiency in spatial and temporal understanding respectively, it was not able to perform fine-grained spatial control in time sequences in direct response to instructions. This substantial spatio-temporal gap complicates efforts to achieve fine-grained intention-oriented control in video. Towards this end, we propose a novel IntentVCNet that unifies the temporal and spatial understanding knowledge inherent in LVLMs to bridge the spatio-temporal gap from both prompting and model perspectives. Specifically, we first propose a prompt combination strategy designed to enable LLM to model the implicit relationship between prompts that characterize user intent and video sequences. We then propose a parameter efficient box adapter that augments the object semantic information in the global visual context so that the visual token has a priori information about the user intent. The final experiment proves that the combination of the two strategies can further enhance the LVLM's ability to model spatial details in video sequences, and facilitate the LVLMs to accurately generate controlled intent-oriented captions. Our proposed method achieved state-of-the-art results in several open source LVLMs and was the runner-up in the IntentVC challenge. Our code is available on this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 74,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18532",
        "abs_url": "https://arxiv.org/abs/2507.18532",
        "pdf_url": "https://arxiv.org/pdf/2507.18532",
        "title": "COT-AD: Cotton Analysis Dataset",
        "authors": [
            "Akbar Ali",
            "Mahek Vyas",
            "Soumyaratna Debnath",
            "Chanda Grover Kamra",
            "Jaidev Sanjay Khalane",
            "Reuben Shibu Devanesan",
            "Indra Deep Mastan",
            "Subramanian Sankaranarayanan",
            "Pankaj Khanna",
            "Shanmuganathan Raman"
        ],
        "comments": "Dataset publicly available at: this https URL. Accepted to IEEE International Conference on Image Processing (ICIP) 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "This paper presents COT-AD, a comprehensive Dataset designed to enhance cotton crop analysis through computer vision. Comprising over 25,000 images captured throughout the cotton growth cycle, with 5,000 annotated images, COT-AD includes aerial imagery for field-scale detection and segmentation and high-resolution DSLR images documenting key diseases. The annotations cover pest and disease recognition, vegetation, and weed analysis, addressing a critical gap in cotton-specific agricultural datasets. COT-AD supports tasks such as classification, segmentation, image restoration, enhancement, deep generative model-based cotton crop synthesis, and early disease management, advancing data-driven crop management",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 75,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18534",
        "abs_url": "https://arxiv.org/abs/2507.18534",
        "pdf_url": "https://arxiv.org/pdf/2507.18534",
        "title": "Elucidating the Design Space of Arbitrary-Noise-Based Diffusion Models",
        "authors": [
            "Xingyu Qiu",
            "Mengying Yang",
            "Xinghua Ma",
            "Dong Liang",
            "Yuzhen Li",
            "Fanding Li",
            "Gongning Luo",
            "Wei Wang",
            "Kuanquan Wang",
            "Shuo Li"
        ],
        "comments": "21 pages, 4 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "EDM elucidates the unified design space of diffusion models, yet its fixed noise patterns restricted to pure Gaussian noise, limit advancements in image restoration. Our study indicates that forcibly injecting Gaussian noise corrupts the degraded images, overextends the image transformation distance, and increases restoration complexity. To address this problem, our proposed EDA Elucidates the Design space of Arbitrary-noise-based diffusion models. Theoretically, EDA expands the freedom of noise pattern while preserving the original module flexibility of EDM, with rigorous proof that increased noise complexity incurs no additional computational overhead during restoration. EDA is validated on three typical tasks: MRI bias field correction (global smooth noise), CT metal artifact reduction (global sharp noise), and natural image shadow removal (local boundary-aware noise). With only 5 sampling steps, EDA outperforms most task-specific methods and achieves state-of-the-art performance in bias field correction and shadow removal.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 76,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18537",
        "abs_url": "https://arxiv.org/abs/2507.18537",
        "pdf_url": "https://arxiv.org/pdf/2507.18537",
        "title": "TTS-VAR: A Test-Time Scaling Framework for Visual Auto-Regressive Generation",
        "authors": [
            "Zhekai Chen",
            "Ruihang Chu",
            "Yukang Chen",
            "Shiwei Zhang",
            "Yujie Wei",
            "Yingya Zhang",
            "Xihui Liu"
        ],
        "comments": "10 Tables, 9 Figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Scaling visual generation models is essential for real-world content creation, yet requires substantial training and computational expenses. Alternatively, test-time scaling has garnered growing attention due to resource efficiency and promising performance. In this work, we present TTS-VAR, the first general test-time scaling framework for visual auto-regressive (VAR) models, modeling the generation process as a path searching problem. To dynamically balance computational efficiency with exploration capacity, we first introduce an adaptive descending batch size schedule throughout the causal generation process. Besides, inspired by VAR's hierarchical coarse-to-fine multi-scale generation, our framework integrates two key components: (i) At coarse scales, we observe that generated tokens are hard for evaluation, possibly leading to erroneous acceptance of inferior samples or rejection of superior samples. Noticing that the coarse scales contain sufficient structural information, we propose clustering-based diversity search. It preserves structural variety through semantic feature clustering, enabling later selection on samples with higher potential. (ii) In fine scales, resampling-based potential selection prioritizes promising candidates using potential scores, which are defined as reward functions incorporating multi-scale generation history. Experiments on the powerful VAR model Infinity show a notable 8.7% GenEval score improvement (from 0.69 to 0.75). Key insights reveal that early-stage structural features effectively influence final quality, and resampling efficacy varies across generation scales. Code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 77,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18541",
        "abs_url": "https://arxiv.org/abs/2507.18541",
        "pdf_url": "https://arxiv.org/pdf/2507.18541",
        "title": "Unposed 3DGS Reconstruction with Probabilistic Procrustes Mapping",
        "authors": [
            "Chong Cheng",
            "Zijian Wang",
            "Sicheng Yu",
            "Yu Hu",
            "Nanjie Yao",
            "Hao Wang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "3D Gaussian Splatting (3DGS) has emerged as a core technique for 3D representation. Its effectiveness largely depends on precise camera poses and accurate point cloud initialization, which are often derived from pretrained Multi-View Stereo (MVS) models. However, in unposed reconstruction task from hundreds of outdoor images, existing MVS models may struggle with memory limits and lose accuracy as the number of input images grows. To address this limitation, we propose a novel unposed 3DGS reconstruction framework that integrates pretrained MVS priors with the probabilistic Procrustes mapping strategy. The method partitions input images into subsets, maps submaps into a global space, and jointly optimizes geometry and poses with 3DGS. Technically, we formulate the mapping of tens of millions of point clouds as a probabilistic Procrustes problem and solve a closed-form alignment. By employing probabilistic coupling along with a soft dustbin mechanism to reject uncertain correspondences, our method globally aligns point clouds and poses within minutes across hundreds of images. Moreover, we propose a joint optimization framework for 3DGS and camera poses. It constructs Gaussians from confidence-aware anchor points and integrates 3DGS differentiable rendering with an analytical Jacobian to jointly refine scene and poses, enabling accurate reconstruction and pose estimation. Experiments on Waymo and KITTI datasets show that our method achieves accurate reconstruction from unposed image sequences, setting a new state of the art for unposed 3DGS reconstruction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 78,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18551",
        "abs_url": "https://arxiv.org/abs/2507.18551",
        "pdf_url": "https://arxiv.org/pdf/2507.18551",
        "title": "A 3D Cross-modal Keypoint Descriptor for MR-US Matching and Registration",
        "authors": [
            "Daniil Morozov",
            "Reuben Dorent",
            "Nazim Haouchine"
        ],
        "comments": "Under review",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Intraoperative registration of real-time ultrasound (iUS) to preoperative Magnetic Resonance Imaging (MRI) remains an unsolved problem due to severe modality-specific differences in appearance, resolution, and field-of-view. To address this, we propose a novel 3D cross-modal keypoint descriptor for MRI-iUS matching and registration. Our approach employs a patient-specific matching-by-synthesis approach, generating synthetic iUS volumes from preoperative MRI. This enables supervised contrastive training to learn a shared descriptor space. A probabilistic keypoint detection strategy is then employed to identify anatomically salient and modality-consistent locations. During training, a curriculum-based triplet loss with dynamic hard negative mining is used to learn descriptors that are i) robust to iUS artifacts such as speckle noise and limited coverage, and ii) rotation-invariant . At inference, the method detects keypoints in MR and real iUS images and identifies sparse matches, which are then used to perform rigid registration. Our approach is evaluated using 3D MRI-iUS pairs from the ReMIND dataset. Experiments show that our approach outperforms state-of-the-art keypoint matching methods across 11 patients, with an average precision of $69.8\\%$. For image registration, our method achieves a competitive mean Target Registration Error of 2.39 mm on the ReMIND2Reg benchmark. Compared to existing iUS-MR registration approach, our framework is interpretable, requires no manual initialization, and shows robustness to iUS field-of-view variation. Code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 79,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18552",
        "abs_url": "https://arxiv.org/abs/2507.18552",
        "pdf_url": "https://arxiv.org/pdf/2507.18552",
        "title": "VideoMind: An Omni-Modal Video Dataset with Intent Grounding for Deep-Cognitive Video Understanding",
        "authors": [
            "Baoyao Yang",
            "Wanyun Li",
            "Dixin Chen",
            "Junxiang Chen",
            "Wenbin Yao",
            "Haifeng Lin"
        ],
        "comments": "7 pages; 14 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "This paper introduces VideoMind, a video-centric omni-modal dataset designed for deep video content cognition and enhanced multi-modal feature representation. The dataset comprises 103K video samples (3K reserved for testing), each paired with audio and systematically detailed textual descriptions. Specifically, every video and its audio is described across three hierarchical layers (factual, abstract, and intent), progressing from surface to depth. It contains over 22 million words, averaging ~225 words per sample. VideoMind's key distinction from existing datasets is its provision of intent expressions, which require contextual integration across the entire video and are not directly observable. These deep-cognitive expressions are generated using a Chain-of-Thought (COT) approach, prompting the mLLM through step-by-step reasoning. Each description includes annotations for subject, place, time, event, action, and intent, supporting downstream recognition tasks. Crucially, we establish a gold-standard benchmark with 3,000 manually validated samples for evaluating deep-cognitive video understanding. We design hybrid-cognitive retrieval experiments, scored by multi-level retrieval metrics, to appropriately assess deep video comprehension. Evaluation results for models (e.g., InternVideo, VAST, UMT-L) are released. VideoMind serves as a powerful benchmark for fine-grained cross-modal alignment and advances fields requiring in-depth video understanding, such as emotion and intent recognition. The data is publicly available on GitHub, HuggingFace, and OpenDataLab, this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 80,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18558",
        "abs_url": "https://arxiv.org/abs/2507.18558",
        "pdf_url": "https://arxiv.org/pdf/2507.18558",
        "title": "Synthetic Data Augmentation for Enhanced Chicken Carcass Instance Segmentation",
        "authors": [
            "Yihong Feng",
            "Chaitanya Pallerla",
            "Xiaomin Lin",
            "Pouya Sohrabipour Sr",
            "Philip Crandall",
            "Wan Shou",
            "Yu She",
            "Dongyi Wang"
        ],
        "comments": "Submitted for journal reviewing",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)",
        "abstract": "The poultry industry has been driven by broiler chicken production and has grown into the world's largest animal protein sector. Automated detection of chicken carcasses on processing lines is vital for quality control, food safety, and operational efficiency in slaughterhouses and poultry processing plants. However, developing robust deep learning models for tasks like instance segmentation in these fast-paced industrial environments is often hampered by the need for laborious acquisition and annotation of large-scale real-world image datasets. We present the first pipeline generating photo-realistic, automatically labeled synthetic images of chicken carcasses. We also introduce a new benchmark dataset containing 300 annotated real-world images, curated specifically for poultry segmentation research. Using these datasets, this study investigates the efficacy of synthetic data and automatic data annotation to enhance the instance segmentation of chicken carcasses, particularly when real annotated data from the processing line is scarce. A small real dataset with varying proportions of synthetic images was evaluated in prominent instance segmentation models. Results show that synthetic data significantly boosts segmentation performance for chicken carcasses across all models. This research underscores the value of synthetic data augmentation as a viable and effective strategy to mitigate data scarcity, reduce manual annotation efforts, and advance the development of robust AI-driven automated detection systems for chicken carcasses in the poultry processing industry.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 81,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18565",
        "abs_url": "https://arxiv.org/abs/2507.18565",
        "pdf_url": "https://arxiv.org/pdf/2507.18565",
        "title": "Deep Learning-Based Age Estimation and Gender Deep Learning-Based Age Estimation and Gender Classification for Targeted Advertisement",
        "authors": [
            "Muhammad Imran Zaman",
            "Nisar Ahmed"
        ],
        "comments": "6",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "This paper presents a novel deep learning-based approach for simultaneous age and gender classification from facial images, designed to enhance the effectiveness of targeted advertising campaigns. We propose a custom Convolutional Neural Network (CNN) architecture, optimized for both tasks, which leverages the inherent correlation between age and gender information present in facial features. Unlike existing methods that often treat these tasks independently, our model learns shared representations, leading to improved performance. The network is trained on a large, diverse dataset of facial images, carefully pre-processed to ensure robustness against variations in lighting, pose, and image quality. Our experimental results demonstrate a significant improvement in gender classification accuracy, achieving 95%, and a competitive mean absolute error of 5.77 years for age estimation. Critically, we analyze the performance across different age groups, identifying specific challenges in accurately estimating the age of younger individuals. This analysis reveals the need for targeted data augmentation and model refinement to address these biases. Furthermore, we explore the impact of different CNN architectures and hyperparameter settings on the overall performance, providing valuable insights for future research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 82,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18566",
        "abs_url": "https://arxiv.org/abs/2507.18566",
        "pdf_url": "https://arxiv.org/pdf/2507.18566",
        "title": "Facial Demorphing from a Single Morph Using a Latent Conditional GAN",
        "authors": [
            "Nitish Shukla",
            "Arun Ross"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "A morph is created by combining two (or more) face images from two (or more) identities to create a composite image that is highly similar to both constituent identities, allowing the forged morph to be biometrically associated with more than one individual. Morph Attack Detection (MAD) can be used to detect a morph, but does not reveal the constituent images. Demorphing - the process of deducing the constituent images - is thus vital to provide additional evidence about a morph. Existing demorphing methods suffer from the morph replication problem, where the outputs tend to look very similar to the morph itself, or assume that train and test morphs are generated using the same morph technique. The proposed method overcomes these issues. The method decomposes a morph in latent space allowing it to demorph images created from unseen morph techniques and face styles. We train our method on morphs created from synthetic faces and test on morphs created from real faces using arbitrary morph techniques. Our method outperforms existing methods by a considerable margin and produces high fidelity demorphed face images.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 83,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18569",
        "abs_url": "https://arxiv.org/abs/2507.18569",
        "pdf_url": "https://arxiv.org/pdf/2507.18569",
        "title": "Adversarial Distribution Matching for Diffusion Distillation Towards Efficient Image and Video Synthesis",
        "authors": [
            "Yanzuo Lu",
            "Yuxi Ren",
            "Xin Xia",
            "Shanchuan Lin",
            "Xing Wang",
            "Xuefeng Xiao",
            "Andy J. Ma",
            "Xiaohua Xie",
            "Jian-Huang Lai"
        ],
        "comments": "Accepted by ICCV 2025 (Highlight)",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Distribution Matching Distillation (DMD) is a promising score distillation technique that compresses pre-trained teacher diffusion models into efficient one-step or multi-step student generators. Nevertheless, its reliance on the reverse Kullback-Leibler (KL) divergence minimization potentially induces mode collapse (or mode-seeking) in certain applications. To circumvent this inherent drawback, we propose Adversarial Distribution Matching (ADM), a novel framework that leverages diffusion-based discriminators to align the latent predictions between real and fake score estimators for score distillation in an adversarial manner. In the context of extremely challenging one-step distillation, we further improve the pre-trained generator by adversarial distillation with hybrid discriminators in both latent and pixel spaces. Different from the mean squared error used in DMD2 pre-training, our method incorporates the distributional loss on ODE pairs collected from the teacher model, and thus providing a better initialization for score distillation fine-tuning in the next stage. By combining the adversarial distillation pre-training with ADM fine-tuning into a unified pipeline termed DMDX, our proposed method achieves superior one-step performance on SDXL compared to DMD2 while consuming less GPU time. Additional experiments that apply multi-step ADM distillation on SD3-Medium, SD3.5-Large, and CogVideoX set a new benchmark towards efficient image and video synthesis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 84,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18575",
        "abs_url": "https://arxiv.org/abs/2507.18575",
        "pdf_url": "https://arxiv.org/pdf/2507.18575",
        "title": "HybridTM: Combining Transformer and Mamba for 3D Semantic Segmentation",
        "authors": [
            "Xinyu Wang",
            "Jinghua Hou",
            "Zhe Liu",
            "Yingying Zhu"
        ],
        "comments": "7 pages, 5 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Transformer-based methods have demonstrated remarkable capabilities in 3D semantic segmentation through their powerful attention mechanisms, but the quadratic complexity limits their modeling of long-range dependencies in large-scale point clouds. While recent Mamba-based approaches offer efficient processing with linear complexity, they struggle with feature representation when extracting 3D features. However, effectively combining these complementary strengths remains an open challenge in this field. In this paper, we propose HybridTM, the first hybrid architecture that integrates Transformer and Mamba for 3D semantic segmentation. In addition, we propose the Inner Layer Hybrid Strategy, which combines attention and Mamba at a finer granularity, enabling simultaneous capture of long-range dependencies and fine-grained local features. Extensive experiments demonstrate the effectiveness and generalization of our HybridTM on diverse indoor and outdoor datasets. Furthermore, our HybridTM achieves state-of-the-art performance on ScanNet, ScanNet200, and nuScenes benchmarks. The code will be made available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 85,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18594",
        "abs_url": "https://arxiv.org/abs/2507.18594",
        "pdf_url": "https://arxiv.org/pdf/2507.18594",
        "title": "DRWKV: Focusing on Object Edges for Low-Light Image Enhancement",
        "authors": [
            "Xuecheng Bai",
            "Yuxiang Wang",
            "Boyu Hu",
            "Qinyuan Jie",
            "Chuanzhi Xu",
            "Hongru Xiao",
            "Kechen Li",
            "Vera Chung"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Low-light image enhancement remains a challenging task, particularly in preserving object edge continuity and fine structural details under extreme illumination degradation. In this paper, we propose a novel model, DRWKV (Detailed Receptance Weighted Key Value), which integrates our proposed Global Edge Retinex (GER) theory, enabling effective decoupling of illumination and edge structures for enhanced edge fidelity. Secondly, we introduce Evolving WKV Attention, a spiral-scanning mechanism that captures spatial edge continuity and models irregular structures more effectively. Thirdly, we design the Bilateral Spectrum Aligner (Bi-SAB) and a tailored MS2-Loss to jointly align luminance and chrominance features, improving visual naturalness and mitigating artifacts. Extensive experiments on five LLIE benchmarks demonstrate that DRWKV achieves leading performance in PSNR, SSIM, and NIQE while maintaining low computational complexity. Furthermore, DRWKV enhances downstream performance in low-light multi-object tracking tasks, validating its generalization capabilities.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 86,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18616",
        "abs_url": "https://arxiv.org/abs/2507.18616",
        "pdf_url": "https://arxiv.org/pdf/2507.18616",
        "title": "SynC: Synthetic Image Caption Dataset Refinement with One-to-many Mapping for Zero-shot Image Captioning",
        "authors": [
            "Si-Woo Kim",
            "MinJu Jeon",
            "Ye-Chan Kim",
            "Soeun Lee",
            "Taewhan Kim",
            "Dong-Jin Kim"
        ],
        "comments": "Accepted to ACM Multimedia 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Zero-shot Image Captioning (ZIC) increasingly utilizes synthetic datasets generated by text-to-image (T2I) models to mitigate the need for costly manual annotation. However, these T2I models often produce images that exhibit semantic misalignments with their corresponding input captions (e.g., missing objects, incorrect attributes), resulting in noisy synthetic image-caption pairs that can hinder model training. Existing dataset pruning techniques are largely designed for removing noisy text in web-crawled data. However, these methods are ill-suited for the distinct challenges of synthetic data, where captions are typically well-formed, but images may be inaccurate representations. To address this gap, we introduce SynC, a novel framework specifically designed to refine synthetic image-caption datasets for ZIC. Instead of conventional filtering or regeneration, SynC focuses on reassigning captions to the most semantically aligned images already present within the synthetic image pool. Our approach employs a one-to-many mapping strategy by initially retrieving multiple relevant candidate images for each caption. We then apply a cycle-consistency-inspired alignment scorer that selects the best image by verifying its ability to retrieve the original caption via image-to-text retrieval. Extensive evaluations demonstrate that SynC consistently and significantly improves performance across various ZIC models on standard benchmarks (MS-COCO, Flickr30k, NoCaps), achieving state-of-the-art results in several scenarios. SynC offers an effective strategy for curating refined synthetic data to enhance ZIC.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 87,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18625",
        "abs_url": "https://arxiv.org/abs/2507.18625",
        "pdf_url": "https://arxiv.org/pdf/2507.18625",
        "title": "3D Software Synthesis Guided by Constraint-Expressive Intermediate Representation",
        "authors": [
            "Shuqing Li",
            "Anson Y. Lam",
            "Yun Peng",
            "Wenxuan Wang",
            "Michael R. Lyu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Multimedia (cs.MM); Software Engineering (cs.SE)",
        "abstract": "Graphical user interface (UI) software has undergone a fundamental transformation from traditional two-dimensional (2D) desktop/web/mobile interfaces to spatial three-dimensional (3D) environments. While existing work has made remarkable success in automated 2D software generation, such as HTML/CSS and mobile app interface code synthesis, the generation of 3D software still remains under-explored. Current methods for 3D software generation usually generate the 3D environments as a whole and cannot modify or control specific elements in the software. Furthermore, these methods struggle to handle the complex spatial and semantic constraints inherent in the real world. To address the challenges, we present Scenethesis, a novel requirement-sensitive 3D software synthesis approach that maintains formal traceability between user specifications and generated 3D software. Scenethesis is built upon ScenethesisLang, a domain-specific language that serves as a granular constraint-aware intermediate representation (IR) to bridge natural language requirements and executable 3D software. It serves both as a comprehensive scene description language enabling fine-grained modification of 3D software elements and as a formal constraint-expressive specification language capable of expressing complex spatial constraints. By decomposing 3D software synthesis into stages operating on ScenethesisLang, Scenethesis enables independent verification, targeted modification, and systematic constraint satisfaction. Our evaluation demonstrates that Scenethesis accurately captures over 80% of user requirements and satisfies more than 90% of hard constraints while handling over 100 constraints simultaneously. Furthermore, Scenethesis achieves a 42.8% improvement in BLIP-2 visual evaluation scores compared to the state-of-the-art method.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 88,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18632",
        "abs_url": "https://arxiv.org/abs/2507.18632",
        "pdf_url": "https://arxiv.org/pdf/2507.18632",
        "title": "SIDA: Synthetic Image Driven Zero-shot Domain Adaptation",
        "authors": [
            "Ye-Chan Kim",
            "SeungJu Cha",
            "Si-Woo Kim",
            "Taewhan Kim",
            "Dong-Jin Kim"
        ],
        "comments": "Accepted to ACM MM 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multimedia (cs.MM)",
        "abstract": "Zero-shot domain adaptation is a method for adapting a model to a target domain without utilizing target domain image data. To enable adaptation without target images, existing studies utilize CLIP's embedding space and text description to simulate target-like style features. Despite the previous achievements in zero-shot domain adaptation, we observe that these text-driven methods struggle to capture complex real-world variations and significantly increase adaptation time due to their alignment process. Instead of relying on text descriptions, we explore solutions leveraging image data, which provides diverse and more fine-grained style cues. In this work, we propose SIDA, a novel and efficient zero-shot domain adaptation method leveraging synthetic images. To generate synthetic images, we first create detailed, source-like images and apply image translation to reflect the style of the target domain. We then utilize the style features of these synthetic images as a proxy for the target domain. Based on these features, we introduce Domain Mix and Patch Style Transfer modules, which enable effective modeling of real-world variations. In particular, Domain Mix blends multiple styles to expand the intra-domain representations, and Patch Style Transfer assigns different styles to individual patches. We demonstrate the effectiveness of our method by showing state-of-the-art performance in diverse zero-shot adaptation scenarios, particularly in challenging domains. Moreover, our approach achieves high efficiency by significantly reducing the overall adaptation time.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 89,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18633",
        "abs_url": "https://arxiv.org/abs/2507.18633",
        "pdf_url": "https://arxiv.org/pdf/2507.18633",
        "title": "Identifying Prompted Artist Names from Generated Images",
        "authors": [
            "Grace Su",
            "Sheng-Yu Wang",
            "Aaron Hertzmann",
            "Eli Shechtman",
            "Jun-Yan Zhu",
            "Richard Zhang"
        ],
        "comments": "Project page: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "A common and controversial use of text-to-image models is to generate pictures by explicitly naming artists, such as \"in the style of Greg Rutkowski\". We introduce a benchmark for prompted-artist recognition: predicting which artist names were invoked in the prompt from the image alone. The dataset contains 1.95M images covering 110 artists and spans four generalization settings: held-out artists, increasing prompt complexity, multiple-artist prompts, and different text-to-image models. We evaluate feature similarity baselines, contrastive style descriptors, data attribution methods, supervised classifiers, and few-shot prototypical networks. Generalization patterns vary: supervised and few-shot models excel on seen artists and complex prompts, whereas style descriptors transfer better when the artist's style is pronounced; multi-artist prompts remain the most challenging. Our benchmark reveals substantial headroom and provides a public testbed to advance the responsible moderation of text-to-image models. We release the dataset and benchmark to foster further research: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 90,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18634",
        "abs_url": "https://arxiv.org/abs/2507.18634",
        "pdf_url": "https://arxiv.org/pdf/2507.18634",
        "title": "Captain Cinema: Towards Short Movie Generation",
        "authors": [
            "Junfei Xiao",
            "Ceyuan Yang",
            "Lvmin Zhang",
            "Shengqu Cai",
            "Yang Zhao",
            "Yuwei Guo",
            "Gordon Wetzstein",
            "Maneesh Agrawala",
            "Alan Yuille",
            "Lu Jiang"
        ],
        "comments": "Under review. Project page: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We present Captain Cinema, a generation framework for short movie generation. Given a detailed textual description of a movie storyline, our approach firstly generates a sequence of keyframes that outline the entire narrative, which ensures long-range coherence in both the storyline and visual appearance (e.g., scenes and characters). We refer to this step as top-down keyframe planning. These keyframes then serve as conditioning signals for a video synthesis model, which supports long context learning, to produce the spatio-temporal dynamics between them. This step is referred to as bottom-up video synthesis. To support stable and efficient generation of multi-scene long narrative cinematic works, we introduce an interleaved training strategy for Multimodal Diffusion Transformers (MM-DiT), specifically adapted for long-context video data. Our model is trained on a specially curated cinematic dataset consisting of interleaved data pairs. Our experiments demonstrate that Captain Cinema performs favorably in the automated creation of visually coherent and narrative consistent short movies in high quality and efficiency. Project page: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 91,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.17764",
        "abs_url": "https://arxiv.org/abs/2507.17764",
        "pdf_url": "https://arxiv.org/pdf/2507.17764",
        "title": "Diffusion-Assisted Frequency Attention Model for Whole-body Low-field MRI Reconstruction",
        "authors": [
            "Xin Xie",
            "Yu Guan",
            "Zhuoxu Cui",
            "Dong Liang",
            "Qiegen Liu"
        ],
        "comments": "29 pages,7 figures",
        "subjects": "Medical Physics (physics.med-ph); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "By integrating the generative strengths of diffusion models with the representation capabilities of frequency-domain attention, DFAM effectively enhances reconstruction performance under low-SNR condi-tions. Experimental results demonstrate that DFAM consistently outperforms both conventional reconstruction algorithms and recent learning-based approaches. These findings highlight the potential of DFAM as a promising solution to advance low-field MRI reconstruction, particularly in resource-constrained or underdeveloped clinical settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 92,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.17768",
        "abs_url": "https://arxiv.org/abs/2507.17768",
        "pdf_url": "https://arxiv.org/pdf/2507.17768",
        "title": "Enhancing Quantization-Aware Training on Edge Devices via Relative Entropy Coreset Selection and Cascaded Layer Correction",
        "authors": [
            "Yujia Tong",
            "Jingling Yuan",
            "Chuang Hu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "With the development of mobile and edge computing, the demand for low-bit quantized models on edge devices is increasing to achieve efficient deployment. To enhance the performance, it is often necessary to retrain the quantized models using edge data. However, due to privacy concerns, certain sensitive data can only be processed on edge devices. Therefore, employing Quantization-Aware Training (QAT) on edge devices has become an effective solution. Nevertheless, traditional QAT relies on the complete dataset for training, which incurs a huge computational cost. Coreset selection techniques can mitigate this issue by training on the most representative subsets. However, existing methods struggle to eliminate quantization errors in the model when using small-scale datasets (e.g., only 10% of the data), leading to significant performance degradation. To address these issues, we propose QuaRC, a QAT framework with coresets on edge devices, which consists of two main phases: In the coreset selection phase, QuaRC introduces the ``Relative Entropy Score\" to identify the subsets that most effectively capture the model's quantization errors. During the training phase, QuaRC employs the Cascaded Layer Correction strategy to align the intermediate layer outputs of the quantized model with those of the full-precision model, thereby effectively reducing the quantization errors in the intermediate layers. Experimental results demonstrate the effectiveness of our approach. For instance, when quantizing ResNet-18 to 2-bit using a 1% data subset, QuaRC achieves a 5.72% improvement in Top-1 accuracy on the ImageNet-1K dataset compared to state-of-the-art techniques.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 93,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.17772",
        "abs_url": "https://arxiv.org/abs/2507.17772",
        "pdf_url": "https://arxiv.org/pdf/2507.17772",
        "title": "Caching Techniques for Reducing the Communication Cost of Federated Learning in IoT Environments",
        "authors": [
            "Ahmad Alhonainy",
            "Praveen Rao"
        ],
        "comments": "Journal",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Federated Learning (FL) allows multiple distributed devices to jointly train a shared model without centralizing data, but communication cost remains a major bottleneck, especially in resource-constrained environments. This paper introduces caching strategies - FIFO, LRU, and Priority-Based - to reduce unnecessary model update transmissions. By selectively forwarding significant updates, our approach lowers bandwidth usage while maintaining model accuracy. Experiments on CIFAR-10 and medical datasets show reduced communication with minimal accuracy loss. Results confirm that intelligent caching improves scalability, memory efficiency, and supports reliable FL in edge IoT networks, making it practical for deployment in smart cities, healthcare, and other latency-sensitive applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 94,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.17800",
        "abs_url": "https://arxiv.org/abs/2507.17800",
        "pdf_url": "https://arxiv.org/pdf/2507.17800",
        "title": "Improving Multislice Electron Ptychography with a Generative Prior",
        "authors": [
            "Christian K. Belardi",
            "Chia-Hao Lee",
            "Yingheng Wang",
            "Justin Lovelace",
            "Kilian Q. Weinberger",
            "David A. Muller",
            "Carla P. Gomes"
        ],
        "comments": "16 pages, 10 figures, 5 tables",
        "subjects": "Image and Video Processing (eess.IV); Materials Science (cond-mat.mtrl-sci); Computer Vision and Pattern Recognition (cs.CV); Optics (physics.optics)",
        "abstract": "Multislice electron ptychography (MEP) is an inverse imaging technique that computationally reconstructs the highest-resolution images of atomic crystal structures from diffraction patterns. Available algorithms often solve this inverse problem iteratively but are both time consuming and produce suboptimal solutions due to their ill-posed nature. We develop MEP-Diffusion, a diffusion model trained on a large database of crystal structures specifically for MEP to augment existing iterative solvers. MEP-Diffusion is easily integrated as a generative prior into existing reconstruction methods via Diffusion Posterior Sampling (DPS). We find that this hybrid approach greatly enhances the quality of the reconstructed 3D volumes, achieving a 90.50% improvement in SSIM over existing methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 95,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.17845",
        "abs_url": "https://arxiv.org/abs/2507.17845",
        "pdf_url": "https://arxiv.org/pdf/2507.17845",
        "title": "Towards Robust Foundation Models for Digital Pathology",
        "authors": [
            "Jonah KÃ¶men",
            "Edwin D. de Jong",
            "Julius Hense",
            "Hannah Marienwald",
            "Jonas Dippel",
            "Philip Naumann",
            "Eric Marcus",
            "Lukas Ruff",
            "Maximilian Alber",
            "Jonas Teuwen",
            "Frederick Klauschen",
            "Klaus-Robert MÃ¼ller"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)",
        "abstract": "Biomedical Foundation Models (FMs) are rapidly transforming AI-enabled healthcare research and entering clinical validation. However, their susceptibility to learning non-biological technical features -- including variations in surgical/endoscopic techniques, laboratory procedures, and scanner hardware -- poses risks for clinical deployment. We present the first systematic investigation of pathology FM robustness to non-biological features. Our work (i) introduces measures to quantify FM robustness, (ii) demonstrates the consequences of limited robustness, and (iii) proposes a framework for FM robustification to mitigate these issues. Specifically, we developed PathoROB, a robustness benchmark with three novel metrics, including the robustness index, and four datasets covering 28 biological classes from 34 medical centers. Our experiments reveal robustness deficits across all 20 evaluated FMs, and substantial robustness differences between them. We found that non-robust FM representations can cause major diagnostic downstream errors and clinical blunders that prevent safe clinical adoption. Using more robust FMs and post-hoc robustification considerably reduced (but did not yet eliminate) the risk of such errors. This work establishes that robustness evaluation is essential for validating pathology FMs before clinical adoption and demonstrates that future FM development must integrate robustness as a core design principle. PathoROB provides a blueprint for assessing robustness across biomedical domains, guiding FM improvement efforts towards more robust, representative, and clinically deployable AI systems that prioritize biological information over technical artifacts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 96,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.17869",
        "abs_url": "https://arxiv.org/abs/2507.17869",
        "pdf_url": "https://arxiv.org/pdf/2507.17869",
        "title": "Integrating Feature Selection and Machine Learning for Nitrogen Assessment in Grapevine Leaves using In-Field Hyperspectral Imaging",
        "authors": [
            "Atif Bilal Asad",
            "Achyut Paudel",
            "Safal Kshetri",
            "Chenchen Kang",
            "Salik Ram Khanal",
            "Nataliya Shcherbatyuk",
            "Pierre Davadant",
            "R. Paul Schreiner",
            "Santosh Kalauni",
            "Manoj Karkee",
            "Markus Keller"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Nitrogen (N) is one of the most crucial nutrients in vineyards, affecting plant growth and subsequent products such as wine and juice. Because soil N has high spatial and temporal variability, it is desirable to accurately estimate the N concentration of grapevine leaves and manage fertilization at the individual plant level to optimally meet plant needs. In this study, we used in-field hyperspectral images with wavelengths ranging from $400 to 1000nm of four different grapevine cultivars collected from distinct vineyards and over two growth stages during two growing seasons to develop models for predicting N concentration at the leaf-level and canopy-level. After image processing, two feature selection methods were employed to identify the optimal set of spectral bands that were responsive to leaf N concentrations. The selected spectral bands were used to train and test two different Machine Learning (ML) models, Gradient Boosting and XGBoost, for predicting nitrogen concentrations. The comparison of selected bands for both leaf-level and canopy-level datasets showed that most of the spectral regions identified by the feature selection methods were across both methods and the dataset types (leaf- and canopy-level datasets), particularly in the key regions, 500-525nm, 650-690nm, 750-800nm, and 900-950nm. These findings indicated the robustness of these spectral regions for predicting nitrogen content. The results for N prediction demonstrated that the ML model achieved an R square of 0.49 for canopy-level data and an R square of 0.57 for leaf-level data, despite using different sets of selected spectral bands for each analysis level. The study demonstrated the potential of using in-field hyperspectral imaging and the use of spectral data in integrated feature selection and ML techniques to monitor N status in vineyards.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 97,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.17897",
        "abs_url": "https://arxiv.org/abs/2507.17897",
        "pdf_url": "https://arxiv.org/pdf/2507.17897",
        "title": "Multimodal Recurrent Ensembles for Predicting Brain Responses to Naturalistic Movies (Algonauts 2025)",
        "authors": [
            "Semih Eren",
            "Deniz Kucukahmetler",
            "Nico Scherf"
        ],
        "comments": "8 pages, 2 figures, 1 table. Invited report, CCN 2025 Algonauts Project session (3rd-place team). Code: this https URL",
        "subjects": "Neurons and Cognition (q-bio.NC); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Accurately predicting distributed cortical responses to naturalistic stimuli requires models that integrate visual, auditory and semantic information over time. We present a hierarchical multimodal recurrent ensemble that maps pretrained video, audio, and language embeddings to fMRI time series recorded while four subjects watched almost 80 hours of movies provided by the Algonauts 2025 challenge. Modality-specific bidirectional RNNs encode temporal dynamics; their hidden states are fused and passed to a second recurrent layer, and lightweight subject-specific heads output responses for 1000 cortical parcels. Training relies on a composite MSE-correlation loss and a curriculum that gradually shifts emphasis from early sensory to late association regions. Averaging 100 model variants further boosts robustness. The resulting system ranked third on the competition leaderboard, achieving an overall Pearson r = 0.2094 and the highest single-parcel peak score (mean r = 0.63) among all participants, with particularly strong gains for the most challenging subject (Subject 5). The approach establishes a simple, extensible baseline for future multimodal brain-encoding benchmarks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 98,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.17911",
        "abs_url": "https://arxiv.org/abs/2507.17911",
        "pdf_url": "https://arxiv.org/pdf/2507.17911",
        "title": "Hierarchical Diffusion Framework for Pseudo-Healthy Brain MRI Inpainting with Enhanced 3D Consistency",
        "authors": [
            "Dou Hoon Kwark",
            "Shirui Luo",
            "Xiyue Zhu",
            "Yudu Li",
            "Zhi-Pei Liang",
            "Volodymyr Kindratenko"
        ],
        "comments": "11 pages, 2 figures",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Pseudo-healthy image inpainting is an essential preprocessing step for analyzing pathological brain MRI scans. Most current inpainting methods favor slice-wise 2D models for their high in-plane fidelity, but their independence across slices produces discontinuities in the volume. Fully 3D models alleviate this issue, but their high model capacity demands extensive training data for reliable, high-fidelity synthesis -- often impractical in medical settings. We address these limitations with a hierarchical diffusion framework by replacing direct 3D modeling with two perpendicular coarse-to-fine 2D stages. An axial diffusion model first yields a coarse, globally consistent inpainting; a coronal diffusion model then refines anatomical details. By combining perpendicular spatial views with adaptive resampling, our method balances data efficiency and volumetric consistency. Our experiments show our approach outperforms state-of-the-art baselines in both realism and volumetric consistency, making it a promising solution for pseudo-healthy image inpainting. Code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 99,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.17958",
        "abs_url": "https://arxiv.org/abs/2507.17958",
        "pdf_url": "https://arxiv.org/pdf/2507.17958",
        "title": "VIBE: Video-Input Brain Encoder for fMRI Response Modeling",
        "authors": [
            "Daniel Carlstrom Schad",
            "Shrey Dixit",
            "Janis Keck",
            "Viktor Studenyak",
            "Aleksandr Shpilevoi",
            "Andrej Bicanski"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We present VIBE, a two-stage Transformer that fuses multi-modal video, audio, and text features to predict fMRI activity. Representations from open-source models (Qwen2.5, BEATs, Whisper, SlowFast, V-JEPA) are merged by a modality-fusion transformer and temporally decoded by a prediction transformer with rotary embeddings. Trained on 65 hours of movie data from the CNeuroMod dataset and ensembled across 20 seeds, VIBE attains mean parcel-wise Pearson correlations of 32.25 on in-distribution Friends S07 and 21.25 on six out-of-distribution films. An earlier iteration of the same architecture obtained 0.3198 and 0.2096, respectively, winning Phase-1 and placing second overall in the Algonauts 2025 Challenge.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 100,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.17963",
        "abs_url": "https://arxiv.org/abs/2507.17963",
        "pdf_url": "https://arxiv.org/pdf/2507.17963",
        "title": "Zero-Shot Dynamic Concept Personalization with Grid-Based LoRA",
        "authors": [
            "Rameen Abdal",
            "Or Patashnik",
            "Ekaterina Deyneka",
            "Hao Chen",
            "Aliaksandr Siarohin",
            "Sergey Tulyakov",
            "Daniel Cohen-Or",
            "Kfir Aberman"
        ],
        "comments": "Project Page and Video : this https URL",
        "subjects": "Graphics (cs.GR); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Recent advances in text-to-video generation have enabled high-quality synthesis from text and image prompts. While the personalization of dynamic concepts, which capture subject-specific appearance and motion from a single video, is now feasible, most existing methods require per-instance fine-tuning, limiting scalability. We introduce a fully zero-shot framework for dynamic concept personalization in text-to-video models. Our method leverages structured 2x2 video grids that spatially organize input and output pairs, enabling the training of lightweight Grid-LoRA adapters for editing and composition within these grids. At inference, a dedicated Grid Fill module completes partially observed layouts, producing temporally coherent and identity preserving outputs. Once trained, the entire system operates in a single forward pass, generalizing to previously unseen dynamic concepts without any test-time optimization. Extensive experiments demonstrate high-quality and consistent results across a wide range of subjects beyond trained concepts and editing scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 101,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.17971",
        "abs_url": "https://arxiv.org/abs/2507.17971",
        "pdf_url": "https://arxiv.org/pdf/2507.17971",
        "title": "Benchmarking of Deep Learning Methods for Generic MRI Multi-OrganAbdominal Segmentation",
        "authors": [
            "Deepa Krishnaswamy",
            "Cosmin Ciausu",
            "Steve Pieper",
            "Ron Kikinis",
            "Benjamin Billot",
            "Andrey Fedorov"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Recent advances in deep learning have led to robust automated tools for segmentation of abdominal computed tomography (CT). Meanwhile, segmentation of magnetic resonance imaging (MRI) is substantially more challenging due to the inherent signal variability and the increased effort required for annotating training datasets. Hence, existing approaches are trained on limited sets of MRI sequences, which might limit their generalizability. To characterize the landscape of MRI abdominal segmentation tools, we present here a comprehensive benchmarking of the three state-of-the-art and open-source models: MRSegmentator, MRISegmentator-Abdomen, and TotalSegmentator MRI. Since these models are trained using labor-intensive manual annotation cycles, we also introduce and evaluate ABDSynth, a SynthSeg-based model purely trained on widely available CT segmentations (no real images). More generally, we assess accuracy and generalizability by leveraging three public datasets (not seen by any of the evaluated methods during their training), which span all major manufacturers, five MRI sequences, as well as a variety of subject conditions, voxel resolutions, and fields-of-view. Our results reveal that MRSegmentator achieves the best performance and is most generalizable. In contrast, ABDSynth yields slightly less accurate results, but its relaxed requirements in training data make it an alternative when the annotation budget is limited. The evaluation code and datasets are given for future benchmarking at this https URL, along with inference code and weights for ABDSynth.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 102,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18012",
        "abs_url": "https://arxiv.org/abs/2507.18012",
        "pdf_url": "https://arxiv.org/pdf/2507.18012",
        "title": "Direct Dual-Energy CT Material Decomposition using Model-based Denoising Diffusion Model",
        "authors": [
            "Hang Xu",
            "Alexandre Bousse",
            "Alessandro Perelli"
        ],
        "comments": "13 pages, 10 figures, 2 tables",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV); Medical Physics (physics.med-ph)",
        "abstract": "Dual-energy X-ray Computed Tomography (DECT) constitutes an advanced technology which enables automatic decomposition of materials in clinical images without manual segmentation using the dependency of the X-ray linear attenuation with energy. However, most methods perform material decomposition in the image domain as a post-processing step after reconstruction but this procedure does not account for the beam-hardening effect and it results in sub-optimal results. In this work, we propose a deep learning procedure called Dual-Energy Decomposition Model-based Diffusion (DEcomp-MoD) for quantitative material decomposition which directly converts the DECT projection data into material images. The algorithm is based on incorporating the knowledge of the spectral DECT model into the deep learning training loss and combining a score-based denoising diffusion learned prior in the material image domain. Importantly the inference optimization loss takes as inputs directly the sinogram and converts to material images through a model-based conditional diffusion model which guarantees consistency of the results. We evaluate the performance with both quantitative and qualitative estimation of the proposed DEcomp-MoD method on synthetic DECT sinograms from the low-dose AAPM dataset. Finally, we show that DEcomp-MoD outperform state-of-the-art unsupervised score-based model and supervised deep learning networks, with the potential to be deployed for clinical diagnosis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 103,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18036",
        "abs_url": "https://arxiv.org/abs/2507.18036",
        "pdf_url": "https://arxiv.org/pdf/2507.18036",
        "title": "NWaaS: Nonintrusive Watermarking as a Service for X-to-Image DNN",
        "authors": [
            "Haonan An",
            "Guang Hua",
            "Yu Guo",
            "Hangcheng Cao",
            "Susanto Rahardja",
            "Yuguang Fang"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The intellectual property of deep neural network (DNN) models can be protected with DNN watermarking, which embeds copyright watermarks into model parameters (white-box), model behavior (black-box), or model outputs (box-free), and the watermarks can be subsequently extracted to verify model ownership or detect model theft. Despite recent advances, these existing methods are inherently intrusive, as they either modify the model parameters or alter the structure. This natural intrusiveness raises concerns about watermarking-induced shifts in model behavior and the additional cost of fine-tuning, further exacerbated by the rapidly growing model size. As a result, model owners are often reluctant to adopt DNN watermarking in practice, which limits the development of practical Watermarking as a Service (WaaS) systems. To address this issue, we introduce Nonintrusive Watermarking as a Service (NWaaS), a novel trustless paradigm designed for X-to-Image models, in which we hypothesize that with the model untouched, an owner-defined watermark can still be extracted from model outputs. Building on this concept, we propose ShadowMark, a concrete implementation of NWaaS which addresses critical deployment challenges by establishing a robust and nonintrusive side channel in the protected model's black-box API, leveraging a key encoder and a watermark decoder. It is significantly distinctive from existing solutions by attaining the so-called absolute fidelity and being applicable to different DNN architectures, while being also robust against existing attacks, eliminating the fidelity-robustness trade-off. Extensive experiments on image-to-image, noise-to-image, noise-and-text-to-image, and text-to-image models, demonstrate the efficacy and practicality of ShadowMark for real-world deployment of nonintrusive DNN watermarking.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 104,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18043",
        "abs_url": "https://arxiv.org/abs/2507.18043",
        "pdf_url": "https://arxiv.org/pdf/2507.18043",
        "title": "GrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs and VLMs",
        "authors": [
            "Duy Nguyen",
            "Archiki Prasad",
            "Elias Stengel-Eskin",
            "Mohit Bansal"
        ],
        "comments": "21 pages. Code: this https URL",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Inference-time steering methods offer a lightweight alternative to fine-tuning large language models (LLMs) and vision-language models (VLMs) by modifying internal activations at test time without updating model weights. However, most existing approaches rely on fixed, global intervention vectors, overlook the causal influence of individual input tokens, and fail to leverage informative gradients from the model's logits, particularly in multimodal settings where visual and textual inputs contribute unevenly. To address these limitations, we introduce GrAInS, an inference-time steering approach that operates across both language-only and vision-language models and tasks. GrAInS uses contrastive, gradient-based attribution via Integrated Gradients to identify the top-k most influential tokens, both positively and negatively attributed based on their contribution to preferred versus dispreferred outputs. These tokens are then used to construct directional steering vectors that capture semantic shifts from undesirable to desirable behavior. During inference, GrAInS adjusts hidden activations at transformer layers guided by token-level attribution signals, and normalizes activations to preserve representational scale. This enables fine-grained, interpretable, and modular control over model behavior, without retraining or auxiliary supervision. Empirically, GrAInS consistently outperforms both fine-tuning and existing steering baselines: it achieves a 13.22% accuracy gain on TruthfulQA using Llama-3.1-8B, reduces hallucination rates on MMHal-Bench from 0.624 to 0.514 with LLaVA-1.6-7B, and improves alignment win rates on SPA-VL by 8.11%, all while preserving the model's fluency and general capabilities.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 105,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18112",
        "abs_url": "https://arxiv.org/abs/2507.18112",
        "pdf_url": "https://arxiv.org/pdf/2507.18112",
        "title": "Parameter-Efficient Fine-Tuning of 3D DDPM for MRI Image Generation Using Tensor Networks",
        "authors": [
            "Binghua Li",
            "Ziqing Chang",
            "Tong Liang",
            "Chao Li",
            "Toshihisa Tanaka",
            "Shigeki Aoki",
            "Qibin Zhao",
            "Zhe Sun"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We address the challenge of parameter-efficient fine-tuning (PEFT) for three-dimensional (3D) U-Net-based denoising diffusion probabilistic models (DDPMs) in magnetic resonance imaging (MRI) image generation. Despite its practical significance, research on parameter-efficient representations of 3D convolution operations remains limited. To bridge this gap, we propose Tensor Volumetric Operator (TenVOO), a novel PEFT method specifically designed for fine-tuning DDPMs with 3D convolutional backbones. Leveraging tensor network modeling, TenVOO represents 3D convolution kernels with lower-dimensional tensors, effectively capturing complex spatial dependencies during fine-tuning with few parameters. We evaluate TenVOO on three downstream brain MRI datasets-ADNI, PPMI, and BraTS2021-by fine-tuning a DDPM pretrained on 59,830 T1-weighted brain MRI scans from the UK Biobank. Our results demonstrate that TenVOO achieves state-of-the-art performance in multi-scale structural similarity index measure (MS-SSIM), outperforming existing approaches in capturing spatial dependencies while requiring only 0.3% of the trainable parameters of the original model. Our code is available at: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 106,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18126",
        "abs_url": "https://arxiv.org/abs/2507.18126",
        "pdf_url": "https://arxiv.org/pdf/2507.18126",
        "title": "U-Net Based Healthy 3D Brain Tissue Inpainting",
        "authors": [
            "Juexin Zhang",
            "Ying Weng",
            "Ke Chen"
        ],
        "comments": "Accepted by the International Brain Tumor Segmentation (BraTS) challenge organized at MICCAI 2024 conference. Included 7 pages, 2 figures",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "This paper introduces a novel approach to synthesize healthy 3D brain tissue from masked input images, specifically focusing on the task of 'ASNR-MICCAI BraTS Local Synthesis of Tissue via Inpainting'. Our proposed method employs a U-Net-based architecture, which is designed to effectively reconstruct the missing or corrupted regions of brain MRI scans. To enhance our model's generalization capabilities and robustness, we implement a comprehensive data augmentation strategy that involves randomly masking healthy images during training. Our model is trained on the BraTS-Local-Inpainting dataset and demonstrates the exceptional performance in recovering healthy brain tissue. The evaluation metrics employed, including Structural Similarity Index (SSIM), Peak Signal-to-Noise Ratio (PSNR), and Mean Squared Error (MSE), consistently yields impressive results. On the BraTS-Local-Inpainting validation set, our model achieved an SSIM score of 0.841, a PSNR score of 23.257, and an MSE score of 0.007. Notably, these evaluation metrics exhibit relatively low standard deviations, i.e., 0.103 for SSIM score, 4.213 for PSNR score and 0.007 for MSE score, which indicates that our model's reliability and consistency across various input scenarios. Our method also secured first place in the challenge.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 107,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18133",
        "abs_url": "https://arxiv.org/abs/2507.18133",
        "pdf_url": "https://arxiv.org/pdf/2507.18133",
        "title": "Deep Learning for Glioblastoma Morpho-pathological Features Identification: A BraTS-Pathology Challenge Solution",
        "authors": [
            "Juexin Zhang",
            "Ying Weng",
            "Ke Chen"
        ],
        "comments": "Accepted by the International Brain Tumor Segmentation (BraTS) challenge organized at MICCAI 2024 conference",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Glioblastoma, a highly aggressive brain tumor with diverse molecular and pathological features, poses a diagnostic challenge due to its heterogeneity. Accurate diagnosis and assessment of this heterogeneity are essential for choosing the right treatment and improving patient outcomes. Traditional methods rely on identifying specific features in tissue samples, but deep learning offers a promising approach for improved glioblastoma diagnosis. In this paper, we present our approach to the BraTS-Path Challenge 2024. We leverage a pre-trained model and fine-tune it on the BraTS-Path training dataset. Our model demonstrates poor performance on the challenging BraTS-Path validation set, as rigorously assessed by the Synapse online platform. The model achieves an accuracy of 0.392229, a recall of 0.392229, and a F1-score of 0.392229, indicating a consistent ability to correctly identify instances under the target condition. Notably, our model exhibits perfect specificity of 0.898704, showing an exceptional capacity to correctly classify negative cases. Moreover, a Matthews Correlation Coefficient (MCC) of 0.255267 is calculated, to signify a limited positive correlation between predicted and actual values and highlight our model's overall predictive power. Our solution also achieves the second place during the testing phase.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 108,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18155",
        "abs_url": "https://arxiv.org/abs/2507.18155",
        "pdf_url": "https://arxiv.org/pdf/2507.18155",
        "title": "GeoAvatar: Adaptive Geometrical Gaussian Splatting for 3D Head Avatar",
        "authors": [
            "SeungJun Moon",
            "Hah Min Lew",
            "Seungeun Lee",
            "Ji-Su Kang",
            "Gyeong-Moon Park"
        ],
        "comments": "ICCV 2025, Project page: this https URL",
        "subjects": "Graphics (cs.GR); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Despite recent progress in 3D head avatar generation, balancing identity preservation, i.e., reconstruction, with novel poses and expressions, i.e., animation, remains a challenge. Existing methods struggle to adapt Gaussians to varying geometrical deviations across facial regions, resulting in suboptimal quality. To address this, we propose GeoAvatar, a framework for adaptive geometrical Gaussian Splatting. GeoAvatar leverages Adaptive Pre-allocation Stage (APS), an unsupervised method that segments Gaussians into rigid and flexible sets for adaptive offset regularization. Then, based on mouth anatomy and dynamics, we introduce a novel mouth structure and the part-wise deformation strategy to enhance the animation fidelity of the mouth. Finally, we propose a regularization loss for precise rigging between Gaussians and 3DMM faces. Moreover, we release DynamicFace, a video dataset with highly expressive facial motions. Extensive experiments show the superiority of GeoAvatar compared to state-of-the-art methods in reconstruction and novel animation scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 109,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18183",
        "abs_url": "https://arxiv.org/abs/2507.18183",
        "pdf_url": "https://arxiv.org/pdf/2507.18183",
        "title": "ChronoSelect: Robust Learning with Noisy Labels via Dynamics Temporal Memory",
        "authors": [
            "Jianchao Wang",
            "Qingfeng Li",
            "Pengcheng Zheng",
            "Xiaorong Pu",
            "Yazhou Ren"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Training deep neural networks on real-world datasets is often hampered by the presence of noisy labels, which can be memorized by over-parameterized models, leading to significant degradation in generalization performance. While existing methods for learning with noisy labels (LNL) have made considerable progress, they fundamentally suffer from static snapshot evaluations and fail to leverage the rich temporal dynamics of learning evolution. In this paper, we propose ChronoSelect (chrono denoting its temporal nature), a novel framework featuring an innovative four-stage memory architecture that compresses prediction history into compact temporal distributions. Our unique sliding update mechanism with controlled decay maintains only four dynamic memory units per sample, progressively emphasizing recent patterns while retaining essential historical knowledge. This enables precise three-way sample partitioning into clean, boundary, and noisy subsets through temporal trajectory analysis and dual-branch consistency. Theoretical guarantees prove the mechanism's convergence and stability under noisy conditions. Extensive experiments demonstrate ChronoSelect's state-of-the-art performance across synthetic and real-world benchmarks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 110,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18231",
        "abs_url": "https://arxiv.org/abs/2507.18231",
        "pdf_url": "https://arxiv.org/pdf/2507.18231",
        "title": "PS-GS: Gaussian Splatting for Multi-View Photometric Stereo",
        "authors": [
            "Yixiao Chen",
            "Bin Liang",
            "Hanzhi Guo",
            "Yongqing Cheng",
            "Jiayi Zhao",
            "Dongdong Weng"
        ],
        "comments": "",
        "subjects": "Graphics (cs.GR); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Integrating inverse rendering with multi-view photometric stereo (MVPS) yields more accurate 3D reconstructions than the inverse rendering approaches that rely on fixed environment illumination. However, efficient inverse rendering with MVPS remains challenging. To fill this gap, we introduce the Gaussian Splatting for Multi-view Photometric Stereo (PS-GS), which efficiently and jointly estimates the geometry, materials, and lighting of the object that is illuminated by diverse directional lights (multi-light). Our method first reconstructs a standard 2D Gaussian splatting model as the initial geometry. Based on the initialization model, it then proceeds with the deferred inverse rendering by the full rendering equation containing a lighting-computing multi-layer perceptron. During the whole optimization, we regularize the rendered normal maps by the uncalibrated photometric stereo estimated normals. We also propose the 2D Gaussian ray-tracing for single directional light to refine the incident lighting. The regularizations and the use of multi-view and multi-light images mitigate the ill-posed problem of inverse rendering. After optimization, the reconstructed object can be used for novel-view synthesis, relighting, and material and shape editing. Experiments on both synthetic and real datasets demonstrate that our method outperforms prior works in terms of reconstruction accuracy and computational efficiency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 111,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18248",
        "abs_url": "https://arxiv.org/abs/2507.18248",
        "pdf_url": "https://arxiv.org/pdf/2507.18248",
        "title": "Evaluation of facial landmark localization performance in a surgical setting",
        "authors": [
            "Ines Frajtag",
            "Marko Å vaco",
            "Filip Å uligoj"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The use of robotics, computer vision, and their applications is becoming increasingly widespread in various fields, including medicine. Many face detection algorithms have found applications in neurosurgery, ophthalmology, and plastic surgery. A common challenge in using these algorithms is variable lighting conditions and the flexibility of detection positions to identify and precisely localize patients. The proposed experiment tests the MediaPipe algorithm for detecting facial landmarks in a controlled setting, using a robotic arm that automatically adjusts positions while the surgical light and the phantom remain in a fixed position. The results of this study demonstrate that the improved accuracy of facial landmark detection under surgical lighting significantly enhances the detection performance at larger yaw and pitch angles. The increase in standard deviation/dispersion occurs due to imprecise detection of selected facial landmarks. This analysis allows for a discussion on the potential integration of the MediaPipe algorithm into medical procedures.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 112,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18262",
        "abs_url": "https://arxiv.org/abs/2507.18262",
        "pdf_url": "https://arxiv.org/pdf/2507.18262",
        "title": "ReSem3D: Refinable 3D Spatial Constraints via Fine-Grained Semantic Grounding for Generalizable Robotic Manipulation",
        "authors": [
            "Chenyu Su",
            "Weiwei Shang",
            "Chen Qian",
            "Fei Zhang",
            "Shuang Cong"
        ],
        "comments": "12 pages,9 figures",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)",
        "abstract": "Semantics-driven 3D spatial constraints align highlevel semantic representations with low-level action spaces, facilitating the unification of task understanding and execution in robotic manipulation. The synergistic reasoning of Multimodal Large Language Models (MLLMs) and Vision Foundation Models (VFMs) enables cross-modal 3D spatial constraint construction. Nevertheless, existing methods have three key limitations: (1) coarse semantic granularity in constraint modeling, (2) lack of real-time closed-loop planning, (3) compromised robustness in semantically diverse environments. To address these challenges, we propose ReSem3D, a unified manipulation framework for semantically diverse environments, leveraging the synergy between VFMs and MLLMs to achieve fine-grained visual grounding and dynamically constructs hierarchical 3D spatial constraints for real-time manipulation. Specifically, the framework is driven by hierarchical recursive reasoning in MLLMs, which interact with VFMs to automatically construct 3D spatial constraints from natural language instructions and RGB-D observations in two stages: part-level extraction and region-level refinement. Subsequently, these constraints are encoded as real-time optimization objectives in joint space, enabling reactive behavior to dynamic disturbances. Extensive simulation and real-world experiments are conducted in semantically rich household and sparse chemical lab environments. The results demonstrate that ReSem3D performs diverse manipulation tasks under zero-shot conditions, exhibiting strong adaptability and generalization. Code and videos at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 113,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18276",
        "abs_url": "https://arxiv.org/abs/2507.18276",
        "pdf_url": "https://arxiv.org/pdf/2507.18276",
        "title": "Adaptive Articulated Object Manipulation On The Fly with Foundation Model Reasoning and Part Grounding",
        "authors": [
            "Xiaojie Zhang",
            "Yuanfei Wang",
            "Ruihai Wu",
            "Kunqi Xu",
            "Yu Li",
            "Liuyu Xiang",
            "Hao Dong",
            "Zhaofeng He"
        ],
        "comments": "ICCV 2025",
        "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Articulated objects pose diverse manipulation challenges for robots. Since their internal structures are not directly observable, robots must adaptively explore and refine actions to generate successful manipulation trajectories. While existing works have attempted cross-category generalization in adaptive articulated object manipulation, two major challenges persist: (1) the geometric diversity of real-world articulated objects complicates visual perception and understanding, and (2) variations in object functions and mechanisms hinder the development of a unified adaptive manipulation strategy. To address these challenges, we propose AdaRPG, a novel framework that leverages foundation models to extract object parts, which exhibit greater local geometric similarity than entire objects, thereby enhancing visual affordance generalization for functional primitive skills. To support this, we construct a part-level affordance annotation dataset to train the affordance model. Additionally, AdaRPG utilizes the common knowledge embedded in foundation models to reason about complex mechanisms and generate high-level control codes that invoke primitive skill functions based on part affordance inference. Simulation and real-world experiments demonstrate AdaRPG's strong generalization ability across novel articulated object categories.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 114,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18288",
        "abs_url": "https://arxiv.org/abs/2507.18288",
        "pdf_url": "https://arxiv.org/pdf/2507.18288",
        "title": "TCM-Tongue: A Standardized Tongue Image Dataset with Pathological Annotations for AI-Assisted TCM Diagnosis",
        "authors": [
            "Xuebo Jin",
            "Longfei Gao",
            "Anshuo Tong",
            "Zhengyang Chen",
            "Jianlei Kong",
            "Ning Sun",
            "Huijun Ma",
            "Qiang Wang",
            "Yuting Bai",
            "Tingli Su"
        ],
        "comments": "16 pages, 11 figures, 2 Tables",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Traditional Chinese medicine (TCM) tongue diagnosis, while clinically valuable, faces standardization challenges due to subjective interpretation and inconsistent imaging protocols, compounded by the lack of large-scale, annotated datasets for AI development. To address this gap, we present the first specialized dataset for AI-driven TCM tongue diagnosis, comprising 6,719 high-quality images captured under standardized conditions and annotated with 20 pathological symptom categories (averaging 2.54 clinically validated labels per image, all verified by licensed TCM practitioners). The dataset supports multiple annotation formats (COCO, TXT, XML) for broad usability and has been benchmarked using nine deep learning models (YOLOv5/v7/v8 variants, SSD, and MobileNetV2) to demonstrate its utility for AI development. This resource provides a critical foundation for advancing reliable computational tools in TCM, bridging the data shortage that has hindered progress in the field, and facilitating the integration of AI into both research and clinical practice through standardized, high-quality diagnostic data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 115,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18362",
        "abs_url": "https://arxiv.org/abs/2507.18362",
        "pdf_url": "https://arxiv.org/pdf/2507.18362",
        "title": "UniSegDiff: Boosting Unified Lesion Segmentation via a Staged Diffusion Model",
        "authors": [
            "Yilong Hu",
            "Shijie Chang",
            "Lihe Zhang",
            "Feng Tian",
            "Weibing Sun",
            "Huchuan Lu"
        ],
        "comments": "MICCAI2025",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The Diffusion Probabilistic Model (DPM) has demonstrated remarkable performance across a variety of generative tasks. The inherent randomness in diffusion models helps address issues such as blurring at the edges of medical images and labels, positioning Diffusion Probabilistic Models (DPMs) as a promising approach for lesion segmentation. However, we find that the current training and inference strategies of diffusion models result in an uneven distribution of attention across different timesteps, leading to longer training times and suboptimal solutions. To this end, we propose UniSegDiff, a novel diffusion model framework designed to address lesion segmentation in a unified manner across multiple modalities and organs. This framework introduces a staged training and inference approach, dynamically adjusting the prediction targets at different stages, forcing the model to maintain high attention across all timesteps, and achieves unified lesion segmentation through pre-training the feature extraction network for segmentation. We evaluate performance on six different organs across various imaging modalities. Comprehensive experimental results demonstrate that UniSegDiff significantly outperforms previous state-of-the-art (SOTA) approaches. The code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 116,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18433",
        "abs_url": "https://arxiv.org/abs/2507.18433",
        "pdf_url": "https://arxiv.org/pdf/2507.18433",
        "title": "DiagR1: A Vision-Language Model Trained via Reinforcement Learning for Digestive Pathology Diagnosis",
        "authors": [
            "Minxi Ouyang",
            "Lianghui Zhu",
            "Yaqing Bao",
            "Qiang Huang",
            "Jingli Ouyang",
            "Tian Guan",
            "Xitong Ling",
            "Jiawen Li",
            "Song Duan",
            "Wenbin Dai",
            "Li Zheng",
            "Xuemei Zhang",
            "Yonghong He"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Multimodal large models have shown great potential in automating pathology image analysis. However, current multimodal models for gastrointestinal pathology are constrained by both data quality and reasoning transparency: pervasive noise and incomplete annotations in public datasets predispose vision language models to factual hallucinations when generating diagnostic text, while the absence of explicit intermediate reasoning chains renders the outputs difficult to audit and thus less trustworthy in clinical practice. To address these issues, we construct a large scale gastrointestinal pathology dataset containing both microscopic descriptions and diagnostic conclusions, and propose a prompt argumentation strategy that incorporates lesion classification and anatomical site information. This design guides the model to better capture image specific features and maintain semantic consistency in generation. Furthermore, we employ a post training pipeline that combines supervised fine tuning with Group Relative Policy Optimization (GRPO) to improve reasoning quality and output structure. Experimental results on real world pathology report generation tasks demonstrate that our approach significantly outperforms state of the art open source and proprietary baselines in terms of generation quality, structural completeness, and clinical relevance. Our solution outperforms state of the art models with 18.7% higher clinical relevance, 32.4% improved structural completeness, and 41.2% fewer diagnostic errors, demonstrating superior accuracy and clinical utility compared to existing solutions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 117,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18550",
        "abs_url": "https://arxiv.org/abs/2507.18550",
        "pdf_url": "https://arxiv.org/pdf/2507.18550",
        "title": "On the Performance of Concept Probing: The Influence of the Data (Extended Version)",
        "authors": [
            "Manuel de Sousa Ribeiro",
            "Afonso Leote",
            "JoÃ£o Leite"
        ],
        "comments": "Extended version of the paper published in Proceedings of the European Conference on Artificial Intelligence (ECAI 2025)",
        "subjects": "Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)",
        "abstract": "Concept probing has recently garnered increasing interest as a way to help interpret artificial neural networks, dealing both with their typically large size and their subsymbolic nature, which ultimately renders them unfeasible for direct human interpretation. Concept probing works by training additional classifiers to map the internal representations of a model into human-defined concepts of interest, thus allowing humans to peek inside artificial neural networks. Research on concept probing has mainly focused on the model being probed or the probing model itself, paying limited attention to the data required to train such probing models. In this paper, we address this gap. Focusing on concept probing in the context of image classification tasks, we investigate the effect of the data used to train probing models on their performance. We also make available concept labels for two widely used datasets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 118,
        "date": "2025-07-25",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-25?abs=True",
        "arxiv_id": "2507.18576",
        "abs_url": "https://arxiv.org/abs/2507.18576",
        "pdf_url": "https://arxiv.org/pdf/2507.18576",
        "title": "SafeWork-R1: Coevolving Safety and Intelligence under the AI-45$^{\\circ}$ Law",
        "authors": [
            "Shanghai AI Lab",
            "Yicheng Bao",
            "Guanxu Chen",
            "Mingkang Chen",
            "Yunhao Chen",
            "Chiyu Chen",
            "Lingjie Chen",
            "Sirui Chen",
            "Xinquan Chen",
            "Jie Cheng",
            "Yu Cheng",
            "Dengke Deng",
            "Yizhuo Ding",
            "Dan Ding",
            "Xiaoshan Ding",
            "Yi Ding",
            "Zhichen Dong",
            "Lingxiao Du",
            "Yuyu Fan",
            "Xinshun Feng",
            "Yanwei Fu",
            "Yuxuan Gao",
            "Ruijun Ge",
            "Tianle Gu",
            "Lujun Gui",
            "Jiaxuan Guo",
            "Qianxi He",
            "Yuenan Hou",
            "Xuhao Hu",
            "Hong Huang",
            "Kaichen Huang",
            "Shiyang Huang",
            "Yuxian Jiang",
            "Shanzhe Lei",
            "Jie Li",
            "Lijun Li",
            "Hao Li",
            "Juncheng Li",
            "Xiangtian Li",
            "Yafu Li",
            "Lingyu Li",
            "Xueyan Li",
            "Haotian Liang",
            "Dongrui Liu",
            "Qihua Liu",
            "Zhixuan Liu",
            "Bangwei Liu",
            "Huacan Liu",
            "Yuexiao Liu",
            "Zongkai Liu",
            "Chaochao Lu",
            "Yudong Lu",
            "Xiaoya Lu",
            "Zhenghao Lu",
            "Qitan Lv",
            "Caoyuan Ma",
            "Jiachen Ma",
            "Xiaoya Ma",
            "Zhongtian Ma",
            "Lingyu Meng",
            "Ziqi Miao",
            "Yazhe Niu",
            "Yuezhang Peng",
            "Yuan Pu",
            "Han Qi",
            "Chen Qian",
            "Xingge Qiao",
            "Jingjing Qu",
            "Jiashu Qu",
            "Wanying Qu",
            "Wenwen Qu",
            "Xiaoye Qu",
            "Qihan Ren",
            "Qingnan Ren",
            "Qingyu Ren",
            "Jing Shao",
            "Wenqi Shao",
            "Shuai Shao",
            "Dongxing Shi",
            "Xin Song",
            "Xinhao Song",
            "Yan Teng",
            "Xuan Tong",
            "Yingchun Wang",
            "Xuhong Wang",
            "Shujie Wang",
            "Xin Wang",
            "Yige Wang",
            "Yixu Wang",
            "Yuanfu Wang",
            "Futing Wang",
            "Ruofan Wang",
            "Wenjie Wang",
            "Yajie Wang",
            "Muhao Wei",
            "Xiaoyu Wen",
            "Fenghua Weng",
            "Yuqi Wu",
            "Yingtong Xiong",
            "Xingcheng Xu"
        ],
        "comments": "47 pages, 18 figures, authors are listed in alphabetical order by their last names",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We introduce SafeWork-R1, a cutting-edge multimodal reasoning model that demonstrates the coevolution of capabilities and safety. It is developed by our proposed SafeLadder framework, which incorporates large-scale, progressive, safety-oriented reinforcement learning post-training, supported by a suite of multi-principled verifiers. Unlike previous alignment methods such as RLHF that simply learn human preferences, SafeLadder enables SafeWork-R1 to develop intrinsic safety reasoning and self-reflection abilities, giving rise to safety `aha' moments. Notably, SafeWork-R1 achieves an average improvement of $46.54\\%$ over its base model Qwen2.5-VL-72B on safety-related benchmarks without compromising general capabilities, and delivers state-of-the-art safety performance compared to leading proprietary models such as GPT-4.1 and Claude Opus 4. To further bolster its reliability, we implement two distinct inference-time intervention methods and a deliberative search mechanism, enforcing step-level verification. Finally, we further develop SafeWork-R1-InternVL3-78B, SafeWork-R1-DeepSeek-70B, and SafeWork-R1-Qwen2.5VL-7B. All resulting models demonstrate that safety and capability can co-evolve synergistically, highlighting the generalizability of our framework in building robust, reliable, and trustworthy general-purpose AI.",
        "gemini2.5flash": "",
        "overall_idea": ""
    }
]