[
    {
        "order": 1,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00029",
        "abs_url": "https://arxiv.org/abs/2511.00029",
        "pdf_url": "https://arxiv.org/pdf/2511.00029",
        "title": "Feature-Guided SAE Steering for Refusal-Rate Control using Contrasting Prompts",
        "authors": [
            "Samaksh Bhargav",
            "Zining Zhu"
        ],
        "comments": "12 pages, 6 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Large Language Model (LLM) deployment requires guiding the LLM to recognize and not answer unsafe prompts while complying with safe prompts. Previous methods for achieving this require adjusting model weights along with other expensive procedures. While recent advances in Sparse Autoencoders (SAEs) have enabled interpretable feature extraction from LLMs, existing approaches lack systematic feature selection methods and principled evaluation of safety-utility tradeoffs. We explored using different steering features and steering strengths using Sparse Auto Encoders (SAEs) to provide a solution. Using an accurate and innovative contrasting prompt method with the AI-Generated Prompts Dataset from teknium/OpenHermes-2p5-Mistral-7B and Air Bench eu-dataset to efficiently choose the best features in the model to steer, we tested this method on Llama-3 8B. We conclude that using this method, our approach achieves an 18.9% improvement in safety performance while simultaneously increasing utility by 11.1%, demonstrating that targeted SAE steering can overcome traditional safety-utility tradeoffs when optimal features are identified through principled selection methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 2,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00032",
        "abs_url": "https://arxiv.org/abs/2511.00032",
        "pdf_url": "https://arxiv.org/pdf/2511.00032",
        "title": "From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators",
        "authors": [
            "Lei Liu",
            "Zhongyi Yu",
            "Hong Wang",
            "Huanshuo Dong",
            "Haiyang Xin",
            "Hongwei Zhao",
            "Bin Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "In recent years, Neural Operators(NO) have gradually emerged as a popular approach for solving Partial Differential Equations (PDEs). However, their application to large-scale engineering tasks suffers from significant computational overhead. And the fact that current models impose a uniform computational cost while physical fields exhibit vastly different complexities constitutes a fundamental mismatch, which is the root of this inefficiency. For instance, in turbulence flows, intricate vortex regions require deeper network processing compared to stable flows. To address this, we introduce a framework: Skip-Block Routing (SBR), a general framework designed for Transformer-based neural operators, capable of being integrated into their multi-layer architectures. First, SBR uses a routing mechanism to learn the complexity and ranking of tokens, which is then applied during inference. Then, in later layers, it decides how many tokens are passed forward based on this ranking. This way, the model focuses more processing capacity on the tokens that are more complex. Experiments demonstrate that SBR is a general framework that seamlessly integrates into various neural operators. Our method reduces computational cost by approximately 50% in terms of Floating Point Operations (FLOPs), while still delivering up to 2x faster inference without sacrificing accuracy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 3,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00035",
        "abs_url": "https://arxiv.org/abs/2511.00035",
        "pdf_url": "https://arxiv.org/pdf/2511.00035",
        "title": "Neural Architecture Search for global multi-step Forecasting of Energy Production Time Series",
        "authors": [
            "Georg Velev",
            "Stefan Lessmann"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The dynamic energy sector requires both predictive accuracy and runtime efficiency for short-term forecasting of energy generation under operational constraints, where timely and precise predictions are crucial. The manual configuration of complex methods, which can generate accurate global multi-step predictions without suffering from a computational bottleneck, represents a procedure with significant time requirements and high risk for human-made errors. A further intricacy arises from the temporal dynamics present in energy-related data. Additionally, the generalization to unseen data is imperative for continuously deploying forecasting techniques over time. To overcome these challenges, in this research, we design a neural architecture search (NAS)-based framework for the automated discovery of time series models that strike a balance between computational efficiency, predictive performance, and generalization power for the global, multi-step short-term forecasting of energy production time series. In particular, we introduce a search space consisting only of efficient components, which can capture distinctive patterns of energy time series. Furthermore, we formulate a novel objective function that accounts for performance generalization in temporal context and the maximal exploration of different regions of our high-dimensional search space. The results obtained on energy production time series show that an ensemble of lightweight architectures discovered with NAS outperforms state-of-the-art techniques, such as Transformers, as well as pre-trained forecasting models, in terms of both efficiency and accuracy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 4,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00043",
        "abs_url": "https://arxiv.org/abs/2511.00043",
        "pdf_url": "https://arxiv.org/pdf/2511.00043",
        "title": "Physics-Informed Neural Network Frameworks for the Analysis of Engineering and Biological Dynamical Systems Governed by Ordinary Differential Equations",
        "authors": [
            "Tyrus Whitman",
            "Andrew Particka",
            "Christopher Diers",
            "Ian Griffin",
            "Charuka Wickramasinghe",
            "Pradeep Ranaweera"
        ],
        "comments": "21 pages, 10 figures, 5 tables",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In this study, we present and validate the predictive capability of the Physics-Informed Neural Networks (PINNs) methodology for solving a variety of engineering and biological dynamical systems governed by ordinary differential equations (ODEs). While traditional numerical methods a re effective for many ODEs, they often struggle to achieve convergence in problems involving high stiffness, shocks, irregular domains, singular perturbations, high dimensions, or boundary discontinuities. Alternatively, PINNs offer a powerful approach for handling challenging numerical scenarios. In this study, classical ODE problems are employed as controlled testbeds to systematically evaluate the accuracy, training efficiency, and generalization capability under controlled conditions of the PINNs framework. Although not a universal solution, PINNs can achieve superior results by embedding physical laws directly into the learning process. We first analyze the existence and uniqueness properties of several benchmark problems and subsequently validate the PINNs methodology on these model systems. Our results demonstrate that for complex problems to converge to correct solutions, the loss function components data loss, initial condition loss, and residual loss must be appropriately balanced through careful weighting. We further establish that systematic tuning of hyperparameters, including network depth, layer width, activation functions, learning rate, optimization algorithms, w eight initialization schemes, and collocation point sampling, plays a crucial role in achieving accurate solutions. Additionally, embedding prior knowledge and imposing hard constraints on the network architecture, without loss the generality of the ODE system, significantly enhances the predictive capability of PINNs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 5,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00044",
        "abs_url": "https://arxiv.org/abs/2511.00044",
        "pdf_url": "https://arxiv.org/pdf/2511.00044",
        "title": "ReLaX-Net: Reusing Layers for Parameter-Efficient Physical Neural Networks",
        "authors": [
            "Kohei Tsuchiyama",
            "Andre Roehm",
            "Takatomo Mihana",
            "Ryoichi Horisaki"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Adaptation and Self-Organizing Systems (nlin.AO)",
        "abstract": "Physical Neural Networks (PNN) are promising platforms for next-generation computing systems. However, recent advances in digital neural network performance are largely driven by the rapid growth in the number of trainable parameters and, so far, demonstrated PNNs are lagging behind by several orders of magnitude in terms of scale. This mirrors size and performance constraints found in early digital neural networks. In that period, efficient reuse of parameters contributed to the development of parameter-efficient architectures such as convolutional neural networks. In this work, we numerically investigate hardware-friendly weight-tying for PNNs. Crucially, with many PNN systems, there is a time-scale separation between the fast dynamic active elements of the forward pass and the only slowly trainable elements implementing weights and biases. With this in mind,we propose the Reuse of Layers for eXpanding a Neural Network (ReLaX-Net) architecture, which employs a simple layer-by-layer time-multiplexing scheme to increase the effective network depth and efficiently use the number of parameters. We only require the addition of fast switches for existing PNNs. We validate ReLaX-Nets via numerical experiments on image classification and natural language processing tasks. Our results show that ReLaX-Net improves computational performance with only minor modifications to a conventional PNN. We observe a favorable scaling, where ReLaX-Nets exceed the performance of equivalent traditional RNNs or DNNs with the same number of parameters.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 6,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00049",
        "abs_url": "https://arxiv.org/abs/2511.00049",
        "pdf_url": "https://arxiv.org/pdf/2511.00049",
        "title": "Adaptive Spatio-Temporal Graphs with Self-Supervised Pretraining for Multi-Horizon Weather Forecasting",
        "authors": [
            "Yao Liu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Accurate and robust weather forecasting remains a fundamental challenge due to the inherent spatio-temporal complexity of atmospheric systems. In this paper, we propose a novel self-supervised learning framework that leverages spatio-temporal structures to improve multi-variable weather prediction. The model integrates a graph neural network (GNN) for spatial reasoning, a self-supervised pretraining scheme for representation learning, and a spatio-temporal adaptation mechanism to enhance generalization across varying forecasting horizons. Extensive experiments on both ERA5 and MERRA-2 reanalysis datasets demonstrate that our approach achieves superior performance compared to traditional numerical weather prediction (NWP) models and recent deep learning methods. Quantitative evaluations and visual analyses in Beijing and Shanghai confirm the model's capability to capture fine-grained meteorological patterns. The proposed framework provides a scalable and label-efficient solution for future data-driven weather forecasting systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 7,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00051",
        "abs_url": "https://arxiv.org/abs/2511.00051",
        "pdf_url": "https://arxiv.org/pdf/2511.00051",
        "title": "Calibrating and Rotating: A Unified Framework for Weight Conditioning in PEFT",
        "authors": [
            "Da Chang",
            "Peng Xue",
            "Yu Li",
            "Yongxiang Liu",
            "Pengxiang Xu",
            "Shixun Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods are crucial for adapting large pre-trained models. Among these, LoRA is considered a foundational approach. Building on this, the influential DoRA method enhances performance by decomposing weight updates into magnitude and direction. However, its underlying mechanism remains unclear, and it introduces significant computational overhead. In this work, we first identify that DoRA's success stems from its capacity to increase the singular value entropy of the weight update matrix, which promotes a more uniform update distribution akin to full fine-tuning. We then reformulate DoRA into a mathematically equivalent and more efficient matrix form, revealing it as a learnable weight conditioning method. Based on this insight, we propose a unified framework for designing advanced PEFT methods by exploring two orthogonal dimensions: the architectural placement and the transformation type of the conditioning matrix. Within this framework, we introduce two novel methods: (1) \\textbf{Pre-Diag}, which applies a diagonal conditioning matrix before the LoRA update to efficiently calibrate the pre-trained weights, thereby enhancing performance while reducing training time; and (2) \\textbf{S}kewed \\textbf{O}rthogonal \\textbf{R}otation \\textbf{A}daptation (\\textbf{SORA}), which employs a parameter-efficient orthogonal rotation to perform a more powerful, norm-preserving transformation of the feature space. Extensive experiments on natural language understanding and generation tasks demonstrate that our proposed methods achieve superior performance and efficiency compared to both LoRA and DoRA. The code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 8,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00052",
        "abs_url": "https://arxiv.org/abs/2511.00052",
        "pdf_url": "https://arxiv.org/pdf/2511.00052",
        "title": "Feature-Guided Analysis of Neural Networks: A Replication Study",
        "authors": [
            "Federico Formica",
            "Stefano Gregis",
            "Aurora Francesca Zanenga",
            "Andrea Rota",
            "Mark Lawford",
            "Claudio Menghi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Understanding why neural networks make certain decisions is pivotal for their use in safety-critical applications. Feature-Guided Analysis (FGA) extracts slices of neural networks relevant to their tasks. Existing feature-guided approaches typically monitor the activation of the neural network neurons to extract the relevant rules. Preliminary results are encouraging and demonstrate the feasibility of this solution by assessing the precision and recall of Feature-Guided Analysis on two pilot case studies. However, the applicability in industrial contexts needs additional empirical evidence. To mitigate this need, this paper assesses the applicability of FGA on a benchmark made by the MNIST and LSC datasets. We assessed the effectiveness of FGA in computing rules that explain the behavior of the neural network. Our results show that FGA has a higher precision on our benchmark than the results from the literature. We also evaluated how the selection of the neural network architecture, training, and feature selection affect the effectiveness of FGA. Our results show that the selection significantly affects the recall of FGA, while it has a negligible impact on its precision.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 9,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00054",
        "abs_url": "https://arxiv.org/abs/2511.00054",
        "pdf_url": "https://arxiv.org/pdf/2511.00054",
        "title": "SpatialTraceGen: High-Fidelity Traces for Efficient VLM Spatial Reasoning Distillation",
        "authors": [
            "Gio Huh",
            "Dhruv Sheth",
            "Rayhan Zirvi",
            "Frank Xiao"
        ],
        "comments": "Accepted to the 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop on Efficient Reasoning",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "While Vision-Language Models (VLMs) excel in many areas, they struggle with complex spatial reasoning, which requires problem decomposition and strategic tool use. Fine-tuning smaller, more deployable models offers an efficient path to strong performance, but this is hampered by a major bottleneck: the absence of high-quality, step-by-step reasoning data. To address this data-efficiency gap, we introduce SpatialTraceGen, a framework to distill the reasoning processes of a large teacher model into a high-quality dataset of multi-hop, multi-tool reasoning traces. A key innovation is our automated Verifier, which scalably ensures the fidelity of each reasoning step, providing a cost-effective alternative to manual human annotation. On the CLEVR-Humans benchmark, this verifier-guided process improves the average quality score of traces by 17\\% while reducing quality variance by over 40\\%. SpatialTraceGen delivers a dataset of expert traces, providing the structured, step-by-step examples of tool use necessary for effective fine-tuning and sample-efficient offline reinforcement learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 10,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00056",
        "abs_url": "https://arxiv.org/abs/2511.00056",
        "pdf_url": "https://arxiv.org/pdf/2511.00056",
        "title": "MISA: Memory-Efficient LLMs Optimization with Module-wise Importance Sampling",
        "authors": [
            "Yuxi Liu",
            "Renjia Deng",
            "Yutong He",
            "Xue Wang",
            "Tao Yao",
            "Kun Yuan"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "The substantial memory demands of pre-training and fine-tuning large language models (LLMs) require memory-efficient optimization algorithms. One promising approach is layer-wise optimization, which treats each transformer block as a single layer and optimizes it sequentially, while freezing the other layers to save optimizer states and activations. Although effective, these methods ignore the varying importance of the modules within each layer, leading to suboptimal performance. Moreover, layer-wise sampling provides only limited memory savings, as at least one full layer must remain active during optimization. To overcome these limitations, we propose Module-wise Importance SAmpling (MISA), a novel method that divides each layer into smaller modules and assigns importance scores to each module. MISA uses a weighted random sampling mechanism to activate modules, provably reducing gradient variance compared to layer-wise sampling. Additionally, we establish an \\(\\mathcal{O}(1/\\sqrt{K})\\) convergence rate under non-convex and stochastic conditions, where $K$ is the total number of block updates, and provide a detailed memory analysis showcasing MISA's superiority over existing baseline methods. Experiments on diverse learning tasks validate the effectiveness of MISA. Source code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 11,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00064",
        "abs_url": "https://arxiv.org/abs/2511.00064",
        "pdf_url": "https://arxiv.org/pdf/2511.00064",
        "title": "EVINGCA: Adaptive Graph Clustering with Evolving Neighborhood Statistics",
        "authors": [
            "Randolph Wiredu-Aidoo"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Clustering algorithms often rely on restrictive assumptions: K-Means and Gaussian Mixtures presuppose convex, Gaussian-like clusters, while DBSCAN and HDBSCAN capture non-convexity but can be highly sensitive. I introduce EVINGCA (Evolving Variance-Informed Nonparametric Graph Construction Algorithm), a density-variance based clustering algorithm that treats cluster formation as an adaptive, evolving process on a nearest-neighbor graph. EVINGCA expands rooted graphs via breadth-first search, guided by continuously updated local distance and shape statistics, replacing fixed density thresholds with local statistical feedback. With spatial indexing, EVINGCA features log-linear complexity in the average case and exhibits competitive performance against baselines across a variety of synthetic, real-world, low-d, and high-d datasets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 12,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00065",
        "abs_url": "https://arxiv.org/abs/2511.00065",
        "pdf_url": "https://arxiv.org/pdf/2511.00065",
        "title": "Aligning Brain Signals with Multimodal Speech and Vision Embeddings",
        "authors": [
            "Kateryna Shapovalenko",
            "Quentin Auster"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "When we hear the word \"house\", we don't just process sound, we imagine walls, doors, memories. The brain builds meaning through layers, moving from raw acoustics to rich, multimodal associations. Inspired by this, we build on recent work from Meta that aligned EEG signals with averaged wav2vec2 speech embeddings, and ask a deeper question: which layers of pre-trained models best reflect this layered processing in the brain? We compare embeddings from two models: wav2vec2, which encodes sound into language, and CLIP, which maps words to images. Using EEG recorded during natural speech perception, we evaluate how these embeddings align with brain activity using ridge regression and contrastive decoding. We test three strategies: individual layers, progressive concatenation, and progressive summation. The findings suggest that combining multimodal, layer-aware representations may bring us closer to decoding how the brain understands language, not just as sound, but as experience.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 13,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00066",
        "abs_url": "https://arxiv.org/abs/2511.00066",
        "pdf_url": "https://arxiv.org/pdf/2511.00066",
        "title": "Token-Regulated Group Relative Policy Optimization for Stable Reinforcement Learning in Large Language Models",
        "authors": [
            "Tue Le",
            "Nghi D.Q.Bui",
            "Linh Ngo Van",
            "Trung Le"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a powerful approach for strengthening the reasoning capabilities of large language models (LLMs). Among existing algorithms, Group Relative Policy Optimization (GRPO) has demonstrated strong performance, yet it suffers from a critical issue: low-probability tokens disproportionately dominate gradient updates due to their inherently large gradient magnitudes. This imbalance leads to unstable training and suppresses the contribution of high-probability tokens that are more reliable for learning. In this work, we introduce Token-Regulated Group Relative Policy Optimization (TR-GRPO), a simple yet effective extension of GRPO that assigns token-level weights positively correlated with the model's predicted probability. By downweighting low-probability tokens and emphasizing high-probability ones, TR-GRPO mitigates gradient over-amplification while preserving informative learning signals. Extensive experiments demonstrate that TR-GRPO consistently outperforms GRPO across RLVR tasks, including logic, math, and agentic reasoning, highlighting the importance of regulating token contributions during RL training and establishing TR-GRPO as a robust framework for enhancing LLM reasoning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 14,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00070",
        "abs_url": "https://arxiv.org/abs/2511.00070",
        "pdf_url": "https://arxiv.org/pdf/2511.00070",
        "title": "Benchmarking Generative AI Against Bayesian Optimization for Constrained Multi-Objective Inverse Design",
        "authors": [
            "Muhammad Bilal Awan",
            "Abdul Razzaq",
            "Abdul Shahid"
        ],
        "comments": "17 pages, 2 Figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "This paper investigates the performance of Large Language Models (LLMs) as generative optimizers for solving constrained multi-objective regression tasks, specifically within the challenging domain of inverse design (property-to-structure mapping). This problem, critical to materials informatics, demands finding complex, feasible input vectors that lie on the Pareto optimal front. While LLMs have demonstrated universal effectiveness across generative and reasoning tasks, their utility in constrained, continuous, high-dimensional numerical spaces tasks they weren't explicitly architected for remains an open research question. We conducted a rigorous comparative study between established Bayesian Optimization (BO) frameworks and a suite of fine-tuned LLMs and BERT models. For BO, we benchmarked the foundational BoTorch Ax implementation against the state-of-the-art q-Expected Hypervolume Improvement (qEHVI, BoTorchM). The generative approach involved fine-tuning models via Parameter-Efficient Fine-Tuning (PEFT), framing the challenge as a regression problem with a custom output head. Our results show that BoTorch qEHVI achieved perfect convergence (GD=0.0), setting the performance ceiling. Crucially, the best-performing LLM (WizardMath-7B) achieved a Generational Distance (GD) of 1.21, significantly outperforming the traditional BoTorch Ax baseline (GD=15.03). We conclude that specialized BO frameworks remain the performance leader for guaranteed convergence, but fine-tuned LLMs are validated as a promising, computationally fast alternative, contributing essential comparative metrics to the field of AI-driven optimization. The findings have direct industrial applications in optimizing formulation design for resins, polymers, and paints, where multi-objective trade-offs between mechanical, rheological, and chemical properties are critical to innovation and production efficiency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 15,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00071",
        "abs_url": "https://arxiv.org/abs/2511.00071",
        "pdf_url": "https://arxiv.org/pdf/2511.00071",
        "title": "Wavelet-Based Feature Extraction and Unsupervised Clustering for Parity Detection: A Feature Engineering Perspective",
        "authors": [
            "Ertugrul Mutlu"
        ],
        "comments": "8 pages, 2 figures. Code: this http URL",
        "subjects": "Machine Learning (cs.LG); Signal Processing (eess.SP)",
        "abstract": "This paper explores a deliberately over-engineered approach to the classical problem of parity detection -- determining whether a number is odd or even -- by combining wavelet-based feature extraction with unsupervised clustering. Instead of relying on modular arithmetic, integers are transformed into wavelet-domain representations, from which multi-scale statistical features are extracted and clustered using the k-means algorithm. The resulting feature space reveals meaningful structural differences between odd and even numbers, achieving a classification accuracy of approximately 69.67% without any label supervision. These results suggest that classical signal-processing techniques, originally designed for continuous data, can uncover latent structure even in purely discrete symbolic domains. Beyond parity detection, the study provides an illustrative perspective on how feature engineering and clustering may be repurposed for unconventional machine learning problems, potentially bridging symbolic reasoning and feature-based learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 16,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00076",
        "abs_url": "https://arxiv.org/abs/2511.00076",
        "pdf_url": "https://arxiv.org/pdf/2511.00076",
        "title": "Bridging Vision, Language, and Mathematics: Pictographic Character Reconstruction with Bézier Curves",
        "authors": [
            "Zihao Wan",
            "Pau Tong Lin Xu",
            "Fuwen Luo",
            "Ziyue Wang",
            "Peng Li",
            "Yang Liu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "While Vision-language Models (VLMs) have demonstrated strong semantic capabilities, their ability to interpret the underlying geometric structure of visual information is less explored. Pictographic characters, which combine visual form with symbolic structure, provide an ideal test case for this capability. We formulate this visual recognition challenge in the mathematical domain, where each character is represented by an executable program of geometric primitives. This is framed as a program synthesis task, training a VLM to decompile raster images into programs composed of Bézier curves. Our model, acting as a \"visual decompiler\", demonstrates performance superior to strong zero-shot baselines, including GPT-4o. The most significant finding is that when trained solely on modern Chinese characters, the model is able to reconstruct ancient Oracle Bone Script in a zero-shot context. This generalization provides strong evidence that the model acquires an abstract and transferable geometric grammar, moving beyond pixel-level pattern recognition to a more structured form of visual understanding.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 17,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00079",
        "abs_url": "https://arxiv.org/abs/2511.00079",
        "pdf_url": "https://arxiv.org/pdf/2511.00079",
        "title": "flowengineR: A Modular and Extensible Framework for Fair and Reproducible Workflow Design in R",
        "authors": [
            "Maximilian Willer",
            "Peter Ruckdeschel"
        ],
        "comments": "27 pages, 7 figures, 1 table",
        "subjects": "Machine Learning (cs.LG); Computers and Society (cs.CY); Methodology (stat.ME)",
        "abstract": "flowengineR is an R package designed to provide a modular and extensible framework for building reproducible algorithmic workflows for general-purpose machine learning pipelines. It is motivated by the rapidly evolving field of algorithmic fairness where new metrics, mitigation strategies, and machine learning methods continuously emerge. A central challenge in fairness, but also far beyond, is that existing toolkits either focus narrowly on single interventions or treat reproducibility and extensibility as secondary considerations rather than core design principles. flowengineR addresses this by introducing a unified architecture of standardized engines for data splitting, execution, preprocessing, training, inprocessing, postprocessing, evaluation, and reporting. Each engine encapsulates one methodological task yet communicates via a lightweight interface, ensuring workflows remain transparent, auditable, and easily extensible. Although implemented in R, flowengineR builds on ideas from workflow languages (CWL, YAWL), graph-oriented visual programming languages (KNIME), and R frameworks (BatchJobs, batchtools). Its emphasis, however, is less on orchestrating engines for resilient parallel execution but rather on the straightforward setup and management of distinct engines as data structures. This orthogonalization enables distributed responsibilities, independent development, and streamlined integration. In fairness context, by structuring fairness methods as interchangeable engines, flowengineR lets researchers integrate, compare, and evaluate interventions across the modeling pipeline. At the same time, the architecture generalizes to explainability, robustness, and compliance metrics without core modifications. While motivated by fairness, it ultimately provides a general infrastructure for any workflow context where reproducibility, transparency, and extensibility are essential.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 18,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00084",
        "abs_url": "https://arxiv.org/abs/2511.00084",
        "pdf_url": "https://arxiv.org/pdf/2511.00084",
        "title": "Application of predictive machine learning in pen & paper RPG game design",
        "authors": [
            "Jolanta Śliwa"
        ],
        "comments": "Master's thesis submitted at AGH University of Science and Technology",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "In recent years, the pen and paper RPG market has experienced significant growth. As a result, companies are increasingly exploring the integration of AI technologies to enhance player experience and gain a competitive edge. One of the key challenges faced by publishers is designing new opponents and estimating their challenge level. Currently, there are no automated methods for determining a monster's level; the only approaches used are based on manual testing and expert evaluation. Although these manual methods can provide reasonably accurate estimates, they are time-consuming and resource-intensive. Level prediction can be approached using ordinal regression techniques. This thesis presents an overview and evaluation of state-of-the-art methods for this task. It also details the construction of a dedicated dataset for level estimation. Furthermore, a human-inspired model was developed to serve as a benchmark, allowing comparison between machine learning algorithms and the approach typically employed by pen and paper RPG publishers. In addition, a specialized evaluation procedure, grounded in domain knowledge, was designed to assess model performance and facilitate meaningful comparisons.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 19,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00086",
        "abs_url": "https://arxiv.org/abs/2511.00086",
        "pdf_url": "https://arxiv.org/pdf/2511.00086",
        "title": "Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph",
        "authors": [
            "Fali Wang",
            "Jihai Chen",
            "Shuhua Yang",
            "Runxue Bao",
            "Tianxiang Zhao",
            "Zhiwei Zhang",
            "Xianfeng Tang",
            "Hui Liu",
            "Qi He",
            "Suhang Wang"
        ],
        "comments": "Under review",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Test-Time Scaling (TTS) improves large language models (LLMs) by allocating additional computation during inference, typically through parallel, sequential, or hybrid scaling. However, prior studies often assume fixed collaboration architectures (e.g., topologies) and single-model usage, overlooking that optimal architectures and model combinations can vary across tasks. Therefore, we study the novel problem of searching for compute-optimal model combinations and architectures in TTS under a fixed budget. We formalize it as a multi-LLM collaboration graph, where nodes encode roles and LLM model assignments, and edges capture information flow. This problem is challenging because (i) the combinatorial search space is prohibitively large, and (ii) task-specific requirements demand tailored designs. To address these, we reformulate the problem as probabilistic graph optimization and, through pilot experiments, derive three empirical insights into TTS collaboration graphs. Guided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented framework that mirrors the REINFORCE pipeline by mapping sampling-gradient-update to sampling-feedback-update, where feedback serves as a textual gradient to update the probabilistic graph and efficiently search for optimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE outperforms both traditional and LLM-based baselines in sample efficiency and search performance, and effectively identifies optimal graphs under joint objectives of accuracy and inference latency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 20,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00099",
        "abs_url": "https://arxiv.org/abs/2511.00099",
        "pdf_url": "https://arxiv.org/pdf/2511.00099",
        "title": "A generative adversarial network optimization method for damage detection and digital twinning by deep AI fault learning: Z24 Bridge structural health monitoring benchmark validation",
        "authors": [
            "Marios Impraimakis",
            "Evangelia Nektaria Palkanoglou"
        ],
        "comments": "21 pages, 23 figures, published in Structural and Multidisciplinary Optimization",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Signal Processing (eess.SP); Systems and Control (eess.SY)",
        "abstract": "The optimization-based damage detection and damage state digital twinning capabilities are examined here of a novel conditional-labeled generative adversarial network methodology. The framework outperforms current approaches for fault anomaly detection as no prior information is required for the health state of the system: a topic of high significance for real-world applications. Specifically, current artificial intelligence-based digital twinning approaches suffer from the uncertainty related to obtaining poor predictions when a low number of measurements is available, physics knowledge is missing, or when the damage state is unknown. To this end, an unsupervised framework is examined and validated rigorously on the benchmark structural health monitoring measurements of Z24 Bridge: a post-tensioned concrete highway bridge in Switzerland. In implementing the approach, firstly, different same damage-level measurements are used as inputs, while the model is forced to converge conditionally to two different damage states. Secondly, the process is repeated for a different group of measurements. Finally, the convergence scores are compared to identify which one belongs to a different damage state. The process for both healthy-to-healthy and damage-to-healthy input data creates, simultaneously, measurements for digital twinning purposes at different damage states, capable of pattern recognition and machine learning data generation. Further to this process, a support vector machine classifier and a principal component analysis procedure is developed to assess the generated and real measurements of each damage category, serving as a secondary new dynamics learning indicator in damage scenarios. Importantly, the approach is shown to capture accurately damage over healthy measurements, providing a powerful tool for vibration-based system-level monitoring and scalable infrastructure resilience.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 21,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00101",
        "abs_url": "https://arxiv.org/abs/2511.00101",
        "pdf_url": "https://arxiv.org/pdf/2511.00101",
        "title": "Loquetier: A Virtualized Multi-LoRA Framework for Unified LLM Fine-tuning and Serving",
        "authors": [
            "Yuchen Zhang",
            "Hanyue Du",
            "Chun Cao",
            "Jingwei Xu"
        ],
        "comments": "26 pages including 10 pages of main text, 6 figures, 39th Conference on Neural Information Processing Systems (NeurIPS 2025)",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Low-Rank Adaptation (LoRA) has become a widely adopted parameter-efficient fine-tuning (PEFT) technique for adapting large language models (LLMs) to downstream tasks. While prior work has explored strategies for integrating LLM training and serving, there still remains a gap in unifying fine-tuning and inference for LoRA-based models. We present Loquetier, a virtualized multi-LoRA framework that seamlessly integrates LoRA fine-tuning and serving within a single runtime. Loquetier introduces two key components: (1) a Virtualized Module that isolates PEFT-based modifications and supports multiple adapters on a shared base model, and (2) an optimized computation flow with a kernel design that merges fine-tuning and inference paths in forward propagation, enabling efficient batching and minimizing kernel invocation overhead. Extensive experiments across three task settings show that Loquetier consistently outperforms existing baselines in both performance and flexibility, achieving up to $3.0\\times$ the throughput of the state-of-the-art co-serving system on inference-only tasks and $46.4\\times$ higher SLO attainment than PEFT on unified fine-tuning and inference tasks. The implementation of Loquetier is publicly available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 22,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00108",
        "abs_url": "https://arxiv.org/abs/2511.00108",
        "pdf_url": "https://arxiv.org/pdf/2511.00108",
        "title": "Pelican-VL 1.0: A Foundation Brain Model for Embodied Intelligence",
        "authors": [
            "Yi Zhang",
            "Che Liu",
            "Xiancong Ren",
            "Hanchu Ni",
            "Shuai Zhang",
            "Zeyuan Ding",
            "Jiayu Hu",
            "Hanzhe Shan",
            "Zhenwei Niu",
            "Zhaoyang Liu",
            "Yue Zhao",
            "Junbo Qi",
            "Qinfan Zhang",
            "Dengjie Li",
            "Yidong Wang",
            "Jiachen Luo",
            "Yong Dai",
            "Jian Tang",
            "Xiaozhu Ju"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Robotics (cs.RO)",
        "abstract": "This report presents Pelican-VL 1.0, a new family of open-source embodied brain models with parameter scales ranging from 7 billion to 72 billion. Our explicit mission is clearly stated as: To embed powerful intelligence into various embodiments. Pelican-VL 1.0 is currently the largest-scale open-source embodied multimodal brain model. Its core advantage lies in the in-depth integration of data power and intelligent adaptive learning mechanisms. Specifically, metaloop distilled a high-quality dataset from a raw dataset containing 4+ billion tokens. Pelican-VL 1.0 is trained on a large-scale cluster of 1000+ A800 GPUs, consuming over 50k+ A800 GPU-hours per checkpoint. This translates to a 20.3% performance uplift from its base model and outperforms 100B-level open-source counterparts by 10.6%, placing it on par with leading proprietary systems on well-known embodied benchmarks. We establish a novel framework, DPPO (Deliberate Practice Policy Optimization), inspired by human metacognition to train Pelican-VL 1.0. We operationalize this as a metaloop that teaches the AI to practice deliberately, which is a RL-Refine-Diagnose-SFT loop.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 23,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00113",
        "abs_url": "https://arxiv.org/abs/2511.00113",
        "pdf_url": "https://arxiv.org/pdf/2511.00113",
        "title": "MeixnerNet: Adaptive and Robust Spectral Graph Neural Networks with Discrete Orthogonal Polynomials",
        "authors": [
            "Huseyin Goksu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Signal Processing (eess.SP)",
        "abstract": "Spectral Graph Neural Networks (GNNs) have achieved state-of-the-art results by defining graph convolutions in the spectral domain. A common approach, popularized by ChebyNet, is to use polynomial filters based on continuous orthogonal polynomials (e.g., Chebyshev). This creates a theoretical disconnect, as these continuous-domain filters are applied to inherently discrete graph structures. We hypothesize this mismatch can lead to suboptimal performance and fragility to hyperparameter settings. In this paper, we introduce MeixnerNet, a novel spectral GNN architecture that employs discrete orthogonal polynomials -- specifically, the Meixner polynomials $M_k(x; \\beta, c)$. Our model makes the two key shape parameters of the polynomial, beta and c, learnable, allowing the filter to adapt its polynomial basis to the specific spectral properties of a given graph. We overcome the significant numerical instability of these polynomials by introducing a novel stabilization technique that combines Laplacian scaling with per-basis LayerNorm. We demonstrate experimentally that MeixnerNet achieves competitive-to-superior performance against the strong ChebyNet baseline at the optimal K = 2 setting (winning on 2 out of 3 benchmarks). More critically, we show that MeixnerNet is exceptionally robust to variations in the polynomial degree K, a hyperparameter to which ChebyNet proves to be highly fragile, collapsing in performance where MeixnerNet remains stable.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 24,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00117",
        "abs_url": "https://arxiv.org/abs/2511.00117",
        "pdf_url": "https://arxiv.org/pdf/2511.00117",
        "title": "DCcluster-Opt: Benchmarking Dynamic Multi-Objective Optimization for Geo-Distributed Data Center Workloads",
        "authors": [
            "Antonio Guillen-Perez",
            "Avisek Naug",
            "Vineet Gundecha",
            "Sahand Ghorbanpour",
            "Ricardo Luna Gutierrez",
            "Ashwin Ramesh Babu",
            "Munther Salim",
            "Shubhanker Banerjee",
            "Eoin H. Oude Essink",
            "Damien Fay",
            "Soumyendu Sarkar"
        ],
        "comments": "Submitted to the NeurIPS 2025 conference",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA); Systems and Control (eess.SY)",
        "abstract": "The increasing energy demands and carbon footprint of large-scale AI require intelligent workload management in globally distributed data centers. Yet progress is limited by the absence of benchmarks that realistically capture the interplay of time-varying environmental factors (grid carbon intensity, electricity prices, weather), detailed data center physics (CPUs, GPUs, memory, HVAC energy), and geo-distributed network dynamics (latency and transmission costs). To bridge this gap, we present DCcluster-Opt: an open-source, high-fidelity simulation benchmark for sustainable, geo-temporal task scheduling. DCcluster-Opt combines curated real-world datasets, including AI workload traces, grid carbon intensity, electricity markets, weather across 20 global regions, cloud transmission costs, and empirical network delay parameters with physics-informed models of data center operations, enabling rigorous and reproducible research in sustainable computing. It presents a challenging scheduling problem where a top-level coordinating agent must dynamically reassign or defer tasks that arrive with resource and service-level agreement requirements across a configurable cluster of data centers to optimize multiple objectives. The environment also models advanced components such as heat recovery. A modular reward system enables an explicit study of trade-offs among carbon emissions, energy costs, service level agreements, and water use. It provides a Gymnasium API with baseline controllers, including reinforcement learning and rule-based strategies, to support reproducible ML research and a fair comparison of diverse algorithms. By offering a realistic, configurable, and accessible testbed, DCcluster-Opt accelerates the development and validation of next-generation sustainable computing solutions for geo-distributed data centers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 25,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00121",
        "abs_url": "https://arxiv.org/abs/2511.00121",
        "pdf_url": "https://arxiv.org/pdf/2511.00121",
        "title": "Analysis of Line Break prediction models for detecting defensive breakthrough in football",
        "authors": [
            "Shoma Yagi",
            "Jun Ichikawa",
            "Genki Ichinose"
        ],
        "comments": "14 pages, 8 figures",
        "subjects": "Machine Learning (cs.LG); Physics and Society (physics.soc-ph); Applications (stat.AP)",
        "abstract": "In football, attacking teams attempt to break through the opponent's defensive line to create scoring opportunities. This action, known as a Line Break, is a critical indicator of offensive effectiveness and tactical performance, yet previous studies have mainly focused on shots or goal opportunities rather than on how teams break the defensive line. In this study, we develop a machine learning model to predict Line Breaks using event and tracking data from the 2023 J1 League season. The model incorporates 189 features, including player positions, velocities, and spatial configurations, and employs an XGBoost classifier to estimate the probability of Line Breaks. The proposed model achieved high predictive accuracy, with an AUC of 0.982 and a Brier score of 0.015. Furthermore, SHAP analysis revealed that factors such as offensive player speed, gaps in the defensive line, and offensive players' spatial distributions significantly contribute to the occurrence of Line Breaks. Finally, we found a moderate positive correlation between the predicted probability of being Line-Broken and the number of shots and crosses conceded at the team level. These results suggest that Line Breaks are closely linked to the creation of scoring opportunities and provide a quantitative framework for understanding tactical dynamics in football.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 26,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00124",
        "abs_url": "https://arxiv.org/abs/2511.00124",
        "pdf_url": "https://arxiv.org/pdf/2511.00124",
        "title": "Cross-fluctuation phase transitions reveal sampling dynamics in diffusion models",
        "authors": [
            "Sai Niranjan Ramachandran",
            "Manish Krishan Lal",
            "Suvrit Sra"
        ],
        "comments": "Accepted at NeurIPS 2025. 10 pages, camera-ready version. appendices included",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "We analyse how the sampling dynamics of distributions evolve in score-based diffusion models using cross-fluctuations, a centered-moment statistic from statistical physics. Specifically, we show that starting from an unbiased isotropic normal distribution, samples undergo sharp, discrete transitions, eventually forming distinct events of a desired distribution while progressively revealing finer structure. As this process is reversible, these transitions also occur in reverse, where intermediate states progressively merge, tracing a path back to the initial distribution. We demonstrate that these transitions can be detected as discontinuities in $n^{\\text{th}}$-order cross-fluctuations. For variance-preserving SDEs, we derive a closed-form for these cross-fluctuations that is efficiently computable for the reverse trajectory. We find that detecting these transitions directly boosts sampling efficiency, accelerates class-conditional and rare-class generation, and improves two zero-shot tasks--image classification and style transfer--without expensive grid search or retraining. We also show that this viewpoint unifies classical coupling and mixing from finite Markov chains with continuous dynamics while extending to stochastic SDEs and non Markovian samplers. Our framework therefore bridges discrete Markov chain theory, phase analysis, and modern generative modeling.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 27,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00129",
        "abs_url": "https://arxiv.org/abs/2511.00129",
        "pdf_url": "https://arxiv.org/pdf/2511.00129",
        "title": "Casing Collar Identification using AlexNet-based Neural Networks for Depth Measurement in Oil and Gas Wells",
        "authors": [
            "Siyu Xiao",
            "Xindi Zhao",
            "Tianhao Mao",
            "Yiwei Wang",
            "Yuqiao Chen",
            "Hongyun Zhang",
            "Jian Wang",
            "Junjie Wang",
            "Shuang Liu",
            "Tupei Chen",
            "Yang Liu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Signal Processing (eess.SP)",
        "abstract": "Accurate downhole depth measurement is essential for oil and gas well operations, directly influencing reservoir contact, production efficiency, and operational safety. Collar correlation using a casing collar locator (CCL) is fundamental for precise depth calibration. While neural network-based CCL signal recognition has achieved significant progress in collar identification, preprocessing methods for such applications remain underdeveloped. Moreover, the limited availability of real well data poses substantial challenges for training neural network models that require extensive datasets. This paper presents a system integrated into downhole tools for CCL signal acquisition to facilitate dataset construction. We propose comprehensive preprocessing methods for data augmentation and evaluate their effectiveness using our AlexNet-based neural network models. Through systematic experimentation across various configuration combinations, we analyze the contribution of each augmentation method. Results demonstrate that standardization, label distribution smoothing (LDS), and random cropping are fundamental requirements for model training, while label smoothing regularization (LSR), time scaling, and multiple sampling significantly enhance model generalization capability. The F1 scores of our two benchmark models trained with the proposed augmentation methods maximumly improve from 0.937 and 0.952 to 1.0 and 1.0, respectively. Performance validation on real CCL waveforms confirms the effectiveness and practical applicability of our approach. This work addresses the gaps in data augmentation methodologies for training casing collar recognition models in CCL data-limited environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 28,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00130",
        "abs_url": "https://arxiv.org/abs/2511.00130",
        "pdf_url": "https://arxiv.org/pdf/2511.00130",
        "title": "A Comparative Analysis of LLM Adaptation: SFT, LoRA, and ICL in Data-Scarce Scenarios",
        "authors": [
            "Bernd Bohnet",
            "Rumen Dangovski",
            "Kevin Swersky",
            "Sherry Moore",
            "Arslan Chaudhry",
            "Kathleen Kenealy",
            "Noah Fiedel"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The remarkable capabilities of Large Language Models (LLMs) often need to be tailored for specific applications, requiring the integration of new knowledge or the acquisition of new skills. While full fine-tuning is a powerful adaptation method, it is computationally expensive and can lead to a degradation of general reasoning abilities, a phenomenon known as catastrophic forgetting. A range of alternative techniques exists, each with its own trade-offs. In-Context Learning (ICL) is fast but limited by context length, while Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation (LoRA) offer a middle ground by minimizing parameter changes. However, the challenge of catastrophic forgetting persists, raising questions about the best adaptation strategy for a given task. This paper presents a comparative analysis of Supervised Finetuning (SFT), LoRA, and ICL in data-scarce scenarios. We find that LoRA provides the most effective balance, successfully instilling new skills with minimal impact on the base model's general knowledge. In contrast, while SFT excels at skill acquisition, it is highly susceptible to catastrophic forgetting. ICL is effective for incorporating factual knowledge but struggles with complex skills. Our findings offer a practical framework for selecting an LLM adaptation strategy. We highlight the critical distinction between skill acquisition and knowledge integration, clarify the trade-offs between task-specific performance and the preservation of general capabilities.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 29,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00134",
        "abs_url": "https://arxiv.org/abs/2511.00134",
        "pdf_url": "https://arxiv.org/pdf/2511.00134",
        "title": "Physiologically Active Vegetation Reverses Its Cooling Effect in Humid Urban Climates",
        "authors": [
            "Angana Borah",
            "Adrija Datta",
            "Ashish S. Kumar",
            "Raviraj Dave",
            "Udit Bhatia"
        ],
        "comments": "27 pages, 5 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Efforts to green cities for cooling are succeeding unevenly because the same vegetation that cools surfaces can also intensify how hot the air feels. Previous studies have identified humid heat as a growing urban hazard, yet how physiologically active vegetation governs this trade-off between cooling and moisture accumulation remains poorly understood, leaving mitigation policy and design largely unguided. Here we quantify how vegetation structure and function influence the Heat Index (HI), a combined measure of temperature and humidity in 138 Indian cities spanning tropical savanna, semi-arid steppe, and humid subtropical climates, and across dense urban cores and semi-urban rings. Using an extreme-aware, one kilometre reconstruction of HI and an interpretable machine-learning framework that integrates SHapley Additive Explanations (SHAP) and Accumulated Local Effects (ALE), we isolate vegetation-climate interactions. Cooling generally strengthens for EVI >= 0.4 and LAI >= 0.05, but joint-high regimes begin to reverse toward warming when EVI >= 0.5, LAI >= 0.2, and fPAR >= 0.5,with an earlier onset for fPAR >= 0.25 in humid, dense cores. In such environments, highly physiologically active vegetation elevates near-surface humidity faster than it removes heat, reversing its cooling effect and amplifying perceived heat stress. These findings establish the climatic limits of vegetation-driven cooling and provide quantitative thresholds for climate-specific greening strategies that promote equitable and heat-resilient cities.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 30,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00166",
        "abs_url": "https://arxiv.org/abs/2511.00166",
        "pdf_url": "https://arxiv.org/pdf/2511.00166",
        "title": "Study on Supply Chain Finance Decision-Making Model and Enterprise Economic Performance Prediction Based on Deep Reinforcement Learning",
        "authors": [
            "Shiman Zhang",
            "Jinghan Zhou",
            "Zhoufan Yu",
            "Ningai Leng"
        ],
        "comments": "9 pages, 3 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "To improve decision-making and planning efficiency in back-end centralized redundant supply chains, this paper proposes a decision model integrating deep learning with intelligent particle swarm optimization. A distributed node deployment model and optimal planning path are constructed for the supply chain network. Deep learning such as convolutional neural networks extracts features from historical data, and linear programming captures high-order statistical features. The model is optimized using fuzzy association rule scheduling and deep reinforcement learning, while neural networks fit dynamic changes. A hybrid mechanism of \"deep learning feature extraction - intelligent particle swarm optimization\" guides global optimization and selects optimal decisions for adaptive control. Simulations show reduced resource consumption, enhanced spatial planning, and in dynamic environments improved real-time decision adjustment, distribution path optimization, and robust intelligent control.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 31,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00177",
        "abs_url": "https://arxiv.org/abs/2511.00177",
        "pdf_url": "https://arxiv.org/pdf/2511.00177",
        "title": "Can SAEs reveal and mitigate racial biases of LLMs in healthcare?",
        "authors": [
            "Hiba Ahsan",
            "Byron C. Wallace"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
        "abstract": "LLMs are increasingly being used in healthcare. This promises to free physicians from drudgery, enabling better care to be delivered at scale. But the use of LLMs in this space also brings risks; for example, such models may worsen existing biases. How can we spot when LLMs are (spuriously) relying on patient race to inform predictions? In this work we assess the degree to which Sparse Autoencoders (SAEs) can reveal (and control) associations the model has made between race and stigmatizing concepts. We first identify SAE latents in Gemma-2 models which appear to correlate with Black individuals. We find that this latent activates on reasonable input sequences (e.g., \"African American\") but also problematic words like \"incarceration\". We then show that we can use this latent to steer models to generate outputs about Black patients, and further that this can induce problematic associations in model outputs as a result. For example, activating the Black latent increases the risk assigned to the probability that a patient will become \"belligerent\". We evaluate the degree to which such steering via latents might be useful for mitigating bias. We find that this offers improvements in simple settings, but is less successful for more realistic and complex clinical tasks. Overall, our results suggest that: SAEs may offer a useful tool in clinical applications of LLMs to identify problematic reliance on demographics but mitigating bias via SAE steering appears to be of marginal utility for realistic tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 32,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00183",
        "abs_url": "https://arxiv.org/abs/2511.00183",
        "pdf_url": "https://arxiv.org/pdf/2511.00183",
        "title": "PDE-SHARP: PDE Solver Hybrids Through Analysis & Refinement Passes",
        "authors": [
            "Shaghayegh Fazliani",
            "Madeleine Udell"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Current LLM-driven approaches using test-time computing to generate PDE solvers execute a large number of solver samples to identify high-accuracy solvers. These paradigms are especially costly for complex PDEs requiring substantial computational resources for numerical evaluation. We introduce PDE-SHARP, a framework to reduce computational costs by replacing expensive scientific computation by cheaper LLM inference that achieves superior solver accuracy with 60-75% fewer computational evaluations. PDE-SHARP employs three stages: (1) Analysis: mathematical chain-of-thought analysis including PDE classification, solution type detection, and stability analysis; (2) Genesis: solver generation based on mathematical insights from the previous stage; and (3) Synthesis: collaborative selection-hybridization tournaments in which LLM judges iteratively refine implementations through flexible performance feedback. To generate high-quality solvers, PDE-SHARP requires fewer than 13 solver evaluations on average compared to 30+ for baseline methods, improving accuracy uniformly across tested PDEs by $4\\times$ on average, and demonstrates robust performance across LLM architectures, from general-purpose to specialized reasoning models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 33,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00192",
        "abs_url": "https://arxiv.org/abs/2511.00192",
        "pdf_url": "https://arxiv.org/pdf/2511.00192",
        "title": "EL-MIA: Quantifying Membership Inference Risks of Sensitive Entities in LLMs",
        "authors": [
            "Ali Satvaty",
            "Suzan Verberne",
            "Fatih Turkmen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Membership inference attacks (MIA) aim to infer whether a particular data point is part of the training dataset of a model. In this paper, we propose a new task in the context of LLM privacy: entity-level discovery of membership risk focused on sensitive information (PII, credit card numbers, etc). Existing methods for MIA can detect the presence of entire prompts or documents in the LLM training data, but they fail to capture risks at a finer granularity. We propose the ``EL-MIA'' framework for auditing entity-level membership risks in LLMs. We construct a benchmark dataset for the evaluation of MIA methods on this task. Using this benchmark, we conduct a systematic comparison of existing MIA techniques as well as two newly proposed methods. We provide a comprehensive analysis of the results, trying to explain the relation of the entity level MIA susceptability with the model scale, training epochs, and other surface level factors. Our findings reveal that existing MIA methods are limited when it comes to entity-level membership inference of the sensitive attributes, while this susceptibility can be outlined with relatively straightforward methods, highlighting the need for stronger adversaries to stress test the provided threat model.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 34,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00203",
        "abs_url": "https://arxiv.org/abs/2511.00203",
        "pdf_url": "https://arxiv.org/pdf/2511.00203",
        "title": "Diffusion LLMs are Natural Adversaries for any LLM",
        "authors": [
            "David Lüdke",
            "Tom Wollschläger",
            "Paul Ungermann",
            "Stephan Günnemann",
            "Leo Schwinn"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "We introduce a novel framework that transforms the resource-intensive (adversarial) prompt optimization problem into an \\emph{efficient, amortized inference task}. Our core insight is that pretrained, non-autoregressive generative LLMs, such as Diffusion LLMs, which model the joint distribution over prompt-response pairs, can serve as powerful surrogates for prompt search. This approach enables the direct conditional generation of prompts, effectively replacing costly, per-instance discrete optimization with a small number of parallelizable samples. We provide a probabilistic analysis demonstrating that under mild fidelity assumptions, only a few conditional samples are required to recover high-reward (harmful) prompts. Empirically, we find that the generated prompts are low-perplexity, diverse jailbreaks that exhibit strong transferability to a wide range of black-box target models, including robustly trained and proprietary LLMs. Beyond adversarial prompting, our framework opens new directions for red teaming, automated prompt optimization, and leveraging emerging Flow- and Diffusion-based LLMs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 35,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00220",
        "abs_url": "https://arxiv.org/abs/2511.00220",
        "pdf_url": "https://arxiv.org/pdf/2511.00220",
        "title": "Iterative Foundation Model Fine-Tuning on Multiple Rewards",
        "authors": [
            "Pouya M. Ghari",
            "Simone Sciabola",
            "Ye Wang"
        ],
        "comments": "Accepted to NeurIPS 2025",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Fine-tuning foundation models has emerged as a powerful approach for generating objects with specific desired properties. Reinforcement learning (RL) provides an effective framework for this purpose, enabling models to generate outputs that maximize a given reward function. However, in many applications such as text generation and drug discovery, it can be suboptimal to optimize using a single reward signal, as multiple evaluation criteria are often necessary. This paper proposes a novel reinforcement learning-based method for fine-tuning foundation models using multiple reward signals. By employing an iterative fine-tuning strategy across these rewards, our approach generalizes state-of-the-art RL-based methods. We further provide a theoretical analysis that offers insights into the performance of multi-reward RL fine-tuning. Experimental results across diverse domains including text, biological sequence, and small molecule generation, demonstrate the effectiveness of the proposed algorithm compared to state-of-the-art baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 36,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00257",
        "abs_url": "https://arxiv.org/abs/2511.00257",
        "pdf_url": "https://arxiv.org/pdf/2511.00257",
        "title": "A Tight Lower Bound for Non-stochastic Multi-armed Bandits with Expert Advice",
        "authors": [
            "Zachary Chase",
            "Shinji Ito",
            "Idan Mehalel"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "We determine the minimax optimal expected regret in the classic non-stochastic multi-armed bandit with expert advice problem, by proving a lower bound that matches the upper bound of Kale (2014). The two bounds determine the minimax optimal expected regret to be $\\Theta\\left( \\sqrt{T K \\log (N/K) } \\right)$, where $K$ is the number of arms, $N$ is the number of experts, and $T$ is the time horizon.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 37,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00266",
        "abs_url": "https://arxiv.org/abs/2511.00266",
        "pdf_url": "https://arxiv.org/pdf/2511.00266",
        "title": "X-TRACK: Physics-Aware xLSTM for Realistic Vehicle Trajectory Prediction",
        "authors": [
            "Aanchal Rajesh Chugh",
            "Marion Neumeier",
            "Sebastian Dorn"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Robotics (cs.RO)",
        "abstract": "Recent advancements in Recurrent Neural Network (RNN) architectures, particularly the Extended Long Short Term Memory (xLSTM), have addressed the limitations of traditional Long Short Term Memory (LSTM) networks by introducing exponential gating and enhanced memory structures. These improvements make xLSTM suitable for time-series prediction tasks as they exhibit the ability to model long-term temporal dependencies better than LSTMs. Despite their potential, these xLSTM-based models remain largely unexplored in the context of vehicle trajectory prediction. Therefore, this paper introduces a novel xLSTM-based vehicle trajectory prediction framework, X-TRAJ, and its physics-aware variant, X-TRACK (eXtended LSTM for TRAjectory prediction Constraint by Kinematics), which explicitly integrates vehicle motion kinematics into the model learning process. By introducing physical constraints, the proposed model generates realistic and feasible trajectories. A comprehensive evaluation on the highD and NGSIM datasets demonstrates that X-TRACK outperforms state-of-the-art baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 38,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00272",
        "abs_url": "https://arxiv.org/abs/2511.00272",
        "pdf_url": "https://arxiv.org/pdf/2511.00272",
        "title": "Improving the Robustness of Control of Chaotic Convective Flows with Domain-Informed Reinforcement Learning",
        "authors": [
            "Michiel Straat",
            "Thorben Markmann",
            "Sebastian Peitz",
            "Barbara Hammer"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Fluid Dynamics (physics.flu-dyn)",
        "abstract": "Chaotic convective flows arise in many real-world systems, such as microfluidic devices and chemical reactors. Stabilizing these flows is highly desirable but remains challenging, particularly in chaotic regimes where conventional control methods often fail. Reinforcement Learning (RL) has shown promise for control in laminar flow settings, but its ability to generalize and remain robust under chaotic and turbulent dynamics is not well explored, despite being critical for real-world deployment. In this work, we improve the practical feasibility of RL-based control of such flows focusing on Rayleigh-Bénard Convection (RBC), a canonical model for convective heat transport. To enhance generalization and sample efficiency, we introduce domain-informed RL agents that are trained using Proximal Policy Optimization across diverse initial conditions and flow regimes. We incorporate domain knowledge in the reward function via a term that encourages Bénard cell merging, as an example of a desirable macroscopic property. In laminar flow regimes, the domain-informed RL agents reduce convective heat transport by up to 33%, and in chaotic flow regimes, they still achieve a 10% reduction, which is significantly better than the conventional controllers used in practice. We compare the domain-informed to uninformed agents: Our results show that the domain-informed reward design results in steady flows, faster convergence during training, and generalization across flow regimes without retraining. Our work demonstrates that elegant domain-informed priors can greatly enhance the robustness of RL-based control of chaotic flows, bringing real-world deployment closer.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 39,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00301",
        "abs_url": "https://arxiv.org/abs/2511.00301",
        "pdf_url": "https://arxiv.org/pdf/2511.00301",
        "title": "A systematic evaluation of uncertainty quantification techniques in deep learning: a case study in photoplethysmography signal analysis",
        "authors": [
            "Ciaran Bench",
            "Oskar Pfeffer",
            "Vivek Desai",
            "Mohammad Moulaeifard",
            "Loïc Coquelin",
            "Peter H. Charlton",
            "Nils Strodthoff",
            "Nando Hegemann",
            "Philip J. Aston",
            "Andrew Thompson"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Medical Physics (physics.med-ph)",
        "abstract": "In principle, deep learning models trained on medical time-series, including wearable photoplethysmography (PPG) sensor data, can provide a means to continuously monitor physiological parameters outside of clinical settings. However, there is considerable risk of poor performance when deployed in practical measurement scenarios leading to negative patient outcomes. Reliable uncertainties accompanying predictions can provide guidance to clinicians in their interpretation of the trustworthiness of model outputs. It is therefore of interest to compare the effectiveness of different approaches. Here we implement an unprecedented set of eight uncertainty quantification (UQ) techniques to models trained on two clinically relevant prediction tasks: Atrial Fibrillation (AF) detection (classification), and two variants of blood pressure regression. We formulate a comprehensive evaluation procedure to enable a rigorous comparison of these approaches. We observe a complex picture of uncertainty reliability across the different techniques, where the most optimal for a given task depends on the chosen expression of uncertainty, evaluation metric, and scale of reliability assessed. We find that assessing local calibration and adaptivity provides practically relevant insights about model behaviour that otherwise cannot be acquired using more commonly implemented global reliability metrics. We emphasise that criteria for evaluating UQ techniques should cater to the model's practical use case, where the use of a small number of measurements per patient places a premium on achieving small-scale reliability for the chosen expression of uncertainty, while preserving as much predictive performance as possible.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 40,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00351",
        "abs_url": "https://arxiv.org/abs/2511.00351",
        "pdf_url": "https://arxiv.org/pdf/2511.00351",
        "title": "Reject Only Critical Tokens: Pivot-Aware Speculative Decoding",
        "authors": [
            "Amir Ziashahabi",
            "Yavuz Faruk Bakman",
            "Duygu Nur Yaldiz",
            "Mostafa El-Khamy",
            "Sai Praneeth Karimireddy",
            "Salman Avestimehr"
        ],
        "comments": "Accepted at NeurIPS 2025 Efficient Reasoning Workshop",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
        "abstract": "Speculative Decoding (SD) ensures that the output matches the target model's distribution exactly. However, we argue that this distribution matching requirement is too stringent and results in unnecessarily low acceptance rates, limiting potential speedups. Instead, we advocate a reformulation of the decoding objective: the proposed decoding strategy should match the expected utility, i.e., the task-specific performance, of the target model. This perspective also aligns better with real-world use cases of LLMs, where utility (e.g., code correctness, factual accuracy) is often more important than sampling distribution. Based on this reformulation, we propose a novel decoding strategy: Pivot-Aware Speculative Decoding, which rejects only those tokens that would lead to a utility drop in the final output. We refer to these critical tokens as pivot tokens. We propose a method for labeling tokens as pivotal or non-pivotal and train a lightweight classifier to detect them. This method can be viewed as a relaxed version of standard SD, which offers much higher acceptance while preserving utility. We evaluate our method across various datasets, demonstrating that we can achieve up to $2.5\\times$ speedup with comparable utility. Source code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 41,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00359",
        "abs_url": "https://arxiv.org/abs/2511.00359",
        "pdf_url": "https://arxiv.org/pdf/2511.00359",
        "title": "Toward Unifying Group Fairness Evaluation from a Sparsity Perspective",
        "authors": [
            "Zhecheng Sheng",
            "Jiawei Zhang",
            "Enmao Diao"
        ],
        "comments": "30 pages, 14 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (stat.ML)",
        "abstract": "Ensuring algorithmic fairness remains a significant challenge in machine learning, particularly as models are increasingly applied across diverse domains. While numerous fairness criteria exist, they often lack generalizability across different machine learning problems. This paper examines the connections and differences among various sparsity measures in promoting fairness and proposes a unified sparsity-based framework for evaluating algorithmic fairness. The framework aligns with existing fairness criteria and demonstrates broad applicability to a wide range of machine learning tasks. We demonstrate the effectiveness of the proposed framework as an evaluation metric through extensive experiments on a variety of datasets and bias mitigation methods. This work provides a novel perspective to algorithmic fairness by framing it through the lens of sparsity and social equity, offering potential for broader impact on fairness research and applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 42,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00369",
        "abs_url": "https://arxiv.org/abs/2511.00369",
        "pdf_url": "https://arxiv.org/pdf/2511.00369",
        "title": "Balancing Interpretability and Performance in Motor Imagery EEG Classification: A Comparative Study of ANFIS-FBCSP-PSO and EEGNet",
        "authors": [
            "Farjana Aktar",
            "Mohd Ruhul Ameen",
            "Akif Islam",
            "Md Ekramul Hamid"
        ],
        "comments": "6 pages, 3 figures, 8 tables, Submitted to ICECTE 2026",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)",
        "abstract": "Achieving both accurate and interpretable classification of motor imagery EEG remains a key challenge in brain computer interface (BCI) research. This paper compares a transparent fuzzy reasoning approach (ANFIS-FBCSP-PSO) with a deep learning benchmark (EEGNet) using the BCI Competition IV-2a dataset. The ANFIS pipeline combines filter bank common spatial pattern feature extraction with fuzzy IF-THEN rules optimized via particle swarm optimization, while EEGNet learns hierarchical spatial temporal representations directly from raw EEG data. In within-subject experiments, the fuzzy neural model performed better (68.58 percent +/- 13.76 percent accuracy, kappa = 58.04 percent +/- 18.43), while in cross-subject (LOSO) tests, the deep model exhibited stronger generalization (68.20 percent +/- 12.13 percent accuracy, kappa = 57.33 percent +/- 16.22). The study provides practical guidance for selecting MI-BCI systems according to design goals: interpretability or robustness across users. Future investigations into transformer based and hybrid neuro symbolic frameworks are expected to advance transparent EEG decoding.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 43,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00375",
        "abs_url": "https://arxiv.org/abs/2511.00375",
        "pdf_url": "https://arxiv.org/pdf/2511.00375",
        "title": "PolyRecommender: A Multimodal Recommendation System for Polymer Discovery",
        "authors": [
            "Xin Wang",
            "Yunhao Xiao",
            "Rui Qiao"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Information Retrieval (cs.IR)",
        "abstract": "We introduce PolyRecommender, a multimodal discovery framework that integrates chemical language representations from PolyBERT with molecular graph-based representations from a graph encoder. The system first retrieves candidate polymers using language-based similarity and then ranks them using fused multimodal embeddings according to multiple target properties. By leveraging the complementary knowledge encoded in both modalities, PolyRecommender enables efficient retrieval and robust ranking across related polymer properties. Our work establishes a generalizable multimodal paradigm, advancing AI-guided design for the discovery of next-generation polymers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 44,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00411",
        "abs_url": "https://arxiv.org/abs/2511.00411",
        "pdf_url": "https://arxiv.org/pdf/2511.00411",
        "title": "Enhancing Adversarial Transferability by Balancing Exploration and Exploitation with Gradient-Guided Sampling",
        "authors": [
            "Zenghao Niu",
            "Weicheng Xie",
            "Siyang Song",
            "Zitong Yu",
            "Feng Liu",
            "Linlin Shen"
        ],
        "comments": "accepted by iccv 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Adversarial attacks present a critical challenge to deep neural networks' robustness, particularly in transfer scenarios across different model architectures. However, the transferability of adversarial attacks faces a fundamental dilemma between Exploitation (maximizing attack potency) and Exploration (enhancing cross-model generalization). Traditional momentum-based methods over-prioritize Exploitation, i.e., higher loss maxima for attack potency but weakened generalization (narrow loss surface). Conversely, recent methods with inner-iteration sampling over-prioritize Exploration, i.e., flatter loss surfaces for cross-model generalization but weakened attack potency (suboptimal local maxima). To resolve this dilemma, we propose a simple yet effective Gradient-Guided Sampling (GGS), which harmonizes both objectives through guiding sampling along the gradient ascent direction to improve both sampling efficiency and stability. Specifically, based on MI-FGSM, GGS introduces inner-iteration random sampling and guides the sampling direction using the gradient from the previous inner-iteration (the sampling's magnitude is determined by a random distribution). This mechanism encourages adversarial examples to reside in balanced regions with both flatness for cross-model generalization and higher local maxima for strong attack potency. Comprehensive experiments across multiple DNN architectures and multimodal large language models (MLLMs) demonstrate the superiority of our method over state-of-the-art transfer attacks. Code is made available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 45,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00413",
        "abs_url": "https://arxiv.org/abs/2511.00413",
        "pdf_url": "https://arxiv.org/pdf/2511.00413",
        "title": "Tree Training: Accelerating Agentic LLMs Training via Shared Prefix Reuse",
        "authors": [
            "Shaojie Wang",
            "Jinghui Wang",
            "Yinghan Cui",
            "Xuxing Chen",
            "Chao Wang",
            "Liang Huang",
            "Xiaojiang Zhang",
            "Junyi Peng",
            "Li Wan",
            "Haotian Zhang",
            "Bin Chen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In agentic LLM scenarios, an agent's interaction process during a single rollout often exhibits branching behaviors. Due to memory retrieval and concurrent tool executions at certain decision points, the token trajectory of one task evolves into a tree-like structure rather than a linear sequence. However, current training pipelines decompose such tree-structured trajectories into separate linear segments, treating each branch as an independent sequence. As a result, shared prefixes across these branches are repeatedly recomputed during both forward and backward passes. To address this inefficiency, we propose Tree Training, a paradigm that computes each shared prefix only once and reuses its intermediate results across related branches during both forward and backward passes, substantially improving computation efficiency in large-scale agentic training. This is achieved via (i) Tree Packing, which efficiently reuses shared computations across trajectories, and (ii) Gradient Restoration, which ensures correct gradient propagation across reused prefixes. Experiments on multiple open-source models demonstrate up to 3.9x reduction in total training time, enabling more efficient agentic LLM SFT and RL training.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 46,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00418",
        "abs_url": "https://arxiv.org/abs/2511.00418",
        "pdf_url": "https://arxiv.org/pdf/2511.00418",
        "title": "Structure-Preserving Physics-Informed Neural Network for the Korteweg--de Vries (KdV) Equation",
        "authors": [
            "Victory Obieke",
            "Emmanuel Oguadimma"
        ],
        "comments": "9 Pages, 11 figures",
        "subjects": "Machine Learning (cs.LG); Mathematical Physics (math-ph); Pattern Formation and Solitons (nlin.PS); Fluid Dynamics (physics.flu-dyn)",
        "abstract": "Physics-Informed Neural Networks (PINNs) offer a flexible framework for solving nonlinear partial differential equations (PDEs), yet conventional implementations often fail to preserve key physical invariants during long-term integration. This paper introduces a \\emph{structure-preserving PINN} framework for the nonlinear Korteweg--de Vries (KdV) equation, a prototypical model for nonlinear and dispersive wave propagation. The proposed method embeds the conservation of mass and Hamiltonian energy directly into the loss function, ensuring physically consistent and energy-stable evolution throughout training and prediction. Unlike standard \\texttt{tanh}-based PINNs~\\cite{raissi2019pinn,wang2022modifiedpinn}, our approach employs sinusoidal activation functions that enhance spectral expressiveness and accurately capture the oscillatory and dispersive nature of KdV solitons. Through representative case studies -- including single-soliton propagation (shape-preserving translation), two-soliton interaction (elastic collision with phase shift), and cosine-pulse initialization (nonlinear dispersive breakup) -- the model successfully reproduces hallmark behaviors of KdV dynamics while maintaining conserved invariants. Ablation studies demonstrate that combining invariant-constrained optimization with sinusoidal feature mappings accelerates convergence, improves long-term stability, and mitigates drift without multi-stage pretraining. These results highlight that computationally efficient, invariant-aware regularization coupled with sinusoidal representations yields robust, energy-consistent PINNs for Hamiltonian partial differential equations such as the KdV equation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 47,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00443",
        "abs_url": "https://arxiv.org/abs/2511.00443",
        "pdf_url": "https://arxiv.org/pdf/2511.00443",
        "title": "Region-Aware Reconstruction Strategy for Pre-training fMRI Foundation Model",
        "authors": [
            "Ruthwik Reddy Doodipala",
            "Pankaj Pandey",
            "Carolina Torres Rojas",
            "Manob Jyoti Saikia",
            "Ranganatha Sitaram"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The emergence of foundation models in neuroimaging is driven by the increasing availability of large-scale and heterogeneous brain imaging datasets. Recent advances in self-supervised learning, particularly reconstruction-based objectives, have demonstrated strong potential for pretraining models that generalize effectively across diverse downstream functional MRI (fMRI) tasks. In this study, we explore region-aware reconstruction strategies for a foundation model in resting-state fMRI, moving beyond approaches that rely on random region masking. Specifically, we introduce an ROI-guided masking strategy using the Automated Anatomical Labelling Atlas (AAL3), applied directly to full 4D fMRI volumes to selectively mask semantically coherent brain regions during self-supervised pretraining. Using the ADHD-200 dataset comprising 973 subjects with resting-state fMRI scans, we show that our method achieves a 4.23% improvement in classification accuracy for distinguishing healthy controls from individuals diagnosed with ADHD, compared to conventional random masking. Region-level attribution analysis reveals that brain volumes within the limbic region and cerebellum contribute most significantly to reconstruction fidelity and model representation. Our results demonstrate that masking anatomical regions during model pretraining not only enhances interpretability but also yields more robust and discriminative representations. In future work, we plan to extend this approach by evaluating it on additional neuroimaging datasets, and developing new loss functions explicitly derived from region-aware reconstruction objectives. These directions aim to further improve the robustness and interpretability of foundation models for functional neuroimaging.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 48,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00462",
        "abs_url": "https://arxiv.org/abs/2511.00462",
        "pdf_url": "https://arxiv.org/pdf/2511.00462",
        "title": "Deep Learning Approach to Anomaly Detection in Enterprise ETL Processes with Autoencoders",
        "authors": [
            "Xin Chen",
            "Saili Uday Gadgil",
            "Kangning Gao",
            "Yi Hu",
            "Cong Nie"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "An anomaly detection method based on deep autoencoders is proposed to address anomalies that often occur in enterprise-level ETL data streams. The study first analyzes multiple types of anomalies in ETL processes, including delays, missing values, duplicate loading, and sudden abnormal changes, and applies data standardization and feature modeling to ensure stable and usable inputs. In the method design, the encoder-decoder structure compresses high-dimensional inputs into latent representations and reconstructs them, while reconstruction error is used to measure anomaly levels. Regularization constraints are introduced in the latent space to enhance feature sparsity and distribution learning, thereby improving robustness in complex data streams. Systematic analyses under different hyperparameter settings, environmental changes, and data characteristics show that the proposed method achieves superior performance in AUC, ACC, Precision, and Recall. The results demonstrate that the deep autoencoder-based detection mechanism can effectively capture latent distribution patterns in enterprise-level ETL data streams and accurately identify diverse anomalies, providing reliable support for enterprise data processing and intelligent analysis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 49,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00475",
        "abs_url": "https://arxiv.org/abs/2511.00475",
        "pdf_url": "https://arxiv.org/pdf/2511.00475",
        "title": "Variational Autoencoder for Calibration: A New Approach",
        "authors": [
            "Travis Barrett",
            "Amit Kumar Mishra",
            "Joyce Mwangama"
        ],
        "comments": "6 pages, 5 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In this paper we present a new implementation of a Variational Autoencoder (VAE) for the calibration of sensors. We propose that the VAE can be used to calibrate sensor data by training the latent space as a calibration output. We discuss this new approach and show a proof-of-concept using an existing multi-sensor gas dataset. We show the performance of the proposed calibration VAE and found that it was capable of performing as calibration model while performing as an autoencoder simultaneously. Additionally, these models have shown that they are capable of creating statistically similar outputs from both the calibration output as well as the reconstruction output to their respective truth data. We then discuss the methods of future testing and planned expansion of this work.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 50,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00532",
        "abs_url": "https://arxiv.org/abs/2511.00532",
        "pdf_url": "https://arxiv.org/pdf/2511.00532",
        "title": "Air Pollution Forecasting in Bucharest",
        "authors": [
            "Dragoş-Andrei Şerban",
            "Răzvan-Alexandru Smădu",
            "Dumitru-Clementin Cercel"
        ],
        "comments": "14 pages 3 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Applications (stat.AP)",
        "abstract": "Air pollution, especially the particulate matter 2.5 (PM2.5), has become a growing concern in recent years, primarily in urban areas. Being exposed to air pollution is linked to developing numerous health problems, like the aggravation of respiratory diseases, cardiovascular disorders, lung function impairment, and even cancer or early death. Forecasting future levels of PM2.5 has become increasingly important over the past few years, as it can provide early warnings and help prevent diseases. This paper aims to design, fine-tune, test, and evaluate machine learning models for predicting future levels of PM2.5 over various time horizons. Our primary objective is to assess and compare the performance of multiple models, ranging from linear regression algorithms and ensemble-based methods to deep learning models, such as advanced recurrent neural networks and transformers, as well as large language models, on this forecasting task.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 51,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00549",
        "abs_url": "https://arxiv.org/abs/2511.00549",
        "pdf_url": "https://arxiv.org/pdf/2511.00549",
        "title": "Robust Single-Agent Reinforcement Learning for Regional Traffic Signal Control Under Demand Fluctuations",
        "authors": [
            "Qiang Li",
            "Jin Niu",
            "Lina Yu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Traffic congestion, primarily driven by intersection queuing, significantly impacts urban living standards, safety, environmental quality, and economic efficiency. While Traffic Signal Control (TSC) systems hold potential for congestion mitigation, traditional optimization models often fail to capture real-world traffic complexity and dynamics. This study introduces a novel single-agent reinforcement learning (RL) framework for regional adaptive TSC, circumventing the coordination complexities inherent in multi-agent systems through a centralized decision-making paradigm. The model employs an adjacency matrix to unify the encoding of road network topology, real-time queue states derived from probe vehicle data, and current signal timing parameters. Leveraging the efficient learning capabilities of the DreamerV3 world model, the agent learns control policies where actions sequentially select intersections and adjust their signal phase splits to regulate traffic inflow/outflow, analogous to a feedback control system. Reward design prioritizes queue dissipation, directly linking congestion metrics (queue length) to control actions. Simulation experiments conducted in SUMO demonstrate the model's effectiveness: under inference scenarios with multi-level (10%, 20%, 30%) Origin-Destination (OD) demand fluctuations, the framework exhibits robust anti-fluctuation capability and significantly reduces queue lengths. This work establishes a new paradigm for intelligent traffic control compatible with probe vehicle technology. Future research will focus on enhancing practical applicability by incorporating stochastic OD demand fluctuations during training and exploring regional optimization mechanisms for contingency events.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 52,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00554",
        "abs_url": "https://arxiv.org/abs/2511.00554",
        "pdf_url": "https://arxiv.org/pdf/2511.00554",
        "title": "Red-teaming Activation Probes using Prompted LLMs",
        "authors": [
            "Phil Blandfort",
            "Robert Graham"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Activation probes are attractive monitors for AI systems due to low cost and latency, but their real-world robustness remains underexplored. We ask: What failure modes arise under realistic, black-box adversarial pressure, and how can we surface them with minimal effort? We present a lightweight black-box red-teaming procedure that wraps an off-the-shelf LLM with iterative feedback and in-context learning (ICL), and requires no fine-tuning, gradients, or architectural access. Running a case study with probes for high-stakes interactions, we show that our approach can help discover valuable insights about a SOTA probe. Our analysis uncovers interpretable brittleness patterns (e.g., legalese-induced FPs; bland procedural tone FNs) and reduced but persistent vulnerabilities under scenario-constraint attacks. These results suggest that simple prompted red-teaming scaffolding can anticipate failure patterns before deployment and might yield promising, actionable insights to harden future probes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 53,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00574",
        "abs_url": "https://arxiv.org/abs/2511.00574",
        "pdf_url": "https://arxiv.org/pdf/2511.00574",
        "title": "Bayesian Network Structure Discovery Using Large Language Models",
        "authors": [
            "Yinghuan Zhang",
            "Yufei Zhang",
            "Parisa Kordjamshidi",
            "Zijun Cui"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Understanding probabilistic relationships among variables is crucial for analyzing complex systems. Traditional structure learning methods often require extensive observational data and incur high computational costs. Recent studies have explored using large language models (LLMs) for structure learning, but most treat LLMs as auxiliary tools for pre-processing or post-processing, leaving the core learning process data-driven. In this work, we propose a unified framework for Bayesian network structure discovery that places LLMs at the center, supporting both data-free and data-aware settings. In the data-free case, we introduce \\textbf{PromptBN} to query LLMs with metadata and efficiently uncover valid probabilistic relationships. When observational data are available, we introduce \\textbf{ReActBN}, which integrates the ReAct reasoning paradigm with structure scores such as the Bayesian Information Criterion (BIC) for iterative refinement. Unlike prior methods that offload refinement to external algorithms, our framework maintains the LLM actively in the loop throughout the discovery process. Experiments demonstrate that our method significantly outperforms both existing LLM-based approaches and traditional data-driven algorithms, particularly in the low- or no-data scenario. Code is publicly available at {\\texttt{\\textcolor{magenta}{this https URL}}}.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 54,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00579",
        "abs_url": "https://arxiv.org/abs/2511.00579",
        "pdf_url": "https://arxiv.org/pdf/2511.00579",
        "title": "Sparse and nonparametric estimation of equations governing dynamical systems with applications to biology",
        "authors": [
            "G. Pillonetto",
            "A. Giaretta",
            "A. Aravkin",
            "M. Bisiacco",
            "T. Elston"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Quantitative Methods (q-bio.QM); Machine Learning (stat.ML)",
        "abstract": "Data-driven discovery of model equations is a powerful approach for understanding the behavior of dynamical systems in many scientific fields. In particular, the ability to learn mathematical models from data would benefit systems biology, where the complex nature of these systems often makes a bottom up approach to modeling unfeasible. In recent years, sparse estimation techniques have gained prominence in system identification, primarily using parametric paradigms to efficiently capture system dynamics with minimal model complexity. In particular, the Sindy algorithm has successfully used sparsity to estimate nonlinear systems by extracting from a library of functions only a few key terms needed to capture the dynamics of these systems. However, parametric models often fall short in accurately representing certain nonlinearities inherent in complex systems. To address this limitation, we introduce a novel framework that integrates sparse parametric estimation with nonparametric techniques. It captures nonlinearities that Sindy cannot describe without requiring a priori information about their functional form. That is, without expanding the library of functions to include the one that is trying to be discovered. We illustrate our approach on several examples related to estimation of complex biological phenomena.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 55,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00588",
        "abs_url": "https://arxiv.org/abs/2511.00588",
        "pdf_url": "https://arxiv.org/pdf/2511.00588",
        "title": "Diagnosing Hallucination Risk in AI Surgical Decision-Support: A Sequential Framework for Sequential Validation",
        "authors": [
            "Dong Chen",
            "Yanzhe Wei",
            "Zonglin He",
            "Guan-Ming Kuang",
            "Canhua Ye",
            "Meiru An",
            "Huili Peng",
            "Yong Hu",
            "Huiren Tao",
            "Kenneth MC Cheung"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computers and Society (cs.CY)",
        "abstract": "Large language models (LLMs) offer transformative potential for clinical decision support in spine surgery but pose significant risks through hallucinations, which are factually inconsistent or contextually misaligned outputs that may compromise patient safety. This study introduces a clinician-centered framework to quantify hallucination risks by evaluating diagnostic precision, recommendation quality, reasoning robustness, output coherence, and knowledge alignment. We assessed six leading LLMs across 30 expert-validated spinal cases. DeepSeek-R1 demonstrated superior overall performance (total score: 86.03 $\\pm$ 2.08), particularly in high-stakes domains such as trauma and infection. A critical finding reveals that reasoning-enhanced model variants did not uniformly outperform standard counterparts: Claude-3.7-Sonnet's extended thinking mode underperformed relative to its standard version (80.79 $\\pm$ 1.83 vs. 81.56 $\\pm$ 1.92), indicating extended chain-of-thought reasoning alone is insufficient for clinical reliability. Multidimensional stress-testing exposed model-specific vulnerabilities, with recommendation quality degrading by 7.4% under amplified complexity. This decline contrasted with marginal improvements in rationality (+2.0%), readability (+1.7%) and diagnosis (+4.7%), highlighting a concerning divergence between perceived coherence and actionable guidance. Our findings advocate integrating interpretability mechanisms (e.g., reasoning chain visualization) into clinical workflows and establish a safety-aware validation framework for surgical LLM deployment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 56,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00615",
        "abs_url": "https://arxiv.org/abs/2511.00615",
        "pdf_url": "https://arxiv.org/pdf/2511.00615",
        "title": "Gaining Momentum: Uncovering Hidden Scoring Dynamics in Hockey through Deep Neural Sequencing and Causal Modeling",
        "authors": [
            "Daniel Griffiths",
            "Piper Moskow"
        ],
        "comments": "5 Pages, 4 Figures, 2 Tables",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We present a unified, data-driven framework for quantifying and enhancing offensive momentum and scoring likelihood (expected goals, xG) in professional hockey. Leveraging a Sportlogiq dataset of 541,000 NHL event records, our end-to-end pipeline comprises five stages: (1) interpretable momentum weighting of micro-events via logistic regression; (2) nonlinear xG estimation using gradient-boosted decision trees; (3) temporal sequence modeling with Long Short-Term Memory (LSTM) networks; (4) spatial formation discovery through principal component analysis (PCA) followed by K-Means clustering on standardized player coordinates; and (5) use of an X-Learner causal inference estimator to quantify the average treatment effect (ATE) of adopting the identified \"optimal\" event sequences and formations. We observe an ATE of 0.12 (95% CI: 0.05-0.17, p < 1e-50), corresponding to a 15% relative gain in scoring potential. These results demonstrate that strategically structured sequences and compact formations causally elevate offensive performance. Our framework delivers real-time, actionable insights for coaches and analysts, advancing hockey analytics toward principled, causally grounded tactical optimization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 57,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00637",
        "abs_url": "https://arxiv.org/abs/2511.00637",
        "pdf_url": "https://arxiv.org/pdf/2511.00637",
        "title": "Stochastic Shortest Path with Sparse Adversarial Costs",
        "authors": [
            "Emmeran Johnson",
            "Alberto Rumi",
            "Ciara Pike-Burke",
            "Patrick Rebeschini"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "We study the adversarial Stochastic Shortest Path (SSP) problem with sparse costs under full-information feedback. In the known transition setting, existing bounds based on Online Mirror Descent (OMD) with negative-entropy regularization scale with $\\sqrt{\\log S A}$, where $SA$ is the size of the state-action space. While we show that this is optimal in the worst-case, this bound fails to capture the benefits of sparsity when only a small number $M \\ll SA$ of state-action pairs incur cost. In fact, we also show that the negative-entropy is inherently non-adaptive to sparsity: it provably incurs regret scaling with $\\sqrt{\\log S}$ on sparse problems. Instead, we propose a family of $\\ell_r$-norm regularizers ($r \\in (1,2)$) that adapts to the sparsity and achieves regret scaling with $\\sqrt{\\log M}$ instead of $\\sqrt{\\log SA}$. We show this is optimal via a matching lower bound, highlighting that $M$ captures the effective dimension of the problem instead of $SA$. Finally, in the unknown transition setting the benefits of sparsity are limited: we prove that even on sparse problems, the minimax regret for any learner scales polynomially with $SA$.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 58,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00648",
        "abs_url": "https://arxiv.org/abs/2511.00648",
        "pdf_url": "https://arxiv.org/pdf/2511.00648",
        "title": "Diluting Restricted Boltzmann Machines",
        "authors": [
            "C. Díaz-Faloh",
            "R. Mulet"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Recent advances in artificial intelligence have relied heavily on increasingly large neural networks, raising concerns about their computational and environmental costs. This paper investigates whether simpler, sparser networks can maintain strong performance by studying Restricted Boltzmann Machines (RBMs) under extreme pruning conditions. Inspired by the Lottery Ticket Hypothesis, we demonstrate that RBMs can achieve high-quality generative performance even when up to 80% of the connections are pruned before training, confirming that they contain viable sub-networks. However, our experiments reveal crucial limitations: trained networks cannot fully recover lost performance through retraining once additional pruning is applied. We identify a sharp transition above which the generative quality degrades abruptly when pruning disrupts a minimal core of essential connections. Moreover, re-trained networks remain constrained by the parameters originally learned performing worse than networks trained from scratch at equivalent sparsity levels. These results suggest that for sparse networks to work effectively, pruning should be implemented early in training rather than attempted afterwards. Our findings provide practical insights for the development of efficient neural architectures and highlight the persistent influence of initial conditions on network capabilities.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 59,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00655",
        "abs_url": "https://arxiv.org/abs/2511.00655",
        "pdf_url": "https://arxiv.org/pdf/2511.00655",
        "title": "Reviving Stale Updates: Data-Free Knowledge Distillation for Asynchronous Federated Learning",
        "authors": [
            "Baris Askin",
            "Holger R. Roth",
            "Zhenyu Sun",
            "Carlee Joe-Wong",
            "Gauri Joshi",
            "Ziyue Xu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Federated Learning (FL) enables collaborative model training across distributed clients without sharing raw data, yet its scalability is limited by synchronization overhead. Asynchronous Federated Learning (AFL) alleviates this issue by allowing clients to communicate independently, thereby improving wall-clock efficiency in large-scale, heterogeneous environments. However, this asynchrony introduces stale updates (client updates computed on outdated global models) that can destabilize optimization and hinder convergence. We propose FedRevive, an asynchronous FL framework that revives stale updates through data-free knowledge distillation (DFKD). FedRevive integrates parameter-space aggregation with a lightweight, server-side DFKD process that transfers knowledge from stale client models to the current global model without access to real or public data. A meta-learned generator synthesizes pseudo-samples, which enables multi-teacher distillation. A hybrid aggregation scheme that combines raw updates with DFKD updates effectively mitigates staleness while retaining the scalability of AFL. Experiments on various vision and text benchmarks show that FedRevive achieves faster training up to 32.1% and higher final accuracy up to 21.5% compared to asynchronous baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 60,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00663",
        "abs_url": "https://arxiv.org/abs/2511.00663",
        "pdf_url": "https://arxiv.org/pdf/2511.00663",
        "title": "Sensitivity Analysis for Climate Science with Generative Flow Models",
        "authors": [
            "Alex Dobra",
            "Jakiw Pidstrigach",
            "Tim Reichelt",
            "Paolo Fraccaro",
            "Johannes Jakubik",
            "Anne Jones",
            "Christian Schroeder de Witt",
            "Philip Stier",
            "Philip Torr"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Sensitivity analysis is a cornerstone of climate science, essential for understanding phenomena ranging from storm intensity to long-term climate feedbacks. However, computing these sensitivities using traditional physical models is often prohibitively expensive in terms of both computation and development time. While modern AI-based generative models are orders of magnitude faster to evaluate, computing sensitivities with them remains a significant bottleneck. This work addresses this challenge by applying the adjoint state method for calculating gradients in generative flow models, with diffusion models as a special case. We apply this method to the cBottle generative model, an emulator of ERA5 data, to perform sensitivity analysis with respect to sea surface temperatures. Furthermore, we propose a novel gradient self-consistency check to quantitatively validate the computed sensitivities against the model's own outputs. Our results provide initial evidence that this approach can produce reliable gradients, reducing the computational cost of sensitivity analysis from weeks on a supercomputer with a physical model to hours on a GPU, thereby simplifying a critical workflow in climate science.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 61,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00699",
        "abs_url": "https://arxiv.org/abs/2511.00699",
        "pdf_url": "https://arxiv.org/pdf/2511.00699",
        "title": "Inference-Time Chain-of-Thought Pruning with Latent Informativeness Signals",
        "authors": [
            "Sophie Li",
            "Nicholas Huang",
            "Nayan Saxena",
            "Nina Luo",
            "Vincent Lin",
            "Kevin Zhu",
            "Sunishchal Dev"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Large language models (LLMs) improve reasoning accuracy when generating multiple candidate solutions at test time, but standard methods like Best-of-N (BoN) incur high computational cost by fully generating all branches. Self-Truncation Best-of-N (ST-BoN) mitigates this by truncating unpromising paths early, but its reliance on consistency-based heuristics is a limitation as it does not directly evaluate branch quality. We present KL-Adjusted Pruned Path Algorithm (KAPPA), an inference-time method that combines Kullback-Leibler divergence, confidence, and entropy into a principled scoring function to guide progressive pruning. By promoting diversity during exploration and selectively eliminating low-scoring branches, KAPPA maintains accuracy while substantially reducing memory and token usage. Experiments on GSM8K and MATH500 with DeepSeek-R1-Distill-Qwen-1.5B and Qwen2.5-7B-Instruct demonstrate that KAPPA stabilizes performance in smaller models and achieves up to ~60% reduction in peak memory and ~90% reduction in total token generation relative to BoN, with minimal impact on accuracy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 62,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00700",
        "abs_url": "https://arxiv.org/abs/2511.00700",
        "pdf_url": "https://arxiv.org/pdf/2511.00700",
        "title": "Privacy-Aware Time Series Synthesis via Public Knowledge Distillation",
        "authors": [
            "Penghang Liu",
            "Haibei Zhu",
            "Eleonora Kreacic",
            "Svitlana Vyetrenko"
        ],
        "comments": "Published on Transactions on Machine Learning Research (TMLR)",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Sharing sensitive time series data in domains such as finance, healthcare, and energy consumption, such as patient records or investment accounts, is often restricted due to privacy concerns. Privacy-aware synthetic time series generation addresses this challenge by enforcing noise during training, inherently introducing a trade-off between privacy and utility. In many cases, sensitive sequences is correlated with publicly available, non-sensitive contextual metadata (e.g., household electricity consumption may be influenced by weather conditions and electricity prices). However, existing privacy-aware data generation methods often overlook this opportunity, resulting in suboptimal privacy-utility trade-offs. In this paper, we present Pub2Priv, a novel framework for generating private time series data by leveraging heterogeneous public knowledge. Our model employs a self-attention mechanism to encode public data into temporal and feature embeddings, which serve as conditional inputs for a diffusion model to generate synthetic private sequences. Additionally, we introduce a practical metric to assess privacy by evaluating the identifiability of the synthetic data. Experimental results show that Pub2Priv consistently outperforms state-of-the-art benchmarks in improving the privacy-utility trade-off across finance, energy, and commodity trading domains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 63,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00704",
        "abs_url": "https://arxiv.org/abs/2511.00704",
        "pdf_url": "https://arxiv.org/pdf/2511.00704",
        "title": "Investigating the Robustness of Knowledge Tracing Models in the Presence of Student Concept Drift",
        "authors": [
            "Morgan Lee",
            "Artem Frenk",
            "Eamon Worden",
            "Karish Gupta",
            "Thinh Pham",
            "Ethan Croteau",
            "Neil Heffernan"
        ],
        "comments": "10 pages, 6 figures",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Knowledge Tracing (KT) has been an established problem in the educational data mining field for decades, and it is commonly assumed that the underlying learning process be- ing modeled remains static. Given the ever-changing land- scape of online learning platforms (OLPs), we investigate how concept drift and changing student populations can im- pact student behavior within an OLP through testing model performance both within a single academic year and across multiple academic years. Four well-studied KT models were applied to five academic years of data to assess how suscep- tible KT models are to concept drift. Through our analysis, we find that all four families of KT models can exhibit de- graded performance, Bayesian Knowledge Tracing (BKT) remains the most stable KT model when applied to newer data, while more complex, attention based models lose pre- dictive power significantly faster. To foster more longitu- dinal evaluations of KT models, the data used to conduct our analysis is available at this https URL only=b936c63dfdae4b0b987a2f0d4038f72a",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 64,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00716",
        "abs_url": "https://arxiv.org/abs/2511.00716",
        "pdf_url": "https://arxiv.org/pdf/2511.00716",
        "title": "Enhancing Heavy Rain Nowcasting with Multimodal Data: Integrating Radar and Satellite Observations",
        "authors": [
            "Rama Kassoumeh",
            "David Rügamer",
            "Henning Oppel"
        ],
        "comments": "accepted to ICMLA 2025",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The increasing frequency of heavy rainfall events, which are a major cause of urban flooding, underscores the urgent need for accurate precipitation forecasting - particularly in urban areas where localized events often go undetected by ground-based sensors. In Germany, only 17.3% of hourly heavy rain events between 2001 and 2018 were recorded by rain gauges, highlighting the limitations of traditional monitoring systems. Radar data are another source that effectively tracks ongoing precipitation; however, forecasting the development of heavy rain using radar alone remains challenging due to the brief and unpredictable nature of such events. Our focus is on evaluating the effectiveness of fusing satellite and radar data for nowcasting. We develop a multimodal nowcasting model that combines both radar and satellite imagery for predicting precipitation at lead times of 5, 15, and 30 minutes. We demonstrate that this multimodal strategy significantly outperforms radar-only approaches. Experimental results show that integrating satellite data improves prediction accuracy, particularly for intense precipitation. The proposed model increases the Critical Success Index for heavy rain by 4% and for violent rain by 3% at a 5-minute lead time. Moreover, it maintains higher predictive skill at longer lead times, where radar-only performance declines. A qualitative analysis of the severe flooding event in the state of North Rhine-Westphalia, Germany in 2021 further illustrates the superior performance of the multimodal model. Unlike the radar-only model, which captures general precipitation patterns, the multimodal model yields more detailed and accurate forecasts for regions affected by heavy rain. This improved precision enables timely, reliable, life-saving warnings. Implementation available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 65,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00747",
        "abs_url": "https://arxiv.org/abs/2511.00747",
        "pdf_url": "https://arxiv.org/pdf/2511.00747",
        "title": "Effective Series Decomposition and Components Learning for Time Series Generation",
        "authors": [
            "Zixuan Ma",
            "Chenfeng Huang"
        ],
        "comments": "Accepted at IEEE International Conference on Data Mining (ICDM 2025). Camera-ready version to appear",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Time series generation focuses on modeling the underlying data distribution and resampling to produce authentic time series data. Key components, such as trend and seasonality, drive temporal fluctuations, yet many existing approaches fail to employ interpretative decomposition methods, limiting their ability to synthesize meaningful trend and seasonal patterns. To address this gap, we introduce Seasonal-Trend Diffusion (STDiffusion), a novel framework for multivariate time series generation that integrates diffusion probabilistic models with advanced learnable series decomposition techniques, enhancing the interpretability of the generation process. Our approach separates the trend and seasonal learning into distinct blocks: a Multi-Layer Perceptron (MLP) structure captures the trend, while adaptive wavelet distillation facilitates effective multi-resolution learning of seasonal components. This decomposition improves the interpretability of the model on multiple scales. In addition, we designed a comprehensive correction mechanism aimed at ensuring that the generated components exhibit a high degree of internal consistency and preserve meaningful interrelationships with one another. Our empirical studies on eight real-world datasets demonstrate that STDiffusion achieves state-of-the-art performance in time series generation tasks. Furthermore, we extend the model's application to multi-window long-sequence time series generation, which delivered reliable results and highlighted its robustness and versatility.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 66,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00792",
        "abs_url": "https://arxiv.org/abs/2511.00792",
        "pdf_url": "https://arxiv.org/pdf/2511.00792",
        "title": "Fast PINN Eigensolvers via Biconvex Reformulation",
        "authors": [
            "Akshay Sai Banderwaar",
            "Abhishek Gupta"
        ],
        "comments": "7 pages, 3 figures, Machine Learning and the Physical Sciences Workshop NeurIPS 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)",
        "abstract": "Eigenvalue problems have a distinctive forward-inverse structure and are fundamental to characterizing a system's thermal response, stability, and natural modes. Physics-Informed Neural Networks (PINNs) offer a mesh-free alternative for solving such problems but are often orders of magnitude slower than classical numerical schemes. In this paper, we introduce a reformulated PINN approach that casts the search for eigenpairs as a biconvex optimization problem, enabling fast and provably convergent alternating convex search (ACS) over eigenvalues and eigenfunctions using analytically optimal updates. Numerical experiments show that PINN-ACS attains high accuracy with convergence speeds up to 500$\\times$ faster than gradient-based PINN training. We release our codes at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 67,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00797",
        "abs_url": "https://arxiv.org/abs/2511.00797",
        "pdf_url": "https://arxiv.org/pdf/2511.00797",
        "title": "Attention Saturation and Gradient Suppression at Inflection Layers: Diagnosing and Mitigating Bottlenecks in Transformer Adaptation",
        "authors": [
            "Wang Zixian"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Pre-trained Transformers often exhibit over-confidence in source patterns and difficulty in forming new target-domain patterns during fine-tuning. We formalize the mechanism of output saturation leading to gradient suppression through standard cross-entropy and softmax analysis, showing that gradient suppression at inflection layers confines adaptation to high-level recombination of existing features while preventing low-level reconstruction. We introduce a set of layer-wise diagnostic metrics -- attention entropy (saturation proxy), activation gradient norm, parameter gradient norm, and Delta-CKA under a shared PCA basis -- to identify inflection layers characterized by both low attention entropy and steep gradient decay. Building on these findings, we propose a diagnose-first, inject-light fine-tuning strategy: selectively inserting LoRA adapters at inflection layers to restore suppressed backward signals with minimal parameter overhead. Experiments on BERT-base transfer from SST-2 to Rotten Tomatoes under under-trained and over-trained source regimes reveal that over-trained initialization benefits from inflection-layer LoRA injection, while under-trained initialization suffers performance degradation. When base features are strong, unblocking inflection layers facilitates high-level compositional adaptation; when base features are weak, full-pathway unblocking is required for low-level reconstruction, as supported by joint analysis of layer-wise activation gradients and Delta-CKA dynamics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 68,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00806",
        "abs_url": "https://arxiv.org/abs/2511.00806",
        "pdf_url": "https://arxiv.org/pdf/2511.00806",
        "title": "Logic-informed reinforcement learning for cross-domain optimization of large-scale cyber-physical systems",
        "authors": [
            "Guangxi Wan",
            "Peng Zeng",
            "Xiaoting Dong",
            "Chunhe Song",
            "Shijie Cui",
            "Dong Li",
            "Qingwei Dong",
            "Yiyang Liu",
            "Hongfei Bai"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Cyber-physical systems (CPS) require the joint optimization of discrete cyber actions and continuous physical parameters under stringent safety logic constraints. However, existing hierarchical approaches often compromise global optimality, whereas reinforcement learning (RL) in hybrid action spaces often relies on brittle reward penalties, masking, or shielding and struggles to guarantee constraint satisfaction. We present logic-informed reinforcement learning (LIRL), which equips standard policy-gradient algorithms with projection that maps a low-dimensional latent action onto the admissible hybrid manifold defined on-the-fly by first-order logic. This guarantees feasibility of every exploratory step without penalty tuning. Experimental evaluations have been conducted across multiple scenarios, including industrial manufacturing, electric vehicle charging stations, and traffic signal control, in all of which the proposed method outperforms existing hierarchical optimization approaches. Taking a robotic reducer assembly system in industrial manufacturing as an example, LIRL achieves a 36.47\\% to 44.33\\% reduction at most in the combined makespan-energy objective compared to conventional industrial hierarchical scheduling methods. Meanwhile, it consistently maintains zero constraint violations and significantly surpasses state-of-the-art hybrid-action reinforcement learning baselines. Thanks to its declarative logic-based constraint formulation, the framework can be seamlessly transferred to other domains such as smart transportation and smart grid, thereby paving the way for safe and real-time optimization in large-scale CPS.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 69,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00811",
        "abs_url": "https://arxiv.org/abs/2511.00811",
        "pdf_url": "https://arxiv.org/pdf/2511.00811",
        "title": "Equilibrium Policy Generalization: A Reinforcement Learning Framework for Cross-Graph Zero-Shot Generalization in Pursuit-Evasion Games",
        "authors": [
            "Runyu Lu",
            "Peng Zhang",
            "Ruochuan Shi",
            "Yuanheng Zhu",
            "Dongbin Zhao",
            "Yang Liu",
            "Dong Wang",
            "Cesare Alippi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Equilibrium learning in adversarial games is an important topic widely examined in the fields of game theory and reinforcement learning (RL). Pursuit-evasion game (PEG), as an important class of real-world games from the fields of robotics and security, requires exponential time to be accurately solved. When the underlying graph structure varies, even the state-of-the-art RL methods require recomputation or at least fine-tuning, which can be time-consuming and impair real-time applicability. This paper proposes an Equilibrium Policy Generalization (EPG) framework to effectively learn a generalized policy with robust cross-graph zero-shot performance. In the context of PEGs, our framework is generally applicable to both pursuer and evader sides in both no-exit and multi-exit scenarios. These two generalizability properties, to our knowledge, are the first to appear in this domain. The core idea of the EPG framework is to train an RL policy across different graph structures against the equilibrium policy for each single graph. To construct an equilibrium oracle for single-graph policies, we present a dynamic programming (DP) algorithm that provably generates pure-strategy Nash equilibrium with near-optimal time complexity. To guarantee scalability with respect to pursuer number, we further extend DP and RL by designing a grouping mechanism and a sequence model for joint policy decomposition, respectively. Experimental results show that, using equilibrium guidance and a distance feature proposed for cross-graph PEG training, the EPG framework guarantees desirable zero-shot performance in various unseen real-world graphs. Besides, when trained under an equilibrium heuristic proposed for the graphs with exits, our generalized pursuer policy can even match the performance of the fine-tuned policies from the state-of-the-art PEG methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 70,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00851",
        "abs_url": "https://arxiv.org/abs/2511.00851",
        "pdf_url": "https://arxiv.org/pdf/2511.00851",
        "title": "Identifying Slug Formation in Oil Well Pipelines: A Use Case from Industrial Analytics",
        "authors": [
            "Abhishek Patange",
            "Sharat Chidambaran",
            "Prabhat Shankar",
            "Manjunath G.B.",
            "Anindya Chatterjee"
        ],
        "comments": "This paper ID 254 has been accepted for presentation in the Demonstration Track of the 13th ACM IKDD CODS Conference on Data Science CODS 2025, IISER Pune, India, from December 17 to 20, 2025",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Slug formation in oil and gas pipelines poses significant challenges to operational safety and efficiency, yet existing detection approaches are often offline, require domain expertise, and lack real-time interpretability. We present an interactive application that enables end-to-end data-driven slug detection through a compact and user-friendly interface. The system integrates data exploration and labeling, configurable model training and evaluation with multiple classifiers, visualization of classification results with time-series overlays, and a real-time inference module that generates persistence-based alerts when slug events are detected. The demo supports seamless workflows from labeled CSV uploads to live inference on unseen datasets, making it lightweight, portable, and easily deployable. By combining domain-relevant analytics with novel UI/UX features such as snapshot persistence, visual labeling, and real-time alerting, our tool adds significant dissemination value as both a research prototype and a practical industrial application. The demo showcases how interactive human-in-the-loop ML systems can bridge the gap between data science methods and real-world decision-making in critical process industries, with broader applicability to time-series fault diagnosis tasks beyond oil and gas.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 71,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00868",
        "abs_url": "https://arxiv.org/abs/2511.00868",
        "pdf_url": "https://arxiv.org/pdf/2511.00868",
        "title": "FlexiCache: Leveraging Temporal Stability of Attention Heads for Efficient KV Cache Management",
        "authors": [
            "Nazmul Takbir",
            "Hamidreza Alikhani",
            "Nikil Dutt",
            "Sangeetha Abdu Jyothi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Large Language Model (LLM) serving is increasingly constrained by the growing size of the key-value (KV) cache, which scales with both context length and generation length. Prior work shows that attention is dominated by a small subset of critical tokens, yet existing systems struggle to exploit this efficiently without degrading accuracy, especially in long generation. We make a key observation: the temporal stability of these critical tokens varies significantly across KV heads: some heads consistently focus on the same tokens, while others shift frequently. Building on this insight, we introduce FlexiCache, a hierarchical KV-cache management system that leverages the temporal stability of KV heads to reduce GPU memory usage and computation overhead, while preserving model accuracy. FlexiCache classifies KV heads as stable or unstable: it retains all KV-cache pages from unstable heads in GPU memory, whereas for stable heads, it keeps only the top-K pages on the GPU and offloads the rest to host memory. By exploiting temporal stability, FlexiCache performs periodic reranking for stable heads to fetch newly promoted top pages. Implemented atop vLLM, FlexiCache reduces GPU memory footprint for long-context requests by up to 70%, improves offline serving throughput by 1.38-1.55x, and lowers online token latency by 1.6-2.1x, all while maintaining accuracy in long-context, long-generation scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 72,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00874",
        "abs_url": "https://arxiv.org/abs/2511.00874",
        "pdf_url": "https://arxiv.org/pdf/2511.00874",
        "title": "Training with Fewer Bits: Unlocking Edge LLMs Training with Stochastic Rounding",
        "authors": [
            "Taowen Liu",
            "Marta Andronic",
            "Deniz Gündüz",
            "George A. Constantinides"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Numerical Analysis (math.NA)",
        "abstract": "LLM training is resource-intensive. Quantized training improves computational and memory efficiency but introduces quantization noise, which can hinder convergence and degrade model accuracy. Stochastic Rounding (SR) has emerged as a theoretically attractive alternative to deterministic rounding, offering unbiased gradient estimates. However, its interaction with other training factors -- especially batch size -- remains under explored. In this paper, we present a theoretical and empirical study of mini-batch stochastic gradient descent (SGD) with SR, showing that increased batch sizes can compensate for reduced precision during back-propagation. Furthermore, we show that quantizing weights and activations impacts gradient variance in distinct ways. Our experiments validate these theoretical insights.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 73,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00885",
        "abs_url": "https://arxiv.org/abs/2511.00885",
        "pdf_url": "https://arxiv.org/pdf/2511.00885",
        "title": "SpEx: A Spectral Approach to Explainable Clustering",
        "authors": [
            "Tal Argov",
            "Tal Wagner"
        ],
        "comments": "NeurIPS 2025",
        "subjects": "Machine Learning (cs.LG); Data Structures and Algorithms (cs.DS)",
        "abstract": "Explainable clustering by axis-aligned decision trees was introduced by Moshkovitz et al. (2020) and has gained considerable interest. Prior work has focused on minimizing the price of explainability for specific clustering objectives, lacking a general method to fit an explanation tree to any given clustering, without restrictions. In this work, we propose a new and generic approach to explainable clustering, based on spectral graph partitioning. With it, we design an explainable clustering algorithm that can fit an explanation tree to any given non-explainable clustering, or directly to the dataset itself. Moreover, we show that prior algorithms can also be interpreted as graph partitioning, through a generalized framework due to Trevisan (2013) wherein cuts are optimized in two graphs simultaneously. Our experiments show the favorable performance of our method compared to baselines on a range of datasets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 74,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00904",
        "abs_url": "https://arxiv.org/abs/2511.00904",
        "pdf_url": "https://arxiv.org/pdf/2511.00904",
        "title": "Random Spiking Neural Networks are Stable and Spectrally Simple",
        "authors": [
            "Ernesto Araya",
            "Massimiliano Datres",
            "Gitta Kutyniok"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Spiking neural networks (SNNs) are a promising paradigm for energy-efficient computation, yet their theoretical foundations-especially regarding stability and robustness-remain limited compared to artificial neural networks. In this work, we study discrete-time leaky integrate-and-fire (LIF) SNNs through the lens of Boolean function analysis. We focus on noise sensitivity and stability in classification tasks, quantifying how input perturbations affect outputs. Our main result shows that wide LIF-SNN classifiers are stable on average, a property explained by the concentration of their Fourier spectrum on low-frequency components. Motivated by this, we introduce the notion of spectral simplicity, which formalizes simplicity in terms of Fourier spectrum concentration and connects our analysis to the simplicity bias observed in deep networks. Within this framework, we show that random LIF-SNNs are biased toward simple functions. Experiments on trained networks confirm that these stability properties persist in practice. Together, these results provide new insights into the stability and robustness properties of SNNs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 75,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00907",
        "abs_url": "https://arxiv.org/abs/2511.00907",
        "pdf_url": "https://arxiv.org/pdf/2511.00907",
        "title": "Transformers as Intrinsic Optimizers: Forward Inference through the Energy Principle",
        "authors": [
            "Ruifeng Ren",
            "Sheng Ouyang",
            "Huayi Tang",
            "Yong Liu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Transformers have demonstrated strong adaptability across a wide range of tasks and have become the backbone of modern Large Language Models (LLMs). However, their underlying mechanisms remain open for further exploration. The energy-based perspective has long provided a valuable principle for understanding neural computation. In this paper, we revisit the principle of energy as a lens to understand attention-based Transformer models. We present a unified energy-based framework which is composed of three key components: the global energy $F^*$, the energy function $E_i$ and the employed gradient descent (GD) form. Within this framework, standard softmax attention can be viewed as a special case of minimizing the Helmholtz free energy as $F^*$ using standard GD when $E_i$ takes the form of elastic potential energy, with residual connections ensuring that this optimization proceeds in an incremental manner. In addition, linear attentions can also be naturally incorporated into this framework by adjusting the corresponding energy forms. We also extend the above analysis to the multi-head setting, where the energy is defined across multiple low-dimensional subspaces. Building on this framework, we propose energy-based modifications of attention structures. Inspired by classical GD algorithms, we extend the original attention formulation based on standard GD to the momentum-based GD, Nesterov Accelerated Gradient (NAG), and Newton's method variants, each inducing a corresponding new attention structure. Our experiments provide preliminary support for the potential of the energy-based framework for designing attention mechanisms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 76,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00949",
        "abs_url": "https://arxiv.org/abs/2511.00949",
        "pdf_url": "https://arxiv.org/pdf/2511.00949",
        "title": "Motion-Robust Multimodal Fusion of PPG and Accelerometer Signals for Three-Class Heart Rhythm Classification",
        "authors": [
            "Yangyang Zhao",
            "Matti Kaisti",
            "Olli Lahdenoja",
            "Tero Koivisto"
        ],
        "comments": "Accepted for publication in the Companion of the 2025 ACM International Joint Conference on Pervasive and Ubiquitous Computing and the 2025 International Symposium on Wearable Computers (UbiComp/ISWC 2025 Companion). 5 pages, 3 figures. Author's accepted manuscript (AAM)",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Atrial fibrillation (AF) is a leading cause of stroke and mortality, particularly in elderly patients. Wrist-worn photoplethysmography (PPG) enables non-invasive, continuous rhythm monitoring, yet suffers from significant vulnerability to motion artifacts and physiological noise. Many existing approaches rely solely on single-channel PPG and are limited to binary AF detection, often failing to capture the broader range of arrhythmias encountered in clinical settings. We introduce RhythmiNet, a residual neural network enhanced with temporal and channel attention modules that jointly leverage PPG and accelerometer (ACC) signals. The model performs three-class rhythm classification: AF, sinus rhythm (SR), and Other. To assess robustness across varying movement conditions, test data are stratified by accelerometer-based motion intensity percentiles without excluding any segments. RhythmiNet achieved a 4.3% improvement in macro-AUC over the PPG-only baseline. In addition, performance surpassed a logistic regression model based on handcrafted HRV features by 12%, highlighting the benefit of multimodal fusion and attention-based learning in noisy, real-world clinical data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 77,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00964",
        "abs_url": "https://arxiv.org/abs/2511.00964",
        "pdf_url": "https://arxiv.org/pdf/2511.00964",
        "title": "Using Synthetic Data to estimate the True Error is theoretically and practically doable",
        "authors": [
            "Hai Hoang Thanh",
            "Duy-Tung Nguyen",
            "Hung The Tran",
            "Khoat Than"
        ],
        "comments": "To appear at Machine Learning journal and ACML",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Accurately evaluating model performance is crucial for deploying machine learning systems in real-world applications. Traditional methods often require a sufficiently large labeled test set to ensure a reliable evaluation. However, in many contexts, a large labeled dataset is costly and labor-intensive. Therefore, we sometimes have to do evaluation by a few labeled samples, which is theoretically challenging. Recent advances in generative models offer a promising alternative by enabling the synthesis of high-quality data. In this work, we make a systematic investigation about the use of synthetic data to estimate the test error of a trained model under limited labeled data conditions. To this end, we develop novel generalization bounds that take synthetic data into account. Those bounds suggest novel ways to optimize synthetic samples for evaluation and theoretically reveal the significant role of the generator's quality. Inspired by those bounds, we propose a theoretically grounded method to generate optimized synthetic data for model evaluation. Experimental results on simulation and tabular datasets demonstrate that, compared to existing baselines, our method achieves accurate and more reliable estimates of the test error.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 78,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00977",
        "abs_url": "https://arxiv.org/abs/2511.00977",
        "pdf_url": "https://arxiv.org/pdf/2511.00977",
        "title": "Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow",
        "authors": [
            "Kristiyan Sakalyan",
            "Alessandro Palma",
            "Filippo Guerranti",
            "Fabian J. Theis",
            "Stephan Günnemann"
        ],
        "comments": "37 pages, 15 figures, to appear in NeurIPS 2025",
        "subjects": "Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)",
        "abstract": "Understanding the evolution of cellular microenvironments in spatiotemporal data is essential for deciphering tissue development and disease progression. While experimental techniques like spatial transcriptomics now enable high-resolution mapping of tissue organization across space and time, current methods that model cellular evolution operate at the single-cell level, overlooking the coordinated development of cellular states in a tissue. We introduce NicheFlow, a flow-based generative model that infers the temporal trajectory of cellular microenvironments across sequential spatial slides. By representing local cell neighborhoods as point clouds, NicheFlow jointly models the evolution of cell states and spatial coordinates using optimal transport and Variational Flow Matching. Our approach successfully recovers both global spatial architecture and local microenvironment composition across diverse spatiotemporal datasets, from embryonic to brain development.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 79,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00987",
        "abs_url": "https://arxiv.org/abs/2511.00987",
        "pdf_url": "https://arxiv.org/pdf/2511.00987",
        "title": "Balanced Multimodal Learning via Mutual Information",
        "authors": [
            "Rongrong Xie",
            "Guido Sanguinetti"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Multimodal learning has increasingly become a focal point in research, primarily due to its ability to integrate complementary information from diverse modalities. Nevertheless, modality imbalance, stemming from factors such as insufficient data acquisition and disparities in data quality, has often been inadequately addressed. This issue is particularly prominent in biological data analysis, where datasets are frequently limited, costly to acquire, and inherently heterogeneous in quality. Conventional multimodal methodologies typically fall short in concurrently harnessing intermodal synergies and effectively resolving modality conflicts. In this study, we propose a novel unified framework explicitly designed to address modality imbalance by utilizing mutual information to quantify interactions between modalities. Our approach adopts a balanced multimodal learning strategy comprising two key stages: cross-modal knowledge distillation (KD) and a multitask-like training paradigm. During the cross-modal KD pretraining phase, stronger modalities are leveraged to enhance the predictive capabilities of weaker modalities. Subsequently, our primary training phase employs a multitask-like learning mechanism, dynamically calibrating gradient contributions based on modality-specific performance metrics and intermodal mutual information. This approach effectively alleviates modality imbalance, thereby significantly improving overall multimodal model performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 80,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00989",
        "abs_url": "https://arxiv.org/abs/2511.00989",
        "pdf_url": "https://arxiv.org/pdf/2511.00989",
        "title": "Hydra: Dual Exponentiated Memory for Multivariate Time Series Analysis",
        "authors": [
            "Asal Meskin",
            "Alireza Mirrokni",
            "Ali Najar",
            "Ali Behrouz"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In recent years, effectively modeling multivariate time series has gained significant popularity, mainly due to its wide range of applications, ranging from healthcare to financial markets and energy management. Transformers, MLPs, and linear models as the de facto backbones of modern time series models have shown promising results in single-variant and/or short-term forecasting. These models, however: (1) are permutation equivariant and so lack temporal inductive bias, being less expressive to capture the temporal dynamics; (2) are naturally designed for univariate setup, missing the inter-dependencies of temporal and variate dimensions; and/or (3) are inefficient for Long-term time series modeling. To overcome training and inference efficiency as well as the lack of temporal inductive bias, recently, linear Recurrent Neural Networks (RNNs) have gained attention as an alternative to Transformer-based models. These models, however, are inherently limited to a single sequence, missing inter-variate dependencies, and can propagate errors due to their additive nature. In this paper, we present Hydra, a by-design two-headed meta in-context memory module that learns how to memorize patterns at test time by prioritizing time series patterns that are more informative about the data. Hydra uses a 2-dimensional recurrence across both time and variate at each step, which is more powerful than mixing methods. Although the 2-dimensional nature of the model makes its training recurrent and non-parallelizable, we present a new 2D-chunk-wise training algorithm that approximates the actual recurrence with $\\times 10$ efficiency improvement, while maintaining the effectiveness. Our experimental results on a diverse set of tasks and datasets, including time series forecasting, classification, and anomaly detection show the superior performance of Hydra compared to state-of-the-art baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 81,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01006",
        "abs_url": "https://arxiv.org/abs/2511.01006",
        "pdf_url": "https://arxiv.org/pdf/2511.01006",
        "title": "None To Optima in Few Shots: Bayesian Optimization with MDP Priors",
        "authors": [
            "Diantong Li",
            "Kyunghyun Cho",
            "Chong Liu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Bayesian Optimization (BO) is an efficient tool for optimizing black-box functions, but its theoretical guarantees typically hold in the asymptotic regime. In many critical real-world applications such as drug discovery or materials design, where each evaluation can be very costly and time-consuming, BO becomes impractical for many evaluations. In this paper, we introduce the Procedure-inFormed BO (ProfBO) algorithm, which solves black-box optimization with remarkably few function evaluations. At the heart of our algorithmic design are Markov Decision Process (MDP) priors that model optimization trajectories from related source tasks, thereby capturing procedural knowledge on efficient optimization. We embed these MDP priors into a prior-fitted neural network and employ model-agnostic meta-learning for fast adaptation to new target tasks. Experiments on real-world Covid and Cancer benchmarks and hyperparameter tuning tasks demonstrate that ProfBO consistently outperforms state-of-the-art methods by achieving high-quality solutions with significantly fewer evaluations, making it ready for practical deployment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 82,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01009",
        "abs_url": "https://arxiv.org/abs/2511.01009",
        "pdf_url": "https://arxiv.org/pdf/2511.01009",
        "title": "Equality Graph Assisted Symbolic Regression",
        "authors": [
            "Fabricio Olivetti de Franca",
            "Gabriel Kronberger"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In Symbolic Regression (SR), Genetic Programming (GP) is a popular search algorithm that delivers state-of-the-art results in term of accuracy. Its success relies on the concept of neutrality, which induces large plateaus that the search can safely navigate to more promising regions. Navigating these plateaus, while necessary, requires the computation of redundant expressions, up to 60% of the total number of evaluation, as noted in a recent study. The equality graph (e-graph) structure can compactly store and group equivalent expressions enabling us to verify if a given expression and their variations were already visited by the search, thus enabling us to avoid unnecessary computation. We propose a new search algorithm for symbolic regression called SymRegg that revolves around the e-graph structure following simple steps: perturb solutions sampled from a selection of expressions stored in the e-graph, if it generates an unvisited expression, insert it into the e-graph and generates its equivalent forms. We show that SymRegg is capable of improving the efficiency of the search, maintaining consistently accurate results across different datasets while requiring a choice of a minimalist set of hyperparameters.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 83,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01015",
        "abs_url": "https://arxiv.org/abs/2511.01015",
        "pdf_url": "https://arxiv.org/pdf/2511.01015",
        "title": "What's the next frontier for Data-centric AI? Data Savvy Agents",
        "authors": [
            "Nabeel Seedat",
            "Jiashuo Liu",
            "Mihaela van der Schaar"
        ],
        "comments": "Presented at ICLR 2025 Data-FM. Seedat & Liu contributed equally",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The recent surge in AI agents that autonomously communicate, collaborate with humans and use diverse tools has unlocked promising opportunities in various real-world settings. However, a vital aspect remains underexplored: how agents handle data. Scalable autonomy demands agents that continuously acquire, process, and evolve their data. In this paper, we argue that data-savvy capabilities should be a top priority in the design of agentic systems to ensure reliable real-world deployment. Specifically, we propose four key capabilities to realize this vision: (1) Proactive data acquisition: enabling agents to autonomously gather task-critical knowledge or solicit human input to address data gaps; (2) Sophisticated data processing: requiring context-aware and flexible handling of diverse data challenges and inputs; (3) Interactive test data synthesis: shifting from static benchmarks to dynamically generated interactive test data for agent evaluation; and (4) Continual adaptation: empowering agents to iteratively refine their data and background knowledge to adapt to shifting environments. While current agent research predominantly emphasizes reasoning, we hope to inspire a reflection on the role of data-savvy agents as the next frontier in data-centric AI.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 84,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01017",
        "abs_url": "https://arxiv.org/abs/2511.01017",
        "pdf_url": "https://arxiv.org/pdf/2511.01017",
        "title": "SARIMAX-Based Power Outage Prediction During Extreme Weather Events",
        "authors": [
            "Haoran Ye",
            "Qiuzhuang Sun",
            "Yang Yang"
        ],
        "comments": "12 pages, 3 figures. This paper presents the solution of Team 12 for the 2025 INFORMS Data Mining Society Data Challenge. The open-source code is available at: this https URL",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "This study develops a SARIMAX-based prediction system for short-term power outage forecasting during extreme weather events. Using hourly data from Michigan counties with outage counts and comprehensive weather features, we implement a systematic two-stage feature engineering pipeline: data cleaning to remove zero-variance and unknown features, followed by correlation-based filtering to eliminate highly correlated predictors. The selected features are augmented with temporal embeddings, multi-scale lag features, and weather variables with their corresponding lags as exogenous inputs to the SARIMAX model. To address data irregularity and numerical instability, we apply standardization and implement a hierarchical fitting strategy with sequential optimization methods, automatic downgrading to ARIMA when convergence fails, and historical mean-based fallback predictions as a final safeguard. The model is optimized separately for short-term (24 hours) and medium-term (48 hours) forecast horizons using RMSE as the evaluation metric. Our approach achieves an RMSE of 177.2, representing an 8.4\\% improvement over the baseline method (RMSE = 193.4), thereby validating the effectiveness of our feature engineering and robust optimization strategy for extreme weather-related outage prediction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 85,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01054",
        "abs_url": "https://arxiv.org/abs/2511.01054",
        "pdf_url": "https://arxiv.org/pdf/2511.01054",
        "title": "MedEqualizer: A Framework Investigating Bias in Synthetic Medical Data and Mitigation via Augmentation",
        "authors": [
            "Sama Salarian",
            "Yue Zhang",
            "Swati Padhee",
            "Srinivasan Parthasarathy"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Synthetic healthcare data generation presents a viable approach to enhance data accessibility and support research by overcoming limitations associated with real-world medical datasets. However, ensuring fairness across protected attributes in synthetic data is critical to avoid biased or misleading results in clinical research and decision-making. In this study, we assess the fairness of synthetic data generated by multiple generative adversarial network (GAN)-based models using the MIMIC-III dataset, with a focus on representativeness across protected demographic attributes. We measure subgroup representation using the logarithmic disparity metric and observe significant imbalances, with many subgroups either underrepresented or overrepresented in the synthetic data, compared to the real data. To mitigate these disparities, we introduce MedEqualizer, a model-agnostic augmentation framework that enriches the underrepresented subgroups prior to synthetic data generation. Our results show that MedEqualizer significantly improves demographic balance in the resulting synthetic datasets, offering a viable path towards more equitable and representative healthcare data synthesis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 86,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01060",
        "abs_url": "https://arxiv.org/abs/2511.01060",
        "pdf_url": "https://arxiv.org/pdf/2511.01060",
        "title": "Window-Based Feature Engineering for Cognitive Workload Detection",
        "authors": [
            "Andrew Hallam",
            "R G Gayathri",
            "Glory Lee",
            "Atul Sajjanhar"
        ],
        "comments": "9 pages, 3 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Cognitive workload is a topic of increasing interest across various fields such as health, psychology, and defense applications. In this research, we focus on classifying cognitive workload using the COLET dataset, employing a window-based approach for feature generation and machine/deep learning techniques for classification. We apply window-based temporal partitioning to enhance features used in existing research, followed by machine learning and deep learning models to classify different levels of cognitive workload. The results demonstrate that deep learning models, particularly tabular architectures, outperformed traditional machine learning methods in precision, F1-score, accuracy, and classification precision. This study highlights the effectiveness of window-based temporal feature extraction and the potential of deep learning techniques for real-time cognitive workload assessment in complex and dynamic tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 87,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01069",
        "abs_url": "https://arxiv.org/abs/2511.01069",
        "pdf_url": "https://arxiv.org/pdf/2511.01069",
        "title": "Happiness as a Measure of Fairness",
        "authors": [
            "Georg Pichler",
            "Marco Romanelli",
            "Pablo Piantanida"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "In this paper, we propose a novel fairness framework grounded in the concept of happi- ness, a measure of the utility each group gains fromdecisionoutcomes. Bycapturingfairness through this intuitive lens, we not only offer a more human-centered approach, but also one that is mathematically rigorous: In order to compute the optimal, fair post-processing strategy, only a linear program needs to be solved. This makes our method both efficient and scalable with existing optimization tools. Furthermore, it unifies and extends several well-known fairness definitions, and our em- pirical results highlight its practical strengths across diverse scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 88,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01077",
        "abs_url": "https://arxiv.org/abs/2511.01077",
        "pdf_url": "https://arxiv.org/pdf/2511.01077",
        "title": "AI Progress Should Be Measured by Capability-Per-Resource, Not Scale Alone: A Framework for Gradient-Guided Resource Allocation in LLMs",
        "authors": [
            "David McCoy",
            "Yulun Wu",
            "Zachary Butzin-Dozier"
        ],
        "comments": "9 pages (main) + appendix, 3 figures. Accepted at NeurIPS 2025 (Position Paper Track), submission #491. OpenReview: this https URL",
        "subjects": "Machine Learning (cs.LG); Computation (stat.CO)",
        "abstract": "This position paper challenges the \"scaling fundamentalism\" dominating AI research, where unbounded growth in model size and computation has led to unsustainable environmental impacts and widening resource inequality. We argue that LLM development should be fundamentally reoriented toward capability-per-resource rather than capability alone. We present a theoretical framework demonstrating that resource-allocation decisions guided by gradient influence patterns can dramatically improve efficiency throughout the AI lifecycle. Our analysis shows that in transformer-based models, where a small fraction of parameters exert outsized influence (following heavy-tailed distributions), three critical insights emerge: (1) updating only high-influence parameters strictly outperforms full-parameter tuning on a performance-per-resource basis; (2) simple gradient norms provide computationally efficient proxies for identifying these high-influence components; and (3) coordinated parameter and data selection yields multiplicative efficiency gains, potentially reducing resource requirements by orders of magnitude. Building on these theoretical foundations, we propose a two stage paradigm marginal-return pretraining for foundation developers and influence guided adaptation for downstream users bridged by gradient blueprints, metadata describing which parameters matter most for various tasks. This capability-per-resource perspective transforms what were once considered pragmatic hardware workarounds into theoretically optimal strategies, democratizing access to cutting-edge AI capabilities while significantly reducing environmental impact. By embedding resource consciousness into how we develop, adapt, and evaluate models, we can reshape AI progress toward a more sustainable and equitable future.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 89,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01125",
        "abs_url": "https://arxiv.org/abs/2511.01125",
        "pdf_url": "https://arxiv.org/pdf/2511.01125",
        "title": "One model to solve them all: 2BSDE families via neural operators",
        "authors": [
            "Takashi Furuya",
            "Anastasis Kratsios",
            "Dylan Possamaï",
            "Bogdan Raonić"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Analysis of PDEs (math.AP); Numerical Analysis (math.NA); Probability (math.PR); Computational Finance (q-fin.CP)",
        "abstract": "We introduce a mild generative variant of the classical neural operator model, which leverages Kolmogorov--Arnold networks to solve infinite families of second-order backward stochastic differential equations ($2$BSDEs) on regular bounded Euclidean domains with random terminal time. Our first main result shows that the solution operator associated with a broad range of $2$BSDE families is approximable by appropriate neural operator models. We then identify a structured subclass of (infinite) families of $2$BSDEs whose neural operator approximation requires only a polynomial number of parameters in the reciprocal approximation rate, as opposed to the exponential requirement in general worst-case neural operator guarantees.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 90,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01126",
        "abs_url": "https://arxiv.org/abs/2511.01126",
        "pdf_url": "https://arxiv.org/pdf/2511.01126",
        "title": "Stochastic Regret Guarantees for Online Zeroth- and First-Order Bilevel Optimization",
        "authors": [
            "Parvin Nazari",
            "Bojian Hou",
            "Davoud Ataee Tarzanagh",
            "Li Shen",
            "George Michailidis"
        ],
        "comments": "Published at NeurIPS 2025. 88 pages and 3 figures",
        "subjects": "Machine Learning (cs.LG); Numerical Analysis (math.NA); Optimization and Control (math.OC); Statistics Theory (math.ST)",
        "abstract": "Online bilevel optimization (OBO) is a powerful framework for machine learning problems where both outer and inner objectives evolve over time, requiring dynamic updates. Current OBO approaches rely on deterministic \\textit{window-smoothed} regret minimization, which may not accurately reflect system performance when functions change rapidly. In this work, we introduce a novel search direction and show that both first- and zeroth-order (ZO) stochastic OBO algorithms leveraging this direction achieve sublinear {stochastic bilevel regret without window smoothing}. Beyond these guarantees, our framework enhances efficiency by: (i) reducing oracle dependence in hypergradient estimation, (ii) updating inner and outer variables alongside the linear system solution, and (iii) employing ZO-based estimation of Hessians, Jacobians, and gradients. Experiments on online parametric loss tuning and black-box adversarial attacks validate our approach.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 91,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01137",
        "abs_url": "https://arxiv.org/abs/2511.01137",
        "pdf_url": "https://arxiv.org/pdf/2511.01137",
        "title": "Regularization Implies balancedness in the deep linear network",
        "authors": [
            "Kathryn Lindsey",
            "Govind Menon"
        ],
        "comments": "18 pages, 3 figures",
        "subjects": "Machine Learning (cs.LG); Algebraic Geometry (math.AG); Dynamical Systems (math.DS); Machine Learning (stat.ML)",
        "abstract": "We use geometric invariant theory (GIT) to study the deep linear network (DLN). The Kempf-Ness theorem is used to establish that the $L^2$ regularizer is minimized on the balanced manifold. This allows us to decompose the training dynamics into two distinct gradient flows: a regularizing flow on fibers and a learning flow on the balanced manifold. We show that the regularizing flow is exactly solvable using the moment map. This approach provides a common mathematical framework for balancedness in deep learning and linear systems theory. We use this framework to interpret balancedness in terms of model reduction and Bayesian principles.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 92,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01185",
        "abs_url": "https://arxiv.org/abs/2511.01185",
        "pdf_url": "https://arxiv.org/pdf/2511.01185",
        "title": "A Comparative Study of Model Adaptation Strategies for Multi-Treatment Uplift Modeling",
        "authors": [
            "Ruyue Zhang",
            "Xiaopeng Ke",
            "Ming Liu",
            "Fangzhou Shi",
            "Chang Men",
            "Zhengdan Zhu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Uplift modeling has emerged as a crucial technique for individualized treatment effect estimation, particularly in fields such as marketing and healthcare. Modeling uplift effects in multi-treatment scenarios plays a key role in real-world applications. Current techniques for modeling multi-treatment uplift are typically adapted from binary-treatment works. In this paper, we investigate and categorize all current model adaptations into two types: Structure Adaptation and Feature Adaptation. Through our empirical experiments, we find that these two adaptation types cannot maintain effectiveness under various data characteristics (noisy data, mixed with observational data, etc.). To enhance estimation ability and robustness, we propose Orthogonal Function Adaptation (OFA) based on the function approximation theorem. We conduct comprehensive experiments with multiple data characteristics to study the effectiveness and robustness of all model adaptation techniques. Our experimental results demonstrate that our proposed OFA can significantly improve uplift model performance compared to other vanilla adaptation methods and exhibits the highest robustness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 93,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01190",
        "abs_url": "https://arxiv.org/abs/2511.01190",
        "pdf_url": "https://arxiv.org/pdf/2511.01190",
        "title": "Analyzing the Power of Chain of Thought through Memorization Capabilities",
        "authors": [
            "Lijia Yu",
            "Xiao-Shan Gao",
            "Lijun Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "It has been shown that the chain of thought (CoT) can enhance the power of large language models (LLMs) to solve certain mathematical reasoning problems. However, the capacity of CoT is still not fully explored. As an important instance, the following basic question has not yet been answered: Does CoT expand the capability of transformers across all reasoning tasks? We demonstrate that reasoning with transformers is essentially a memorization problem for reasoning datasets. Thus, examining the power of CoT across all reasoning tasks amounts to analyzing the memorization capabilities of CoT transformers. In this paper, we give a complete description of the memorization capabilities of fixed-precision transformers with or without CoT and give a negative answer to the above-mentioned question. Precisely, we first give necessary and sufficient conditions for fixed-precision transformers with and without CoT to memorize a finite reasoning dataset and show that these two conditions do not imply each other. Then, we give lower and upper bounds for the number of parameters needed for transformers with or without CoT to memorize a finite reasoning dataset with $N$ elements, which are $\\overline{\\Theta}(N)$ in all cases. This implies that there exist reasoning tasks for which CoT does not enhance the reasoning power of transformers, leading to a negative answer to the above-mentioned question. Finally, we give the first results on memorizing infinite reasoning datasets by CoT transformers and show that some simple infinite datasets cannot be memorized by transformers with or without CoT.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 94,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01198",
        "abs_url": "https://arxiv.org/abs/2511.01198",
        "pdf_url": "https://arxiv.org/pdf/2511.01198",
        "title": "Transmitter Identification and Protocol Categorization in Shared Spectrum via Multi-Task RF Classification at the Network Edge",
        "authors": [
            "Tariq Abdul-Quddoos",
            "Tasnia Sharmin",
            "Xiangfang Li",
            "Lijun Qian"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "As spectrum sharing becomes increasingly vital to meet rising wireless demands in the future, spectrum monitoring and transmitter identification are indispensable for enforcing spectrum usage policy, efficient spectrum utilization, and net- work security. This study proposed a robust framework for transmitter identification and protocol categorization via multi- task RF signal classification in shared spectrum environments, where the spectrum monitor will classify transmission protocols (e.g., 4G LTE, 5G-NR, IEEE 802.11a) operating within the same frequency bands, and identify different transmitting base stations, as well as their combinations. A Convolutional Neural Network (CNN) is designed to tackle critical challenges such as overlapping signal characteristics and environmental variability. The proposed method employs a multi-channel input strategy to extract meaningful signal features, achieving remarkable accuracy: 90% for protocol classification, 100% for transmitting base station classification, and 92% for joint classification tasks, utilizing RF data from the POWDER platform. These results highlight the significant potential of the proposed method to enhance spectrum monitoring, management, and security in modern wireless networks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 95,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01203",
        "abs_url": "https://arxiv.org/abs/2511.01203",
        "pdf_url": "https://arxiv.org/pdf/2511.01203",
        "title": "FEval-TTC: Fair Evaluation Protocol for Test-Time Compute",
        "authors": [
            "Pavel Rumiantsev",
            "Soumyasundar Pal",
            "Yingxue Zhang",
            "Mark Coates"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
        "abstract": "The performance of Large Language Models (LLMs) and the associated dollar costs of API calls can fluctuate over time, potentially invalidating conclusions drawn in prior research. To address this, we propose a Fair Evaluation protocol for Test-Time Compute (FEval-TTC), designed to ensure consistent assessment of test-time compute (TTC) methods, regardless of such fluctuations. FEval-TTC focuses on the evaluation of TTC methods that utilize underlying Chains-of-Thought (CoT). It supports evaluations across multiple LLMs on a diverse set of mathematical and commonsense reasoning datasets. The few-shot prompting and answer extraction processes are standardized across datasets, reducing both time and monetary overhead for researchers. Furthermore, we provide a cost modelling procedure that estimates both the token and dollar cost per query, facilitating equitable comparisons of prevalent TTC methods. We open-source FEval-TTC for public use at this https URL .",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 96,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01218",
        "abs_url": "https://arxiv.org/abs/2511.01218",
        "pdf_url": "https://arxiv.org/pdf/2511.01218",
        "title": "Optimizing Electric Vehicle Charging Station Placement Using Reinforcement Learning and Agent-Based Simulations",
        "authors": [
            "Minh-Duc Nguyen",
            "Dung D. Le",
            "Phi Long Nguyen"
        ],
        "comments": "Under Review",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The rapid growth of electric vehicles (EVs) necessitates the strategic placement of charging stations to optimize resource utilization and minimize user inconvenience. Reinforcement learning (RL) offers an innovative approach to identifying optimal charging station locations; however, existing methods face challenges due to their deterministic reward systems, which limit efficiency. Because real-world conditions are dynamic and uncertain, a deterministic reward structure cannot fully capture the complexities of charging station placement. As a result, evaluation becomes costly and time-consuming, and less reflective of real-world scenarios. To address this challenge, we propose a novel framework that integrates deep RL with agent-based simulations to model EV movement and estimate charging demand in real time. Our approach employs a hybrid RL agent with dual Q-networks to select optimal locations and configure charging ports, guided by a hybrid reward function that combines deterministic factors with simulation-derived feedback. Case studies in Hanoi, Vietnam, show that our method reduces average waiting times by 53.28% compared to the initial state, outperforming static baseline methods. This scalable and adaptive solution enhances EV infrastructure planning, effectively addressing real-world complexities and improving user experience.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 97,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01226",
        "abs_url": "https://arxiv.org/abs/2511.01226",
        "pdf_url": "https://arxiv.org/pdf/2511.01226",
        "title": "WindMiL: Equivariant Graph Learning for Wind Loading Prediction",
        "authors": [
            "Themistoklis Vargiemezis",
            "Charilaos Kanatsoulis",
            "Catherine Gorlé"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Accurate prediction of wind loading on buildings is crucial for structural safety and sustainable design, yet conventional approaches such as wind tunnel testing and large-eddy simulation (LES) are prohibitively expensive for large-scale exploration. Each LES case typically requires at least 24 hours of computation, making comprehensive parametric studies infeasible. We introduce WindMiL, a new machine learning framework that combines systematic dataset generation with symmetry-aware graph neural networks (GNNs). First, we introduce a large-scale dataset of wind loads on low-rise buildings by applying signed distance function interpolation to roof geometries and simulating 462 cases with LES across varying shapes and wind directions. Second, we develop a reflection-equivariant GNN that guarantees physically consistent predictions under mirrored geometries. Across interpolation and extrapolation evaluations, WindMiL achieves high accuracy for both the mean and the standard deviation of surface pressure coefficients (e.g., RMSE $\\leq 0.02$ for mean $C_p$) and remains accurate under reflected-test evaluation, maintaining hit rates above $96\\%$ where the non-equivariant baseline model drops by more than $10\\%$. By pairing a systematic dataset with an equivariant surrogate, WindMiL enables efficient, scalable, and accurate predictions of wind loads on buildings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 98,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01234",
        "abs_url": "https://arxiv.org/abs/2511.01234",
        "pdf_url": "https://arxiv.org/pdf/2511.01234",
        "title": "A Saddle Point Remedy: Power of Variable Elimination in Non-convex Optimization",
        "authors": [
            "Min Gan",
            "Guang-Yong Chen",
            "Yang Yi",
            "Lin Yang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "The proliferation of saddle points, rather than poor local minima, is increasingly understood to be a primary obstacle in large-scale non-convex optimization for machine learning. Variable elimination algorithms, like Variable Projection (VarPro), have long been observed to exhibit superior convergence and robustness in practice, yet a principled understanding of why they so effectively navigate these complex energy landscapes has remained elusive. In this work, we provide a rigorous geometric explanation by comparing the optimization landscapes of the original and reduced formulations. Through a rigorous analysis based on Hessian inertia and the Schur complement, we prove that variable elimination fundamentally reshapes the critical point structure of the objective function, revealing that local maxima in the reduced landscape are created from, and correspond directly to, saddle points in the original formulation. Our findings are illustrated on the canonical problem of non-convex matrix factorization, visualized directly on two-parameter neural networks, and finally validated in training deep Residual Networks, where our approach yields dramatic improvements in stability and convergence to superior minima. This work goes beyond explaining an existing method; it establishes landscape simplification via saddle point transformation as a powerful principle that can guide the design of a new generation of more robust and efficient optimization algorithms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 99,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01249",
        "abs_url": "https://arxiv.org/abs/2511.01249",
        "pdf_url": "https://arxiv.org/pdf/2511.01249",
        "title": "KAT-GNN: A Knowledge-Augmented Temporal Graph Neural Network for Risk Prediction in Electronic Health Records",
        "authors": [
            "Kun-Wei Lin",
            "Yu-Chen Kuo",
            "Hsin-Yao Wang",
            "Yi-Ju Tseng"
        ],
        "comments": "10 pages, 3 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Clinical risk prediction using electronic health records (EHRs) is vital to facilitate timely interventions and clinical decision support. However, modeling heterogeneous and irregular temporal EHR data presents significant challenges. We propose \\textbf{KAT-GNN} (Knowledge-Augmented Temporal Graph Neural Network), a graph-based framework that integrates clinical knowledge and temporal dynamics for risk prediction. KAT-GNN first constructs modality-specific patient graphs from EHRs. These graphs are then augmented using two knowledge sources: (1) ontology-driven edges derived from SNOMED CT and (2) co-occurrence priors extracted from EHRs. Subsequently, a time-aware transformer is employed to capture longitudinal dynamics from the graph-encoded patient representations. KAT-GNN is evaluated on three distinct datasets and tasks: coronary artery disease (CAD) prediction using the Chang Gung Research Database (CGRD) and in-hospital mortality prediction using the MIMIC-III and MIMIC-IV datasets. KAT-GNN achieves state-of-the-art performance in CAD prediction (AUROC: 0.9269 $\\pm$ 0.0029) and demonstrated strong results in mortality prediction in MIMIC-III (AUROC: 0.9230 $\\pm$ 0.0070) and MIMIC-IV (AUROC: 0.8849 $\\pm$ 0.0089), consistently outperforming established baselines such as GRASP and RETAIN. Ablation studies confirm that both knowledge-based augmentation and the temporal modeling component are significant contributors to performance gains. These findings demonstrate that the integration of clinical knowledge into graph representations, coupled with a time-aware attention mechanism, provides an effective and generalizable approach for risk prediction across diverse clinical tasks and datasets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 100,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01267",
        "abs_url": "https://arxiv.org/abs/2511.01267",
        "pdf_url": "https://arxiv.org/pdf/2511.01267",
        "title": "A Spatio-Temporal Online Robust Tensor Recovery Approach for Streaming Traffic Data Imputation",
        "authors": [
            "Yiyang Yang",
            "Xiejian Chi",
            "Shanxing Gao",
            "Kaidong Wang",
            "Yao Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Data quality is critical to Intelligent Transportation Systems (ITS), as complete and accurate traffic data underpin reliable decision-making in traffic control and management. Recent advances in low-rank tensor recovery algorithms have shown strong potential in capturing the inherent structure of high-dimensional traffic data and restoring degraded observations. However, traditional batch-based methods demand substantial computational and storage resources, which limits their scalability in the face of continuously expanding traffic data volumes. Moreover, recent online tensor recovery methods often suffer from severe performance degradation in complex real-world scenarios due to their insufficient exploitation of the intrinsic structural properties of traffic data. To address these challenges, we reformulate the traffic data recovery problem within a streaming framework, and propose a novel online robust tensor recovery algorithm that simultaneously leverages both the global spatio-temporal correlations and local consistency of traffic data, achieving high recovery accuracy and significantly improved computational efficiency in large-scale scenarios. Our method is capable of simultaneously handling missing and anomalous values in traffic data, and demonstrates strong adaptability across diverse missing patterns. Experimental results on three real-world traffic datasets demonstrate that the proposed approach achieves high recovery accuracy while significantly improving computational efficiency by up to three orders of magnitude compared to state-of-the-art batch-based methods. These findings highlight the potential of the proposed approach as a scalable and effective solution for traffic data quality enhancement in ITS.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 101,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01277",
        "abs_url": "https://arxiv.org/abs/2511.01277",
        "pdf_url": "https://arxiv.org/pdf/2511.01277",
        "title": "Identification of Capture Phases in Nanopore Protein Sequencing Data Using a Deep Learning Model",
        "authors": [
            "Annabelle Martin",
            "Daphne Kontogiorgos-Heintz",
            "Jeff Nivala"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)",
        "abstract": "Nanopore protein sequencing produces long, noisy ionic current traces in which key molecular phases, such as protein capture and translocation, are embedded. Capture phases mark the successful entry of a protein into the pore and serve as both a checkpoint and a signal that a channel merits further analysis. However, manual identification of capture phases is time-intensive, often requiring several days for expert reviewers to annotate the data due to the need for domain-specific interpretation of complex signal patterns. To address this, a lightweight one-dimensional convolutional neural network (1D CNN) was developed and trained to detect capture phases in down-sampled signal windows. Evaluated against CNN-LSTM (Long Short-Term Memory) hybrids, histogram-based classifiers, and other CNN variants using run-level data splits, our best model, CaptureNet-Deep, achieved an F1 score of 0.94 and precision of 93.39% on held-out test data. The model supports low-latency inference and is integrated into a dashboard for Oxford Nanopore experiments, reducing the total analysis time from several days to under thirty minutes. These results show that efficient, real-time capture detection is possible using simple, interpretable architectures and suggest a broader role for lightweight ML models in sequencing workflows.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 102,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01283",
        "abs_url": "https://arxiv.org/abs/2511.01283",
        "pdf_url": "https://arxiv.org/pdf/2511.01283",
        "title": "Lyapunov Stability Learning with Nonlinear Control via Inductive Biases",
        "authors": [
            "Yupu Lu",
            "Shijie Lin",
            "Hao Xu",
            "Zeqing Zhang",
            "Jia Pan"
        ],
        "comments": "Accepted by IEEE Robio 2025",
        "subjects": "Machine Learning (cs.LG); Robotics (cs.RO)",
        "abstract": "Finding a control Lyapunov function (CLF) in a dynamical system with a controller is an effective way to guarantee stability, which is a crucial issue in safety-concerned applications. Recently, deep learning models representing CLFs have been applied into a learner-verifier framework to identify satisfiable candidates. However, the learner treats Lyapunov conditions as complex constraints for optimisation, which is hard to achieve global convergence. It is also too complicated to implement these Lyapunov conditions for verification. To improve this framework, we treat Lyapunov conditions as inductive biases and design a neural CLF and a CLF-based controller guided by this knowledge. This design enables a stable optimisation process with limited constraints, and allows end-to-end learning of both the CLF and the controller. Our approach achieves a higher convergence rate and larger region of attraction (ROA) in learning the CLF compared to existing methods among abundant experiment cases. We also thoroughly reveal why the success rate decreases with previous methods during learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 103,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01286",
        "abs_url": "https://arxiv.org/abs/2511.01286",
        "pdf_url": "https://arxiv.org/pdf/2511.01286",
        "title": "Koopman-based Prediction of Connectivity for Flying Ad Hoc Networks",
        "authors": [
            "Sivaram Krishnan",
            "Jinho Choi",
            "Jihong Park",
            "Gregory Sherman",
            "Benjamin Campbell"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Systems and Control (eess.SY)",
        "abstract": "The application of machine learning (ML) to communication systems is expected to play a pivotal role in future artificial intelligence (AI)-based next-generation wireless networks. While most existing works focus on ML techniques for static wireless environments, they often face limitations when applied to highly dynamic environments, such as flying ad hoc networks (FANETs). This paper explores the use of data-driven Koopman approaches to address these challenges. Specifically, we investigate how these approaches can model UAV trajectory dynamics within FANETs, enabling more accurate predictions and improved network performance. By leveraging Koopman operator theory, we propose two possible approaches -- centralized and distributed -- to efficiently address the challenges posed by the constantly changing topology of FANETs. To demonstrate this, we consider a FANET performing surveillance with UAVs following pre-determined trajectories and predict signal-to-interference-plus-noise ratios (SINRs) to ensure reliable communication between UAVs. Our results show that these approaches can accurately predict connectivity and isolation events that lead to modelled communication outages. This capability could help UAVs schedule their transmissions based on these predictions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 104,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01296",
        "abs_url": "https://arxiv.org/abs/2511.01296",
        "pdf_url": "https://arxiv.org/pdf/2511.01296",
        "title": "LSHFed: Robust and Communication-Efficient Federated Learning with Locally-Sensitive Hashing Gradient Mapping",
        "authors": [
            "Guanjie Cheng",
            "Mengzhen Yang",
            "Xinkui Zhao",
            "Shuyi Yu",
            "Tianyu Du",
            "Yangyang Wu",
            "Mengying Zhu",
            "Shuiguang Deng"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Federated learning (FL) enables collaborative model training across distributed nodes without exposing raw data, but its decentralized nature makes it vulnerable in trust-deficient environments. Inference attacks may recover sensitive information from gradient updates, while poisoning attacks can degrade model performance or induce malicious behaviors. Existing defenses often suffer from high communication and computation costs, or limited detection precision. To address these issues, we propose LSHFed, a robust and communication-efficient FL framework that simultaneously enhances aggregation robustness and privacy preservation. At its core, LSHFed incorporates LSHGM, a novel gradient verification mechanism that projects high-dimensional gradients into compact binary representations via multi-hyperplane locally-sensitive hashing. This enables accurate detection and filtering of malicious gradients using only their irreversible hash forms, thus mitigating privacy leakage risks and substantially reducing transmission overhead. Extensive experiments demonstrate that LSHFed maintains high model performance even when up to 50% of participants are collusive adversaries while achieving up to a 1000x reduction in gradient verification communication compared to full-gradient methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 105,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01343",
        "abs_url": "https://arxiv.org/abs/2511.01343",
        "pdf_url": "https://arxiv.org/pdf/2511.01343",
        "title": "Diffusion-Based Solver for CNF Placement on the Cloud-Continuum",
        "authors": [
            "Álvaro Vázquez Rodríguez",
            "Manuel Fernández-Veiga",
            "Carlos Giraldo-Rodríguez"
        ],
        "comments": "7 pages, 7 figures. Presented at PE-WASUN'25 (IEEE International Symposium on Performance Evaluation of Wireless Ad Hoc, Sensor, and Ubiquitous Networks)",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The placement of Cloud-Native Network Functions (CNFs) across the Cloud-Continuum represents a core challenge in the orchestration of current 5G and future 6G networks. The process involves the placement of interdependent computing tasks, structured as Service Function Chains, over distributed cloud infrastructures. This is achieved while satisfying strict resource, bandwidth and latency constraints. It is acknowledged that classical approaches, including mixed-integer nonlinear programming, heuristics and reinforcement learning are limited in terms of scalability, constraint handling and generalisation capacity. In the present study, a novel theoretical framework is proposed, which is based on Denoising Diffusion Probabilistic Models (DDPM) for CNF placement. The present approach proposes a reconceptualisation of placement as a generative graph to assignment task, where the placement problem is encoded as a heterogeneous graph, and a Graph Neural Network denoiser is trained to iteratively refine noisy CNF-to-cloud assignment matrices. The model incorporates constraint-specific losses directly into the loss function, thereby allowing it to learn feasible solution spaces. The integration of the DDPM formulation with structured combinatorial constraints is achieved through a rigorous and systematic approach. Extensive evaluations across diverse topologies have been conducted, which have confirmed that the model consistently produces feasible solutions with orders of magnitude faster inference than MINLP solvers. The results obtained demonstrate the potential of diffusion-based generative modelling for constrained network embedding problems, making an impact towards the practical, scalable orchestration of distributed Cloud-Native Network Functions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 106,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01352",
        "abs_url": "https://arxiv.org/abs/2511.01352",
        "pdf_url": "https://arxiv.org/pdf/2511.01352",
        "title": "MiniFool - Physics-Constraint-Aware Minimizer-Based Adversarial Attacks in Deep Neural Networks",
        "authors": [
            "Lucie Flek",
            "Oliver Janik",
            "Philipp Alexander Jung",
            "Akbar Karimi",
            "Timo Saala",
            "Alexander Schmidt",
            "Matthias Schott",
            "Philipp Soldin",
            "Matthias Thiesmeyer",
            "Christopher Wiebusch",
            "Ulrich Willemsen"
        ],
        "comments": "Submitted to Computing and Software for Big Science",
        "subjects": "Machine Learning (cs.LG); High Energy Astrophysical Phenomena (astro-ph.HE); Instrumentation and Methods for Astrophysics (astro-ph.IM); High Energy Physics - Experiment (hep-ex); Data Analysis, Statistics and Probability (physics.data-an)",
        "abstract": "In this paper, we present a new algorithm, MiniFool, that implements physics-inspired adversarial attacks for testing neural network-based classification tasks in particle and astroparticle physics. While we initially developed the algorithm for the search for astrophysical tau neutrinos with the IceCube Neutrino Observatory, we apply it to further data from other science domains, thus demonstrating its general applicability. Here, we apply the algorithm to the well-known MNIST data set and furthermore, to Open Data data from the CMS experiment at the Large Hadron Collider. The algorithm is based on minimizing a cost function that combines a $\\chi^2$ based test-statistic with the deviation from the desired target score. The test statistic quantifies the probability of the perturbations applied to the data based on the experimental uncertainties. For our studied use cases, we find that the likelihood of a flipped classification differs for both the initially correctly and incorrectly classified events. When testing changes of the classifications as a function of an attack parameter that scales the experimental uncertainties, the robustness of the network decision can be quantified. Furthermore, this allows testing the robustness of the classification of unlabeled experimental data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 107,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01356",
        "abs_url": "https://arxiv.org/abs/2511.01356",
        "pdf_url": "https://arxiv.org/pdf/2511.01356",
        "title": "Verifiable Split Learning via zk-SNARKs",
        "authors": [
            "Rana Alaa",
            "Darío González-Ferreiro",
            "Carlos Beis-Penedo",
            "Manuel Fernández-Veiga",
            "Rebeca P. Díaz-Redondo",
            "Ana Fernández-Vilas"
        ],
        "comments": "Submitted to CAI'26 (IEEE Conference on Artificial Intelligence 2026)",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Split learning is an approach to collaborative learning in which a deep neural network is divided into two parts: client-side and server-side at a cut layer. The client side executes its model using its raw input data and sends the intermediate activation to the server side. This configuration architecture is very useful for enabling collaborative training when data or resources are separated between devices. However, split learning lacks the ability to verify the correctness and honesty of the computations that are performed and exchanged between the parties. To this purpose, this paper proposes a verifiable split learning framework that integrates a zk-SNARK proof to ensure correctness and verifiability. The zk-SNARK proof and verification are generated for both sides in forward propagation and backward propagation on the server side, guaranteeing verifiability on both sides. The verifiable split learning architecture is compared to a blockchain-enabled system for the same deep learning network, one that records updates but without generating the zero-knowledge proof. From the comparison, it can be deduced that applying the zk-SNARK test achieves verifiability and correctness, while blockchains are lightweight but unverifiable.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 108,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01374",
        "abs_url": "https://arxiv.org/abs/2511.01374",
        "pdf_url": "https://arxiv.org/pdf/2511.01374",
        "title": "Learning Intractable Multimodal Policies with Reparameterization and Diversity Regularization",
        "authors": [
            "Ziqi Wang",
            "Jiashun Liu",
            "Ling Pan"
        ],
        "comments": "NeurIPS 2025",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Traditional continuous deep reinforcement learning (RL) algorithms employ deterministic or unimodal Gaussian actors, which cannot express complex multimodal decision distributions. This limitation can hinder their performance in diversity-critical scenarios. There have been some attempts to design online multimodal RL algorithms based on diffusion or amortized actors. However, these actors are intractable, making existing methods struggle with balancing performance, decision diversity, and efficiency simultaneously. To overcome this challenge, we first reformulate existing intractable multimodal actors within a unified framework, and prove that they can be directly optimized by policy gradient via reparameterization. Then, we propose a distance-based diversity regularization that does not explicitly require decision probabilities. We identify two diversity-critical domains, namely multi-goal achieving and generative RL, to demonstrate the advantages of multimodal policies and our method, particularly in terms of few-shot robustness. In conventional MuJoCo benchmarks, our algorithm also shows competitive performance. Moreover, our experiments highlight that the amortized actor is a promising policy model class with strong multimodal expressivity and high performance. Our code is available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 109,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01377",
        "abs_url": "https://arxiv.org/abs/2511.01377",
        "pdf_url": "https://arxiv.org/pdf/2511.01377",
        "title": "Protecting the Neural Networks against FGSM Attack Using Machine Unlearning",
        "authors": [
            "Amir Hossein Khorasani",
            "Ali Jahanian",
            "Maryam Rastgarpour"
        ],
        "comments": "7 pages, 9 figures, 1 table",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Machine learning is a powerful tool for building predictive models. However, it is vulnerable to adversarial attacks. Fast Gradient Sign Method (FGSM) attacks are a common type of adversarial attack that adds small perturbations to input data to trick a model into misclassifying it. In response to these attacks, researchers have developed methods for \"unlearning\" these attacks, which involves retraining a model on the original data without the added perturbations. Machine unlearning is a technique that tries to \"forget\" specific data points from the training dataset, to improve the robustness of a machine learning model against adversarial attacks like FGSM. In this paper, we focus on applying unlearning techniques to the LeNet neural network, a popular architecture for image classification. We evaluate the efficacy of unlearning FGSM attacks on the LeNet network and find that it can significantly improve its robustness against these types of attacks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 110,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01385",
        "abs_url": "https://arxiv.org/abs/2511.01385",
        "pdf_url": "https://arxiv.org/pdf/2511.01385",
        "title": "Memory-Efficient Training with In-Place FFT Implementation",
        "authors": [
            "Xinyu Ding",
            "Bangtian Liu",
            "Siyu Liao",
            "Zhongfeng Wang"
        ],
        "comments": "Accepted at NeurIPS 2025. Presents a real-domain in-place FFT (rdFFT) operator for memory-efficient fine-tuning of large language models",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Fast Fourier Transforms (FFT) are widely used to reduce memory and computational costs in deep learning. However, existing implementations, including standard FFT and real FFT (rFFT), cannot achieve true in-place computation. In particular, rFFT maps an input of size n to a complex output of size n/2+1, causing dimensional mismatch and requiring additional memory allocation. We propose the first real-domain, fully in-place FFT framework (rdFFT) that preserves input-output memory space consistency. By leveraging butterfly operation symmetry and conjugate properties in the frequency domain, we design an implicit complex encoding scheme that eliminates intermediate cache usage entirely. Experiments on multiple natural language understanding tasks demonstrate the method effectiveness in reducing training memory cost, offering a promising direction for frequency-domain lightweight adaptation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 111,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01408",
        "abs_url": "https://arxiv.org/abs/2511.01408",
        "pdf_url": "https://arxiv.org/pdf/2511.01408",
        "title": "Leveraging Compact Satellite Embeddings and Graph Neural Networks for Large-Scale Poverty Mapping",
        "authors": [
            "Markus B. Pettersson",
            "Adel Daoud"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Accurate, fine-grained poverty maps remain scarce across much of the Global South. While Demographic and Health Surveys (DHS) provide high-quality socioeconomic data, their spatial coverage is limited and reported coordinates are randomly displaced for privacy, further reducing their quality. We propose a graph-based approach leveraging low-dimensional AlphaEarth satellite embeddings to predict cluster-level wealth indices across Sub-Saharan Africa. By modeling spatial relations between surveyed and unlabeled locations, and by introducing a probabilistic \"fuzzy label\" loss to account for coordinate displacement, we improve the generalization of wealth predictions beyond existing surveys. Our experiments on 37 DHS datasets (2017-2023) show that incorporating graph structure slightly improves accuracy compared to \"image-only\" baselines, demonstrating the potential of compact EO embeddings for large-scale socioeconomic mapping.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 112,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01433",
        "abs_url": "https://arxiv.org/abs/2511.01433",
        "pdf_url": "https://arxiv.org/pdf/2511.01433",
        "title": "CG-FKAN: Compressed-Grid Federated Kolmogorov-Arnold Networks for Communication Constrained Environment",
        "authors": [
            "Seunghun Yu",
            "Youngjoon Lee",
            "Jinu Gong",
            "Joonhyuk Kang"
        ],
        "comments": "5 pages",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Federated learning (FL), widely used in privacy-critical applications, suffers from limited interpretability, whereas Kolmogorov-Arnold Networks (KAN) address this limitation via learnable spline functions. However, existing FL studies applying KAN overlook the communication overhead introduced by grid extension, which is essential for modeling complex functions. In this letter, we propose CG-FKAN, which compresses extended grids by sparsifying and transmitting only essential coefficients under a communication budget. Experiments show that CG-FKAN achieves up to 13.6% lower RMSE than fixed-grid KAN in communication-constrained settings. In addition, we derive a theoretical upper bound on its approximation error.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 113,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01438",
        "abs_url": "https://arxiv.org/abs/2511.01438",
        "pdf_url": "https://arxiv.org/pdf/2511.01438",
        "title": "The Curvature Rate λ: A Scalar Measure of Input-Space Sharpness in Neural Networks",
        "authors": [
            "Jacob Poschl"
        ],
        "comments": "14 pages",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Curvature influences generalization, robustness, and how reliably neural networks respond to small input perturbations. Existing sharpness metrics are typically defined in parameter space (e.g., Hessian eigenvalues) and can be expensive, sensitive to reparameterization, and difficult to interpret in functional terms. We introduce a scalar curvature measure defined directly in input space: the curvature rate {\\lambda}, given by the exponential growth rate of higher-order input derivatives. Empirically, {\\lambda} is estimated as the slope of log ||D^n f|| versus n for small n. This growth-rate perspective unifies classical analytic quantities: for analytic functions, {\\lambda} corresponds to the inverse radius of convergence, and for bandlimited signals, it reflects the spectral cutoff. The same principle extends to neural networks, where {\\lambda} tracks the emergence of high-frequency structure in the decision boundary. Experiments on analytic functions and neural networks (Two Moons and MNIST) show that {\\lambda} evolves predictably during training and can be directly shaped using a simple derivative-based regularizer, Curvature Rate Regularization (CRR). Compared to Sharpness-Aware Minimization (SAM), CRR achieves similar accuracy while yielding flatter input-space geometry and improved confidence calibration. By grounding curvature in differentiation dynamics, {\\lambda} provides a compact, interpretable, and parameterization-invariant descriptor of functional smoothness in learned models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 114,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01443",
        "abs_url": "https://arxiv.org/abs/2511.01443",
        "pdf_url": "https://arxiv.org/pdf/2511.01443",
        "title": "Efficient Curvature-aware Graph Network",
        "authors": [
            "Chaoqun Fei",
            "Tinglve Zhou",
            "Tianyong Hao",
            "Yangyang Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Graph curvature provides geometric priors for Graph Neural Networks (GNNs), enhancing their ability to model complex graph structures, particularly in terms of structural awareness, robustness, and theoretical interpretability. Among existing methods, Ollivier-Ricci curvature has been extensively studied due to its strong geometric interpretability, effectively characterizing the local geometric distribution between nodes. However, its prohibitively high computational complexity limits its applicability to large-scale graph datasets. To address this challenge, we propose a novel graph curvature measure--Effective Resistance Curvature--which quantifies the ease of message passing along graph edges using the effective resistance between node pairs, instead of the optimal transport distance. This method significantly outperforms Ollivier-Ricci curvature in computational efficiency while preserving comparable geometric expressiveness. Theoretically, we prove the low computational complexity of effective resistance curvature and establish its substitutability for Ollivier-Ricci curvature. Furthermore, extensive experiments on diverse GNN tasks demonstrate that our method achieves competitive performance with Ollivier-Ricci curvature while drastically reducing computational overhead.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 115,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01553",
        "abs_url": "https://arxiv.org/abs/2511.01553",
        "pdf_url": "https://arxiv.org/pdf/2511.01553",
        "title": "Real-time Continual Learning on Intel Loihi 2",
        "authors": [
            "Elvin Hajizada",
            "Danielle Rager",
            "Timothy Shea",
            "Leobardo Campos-Macias",
            "Andreas Wild",
            "Eyke Hüllermeier",
            "Yulia Sandamirskaya",
            "Mike Davies"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC); Neural and Evolutionary Computing (cs.NE)",
        "abstract": "AI systems on edge devices face a critical challenge in open-world environments: adapting when data distributions shift and novel classes emerge. While offline training dominates current paradigms, online continual learning (OCL)--where models learn incrementally from non-stationary streams without catastrophic forgetting--remains challenging in power-constrained settings. We present a neuromorphic solution called CLP-SNN: a spiking neural network architecture for Continually Learning Prototypes and its implementation on Intel's Loihi 2 chip. Our approach introduces three innovations: (1) event-driven and spatiotemporally sparse local learning, (2) a self-normalizing three-factor learning rule maintaining weight normalization, and (3) integrated neurogenesis and metaplasticity for capacity expansion and forgetting mitigation. On OpenLORIS few-shot learning experiments, CLP-SNN achieves accuracy competitive with replay methods while being rehearsal-free. CLP-SNN delivers transformative efficiency gains: 70\\times faster (0.33ms vs 23.2ms), and 5,600\\times more energy efficient (0.05mJ vs 281mJ) than the best alternative OCL on edge GPU. This demonstrates that co-designed brain-inspired algorithms and neuromorphic hardware can break traditional accuracy-efficiency trade-offs for future edge AI systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 116,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01570",
        "abs_url": "https://arxiv.org/abs/2511.01570",
        "pdf_url": "https://arxiv.org/pdf/2511.01570",
        "title": "Gated Fusion Enhanced Multi-Scale Hierarchical Graph Convolutional Network for Stock Movement Prediction",
        "authors": [
            "Xiaosha Xue",
            "Peibo Duan",
            "Zhipeng Liu",
            "Qi Chu",
            "Changsheng Zhang",
            "Bin zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Accurately predicting stock market movements remains a formidable challenge due to the inherent volatility and complex interdependencies among stocks. Although multi-scale Graph Neural Networks (GNNs) hold potential for modeling these relationships, they frequently neglect two key points: the subtle intra-attribute patterns within each stock affecting inter-stock correlation, and the biased attention to coarse- and fine-grained features during multi-scale sampling. To overcome these challenges, we introduce MS-HGFN (Multi-Scale Hierarchical Graph Fusion Network). The model features a hierarchical GNN module that forms dynamic graphs by learning patterns from intra-attributes and features from inter-attributes over different time scales, thus comprehensively capturing spatio-temporal dependencies. Additionally, a top-down gating approach facilitates the integration of multi-scale spatio-temporal features, preserving critical coarse- and fine-grained features without too much interference. Experiments utilizing real-world datasets from U.S. and Chinese stock markets demonstrate that MS-HGFN outperforms both traditional and advanced models, yielding up to a 1.4% improvement in prediction accuracy and enhanced stability in return simulations. The code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 117,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01588",
        "abs_url": "https://arxiv.org/abs/2511.01588",
        "pdf_url": "https://arxiv.org/pdf/2511.01588",
        "title": "Explore More, Learn Better: Parallel MLLM Embeddings under Mutual Information Minimization",
        "authors": [
            "Zhicheng Wang",
            "Chen Ju",
            "Xu Chen",
            "Shuai Xiao",
            "Jinsong Lan",
            "Xiaoyong Zhu",
            "Ying Chen",
            "Zhiguo Cao"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Embedding models are a cornerstone of modern AI. Driven by Multimodal Large Language Models (MLLMs), they have made great progress in architecture and data curation, while the holistic paradigm is still limited to SSC, i.e., single input, singular embedding, contrastive supervision, which collapses rich, multifaceted inputs into monolithic embeddings and fails to fully exploit MLLM capabilities. In this paper, we tailor one Parallel Decoupling Framework (PDF) for multimodal embedding learning, by utilizing the proprietary steerability of MLLMs, i.e., their ability to flexibly generate quite differentiated response under explicit instructions. Concretely, PDF conditions a shared MLLM backbone on distinct, learnable prefixes to roll out multiple parallel paths for one input, then relies on these paths to obtain parallel embeddings. To promote full parallel diversity, we employ Mutual Information Minimization (MIM) as an explicit constraint, coupled with per-path contrastive supervision to maintain semantic alignment. Such dual-objectives force PDF to yield robust semantic coverage and a generalizable embedding space. Ultimately, the remarkable embedding space are accessible at inference via one single forward pass, incurring negligible computational overhead. We instantiate PDF on multiple MLLM backbones and prove its effectiveness on MMEB benchmark. Significant gains are consistently achieved across various resolutions and model sizes, e.g., boosting the VLM2Vec-LLaVA-1.6-LR model by a remarkable +8.9% (7B), while the VLM2Vec-Qwen2VL models by +4.2% (2B) and +3.1% (7B). In terms of efficiency, our 2B model surpasses its baseline by +2.6% using only half the computational budget.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 118,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01592",
        "abs_url": "https://arxiv.org/abs/2511.01592",
        "pdf_url": "https://arxiv.org/pdf/2511.01592",
        "title": "Defining Energy Indicators for Impact Identification on Aerospace Composites: A Physics-Informed Machine Learning Perspective",
        "authors": [
            "Natália Ribeiro Marinho",
            "Richard Loendersloot",
            "Frank Grooteman",
            "Jan Willem Wiegman",
            "Uraz Odyurt",
            "Tiedo Tinga"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Applied Physics (physics.app-ph)",
        "abstract": "Energy estimation is critical to impact identification on aerospace composites, where low-velocity impacts can induce internal damage that is undetectable at the surface. Current methodologies for energy prediction are often constrained by data sparsity, signal noise, complex feature interdependencies, non-linear dynamics, massive design spaces, and the ill-posed nature of the inverse problem. This study introduces a physics-informed framework that embeds domain knowledge into machine learning through a dedicated input space. The approach combines observational biases, which guide the design of physics-motivated features, with targeted feature selection to retain only the most informative indicators. Features are extracted from time, frequency, and time-frequency domains to capture complementary aspects of the structural response. A structured feature selection process integrating statistical significance, correlation filtering, dimensionality reduction, and noise robustness ensures physical relevance and interpretability. Exploratory data analysis further reveals domain-specific trends, yielding a reduced feature set that captures essential dynamic phenomena such as amplitude scaling, spectral redistribution, and transient signal behaviour. Together, these steps produce a compact set of energy-sensitive indicators with both statistical robustness and physical significance, resulting in impact energy predictions that remain interpretable and traceable to measurable structural responses. Using this optimised input space, a fully-connected neural network is trained and validated with experimental data from multiple impact scenarios, including pristine and damaged states. The resulting model demonstrates significantly improved impact energy prediction accuracy, reducing errors by a factor of three compared to conventional time-series techniques and purely data-driven models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 119,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01605",
        "abs_url": "https://arxiv.org/abs/2511.01605",
        "pdf_url": "https://arxiv.org/pdf/2511.01605",
        "title": "Estimation of Toeplitz Covariance Matrices using Overparameterized Gradient Descent",
        "authors": [
            "Daniel Busbib",
            "Ami Wiesel"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "We consider covariance estimation under Toeplitz structure. Numerous sophisticated optimization methods have been developed to maximize the Gaussian log-likelihood under Toeplitz constraints. In contrast, recent advances in deep learning demonstrate the surprising power of simple gradient descent (GD) applied to overparameterized models. Motivated by this trend, we revisit Toeplitz covariance estimation through the lens of overparameterized GD. We model the $P\\times P$ covariance as a sum of $K$ complex sinusoids with learnable parameters and optimize them via GD. We show that when $K = P$, GD may converge to suboptimal solutions. However, mild overparameterization ($K = 2P$ or $4P$) consistently enables global convergence from random initializations. We further propose an accelerated GD variant with separate learning rates for amplitudes and frequencies. When frequencies are fixed and only amplitudes are optimized, we prove that the optimization landscape is asymptotically benign and any stationary point recovers the true covariance. Finally, numerical experiments demonstrate that overparameterized GD can match or exceed the accuracy of state-of-the-art methods in challenging settings, while remaining simple and scalable.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 120,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01641",
        "abs_url": "https://arxiv.org/abs/2511.01641",
        "pdf_url": "https://arxiv.org/pdf/2511.01641",
        "title": "Cross-Treatment Effect Estimation for Multi-Category, Multi-Valued Causal Inference via Dynamic Neural Masking",
        "authors": [
            "Xiaopeng Ke",
            "Yihan Yu",
            "Ruyue Zhang",
            "Zhishuo Zhou",
            "Fangzhou Shi",
            "Chang Men",
            "Zhengdan Zhu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Counterfactual causal inference faces significant challenges when extended to multi-category, multi-valued treatments, where complex cross-effects between heterogeneous interventions are difficult to model. Existing methodologies remain constrained to binary or single-type treatments and suffer from restrictive assumptions, limited scalability, and inadequate evaluation frameworks for complex intervention scenarios. We present XTNet, a novel network architecture for multi-category, multi-valued treatment effect estimation. Our approach introduces a cross-effect estimation module with dynamic masking mechanisms to capture treatment interactions without restrictive structural assumptions. The architecture employs a decomposition strategy separating basic effects from cross-treatment interactions, enabling efficient modeling of combinatorial treatment spaces. We also propose MCMV-AUCC, a suitable evaluation metric that accounts for treatment costs and interaction effects. Extensive experiments on synthetic and real-world datasets demonstrate that XTNet consistently outperforms state-of-the-art baselines in both ranking accuracy and effect estimation quality. The results of the real-world A/B test further confirm its effectiveness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 121,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01695",
        "abs_url": "https://arxiv.org/abs/2511.01695",
        "pdf_url": "https://arxiv.org/pdf/2511.01695",
        "title": "Collaborative Large Language Model Inference via Resource-Aware Parallel Speculative Decoding",
        "authors": [
            "Jungyeon Koh",
            "Hyun Jong Yang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Signal Processing (eess.SP)",
        "abstract": "The growing demand for on-device large language model (LLM) inference highlights the need for efficient mobile edge computing (MEC) solutions, especially in resource-constrained settings. Speculative decoding offers a promising solution by partitioning token generation between a lightweight draft model on mobile devices and a powerful target model on edge servers, but suffers from communication overhead and asynchronous delays. This paper is the first to propose a unified framework that jointly optimizes user association and resource allocation (UARA) to support efficient parallel speculative decoding. We solve the UARA problem using a multi-agent deep reinforcement learning algorithm. To evaluate our approach under realistic conditions, we conduct experiments using the Sionna simulator. Results show that our method achieves up to 28.0% and an average of 23.7% reduction in end-to-end latency without compromising inference accuracy, enabling scalable and low-latency LLM services in MEC systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 122,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01737",
        "abs_url": "https://arxiv.org/abs/2511.01737",
        "pdf_url": "https://arxiv.org/pdf/2511.01737",
        "title": "Edge AI in Highly Volatile Environments: Is Fairness Worth the Accuracy Trade-off?",
        "authors": [
            "Obaidullah Zaland",
            "Feras M. Awaysheh",
            "Sawsan Al Zubi",
            "Abdul Rahman Safi",
            "Monowar Bhuyan"
        ],
        "comments": "Presented at IEEE FLTA 2025",
        "subjects": "Machine Learning (cs.LG); Distributed, Parallel, and Cluster Computing (cs.DC)",
        "abstract": "Federated learning (FL) has emerged as a transformative paradigm for edge intelligence, enabling collaborative model training while preserving data privacy across distributed personal devices. However, the inherent volatility of edge environments, characterized by dynamic resource availability and heterogeneous client capabilities, poses significant challenges for achieving high accuracy and fairness in client participation. This paper investigates the fundamental trade-off between model accuracy and fairness in highly volatile edge environments. This paper provides an extensive empirical evaluation of fairness-based client selection algorithms such as RBFF and RBCSF against random and greedy client selection regarding fairness, model performance, and time, in three benchmarking datasets (CIFAR10, FashionMNIST, and EMNIST). This work aims to shed light on the fairness-performance and fairness-speed trade-offs in a volatile edge environment and explore potential future research opportunities to address existing pitfalls in \\textit{fair client selection} strategies in FL. Our results indicate that more equitable client selection algorithms, while providing a marginally better opportunity among clients, can result in slower global training in volatile environments\\footnote{The code for our experiments can be found at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 123,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01740",
        "abs_url": "https://arxiv.org/abs/2511.01740",
        "pdf_url": "https://arxiv.org/pdf/2511.01740",
        "title": "Game-theoretic distributed learning of generative models for heterogeneous data collections",
        "authors": [
            "Dmitrij Schlesinger",
            "Boris Flach"
        ],
        "comments": "The manuscript is accepted for publishing at the 2025 Symposium on Federated Learning and Intelligent Computing Systems (FLICS 2025)",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "One of the main challenges in distributed learning arises from the difficulty of handling heterogeneous local models and data. In light of the recent success of generative models, we propose to meet this challenge by building on the idea of exchanging synthetic data instead of sharing model parameters. Local models can then be treated as ``black boxes'' with the ability to learn their parameters from data and to generate data according to these parameters. Moreover, if the local models admit semi-supervised learning, we can extend the approach by enabling local models on different probability spaces. This allows to handle heterogeneous data with different modalities. We formulate the learning of the local models as a cooperative game starting from the principles of game theory. We prove the existence of a unique Nash equilibrium for exponential family local models and show that the proposed learning approach converges to this equilibrium. We demonstrate the advantages of our approach on standard benchmark vision datasets for image classification and conditional generation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 124,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01741",
        "abs_url": "https://arxiv.org/abs/2511.01741",
        "pdf_url": "https://arxiv.org/pdf/2511.01741",
        "title": "HyperNQ: A Hypergraph Neural Network Decoder for Quantum LDPC Codes",
        "authors": [
            "Ameya S. Bhave",
            "Navnil Choudhury",
            "Kanad Basu"
        ],
        "comments": "6 pages, 4 figures, Submitted to the IEEE International Conference on Communications (ICC 2026). Preprint version",
        "subjects": "Machine Learning (cs.LG); Information Theory (cs.IT); Quantum Physics (quant-ph)",
        "abstract": "Quantum computing requires effective error correction strategies to mitigate noise and decoherence. Quantum Low-Density Parity-Check (QLDPC) codes have emerged as a promising solution for scalable Quantum Error Correction (QEC) applications by supporting constant-rate encoding and a sparse parity-check structure. However, decoding QLDPC codes via traditional approaches such as Belief Propagation (BP) suffers from poor convergence in the presence of short cycles. Machine learning techniques like Graph Neural Networks (GNNs) utilize learned message passing over their node features; however, they are restricted to pairwise interactions on Tanner graphs, which limits their ability to capture higher-order correlations. In this work, we propose HyperNQ, the first Hypergraph Neural Network (HGNN)- based QLDPC decoder that captures higher-order stabilizer constraints by utilizing hyperedges-thus enabling highly expressive and compact decoding. We use a two-stage message passing scheme and evaluate the decoder over the pseudo-threshold region. Below the pseudo-threshold mark, HyperNQ improves the Logical Error Rate (LER) up to 84% over BP and 50% over GNN-based strategies, demonstrating enhanced performance over the existing state-of-the-art decoders.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 125,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01745",
        "abs_url": "https://arxiv.org/abs/2511.01745",
        "pdf_url": "https://arxiv.org/pdf/2511.01745",
        "title": "An Open-Access Benchmark of Statistical and Machine-Learning Anomaly Detection Methods for Battery Applications",
        "authors": [
            "Mei-Chin Pang",
            "Suraj Adhikari",
            "Takuma Kasahara",
            "Nagihiro Haba",
            "Saneyuki Ohno"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Methodology (stat.ME)",
        "abstract": "Battery safety is critical in applications ranging from consumer electronics to electric vehicles and aircraft, where undetected anomalies could trigger safety hazards or costly downtime. In this study, we present OSBAD as an open-source benchmark for anomaly detection frameworks in battery applications. By benchmarking 15 diverse algorithms encompassing statistical, distance-based, and unsupervised machine-learning methods, OSBAD enables a systematic comparison of anomaly detection methods across heterogeneous datasets. In addition, we demonstrate how a physics- and statistics-informed feature transformation workflow enhances anomaly separability by decomposing collective anomalies into point anomalies. To address a major bottleneck in unsupervised anomaly detection due to incomplete labels, we propose a Bayesian optimization pipeline that facilitates automated hyperparameter tuning based on transfer-learning and regression proxies. Through validation on datasets covering both liquid and solid-state chemistries, we further demonstrate the cross-chemistry generalization capability of OSBAD to identify irregularities across different electrochemical systems. By making benchmarking database with open-source reproducible anomaly detection workflows available to the community, OSBAD establishes a unified foundation for developing safe, scalable, and transferable anomaly detection tools in battery analytics. This research underscores the significance of physics- and statistics-informed feature engineering as well as model selection with probabilistic hyperparameter tuning, in advancing trustworthy, data-driven diagnostics for safety-critical energy systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 126,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01794",
        "abs_url": "https://arxiv.org/abs/2511.01794",
        "pdf_url": "https://arxiv.org/pdf/2511.01794",
        "title": "Random Initialization of Gated Sparse Adapters",
        "authors": [
            "Vi Retault",
            "Yohaï-Eliel Berreby"
        ],
        "comments": "13 pages (8 main), 6 figures (4 main). Accepted by NewInML workshop @ ICML 2025 on June 27, 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "When fine-tuning language models on new tasks, catastrophic forgetting -- performance degradation on previously-learned tasks -- is a ubiquitous problem. While Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA address this through low-rank adapters, sparse adaptation offers an alternative that doesn't impose rank constraints. We introduce Random Initialization of Gated Sparse Adapters (RIGSA), which starts from randomly-initialized full-rank adapters, gates them with a ReZero analog, and sparsifies them with iterative magnitude pruning. We evaluate RIGSA on SmolLM2-1.7B-Instruct using a novel vision-in-text task (Textual MNIST) and measure forgetting on PIQA, HellaSwag, and GSM8k. SmolLM2-1.7B-Instruct initially performs around chance level on Textual MNIST, and is capable of learning the task through RIGSA, 4-bit QLoRA and random masking. In spite of having more trainable parameters than QLoRA, the RIGSA configurations that we studied displayed less forgetting than QLoRA, particularly on GSM8k, though it performs comparably to random masking.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 127,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01800",
        "abs_url": "https://arxiv.org/abs/2511.01800",
        "pdf_url": "https://arxiv.org/pdf/2511.01800",
        "title": "Bayesian Coreset Optimization for Personalized Federated Learning",
        "authors": [
            "Prateek Chanda",
            "Shrey Modi",
            "Ganesh Ramakrishnan"
        ],
        "comments": "9 pages, 5 figures, ICLR 2024",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In a distributed machine learning setting like Federated Learning where there are multiple clients involved which update their individual weights to a single central server, often training on the entire individual client's dataset for each client becomes cumbersome. To address this issue we propose $\\methodprop$: a personalized coreset weighted federated learning setup where the training updates for each individual clients are forwarded to the central server based on only individual client coreset based representative data points instead of the entire client data. Through theoretical analysis we present how the average generalization error is minimax optimal up to logarithm bounds (upper bounded by $\\mathcal{O}(n_k^{-\\frac{2 \\beta}{2 \\beta+\\boldsymbol{\\Lambda}}} \\log ^{2 \\delta^{\\prime}}(n_k))$) and lower bounds of $\\mathcal{O}(n_k^{-\\frac{2 \\beta}{2 \\beta+\\boldsymbol{\\Lambda}}})$, and how the overall generalization error on the data likelihood differs from a vanilla Federated Learning setup as a closed form function ${\\boldsymbol{\\Im}}(\\boldsymbol{w}, n_k)$ of the coreset weights $\\boldsymbol{w}$ and coreset sample size $n_k$. Our experiments on different benchmark datasets based on a variety of recent personalized federated learning architectures show significant gains as compared to random sampling on the training data followed by federated learning, thereby indicating how intelligently selecting such training samples can help in performance. Additionally, through experiments on medical datasets our proposed method showcases some gains as compared to other submodular optimization based approaches used for subset selection on client's data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 128,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01804",
        "abs_url": "https://arxiv.org/abs/2511.01804",
        "pdf_url": "https://arxiv.org/pdf/2511.01804",
        "title": "Dynamic Reconstruction of Ultrasound-Derived Flow Fields With Physics-Informed Neural Fields",
        "authors": [
            "Viraj Patel",
            "Lisa Kreusser",
            "Katharine Fraser"
        ],
        "comments": "29 pages, 18 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Blood flow is sensitive to disease and provides insight into cardiac function, making flow field analysis valuable for diagnosis. However, while safer than radiation-based imaging and more suitable for patients with medical implants, ultrasound suffers from attenuation with depth, limiting the quality of the image. Despite advances in echocardiographic particle image velocimetry (EchoPIV), accurately measuring blood velocity remains challenging due to the technique's limitations and the complexity of blood flow dynamics. Physics-informed machine learning can enhance accuracy and robustness, particularly in scenarios where noisy or incomplete data challenge purely data-driven approaches. We present a physics-informed neural field model with multi-scale Fourier Feature encoding for estimating blood flow from sparse and noisy ultrasound data without requiring ground truth supervision. We demonstrate that this model achieves consistently low mean squared error in denoising and inpainting both synthetic and real datasets, verified against reference flow fields and ground truth flow rate measurements. While physics-informed neural fields have been widely used to reconstruct medical images, applications to medical flow reconstruction are mostly prominent in Flow MRI. In this work, we adapt methods that have proven effective in other imaging modalities to address the specific challenge of ultrasound-based flow reconstruction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 129,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01816",
        "abs_url": "https://arxiv.org/abs/2511.01816",
        "pdf_url": "https://arxiv.org/pdf/2511.01816",
        "title": "No-rank Tensor Decomposition Using Metric Learning",
        "authors": [
            "Maryam Bagherian"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Optimization and Control (math.OC)",
        "abstract": "Tensor decomposition faces fundamental challenges in analyzing high-dimensional data, where traditional methods based on reconstruction and fixed-rank constraints often fail to capture semantically meaningful structures. This paper introduces a no-rank tensor decomposition framework grounded in metric learning, which replaces reconstruction objectives with a discriminative, similarity-based optimization. The proposed approach learns data-driven embeddings by optimizing a triplet loss with diversity and uniformity regularization, creating a feature space where distance directly reflects semantic similarity. We provide theoretical guarantees for the framework's convergence and establish bounds on its metric properties. Evaluations across diverse domains --including face recognition (LFW, Olivetti), brain connectivity analysis (ABIDE), and simulated data (galaxy morphology, crystal structures)-- demonstrate that our method outperforms baseline techniques, including PCA, t-SNE, UMAP, and tensor decomposition baselines (CP and Tucker). Results show substantial improvements in clustering metrics (Silhouette Score, Davies--Bouldin Index, Calinski--Harabasz Index, Separation Ratio, Adjusted Rand Index, Normalized Mutual Information) and reveal a fundamental trade-off: while metric learning optimizes global class separation, it deliberately transforms local geometry to align with semantic relationships. Crucially, our approach achieves superior performance with smaller training datasets compared to transformer-based methods, offering an efficient alternative for domains with limited labeled data. This work establishes metric learning as a paradigm for tensor-based analysis, prioritizing semantic relevance over pixel-level fidelity while providing computational advantages in data-scarce scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 130,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01830",
        "abs_url": "https://arxiv.org/abs/2511.01830",
        "pdf_url": "https://arxiv.org/pdf/2511.01830",
        "title": "Towards Multi-Fidelity Scaling Laws of Neural Surrogates in CFD",
        "authors": [
            "Paul Setinek",
            "Gianluca Galletti",
            "Johannes Brandstetter"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Fluid Dynamics (physics.flu-dyn)",
        "abstract": "Scaling laws describe how model performance grows with data, parameters and compute. While large datasets can usually be collected at relatively low cost in domains such as language or vision, scientific machine learning is often limited by the high expense of generating training data through numerical simulations. However, by adjusting modeling assumptions and approximations, simulation fidelity can be traded for computational cost, an aspect absent in other domains. We investigate this trade-off between data fidelity and cost in neural surrogates using low- and high-fidelity Reynolds-Averaged Navier-Stokes (RANS) simulations. Reformulating classical scaling laws, we decompose the dataset axis into compute budget and dataset composition. Our experiments reveal compute-performance scaling behavior and exhibit budget-dependent optimal fidelity mixes for the given dataset configuration. These findings provide the first study of empirical scaling laws for multi-fidelity neural surrogate datasets and offer practical considerations for compute-efficient dataset generation in scientific machine learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 131,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01836",
        "abs_url": "https://arxiv.org/abs/2511.01836",
        "pdf_url": "https://arxiv.org/pdf/2511.01836",
        "title": "Priors in Time: Missing Inductive Biases for Language Model Interpretability",
        "authors": [
            "Ekdeep Singh Lubana",
            "Can Rager",
            "Sai Sumedh R. Hindupur",
            "Valerie Costa",
            "Greta Tuckute",
            "Oam Patel",
            "Sonia Krishna Murthy",
            "Thomas Fel",
            "Daniel Wurgaft",
            "Eric J. Bigelow",
            "Johnny Lin",
            "Demba Ba",
            "Martin Wattenberg",
            "Fernanda Viegas",
            "Melanie Weber",
            "Aaron Mueller"
        ],
        "comments": "Preprint",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Recovering meaningful concepts from language model activations is a central aim of interpretability. While existing feature extraction methods aim to identify concepts that are independent directions, it is unclear if this assumption can capture the rich temporal structure of language. Specifically, via a Bayesian lens, we demonstrate that Sparse Autoencoders (SAEs) impose priors that assume independence of concepts across time, implying stationarity. Meanwhile, language model representations exhibit rich temporal dynamics, including systematic growth in conceptual dimensionality, context-dependent correlations, and pronounced non-stationarity, in direct conflict with the priors of SAEs. Taking inspiration from computational neuroscience, we introduce a new interpretability objective -- Temporal Feature Analysis -- which possesses a temporal inductive bias to decompose representations at a given time into two parts: a predictable component, which can be inferred from the context, and a residual component, which captures novel information unexplained by the context. Temporal Feature Analyzers correctly parse garden path sentences, identify event boundaries, and more broadly delineate abstract, slow-moving information from novel, fast-moving information, while existing SAEs show significant pitfalls in all the above tasks. Overall, our results underscore the need for inductive biases that match the data in designing robust interpretability tools.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 132,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01837",
        "abs_url": "https://arxiv.org/abs/2511.01837",
        "pdf_url": "https://arxiv.org/pdf/2511.01837",
        "title": "Interpretable Machine Learning for Reservoir Water Temperatures in the U.S. Red River Basin of the South",
        "authors": [
            "Isabela Suaza-Sierra",
            "Hernan A. Moreno",
            "Luis A De la Fuente",
            "Thomas M. Neeson"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Accurate prediction of Reservoir Water Temperature (RWT) is vital for sustainable water management, ecosystem health, and climate resilience. Yet, prediction alone offers limited insight into the governing physical processes. To bridge this gap, we integrated explainable machine learning (ML) with symbolic modeling to uncover the drivers of RWT dynamics across ten reservoirs in the Red River Basin, USA, using over 10,000 depth-resolved temperature profiles. We first employed ensemble and neural models, including Random Forest (RF), Extreme Gradient Boosting (XGBoost), and Multilayer Perceptron (MLP), achieving high predictive skill (best RMSE = 1.20 degree Celsius, R^2 = 0.97). Using SHAP (SHapley Additive exPlanations), we quantified the contribution of physical drivers such as air temperature, depth, wind, and lake volume, revealing consistent patterns across reservoirs. To translate these data-driven insights into compact analytical expressions, we developed Kolmogorov Arnold Networks (KANs) to symbolically approximate RWT. Ten progressively complex KAN equations were derived, improving from R^2 = 0.84 using a single predictor (7-day antecedent air temperature) to R^2 = 0.92 with ten predictors, though gains diminished beyond five, highlighting a balance between simplicity and accuracy. The resulting equations, dominated by linear and rational forms, incrementally captured nonlinear behavior while preserving interpretability. Depth consistently emerged as a secondary but critical predictor, whereas precipitation had limited effect. By coupling predictive accuracy with explanatory power, this framework demonstrates how KANs and explainable ML can transform black-box models into transparent surrogates that advance both prediction and understanding of reservoir thermal dynamics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 133,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01847",
        "abs_url": "https://arxiv.org/abs/2511.01847",
        "pdf_url": "https://arxiv.org/pdf/2511.01847",
        "title": "Bridging Lifelong and Multi-Task Representation Learning via Algorithm and Complexity Measure",
        "authors": [
            "Zhi Wang",
            "Chicheng Zhang",
            "Ramya Korlakai Vinayak"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "In lifelong learning, a learner faces a sequence of tasks with shared structure and aims to identify and leverage it to accelerate learning. We study the setting where such structure is captured by a common representation of data. Unlike multi-task learning or learning-to-learn, where tasks are available upfront to learn the representation, lifelong learning requires the learner to make use of its existing knowledge while continually gathering partial information in an online fashion. In this paper, we consider a generalized framework of lifelong representation learning. We propose a simple algorithm that uses multi-task empirical risk minimization as a subroutine and establish a sample complexity bound based on a new notion we introduce--the task-eluder dimension. Our result applies to a wide range of learning problems involving general function classes. As concrete examples, we instantiate our result on classification and regression tasks under noise.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 134,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01855",
        "abs_url": "https://arxiv.org/abs/2511.01855",
        "pdf_url": "https://arxiv.org/pdf/2511.01855",
        "title": "Coordinate ascent neural Kalman-MLE for state estimation",
        "authors": [
            "Bettina Hanlon",
            "Angel Garcia Fernandez"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "This paper presents a coordinate ascent algorithm to learn dynamic and measurement models in dynamic state estimation using maximum likelihood estimation in a supervised manner. In particular, the dynamic and measurement models are assumed to be Gaussian and the algorithm learns the neural network parameters that model the dynamic and measurement functions, and also the noise covariance matrices. The trained dynamic and measurement models are then used with a non-linear Kalman filter algorithm to estimate the state during the testing phase.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 135,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2510.26081",
        "abs_url": "https://arxiv.org/abs/2510.26081",
        "pdf_url": "https://arxiv.org/pdf/2510.26081",
        "title": "Group-Equivariant Diffusion Models for Lattice Field Theory",
        "authors": [
            "Octavio Vega",
            "Javad Komijani",
            "Aida El-Khadra",
            "Marina Marinkovic"
        ],
        "comments": "45 pages, 12 figures",
        "subjects": "High Energy Physics - Lattice (hep-lat); Machine Learning (cs.LG)",
        "abstract": "Near the critical point, Markov Chain Monte Carlo (MCMC) simulations of lattice quantum field theories (LQFT) become increasingly inefficient due to critical slowing down. In this work, we investigate score-based symmetry-preserving diffusion models as an alternative strategy to sample two-dimensional $\\phi^4$ and ${\\rm U}(1)$ lattice field theories. We develop score networks that are equivariant to a range of group transformations, including global $\\mathbb{Z}_2$ reflections, local ${\\rm U}(1)$ rotations, and periodic translations $\\mathbb{T}$. The score networks are trained using an augmented training scheme, which significantly improves sample quality in the simulated field theories. We also demonstrate empirically that our symmetry-aware models outperform generic score networks in sample quality, expressivity, and effective sample size.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 136,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00012",
        "abs_url": "https://arxiv.org/abs/2511.00012",
        "pdf_url": "https://arxiv.org/pdf/2511.00012",
        "title": "Matrix Phylogeny: Compact Spectral Fingerprints for Trap-Robust Preconditioner Selection",
        "authors": [
            "Jinwoo Baek"
        ],
        "comments": "16 Pages",
        "subjects": "Numerical Analysis (math.NA); Machine Learning (cs.LG)",
        "abstract": "Matrix Phylogeny introduces compact spectral fingerprints (CSF/ASF) that characterize matrices at the family level. These fingerprints are low-dimensional, eigendecomposition-free descriptors built from Chebyshev trace moments estimated by Hutchinson sketches. A simple affine rescaling to [-1,1] makes them permutation/similarity invariant and robust to global scaling. Across synthetic and real tests, we observe phylogenetic compactness: only a few moments are needed. CSF with K=3-5 already yields perfect clustering (ARI=1.0; silhouettes ~0.89) on four synthetic families and a five-family set including BA vs ER, while ASF adapts the dimension on demand (median K*~9). On a SuiteSparse mini-benchmark (Hutchinson p~100), both CSF-H and ASF-H reach ARI=1.0. Against strong alternatives (eigenvalue histograms + Wasserstein, heat-kernel traces, WL-subtree), CSF-K=5 matches or exceeds accuracy while avoiding eigendecompositions and using far fewer features (K<=10 vs 64/9153). The descriptors are stable to noise (log-log slope ~1.03, R^2~0.993) and support a practical trap->recommend pipeline for automated preconditioner selection. In an adversarial E6+ setting with a probe-and-switch mechanism, our physics-guided recommender attains near-oracle iteration counts (p90 regret=0), whereas a Frobenius 1-NN baseline exhibits large spikes (p90~34-60). CSF/ASF deliver compact (K<=10), fast, invariant fingerprints that enable scalable, structure-aware search and recommendation over large matrix repositories. We recommend CSF with K=5 by default, and ASF when domain-specific adaptivity is desired.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 137,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00013",
        "abs_url": "https://arxiv.org/abs/2511.00013",
        "pdf_url": "https://arxiv.org/pdf/2511.00013",
        "title": "Using machine learning methods to predict cognitive age from psychophysiological tests",
        "authors": [
            "Daria D. Tyurina",
            "Sergey V. Stasenko",
            "Konstantin V. Lushnikov",
            "Maria V. Vedunova"
        ],
        "comments": "",
        "subjects": "Neurons and Cognition (q-bio.NC); Machine Learning (cs.LG)",
        "abstract": "This study introduces a novel method for predicting cognitive age using psychophysiological tests. To determine cognitive age, subjects were asked to complete a series of psychological tests measuring various cognitive functions, including reaction time and cognitive conflict, short-term memory, verbal functions, and color and spatial perception. Based on the tests completed, the average completion time, proportion of correct answers, average absolute delta of the color campimetry test, number of guessed words in the Münsterberg matrix, and other parameters were calculated for each subject. The obtained characteristics of the subjects were preprocessed and used to train a machine learning algorithm implementing a regression task for predicting a person's cognitive age. These findings contribute to the field of remote screening using mobile devices for human health for diagnosing and monitoring cognitive aging.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 138,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00024",
        "abs_url": "https://arxiv.org/abs/2511.00024",
        "pdf_url": "https://arxiv.org/pdf/2511.00024",
        "title": "Chitchat with AI: Understand the supply chain carbon disclosure of companies worldwide through Large Language Model",
        "authors": [
            "Haotian Hang",
            "Yueyang Shen",
            "Vicky Zhu",
            "Jose Cruz",
            "Michelle Li"
        ],
        "comments": "",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG); Applications (stat.AP)",
        "abstract": "In the context of global sustainability mandates, corporate carbon disclosure has emerged as a critical mechanism for aligning business strategy with environmental responsibility. The Carbon Disclosure Project (CDP) hosts the world's largest longitudinal dataset of climate-related survey responses, combining structured indicators with open-ended narratives, but the heterogeneity and free-form nature of these disclosures present significant analytical challenges for benchmarking, compliance monitoring, and investment screening. This paper proposes a novel decision-support framework that leverages large language models (LLMs) to assess corporate climate disclosure quality at scale. It develops a master rubric that harmonizes narrative scoring across 11 years of CDP data (2010-2020), enabling cross-sector and cross-country benchmarking. By integrating rubric-guided scoring with percentile-based normalization, our method identifies temporal trends, strategic alignment patterns, and inconsistencies in disclosure across industries and regions. Results reveal that sectors such as technology and countries like Germany consistently demonstrate higher rubric alignment, while others exhibit volatility or superficial engagement, offering insights that inform key decision-making processes for investors, regulators, and corporate environmental, social, and governance (ESG) strategists. The proposed LLM-based approach transforms unstructured disclosures into quantifiable, interpretable, comparable, and actionable intelligence, advancing the capabilities of AI-enabled decision support systems (DSSs) in the domain of climate governance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 139,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00025",
        "abs_url": "https://arxiv.org/abs/2511.00025",
        "pdf_url": "https://arxiv.org/pdf/2511.00025",
        "title": "On the Structure of Floating-Point Noise in Batch-Invariant GPU Matrix Multiplication",
        "authors": [
            "Tadisetty Sai Yashwanth"
        ],
        "comments": "",
        "subjects": "Numerical Analysis (math.NA); Machine Learning (cs.LG)",
        "abstract": "Floating-point non-associativity makes fundamental deep learning operations, such as matrix multiplication (matmul) on GPUs, inherently non-deterministic. Despite this, the statistical structure of the resulting numerical error remains poorly understood. A common working assumption is that these errors behave as independent and identically distributed (i.i.d.) Gaussian noise. In this paper, we empirically test this assumption and show that it fails to describe real GPU behavior. By comparing outputs of single-input and batched matmuls, we find that while the i.i.d. model predicts non-zero output instability, empirical results show a 0.00% prediction flip rate. Through covariance analysis, we uncover the cause: the floating-point error is structured and highly correlated. For float16, nearly 50% of the total error variance lies in off-diagonal terms, revealing that the noise behaves as a coordinated, directional perturbation rather than random static. This result challenges the prevailing stochastic view of numerical noise and provides a principled foundation for analyzing deep learning reliability under hardware non-determinism.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 140,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00034",
        "abs_url": "https://arxiv.org/abs/2511.00034",
        "pdf_url": "https://arxiv.org/pdf/2511.00034",
        "title": "On the Fundamental Limitations of Decentralized Learnable Reward Shaping in Cooperative Multi-Agent Reinforcement Learning",
        "authors": [
            "Aditya Akella"
        ],
        "comments": "8 pages, 5 figures, 2 tables",
        "subjects": "Multiagent Systems (cs.MA); Machine Learning (cs.LG)",
        "abstract": "Recent advances in learnable reward shaping have shown promise in single-agent reinforcement learning by automatically discovering effective feedback signals. However, the effectiveness of decentralized learnable reward shaping in cooperative multi-agent settings remains poorly understood. We propose DMARL-RSA, a fully decentralized system where each agent learns individual reward shaping, and evaluate it on cooperative navigation tasks in the simple_spread_v3 environment. Despite sophisticated reward learning, DMARL-RSA achieves only -24.20 +/- 0.09 average reward, compared to MAPPO with centralized training at 1.92 +/- 0.87--a 26.12-point gap. DMARL-RSA performs similarly to simple independent learning (IPPO: -23.19 +/- 0.96), indicating that advanced reward shaping cannot overcome fundamental decentralized coordination limitations. Interestingly, decentralized methods achieve higher landmark coverage (0.888 +/- 0.029 for DMARL-RSA, 0.960 +/- 0.045 for IPPO out of 3 total) but worse overall performance than centralized MAPPO (0.273 +/- 0.008 landmark coverage)--revealing a coordination paradox between local optimization and global performance. Analysis identifies three critical barriers: (1) non-stationarity from concurrent policy updates, (2) exponential credit assignment complexity, and (3) misalignment between individual reward optimization and global objectives. These results establish empirical limits for decentralized reward learning and underscore the necessity of centralized coordination for effective multi-agent cooperation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 141,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00039",
        "abs_url": "https://arxiv.org/abs/2511.00039",
        "pdf_url": "https://arxiv.org/pdf/2511.00039",
        "title": "Graph-Attentive MAPPO for Dynamic Retail Pricing",
        "authors": [
            "Krishna Kumar Neelakanta Pillai Santha Kumari Amma"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Dynamic pricing in retail requires policies that adapt to shifting demand while coordinating decisions across related products. We present a systematic empirical study of multi-agent reinforcement learning for retail price optimization, comparing a strong MAPPO baseline with a graph-attention-augmented variant (MAPPO+GAT) that leverages learned interactions among products. Using a simulated pricing environment derived from real transaction data, we evaluate profit, stability across random seeds, fairness across products, and training efficiency under a standardized evaluation protocol. The results indicate that MAPPO provides a robust and reproducible foundation for portfolio-level price control, and that MAPPO+GAT further enhances performance by sharing information over the product graph without inducing excessive price volatility. These results indicate that graph-integrated MARL provides a more scalable and stable solution than independent learners for dynamic retail pricing, offering practical advantages in multi-product decision-making.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 142,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00072",
        "abs_url": "https://arxiv.org/abs/2511.00072",
        "pdf_url": "https://arxiv.org/pdf/2511.00072",
        "title": "LookSync: Large-Scale Visual Product Search System for AI-Generated Fashion Looks",
        "authors": [
            "Pradeep M",
            "Ritesh Pallod",
            "Satyen Abrol",
            "Muthu Raman",
            "Ian Anderson"
        ],
        "comments": "4 pages, 5 figures. Accepted at the International Conference on Data Science (IKDD CODS 2025), Demonstration Track. Demo video: this https URL",
        "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Generative AI is reshaping fashion by enabling virtual looks and avatars making it essential to find real products that best match AI-generated styles. We propose an end-to-end product search system that has been deployed in a real-world, internet scale which ensures that AI-generated looks presented to users are matched with the most visually and semantically similar products from the indexed vector space. The search pipeline is composed of four key components: query generation, vectorization, candidate retrieval, and reranking based on AI-generated looks. Recommendation quality is evaluated using human-judged accuracy scores. The system currently serves more than 350,000 AI Looks in production per day, covering diverse product categories across global markets of over 12 million products. In our experiments, we observed that across multiple annotators and categories, CLIP outperformed alternative models by a small relative margin of 3--7\\% in mean opinion scores. These improvements, though modest in absolute numbers, resulted in noticeably better user perception matches, establishing CLIP as the most reliable backbone for production deployment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 143,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00075",
        "abs_url": "https://arxiv.org/abs/2511.00075",
        "pdf_url": "https://arxiv.org/pdf/2511.00075",
        "title": "PDA-LSTM: Knowledge-driven page data arrangement based on LSTM for LCM supression in QLC 3D NAND flash memories",
        "authors": [
            "Qianhui Li",
            "Weiya Wang",
            "Qianqi Zhao",
            "Tong Qu",
            "Jing He",
            "Xuhong Qiang",
            "Jingwen Hou",
            "Ke Chen",
            "Bao Zhang",
            "Qi Wang"
        ],
        "comments": "",
        "subjects": "Hardware Architecture (cs.AR); Machine Learning (cs.LG)",
        "abstract": "Quarter level cell (QLC) 3D NAND flash memory is emerging as the predominant storage solution in the era of artificial intelligence. QLC 3D NAND flash stores 4 bit per cell to expand the storage density, resulting in narrower read margins. Constrained to read margins, QLC always suffers from lateral charge migration (LCM), which caused by non-uniform charge density across adjacent memory cells. To suppress charge density gap between cells, there are some algorithm in form of intra-page data mapping such as WBVM, DVDS. However, we observe inter-page data arrangements also approach the suppression. Thus, we proposed an intelligent model PDA-LSTM to arrange intra-page data for LCM suppression, which is a physics-knowledge-driven neural network model. PDA-LSTM applies a long-short term memory (LSTM) neural network to compute a data arrangement probability matrix from input page data pattern. The arrangement is to minimize the global impacts derived from the LCM among wordlines. Since each page data can be arranged only once, we design a transformation from output matrix of LSTM network to non-repetitive sequence generation probability matrix to assist training process. The arranged data pattern can decrease the bit error rate (BER) during data retention. In addition, PDA-LSTM do not need extra flag bits to record data transport of 3D NAND flash compared with WBVM, DVDS. The experiment results show that the PDA-LSTM reduces the average BER by 80.4% compared with strategy without data arrangement, and by 18.4%, 15.2% compared respectively with WBVM and DVDS with code-length 64.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 144,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00081",
        "abs_url": "https://arxiv.org/abs/2511.00081",
        "pdf_url": "https://arxiv.org/pdf/2511.00081",
        "title": "Forecasting Occupational Survivability of Rickshaw Pullers in a Changing Climate with Wearable Data",
        "authors": [
            "Masfiqur Rahaman",
            "Maoyejatun Hasana",
            "Shahad Shahriar Rahman",
            "MD Sajid Mostafiz Noor",
            "Razin Reaz Abedin",
            "Md Toki Tahmid",
            "Duncan Watson Parris",
            "Tanzeem Choudhury",
            "A. B. M. Alim Al Islam",
            "Tauhidur Rahman"
        ],
        "comments": "This is a preprint version of a manuscript accepted and to be published in the Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)",
        "subjects": "Computers and Society (cs.CY); Human-Computer Interaction (cs.HC); Information Retrieval (cs.IR); Machine Learning (cs.LG); Applications (stat.AP)",
        "abstract": "Cycle rickshaw pullers are highly vulnerable to extreme heat, yet little is known about how their physiological biomarkers respond under such conditions. This study collected real-time weather and physiological data using wearable sensors from 100 rickshaw pullers in Dhaka, Bangladesh. In addition, interviews with 12 pullers explored their knowledge, perceptions, and experiences related to climate change. We developed a Linear Gaussian Bayesian Network (LGBN) regression model to predict key physiological biomarkers based on activity, weather, and demographic features. The model achieved normalized mean absolute error values of 0.82, 0.47, 0.65, and 0.67 for skin temperature, relative cardiac cost, skin conductance response, and skin conductance level, respectively. Using projections from 18 CMIP6 climate models, we layered the LGBN on future climate forecasts to analyze survivability for current (2023-2025) and future years (2026-2100). Based on thresholds of WBGT above 31.1°C and skin temperature above 35°C, 32% of rickshaw pullers already face high heat exposure risk. By 2026-2030, this percentage may rise to 37% with average exposure lasting nearly 12 minutes, or about two-thirds of the trip duration. A thematic analysis of interviews complements these findings, showing that rickshaw pullers recognize their increasing climate vulnerability and express concern about its effects on health and occupational survivability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 145,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00092",
        "abs_url": "https://arxiv.org/abs/2511.00092",
        "pdf_url": "https://arxiv.org/pdf/2511.00092",
        "title": "QuantumBench: A Benchmark for Quantum Problem Solving",
        "authors": [
            "Shunya Minami",
            "Tatsuya Ishigaki",
            "Ikko Hamamura",
            "Taku Mikuriya",
            "Youmi Ma",
            "Naoaki Okazaki",
            "Hiroya Takamura",
            "Yohichi Suzuki",
            "Tadashi Kadowaki"
        ],
        "comments": "11 pages, 8 figures",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG); Quantum Physics (quant-ph)",
        "abstract": "Large language models are now integrated into many scientific workflows, accelerating data analysis, hypothesis generation, and design space exploration. In parallel with this growth, there is a growing need to carefully evaluate whether models accurately capture domain-specific knowledge and notation, since general-purpose benchmarks rarely reflect these requirements. This gap is especially clear in quantum science, which features non-intuitive phenomena and requires advanced mathematics. In this study, we introduce QuantumBench, a benchmark for the quantum domain that systematically examine how well LLMs understand and can be applied to this non-intuitive field. Using publicly available materials, we compiled approximately 800 questions with their answers spanning nine areas related to quantum science and organized them into an eight-option multiple-choice dataset. With this benchmark, we evaluate several existing LLMs and analyze their performance in the quantum domain, including sensitivity to changes in question format. QuantumBench is the first LLM evaluation dataset built for the quantum domain, and it is intended to guide the effective use of LLMs in quantum research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 146,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00114",
        "abs_url": "https://arxiv.org/abs/2511.00114",
        "pdf_url": "https://arxiv.org/pdf/2511.00114",
        "title": "End-to-End Framework Integrating Generative AI and Deep Reinforcement Learning for Autonomous Ultrasound Scanning",
        "authors": [
            "Hanae Elmekki",
            "Amanda Spilkin",
            "Ehsan Zakeri",
            "Antonela Mariel Zanuttini",
            "Ahmed Alagha",
            "Hani Sami",
            "Jamal Bentahar",
            "Lyes Kadem",
            "Wen-Fang Xie",
            "Philippe Pibarot",
            "Rabeb Mizouni",
            "Hadi Otrok",
            "Azzam Mourad",
            "Sami Muhaidat"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Cardiac ultrasound (US) is among the most widely used diagnostic tools in cardiology for assessing heart health, but its effectiveness is limited by operator dependence, time constraints, and human error. The shortage of trained professionals, especially in remote areas, further restricts access. These issues underscore the need for automated solutions that can ensure consistent, and accessible cardiac imaging regardless of operator skill or location. Recent progress in artificial intelligence (AI), especially in deep reinforcement learning (DRL), has gained attention for enabling autonomous decision-making. However, existing DRL-based approaches to cardiac US scanning lack reproducibility, rely on proprietary data, and use simplified models. Motivated by these gaps, we present the first end-to-end framework that integrates generative AI and DRL to enable autonomous and reproducible cardiac US scanning. The framework comprises two components: (i) a conditional generative simulator combining Generative Adversarial Networks (GANs) with Variational Autoencoders (VAEs), that models the cardiac US environment producing realistic action-conditioned images; and (ii) a DRL module that leverages this simulator to learn autonomous, accurate scanning policies. The proposed framework delivers AI-driven guidance through expert-validated models that classify image type and assess quality, supports conditional generation of realistic US images, and establishes a reproducible foundation extendable to other organs. To ensure reproducibility, a publicly available dataset of real cardiac US scans is released. The solution is validated through several experiments. The VAE-GAN is benchmarked against existing GAN variants, with performance assessed using qualitative and quantitative approaches, while the DRL-based scanning system is evaluated under varying configurations to demonstrate effectiveness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 147,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00162",
        "abs_url": "https://arxiv.org/abs/2511.00162",
        "pdf_url": "https://arxiv.org/pdf/2511.00162",
        "title": "ARC-GEN: A Mimetic Procedural Benchmark Generator for the Abstraction and Reasoning Corpus",
        "authors": [
            "Michael D. Moffitt"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "The Abstraction and Reasoning Corpus remains one of the most compelling and challenging benchmarks for tracking progress toward achieving Artificial General Intelligence. In contrast to other evaluation datasets designed to assess an agent's task-specific skills or accumulated knowledge, the ARC-AGI suite is specifically targeted at measuring skill acquisition efficiency, a trait that has (so far) been lacking in even the most sophisticated machine learning systems. For algorithms that require extensive intra-task exemplars, a significant constraint imposed by ARC-AGI is the modest cardinality of its demonstration set, comprising a small number of $\\langle$ input, output $\\rangle$ grids per task specifying the corresponding transformation. To embellish the space of viable sample pairs, this paper introduces ARC-GEN, an open-source procedural generator aimed at extending the original ARC-AGI training dataset as faithfully as possible. Unlike prior efforts, our generator is both exhaustive (covering all four-hundred tasks) and mimetic (more closely honoring the distributional properties and characteristics embodied in the initial ARC-AGI-1 release). We also discuss the use of this generator in establishing a static benchmark suite to verify the correctness of programs submitted to the 2025 Google Code Golf Championship.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 148,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00180",
        "abs_url": "https://arxiv.org/abs/2511.00180",
        "pdf_url": "https://arxiv.org/pdf/2511.00180",
        "title": "ParaScopes: What do Language Models Activations Encode About Future Text?",
        "authors": [
            "Nicky Pochinkov",
            "Yulia Volkova",
            "Anna Vasileva",
            "Sai V R Chereddy"
        ],
        "comments": "Main paper: 9 pages, 10 figures. Total 24 pages",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Interpretability studies in language models often investigate forward-looking representations of activations. However, as language models become capable of doing ever longer time horizon tasks, methods for understanding activations often remain limited to testing specific concepts or tokens. We develop a framework of Residual Stream Decoders as a method of probing model activations for paragraph-scale and document-scale plans. We test several methods and find information can be decoded equivalent to 5+ tokens of future context in small models. These results lay the groundwork for better monitoring of language models and better understanding how they might encode longer-term planning information.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 149,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00193",
        "abs_url": "https://arxiv.org/abs/2511.00193",
        "pdf_url": "https://arxiv.org/pdf/2511.00193",
        "title": "Reducing Robotic Upper-Limb Assessment Time While Maintaining Precision: A Time Series Foundation Model Approach",
        "authors": [
            "Faranak Akbarifar",
            "Nooshin Maghsoodi",
            "Sean P Dukelow",
            "Stephen Scott",
            "Parvin Mousavi"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
        "abstract": "Purpose: Visually Guided Reaching (VGR) on the Kinarm robot yields sensitive kinematic biomarkers but requires 40-64 reaches, imposing time and fatigue burdens. We evaluate whether time-series foundation models can replace unrecorded trials from an early subset of reaches while preserving the reliability of standard Kinarm parameters. Methods: We analyzed VGR speed signals from 461 stroke and 599 control participants across 4- and 8-target reaching protocols. We withheld all but the first 8 or 16 reaching trials and used ARIMA, MOMENT, and Chronos models, fine-tuned on 70 percent of subjects, to forecast synthetic trials. We recomputed four kinematic features of reaching (reaction time, movement time, posture speed, maximum speed) on combined recorded plus forecasted trials and compared them to full-length references using ICC(2,1). Results: Chronos forecasts restored ICC >= 0.90 for all parameters with only 8 recorded trials plus forecasts, matching the reliability of 24-28 recorded reaches (Delta ICC <= 0.07). MOMENT yielded intermediate gains, while ARIMA improvements were minimal. Across cohorts and protocols, synthetic trials replaced reaches without materially compromising feature reliability. Conclusion: Foundation-model forecasting can greatly shorten Kinarm VGR assessment time. For the most impaired stroke survivors, sessions drop from 4-5 minutes to about 1 minute while preserving kinematic precision. This forecast-augmented paradigm promises efficient robotic evaluations for assessing motor impairments following stroke.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 150,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00202",
        "abs_url": "https://arxiv.org/abs/2511.00202",
        "pdf_url": "https://arxiv.org/pdf/2511.00202",
        "title": "Position: Vibe Coding Needs Vibe Reasoning: Improving Vibe Coding with Formal Verification",
        "authors": [
            "Jacqueline Mitchell",
            "Yasser Shaaban"
        ],
        "comments": "7 pages, 3 figures, In Proceedings of the 1st ACM SIGPLAN International Workshop on Language Models and Programming Languages (LMPL'25), October 12-18, 2025, Singapore, Singapore. ACM, New York, NY, USA",
        "subjects": "Software Engineering (cs.SE); Machine Learning (cs.LG); Logic in Computer Science (cs.LO)",
        "abstract": "``Vibe coding'' -- the practice of developing software through iteratively conversing with a large language model (LLM) -- has exploded in popularity within the last year. However, developers report key limitations including the accumulation of technical debt, security issues, and code churn to achieve satisfactory results. We argue that these pitfalls result from LLMs' inability to reconcile accumulating human-imposed constraints during vibe coding, with developers inadvertently failing to resolve contradictions because LLMs prioritize user commands over code consistency. Given LLMs' receptiveness to verification-based feedback, we argue that formal methods can mitigate these pitfalls, making vibe coding more reliable. However, we posit that integrating formal methods must transcend existing approaches that combine formal methods and LLMs. We advocate for a side-car system throughout the vibe coding process which: (1) \\emph{Autoformalizes} specifications (2) Validates against targets, (3) Delivers \\emph{actionable} feedback to the LLM, and (4) Allows intuitive developer influence on specifications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 151,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00204",
        "abs_url": "https://arxiv.org/abs/2511.00204",
        "pdf_url": "https://arxiv.org/pdf/2511.00204",
        "title": "Transfer learning discovery of molecular modulators for perovskite solar cells",
        "authors": [
            "Haoming Yan",
            "Xinyu Chen",
            "Yanran Wang",
            "Zhengchao Luo",
            "Weizheng Huang",
            "Hongshuai Wang",
            "Peng Chen",
            "Yuzhi Zhang",
            "Weijie Sun",
            "Jinzhuo Wang",
            "Qihuang Gong",
            "Rui Zhu",
            "Lichen Zhao"
        ],
        "comments": "",
        "subjects": "Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG); Applied Physics (physics.app-ph)",
        "abstract": "The discovery of effective molecular modulators is essential for advancing perovskite solar cells (PSCs), but the research process is hindered by the vastness of chemical space and the time-consuming and expensive trial-and-error experimental screening. Concurrently, machine learning (ML) offers significant potential for accelerating materials discovery. However, applying ML to PSCs remains a major challenge due to data scarcity and limitations of traditional quantitative structure-property relationship (QSPR) models. Here, we apply a chemical informed transfer learning framework based on pre-trained deep neural networks, which achieves high accuracy in predicting the molecular modulator's effect on the power conversion efficiency (PCE) of PSCs. This framework is established through systematical benchmarking of diverse molecular representations, enabling lowcost and high-throughput virtual screening over 79,043 commercially available molecules. Furthermore, we leverage interpretability techniques to visualize the learned chemical representation and experimentally characterize the resulting modulator-perovskite interactions. The top molecular modulators identified by the framework are subsequently validated experimentally, delivering a remarkably improved champion PCE of 26.91% in PSCs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 152,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00217",
        "abs_url": "https://arxiv.org/abs/2511.00217",
        "pdf_url": "https://arxiv.org/pdf/2511.00217",
        "title": "Gradient Boosted Mixed Models: Flexible Joint Estimation of Mean and Variance Components for Clustered Data",
        "authors": [
            "Mitchell L. Prevett",
            "Francis K. C. Hui",
            "Zhi Yang Tho",
            "A. H. Welsh",
            "Anton H. Westveld"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Computation (stat.CO); Methodology (stat.ME)",
        "abstract": "Linear mixed models are widely used for clustered data, but their reliance on parametric forms limits flexibility in complex and high-dimensional settings. In contrast, gradient boosting methods achieve high predictive accuracy through nonparametric estimation, but do not accommodate clustered data structures or provide uncertainty quantification. We introduce Gradient Boosted Mixed Models (GBMixed), a framework and algorithm that extends boosting to jointly estimate mean and variance components via likelihood-based gradients. In addition to nonparametric mean estimation, the method models both random effects and residual variances as potentially covariate-dependent functions using flexible base learners such as regression trees or splines, enabling nonparametric estimation while maintaining interpretability. Simulations and real-world applications demonstrate accurate recovery of variance components, calibrated prediction intervals, and improved predictive accuracy relative to standard linear mixed models and nonparametric methods. GBMixed provides heteroscedastic uncertainty quantification and introduces boosting for heterogeneous random effects. This enables covariate-dependent shrinkage for cluster-specific predictions to adapt between population and cluster-level data. Under standard causal assumptions, the framework enables estimation of heterogeneous treatment effects with reliable uncertainty quantification.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 153,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00256",
        "abs_url": "https://arxiv.org/abs/2511.00256",
        "pdf_url": "https://arxiv.org/pdf/2511.00256",
        "title": "NaturalVoices: A Large-Scale, Spontaneous and Emotional Podcast Dataset for Voice Conversion",
        "authors": [
            "Zongyang Du",
            "Shreeram Suresh Chandra",
            "Ismail Rasim Ulgen",
            "Aurosweta Mahapatra",
            "Ali N. Salman",
            "Carlos Busso",
            "Berrak Sisman"
        ],
        "comments": "Under review for IEEE Transactions on Affective Computing",
        "subjects": "Audio and Speech Processing (eess.AS); Machine Learning (cs.LG); Sound (cs.SD)",
        "abstract": "Everyday speech conveys far more than words, it reflects who we are, how we feel, and the circumstances surrounding our interactions. Yet, most existing speech datasets are acted, limited in scale, and fail to capture the expressive richness of real-life communication. With the rise of large neural networks, several large-scale speech corpora have emerged and been widely adopted across various speech processing tasks. However, the field of voice conversion (VC) still lacks large-scale, expressive, and real-life speech resources suitable for modeling natural prosody and emotion. To fill this gap, we release NaturalVoices (NV), the first large-scale spontaneous podcast dataset specifically designed for emotion-aware voice conversion. It comprises 5,049 hours of spontaneous podcast recordings with automatic annotations for emotion (categorical and attribute-based), speech quality, transcripts, speaker identity, and sound events. The dataset captures expressive emotional variation across thousands of speakers, diverse topics, and natural speaking styles. We also provide an open-source pipeline with modular annotation tools and flexible filtering, enabling researchers to construct customized subsets for a wide range of VC tasks. Experiments demonstrate that NaturalVoices supports the development of robust and generalizable VC models capable of producing natural, expressive speech, while revealing limitations of current architectures when applied to large-scale spontaneous data. These results suggest that NaturalVoices is both a valuable resource and a challenging benchmark for advancing the field of voice conversion. Dataset is available at: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 154,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00268",
        "abs_url": "https://arxiv.org/abs/2511.00268",
        "pdf_url": "https://arxiv.org/pdf/2511.00268",
        "title": "IL-PCSR: Legal Corpus for Prior Case and Statute Retrieval",
        "authors": [
            "Shounak Paul",
            "Dhananjay Ghumare",
            "Pawan Goyal",
            "Saptarshi Ghosh",
            "Ashutosh Modi"
        ],
        "comments": "Accepted at EMNLP 2025 (Main)",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "Identifying/retrieving relevant statutes and prior cases/precedents for a given legal situation are common tasks exercised by law practitioners. Researchers to date have addressed the two tasks independently, thus developing completely different datasets and models for each task; however, both retrieval tasks are inherently related, e.g., similar cases tend to cite similar statutes (due to similar factual situation). In this paper, we address this gap. We propose IL-PCR (Indian Legal corpus for Prior Case and Statute Retrieval), which is a unique corpus that provides a common testbed for developing models for both the tasks (Statute Retrieval and Precedent Retrieval) that can exploit the dependence between the two. We experiment extensively with several baseline models on the tasks, including lexical models, semantic models and ensemble based on GNNs. Further, to exploit the dependence between the two tasks, we develop an LLM-based re-ranking approach that gives the best performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 155,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00279",
        "abs_url": "https://arxiv.org/abs/2511.00279",
        "pdf_url": "https://arxiv.org/pdf/2511.00279",
        "title": "LongCat-Flash-Omni Technical Report",
        "authors": [
            "Meituan LongCat Team",
            "Bairui Wang",
            "Bayan",
            "Bin Xiao",
            "Bo Zhang",
            "Bolin Rong",
            "Borun Chen",
            "Chang Wan",
            "Chao Zhang",
            "Chen Huang",
            "Chen Chen",
            "Chen Chen",
            "Chengxu Yang",
            "Chengzuo Yang",
            "Cong Han",
            "Dandan Peng",
            "Delian Ruan",
            "Detai Xin",
            "Disong Wang",
            "Dongchao Yang",
            "Fanfan Liu",
            "Fengjiao Chen",
            "Fengyu Yang",
            "Gan Dong",
            "Gang Huang",
            "Gang Xu",
            "Guanglu Wan",
            "Guoqiang Tan",
            "Guoqiao Yu",
            "Haibo Qiu",
            "Hao Lu",
            "Hongbo Liu",
            "Hongyu Xiang",
            "Jiaheng Wu",
            "Jian Yang",
            "Jiaxing Liu",
            "Jing Huang",
            "Jingang Wang",
            "Jinrui Ding",
            "Juchao Jiang",
            "Jun Kuang",
            "Jun Wang",
            "Junhui Mei",
            "Ke Ding",
            "Kefeng Zhang",
            "Lei Chen",
            "Liang Shi",
            "Limeng Qiao",
            "Liming Zheng",
            "Lin Ma",
            "Liuyang Guo",
            "Liya Ma",
            "Luying Sun",
            "Man Gao",
            "Mengshen Zhu",
            "Miao Cao",
            "Minliang Lin",
            "Nuo Xu",
            "Peng Shi",
            "Qi Zhang",
            "Qian Fang",
            "Qian Wang",
            "Qian Yang",
            "Quanxiu Wang",
            "Rongxiang Weng",
            "Rongxin Guo",
            "Ruoxuan Liang",
            "Senbin Yang",
            "Shanbo Xu",
            "Shanglin Lei",
            "Shengze Ye",
            "Shimin Chen",
            "Shuaiqi Chen",
            "Shujie Hu",
            "Shuo Li",
            "Siqi Yang",
            "Siyu Xu",
            "Siyu Ren",
            "Song Li",
            "Songxiang Liu",
            "Tianhao Bai",
            "Tianye Dai",
            "Wei Hong",
            "Wei Wang",
            "Weixiao Zhao",
            "Wengang Cao",
            "Wenlong Zhu",
            "Wenlong He",
            "Xi Su",
            "Xi Nan",
            "Xiaohan Zhao",
            "Xiaohao Wang",
            "Xiaoyu Zhao",
            "Xiaoyu Wang",
            "Xiaoyu Li",
            "Xin Pan",
            "Xin Chen",
            "Xiusong Sun",
            "Xu Xiang",
            "Xudong Xing"
        ],
        "comments": "",
        "subjects": "Multimedia (cs.MM); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG); Sound (cs.SD)",
        "abstract": "We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal model with 560 billion parameters, excelling at real-time audio-visual interaction. By adopting a curriculum-inspired progressive training strategy that transitions from simpler to increasingly complex modality sequence modeling tasks, LongCat-Flash-Omni attains comprehensive multimodal capabilities while maintaining strong unimodal capability. Building upon LongCat-Flash, which adopts a high-performance Shortcut-connected Mixture-of-Experts (MoE) architecture with zero-computation experts, LongCat-Flash-Omni integrates efficient multimodal perception and speech reconstruction modules. Despite its immense size of 560B parameters (with 27B activated), LongCat-Flash-Omni achieves low-latency real-time audio-visual interaction. For training infrastructure, we developed a modality-decoupled parallelism scheme specifically designed to manage the data and model heterogeneity inherent in large-scale multimodal training. This innovative approach demonstrates exceptional efficiency by sustaining over 90% of the throughput achieved by text-only training. Extensive evaluations show that LongCat-Flash-Omni achieves state-of-the-art performance on omni-modal benchmarks among open-source models. Furthermore, it delivers highly competitive results across a wide range of modality-specific tasks, including text, image, and video understanding, as well as audio understanding and generation. We provide a comprehensive overview of the model architecture design, training procedures, and data strategies, and open-source the model to foster future research and development in the community.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 156,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00336",
        "abs_url": "https://arxiv.org/abs/2511.00336",
        "pdf_url": "https://arxiv.org/pdf/2511.00336",
        "title": "Split Learning-Enabled Framework for Secure and Light-weight Internet of Medical Things Systems",
        "authors": [
            "Siva Sai",
            "Manish Prasad",
            "Animesh Bhargava",
            "Vinay Chamola",
            "Rajkumar Buyya"
        ],
        "comments": "11 pages, 5 figures, Under review in an IEEE Transactions journal",
        "subjects": "Cryptography and Security (cs.CR); Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)",
        "abstract": "The rapid growth of Internet of Medical Things (IoMT) devices has resulted in significant security risks, particularly the risk of malware attacks on resource-constrained devices. Conventional deep learning methods are impractical due to resource limitations, while Federated Learning (FL) suffers from high communication overhead and vulnerability to non-IID (heterogeneous) data. In this paper, we propose a split learning (SL) based framework for IoT malware detection through image-based classification. By dividing the neural network training between the clients and an edge server, the framework reduces computational burden on resource-constrained clients while ensuring data privacy. We formulate a joint optimization problem that balances computation cost and communication efficiency by using a game-theoretic approach for attaining better training performance. Experimental evaluations show that the proposed framework outperforms popular FL methods in terms of accuracy (+6.35%), F1-score (+5.03%), high convergence speed (+14.96%), and less resource consumption (33.83%). These results establish the potential of SL as a scalable and secure paradigm for next-generation IoT security.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 157,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00342",
        "abs_url": "https://arxiv.org/abs/2511.00342",
        "pdf_url": "https://arxiv.org/pdf/2511.00342",
        "title": "MH-1M: A 1.34 Million-Sample Comprehensive Multi-Feature Android Malware Dataset for Machine Learning, Deep Learning, Large Language Models, and Threat Intelligence Research",
        "authors": [
            "Hendrio Braganca",
            "Diego Kreutz",
            "Vanderson Rocha",
            "Joner Assolin",
            "and Eduardo Feitosa"
        ],
        "comments": "17 pages, 7 figures, 13 tables, submitted to the Scientific Data journal published by Nature Research",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Performance (cs.PF)",
        "abstract": "We present MH-1M, one of the most comprehensive and up-to-date datasets for advanced Android malware research. The dataset comprises 1,340,515 applications, encompassing a wide range of features and extensive metadata. To ensure accurate malware classification, we employ the VirusTotal API, integrating multiple detection engines for comprehensive and reliable assessment. Our GitHub, Figshare, and Harvard Dataverse repositories provide open access to the processed dataset and its extensive supplementary metadata, totaling more than 400 GB of data and including the outputs of the feature extraction pipeline as well as the corresponding VirusTotal reports. Our findings underscore the MH-1M dataset's invaluable role in understanding the evolving landscape of malware.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 158,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00346",
        "abs_url": "https://arxiv.org/abs/2511.00346",
        "pdf_url": "https://arxiv.org/pdf/2511.00346",
        "title": "Exploiting Latent Space Discontinuities for Building Universal LLM Jailbreaks and Data Extraction Attacks",
        "authors": [
            "Kayua Oleques Paim",
            "Rodrigo Brandao Mansilha",
            "Diego Kreutz",
            "Muriel Figueredo Franco",
            "Weverton Cordeiro"
        ],
        "comments": "10 pages, 5 figures, 4 tables, Published at the Brazilian Symposium on Cybersecurity (SBSeg 2025)",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "The rapid proliferation of Large Language Models (LLMs) has raised significant concerns about their security against adversarial attacks. In this work, we propose a novel approach to crafting universal jailbreaks and data extraction attacks by exploiting latent space discontinuities, an architectural vulnerability related to the sparsity of training data. Unlike previous methods, our technique generalizes across various models and interfaces, proving highly effective in seven state-of-the-art LLMs and one image generation model. Initial results indicate that when these discontinuities are exploited, they can consistently and profoundly compromise model behavior, even in the presence of layered defenses. The findings suggest that this strategy has substantial potential as a systemic attack vector.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 159,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00361",
        "abs_url": "https://arxiv.org/abs/2511.00361",
        "pdf_url": "https://arxiv.org/pdf/2511.00361",
        "title": "MalDataGen: A Modular Framework for Synthetic Tabular Data Generation in Malware Detection",
        "authors": [
            "Kayua Oleques Paim",
            "Angelo Gaspar Diniz Nogueira",
            "Diego Kreutz",
            "Weverton Cordeiro",
            "Rodrigo Brandao Mansilha"
        ],
        "comments": "10 pages, 6 figures, 2 tables. Published at the Brazilian Symposium on Cybersecurity (SBSeg 2025)",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "High-quality data scarcity hinders malware detection, limiting ML performance. We introduce MalDataGen, an open-source modular framework for generating high-fidelity synthetic tabular data using modular deep learning models (e.g., WGAN-GP, VQ-VAE). Evaluated via dual validation (TR-TS/TS-TR), seven classifiers, and utility metrics, MalDataGen outperforms benchmarks like SDV while preserving data utility. Its flexible design enables seamless integration into detection pipelines, offering a practical solution for cybersecurity applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 160,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00366",
        "abs_url": "https://arxiv.org/abs/2511.00366",
        "pdf_url": "https://arxiv.org/pdf/2511.00366",
        "title": "A Streaming Sparse Cholesky Method for Derivative-Informed Gaussian Process Surrogates Within Digital Twin Applications",
        "authors": [
            "Krishna Prasath Logakannan",
            "Shridhar Vashishtha",
            "Jacob Hochhalter",
            "Shandian Zhe",
            "Robert M. Kirby"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)",
        "abstract": "Digital twins are developed to model the behavior of a specific physical asset (or twin), and they can consist of high-fidelity physics-based models or surrogates. A highly accurate surrogate is often preferred over multi-physics models as they enable forecasting the physical twin future state in real-time. To adapt to a specific physical twin, the digital twin model must be updated using in-service data from that physical twin. Here, we extend Gaussian process (GP) models to include derivative data, for improved accuracy, with dynamic updating to ingest physical twin data during service. Including derivative data, however, comes at a prohibitive cost of increased covariance matrix dimension. We circumvent this issue by using a sparse GP approximation, for which we develop extensions to incorporate derivatives. Numerical experiments demonstrate that the prediction accuracy of the derivative-enhanced sparse GP method produces improved models upon dynamic data additions. Lastly, we apply the developed algorithm within a DT framework to model fatigue crack growth in an aerospace vehicle.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 161,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00421",
        "abs_url": "https://arxiv.org/abs/2511.00421",
        "pdf_url": "https://arxiv.org/pdf/2511.00421",
        "title": "MedRECT: A Medical Reasoning Benchmark for Error Correction in Clinical Texts",
        "authors": [
            "Naoto Iwase",
            "Hiroki Okuyama",
            "Junichiro Iwasawa"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Large language models (LLMs) show increasing promise in medical applications, but their ability to detect and correct errors in clinical texts -- a prerequisite for safe deployment -- remains under-evaluated, particularly beyond English. We introduce MedRECT, a cross-lingual benchmark (Japanese/English) that formulates medical error handling as three subtasks: error detection, error localization (sentence extraction), and error correction. MedRECT is built with a scalable, automated pipeline from the Japanese Medical Licensing Examinations (JMLE) and a curated English counterpart, yielding MedRECT-ja (663 texts) and MedRECT-en (458 texts) with comparable error/no-error balance. We evaluate 9 contemporary LLMs spanning proprietary, open-weight, and reasoning families. Key findings: (i) reasoning models substantially outperform standard architectures, with up to 13.5% relative improvement in error detection and 51.0% in sentence extraction; (ii) cross-lingual evaluation reveals 5-10% performance gaps from English to Japanese, with smaller disparities for reasoning models; (iii) targeted LoRA fine-tuning yields asymmetric improvements in error correction performance (Japanese: +0.078, English: +0.168) while preserving reasoning capabilities; and (iv) our fine-tuned model exceeds human expert performance on structured medical error correction tasks. To our knowledge, MedRECT is the first comprehensive cross-lingual benchmark for medical error correction, providing a reproducible framework and resources for developing safer medical LLMs across languages.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 162,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00434",
        "abs_url": "https://arxiv.org/abs/2511.00434",
        "pdf_url": "https://arxiv.org/pdf/2511.00434",
        "title": "Trust-Region Methods with Low-Fidelity Objective Models",
        "authors": [
            "Andrea Angino",
            "Matteo Aurina",
            "Alena Kopaničáková",
            "Matthias Voigt",
            "Marco Donatelli",
            "Rolf Krause"
        ],
        "comments": "Submitted to the Proceedings of Domain Decomposition Methods in Science and Engineering XXIX",
        "subjects": "Numerical Analysis (math.NA); Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "We introduce two multifidelity trust-region methods based on the Magical Trust Region (MTR) framework. MTR augments the classical trust-region step with a secondary, informative direction. In our approaches, the secondary ``magical'' directions are determined by solving coarse trust-region subproblems based on low-fidelity objective models. The first proposed method, Sketched Trust-Region (STR), constructs this secondary direction using a sketched matrix to reduce the dimensionality of the trust-region subproblem. The second method, SVD Trust-Region (SVDTR), defines the magical direction via a truncated singular value decomposition of the dataset, capturing the leading directions of variability. Several numerical examples illustrate the potential gain in efficiency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 163,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00449",
        "abs_url": "https://arxiv.org/abs/2511.00449",
        "pdf_url": "https://arxiv.org/pdf/2511.00449",
        "title": "Towards Reliable Pediatric Brain Tumor Segmentation: Task-Specific nnU-Net Enhancements",
        "authors": [
            "Xiaolong Li",
            "Zhi-Qin John Xu",
            "Yan Ren",
            "Tianming Qiu",
            "Xiaowen Wang"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Accurate segmentation of pediatric brain tumors in multi-parametric magnetic resonance imaging (mpMRI) is critical for diagnosis, treatment planning, and monitoring, yet faces unique challenges due to limited data, high anatomical variability, and heterogeneous imaging across institutions. In this work, we present an advanced nnU-Net framework tailored for BraTS 2025 Task-6 (PED), the largest public dataset of pre-treatment pediatric high-grade gliomas. Our contributions include: (1) a widened residual encoder with squeeze-and-excitation (SE) attention; (2) 3D depthwise separable convolutions; (3) a specificity-driven regularization term; and (4) small-scale Gaussian weight initialization. We further refine predictions with two postprocessing steps. Our models achieved first place on the Task-6 validation leaderboard, attaining lesion-wise Dice scores of 0.759 (CC), 0.967 (ED), 0.826 (ET), 0.910 (NET), 0.928 (TC) and 0.928 (WT).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 164,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00490",
        "abs_url": "https://arxiv.org/abs/2511.00490",
        "pdf_url": "https://arxiv.org/pdf/2511.00490",
        "title": "Accuracy estimation of neural networks by extreme value theory",
        "authors": [
            "Gero Junike",
            "Marco Oesting"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Probability (math.PR)",
        "abstract": "Neural networks are able to approximate any continuous function on a compact set. However, it is not obvious how to quantify the error of the neural network, i.e., the remaining bias between the function and the neural network. Here, we propose the application of extreme value theory to quantify large values of the error, which are typically relevant in applications. The distribution of the error beyond some threshold is approximately generalized Pareto distributed. We provide a new estimator of the shape parameter of the Pareto distribution suitable to describe the error of neural networks. Numerical experiments are provided.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 165,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00537",
        "abs_url": "https://arxiv.org/abs/2511.00537",
        "pdf_url": "https://arxiv.org/pdf/2511.00537",
        "title": "Multi-refined Feature Enhanced Sentiment Analysis Using Contextual Instruction",
        "authors": [
            "Peter Atandoh",
            "Jie Zou",
            "Weikang Guo",
            "Jiwei Wei",
            "Zheng Wang"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Sentiment analysis using deep learning and pre-trained language models (PLMs) has gained significant traction due to their ability to capture rich contextual representations. However, existing approaches often underperform in scenarios involving nuanced emotional cues, domain shifts, and imbalanced sentiment distributions. We argue that these limitations stem from inadequate semantic grounding, poor generalization to diverse linguistic patterns, and biases toward dominant sentiment classes. To overcome these challenges, we propose CISEA-MRFE, a novel PLM-based framework integrating Contextual Instruction (CI), Semantic Enhancement Augmentation (SEA), and Multi-Refined Feature Extraction (MRFE). CI injects domain-aware directives to guide sentiment disambiguation; SEA improves robustness through sentiment-consistent paraphrastic augmentation; and MRFE combines a Scale-Adaptive Depthwise Encoder (SADE) for multi-scale feature specialization with an Emotion Evaluator Context Encoder (EECE) for affect-aware sequence modeling. Experimental results on four benchmark datasets demonstrate that CISEA-MRFE consistently outperforms strong baselines, achieving relative improvements in accuracy of up to 4.6% on IMDb, 6.5% on Yelp, 30.3% on Twitter, and 4.1% on Amazon. These results validate the effectiveness and generalization ability of our approach for sentiment classification across varied domains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 166,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00592",
        "abs_url": "https://arxiv.org/abs/2511.00592",
        "pdf_url": "https://arxiv.org/pdf/2511.00592",
        "title": "Agentic Auto-Scheduling: An Experimental Study of LLM-Guided Loop Optimization",
        "authors": [
            "Massinissa Merouani",
            "Islem Kara Bernou",
            "Riyadh Baghdadi"
        ],
        "comments": "Accepted at the 34th International Conference on Parallel Architectures and Compilation Techniques (PACT 2025). 12 pages, plus appendix",
        "subjects": "Programming Languages (cs.PL); Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG); Performance (cs.PF)",
        "abstract": "Automatic code optimization remains a difficult challenge, particularly for complex loop nests on modern hardware. This paper investigates a novel approach to code optimization where Large Language Models (LLMs) guide the process through a closed-loop interaction with a compiler. We present ComPilot, an experimental framework that leverages off-the-shelf LLMs, without any task-specific fine-tuning, as interactive optimization agents. ComPilot establishes a feedback loop where an LLM proposes transformations for a given loop nest to a compiler. The compiler attempts the transformations, reporting back legality status and measured speedup or slowdown. The LLM utilizes this concrete feedback to iteratively refine its optimization strategy. Our extensive evaluation across the PolyBench benchmark suite demonstrates the effectiveness of this zero-shot approach. ComPilot achieves geometric mean speedups of 2.66x (single run) and 3.54x (best-of-5 runs) over the original code. Furthermore, ComPilot demonstrates competitive performance against the state-of-the-art Pluto polyhedral optimizer, outperforming it in many cases. This experimental study demonstrates that general-purpose LLMs can effectively guide the code optimization process when grounded by compiler feedback, opening promising research directions for agentic AI in code optimization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 167,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00640",
        "abs_url": "https://arxiv.org/abs/2511.00640",
        "pdf_url": "https://arxiv.org/pdf/2511.00640",
        "title": "DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching",
        "authors": [
            "Zicheng Xu",
            "Guanchu Wang",
            "Yu-Neng Chuang",
            "Guangyao Zheng",
            "Alexander S. Szalay",
            "Zirui Liu",
            "Vladimir Braverman"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Large Reasoning Models (LRMs) demonstrate strong performance on complex reasoning tasks, yet they often suffer from overthinking, producing excessively long chain-of-thought (CoT) traces that increase inference cost and may degrade accuracy. Our analysis reveals a clear anti-correlation between reasoning length and accuracy, where across multiple stochastic decodes, the short reasoning paths consistently achieve the highest correctness, while longer ones accumulate errors and repetitions. These short optimal reasoning paths can be found ideally through full enumeration of the reasoning space. However, the tree-structured reasoning space grows exponentially with sequence length, rendering exhaustive exploration infeasible. To address this, we propose DTS, a model-agnostic decoding framework that sketches the reasoning space by selectively branching at high-entropy tokens and applies early stopping to select the shortest completed reasoning path. This approach approximates the optimal solution that enhances both efficiency and accuracy, without requiring additional training or supervision. Experiments on AIME2024 and AIME2025 datasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves accuracy by up to 8%, reduces average reasoning length by 23%, and decreases repetition frequency by 12%, demonstrating DTS's ability for scalable and efficient LRM reasoning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 168,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00670",
        "abs_url": "https://arxiv.org/abs/2511.00670",
        "pdf_url": "https://arxiv.org/pdf/2511.00670",
        "title": "Filtered Neural Galerkin model reduction schemes for efficient propagation of initial condition uncertainties in digital twins",
        "authors": [
            "Zhiyang Ning",
            "Benjamin Peherstorfer"
        ],
        "comments": "",
        "subjects": "Numerical Analysis (math.NA); Machine Learning (cs.LG)",
        "abstract": "Uncertainty quantification in digital twins is critical to enable reliable and credible predictions beyond available data. A key challenge is that ensemble-based approaches can become prohibitively expensive when embedded in control and data assimilation loops in digital twins, even when reduced models are used. We introduce a reduced modeling approach that advances in time the mean and covariance of the reduced solution distribution induced by the initial condition uncertainties, which eliminates the need to maintain and propagate a costly ensemble of reduced solutions. The mean and covariance dynamics are obtained as a moment closure from Neural Galerkin schemes on pre-trained neural networks, which can be interpreted as filtered Neural Galerkin dynamics analogous to Gaussian filtering and the extended Kalman filter. Numerical experiments demonstrate that filtered Neural Galerkin schemes achieve more than one order of magnitude speedup compared to ensemble-based uncertainty propagation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 169,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00681",
        "abs_url": "https://arxiv.org/abs/2511.00681",
        "pdf_url": "https://arxiv.org/pdf/2511.00681",
        "title": "Metadata-Aligned 3D MRI Representations for Contrast Understanding and Quality Control",
        "authors": [
            "Mehmet Yigit Avci",
            "Pedro Borges",
            "Virginia Fernandez",
            "Paul Wright",
            "Mehmet Yigitsoy",
            "Sebastien Ourselin",
            "Jorge Cardoso"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Magnetic Resonance Imaging suffers from substantial data heterogeneity and the absence of standardized contrast labels across scanners, protocols, and institutions, which severely limits large-scale automated analysis. A unified representation of MRI contrast would enable a wide range of downstream utilities, from automatic sequence recognition to harmonization and quality control, without relying on manual annotations. To this end, we introduce MR-CLIP, a metadata-guided framework that learns MRI contrast representations by aligning volumetric images with their DICOM acquisition parameters. The resulting embeddings shows distinct clusters of MRI sequences and outperform supervised 3D baselines under data scarcity in few-shot sequence classification. Moreover, MR-CLIP enables unsupervised data quality control by identifying corrupted or inconsistent metadata through image-metadata embedding distances. By transforming routinely available acquisition metadata into a supervisory signal, MR-CLIP provides a scalable foundation for label-efficient MRI analysis across diverse clinical datasets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 170,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00685",
        "abs_url": "https://arxiv.org/abs/2511.00685",
        "pdf_url": "https://arxiv.org/pdf/2511.00685",
        "title": "SOCRATES: Simulation Optimization with Correlated Replicas and Adaptive Trajectory Evaluations",
        "authors": [
            "Haoting Zhang",
            "Haoxian Chen",
            "Donglin Zhan",
            "Hanyang Zhao",
            "Henry Lam",
            "Wenpin Tang",
            "David Yao",
            "Zeyu Zheng"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "The field of simulation optimization (SO) encompasses various methods developed to optimize complex, expensive-to-sample stochastic systems. Established methods include, but are not limited to, ranking-and-selection for finite alternatives and surrogate-based methods for continuous domains, with broad applications in engineering and operations management. The recent advent of large language models (LLMs) offers a new paradigm for exploiting system structure and automating the strategic selection and composition of these established SO methods into a tailored optimization procedure. This work introduces SOCRATES (Simulation Optimization with Correlated Replicas and Adaptive Trajectory Evaluations), a novel two-stage procedure that leverages LLMs to automate the design of tailored SO algorithms. The first stage constructs an ensemble of digital replicas of the real system. An LLM is employed to implement causal discovery from a textual description of the system, generating a structural `skeleton' that guides the sample-efficient learning of the replicas. In the second stage, this replica ensemble is used as an inexpensive testbed to evaluate a set of baseline SO algorithms. An LLM then acts as a meta-optimizer, analyzing the performance trajectories of these algorithms to iteratively revise and compose a final, hybrid optimization schedule. This schedule is designed to be adaptive, with the ability to be updated during the final execution on the real system when the optimization performance deviates from expectations. By integrating LLM-driven reasoning with LLM-assisted trajectory-aware meta-optimization, SOCRATES creates an effective and sample-efficient solution for complex SO optimization problems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 171,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00746",
        "abs_url": "https://arxiv.org/abs/2511.00746",
        "pdf_url": "https://arxiv.org/pdf/2511.00746",
        "title": "Correspondence Between Ising Machines and Neural Networks",
        "authors": [
            "Andrew G. Moore"
        ],
        "comments": "22 pages, 4 figures",
        "subjects": "Disordered Systems and Neural Networks (cond-mat.dis-nn); Emerging Technologies (cs.ET); Machine Learning (cs.LG); Quantum Physics (quant-ph)",
        "abstract": "Computation with the Ising model is central to future computing technologies like quantum annealing, adiabatic quantum computing, and thermodynamic classical computing. Traditionally, computed values have been equated with ground states. This paper generalizes computation with ground states to computation with spin averages, allowing computations to take place at high temperatures. It then introduces a systematic correspondence between Ising devices and neural networks and a simple method to run trained feed-forward neural networks on Ising-type hardware. Finally, a mathematical proof is offered that these implementations are always successful.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 172,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00750",
        "abs_url": "https://arxiv.org/abs/2511.00750",
        "pdf_url": "https://arxiv.org/pdf/2511.00750",
        "title": "Trust Region-Based Bayesian Optimisation to Discover Diverse Solutions",
        "authors": [
            "Kokila Kasuni Perera",
            "Frank Neumann",
            "Aneta Neumann"
        ],
        "comments": "",
        "subjects": "Neural and Evolutionary Computing (cs.NE); Machine Learning (cs.LG)",
        "abstract": "Bayesian optimisation (BO) is a surrogate-based optimisation technique that efficiently solves expensive black-box functions with small evaluation budgets. Recent studies consider trust regions to improve the scalability of BO approaches when the problem space scales to more dimensions. Motivated by this research, we explore the effectiveness of trust region-based BO algorithms for diversity optimisation in different dimensional black box problems. We propose diversity optimisation approaches extending TuRBO1, which is the first BO method that uses a trust region-based approach for scalability. We extend TuRBO1 as divTuRBO1, which finds an optimal solution while maintaining a given distance threshold relative to a reference solution set. We propose two approaches to find diverse solutions for black-box functions by combining divTuRBO1 runs in a sequential and an interleaving fashion. We conduct experimental investigations on the proposed algorithms and compare their performance with that of the baseline method, ROBOT (rank-ordered Bayesian optimisation with trust regions). We evaluate proposed algorithms on benchmark functions with dimensions 2 to 20. Experimental investigations demonstrate that the proposed methods perform well, particularly in larger dimensions, even with a limited evaluation budget.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 173,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00768",
        "abs_url": "https://arxiv.org/abs/2511.00768",
        "pdf_url": "https://arxiv.org/pdf/2511.00768",
        "title": "A Framework Based on Graph Cellular Automata for Similarity Evaluation in Urban Spatial Networks",
        "authors": [
            "Peiru Wu",
            "Maojun Zhai",
            "Lingzhu Zhang"
        ],
        "comments": "",
        "subjects": "Social and Information Networks (cs.SI); Machine Learning (cs.LG)",
        "abstract": "Measuring similarity in urban spatial networks is key to understanding cities as complex systems. Yet most existing methods are not tailored for spatial networks and struggle to differentiate them effectively. We propose GCA-Sim, a similarity-evaluation framework based on graph cellular automata. Each submodel measures similarity by the divergence between value distributions recorded at multiple stages of an information evolution process. We find that some propagation rules magnify differences among network signals; we call this \"network resonance.\" With an improved differentiable logic-gate network, we learn several submodels that induce network resonance. We evaluate similarity through clustering performance on fifty city-level and fifty district-level road networks. The submodels in this framework outperform existing methods, with Silhouette scores above 0.9. Using the best submodel, we further observe that planning-led street networks are less internally homogeneous than organically grown ones; morphological categories from different domains contribute with comparable importance; and degree, as a basic topological signal, becomes increasingly aligned with land value and related variables over iterations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 174,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00772",
        "abs_url": "https://arxiv.org/abs/2511.00772",
        "pdf_url": "https://arxiv.org/pdf/2511.00772",
        "title": "Reliable Curation of EHR Dataset via Large Language Models under Environmental Constraints",
        "authors": [
            "Raymond M. Xiong",
            "Panyu Chen",
            "Tianze Dong",
            "Jian Lu",
            "Benjamin Goldstein",
            "Danyang Zhuo",
            "Anru R. Zhang"
        ],
        "comments": "",
        "subjects": "Databases (cs.DB); Machine Learning (cs.LG); Applications (stat.AP)",
        "abstract": "Electronic health records (EHRs) are central to modern healthcare delivery and research; yet, many researchers lack the database expertise necessary to write complex SQL queries or generate effective visualizations, limiting efficient data use and scientific discovery. To address this barrier, we introduce CELEC, a large language model (LLM)-powered framework for automated EHR data extraction and analytics. CELEC translates natural language queries into SQL using a prompting strategy that integrates schema information, few-shot demonstrations, and chain-of-thought reasoning, which together improve accuracy and robustness. On a subset of the EHRSQL benchmark, CELEC achieves execution accuracy comparable to prior systems while maintaining low latency, cost efficiency, and strict privacy by exposing only database metadata to the LLM. CELEC also adheres to strict privacy protocols: the LLM accesses only database metadata (e.g., table and column names), while all query execution occurs securely within the institutional environment, ensuring that no patient-level data is ever transmitted to or shared with the LLM. Ablation studies confirm that each component of the SQL generation pipeline, particularly the few-shot demonstrations, plays a critical role in performance. By lowering technical barriers and enabling medical researchers to query EHR databases directly, CELEC streamlines research workflows and accelerates biomedical discovery.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 175,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00796",
        "abs_url": "https://arxiv.org/abs/2511.00796",
        "pdf_url": "https://arxiv.org/pdf/2511.00796",
        "title": "AReaL-Hex: Accommodating Asynchronous RL Training over Heterogeneous GPUs",
        "authors": [
            "Ran Yan",
            "Youhe Jiang",
            "Tianyuan Wu",
            "Jiaxuan Gao",
            "Zhiyu Mei",
            "Wei Fu",
            "Haohui Mai",
            "Wei Wang",
            "Yi Wu",
            "Binhang Yuan"
        ],
        "comments": "",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)",
        "abstract": "Maximizing training throughput and cost-efficiency of RL for LLMs is essential to democratize this advanced technique. One promising but challenging approach is to deploy such a computational workflow over heterogeneous GPUs. Unlike conventional large-scale LLM pretraining, RL training generally decomposes into three coupled stages, i.e., rollout generation, reward computation, and policy/value updates, which exhibit markedly different compute intensities, memory footprints, and communication patterns. Recent research shows that fully asynchronous RL training can disaggregate these stages across disjoint hardware pools without sacrificing training stability, creating a great opportunity for real-world heterogeneous deployment. To this end, we present AReaL-Hex, a heterogeneity-aware asynchronous RL training system that effectively schedules how to execute rollout generation and policy model training over heterogeneous GPUs while enforcing data staleness bounds. Concretely, we use a two-phase scheduler: (i) a constrained search with MILP to select per-stage parallelization strategies and workload assignments given a resource budget, and (ii) a graph-partitioning step that allocates heterogeneous GPUs and interconnects to maximize end-to-end throughput. Built atop a fully asynchronous RL architecture, AReaL-Hex maps HBM-I/O-bound generation and compute-bound optimization to more cost-efficient resources and balances their producer-consumer interactions to avoid both idleness and stale rollout trajectories. On the mathematical reasoning task with various model scales (1.5B, 7B, and 14B), compared to homogeneous deployments of state-of-the-art asynchronous RL systems: (i) When maintaining the same total budgets, AReaL-Hex delivers up to 1.50x higher training throughput; (ii) When achieving the same training throughput, AReaL-Hex results in up to 1.46x reduction in training cost.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 176,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00802",
        "abs_url": "https://arxiv.org/abs/2511.00802",
        "pdf_url": "https://arxiv.org/pdf/2511.00802",
        "title": "GrowthHacker: Automated Off-Policy Evaluation Optimization Using Code-Modifying LLM Agents",
        "authors": [
            "Jie JW Wu",
            "Ayanda Patrick Herlihy",
            "Ahmad Saleem Mirza",
            "Ali Afoud",
            "Fatemeh Fard"
        ],
        "comments": "",
        "subjects": "Software Engineering (cs.SE); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "With the software industry shifting toward a data-driven culture, online A/B testing is a key tool for evaluating new technologies. However, deploying such experiments requires substantial resources, may negatively impact users, and involves long data collection periods. To address this, \\textit{off-policy evaluation (OPE)}, or offline A/B testing, uses logged data to assess technologies and is fundamental in Reinforcement Learning, making it crucial in domains where online testing is costly or risky, such as healthcare, recommender systems, education, dialog systems, and robotics. Despite advances in coding LLMs and agentic AI, little is known about leveraging them to optimize OPE results. We investigate whether LLMs and LLM-based agents can improve OPE performance via code optimization. We propose \\textit{GrowthHacker}, a benchmark with agent and baseline methods on large-scale real-world datasets, which iteratively optimizes code, evaluates results, and begins new optimization cycles. We collected datasets, established protocols, implemented baselines for OPE on the Open Bandit Pipeline (OBP)~\\cite{saito2021openbanditdatasetpipeline} and Scope-RL~\\cite{kiyohara2023scope}, and developed the \\textit{two_agent} framework, which reduces system complexity while preserving optimization effectiveness. Results show the two_agent framework achieves 100% reliability and the highest average improvement of 106.7% among positive outcomes. Both two_agent and CrewAI reach 45% success rates, outperforming AutoGen's 34%. These findings demonstrate the feasibility of LLM-based agents as automated \"growth hackers\" to enhance OPE systems, with implications for scaling data-driven decision-making in production.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 177,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00814",
        "abs_url": "https://arxiv.org/abs/2511.00814",
        "pdf_url": "https://arxiv.org/pdf/2511.00814",
        "title": "Real-Time Learning of Predictive Dynamic Obstacle Models for Robotic Motion Planning",
        "authors": [
            "Stella Kombo",
            "Masih Haseli",
            "Skylar Wei",
            "Joel W. Burdick"
        ],
        "comments": "10 pages, 6 figures, submitted to IEEE International Conference on Robotics and Automation (ICRA) 2025",
        "subjects": "Robotics (cs.RO); Machine Learning (cs.LG); Systems and Control (eess.SY)",
        "abstract": "Autonomous systems often must predict the motions of nearby agents from partial and noisy data. This paper asks and answers the question: \"can we learn, in real-time, a nonlinear predictive model of another agent's motions?\" Our online framework denoises and forecasts such dynamics using a modified sliding-window Hankel Dynamic Mode Decomposition (Hankel-DMD). Partial noisy measurements are embedded into a Hankel matrix, while an associated Page matrix enables singular-value hard thresholding (SVHT) to estimate the effective rank. A Cadzow projection enforces structured low-rank consistency, yielding a denoised trajectory and local noise variance estimates. From this representation, a time-varying Hankel-DMD lifted linear predictor is constructed for multi-step forecasts. The residual analysis provides variance-tracking signals that can support downstream estimators and risk-aware planning. We validate the approach in simulation under Gaussian and heavy-tailed noise, and experimentally on a dynamic crane testbed. Results show that the method achieves stable variance-aware denoising and short-horizon prediction suitable for integration into real-time control frameworks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 178,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00849",
        "abs_url": "https://arxiv.org/abs/2511.00849",
        "pdf_url": "https://arxiv.org/pdf/2511.00849",
        "title": "Perturbations in the Orthogonal Complement Subspace for Efficient Out-of-Distribution Detection",
        "authors": [
            "Zhexiao Huang",
            "Weihao He",
            "Shutao Deng",
            "Junzhe Chen",
            "Chao Yuan",
            "Hongxin Wang",
            "Changsheng Zhou"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Out-of-distribution (OOD) detection is essential for deploying deep learning models in open-world environments. Existing approaches, such as energy-based scoring and gradient-projection methods, typically rely on high-dimensional representations to separate in-distribution (ID) and OOD samples. We introduce P-OCS (Perturbations in the Orthogonal Complement Subspace), a lightweight and theoretically grounded method that operates in the orthogonal complement of the principal subspace defined by ID features. P-OCS applies a single projected perturbation restricted to this complementary subspace, enhancing subtle ID-OOD distinctions while preserving the geometry of ID representations. We show that a one-step update is sufficient in the small-perturbation regime and provide convergence guarantees for the resulting detection score. Experiments across multiple architectures and datasets demonstrate that P-OCS achieves state-of-the-art OOD detection with negligible computational cost and without requiring model retraining, access to OOD data, or changes to model architecture.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 179,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00875",
        "abs_url": "https://arxiv.org/abs/2511.00875",
        "pdf_url": "https://arxiv.org/pdf/2511.00875",
        "title": "Controlling Gender Bias in Retrieval via a Backpack Architecture",
        "authors": [
            "Amirabbas Afzali",
            "Amirreza Velae",
            "Iman Ahmadi",
            "Mohammad Aliannejadi"
        ],
        "comments": "",
        "subjects": "Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "The presence of social biases in large language models (LLMs) has become a significant concern in AI research. These biases, often embedded in training data, can perpetuate harmful stereotypes and distort decision-making processes. When LLMs are integrated into ranking systems, they can propagate these biases, leading to unfair outcomes in critical applications such as search engines and recommendation systems. Backpack Language Models, unlike traditional transformer-based models that treat text sequences as monolithic structures, generate outputs as weighted combinations of non-contextual, learned word aspects, also known as senses. Leveraging this architecture, we propose a framework for debiasing ranking tasks. Our experimental results show that this framework effectively mitigates gender bias in text retrieval and ranking with minimal degradation in performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 180,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00881",
        "abs_url": "https://arxiv.org/abs/2511.00881",
        "pdf_url": "https://arxiv.org/pdf/2511.00881",
        "title": "Deep Generative Models for Enhanced Vitreous OCT Imaging",
        "authors": [
            "Simone Sarrocco",
            "Philippe C. Cattin",
            "Peter M. Maloca",
            "Paul Friedrich",
            "Philippe Valmaggia"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Purpose: To evaluate deep learning (DL) models for enhancing vitreous optical coherence tomography (OCT) image quality and reducing acquisition time. Methods: Conditional Denoising Diffusion Probabilistic Models (cDDPMs), Brownian Bridge Diffusion Models (BBDMs), U-Net, Pix2Pix, and Vector-Quantised Generative Adversarial Network (VQ-GAN) were used to generate high-quality spectral-domain (SD) vitreous OCT images. Inputs were SD ART10 images, and outputs were compared to pseudoART100 images obtained by averaging ten ART10 images per eye location. Model performance was assessed using image quality metrics and Visual Turing Tests, where ophthalmologists ranked generated images and evaluated anatomical fidelity. The best model's performance was further tested within the manually segmented vitreous on newly acquired data. Results: U-Net achieved the highest Peak Signal-to-Noise Ratio (PSNR: 30.230) and Structural Similarity Index Measure (SSIM: 0.820), followed by cDDPM. For Learned Perceptual Image Patch Similarity (LPIPS), Pix2Pix (0.697) and cDDPM (0.753) performed best. In the first Visual Turing Test, cDDPM ranked highest (3.07); in the second (best model only), cDDPM achieved a 32.9% fool rate and 85.7% anatomical preservation. On newly acquired data, cDDPM generated vitreous regions more similar in PSNR to the ART100 reference than true ART1 or ART10 B-scans and achieved higher PSNR on whole images when conditioned on ART1 than ART10. Conclusions: Results reveal discrepancies between quantitative metrics and clinical evaluation, highlighting the need for combined assessment. cDDPM showed strong potential for generating clinically meaningful vitreous OCT images while reducing acquisition time fourfold. Translational Relevance: cDDPMs show promise for clinical integration, supporting faster, higher-quality vitreous imaging. Dataset and code will be made publicly available.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 181,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00886",
        "abs_url": "https://arxiv.org/abs/2511.00886",
        "pdf_url": "https://arxiv.org/pdf/2511.00886",
        "title": "HEATNETs: Explainable Random Feature Neural Networks for High-Dimensional Parabolic PDEs",
        "authors": [
            "Kyriakos Georgiou",
            "Gianluca Fabiani",
            "Constantinos Siettos",
            "Athanasios N. Yannacopoulos"
        ],
        "comments": "",
        "subjects": "Numerical Analysis (math.NA); Machine Learning (cs.LG)",
        "abstract": "We deal with the solution of the forward problem for high-dimensional parabolic PDEs with random feature (projection) neural networks (RFNNs). We first prove that there exists a single-hidden layer neural network with randomized heat-kernels arising from the fundamental solution (Green's functions) of the heat operator, that we call HEATNET, that provides an unbiased universal approximator to the solution of parabolic PDEs in arbitrary (high) dimensions, with the rate of convergence being analogous to the ${O}(N^{-1/2})$, where $N$ is the size of HEATNET. Thus, HEATNETs are explainable schemes, based on the analytical framework of parabolic PDEs, exploiting insights from physics-informed neural networks aided by numerical and functional analysis, and the structure of the corresponding solution operators. Importantly, we show how HEATNETs can be scaled up for the efficient numerical solution of arbitrary high-dimensional parabolic PDEs using suitable transformations and importance Monte Carlo sampling of the integral representation of the solution, in order to deal with the singularities of the heat kernel around the collocation points. We evaluate the performance of HEATNETs through benchmark linear parabolic problems up to 2,000 dimensions. We show that HEATNETs result in remarkable accuracy with the order of the approximation error ranging from $1.0E-05$ to $1.0E-07$ for problems up to 500 dimensions, and of the order of $1.0E-04$ to $1.0E-03$ for 1,000 to 2,000 dimensions, with a relatively low number (up to 15,000) of features.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 182,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00919",
        "abs_url": "https://arxiv.org/abs/2511.00919",
        "pdf_url": "https://arxiv.org/pdf/2511.00919",
        "title": "Towards Channel Charting Enhancement with Non-Reconfigurable Intelligent Surfaces",
        "authors": [
            "Mahdi Maleki",
            "Reza Agahzadeh Ayoubi",
            "Marouan Mizmizi",
            "Umberto Spagnolini"
        ],
        "comments": "",
        "subjects": "Signal Processing (eess.SP); Machine Learning (cs.LG)",
        "abstract": "We investigate how fully-passive electromagnetic skins (EMSs) can be engineered to enhance channel charting (CC) in dense urban environments. We employ two complementary state-of-the-art CC techniques, semi-supervised t-distributed stochastic neighbor embedding (t-SNE) and a semi-supervised Autoencoder (AE), to verify the consistency of results across nonparametric and parametric mappings. We show that the accuracy of CC hinges on a balance between signal-to-noise ratio (SNR) and spatial dissimilarity: EMS codebooks that only maximize gain, as in conventional Reconfigurable Intelligent Surface (RIS) optimization, suppress location fingerprints and degrade CC, while randomized phases increase diversity but reduce SNR. To address this trade-off, we design static EMS phase profiles via a quantile-driven criterion that targets worst-case users and improves both trustworthiness and continuity. In a 3D ray-traced city at 30 GHz, the proposed EMS reduces the 90th-percentile localization error from > 50 m to < 25 m for both t-SNE and AE-based CC, and decreases severe trajectory dropouts by over 4x under 15% supervision. The improvements hold consistently across the evaluated configurations, establishing static, pre-configured EMS as a practical enabler of CC without reconfiguration overheads.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 183,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.00999",
        "abs_url": "https://arxiv.org/abs/2511.00999",
        "pdf_url": "https://arxiv.org/pdf/2511.00999",
        "title": "Transformer-Based Decoding in Concatenated Coding Schemes Under Synchronization Errors",
        "authors": [
            "Julian Streit",
            "Franziska Weindel",
            "Reinhard Heckel"
        ],
        "comments": "16 pages, 19 figures, a shortened version was published in the ISIT 2025 conference",
        "subjects": "Information Theory (cs.IT); Machine Learning (cs.LG)",
        "abstract": "We consider the reconstruction of a codeword from multiple noisy copies that are independently corrupted by insertions, deletions, and substitutions. This problem arises, for example, in DNA data storage. A common code construction uses a concatenated coding scheme that combines an outer linear block code with an inner code, which can be either a nonlinear marker code or a convolutional code. Outer decoding is done with Belief Propagation, and inner decoding is done with the Bahl-Cocke-Jelinek-Raviv (BCJR) algorithm. However, the BCJR algorithm scales exponentially with the number of noisy copies, which makes it infeasible to reconstruct a codeword from more than about four copies. In this work, we introduce BCJRFormer, a transformer-based neural inner decoder. BCJRFormer achieves error rates comparable to the BCJR algorithm for binary and quaternary single-message transmissions of marker codes. Importantly, BCJRFormer scales quadratically with the number of noisy copies. This property makes BCJRFormer well-suited for DNA data storage, where multiple reads of the same DNA strand occur. To lower error rates, we replace the Belief Propagation outer decoder with a transformer-based decoder. Together, these modifications yield an efficient and performant end-to-end transformer-based pipeline for decoding multiple noisy copies affected by insertion, deletion, and substitution errors. Additionally, we propose a novel cross-attending transformer architecture called ConvBCJRFormer. This architecture extends BCJRFormer to decode transmissions of convolutional codewords, serving as an initial step toward joint inner and outer decoding for more general linear code classes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 184,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01019",
        "abs_url": "https://arxiv.org/abs/2511.01019",
        "pdf_url": "https://arxiv.org/pdf/2511.01019",
        "title": "OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights",
        "authors": [
            "Bowen Chen",
            "Jayesh Gajbhar",
            "Gregory Dusek",
            "Rob Redmon",
            "Patrick Hogan",
            "Paul Liu",
            "DelWayne Bohnenstiehl",
            "Dongkuan",
            "Ruoying He"
        ],
        "comments": "A related presentation will be given at the AGU(American Geophysical Union) and AMS(American Meteorological Society) Annual Meetings",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG); Atmospheric and Oceanic Physics (physics.ao-ph)",
        "abstract": "Artificial intelligence is transforming the sciences, yet general conversational AI systems often generate unverified \"hallucinations\" undermining scientific rigor. We present OceanAI, a conversational platform that integrates the natural-language fluency of open-source large language models (LLMs) with real-time, parameterized access to authoritative oceanographic data streams hosted by the National Oceanic and Atmospheric Administration (NOAA). Each query such as \"What was Boston Harbor's highest water level in 2024?\" triggers real-time API calls that identify, parse, and synthesize relevant datasets into reproducible natural-language responses and data visualizations. In a blind comparison with three widely used AI chat-interface products, only OceanAI produced NOAA-sourced values with original data references; others either declined to answer or provided unsupported results. Designed for extensibility, OceanAI connects to multiple NOAA data products and variables, supporting applications in marine hazard forecasting, ecosystem assessment, and water-quality monitoring. By grounding outputs and verifiable observations, OceanAI advances transparency, reproducibility, and trust, offering a scalable framework for AI-enabled decision support within the oceans. A public demonstration is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 185,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01037",
        "abs_url": "https://arxiv.org/abs/2511.01037",
        "pdf_url": "https://arxiv.org/pdf/2511.01037",
        "title": "Binary perceptron computational gap -- a parametric fl RDT view",
        "authors": [
            "Mihailo Stojnic"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Disordered Systems and Neural Networks (cond-mat.dis-nn); Information Theory (cs.IT); Machine Learning (cs.LG); Probability (math.PR)",
        "abstract": "Recent studies suggest that asymmetric binary perceptron (ABP) likely exhibits the so-called statistical-computational gap characterized with the appearance of two phase transitioning constraint density thresholds: \\textbf{\\emph{(i)}} the \\emph{satisfiability threshold} $\\alpha_c$, below/above which ABP succeeds/fails to operate as a storage memory; and \\textbf{\\emph{(ii)}} \\emph{algorithmic threshold} $\\alpha_a$, below/above which one can/cannot efficiently determine ABP's weight so that it operates as a storage memory. We consider a particular parametric utilization of \\emph{fully lifted random duality theory} (fl RDT) [85] and study its potential ABP's algorithmic implications. A remarkable structural parametric change is uncovered as one progresses through fl RDT lifting levels. On the first two levels, the so-called $\\c$ sequence -- a key parametric fl RDT component -- is of the (natural) decreasing type. A change of such phenomenology on higher levels is then connected to the $\\alpha_c$ -- $\\alpha_a$ threshold change. Namely, on the second level concrete numerical values give for the critical constraint density $\\alpha=\\alpha_c\\approx 0.8331$. While progressing through higher levels decreases this estimate, already on the fifth level we observe a satisfactory level of convergence and obtain $\\alpha\\approx 0.7764$. This allows to draw two striking parallels: \\textbf{\\emph{(i)}} the obtained constraint density estimate is in a remarkable agrement with range $\\alpha\\in (0.77,0.78)$ of clustering defragmentation (believed to be responsible for failure of locally improving algorithms) [17,88]; and \\textbf{\\emph{(ii)}} the observed change of $\\c$ sequence phenomenology closely matches the one of the negative Hopfield model for which the existence of efficient algorithms that closely approach similar type of threshold has been demonstrated recently [87].",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 186,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01064",
        "abs_url": "https://arxiv.org/abs/2511.01064",
        "pdf_url": "https://arxiv.org/pdf/2511.01064",
        "title": "Generalized Guarantees for Variational Inference in the Presence of Even and Elliptical Symmetry",
        "authors": [
            "Charles C. Margossian",
            "Lawrence K. Saul"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Computation (stat.CO)",
        "abstract": "We extend several recent results providing symmetry-based guarantees for variational inference (VI) with location-scale families. VI approximates a target density~$p$ by the best match $q^*$ in a family $Q$ of tractable distributions that in general does not contain $p$. It is known that VI can recover key properties of $p$, such as its mean and correlation matrix, when $p$ and $Q$ exhibit certain symmetries and $q^*$ is found by minimizing the reverse Kullback-Leibler divergence. We extend these guarantees in two important directions. First, we provide symmetry-based guarantees for a broader family of divergences, highlighting the properties of variational objectives under which VI provably recovers the mean and correlation matrix. Second, we obtain further guarantees for VI when the target density $p$ exhibits even and elliptical symmetries in some but not all of its coordinates. These partial symmetries arise naturally in Bayesian hierarchical models, where the prior induces a challenging geometry but still possesses axes of symmetry. We illustrate these theoretical results in a number of experimental settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 187,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01087",
        "abs_url": "https://arxiv.org/abs/2511.01087",
        "pdf_url": "https://arxiv.org/pdf/2511.01087",
        "title": "SliceVision-F2I: A Synthetic Feature-to-Image Dataset for Visual Pattern Representation on Network Slices",
        "authors": [
            "Md. Abid Hasan Rafi",
            "Mst. Fatematuj Johora",
            "Pankaj Bhowmik"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "The emergence of 5G and 6G networks has established network slicing as a significant part of future service-oriented architectures, demanding refined identification methods supported by robust datasets. The article presents SliceVision-F2I, a dataset of synthetic samples for studying feature visualization in network slicing for next-generation networking systems. The dataset transforms multivariate Key Performance Indicator (KPI) vectors into visual representations through four distinct encoding methods: physically inspired mappings, Perlin noise, neural wallpapering, and fractal branching. For each encoding method, 30,000 samples are generated, each comprising a raw KPI vector and a corresponding RGB image at low-resolution pixels. The dataset simulates realistic and noisy network conditions to reflect operational uncertainties and measurement imperfections. SliceVision-F2I is suitable for tasks involving visual learning, network state classification, anomaly detection, and benchmarking of image-based machine learning techniques applied to network data. The dataset is publicly available and can be reused in various research contexts, including multivariate time series analysis, synthetic data generation, and feature-to-image transformations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 188,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01096",
        "abs_url": "https://arxiv.org/abs/2511.01096",
        "pdf_url": "https://arxiv.org/pdf/2511.01096",
        "title": "Hyper Hawkes Processes: Interpretable Models of Marked Temporal Point Processes",
        "authors": [
            "Alex Boyd",
            "Andrew Warrington",
            "Taha Kass-Hout",
            "Parminder Bhatia",
            "Danica Xiao"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Foundational marked temporal point process (MTPP) models, such as the Hawkes process, often use inexpressive model families in order to offer interpretable parameterizations of event data. On the other hand, neural MTPPs models forego this interpretability in favor of absolute predictive performance. In this work, we present a new family MTPP models: the hyper Hawkes process (HHP), which aims to be as flexible and performant as neural MTPPs, while retaining interpretable aspects. To achieve this, the HHP extends the classical Hawkes process to increase its expressivity by first expanding the dimension of the process into a latent space, and then introducing a hypernetwork to allow time- and data-dependent dynamics. These extensions define a highly performant MTPP family, achieving state-of-the-art performance across a range of benchmark tasks and metrics. Furthermore, by retaining the linearity of the recurrence, albeit now piecewise and conditionally linear, the HHP also retains much of the structure of the original Hawkes process, which we exploit to create direct probes into how the model creates predictions. HHP models therefore offer both state-of-the-art predictions, while also providing an opportunity to ``open the box'' and inspect how predictions were generated.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 189,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01107",
        "abs_url": "https://arxiv.org/abs/2511.01107",
        "pdf_url": "https://arxiv.org/pdf/2511.01107",
        "title": "SLAP: Shortcut Learning for Abstract Planning",
        "authors": [
            "Y. Isabel Liu",
            "Bowen Li",
            "Benjamin Eysenbach",
            "Tom Silver"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
        "abstract": "Long-horizon decision-making with sparse rewards and continuous states and actions remains a fundamental challenge in AI and robotics. Task and motion planning (TAMP) is a model-based framework that addresses this challenge by planning hierarchically with abstract actions (options). These options are manually defined, limiting the agent to behaviors that we as human engineers know how to program (pick, place, move). In this work, we propose Shortcut Learning for Abstract Planning (SLAP), a method that leverages existing TAMP options to automatically discover new ones. Our key idea is to use model-free reinforcement learning (RL) to learn shortcuts in the abstract planning graph induced by the existing options in TAMP. Without any additional assumptions or inputs, shortcut learning leads to shorter solutions than pure planning, and higher task success rates than flat and hierarchical RL. Qualitatively, SLAP discovers dynamic physical improvisations (e.g., slap, wiggle, wipe) that differ significantly from the manually-defined ones. In experiments in four simulated robotic environments, we show that SLAP solves and generalizes to a wide range of tasks, reducing overall plan lengths by over 50% and consistently outperforming planning and RL baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 190,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01118",
        "abs_url": "https://arxiv.org/abs/2511.01118",
        "pdf_url": "https://arxiv.org/pdf/2511.01118",
        "title": "Generative Machine Learning Models for the Deconvolution of Charge Carrier Dynamics in Organic Photovoltaic Cells",
        "authors": [
            "Li Raymond",
            "Salim Flora",
            "Wang Sijin",
            "Wright Brendan"
        ],
        "comments": "",
        "subjects": "Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG)",
        "abstract": "Charge carrier dynamics critically affect the efficiency and stability of organic photovoltaic devices, but they are challenging to model with traditional analytical methods. We introduce \\b{eta}-Linearly Decoded Latent Ordinary Differential Equations (\\b{eta}-LLODE), a machine learning framework that disentangles and reconstructs extraction dynamics from time-resolved charge extraction measurements of P3HT:PCBM cells. This model enables the isolated analysis of the underlying charge carrier behaviour, which was found to be well described by a compressed exponential decay. Furthermore, the learnt interpretable latent space enables simulation, including both interpolation and extrapolation of experimental measurement conditions, offering a predictive tool for solar cell research to support device study and optimisation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 191,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01140",
        "abs_url": "https://arxiv.org/abs/2511.01140",
        "pdf_url": "https://arxiv.org/pdf/2511.01140",
        "title": "Few-Shot Multimodal Medical Imaging: A Theoretical Framework",
        "authors": [
            "Md Talha Mohsin",
            "Ismail Abdulrashid"
        ],
        "comments": "6 Pages",
        "subjects": "Machine Learning (stat.ML); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Image and Video Processing (eess.IV)",
        "abstract": "Medical imaging relies heavily on large, labeled datasets. But, unfortunately, they are not always easily accessible in clinical settings. Additionally, many practitioners often face various structural obstacles like limited data availability, fragmented data systems, and unbalanced datasets. These barriers often lead to the increased diagnostic uncertainty, underrepresentation of certain conditions, reduced model robustness, and biased diagnostic decisions. In response to these challenges, approaches such as transfer learning, meta-learning, and multimodal fusion have made great strides. However, they still need a solid theoretical justification for why they succeed or fail in situations where data is scarce. To address this gap, we propose a unified theoretical framework that characterizes learning and inference under low-resource medical imaging conditions. We first formalize the learning objective under few-shot conditions and compute sample complexity constraints to estimate the smallest quantity of data needed to achieve clinically reliable accuracy. Then based on ideas from PAC-learning and PAC-Bayesian theory, we explain how multimodal integration encourages generalization and quantifies uncertainty under sparse supervision. We further propose a formal metric for explanation stability, offering interpretability guarantees under low-data conditions. Taken together, the proposed framework establishes a principled foundation for constructing dependable, data-efficient diagnostic systems by jointly characterizing sample efficiency, uncertainty quantification, and interpretability in a unified theoretical setting.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 192,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01154",
        "abs_url": "https://arxiv.org/abs/2511.01154",
        "pdf_url": "https://arxiv.org/pdf/2511.01154",
        "title": "Stability of the Kim--Milman flow map",
        "authors": [
            "Sinho Chewi",
            "Aram-Alexandre Pooladian",
            "Matthew S. Zhang"
        ],
        "comments": "",
        "subjects": "Probability (math.PR); Machine Learning (cs.LG); Statistics Theory (math.ST)",
        "abstract": "In this short note, we characterize stability of the Kim--Milman flow map -- also known as the probability flow ODE -- with respect to variations in the target measure. Rather than the Wasserstein distance, we show that stability holds with respect to the relative Fisher information",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 193,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01181",
        "abs_url": "https://arxiv.org/abs/2511.01181",
        "pdf_url": "https://arxiv.org/pdf/2511.01181",
        "title": "Learning When to Quit in Sales Conversations",
        "authors": [
            "Emaad Manzoor",
            "Eva Ascarza",
            "Oded Netzer"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Salespeople frequently face the dynamic screening decision of whether to persist in a conversation or abandon it to pursue the next lead. Yet, little is known about how these decisions are made, whether they are efficient, or how to improve them. We study these decisions in the context of high-volume outbound sales where leads are ample, but time is scarce and failure is common. We formalize the dynamic screening decision as an optimal stopping problem and develop a generative language model-based sequential decision agent - a stopping agent - that learns whether and when to quit conversations by imitating a retrospectively-inferred optimal stopping policy. Our approach handles high-dimensional textual states, scales to large language models, and works with both open-source and proprietary language models. When applied to calls from a large European telecommunications firm, our stopping agent reduces the time spent on failed calls by 54% while preserving nearly all sales; reallocating the time saved increases expected sales by up to 37%. Upon examining the linguistic cues that drive salespeople's quitting decisions, we find that they tend to overweight a few salient expressions of consumer disinterest and mispredict call failure risk, suggesting cognitive bounds on their ability to make real-time conversational decisions. Our findings highlight the potential of artificial intelligence algorithms to correct cognitively-bounded human decisions and improve salesforce efficiency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 194,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01196",
        "abs_url": "https://arxiv.org/abs/2511.01196",
        "pdf_url": "https://arxiv.org/pdf/2511.01196",
        "title": "An Interdisciplinary and Cross-Task Review on Missing Data Imputation",
        "authors": [
            "Jicong Fan"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Missing data is a fundamental challenge in data science, significantly hindering analysis and decision-making across a wide range of disciplines, including healthcare, bioinformatics, social science, e-commerce, and industrial monitoring. Despite decades of research and numerous imputation methods, the literature remains fragmented across fields, creating a critical need for a comprehensive synthesis that connects statistical foundations with modern machine learning advances. This work systematically reviews core concepts-including missingness mechanisms, single versus multiple imputation, and different imputation goals-and examines problem characteristics across various domains. It provides a thorough categorization of imputation methods, spanning classical techniques (e.g., regression, the EM algorithm) to modern approaches like low-rank and high-rank matrix completion, deep learning models (autoencoders, GANs, diffusion models, graph neural networks), and large language models. Special attention is given to methods for complex data types, such as tensors, time series, streaming data, graph-structured data, categorical data, and multimodal data. Beyond methodology, we investigate the crucial integration of imputation with downstream tasks like classification, clustering, and anomaly detection, examining both sequential pipelines and joint optimization frameworks. The review also assesses theoretical guarantees, benchmarking resources, and evaluation metrics. Finally, we identify critical challenges and future directions, emphasizing model selection and hyperparameter optimization, the growing importance of privacy-preserving imputation via federated learning, and the pursuit of generalizable models that can adapt across domains and data types, thereby outlining a roadmap for future research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 195,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01253",
        "abs_url": "https://arxiv.org/abs/2511.01253",
        "pdf_url": "https://arxiv.org/pdf/2511.01253",
        "title": "Quantum Deep Learning Still Needs a Quantum Leap",
        "authors": [
            "Hans Gundlach",
            "Hrvoje Kukina",
            "Jayson Lynch",
            "Neil Thompson"
        ],
        "comments": "",
        "subjects": "Quantum Physics (quant-ph); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Quantum computing technology is advancing rapidly. Yet, even accounting for these trends, a quantum leap would be needed for quantum computers to mean- ingfully impact deep learning over the coming decade or two. We arrive at this conclusion based on a first-of-its-kind survey of quantum algorithms and how they match potential deep learning applications. This survey reveals three important areas where quantum computing could potentially accelerate deep learning, each of which faces a challenging roadblock to realizing its potential. First, quantum algorithms for matrix multiplication and other algorithms central to deep learning offer small theoretical improvements in the number of operations needed, but this advantage is overwhelmed on practical problem sizes by how slowly quantum computers do each operation. Second, some promising quantum algorithms depend on practical Quantum Random Access Memory (QRAM), which is underdeveloped. Finally, there are quantum algorithms that offer large theoretical advantages, but which are only applicable to special cases, limiting their practical benefits. In each of these areas, we support our arguments using quantitative forecasts of quantum advantage that build on the work by Choi et al. [2023] as well as new research on limitations and quantum hardware trends. Our analysis outlines the current scope of quantum deep learning and points to research directions that could lead to greater practical advances in the field.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 196,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01292",
        "abs_url": "https://arxiv.org/abs/2511.01292",
        "pdf_url": "https://arxiv.org/pdf/2511.01292",
        "title": "Optimal Attention Temperature Enhances In-Context Learning under Distribution Shift",
        "authors": [
            "Samet Demir",
            "Zafer Dogan"
        ],
        "comments": "26 pages, 6 figures",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Pretrained Transformers excel at in-context learning (ICL), inferring new tasks from only a handful of examples. Yet, their ICL performance can degrade sharply under distribution shift between pretraining and test data, a regime increasingly common in real-world deployments. While recent empirical work hints that adjusting the attention temperature in the softmax can enhance Transformer performance, the attention temperature's role in ICL under distribution shift remains unexplored. This paper provides the first theoretical and empirical study of attention temperature for ICL under distribution shift. Using a simplified but expressive \"linearized softmax\" framework, we derive closed-form generalization error expressions and prove that shifts in input covariance or label noise substantially impair ICL, but that an optimal attention temperature exists which minimizes this error. We then validate our predictions through extensive simulations on linear regression tasks and large-scale experiments with GPT-2 and LLaMA2-7B on question-answering benchmarks. Our results establish attention temperature as a principled and powerful mechanism for improving the robustness of ICL in pretrained Transformers, advancing theoretical understanding and providing actionable guidance for selecting attention temperature in practice.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 197,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01303",
        "abs_url": "https://arxiv.org/abs/2511.01303",
        "pdf_url": "https://arxiv.org/pdf/2511.01303",
        "title": "Black-Box Differentially Private Nonparametric Confidence Intervals Under Minimal Assumptions",
        "authors": [
            "Tomer Shoham",
            "Moshe Shenfeld",
            "Noa Velner-Harris",
            "Katrina Ligett"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
        "abstract": "We introduce a simple, general framework that takes any differentially private estimator of any arbitrary quantity as a black box, and from it constructs a differentially private nonparametric confidence interval of that quantity. Our approach repeatedly subsamples the data, applies the private estimator to each subsample, and then post-processes the resulting empirical CDF to a confidence interval. Our analysis uses the randomness from the subsampling to achieve privacy amplification. Under mild assumptions, the empirical CDF we obtain approaches the CDF of the private statistic as the sample size grows. We use this to show that the confidence intervals we estimate are asymptotically valid, tight, and equivalent to their non-private counterparts. We provide empirical evidence that our method performs well compared with the (less-general) state-of-the-art algorithms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 198,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01331",
        "abs_url": "https://arxiv.org/abs/2511.01331",
        "pdf_url": "https://arxiv.org/pdf/2511.01331",
        "title": "RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models",
        "authors": [
            "Hongyin Zhang",
            "Shuo Zhang",
            "Junxi Jin",
            "Qixin Zeng",
            "Runze Li",
            "Donglin Wang"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
        "abstract": "Vision-Language-Action (VLA) models have recently emerged as powerful general-purpose policies for robotic manipulation, benefiting from large-scale multi-modal pre-training. However, they often fail to generalize reliably in out-of-distribution deployments, where unavoidable disturbances such as observation noise, sensor errors, or actuation perturbations become prevalent. While recent Reinforcement Learning (RL)-based post-training provides a practical means to adapt pre-trained VLA models, existing methods mainly emphasize reward maximization and overlook robustness to environmental uncertainty. In this work, we introduce RobustVLA, a lightweight online RL post-training method designed to explicitly enhance the resilience of VLA models. Through a systematic robustness analysis, we identify two key regularizations: Jacobian regularization, which mitigates sensitivity to observation noise, and smoothness regularization, which stabilizes policies under action perturbations. Extensive experiments across diverse robotic environments demonstrate that RobustVLA significantly outperforms prior state-of-the-art methods in robustness and reliability. Our results highlight the importance of principled robustness-aware RL post-training as a key step toward improving the reliability and robustness of VLA models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 199,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01364",
        "abs_url": "https://arxiv.org/abs/2511.01364",
        "pdf_url": "https://arxiv.org/pdf/2511.01364",
        "title": "A semantic-based deep learning approach for mathematical expression retrieval",
        "authors": [
            "Pavan Kumar Perepu"
        ],
        "comments": "",
        "subjects": "Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "Mathematical expressions (MEs) have complex two-dimensional structures in which symbols can be present at any nested depth like superscripts, subscripts, above, below etc. As MEs are represented using LaTeX format, several text retrieval methods based on string matching, vector space models etc., have also been applied for ME retrieval problem in the literature. As these methods are based on syntactic similarity, recently deep learning approaches based on embedding have been used for semantic similarity. In our present work, we have focused on the retrieval of mathematical expressions using deep learning approaches. In our approach, semantic features are extracted from the MEs using a deep recurrent neural network (DRNN) and these features have been used for matching and retrieval. We have trained the network for a classification task which determines the complexity of an ME. ME complexity has been quantified in terms of its nested depth. Based on the nested depth, we have considered three complexity classes of MEs: Simple, Medium and Complex. After training the network, outputs just before the the final fully connected layer are extracted for all the MEs. These outputs form the semantic features of MEs and are stored in a database. For a given ME query, its semantic features are computed using the trained DRNN and matched against the semantic feature database. Matching is performed based on the standard euclidean distance and top 'k' nearest matches are retrieved, where 'k' is a user-defined parameter. Our approach has been illustrated on a database of 829 MEs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 200,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01464",
        "abs_url": "https://arxiv.org/abs/2511.01464",
        "pdf_url": "https://arxiv.org/pdf/2511.01464",
        "title": "Split-Flows: Measure Transport and Information Loss Across Molecular Resolutions",
        "authors": [
            "Sander Hummerich",
            "Tristan Bereau",
            "Ullrich Köthe"
        ],
        "comments": "",
        "subjects": "Chemical Physics (physics.chem-ph); Machine Learning (cs.LG); Computational Physics (physics.comp-ph)",
        "abstract": "By reducing resolution, coarse-grained models greatly accelerate molecular simulations, unlocking access to long-timescale phenomena, though at the expense of microscopic information. Recovering this fine-grained detail is essential for tasks that depend on atomistic accuracy, making backmapping a central challenge in molecular modeling. We introduce split-flows, a novel flow-based approach that reinterprets backmapping as a continuous-time measure transport across resolutions. Unlike existing generative strategies, split-flows establish a direct probabilistic link between resolutions, enabling expressive conditional sampling of atomistic structures and -- for the first time -- a tractable route to computing mapping entropies, an information-theoretic measure of the irreducible detail lost in coarse-graining. We demonstrate these capabilities on diverse molecular systems, including chignolin, a lipid bilayer, and alanine dipeptide, highlighting split-flows as a principled framework for accurate backmapping and systematic evaluation of coarse-grained models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 201,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01467",
        "abs_url": "https://arxiv.org/abs/2511.01467",
        "pdf_url": "https://arxiv.org/pdf/2511.01467",
        "title": "Quantum Blackwell's Ordering and Differential Privacy",
        "authors": [
            "Ayanava Dasgupta",
            "Naqueeb Ahmad Warsi",
            "Masahito Hayashi"
        ],
        "comments": "46 pages, 3 figures",
        "subjects": "Quantum Physics (quant-ph); Information Theory (cs.IT); Machine Learning (cs.LG)",
        "abstract": "We develop a framework for quantum differential privacy (QDP) based on quantum hypothesis testing and Blackwell's ordering. This approach characterizes $(\\eps,\\delta)$-QDP via hypothesis testing divergences and identifies the most informative quantum state pairs under privacy constraints. We apply this to analyze the stability of quantum learning algorithms, generalizing classical results to the case $\\delta>0$. Additionally, we study privatized quantum parameter estimation, deriving tight bounds on the quantum Fisher information under QDP. Finally, we establish near-optimal contraction bounds for differentially private quantum channels with respect to the hockey-stick divergence.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 202,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01491",
        "abs_url": "https://arxiv.org/abs/2511.01491",
        "pdf_url": "https://arxiv.org/pdf/2511.01491",
        "title": "Deep Learning Prediction of Beam Coherence Time for Near-FieldTeraHertz Networks",
        "authors": [
            "Irched Chafaa",
            "E. Veronica Belmega",
            "Giacomo Bacci"
        ],
        "comments": "IEEE Wireless Communication Letters (accepted October 2025)",
        "subjects": "Systems and Control (eess.SY); Machine Learning (cs.LG)",
        "abstract": "Large multiple antenna arrays coupled with accu- rate beamforming are essential in terahertz (THz) communi- cations to ensure link reliability. However, as the number of antennas increases, beam alignment (focusing) and beam tracking in mobile networks incur prohibitive overhead. Additionally, the near-field region expands both with the size of antenna arrays and the carrier frequency, calling for adjustments in the beamforming to account for spherical wavefront instead of the conventional planar wave assumption. In this letter, we introduce a novel beam coherence time for mobile THz networks, to drastically reduce the rate of beam updates. Then, we propose a deep learning model, relying on a simple feedforward neural network with a time-dependent input, to predict the beam coherence time and adjust the beamforming on the fly with minimal overhead. Our numerical results demonstrate the effectiveness of the proposed approach by enabling higher data rates while reducing the overhead, especially at high (i.e., vehicular) mobility.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 203,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01554",
        "abs_url": "https://arxiv.org/abs/2511.01554",
        "pdf_url": "https://arxiv.org/pdf/2511.01554",
        "title": "Learning what to say and how precisely: Efficient Communication via Differentiable Discrete Communication Learning",
        "authors": [
            "Aditya Kapoor",
            "Yash Bhisikar",
            "Benjamin Freed",
            "Jan Peters",
            "Mingfei Sun"
        ],
        "comments": "30 pages, 12 figures, 6 tables",
        "subjects": "Multiagent Systems (cs.MA); Information Theory (cs.IT); Machine Learning (cs.LG)",
        "abstract": "Effective communication in multi-agent reinforcement learning (MARL) is critical for success but constrained by bandwidth, yet past approaches have been limited to complex gating mechanisms that only decide \\textit{whether} to communicate, not \\textit{how precisely}. Learning to optimize message precision at the bit-level is fundamentally harder, as the required discretization step breaks gradient flow. We address this by generalizing Differentiable Discrete Communication Learning (DDCL), a framework for end-to-end optimization of discrete messages. Our primary contribution is an extension of DDCL to support unbounded signals, transforming it into a universal, plug-and-play layer for any MARL architecture. We verify our approach with three key results. First, through a qualitative analysis in a controlled environment, we demonstrate \\textit{how} agents learn to dynamically modulate message precision according to the informational needs of the task. Second, we integrate our variant of DDCL into four state-of-the-art MARL algorithms, showing it reduces bandwidth by over an order of magnitude while matching or exceeding task performance. Finally, we provide direct evidence for the \\enquote{Bitter Lesson} in MARL communication: a simple Transformer-based policy leveraging DDCL matches the performance of complex, specialized architectures, questioning the necessity of bespoke communication designs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 204,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01555",
        "abs_url": "https://arxiv.org/abs/2511.01555",
        "pdf_url": "https://arxiv.org/pdf/2511.01555",
        "title": "Fast, memory-efficient genomic interval tokenizers for modern machine learning",
        "authors": [
            "Nathan J. LeRoy",
            "Donald R. Campbell Jr",
            "Seth Stadick",
            "Oleksandr Khoroshevskyi",
            "Sang-Hoon Park",
            "Ziyang Hu",
            "Nathan C. Sheffield"
        ],
        "comments": "4 pages, 1 figure",
        "subjects": "Genomics (q-bio.GN); Machine Learning (cs.LG)",
        "abstract": "Introduction: Epigenomic datasets from high-throughput sequencing experiments are commonly summarized as genomic intervals. As the volume of this data grows, so does interest in analyzing it through deep learning. However, the heterogeneity of genomic interval data, where each dataset defines its own regions, creates barriers for machine learning methods that require consistent, discrete vocabularies. Methods: We introduce gtars-tokenizers, a high-performance library that maps genomic intervals to a predefined universe or vocabulary of regions, analogous to text tokenization in natural language processing. Built in Rust with bindings for Python, R, CLI, and WebAssembly, gtars-tokenizers implements two overlap methods (BITS and AIList) and integrates seamlessly with modern ML frameworks through Hugging Face-compatible APIs. Results: The gtars-tokenizers package achieves top efficiency for large-scale datasets, while enabling genomic intervals to be processed using standard ML workflows in PyTorch and TensorFlow without ad hoc preprocessing. This token-based approach bridges genomics and machine learning, supporting scalable and standardized analysis of interval data across diverse computational environments. Availability: PyPI and GitHub: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 205,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01602",
        "abs_url": "https://arxiv.org/abs/2511.01602",
        "pdf_url": "https://arxiv.org/pdf/2511.01602",
        "title": "L2T-Tune:LLM-Guided Hybrid Database Tuning with LHS and TD3",
        "authors": [
            "Xinyue Yang",
            "Chen Zheng",
            "Yaoyang Hou",
            "Renhao Zhang",
            "Yiyan Zhang",
            "Yanjun Wu",
            "Heng Zhang"
        ],
        "comments": "",
        "subjects": "Databases (cs.DB); Machine Learning (cs.LG)",
        "abstract": "Configuration tuning is critical for database performance. Although recent advancements in database tuning have shown promising results in throughput and latency improvement, challenges remain. First, the vast knob space makes direct optimization unstable and slow to converge. Second, reinforcement learning pipelines often lack effective warm-start guidance and require long offline training. Third, transferability is limited: when hardware or workloads change, existing models typically require substantial retraining to recover performance. To address these limitations, we propose L2T-Tune, a new LLM-guided hybrid database tuning framework that features a three-stage pipeline: Stage one performs a warm start that simultaneously generates uniform samples across the knob space and logs them into a shared pool; Stage two leverages a large language model to mine and prioritize tuning hints from manuals and community documents for rapid convergence. Stage three uses the warm-start sample pool to reduce the dimensionality of knobs and state features, then fine-tunes the configuration with the Twin Delayed Deep Deterministic Policy Gradient algorithm. We conduct experiments on L2T-Tune and the state-of-the-art models. Compared with the best-performing alternative, our approach improves performance by an average of 37.1% across all workloads, and by up to 73% on TPC-C. Compared with models trained with reinforcement learning, it achieves rapid convergence in the offline tuning stage on a single server. Moreover, during the online tuning stage, it only takes 30 steps to achieve best results.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 206,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01628",
        "abs_url": "https://arxiv.org/abs/2511.01628",
        "pdf_url": "https://arxiv.org/pdf/2511.01628",
        "title": "Partial Trace-Class Bayesian Neural Networks",
        "authors": [
            "Arran Carter",
            "Torben Sell"
        ],
        "comments": "10 pages, 4 figures",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Methodology (stat.ME)",
        "abstract": "Bayesian neural networks (BNNs) allow rigorous uncertainty quantification in deep learning, but often come at a prohibitive computational cost. We propose three different innovative architectures of partial trace-class Bayesian neural networks (PaTraC BNNs) that enable uncertainty quantification comparable to standard BNNs but use significantly fewer Bayesian parameters. These PaTraC BNNs have computational and statistical advantages over standard Bayesian neural networks in terms of speed and memory requirements. Our proposed methodology therefore facilitates reliable, robust, and scalable uncertainty quantification in neural networks. The three architectures build on trace-class neural network priors which induce an ordering of the neural network parameters, and are thus a natural choice in our framework. In a numerical simulation study, we verify the claimed benefits, and further illustrate the performance of our proposed methodology on a real-world dataset.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 207,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01654",
        "abs_url": "https://arxiv.org/abs/2511.01654",
        "pdf_url": "https://arxiv.org/pdf/2511.01654",
        "title": "Panther: A Cost-Effective Privacy-Preserving Framework for GNN Training and Inference Services in Cloud Environments",
        "authors": [
            "Congcong Chen",
            "Xinyu Liu",
            "Kaifeng Huang",
            "Lifei Wei",
            "Yang Shi"
        ],
        "comments": "Accepted for publication in IEEE Transactions on Services Computing (TSC)",
        "subjects": "Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
        "abstract": "Graph Neural Networks (GNNs) have marked significant impact in traffic state prediction, social recommendation, knowledge-aware question answering and so on. As more and more users move towards cloud computing, it has become a critical issue to unleash the power of GNNs while protecting the privacy in cloud environments. Specifically, the training data and inference data for GNNs need to be protected from being stolen by external adversaries. Meanwhile, the financial cost of cloud computing is another primary concern for users. Therefore, although existing studies have proposed privacy-preserving techniques for GNNs in cloud environments, their additional computational and communication overhead remain relatively high, causing high financial costs that limit their widespread adoption among users. To protect GNN privacy while lowering the additional financial costs, we introduce Panther, a cost-effective privacy-preserving framework for GNN training and inference services in cloud environments. Technically, Panther leverages four-party computation to asynchronously executing the secure array access protocol, and randomly pads the neighbor information of GNN nodes. We prove that Panther can protect privacy for both training and inference of GNN models. Our evaluation shows that Panther reduces the training and inference time by an average of 75.28% and 82.80%, respectively, and communication overhead by an average of 52.61% and 50.26% compared with the state-of-the-art, which is estimated to save an average of 55.05% and 59.00% in financial costs (based on on-demand pricing model) for the GNN training and inference process on Google Cloud Platform.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 208,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01680",
        "abs_url": "https://arxiv.org/abs/2511.01680",
        "pdf_url": "https://arxiv.org/pdf/2511.01680",
        "title": "Making Interpretable Discoveries from Unstructured Data: A High-Dimensional Multiple Hypothesis Testing Approach",
        "authors": [
            "Jacob Carlson"
        ],
        "comments": "",
        "subjects": "Econometrics (econ.EM); Machine Learning (cs.LG)",
        "abstract": "Social scientists are increasingly turning to unstructured datasets to unlock new empirical insights, e.g., estimating causal effects on text outcomes, measuring beliefs from open-ended survey responses. In such settings, unsupervised analysis is often of interest, in that the researcher does not want to pre-specify the objects of measurement or otherwise artificially delimit the space of measurable concepts; they are interested in discovery. This paper proposes a general and flexible framework for pursuing discovery from unstructured data in a statistically principled way. The framework leverages recent methods from the literature on machine learning interpretability to map unstructured data points to high-dimensional, sparse, and interpretable dictionaries of concepts; computes (test) statistics of these dictionary entries; and then performs selective inference on them using newly developed statistical procedures for high-dimensional exceedance control of the $k$-FWER under arbitrary dependence. The proposed framework has few researcher degrees of freedom, is fully replicable, and is cheap to implement -- both in terms of financial cost and researcher time. Applications to recent descriptive and causal analyses of unstructured data in empirical economics are explored. An open source Jupyter notebook is provided for researchers to implement the framework in their own projects.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 209,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01701",
        "abs_url": "https://arxiv.org/abs/2511.01701",
        "pdf_url": "https://arxiv.org/pdf/2511.01701",
        "title": "Solution Space Topology Guides CMTS Search",
        "authors": [
            "Mirco A. Mannucci"
        ],
        "comments": "15 pages, 3 figures",
        "subjects": "Computational Engineering, Finance, and Science (cs.CE); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "A fundamental question in search-guided AI: what topology should guide Monte Carlo Tree Search (MCTS) in puzzle solving? Prior work applied topological features to guide MCTS in ARC-style tasks using grid topology -- the Laplacian spectral properties of cell connectivity -- and found no benefit. We identify the root cause: grid topology is constant across all instances. We propose measuring \\emph{solution space topology} instead: the structure of valid color assignments constrained by detected pattern rules. We build this via compatibility graphs where nodes are $(cell, color)$ pairs and edges represent compatible assignments under pattern constraints. Our method: (1) detect pattern rules automatically with 100\\% accuracy on 5 types, (2) construct compatibility graphs encoding solution space structure, (3) extract topological features (algebraic connectivity, rigidity, color structure) that vary with task difficulty, (4) integrate these features into MCTS node selection via sibling-normalized scores. We provide formal definitions, a rigorous selection formula, and comprehensive ablations showing that algebraic connectivity is the dominant signal. The work demonstrates that topology matters for search -- but only the \\emph{right} topology. For puzzle solving, this is solution space structure, not problem space structure.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 210,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01716",
        "abs_url": "https://arxiv.org/abs/2511.01716",
        "pdf_url": "https://arxiv.org/pdf/2511.01716",
        "title": "SemBench: A Benchmark for Semantic Query Processing Engines",
        "authors": [
            "Jiale Lao",
            "Andreas Zimmerer",
            "Olga Ovcharenko",
            "Tianji Cong",
            "Matthew Russo",
            "Gerardo Vitagliano",
            "Michael Cochez",
            "Fatma Özcan",
            "Gautam Gupta",
            "Thibaud Hottelier",
            "H. V. Jagadish",
            "Kris Kissel",
            "Sebastian Schelter",
            "Andreas Kipf",
            "Immanuel Trummer"
        ],
        "comments": "",
        "subjects": "Databases (cs.DB); Machine Learning (cs.LG)",
        "abstract": "We present a benchmark targeting a novel class of systems: semantic query processing engines. Those systems rely inherently on generative and reasoning capabilities of state-of-the-art large language models (LLMs). They extend SQL with semantic operators, configured by natural language instructions, that are evaluated via LLMs and enable users to perform various operations on multimodal data. Our benchmark introduces diversity across three key dimensions: scenarios, modalities, and operators. Included are scenarios ranging from movie review analysis to medical question-answering. Within these scenarios, we cover different data modalities, including images, audio, and text. Finally, the queries involve a diverse set of operators, including semantic filters, joins, mappings, ranking, and classification operators. We evaluated our benchmark on three academic systems (LOTUS, Palimpzest, and ThalamusDB) and one industrial system, Google BigQuery. Although these results reflect a snapshot of systems under continuous development, our study offers crucial insights into their current strengths and weaknesses, illuminating promising directions for future research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 211,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01734",
        "abs_url": "https://arxiv.org/abs/2511.01734",
        "pdf_url": "https://arxiv.org/pdf/2511.01734",
        "title": "A Proof of Learning Rate Transfer under $μ$P",
        "authors": [
            "Soufiane Hayou"
        ],
        "comments": "23 pages",
        "subjects": "Machine Learning (stat.ML); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "We provide the first proof of learning rate transfer with width in a linear multi-layer perceptron (MLP) parametrized with $\\mu$P, a neural network parameterization designed to ``maximize'' feature learning in the infinite-width limit. We show that under $\\mu P$, the optimal learning rate converges to a \\emph{non-zero constant} as width goes to infinity, providing a theoretical explanation to learning rate transfer. In contrast, we show that this property fails to hold under alternative parametrizations such as Standard Parametrization (SP) and Neural Tangent Parametrization (NTP). We provide intuitive proofs and support the theoretical findings with extensive empirical results.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 212,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01773",
        "abs_url": "https://arxiv.org/abs/2511.01773",
        "pdf_url": "https://arxiv.org/pdf/2511.01773",
        "title": "ADNAC: Audio Denoiser using Neural Audio Codec",
        "authors": [
            "Daniel Jimon",
            "Mircea Vaida",
            "Adriana Stan"
        ],
        "comments": "Accepted and presented at the 13th International Conference on Speech Technology and Human-Computer Dialogue (SpeD), Cluj-Napoca, Romania, October 19-22, 2025. 4 pages, 1 figure. IEEE Catalog Number: CFP2555H-USB, ISBN: 979-8-3315-7485-7",
        "subjects": "Sound (cs.SD); Machine Learning (cs.LG)",
        "abstract": "Audio denoising is critical in signal processing, enhancing intelligibility and fidelity for applications like restoring musical recordings. This paper presents a proof-of-concept for adapting a state-of-the-art neural audio codec, the Descript Audio Codec (DAC), for music denoising. This work overcomes the limitations of traditional architectures like U-Nets by training the model on a large-scale, custom-synthesized dataset built from diverse sources. Training is guided by a multi objective loss function that combines time-domain, spectral, and signal-level fidelity metrics. Ultimately, this paper aims to present a PoC for high-fidelity, generative audio restoration.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 213,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01797",
        "abs_url": "https://arxiv.org/abs/2511.01797",
        "pdf_url": "https://arxiv.org/pdf/2511.01797",
        "title": "Hybrid Neural Network-Based Indoor Localisation System for Mobile Robots Using CSI Data in a Robotics Simulator",
        "authors": [
            "Javier Ballesteros-Jerez",
            "Jesus Martínez-Gómez",
            "Ismael García-Varea",
            "Luis Orozco-Barbosa",
            "Manuel Castillo-Cara"
        ],
        "comments": "13 pages, 7 figures. Conference paper (ROBOVIS 2025)",
        "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
        "abstract": "We present a hybrid neural network model for inferring the position of mobile robots using Channel State Information (CSI) data from a Massive MIMO system. By leveraging an existing CSI dataset, our approach integrates a Convolutional Neural Network (CNN) with a Multilayer Perceptron (MLP) to form a Hybrid Neural Network (HyNN) that estimates 2D robot positions. CSI readings are converted into synthetic images using the TINTO tool. The localisation solution is integrated with a robotics simulator, and the Robot Operating System (ROS), which facilitates its evaluation through heterogeneous test cases, and the adoption of state estimators like Kalman filters. Our contributions illustrate the potential of our HyNN model in achieving precise indoor localisation and navigation for mobile robots in complex environments. The study follows, and proposes, a generalisable procedure applicable beyond the specific use case studied, making it adaptable to different scenarios and datasets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 214,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01813",
        "abs_url": "https://arxiv.org/abs/2511.01813",
        "pdf_url": "https://arxiv.org/pdf/2511.01813",
        "title": "Disciplined Biconvex Programming",
        "authors": [
            "Hao Zhu",
            "Joschka Boedecker"
        ],
        "comments": "",
        "subjects": "Optimization and Control (math.OC); Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG); Mathematical Software (cs.MS)",
        "abstract": "We introduce disciplined biconvex programming (DBCP), a modeling framework for specifying and solving biconvex optimization problems. Biconvex optimization problems arise in various applications, including machine learning, signal processing, computational science, and control. Solving a biconvex optimization problem in practice usually resolves to heuristic methods based on alternate convex search (ACS), which iteratively optimizes over one block of variables while keeping the other fixed, so that the resulting subproblems are convex and can be efficiently solved. However, designing and implementing an ACS solver for a specific biconvex optimization problem usually requires significant effort from the user, which can be tedious and error-prone. DBCP extends the principles of disciplined convex programming to biconvex problems, allowing users to specify biconvex optimization problems in a natural way based on a small number of syntax rules. The resulting problem can then be automatically split and transformed into convex subproblems, for which a customized ACS solver is then generated and applied. DBCP allows users to quickly experiment with different biconvex problem formulations, without expertise in convex optimization. We implement DBCP into the open source Python package dbcp, as an extension to the famous domain specific language CVXPY for convex optimization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 215,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01824",
        "abs_url": "https://arxiv.org/abs/2511.01824",
        "pdf_url": "https://arxiv.org/pdf/2511.01824",
        "title": "Simulating Environments with Reasoning Models for Agent Training",
        "authors": [
            "Yuetai Li",
            "Huseyin A Inan",
            "Xiang Yue",
            "Wei-Ning Chen",
            "Lukas Wutschitz",
            "Janardhan Kulkarni",
            "Radha Poovendran",
            "Robert Sim",
            "Saravan Rajmohan"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "LLM agents excel in compact environments requiring deep reasoning but remain brittle when operating in broader, more complex contexts that demand robustness across diverse tools and schemas. Building bespoke environments for training is heavy, brittle, and limits progress. In this paper, we demonstrate that LLMs can simulate realistic environment feedback without access to actual testbed data or APIs. Inspired by this capability, we propose two frameworks: Simia-SFT, a pipeline that synthesizes SFT data by amplifying small seed sets into diverse trajectories in an environment-agnostic manner, and Simia-RL, a framework that enables RL training without real environment implementations through LLM-simulated feedback. Fine-tuning open models yields consistent improvements across multiple benchmarks, surpassing GPT-4o and approaching o4-mini on $\\tau^2$-Bench. Together, Simia-SFT and Simia-RL enable scalable agent training without environment engineering, replacing heavy and brittle implementations with flexible LLM-based simulation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 216,
        "date": "2025-11-04",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-04?abs=True",
        "arxiv_id": "2511.01852",
        "abs_url": "https://arxiv.org/abs/2511.01852",
        "pdf_url": "https://arxiv.org/pdf/2511.01852",
        "title": "Proximal Regret and Proximal Correlated Equilibria: A New Tractable Solution Concept for Online Learning and Games",
        "authors": [
            "Yang Cai",
            "Constantinos Daskalakis",
            "Haipeng Luo",
            "Chen-Yu Wei",
            "Weiqiang Zheng"
        ],
        "comments": "This paper presents proximal regret and proximal correlated equilibria results that do not appear in the NeurIPS version of arXiv:2403.08171",
        "subjects": "Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG)",
        "abstract": "Learning and computation of equilibria are central problems in algorithmic game theory. In this work, we introduce proximal regret, a new notion of regret based on proximal operators that lies strictly between external and swap regret. When every player employs a no-proximal-regret algorithm in a general convex game, the empirical distribution of play converges to proximal correlated equilibria (PCE), a refinement of coarse correlated equilibria. Our framework unifies several emerging notions in online learning and game theory -- such as gradient equilibrium and semicoarse correlated equilibrium -- and introduces new ones. Our main result shows that the classic Online Gradient Descent (GD) algorithm achieves an optimal $O(\\sqrt{T})$ bound on proximal regret, revealing that GD, without modification, minimizes a stronger regret notion than external regret. This provides a new explanation for the empirically superior performance of gradient descent in online learning and games. We further extend our analysis to Mirror Descent in the Bregman setting and to Optimistic Gradient Descent, which yields faster convergence in smooth convex games.",
        "gemini2.5flash": "",
        "overall_idea": ""
    }
]