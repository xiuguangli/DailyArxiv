[
    {
        "order": 1,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17564",
        "abs_url": "https://arxiv.org/abs/2511.17564",
        "pdf_url": "https://arxiv.org/pdf/2511.17564",
        "title": "Classification of Transient Astronomical Object Light Curves Using LSTM Neural Networks",
        "authors": [
            "Guilherme Grancho D. Fernandes",
            "Marco A. Barroca",
            "Mateus dos Santos",
            "Rafael S. Oliveira"
        ],
        "comments": "12 pages, 11 figures, 2 tables",
        "subjects": "Machine Learning (cs.LG); Instrumentation and Methods for Astrophysics (astro-ph.IM); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "This study presents a bidirectional Long Short-Term Memory (LSTM) neural network for classifying transient astronomical object light curves from the Photometric LSST Astronomical Time-series Classification Challenge (PLAsTiCC) dataset. The original fourteen object classes were reorganized into five generalized categories (S-Like, Fast, Long, Periodic, and Non-Periodic) to address class imbalance. After preprocessing with padding, temporal rescaling, and flux normalization, a bidirectional LSTM network with masking layers was trained and evaluated on a test set of 19,920 objects. The model achieved strong performance for S-Like and Periodic classes, with ROC area under the curve (AUC) values of 0.95 and 0.99, and Precision-Recall AUC values of 0.98 and 0.89, respectively. However, performance was significantly lower for Fast and Long classes (ROC AUC of 0.68 for Long class), and the model exhibited difficulty distinguishing between Periodic and Non-Periodic objects. Evaluation on partial light curve data (5, 10,and 20 days from detection) revealed substantial performance degradation, with increased misclassification toward the S-Like class. These findings indicate that class imbalance and limited temporal information are primary limitations, suggesting that class balancing strategies and preprocessing techniques focusing on detection moments could improve performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 2,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17566",
        "abs_url": "https://arxiv.org/abs/2511.17566",
        "pdf_url": "https://arxiv.org/pdf/2511.17566",
        "title": "Root Cause Analysis for Microservice Systems via Cascaded Conditional Learning with Hypergraphs",
        "authors": [
            "Shuaiyu Xie",
            "Hanbin He",
            "Jian Wang",
            "Bing Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Distributed, Parallel, and Cluster Computing (cs.DC); Software Engineering (cs.SE)",
        "abstract": "Root cause analysis in microservice systems typically involves two core tasks: root cause localization (RCL) and failure type identification (FTI). Despite substantial research efforts, conventional diagnostic approaches still face two key challenges. First, these methods predominantly adopt a joint learning paradigm for RCL and FTI to exploit shared information and reduce training time. However, this simplistic integration neglects the causal dependencies between tasks, thereby impeding inter-task collaboration and information transfer. Second, these existing methods primarily focus on point-to-point relationships between instances, overlooking the group nature of inter-instance influences induced by deployment configurations and load balancing. To overcome these limitations, we propose CCLH, a novel root cause analysis framework that orchestrates diagnostic tasks based on cascaded conditional learning. CCLH provides a three-level taxonomy for group influences between instances and incorporates a heterogeneous hypergraph to model these relationships, facilitating the simulation of failure propagation. Extensive experiments conducted on datasets from three microservice benchmarks demonstrate that CCLH outperforms state-of-the-art methods in both RCL and FTI.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 3,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17568",
        "abs_url": "https://arxiv.org/abs/2511.17568",
        "pdf_url": "https://arxiv.org/pdf/2511.17568",
        "title": "Enhancing Robustness of Offline Reinforcement Learning Under Data Corruption via Sharpness-Aware Minimization",
        "authors": [
            "Le Xu",
            "Jiayu Chen"
        ],
        "comments": "Accepted as an Oral Presentation at the AAAI 2026 Student Abstract and Poster Program (SAPP)",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Offline reinforcement learning (RL) is vulnerable to real-world data corruption, with even robust algorithms failing under challenging observation and mixture corruptions. We posit this failure stems from data corruption creating sharp minima in the loss landscape, leading to poor generalization. To address this, we are the first to apply Sharpness-Aware Minimization (SAM) as a general-purpose, plug-and-play optimizer for offline RL. SAM seeks flatter minima, guiding models to more robust parameter regions. We integrate SAM into strong baselines for data corruption: IQL, a top-performing offline RL algorithm in this setting, and RIQL, an algorithm designed specifically for data-corruption robustness. We evaluate them on D4RL benchmarks with both random and adversarial corruption. Our SAM-enhanced methods consistently and significantly outperform the original baselines. Visualizations of the reward surface confirm that SAM finds smoother solutions, providing strong evidence for its effectiveness in improving the robustness of offline RL agents.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 4,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17577",
        "abs_url": "https://arxiv.org/abs/2511.17577",
        "pdf_url": "https://arxiv.org/pdf/2511.17577",
        "title": "Efficient Mathematical Reasoning Models via Dynamic Pruning and Knowledge Distillation",
        "authors": [
            "Fengming Yu",
            "Qingyu Meng",
            "Haiwei Pan",
            "Kejia Zhang"
        ],
        "comments": "12 pages, 1 figure",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "With the rapid development of deep learning, large language models have shown strong capabilities in complex reasoning tasks such as mathematical equation solving. However, their substantial computational and storage costs hinder practical deployment. This paper proposes a lightweight optimization method that integrates dynamic attention head pruning with knowledge distillation. The approach dynamically evaluates the importance of each attention head in the multi-head attention mechanism using a combination of weight norms and entropy, and prunes redundant heads in real time to reduce computational overhead. To mitigate performance degradation, knowledge distillation transfers information from the original model to the pruned student, enabling the smaller model to preserve reasoning ability. Experiments conducted on both Math23k and ASDiv-A verify the effectiveness of the proposed method. For example, on Math23k with a 30% pruning ratio, parameters are reduced by 18.7%, inference speed is improved by 27.5%, FLOPs are reduced by 19.3%, and accuracy drops only 0.7% (from 84.4% to 83.7%). These results demonstrate that the method achieves substantial efficiency gains while maintaining strong reasoning performance, providing a practical solution for efficient deployment of large language models in mathematical reasoning tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 5,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17581",
        "abs_url": "https://arxiv.org/abs/2511.17581",
        "pdf_url": "https://arxiv.org/pdf/2511.17581",
        "title": "EgoCogNav: Cognition-aware Human Egocentric Navigation",
        "authors": [
            "Zhiwen Qiu",
            "Ziang Liu",
            "Wenqian Niu",
            "Tapomayukh Bhattacharjee",
            "Saleh Kalantari"
        ],
        "comments": "11 pages, 4 figures",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Modeling the cognitive and experiential factors of human navigation is central to deepening our understanding of human-environment interaction and to enabling safe social navigation and effective assistive wayfinding. Most existing methods focus on forecasting motions in fully observed scenes and often neglect human factors that capture how people feel and respond to space. To address this gap, We propose EgoCogNav, a multimodal egocentric navigation framework that predicts perceived path uncertainty as a latent state and jointly forecasts trajectories and head motion by fusing scene features with sensory cues. To facilitate research in the field, we introduce the Cognition-aware Egocentric Navigation (CEN) dataset consisting 6 hours of real-world egocentric recordings capturing diverse navigation behaviors in real-world scenarios. Experiments show that EgoCogNav learns the perceived uncertainty that highly correlates with human-like behaviors such as scanning, hesitation, and backtracking while generalizing to unseen environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 6,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17583",
        "abs_url": "https://arxiv.org/abs/2511.17583",
        "pdf_url": "https://arxiv.org/pdf/2511.17583",
        "title": "Learning Straight Flows: Variational Flow Matching for Efficient Generation",
        "authors": [
            "Chenrui Ma",
            "Xi Xiao",
            "Tianyang Wang",
            "Xiao Wang",
            "Yanning Shen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Flow Matching has limited ability in achieving one-step generation due to its reliance on learned curved trajectories. Previous studies have attempted to address this limitation by either modifying the coupling distribution to prevent interpolant intersections or introducing consistency and mean-velocity modeling to promote straight trajectory learning. However, these approaches often suffer from discrete approximation errors, training instability, and convergence difficulties. To tackle these issues, in the present work, we propose \\textbf{S}traight \\textbf{V}ariational \\textbf{F}low \\textbf{M}atching (\\textbf{S-VFM}), which integrates a variational latent code representing the ``generation overview'' into the Flow Matching framework. \\textbf{S-VFM} explicitly enforces trajectory straightness, ideally producing linear generation paths. The proposed method achieves competitive performance across three challenge benchmarks and demonstrates advantages in both training and inference efficiency compared with existing methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 7,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17585",
        "abs_url": "https://arxiv.org/abs/2511.17585",
        "pdf_url": "https://arxiv.org/pdf/2511.17585",
        "title": "PaSE: Prototype-aligned Calibration and Shapley-based Equilibrium for Multimodal Sentiment Analysis",
        "authors": [
            "Kang He",
            "Boyu Chen",
            "Yuzhe Ding",
            "Fei Li",
            "Chong Teng",
            "Donghong Ji"
        ],
        "comments": "Accepted by AAAI 2026",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Multimodal Sentiment Analysis (MSA) seeks to understand human emotions by integrating textual, acoustic, and visual signals. Although multimodal fusion is designed to leverage cross-modal complementarity, real-world scenarios often exhibit modality competition: dominant modalities tend to overshadow weaker ones, leading to suboptimal this http URL this paper, we propose PaSE, a novel Prototype-aligned Calibration and Shapley-optimized Equilibrium framework, which enhances collaboration while explicitly mitigating modality competition. PaSE first applies Prototype-guided Calibration Learning (PCL) to refine unimodal representations and align them through an Entropic Optimal Transport mechanism that ensures semantic consistency. To further stabilize optimization, we introduce a Dual-Phase Optimization strategy. A prototype-gated fusion module is first used to extract shared representations, followed by Shapley-based Gradient Modulation (SGM), which adaptively adjusts gradients according to the contribution of each modality. Extensive experiments on IEMOCAP, MOSI, and MOSEI confirm that PaSE achieves the superior performance and effectively alleviates modality competition.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 8,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17589",
        "abs_url": "https://arxiv.org/abs/2511.17589",
        "pdf_url": "https://arxiv.org/pdf/2511.17589",
        "title": "Llamazip: Leveraging LLaMA for Lossless Text Compression and Training Dataset Detection",
        "authors": [
            "Sören Dréano",
            "Derek Molloy",
            "Noel Murphy"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
        "abstract": "This work introduces Llamazip, a novel lossless text compression algorithm based on the predictive capabilities of the LLaMA3 language model. Llamazip achieves significant data reduction by only storing tokens that the model fails to predict, optimizing storage efficiency without compromising data integrity. Key factors affecting its performance, including quantization and context window size, are analyzed, revealing their impact on compression ratios and computational requirements. Beyond compression, Llamazip demonstrates the potential to identify whether a document was part of the training dataset of a language model. This capability addresses critical concerns about data provenance, intellectual property, and transparency in language model training.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 9,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17593",
        "abs_url": "https://arxiv.org/abs/2511.17593",
        "pdf_url": "https://arxiv.org/pdf/2511.17593",
        "title": "Comparative Analysis of Large Language Model Inference Serving Systems: A Performance Study of vLLM and HuggingFace TGI",
        "authors": [
            "Saicharan Kolluru"
        ],
        "comments": "10 pages, benchmarking study of LLM inference systems",
        "subjects": "Machine Learning (cs.LG); Distributed, Parallel, and Cluster Computing (cs.DC); Performance (cs.PF)",
        "abstract": "The deployment of Large Language Models (LLMs) in production environments requires efficient inference serving systems that balance throughput, latency, and resource utilization. This paper presents a comprehensive empirical evaluation of two prominent open-source LLM serving frameworks: vLLM and HuggingFace Text Generation Inference (TGI). We benchmark these systems across multiple dimensions including throughput performance, end-to-end latency, GPU memory utilization, and scalability characteristics using LLaMA-2 models ranging from 7B to 70B parameters. Our experiments reveal that vLLM achieves up to 24x higher throughput than TGI under high-concurrency workloads through its novel PagedAttention mechanism, while TGI demonstrates lower tail latencies for interactive single-user scenarios. We provide detailed performance profiles for different deployment scenarios and offer practical recommendations for system selection based on workload characteristics. Our findings indicate that the choice between these frameworks should be guided by specific use-case requirements: vLLM excels in high-throughput batch processing scenarios, while TGI is better suited for latency-sensitive interactive applications with moderate concurrency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 10,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17594",
        "abs_url": "https://arxiv.org/abs/2511.17594",
        "pdf_url": "https://arxiv.org/pdf/2511.17594",
        "title": "AutoSAGE: Input-Aware CUDA Scheduling for Sparse GNN Aggregation (SpMM/SDDMM) and CSR Attention",
        "authors": [
            "Aleksandar Stankovic"
        ],
        "comments": "10 pages, several figures. Code and artifacts: this https URL",
        "subjects": "Machine Learning (cs.LG); Performance (cs.PF)",
        "abstract": "Sparse GNN aggregations (CSR SpMM/SDDMM) vary widely in performance with degree skew, feature width, and GPU micro-architecture. We present AutoSAGE, an input-aware CUDA scheduler that chooses tiling and mapping per input using a lightweight estimate refined by on-device micro-probes, with a guardrail that safely falls back to vendor kernels and a persistent cache for deterministic replay. AutoSAGE covers SpMM and SDDMM and composes into a CSR attention pipeline (SDDMM -> row-softmax -> SpMM). On Reddit and OGBN-Products, it matches vendor baselines at bandwidth-bound feature widths and finds gains at small widths; on synthetic sparsity and skew stress tests it achieves up to 4.7x kernel-level speedups. We release CUDA sources, Python bindings, a reproducible harness, and replayable cache logs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 11,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17598",
        "abs_url": "https://arxiv.org/abs/2511.17598",
        "pdf_url": "https://arxiv.org/pdf/2511.17598",
        "title": "Non-stationary and Varying-discounting Markov Decision Processes for Reinforcement Learning",
        "authors": [
            "Zhizuo Chen",
            "Theodore T. Allen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Optimization and Control (math.OC); Machine Learning (stat.ML)",
        "abstract": "Algorithms developed under stationary Markov Decision Processes (MDPs) often face challenges in non-stationary environments, and infinite-horizon formulations may not directly apply to finite-horizon tasks. To address these limitations, we introduce the Non-stationary and Varying-discounting MDP (NVMDP) framework, which naturally accommodates non-stationarity and allows discount rates to vary with time and transitions. Infinite-horizon, stationary MDPs emerge as special cases of NVMDPs for identifying an optimal policy, and finite-horizon MDPs are also subsumed within the NVMDP formulations. Moreover, NVMDPs provide a flexible mechanism to shape optimal policies, without altering the state space, action space, or the reward structure. We establish the theoretical foundations of NVMDPs, including assumptions, state- and action-value formulation and recursion, matrix representation, optimality conditions, and policy improvement under finite state and action spaces. Building on these results, we adapt dynamic programming and generalized Q-learning algorithms to NVMDPs, along with formal convergence proofs. For problems requiring function approximation, we extend the Policy Gradient Theorem and the policy improvement bound in Trust Region Policy Optimization (TRPO), offering proofs in both scalar and matrix forms. Empirical evaluations in a non-stationary gridworld environment demonstrate that NVMDP-based algorithms successfully recover optimal trajectories under multiple reward and discounting schemes, whereas original Q-learning fails. These results collectively show that NVMDPs provide a theoretically sound and practically effective framework for reinforcement learning, requiring only minor algorithmic modifications while enabling robust handling of non-stationarity and explicit optimal policy shaping.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 12,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17599",
        "abs_url": "https://arxiv.org/abs/2511.17599",
        "pdf_url": "https://arxiv.org/pdf/2511.17599",
        "title": "From Projection to Prediction: Beyond Logits for Scalable Language Models",
        "authors": [
            "Jianbing Dong",
            "Jianbin Chang"
        ],
        "comments": "17 pages, 2 figures, 4 algorithms",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Training Large Language Models (LLMs) typically involves a two-stage pipeline at the output layer: hidden states are projected into vocabulary logits via a linear transformation (lm_head), followed by cross-entropy loss computation against target tokens. While conceptually simple, this design incurs substantial overhead. The intermediate logits tensor, with dimensions proportional to batch size, sequence length, and vocabulary size, must be fully materialized in GPU memory, even though only one target token per position is ultimately used. This leads to significant memory footprint and bandwidth comsumption, limiting scalability and slowing training throughput. In this work, we introduce a novel approach to integrates the output projection and loss prediction into a single operation. By directly computing the loss from hidden states and target tokens, our approach bypasses explicit logits materialization. This design reduces memory usage and alleviates bandwidth pressure. Experiments on LLM training demonstrate that our method achieves substantial memory savings and measurable speedups compared to the standard two-stage pipeline, enabling large batch sizes and longer sequences without sacrificing accuracy. Our work highlights the benefits of rethinking the boundary between projection and prediction, offering a practical systems optimization for efficient LLM training.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 13,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17601",
        "abs_url": "https://arxiv.org/abs/2511.17601",
        "pdf_url": "https://arxiv.org/pdf/2511.17601",
        "title": "Generalizable and Efficient Automated Scoring with a Knowledge-Distilled Multi-Task Mixture-of-Experts",
        "authors": [
            "Luyang Fang",
            "Tao Wang",
            "Ping Ma",
            "Xiaoming Zhai"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Automated scoring of written constructed responses typically relies on separate models per task, straining computational resources, storage, and maintenance in real-world education settings. We propose UniMoE-Guided, a knowledge-distilled multi-task Mixture-of-Experts (MoE) approach that transfers expertise from multiple task-specific large models (teachers) into a single compact, deployable model (student). The student combines (i) a shared encoder for cross-task representations, (ii) a gated MoE block that balances shared and task-specific processing, and (iii) lightweight task heads. Trained with both ground-truth labels and teacher guidance, the student matches strong task-specific models while being far more efficient to train, store, and deploy. Beyond efficiency, the MoE layer improves transfer and generalization: experts develop reusable skills that boost cross-task performance and enable rapid adaptation to new tasks with minimal additions and tuning. On nine NGSS-aligned science-reasoning tasks (seven for training/evaluation and two held out for adaptation), UniMoE-Guided attains performance comparable to per-task models while using $\\sim$6$\\times$ less storage than maintaining separate students, and $87\\times$ less than the 20B-parameter teacher. The method offers a practical path toward scalable, reliable, and resource-efficient automated scoring for classroom and large-scale assessment systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 14,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17604",
        "abs_url": "https://arxiv.org/abs/2511.17604",
        "pdf_url": "https://arxiv.org/pdf/2511.17604",
        "title": "BrainHGT: A Hierarchical Graph Transformer for Interpretable Brain Network Analysis",
        "authors": [
            "Jiajun Ma",
            "Yongchao Zhang",
            "Chao Zhang",
            "Zhao Lv",
            "Shengbing Pei"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Graph Transformer shows remarkable potential in brain network analysis due to its ability to model graph structures and complex node relationships. Most existing methods typically model the brain as a flat network, ignoring its modular structure, and their attention mechanisms treat all brain region connections equally, ignoring distance-related node connection patterns. However, brain information processing is a hierarchical process that involves local and long-range interactions between brain regions, interactions between regions and sub-functional modules, and interactions among functional modules themselves. This hierarchical interaction mechanism enables the brain to efficiently integrate local computations and global information flow, supporting the execution of complex cognitive functions. To address this issue, we propose BrainHGT, a hierarchical Graph Transformer that simulates the brain's natural information processing from local regions to global communities. Specifically, we design a novel long-short range attention encoder that utilizes parallel pathways to handle dense local interactions and sparse long-range connections, thereby effectively alleviating the over-globalizing issue. To further capture the brain's modular architecture, we designe a prior-guided clustering module that utilizes a cross-attention mechanism to group brain regions into functional communities and leverage neuroanatomical prior to guide the clustering process, thereby improving the biological plausibility and interpretability. Experimental results indicate that our proposed method significantly improves performance of disease identification, and can reliably capture the sub-functional modules of the brain, demonstrating its interpretability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 15,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17605",
        "abs_url": "https://arxiv.org/abs/2511.17605",
        "pdf_url": "https://arxiv.org/pdf/2511.17605",
        "title": "Copula Based Fusion of Clinical and Genomic Machine Learning Risk Scores for Breast Cancer Risk Stratification",
        "authors": [
            "Agnideep Aich",
            "Sameera Hewage",
            "Md Monzur Murshed"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Methodology (stat.ME); Machine Learning (stat.ML)",
        "abstract": "Clinical and genomic models are both used to predict breast cancer outcomes, but they are often combined using simple linear rules that do not account for how their risk scores relate, especially at the extremes. Using the METABRIC breast cancer cohort, we studied whether directly modeling the joint relationship between clinical and genomic machine learning risk scores could improve risk stratification for 5-year cancer-specific mortality. We created a binary 5-year cancer-death outcome and defined two sets of predictors: a clinical set (demographic, tumor, and treatment variables) and a genomic set (gene-expression $z$-scores). We trained several supervised classifiers, such as Random Forest and XGBoost, and used 5-fold cross-validated predicted probabilities as unbiased risk scores. These scores were converted to pseudo-observations on $(0,1)^2$ to fit Gaussian, Clayton, and Gumbel copulas. Clinical models showed good discrimination (AUC 0.783), while genomic models had moderate performance (AUC 0.681). The joint distribution was best captured by a Gaussian copula (bootstrap $p=0.997$), which suggests a symmetric, moderately strong positive relationship. When we grouped patients based on this relationship, Kaplan-Meier curves showed clear differences: patients who were high-risk in both clinical and genomic scores had much poorer survival than those high-risk in only one set. These results show that copula-based fusion works in real-world cohorts and that considering dependencies between scores can better identify patient subgroups with the worst prognosis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 16,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17610",
        "abs_url": "https://arxiv.org/abs/2511.17610",
        "pdf_url": "https://arxiv.org/pdf/2511.17610",
        "title": "Finding Pre-Injury Patterns in Triathletes from Lifestyle, Recovery and Load Dynamics Features",
        "authors": [
            "Leonardo Rossi",
            "Bruno Rodrigues"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Triathlon training, which involves high-volume swimming, cycling, and running, places athletes at substantial risk for overuse injuries due to repetitive physiological stress. Current injury prediction approaches primarily rely on training load metrics, often neglecting critical factors such as sleep quality, stress, and individual lifestyle patterns that significantly influence recovery and injury susceptibility. We introduce a novel synthetic data generation framework tailored explicitly for triathlon. This framework generates physiologically plausible athlete profiles, simulates individualized training programs that incorporate periodization and load-management principles, and integrates daily-life factors such as sleep quality, stress levels, and recovery states. We evaluated machine learning models (LASSO, Random Forest, and XGBoost) showing high predictive performance (AUC up to 0.86), identifying sleep disturbances, heart rate variability, and stress as critical early indicators of injury risk. This wearable-driven approach not only enhances injury prediction accuracy but also provides a practical solution to overcoming real-world data limitations, offering a pathway toward a holistic, context-aware athlete monitoring.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 17,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17611",
        "abs_url": "https://arxiv.org/abs/2511.17611",
        "pdf_url": "https://arxiv.org/pdf/2511.17611",
        "title": "AI-driven Generation of MALDI-TOF MS for Microbial Characterization",
        "authors": [
            "Lucía Schmidt-Santiago",
            "David Rodríguez-Temporal",
            "Carlos Sevilla-Salcedo",
            "Vanessa Gómez-Verdejo"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)",
        "abstract": "Matrix-Assisted Laser Desorption/Ionization Time-of-Flight Mass Spectrometry (MALDI-TOF MS) has become a cornerstone technology in clinical microbiology, enabling rapid and accurate microbial identification. However, the development of data-driven diagnostic models remains limited by the lack of sufficiently large, balanced, and standardized spectral datasets. This study investigates the use of deep generative models to synthesize realistic MALDI-TOF MS spectra, aiming to overcome data scarcity and support the development of robust machine learning tools in microbiology. We adapt and evaluate three generative models, Variational Autoencoders (MALDIVAEs), Generative Adversarial Networks (MALDIGANs), and Denoising Diffusion Probabilistic Model (MALDIffusion), for the conditional generation of microbial spectra guided by species labels. Generation is conditioned on species labels, and spectral fidelity and diversity are assessed using diverse metrics. Our experiments show that synthetic data generated by MALDIVAE, MALDIGAN, and MALDIffusion are statistically and diagnostically comparable to real measurements, enabling classifiers trained exclusively on synthetic samples to reach performance levels similar to those trained on real data. While all models faithfully reproduce the peak structure and variability of MALDI-TOF spectra, MALDIffusion obtains this fidelity at a substantially higher computational cost, and MALDIGAN shows competitive but slightly less stable behaviour. In contrast, MALDIVAE offers the most favorable balance between realism, stability, and efficiency. Furthermore, augmenting minority species with synthetic spectra markedly improves classification accuracy, effectively mitigating class imbalance and domain mismatch without compromising the authenticity of the generated data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 18,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17622",
        "abs_url": "https://arxiv.org/abs/2511.17622",
        "pdf_url": "https://arxiv.org/pdf/2511.17622",
        "title": "Neurocircuitry-Inspired Hierarchical Graph Causal Attention Networks for Explainable Depression Identification",
        "authors": [
            "Weidao Chen",
            "Yuxiao Yang",
            "Yueming Wang"
        ],
        "comments": "Under review for ICLR 2026",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Major Depressive Disorder (MDD), affecting millions worldwide, exhibits complex pathophysiology manifested through disrupted brain network dynamics. Although graph neural networks that leverage neuroimaging data have shown promise in depression diagnosis, existing approaches are predominantly data-driven and operate largely as black-box models, lacking neurobiological interpretability. Here, we present NH-GCAT (Neurocircuitry-Inspired Hierarchical Graph Causal Attention Networks), a novel framework that bridges neuroscience domain knowledge with deep learning by explicitly and hierarchically modeling depression-specific mechanisms at different spatial scales. Our approach introduces three key technical contributions: (1) at the local brain regional level, we design a residual gated fusion module that integrates temporal blood oxygenation level dependent (BOLD) dynamics with functional connectivity patterns, specifically engineered to capture local depression-relevant low-frequency neural oscillations; (2) at the multi-regional circuit level, we propose a hierarchical circuit encoding scheme that aggregates regional node representations following established depression neurocircuitry organization, and (3) at the multi-circuit network level, we develop a variational latent causal attention mechanism that leverages a continuous probabilistic latent space to infer directed information flow among critical circuits, characterizing disease-altered whole-brain inter-circuit interactions. Rigorous leave-one-site-out cross-validation on the REST-meta-MDD dataset demonstrates NH-GCAT's state-of-the-art performance in depression classification, achieving a sample-size weighted-average accuracy of 73.3\\% and an AUROC of 76.4\\%, while simultaneously providing neurobiologically meaningful explanations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 19,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17624",
        "abs_url": "https://arxiv.org/abs/2511.17624",
        "pdf_url": "https://arxiv.org/pdf/2511.17624",
        "title": "QML-HCS: A Hypercausal Quantum Machine Learning Framework for Non-Stationary Environments",
        "authors": [
            "Hector E Mozo"
        ],
        "comments": "11 pages, 10 figures, and 8 tables. The implementation and full source code of the Hypercausal Quantum Machine Learning System (QML-HCS) are openly available on GitHub at: this https URL",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "QML-HCS is a research-grade framework for constructing and analyzing quantum-inspired machine learning models operating under hypercausal feedback dynamics. Hypercausal refers to AI systems that leverage extended, deep, or nonlinear causal relationships (expanded causality) to reason, predict, and infer states beyond the capabilities of traditional causal models. Current machine learning and quantum-inspired systems struggle in non-stationary environments, where data distributions drift and models lack mechanisms for continuous adaptation, causal stability, and coherent state updating. QML-HCS addresses this limitation through a unified computational architecture that integrates quantum-inspired superposition principles, dynamic causal feedback, and deterministic-stochastic hybrid execution to enable adaptive behavior in changing environments. The framework implements a hypercausal processing core capable of reversible transformations, multipath causal propagation, and evaluation of alternative states under drift. Its architecture incorporates continuous feedback to preserve causal consistency and adjust model behavior without requiring full retraining. QML-HCS provides a reproducible and extensible Python interface backed by efficient computational routines, enabling experimentation in quantum-inspired learning, causal reasoning, and hybrid computation without the need for specialized hardware. A minimal simulation demonstrates how a hypercausal model adapts to a sudden shift in the input distribution while preserving internal coherence. This initial release establishes the foundational architecture for future theoretical extensions, benchmarking studies, and integration with classical and quantum simulation platforms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 20,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17626",
        "abs_url": "https://arxiv.org/abs/2511.17626",
        "pdf_url": "https://arxiv.org/pdf/2511.17626",
        "title": "Efficient Large-Scale Learning of Minimax Risk Classifiers",
        "authors": [
            "Kartheek Bondugula",
            "Santiago Mazuelas",
            "Aritz Pérez"
        ],
        "comments": "In IEEE ICDM (2025)",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Supervised learning with large-scale data usually leads to complex optimization problems, especially for classification tasks with multiple classes. Stochastic subgradient methods can enable efficient learning with a large number of samples for classification techniques that minimize the average loss over the training samples. However, recent techniques, such as minimax risk classifiers (MRCs), minimize the maximum expected loss and are not amenable to stochastic subgradient methods. In this paper, we present a learning algorithm based on the combination of constraint and column generation that enables efficient learning of MRCs with large-scale data for classification tasks with multiple classes. Experiments on multiple benchmark datasets show that the proposed algorithm provides upto a 10x speedup for general large-scale data and around a 100x speedup with a sizeable number of classes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 21,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17628",
        "abs_url": "https://arxiv.org/abs/2511.17628",
        "pdf_url": "https://arxiv.org/pdf/2511.17628",
        "title": "Rectifying Mean-Shift in Cascaded Precipitation Nowcasting",
        "authors": [
            "Fanbo Ju",
            "Haiyuan Shi",
            "Qingjian Ni"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Precipitation nowcasting, which aims to provide high spatio-temporal resolution precipitation forecasts by leveraging current radar observations, is a core task in regional weather forecasting. The cascaded architecture has emerged as the mainstream paradigm for deep learning-based precipitation nowcasting. This paradigm involves a deterministic model to predict macroscopic trends (or posterior mean), followed by a probabilistic model to generate local details (or local stochasticity). However, existing methods commonly overlook the conflation of the systematic distribution shift in deterministic predictions and the local stochasticity. As a result, the deterministic component's distribution shift contaminates the predictions of the probabilistic component, leading to inaccuracies in precipitation patterns and intensity, particularly over longer lead times. To address this issue, we introduce RectiCast, a two-stage framework that explicitly decouples the correction of mean-field shift from the generation of local stochasticity via a dual Flow Matching model. In the first stage, a deterministic model generates the posterior mean. In the second stage, we introduce a Rectifier to explicitly learn the distribution shift and produce a rectified mean. Subsequently, a Generator focuses on modeling the local stochasticity conditioned on the rectified mean. Experiments on SEVIR and MeteoNet demonstrate that RectiCast achieves significant performance improvements over existing state-of-the-art methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 22,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17629",
        "abs_url": "https://arxiv.org/abs/2511.17629",
        "pdf_url": "https://arxiv.org/pdf/2511.17629",
        "title": "Boundary-Aware Adversarial Filtering for Reliable Diagnosis under Extreme Class Imbalance",
        "authors": [
            "Yanxuan Yu",
            "Michael S. Hughes",
            "Julien Lee",
            "Jiacheng Zhou",
            "Andrew F. Laine"
        ],
        "comments": "5 pages, 3 figures. Submitted to IEEE ISBI (under review)",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We study classification under extreme class imbalance where recall and calibration are both critical, for example in medical diagnosis scenarios. We propose AF-SMOTE, a mathematically motivated augmentation framework that first synthesizes minority points and then filters them by an adversarial discriminator and a boundary utility model. We prove that, under mild assumptions on the decision boundary smoothness and class-conditional densities, our filtering step monotonically improves a surrogate of F_beta (for beta >= 1) while not inflating Brier score. On MIMIC-IV proxy label prediction and canonical fraud detection benchmarks, AF-SMOTE attains higher recall and average precision than strong oversampling baselines (SMOTE, ADASYN, Borderline-SMOTE, SVM-SMOTE), and yields the best calibration. We further validate these gains across multiple additional datasets beyond MIMIC-IV. Our successful application of AF-SMOTE to a healthcare dataset using a proxy label demonstrates in a disease-agnostic way its practical value in clinical situations, where missing true positive cases in rare diseases can have severe consequences.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 23,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17631",
        "abs_url": "https://arxiv.org/abs/2511.17631",
        "pdf_url": "https://arxiv.org/pdf/2511.17631",
        "title": "Enhanced Federated Deep Multi-View Clustering under Uncertainty Scenario",
        "authors": [
            "Bingjun Wei",
            "Xuemei Cao",
            "Jiafen Liu",
            "Haoyang Liang",
            "Xin Yang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Traditional Federated Multi-View Clustering assumes uniform views across clients, yet practical deployments reveal heterogeneous view completeness with prevalent incomplete, redundant, or corrupted data. While recent approaches model view heterogeneity, they neglect semantic conflicts from dynamic view combinations, failing to address dual uncertainties: view uncertainty (semantic inconsistency from arbitrary view pairings) and aggregation uncertainty (divergent client updates with imbalanced contributions). To address these, we propose a novel Enhanced Federated Deep Multi-View Clustering framework: first align local semantics, hierarchical contrastive fusion within clients resolves view uncertainty by eliminating semantic conflicts; a view adaptive drift module mitigates aggregation uncertainty through global-local prototype contrast that dynamically corrects parameter deviations; and a balanced aggregation mechanism coordinates client updates. Experimental results demonstrate that EFDMVC achieves superior robustness against heterogeneous uncertain views across multiple benchmark datasets, consistently outperforming all state-of-the-art baselines in comprehensive evaluations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 24,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17632",
        "abs_url": "https://arxiv.org/abs/2511.17632",
        "pdf_url": "https://arxiv.org/pdf/2511.17632",
        "title": "Smart Manufacturing: MLOps-Enabled Event-Driven Architecture for Enhanced Control in Steel Production",
        "authors": [
            "Bestoun S. Ahmed",
            "Tommaso Azzalin",
            "Andreas Kassler",
            "Andreas Thore",
            "Hans Lindback"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We explore a Digital Twin-Based Approach for Smart Manufacturing to improve Sustainability, Efficiency, and Cost-Effectiveness for a steel production plant. Our system is based on a micro-service edge-compute platform that ingests real-time sensor data from the process into a digital twin over a converged network infrastructure. We implement agile machine learning-based control loops in the digital twin to optimize induction furnace heating, enhance operational quality, and reduce process waste. Key to our approach is a Deep Reinforcement learning-based agent used in our machine learning operation (MLOps) driven system to autonomously correlate the system state with its digital twin to identify correction actions that aim to optimize power settings for the plant. We present the theoretical basis, architectural details, and practical implications of our approach to reduce manufacturing waste and increase production quality. We design the system for flexibility so that our scalable event-driven architecture can be adapted to various industrial applications. With this research, we propose a pivotal step towards the transformation of traditional processes into intelligent systems, aligning with sustainability goals and emphasizing the role of MLOps in shaping the future of data-driven manufacturing.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 25,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17637",
        "abs_url": "https://arxiv.org/abs/2511.17637",
        "pdf_url": "https://arxiv.org/pdf/2511.17637",
        "title": "PocketLLM: Ultimate Compression of Large Language Models via Meta Networks",
        "authors": [
            "Ye Tian",
            "Chengcheng Wang",
            "Jing Han",
            "Yehui Tang",
            "Kai Han"
        ],
        "comments": "AAAI 2026 camera ready",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
        "abstract": "As Large Language Models (LLMs) continue to grow in size, storing and transmitting them on edge devices becomes increasingly challenging. Traditional methods like quantization and pruning struggle to achieve extreme compression of LLMs without sacrificing accuracy. In this paper, we introduce PocketLLM, a novel approach to compress LLMs in a latent space via meta-networks. A simple encoder network is proposed to project the weights of LLMs into discrete latent vectors, which are then represented using a compact codebook. A lightweight decoder network is employed to map the codebook's representative vectors back to the original weight space. This method allows for significant compression of the large weights in LLMs, consisting solely of a small decoder, a concise codebook, and an index. Extensive experiments show that PocketLLM achieves superior performance even at significantly high compression ratios, e.g., compressing Llama 2-7B by 10x with a negligible drop in accuracy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 26,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17639",
        "abs_url": "https://arxiv.org/abs/2511.17639",
        "pdf_url": "https://arxiv.org/pdf/2511.17639",
        "title": "TTF: A Trapezoidal Temporal Fusion Framework for LTV Forecasting in Douyin",
        "authors": [
            "Yibing Wan",
            "Zhengxiong Guan",
            "Chaoli Zhang",
            "Xiaoyang Li",
            "Lai Xu",
            "Beibei Jia",
            "Zhenzhe Zheng",
            "Fan Wu"
        ],
        "comments": "Accepted by AAAI IAAI Track",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In the user growth scenario, Internet companies invest heavily in paid acquisition channels to acquire new users. But sustainable growth depends on acquired users' generating lifetime value (LTV) exceeding customer acquisition cost (CAC). In order to maximize LTV/CAC ratio, it is crucial to predict channel-level LTV in an early stage for further optimization of budget allocation. The LTV forecasting problem is significantly different from traditional time series forecasting problems, and there are three main challenges. Firstly, it is an unaligned multi-time series forecasting problem that each channel has a number of LTV series of different activation dates. Secondly, to predict in the early stage, it faces the imbalanced short-input long-output (SILO) challenge. Moreover, compared with the commonly used time series datasets, the real LTV series are volatile and non-stationary, with more frequent fluctuations and higher variance. In this work, we propose a novel framework called Trapezoidal Temporal Fusion (TTF) to address the above challenges. We introduce a trapezoidal multi-time series module to deal with data unalignment and SILO challenges, and output accurate predictions with a multi-tower structure called MT-FusionNet. The framework has been deployed to the online system for Douyin. Compared to the previously deployed online model, MAPEp decreased by 4.3%, and MAPEa decreased by 3.2%, where MAPEp denotes the point-wise MAPE of the LTV curve and MAPEa denotes the MAPE of the aggregated LTV.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 27,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17645",
        "abs_url": "https://arxiv.org/abs/2511.17645",
        "pdf_url": "https://arxiv.org/pdf/2511.17645",
        "title": "BlockCert: Certified Blockwise Extraction of Transformer Mechanisms",
        "authors": [
            "Sandro Andric"
        ],
        "comments": "16 pages, 1 figure",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Mechanistic interpretability aspires to reverse-engineer neural networks into explicit algorithms, while model editing seeks to modify specific behaviours without retraining. Both areas are typically evaluated with informal evidence and ad-hoc experiments, with few explicit guarantees about how far an extracted or edited model can drift from the original on relevant inputs. We introduce BlockCert, a framework for certified blockwise extraction of transformer mechanisms, and outline how a lightweight extension can support certified local edits. Given a pre-trained transformer and a prompt distribution, BlockCert extracts structured surrogate implementations for residual blocks together with machine-checkable certificates that bound approximation error, record coverage metrics, and hash the underlying artifacts. We formalize a simple Lipschitz-based composition theorem in Lean 4 that lifts these local guarantees to a global deviation bound. Empirically, we apply the framework to GPT-2 small, TinyLlama-1.1B-Chat, and Llama-3.2-3B. Across these models we obtain high per-block coverage and small residual errors on the evaluated prompts, and in the TinyLlama setting we show that a fully stitched model matches the baseline perplexity within approximately 6e-5 on stress prompts. Our results suggest that blockwise extraction with explicit certificates is feasible for real transformer language models and offers a practical bridge between mechanistic interpretability and formal reasoning about model behaviour.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 28,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17660",
        "abs_url": "https://arxiv.org/abs/2511.17660",
        "pdf_url": "https://arxiv.org/pdf/2511.17660",
        "title": "Frugality in second-order optimization: floating-point approximations for Newton's method",
        "authors": [
            "Giuseppe Carrino",
            "Elena Loli Piccolomini",
            "Elisa Riccietti",
            "Theo Mary"
        ],
        "comments": "Master Thesis for the Artificial Intelligence course at University of Bologna",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Optimization and Control (math.OC)",
        "abstract": "Minimizing loss functions is central to machine-learning training. Although first-order methods dominate practical applications, higher-order techniques such as Newton's method can deliver greater accuracy and faster convergence, yet are often avoided due to their computational cost. This work analyzes the impact of finite-precision arithmetic on Newton steps and establishes a convergence theorem for mixed-precision Newton optimizers, including \"quasi\" and \"inexact\" variants. The theorem provides not only convergence guarantees but also a priori estimates of the achievable solution accuracy. Empirical evaluations on standard regression benchmarks demonstrate that the proposed methods outperform Adam on the Australian and MUSH datasets. The second part of the manuscript introduces GN_k, a generalized Gauss-Newton method that enables partial computation of second-order derivatives. GN_k attains performance comparable to full Newton's method on regression tasks while requiring significantly fewer derivative evaluations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 29,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17662",
        "abs_url": "https://arxiv.org/abs/2511.17662",
        "pdf_url": "https://arxiv.org/pdf/2511.17662",
        "title": "Enhancing Breast Cancer Prediction with LLM-Inferred Confounders",
        "authors": [
            "Debmita Roy"
        ],
        "comments": "2 pages, 1 figure, 1 table",
        "subjects": "Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)",
        "abstract": "This study enhances breast cancer prediction by using large language models to infer the likelihood of confounding diseases, namely diabetes, obesity, and cardiovascular disease, from routine clinical data. These AI-generated features improved Random Forest model performance, particularly for LLMs like Gemma (3.9%) and Llama (6.4%). The approach shows promise for noninvasive prescreening and clinical integration, supporting improved early detection and shared decision-making in breast cancer diagnosis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 30,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17664",
        "abs_url": "https://arxiv.org/abs/2511.17664",
        "pdf_url": "https://arxiv.org/pdf/2511.17664",
        "title": "CubeletWorld: A New Abstraction for Scalable 3D Modeling",
        "authors": [
            "Azlaan Mustafa Samad",
            "Hoang H. Nguyen",
            "Lukas Berg",
            "Henrik Müller",
            "Yuan Xue",
            "Daniel Kudenko",
            "Zahra Ahmadi"
        ],
        "comments": "10 pages, 5 figures",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV); Computers and Society (cs.CY)",
        "abstract": "Modern cities produce vast streams of heterogeneous data, from infrastructure maps to mobility logs and satellite imagery. However, integrating these sources into coherent spatial models for planning and prediction remains a major challenge. Existing agent-centric methods often rely on direct environmental sensing, limiting scalability and raising privacy concerns. This paper introduces CubeletWorld, a novel framework for representing and analyzing urban environments through a discretized 3D grid of spatial units called cubelets. This abstraction enables privacy-preserving modeling by embedding diverse data signals, such as infrastructure, movement, or environmental indicators, into localized cubelet states. CubeletWorld supports downstream tasks such as planning, navigation, and occupancy prediction without requiring agent-driven sensing. To evaluate this paradigm, we propose the CubeletWorld State Prediction task, which involves predicting the cubelet state using a realistic dataset containing various urban elements like streets and buildings through this discretized representation. We explore a range of modified core models suitable for our setting and analyze challenges posed by increasing spatial granularity, specifically the issue of sparsity in representation and scalability of baselines. In contrast to existing 3D occupancy prediction models, our cubelet-centric approach focuses on inferring state at the spatial unit level, enabling greater generalizability across regions and improved privacy compliance. Our results demonstrate that CubeletWorld offers a flexible and extensible framework for learning from complex urban data, and it opens up new possibilities for scalable simulation and decision support in domains such as socio-demographic modeling, environmental monitoring, and emergency response. The code and datasets can be downloaded from here.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 31,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17665",
        "abs_url": "https://arxiv.org/abs/2511.17665",
        "pdf_url": "https://arxiv.org/pdf/2511.17665",
        "title": "GANGR: GAN-Assisted Scalable and Efficient Global Routing Parallelization",
        "authors": [
            "Hadi Khodaei Jooshin",
            "Inna Partin-Vaisband"
        ],
        "comments": "Accepted in DATE 2026",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Global routing is a critical stage in electronic design automation (EDA) that enables early estimation and optimization of the routability of modern integrated circuits with respect to congestion, power dissipation, and design complexity. Batching is a primary concern in top-performing global routers, grouping nets into manageable sets to enable parallel processing and efficient resource usage. This process improves memory usage, scalable parallelization on modern hardware, and routing congestion by controlling net interactions within each batch. However, conventional batching methods typically depend on heuristics that are computationally expensive and can lead to suboptimal results (oversized batches with conflicting nets, excessive batch counts degrading parallelization, and longer batch generation times), ultimately limiting scalability and efficiency. To address these limitations, a novel batching algorithm enhanced with Wasserstein generative adversarial networks (WGANs) is introduced in this paper, enabling more effective parallelization by generating fewer higher-quality batches in less time. The proposed algorithm is tested on the latest ISPD'24 contest benchmarks, demonstrating up to 40% runtime reduction with only 0.002% degradation in routing quality as compared to state-of-the-art router.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 32,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17675",
        "abs_url": "https://arxiv.org/abs/2511.17675",
        "pdf_url": "https://arxiv.org/pdf/2511.17675",
        "title": "Lane-Frame Quantum Multimodal Driving Forecasts for the Trajectory of Autonomous Vehicles",
        "authors": [
            "Navneet Singh",
            "Shiva Raj Pokhrel"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Quantum Physics (quant-ph)",
        "abstract": "Trajectory forecasting for autonomous driving must deliver accurate, calibrated multi-modal futures under tight compute and latency constraints. We propose a compact hybrid quantum architecture that aligns quantum inductive bias with road-scene structure by operating in an ego-centric, lane-aligned frame and predicting residual corrections to a kinematic baseline instead of absolute poses. The model combines a transformer-inspired quantum attention encoder (9 qubits), a parameter-lean quantum feedforward stack (64 layers, ${\\sim}1200$ trainable angles), and a Fourier-based decoder that uses shallow entanglement and phase superposition to generate 16 trajectory hypotheses in a single pass, with mode confidences derived from the latent spectrum. All circuit parameters are trained with Simultaneous Perturbation Stochastic Approximation (SPSA), avoiding backpropagation through non-analytic components. In the Waymo Open Motion Dataset, the model achieves minADE (minimum Average Displacement Error) of \\SI{1.94}{m} and minFDE (minimum Final Displacement Error) of \\SI{3.56}{m} in the $16$ models predicted over the horizon of \\SI{2.0}{s}, consistently outperforming a kinematic baseline with reduced miss rates and strong recall. Ablations confirm that residual learning in the lane frame, truncated Fourier decoding, shallow entanglement, and spectrum-based ranking focus capacity where it matters, yielding stable optimization and reliable multi-modal forecasts from small, shallow quantum circuits on a modern autonomous-driving benchmark.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 33,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17677",
        "abs_url": "https://arxiv.org/abs/2511.17677",
        "pdf_url": "https://arxiv.org/pdf/2511.17677",
        "title": "A Hybrid Classical-Quantum Fine Tuned BERT for Text Classification",
        "authors": [
            "Abu Kaisar Mohammad Masum",
            "Naveed Mahmud",
            "M. Hassan Najafi",
            "Sercan Aygun"
        ],
        "comments": "This paper has been accepted by First AAAI Symposium on Quantum Information & Machine Learning (QIML): Bridging Quantum Computing and Artificial Intelligence at AAAI 2025 Fall Symposium",
        "subjects": "Machine Learning (cs.LG); Quantum Physics (quant-ph)",
        "abstract": "Fine-tuning BERT for text classification can be computationally challenging and requires careful hyper-parameter tuning. Recent studies have highlighted the potential of quantum algorithms to outperform conventional methods in machine learning and text classification tasks. In this work, we propose a hybrid approach that integrates an n-qubit quantum circuit with a classical BERT model for text classification. We evaluate the performance of the fine-tuned classical-quantum BERT and demonstrate its feasibility as well as its potential in advancing this research area. Our experimental results show that the proposed hybrid model achieves performance that is competitive with, and in some cases better than, the classical baselines on standard benchmark datasets. Furthermore, our approach demonstrates the adaptability of classical-quantum models for fine-tuning pre-trained models across diverse datasets. Overall, the hybrid model highlights the promise of quantum computing in achieving improved performance for text classification tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 34,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17687",
        "abs_url": "https://arxiv.org/abs/2511.17687",
        "pdf_url": "https://arxiv.org/pdf/2511.17687",
        "title": "Boosting Brain-inspired Path Integration Efficiency via Learning-based Replication of Continuous Attractor Neurodynamics",
        "authors": [
            "Zhangyu Ge",
            "Xu He",
            "Lingfei Mo",
            "Xiaolin Meng",
            "Wenxuan Yin",
            "Youdong Zhang",
            "Lansong Jiang",
            "Fengyuan Liu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)",
        "abstract": "The brain's Path Integration (PI) mechanism offers substantial guidance and inspiration for Brain-Inspired Navigation (BIN). However, the PI capability constructed by the Continuous Attractor Neural Networks (CANNs) in most existing BIN studies exhibits significant computational redundancy, and its operational efficiency needs to be improved; otherwise, it will not be conducive to the practicality of BIN technology. To address this, this paper proposes an efficient PI approach using representation learning models to replicate CANN neurodynamic patterns. This method successfully replicates the neurodynamic patterns of CANN-modeled Head Direction Cells (HDCs) and Grid Cells (GCs) using lightweight Artificial Neural Networks (ANNs). These ANN-reconstructed HDC and GC models are then integrated to achieve brain-inspired PI for Dead Reckoning (DR). Benchmark tests in various environments, compared with the well-known NeuroSLAM system, demonstrate that this work not only accurately replicates the neurodynamic patterns of navigation cells but also matches NeuroSLAM in positioning accuracy. Moreover, efficiency improvements of approximately 17.5% on the general-purpose device and 40~50% on the edge device were observed, compared with NeuroSLAM. This work offers a novel implementation strategy to enhance the practicality of BIN technology and holds potential for further extension.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 35,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17693",
        "abs_url": "https://arxiv.org/abs/2511.17693",
        "pdf_url": "https://arxiv.org/pdf/2511.17693",
        "title": "DeepCoT: Deep Continual Transformers for Real-Time Inference on Data Streams",
        "authors": [
            "Ginés Carreto Picón",
            "Peng Yuan Zhou",
            "Qi Zhang",
            "Alexandros Iosifidis"
        ],
        "comments": "13 pages, 5 figures",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Transformer-based models have dramatically increased their size and parameter count to tackle increasingly complex tasks. At the same time, there is a growing demand for low-latency inference on resource-constrained devices that achieves high performance. In particular, stream data inference is typically performed over a sliding temporal window, leading to highly redundant computations. The recent Continual Transformers have addressed this issue, but they can only be effectively used in shallow models, which limits their scope and generalization power. In this paper, we propose the Deep Continual Transformer (DeepCoT), a redundancy-free encoder-only model that can be applied over existing deep encoder architectures with minimal changes. In our experiments over audio, video, and text streams, we show that DeepCoTs retain comparative performance to their non-continual baselines while offering a linear computational cost for all Transformer layers, which reduces up to two orders of magnitude in the running time compared to previous efficient models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 36,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17741",
        "abs_url": "https://arxiv.org/abs/2511.17741",
        "pdf_url": "https://arxiv.org/pdf/2511.17741",
        "title": "Diffusion Models are Molecular Dynamics Simulators",
        "authors": [
            "Justin Diamond",
            "Markus Lill"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "We prove that a denoising diffusion sampler equipped with a sequential bias across the batch dimension is exactly an Euler-Maruyama integrator for overdamped Langevin dynamics. Each reverse denoising step, with its associated spring stiffness, can be interpreted as one step of a stochastic differential equation with an effective time step set jointly by the noise schedule and that stiffness. The learned score then plays the role of the drift, equivalently the gradient of a learned energy, yielding a precise correspondence between diffusion sampling and Langevin time evolution. This equivalence recasts molecular dynamics (MD) in terms of diffusion models. Accuracy is no longer tied to a fixed, extremely small MD time step; instead, it is controlled by two scalable knobs: model capacity, which governs how well the drift is approximated, and the number of denoising steps, which sets the integrator resolution. In practice, this leads to a fully data-driven MD framework that learns forces from uncorrelated equilibrium snapshots, requires no hand-engineered force fields, uses no trajectory data for training, and still preserves the Boltzmann distribution associated with the learned energy. We derive trajectory-level, information-theoretic error bounds that cleanly separate discretization error from score-model error, clarify how temperature enters through the effective spring, and show that the resulting sampler generates molecular trajectories with MD-like temporal correlations, even though the model is trained only on static configurations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 37,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17754",
        "abs_url": "https://arxiv.org/abs/2511.17754",
        "pdf_url": "https://arxiv.org/pdf/2511.17754",
        "title": "Periodicity-Enforced Neural Network for Designing Deterministic Lateral Displacement Devices",
        "authors": [
            "Andrew Lee",
            "Mahir Mobarrat",
            "Xiaolin Chen"
        ],
        "comments": "Accepted to IEEE International Conference on Data Mining (ICDM) 2025 REU Symposium",
        "subjects": "Machine Learning (cs.LG); Fluid Dynamics (physics.flu-dyn)",
        "abstract": "Deterministic Lateral Displacement (DLD) devices enable liquid biopsy for cancer detection by separating circulating tumor cells (CTCs) from blood samples based on size, but designing these microfluidic devices requires computationally expensive Navier-Stokes simulations and particle-tracing analyses. While recent surrogate modeling approaches using deep learning have accelerated this process, they often inadequately handle the critical periodic boundary conditions of DLD unit cells, leading to cumulative errors in multi-unit device predictions. This paper introduces a periodicity-enforced surrogate modeling approach that incorporates periodic layers, neural network components that guarantee exact periodicity without penalty terms or output modifications, into deep learning architectures for DLD device design. The proposed method employs three sub-networks to predict steady-state, non-dimensional velocity and pressure fields (u, v, p) rather than directly predicting critical diameters or particle trajectories, enabling complete flow field characterization and enhanced design flexibility. Periodic layers ensure exact matching of flow variables across unit cell boundaries through architectural enforcement rather than soft penalty-based approaches. Validation on 120 CFD-generated geometries demonstrates that the periodic layer implementation achieves 0.478% critical diameter error while maintaining perfect periodicity consistency, representing an 85.4% improvement over baseline methods. The approach enables efficient and accurate DLD device design with guaranteed boundary condition satisfaction for multi-unit device applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 38,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17776",
        "abs_url": "https://arxiv.org/abs/2511.17776",
        "pdf_url": "https://arxiv.org/pdf/2511.17776",
        "title": "PrismSSL: One Interface, Many Modalities; A Single-Interface Library for Multimodal Self-Supervised Learning",
        "authors": [
            "Melika Shirian",
            "Kianoosh Vadaei",
            "Kian Majlessi",
            "Audrina Ebrahimi",
            "Arshia Hemmat",
            "Peyman Adibi",
            "Hossein Karshenas"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Multimedia (cs.MM)",
        "abstract": "We present PrismSSL, a Python library that unifies state-of-the-art self-supervised learning (SSL) methods across audio, vision, graphs, and cross-modal settings in a single, modular codebase. The goal of the demo is to show how researchers and practitioners can: (i) install, configure, and run pretext training with a few lines of code; (ii) reproduce compact benchmarks; and (iii) extend the framework with new modalities or methods through clean trainer and dataset abstractions. PrismSSL is packaged on PyPI, released under the MIT license, integrates tightly with HuggingFace Transformers, and provides quality-of-life features such as distributed training in PyTorch, Optuna-based hyperparameter search, LoRA fine-tuning for Transformer backbones, animated embedding visualizations for sanity checks, Weights & Biases logging, and colorful, structured terminal logs for improved usability and clarity. In addition, PrismSSL offers a graphical dashboard - built with Flask and standard web technologies - that enables users to configure and launch training pipelines with minimal coding. The artifact (code and data recipes) will be publicly available and reproducible.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 39,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17782",
        "abs_url": "https://arxiv.org/abs/2511.17782",
        "pdf_url": "https://arxiv.org/pdf/2511.17782",
        "title": "Smoothed Agnostic Learning of Halfspaces over the Hypercube",
        "authors": [
            "Yiwen Kou",
            "Raghu Meka"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computational Complexity (cs.CC); Machine Learning (stat.ML)",
        "abstract": "Agnostic learning of Boolean halfspaces is a fundamental problem in computational learning theory, but it is known to be computationally hard even for weak learning. Recent work [CKKMK24] proposed smoothed analysis as a way to bypass such hardness, but existing frameworks rely on additive Gaussian perturbations, making them unsuitable for discrete domains. We introduce a new smoothed agnostic learning framework for Boolean inputs, where perturbations are modeled via random bit flips. This defines a natural discrete analogue of smoothed optimality generalizing the Gaussian case. Under strictly subexponential assumptions on the input distribution, we give an efficient algorithm for learning halfspaces in this model, with runtime and sample complexity approximately n raised to a poly(1/(sigma * epsilon)) factor. Previously, such algorithms were known only with strong structural assumptions for the discrete hypercube, for example, independent coordinates or symmetric distributions. Our result provides the first computationally efficient guarantee for smoothed agnostic learning of halfspaces over the Boolean hypercube, bridging the gap between worst-case intractability and practical learnability in discrete settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 40,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17784",
        "abs_url": "https://arxiv.org/abs/2511.17784",
        "pdf_url": "https://arxiv.org/pdf/2511.17784",
        "title": "Improved Sample Complexity for Full Coverage in Compact and Continuous Spaces",
        "authors": [
            "Lyu Yuhuan"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Verifying uniform conditions over continuous spaces through random sampling is fundamental in machine learning and control theory, yet classical coverage analyses often yield conservative bounds, particularly at small failure probabilities. We study uniform random sampling on the $d$-dimensional unit hypercube and analyze the number of uncovered subcubes after discretization. By applying a concentration inequality to the uncovered-count statistic, we derive a sample complexity bound with a logarithmic dependence on the failure probability ($\\delta$), i.e., $M =O( \\tilde{C}\\ln(\\frac{2\\tilde{C}}{\\delta}))$, which contrasts sharply with the classical linear $1/\\delta$ dependence. Under standard Lipschitz and uniformity assumptions, we present a self-contained derivation and compare our result with classical coupon-collector rates. Numerical studies across dimensions, precision levels, and confidence targets indicate that our bound tracks practical coverage requirements more tightly and scales favorably as $\\delta \\to 0$. Our findings offer a sharper theoretical tool for algorithms that rely on grid-based coverage guarantees, enabling more efficient sampling, especially in high-confidence regimes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 41,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17787",
        "abs_url": "https://arxiv.org/abs/2511.17787",
        "pdf_url": "https://arxiv.org/pdf/2511.17787",
        "title": "Data-Driven Predictive Modeling of Microfluidic Cancer Cell Separation Using a Deterministic Lateral Displacement Device",
        "authors": [
            "Elizabeth Chen",
            "Andrew Lee",
            "Tanbir Sarowar",
            "Xiaolin Chen"
        ],
        "comments": "Accepted to IEEE International Conference on Data Mining (ICDM) 2025 REU Symposium",
        "subjects": "Machine Learning (cs.LG); Medical Physics (physics.med-ph); Quantitative Methods (q-bio.QM)",
        "abstract": "Deterministic Lateral Displacement (DLD) devices are widely used in microfluidics for label-free, size-based separation of particles and cells, with particular promise in isolating circulating tumor cells (CTCs) for early cancer diagnostics. This study focuses on the optimization of DLD design parameters, such as row shift fraction, post size, and gap distance, to enhance the selective isolation of lung cancer cells based on their physical properties. To overcome the challenges of rare CTC detection and reduce reliance on computationally intensive simulations, machine learning models including gradient boosting, k-nearest neighbors, random forest, and multilayer perceptron (MLP) regressors are employed. Trained on a large, numerically validated dataset, these models predict particle trajectories and identify optimal device configurations, enabling high-throughput and cost-effective DLD design. Beyond trajectory prediction, the models aid in isolating critical design variables, offering a systematic, data-driven framework for automated DLD optimization. This integrative approach advances the development of scalable and precise microfluidic systems for cancer diagnostics, contributing to the broader goals of early detection and personalized medicine.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 42,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17789",
        "abs_url": "https://arxiv.org/abs/2511.17789",
        "pdf_url": "https://arxiv.org/pdf/2511.17789",
        "title": "Physical Reinforcement Learning",
        "authors": [
            "Sam Dillavou",
            "Shruti Mishra"
        ],
        "comments": "9 pages 4 figures",
        "subjects": "Machine Learning (cs.LG); Disordered Systems and Neural Networks (cond-mat.dis-nn)",
        "abstract": "Digital computers are power-hungry and largely intolerant of damaged components, making them potentially difficult tools for energy-limited autonomous agents in uncertain environments. Recently developed Contrastive Local Learning Networks (CLLNs) - analog networks of self-adjusting nonlinear resistors - are inherently low-power and robust to physical damage, but were constructed to perform supervised learning. In this work we demonstrate success on two simple RL problems using Q-learning adapted for simulated CLLNs. Doing so makes explicit the components (beyond the network being trained) required to enact various tools in the RL toolbox, some of which (policy function and value function) are more natural in this system than others (replay buffer). We discuss assumptions such as the physical safety that digital hardware requires, CLLNs can forgo, and biological systems cannot rely on, and highlight secondary goals that are important in biology and trainable in CLLNs, but make little sense in digital computers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 43,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17796",
        "abs_url": "https://arxiv.org/abs/2511.17796",
        "pdf_url": "https://arxiv.org/pdf/2511.17796",
        "title": "Semi-Supervised Federated Multi-Label Feature Selection with Fuzzy Information Measures",
        "authors": [
            "Afsaneh Mahanipour",
            "Hana Khamfroush"
        ],
        "comments": "This paper has been accepted for presentation at GLOBECOM 2025",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Multi-label feature selection (FS) reduces the dimensionality of multi-label data by removing irrelevant, noisy, and redundant features, thereby boosting the performance of multi-label learning models. However, existing methods typically require centralized data, which makes them unsuitable for distributed and federated environments where each device/client holds its own local dataset. Additionally, federated methods often assume that clients have labeled data, which is unrealistic in cases where clients lack the expertise or resources to label task-specific data. To address these challenges, we propose a Semi-Supervised Federated Multi-Label Feature Selection method, called SSFMLFS, where clients hold only unlabeled data, while the server has limited labeled data. SSFMLFS adapts fuzzy information theory to a federated setting, where clients compute fuzzy similarity matrices and transmit them to the server, which then calculates feature redundancy and feature-label relevancy degrees. A feature graph is constructed by modeling features as vertices, assigning relevancy and redundancy degrees as vertex weights and edge weights, respectively. PageRank is then applied to rank the features by importance. Extensive experiments on five real-world datasets from various domains, including biology, images, music, and text, demonstrate that SSFMLFS outperforms other federated and centralized supervised and semi-supervised approaches in terms of three different evaluation metrics in non-IID data distribution setting.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 44,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17801",
        "abs_url": "https://arxiv.org/abs/2511.17801",
        "pdf_url": "https://arxiv.org/pdf/2511.17801",
        "title": "Layer-Wise High-Impact Parameter Ratio Optimization in Post-Training Quantization for Large Language Models",
        "authors": [
            "Cuong Pham",
            "Hoang Anh Dung",
            "Cuong C. Nguyen",
            "Trung Le",
            "Gustavo Carneiro",
            "Thanh-Toan Do"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Large language models (LLMs) have significantly advanced natural language processing, but their massive parameter counts create substantial computational and memory challenges during deployment. Post-training quantization (PTQ) has emerged as a promising approach to mitigate these challenges with minimal overhead. While existing PTQ methods can effectively quantize LLMs, they experience substantial accuracy loss at extremely low bit-widths, primarily due to high-impact parameters that significantly influence quantization performance. Several approaches address these issues by identifying and retaining the high-impact parameters in FP16 format. However, they apply fixed ratios of high-impact parameters across all layers, overlooking layer-wise sensitivity variations. In this paper, we propose a quadratic optimization framework that determines layer-specific ratios of high-impact parameters while considering inter-layer dependencies. We quantize high-impact parameters to moderate bit-widths, which often result in negligible performance degradation in quantized LLMs, while the remaining parameters can be quantized to extremely low bit-widths. Under the same resource-constrained budget, this allows for preserving more high-impact parameters than methods that keep selecting a few in FP16 format. Additionally, the proposed framework allows us to leverage an advanced quantization method that often requires extensive learnable parameters solely for high-impact parameters, while applying a computationally efficient method to the rest. Our approach achieves an effective balance between computational efficiency and model accuracy while maintaining high performance compared to state-of-the-art methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 45,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17809",
        "abs_url": "https://arxiv.org/abs/2511.17809",
        "pdf_url": "https://arxiv.org/pdf/2511.17809",
        "title": "Adaptive Layer-Wise Transformations for Post-Training Quantization of Large Language Models",
        "authors": [
            "Cuong Pham",
            "Hoang Anh Dung",
            "Cuong C. Nguyen",
            "Trung Le",
            "Gustavo Carneiro",
            "Jianfei Cai",
            "Thanh-Toan Do"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Large language models require significant computational resources for deployment, making quantization essential for practical applications. However, the main obstacle to effective quantization lies in systematic outliers in activations and weights, which cause substantial LLM performance degradation, especially at low-bit settings. While existing transformation-based methods like affine and rotation transformations successfully mitigate outliers, they apply the homogeneous transformation setting, i.e., using the same transformation types across all layers, ignoring the heterogeneous distribution characteristics within LLMs. In this paper, we propose an adaptive transformation selection framework that systematically determines optimal transformations on a per-layer basis. To this end, we first formulate transformation selection as a differentiable optimization problem to achieve the accurate transformation type for each layer. However, searching for optimal layer-wise transformations for every model is computationally expensive. To this end, we establish the connection between weight distribution kurtosis and accurate transformation type. Specifically, we propose an outlier-guided layer selection method using robust $z$-score normalization that achieves comparable performance to differentiable search with significantly reduced overhead. Comprehensive experiments on LLaMA family models demonstrate that our adaptive approach consistently outperforms the widely-used fixed transformation settings. For example, our method achieves an improvement of up to 4.58 perplexity points and a 2.11% gain in average six-task zero-shot accuracy under aggressive W3A3K2V2 quantization settings for the LLaMA-3-8B model compared to the current best existing method, FlatQuant, demonstrating the necessity of heterogeneous transformation selection for optimal LLM quantization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 46,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17822",
        "abs_url": "https://arxiv.org/abs/2511.17822",
        "pdf_url": "https://arxiv.org/pdf/2511.17822",
        "title": "High-Accuracy List-Decodable Mean Estimation",
        "authors": [
            "Ziyun Chen",
            "Spencer Compton",
            "Daniel Kane",
            "Jerry Li"
        ],
        "comments": "Abstract shortened to meet arXiv requirement",
        "subjects": "Machine Learning (cs.LG); Data Structures and Algorithms (cs.DS); Machine Learning (stat.ML)",
        "abstract": "In list-decodable learning, we are given a set of data points such that an $\\alpha$-fraction of these points come from a nice distribution $D$, for some small $\\alpha \\ll 1$, and the goal is to output a short list of candidate solutions, such that at least one element of this list recovers some non-trivial information about $D$. By now, there is a large body of work on this topic; however, while many algorithms can achieve optimal list size in terms of $\\alpha$, all known algorithms must incur error which decays, in some cases quite poorly, with $1 / \\alpha$. In this paper, we ask if this is inherent: is it possible to trade off list size with accuracy in list-decodable learning? More formally, given $\\epsilon > 0$, can we can output a slightly larger list in terms of $\\alpha$ and $\\epsilon$, but so that one element of this list has error at most $\\epsilon$ with the ground truth? We call this problem high-accuracy list-decodable learning. Our main result is that non-trivial high-accuracy guarantees, both information-theoretically and algorithmically, are possible for the canonical setting of list-decodable mean estimation of identity-covariance Gaussians. Specifically, we demonstrate that there exists a list of candidate means of size at most $L = \\exp \\left( O\\left( \\tfrac{\\log^2 1 / \\alpha}{\\epsilon^2} \\right)\\right)$ so that one of the elements of this list has $\\ell_2$ distance at most $\\epsilon$ to the true mean. We also design an algorithm that outputs such a list with runtime and sample complexity $n = d^{O(\\log L)} + \\exp \\exp (\\widetilde{O}(\\log L))$. We do so by demonstrating a completely novel proof of identifiability, as well as a new algorithmic way of leveraging this proof without the sum-of-squares hierarchy, which may be of independent technical interest.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 47,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17823",
        "abs_url": "https://arxiv.org/abs/2511.17823",
        "pdf_url": "https://arxiv.org/pdf/2511.17823",
        "title": "A novel k-means clustering approach using two distance measures for Gaussian data",
        "authors": [
            "Naitik Gada"
        ],
        "comments": "Keywords: machine learning, clustering algorithms, k-means",
        "subjects": "Machine Learning (cs.LG); Computational Engineering, Finance, and Science (cs.CE); Methodology (stat.ME); Machine Learning (stat.ML)",
        "abstract": "Clustering algorithms have long been the topic of research, representing the more popular side of unsupervised learning. Since clustering analysis is one of the best ways to find some clarity and structure within raw data, this paper explores a novel approach to \\textit{k}-means clustering. Here we present a \\textit{k}-means clustering algorithm that takes both the within cluster distance (WCD) and the inter cluster distance (ICD) as the distance metric to cluster the data into \\emph{k} clusters pre-determined by the Calinski-Harabasz criterion in order to provide a more robust output for the clustering analysis. The idea with this approach is that by including both the measurement metrics, the convergence of the data into their clusters becomes solidified and more robust. We run the algorithm with some synthetically produced data and also some benchmark data sets obtained from the UCI repository. The results show that the convergence of the data into their respective clusters is more accurate by using both WCD and ICD measurement metrics. The algorithm is also better at clustering the outliers into their true clusters as opposed to the traditional \\textit{k} means method. We also address some interesting possible research topics that reveal themselves as we answer the questions we initially set out to address.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 48,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17826",
        "abs_url": "https://arxiv.org/abs/2511.17826",
        "pdf_url": "https://arxiv.org/pdf/2511.17826",
        "title": "Deterministic Inference across Tensor Parallel Sizes That Eliminates Training-Inference Mismatch",
        "authors": [
            "Ziyang Zhang",
            "Xinheng Ding",
            "Jiayi Yuan",
            "Rixin Liu",
            "Huizi Mao",
            "Jiarong Xing",
            "Zirui Liu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL); Machine Learning (stat.ML)",
        "abstract": "Deterministic inference is increasingly critical for large language model (LLM) applications such as LLM-as-a-judge evaluation, multi-agent systems, and Reinforcement Learning (RL). However, existing LLM serving frameworks exhibit non-deterministic behavior: identical inputs can yield different outputs when system configurations (e.g., tensor parallel (TP) size, batch size) vary, even under greedy decoding. This arises from the non-associativity of floating-point arithmetic and inconsistent reduction orders across GPUs. While prior work has addressed batch-size-related nondeterminism through batch-invariant kernels, determinism across different TP sizes remains an open problem, particularly in RL settings, where the training engine typically uses Fully Sharded Data Parallel (i.e., TP = 1) while the rollout engine relies on multi-GPU TP to maximize the inference throughput, creating a natural mismatch between the two. This precision mismatch problem may lead to suboptimal performance or even collapse for RL training. We identify and analyze the root causes of TP-induced inconsistency and propose Tree-Based Invariant Kernels (TBIK), a set of TP-invariant matrix multiplication and reduction primitives that guarantee bit-wise identical results regardless of TP size. Our key insight is to align intra- and inter-GPU reduction orders through a unified hierarchical binary tree structure. We implement these kernels in Triton and integrate them into vLLM and FSDP. Experiments confirm zero probability divergence and bit-wise reproducibility for deterministic inference across different TP sizes. Also, we achieve bit-wise identical results between vLLM and FSDP in RL training pipelines with different parallel strategy. Code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 49,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17829",
        "abs_url": "https://arxiv.org/abs/2511.17829",
        "pdf_url": "https://arxiv.org/pdf/2511.17829",
        "title": "Unified Class and Domain Incremental Learning with Mixture of Experts for Indoor Localization",
        "authors": [
            "Akhil Singampalli",
            "Sudeep Pasricha"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Indoor localization using machine learning has gained traction due to the growing demand for location-based services. However, its long-term reliability is hindered by hardware/software variations across mobile devices, which shift the model's input distribution to create domain shifts. Further, evolving indoor environments can introduce new locations over time, expanding the output space to create class shifts, making static machine learning models ineffective over time. To address these challenges, we propose a novel unified continual learning framework for indoor localization called MOELO that, for the first time, jointly addresses domain-incremental and class-incremental learning scenarios. MOELO enables a lightweight, robust, and adaptive localization solution that can be deployed on resource-limited mobile devices and is capable of continual learning in dynamic, heterogeneous real-world settings. This is made possible by a mixture-of-experts architecture, where experts are incrementally trained per region and selected through an equiangular tight frame based gating mechanism ensuring efficient routing, and low-latency inference, all within a compact model footprint. Experimental evaluations show that MOELO achieves improvements of up to 25.6x in mean localization error, 44.5x in worst-case localization error, and 21.5x lesser forgetting compared to state-of-the-art frameworks across diverse buildings, mobile devices, and learning scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 50,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17840",
        "abs_url": "https://arxiv.org/abs/2511.17840",
        "pdf_url": "https://arxiv.org/pdf/2511.17840",
        "title": "Internalizing Tools as Morphisms in Graded Transformers",
        "authors": [
            "Tony Shaska"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Category Theory (math.CT)",
        "abstract": "We introduce a graded formulation of internal symbolic computation for transformers. The hidden space is endowed with a grading $V=\\bigoplus_{g\\in G}V_g$, and symbolic operations are realized as typed block maps (morphisms) $\\phi_{h\\leftarrow g}:V_g\\to V_h$ that are activated selectively by a differentiable routing policy. A self-supervised \\emph{graded utility functional}, defined as the loss reduction induced by a candidate morphism, governs activation and yields sparse, interpretable behavior. We develop the algebraic and geometric foundations: an internal model category whose objects are homogeneous components and whose morphisms are admissible grade transitions; adjoint pairs encoding typed round trips; and information-geometric interpretations in terms of KL gain, mirror descent with Bregman divergences, and Fisher natural gradients. Methodologically, we specify a utility--aware routing mechanism and objective that remain fully end-to-end differentiable. Analytic case studies and lightweight sanity checks illustrate selective morphic activation on hybrid symbolic-linguistic tasks. The framework unifies symbolic computation, geometry, and self--supervised learning within the \\emph{graded transformer} formalism \\cite{sh-89,sh-95}, while subsuming prior external-tool paradigms (e.g., Toolformer \\cite{toolformer2023}) as a special case via functorial internalization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 51,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17848",
        "abs_url": "https://arxiv.org/abs/2511.17848",
        "pdf_url": "https://arxiv.org/pdf/2511.17848",
        "title": "Scaling Kinetic Monte-Carlo Simulations of Grain Growth with Combined Convolutional and Graph Neural Networks",
        "authors": [
            "Zhihui Tian",
            "Ethan Suwandi",
            "Tomas Oppelstrup",
            "Vasily V. Bulatov",
            "Joel B. Harley",
            "Fei Zhou"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Materials Science (cond-mat.mtrl-sci)",
        "abstract": "Graph neural networks (GNN) have emerged as a promising machine learning method for microstructure simulations such as grain growth. However, accurate modeling of realistic grain boundary networks requires large simulation cells, which GNN has difficulty scaling up to. To alleviate the computational costs and memory footprint of GNN, we propose a hybrid architecture combining a convolutional neural network (CNN) based bijective autoencoder to compress the spatial dimensions, and a GNN that evolves the microstructure in the latent space of reduced spatial sizes. Our results demonstrate that the new design significantly reduces computational costs with using fewer message passing layer (from 12 down to 3) compared with GNN alone. The reduction in computational cost becomes more pronounced as the spatial size increases, indicating strong computational scalability. For the largest mesh evaluated (160^3), our method reduces memory usage and runtime in inference by 117x and 115x, respectively, compared with GNN-only baseline. More importantly, it shows higher accuracy and stronger spatiotemporal capability than the GNN-only baseline, especially in long-term testing. Such combination of scalability and accuracy is essential for simulating realistic material microstructures over extended time scales. The improvements can be attributed to the bijective autoencoder's ability to compress information losslessly from spatial domain into a high dimensional feature space, thereby producing more expressive latent features for the GNN to learn from, while also contributing its own spatiotemporal modeling capability. The training was optimized to learn from the stochastic Potts Monte Carlo method. Our findings provide a highly scalable approach for simulating grain growth.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 52,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17852",
        "abs_url": "https://arxiv.org/abs/2511.17852",
        "pdf_url": "https://arxiv.org/pdf/2511.17852",
        "title": "Transformers with RL or SFT Provably Learn Sparse Boolean Functions, But Differently",
        "authors": [
            "Bochen Lyu",
            "Yiyang Jia",
            "Xiaohao Cai",
            "Zhanxing Zhu"
        ],
        "comments": "43 pages, 5 figures",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Transformers can acquire Chain-of-Thought (CoT) capabilities to solve complex reasoning tasks through fine-tuning. Reinforcement learning (RL) and supervised fine-tuning (SFT) are two primary approaches to this end, yet their underlying mechanisms and differences remain theoretically unclear. In this work, we examine these aspects specifically for learning $k$-sparse Boolean functions with a one-layer transformer and intermediate supervision that is akin to CoT. In particular, we consider $k$-sparse Boolean functions that can be recursively decomposed into fixed 2-sparse Boolean functions. We analyze the learning dynamics of fine-tuning the transformer via either RL or SFT with CoT to identify sufficient conditions for it to provably learn these functions. We verify that these conditions hold for three basic examples, including $k$-PARITY, $k$-AND, and $k$-OR, thus demonstrating the learnability of both approaches. Notably, we reveal that RL and SFT exhibit distinct learning behaviors: RL learns the whole CoT chain simultaneously, whereas SFT learns the CoT chain step-by-step. Overall, our findings provide theoretical insights into the underlying mechanisms of RL and SFT as well as how they differ in triggering the CoT capabilities of transformers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 53,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17861",
        "abs_url": "https://arxiv.org/abs/2511.17861",
        "pdf_url": "https://arxiv.org/pdf/2511.17861",
        "title": "Cost-Sensitive Conformal Training with Provably Controllable Learning Bounds",
        "authors": [
            "Xuesong Jia",
            "Yuanjie Shi",
            "Ziquan Liu",
            "Yi Xu",
            "Yan Yan"
        ],
        "comments": "Accepted for Publication at Association for the Advancement of Artificial Intelligence (AAAI), 2026",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Conformal prediction (CP) is a general framework to quantify the predictive uncertainty of machine learning models that uses a set prediction to include the true label with a valid probability. To align the uncertainty measured by CP, conformal training methods minimize the size of the prediction sets. A typical way is to use a surrogate indicator function, usually Sigmoid or Gaussian error function. However, these surrogate functions do not have a uniform error bound to the indicator function, leading to uncontrollable learning bounds. In this paper, we propose a simple cost-sensitive conformal training algorithm that does not rely on the indicator approximation mechanism. Specifically, we theoretically show that minimizing the expected size of prediction sets is upper bounded by the expected rank of true labels. To this end, we develop a rank weighting strategy that assigns the weight using the rank of true label on each data sample. Our analysis provably demonstrates the tightness between the proposed weighted objective and the expected size of conformal prediction sets. Extensive experiments verify the validity of our theoretical insights, and superior empirical performance over other conformal training in terms of predictive efficiency with 21.38% reduction for average prediction set size.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 54,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17864",
        "abs_url": "https://arxiv.org/abs/2511.17864",
        "pdf_url": "https://arxiv.org/pdf/2511.17864",
        "title": "Equivalence of Context and Parameter Updates in Modern Transformer Blocks",
        "authors": [
            "Adrian Goldwaser",
            "Michael Munn",
            "Javier Gonzalvo",
            "Benoit Dherin"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Recent research has established that the impact of context in a vanilla transformer can be represented implicitly by forming a token-dependent, rank-1 patch to its MLP weights. This work extends that foundational theory to the diverse architectures of modern Large Language Models. We first demonstrate a precise, analytical solution for a Gemma-style transformer block, proving that the entire effect of a context can be perfectly mapped to rank-1 patches on its MLP weight matrices and a patch to the RMSNorm scale. We then generalize this result, providing a constructive proof and algorithm for multi-layer models. To unify these findings, we introduce a general framework centered on two core properties: input controllability and output controllability. We prove that a perfect implicit weight patch is possible for any MLP block where the inner function is input-controllable and the outer function is output-controllable. This provides a simpler and more powerful lens for understanding how transformer models transmute prompts into effective weights. This setup generalizes to a wide range of modern LLM architectures including gating, pre-/post-norm, mixture of experts and sequential/parallel transformer blocks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 55,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17869",
        "abs_url": "https://arxiv.org/abs/2511.17869",
        "pdf_url": "https://arxiv.org/pdf/2511.17869",
        "title": "The Horcrux: Mechanistically Interpretable Task Decomposition for Detecting and Mitigating Reward Hacking in Embodied AI Systems",
        "authors": [
            "Subramanyam Sahoo",
            "Jared Junkin"
        ],
        "comments": "Accepted to the NeurIPS (Mexico City) 2025 Workshop on Embodied and Safe-Assured Robotic Systems (E-SARS). Thanks to Aman Chadha",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Embodied AI agents exploit reward signal flaws through reward hacking, achieving high proxy scores while failing true objectives. We introduce Mechanistically Interpretable Task Decomposition (MITD), a hierarchical transformer architecture with Planner, Coordinator, and Executor modules that detects and mitigates reward hacking. MITD decomposes tasks into interpretable subtasks while generating diagnostic visualizations including Attention Waterfall Diagrams and Neural Pathway Flow Charts. Experiments on 1,000 HH-RLHF samples reveal that decomposition depths of 12 to 25 steps reduce reward hacking frequency by 34 percent across four failure modes. We present new paradigms showing that mechanistically grounded decomposition offers a more effective way to detect reward hacking than post-hoc behavioral monitoring.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 56,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17879",
        "abs_url": "https://arxiv.org/abs/2511.17879",
        "pdf_url": "https://arxiv.org/pdf/2511.17879",
        "title": "Generative Adversarial Post-Training Mitigates Reward Hacking in Live Human-AI Music Interaction",
        "authors": [
            "Yusong Wu",
            "Stephen Brade",
            "Teng Ma",
            "Tia-Jane Fowler",
            "Enning Yang",
            "Berker Banar",
            "Aaron Courville",
            "Natasha Jaques",
            "Cheng-Zhi Anna Huang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Sound (cs.SD)",
        "abstract": "Most applications of generative AI involve a sequential interaction in which a person inputs a prompt and waits for a response, and where reaction time and adaptivity are not important factors. In contrast, live jamming is a collaborative interaction that requires real-time coordination and adaptation without access to the other player's future moves, while preserving diversity to sustain a creative flow. Reinforcement learning post-training enables effective adaptation through on-policy interaction, yet it often reduces output diversity by exploiting coherence-based rewards. This collapse, known as ``reward hacking'', affects many RL post-training pipelines, but is especially harmful in live jamming, where musical creativity relies on dynamic variation and mutual responsiveness. In this paper, we propose a novel adversarial training method on policy-generated trajectories to mitigate reward hacking in RL post-training for melody-to-chord accompaniment. A co-evolving discriminator separates policy trajectories from the data distribution, while the policy maximizes the discriminator output in addition to coherence rewards to prevent collapse to trivial outputs. We evaluate accompaniment quality and output diversity in simulation with both fixed test melodies and learned melody agents, and we conduct a user study with the model deployed in a real-time interactive system with expert musicians. Quantitative evaluation and user feedback demonstrate improved output diversity, harmonic coherence, adaptation speed and user agency. Our results demonstrate a simple yet effective method to mitigate reward hacking in RL post-training of generative sequence models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 57,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17936",
        "abs_url": "https://arxiv.org/abs/2511.17936",
        "pdf_url": "https://arxiv.org/pdf/2511.17936",
        "title": "Mitigating Catastrophic Forgetting in Streaming Generative and Predictive Learning via Stateful Replay",
        "authors": [
            "Wenzhang Du"
        ],
        "comments": "11 pages, 4 figures",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Many deployed learning systems must update models on streaming data under memory constraints. The default strategy, sequential fine-tuning on each new phase, is architecture-agnostic but often suffers catastrophic forgetting when later phases correspond to different sub-populations or tasks. Replay with a finite buffer is a simple alternative, yet its behaviour across generative and predictive objectives is not well understood. We present a unified study of stateful replay for streaming autoencoding, time series forecasting, and classification. We view both sequential fine-tuning and replay as stochastic gradient methods for an ideal joint objective, and use a gradient alignment analysis to show when mixing current and historical samples should reduce forgetting. We then evaluate a single replay mechanism on six streaming scenarios built from Rotated MNIST, ElectricityLoadDiagrams 2011-2014, and Airlines delay data, using matched training budgets and three seeds. On heterogeneous multi task streams, replay reduces average forgetting by a factor of two to three, while on benign time based streams both methods perform similarly. These results position stateful replay as a strong and simple baseline for continual learning in streaming environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 58,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17953",
        "abs_url": "https://arxiv.org/abs/2511.17953",
        "pdf_url": "https://arxiv.org/pdf/2511.17953",
        "title": "On Transportability for Structural Causal Bandits",
        "authors": [
            "Min Woo Park",
            "Sanghack Lee"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Intelligent agents equipped with causal knowledge can optimize their action spaces to avoid unnecessary exploration. The structural causal bandit framework provides a graphical characterization for identifying actions that are unable to maximize rewards by leveraging prior knowledge of the underlying causal structure. While such knowledge enables an agent to estimate the expected rewards of certain actions based on others in online interactions, there has been little guidance on how to transfer information inferred from arbitrary combinations of datasets collected under different conditions -- observational or experimental -- and from heterogeneous environments. In this paper, we investigate the structural causal bandit with transportability, where priors from the source environments are fused to enhance learning in the deployment setting. We demonstrate that it is possible to exploit invariances across environments to consistently improve learning. The resulting bandit algorithm achieves a sub-linear regret bound with an explicit dependence on informativeness of prior data, and it may outperform standard bandit approaches that rely solely on online learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 59,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17963",
        "abs_url": "https://arxiv.org/abs/2511.17963",
        "pdf_url": "https://arxiv.org/pdf/2511.17963",
        "title": "Hybrid LSTM and PPO Networks for Dynamic Portfolio Optimization",
        "authors": [
            "Jun Kevin",
            "Pujianto Yugopuspito"
        ],
        "comments": "12 pages, 8 figures, 2 tables, accepted at 2025 8th Artificial Intelligence and Cloud Computing Conference",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Portfolio Management (q-fin.PM)",
        "abstract": "This paper introduces a hybrid framework for portfolio optimization that fuses Long Short-Term Memory (LSTM) forecasting with a Proximal Policy Optimization (PPO) reinforcement learning strategy. The proposed system leverages the predictive power of deep recurrent networks to capture temporal dependencies, while the PPO agent adaptively refines portfolio allocations in continuous action spaces, allowing the system to anticipate trends while adjusting dynamically to market shifts. Using multi-asset datasets covering U.S. and Indonesian equities, U.S. Treasuries, and major cryptocurrencies from January 2018 to December 2024, the model is evaluated against several baselines, including equal-weight, index-style, and single-model variants (LSTM-only and PPO-only). The framework's performance is benchmarked against equal-weighted, index-based, and single-model approaches (LSTM-only and PPO-only) using annualized return, volatility, Sharpe ratio, and maximum drawdown metrics, each adjusted for transaction costs. The results indicate that the hybrid architecture delivers higher returns and stronger resilience under non-stationary market regimes, suggesting its promise as a robust, AI-driven framework for dynamic portfolio optimization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 60,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17968",
        "abs_url": "https://arxiv.org/abs/2511.17968",
        "pdf_url": "https://arxiv.org/pdf/2511.17968",
        "title": "Uncertainty-Aware Federated Learning for Cyber-Resilient Microgrid Energy Management",
        "authors": [
            "Oluleke Babayomi",
            "Dong-Seong Kim"
        ],
        "comments": "6 pages",
        "subjects": "Machine Learning (cs.LG); Cryptography and Security (cs.CR)",
        "abstract": "Maintaining economic efficiency and operational reliability in microgrid energy management systems under cyberattack conditions remains challenging. Most approaches assume non-anomalous measurements, make predictions with unquantified uncertainties, and do not mitigate malicious attacks on renewable forecasts for energy management optimization. This paper presents a comprehensive cyber-resilient framework integrating federated Long Short-Term Memory-based photovoltaic forecasting with a novel two-stage cascade false data injection attack detection and energy management system optimization. The approach combines autoencoder reconstruction error with prediction uncertainty quantification to enable attack-resilient energy storage scheduling while preserving data privacy. Extreme false data attack conditions were studied that caused 58% forecast degradation and 16.9\\% operational cost increases. The proposed integrated framework reduced false positive detections by 70%, recovered 93.7% of forecasting performance losses, and achieved 5\\% operational cost savings, mitigating 34.7% of attack-induced economic losses. Results demonstrate that precision-focused cascade detection with multi-signal fusion outperforms single-signal approaches, validating security-performance synergy for decentralized microgrids.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 61,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17970",
        "abs_url": "https://arxiv.org/abs/2511.17970",
        "pdf_url": "https://arxiv.org/pdf/2511.17970",
        "title": "Controllability Analysis of State Space-based Language Model",
        "authors": [
            "Mohamed Mabrok",
            "Yalda Zafari"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "State-space models (SSMs), particularly Mamba, have become powerful architectures for sequence modeling, yet their internal dynamics remain poorly understood compared to attention-based models. We introduce and validate the Influence Score, a controllability-based metric derived from the discretized state-space parameters of Mamba and computed through a backward recurrence analogous to system observability. The score quantifies how strongly a token at position k affects all later states and outputs. We evaluate this measure across three Mamba variants: mamba-130m, mamba-2.8b, and mamba-2.8b-slimpj, using six experiments that test its sensitivity to temperature, prompt complexity, token type, layer depth, token position, and input perturbations. The results show three main insights: (1) the Influence Score increases with model size and training data, reflecting model capacity; (2) Mamba exhibits consistent architectural patterns, including recency bias and concentrated influence in mid-to-late layers; and (3) emergent behaviors appear only at scale, with mamba-2.8b-slimpj uniquely prioritizing content words and reducing internal influence in the presence of noise. These findings establish the Influence Score as a practical diagnostic tool for interpreting and comparing SSM-based language models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 62,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17978",
        "abs_url": "https://arxiv.org/abs/2511.17978",
        "pdf_url": "https://arxiv.org/pdf/2511.17978",
        "title": "Federated Anomaly Detection and Mitigation for EV Charging Forecasting Under Cyberattacks",
        "authors": [
            "Oluleke Babayomi",
            "Dong-Seong Kim"
        ],
        "comments": "6 pages",
        "subjects": "Machine Learning (cs.LG); Cryptography and Security (cs.CR)",
        "abstract": "Electric Vehicle (EV) charging infrastructure faces escalating cybersecurity threats that can severely compromise operational efficiency and grid stability. Existing forecasting techniques are limited by the lack of combined robust anomaly mitigation solutions and data privacy preservation. Therefore, this paper addresses these challenges by proposing a novel anomaly-resilient federated learning framework that simultaneously preserves data privacy, detects cyber-attacks, and maintains trustworthy demand prediction accuracy under adversarial conditions. The proposed framework integrates three key innovations: LSTM autoencoder-based distributed anomaly detection deployed at each federated client, interpolation-based anomalous data mitigation to preserve temporal continuity, and federated Long Short-Term Memory (LSTM) networks that enable collaborative learning without centralized data aggregation. The framework is validated on real-world EV charging infrastructure datasets combined with real-world DDoS attack datasets, providing robust validation of the proposed approach under realistic threat scenarios. Experimental results demonstrate that the federated approach achieves superior performance compared to centralized models, with 15.2% improvement in R2 accuracy while maintaining data locality. The integrated cyber-attack detection and mitigation system produces trustworthy datasets that enhance prediction reliability, recovering 47.9% of attack-induced performance degradation while maintaining exceptional precision (91.3%) and minimal false positive rates (1.21%). The proposed architecture enables enhanced EV infrastructure planning, privacy-preserving collaborative forecasting, cybersecurity resilience, and rapid recovery from malicious threats across distributed charging networks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 63,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17983",
        "abs_url": "https://arxiv.org/abs/2511.17983",
        "pdf_url": "https://arxiv.org/pdf/2511.17983",
        "title": "An Adaptive Resonance Theory-based Topological Clustering Algorithm with a Self-Adjusting Vigilance Parameter",
        "authors": [
            "Naoki Masuyama",
            "Yuichiro Toda",
            "Yusuke Nojima",
            "Hisao Ishibuchi"
        ],
        "comments": "This manuscript is currently under review",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Clustering in stationary and nonstationary settings, where data distributions remain static or evolve over time, requires models that can adapt to distributional shifts while preserving previously learned cluster structures. This paper proposes an Adaptive Resonance Theory (ART)-based topological clustering algorithm that autonomously adjusts its recalculation interval and vigilance threshold through a diversity-driven adaptation mechanism. This mechanism enables hyperparameter-free learning that maintains cluster stability and continuity in dynamic environments. Experiments on 24 real-world datasets demonstrate that the proposed algorithm outperforms state-of-the-art methods in both clustering performance and continual learning capability. These results highlight the effectiveness of the proposed parameter adaptation in mitigating catastrophic forgetting and maintaining consistent clustering in evolving data streams. Source code is available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 64,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17987",
        "abs_url": "https://arxiv.org/abs/2511.17987",
        "pdf_url": "https://arxiv.org/pdf/2511.17987",
        "title": "Escaping Optimization Stagnation: Taking Steps Beyond Task Arithmetic via Difference Vectors",
        "authors": [
            "Jinping Wang",
            "Zhiqiang Gao",
            "Dinggen Zhang",
            "Zhiwu Xie"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Current methods for editing pre-trained models face significant challenges, primarily high computational costs and limited scalability. Task arithmetic has recently emerged as a promising solution, using simple arithmetic operations-addition and negation-based on task vectors which are the differences between fine-tuned and pre-trained model weights, to efficiently modify model behavior. However, the full potential of task arithmetic remains underexplored, primarily due to limited mechanisms for overcoming optimization stagnation. To address this challenge, we introduce the notion of difference vector, a generalized form of task vectors derived from the historical movements during optimization. Using difference vectors as directed perturbations, we propose the Difference Vector-based Anisotropic Scaling Iterative algorithm (DV-BASI) to enable a continuous optimization process for task arithmetic methods without relying on any additional modules or components. Notably, by leveraging escapability and directional advantages of difference vectors, the average performance on different tasks of the multi-task model merged by DV-BASI may even outperform models individually fine-tuned. Based on this observation, we extend the application of difference vectors to a feasible fine-tuning method for single-task models. On the practical side, DV-BASI allows expressive searching directions with few learnable parameters and forms a scalable framework. We also integrate DV-BASI with task arithmetic methods and advanced optimization techniques to achieve state-of-the-art performance on both supervised and unsupervised evaluation protocols.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 65,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17994",
        "abs_url": "https://arxiv.org/abs/2511.17994",
        "pdf_url": "https://arxiv.org/pdf/2511.17994",
        "title": "Learning Rate Scheduling with Matrix Factorization for Private Training",
        "authors": [
            "Nikita P. Kalinin",
            "Joel Daniel Andersson"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "We study differentially private model training with stochastic gradient descent under learning rate scheduling and correlated noise. Although correlated noise, in particular via matrix factorizations, has been shown to improve accuracy, prior theoretical work focused primarily on the prefix-sum workload. That workload assumes a constant learning rate, whereas in practice learning rate schedules are widely used to accelerate training and improve convergence. We close this gap by deriving general upper and lower bounds for a broad class of learning rate schedules in both single- and multi-epoch settings. Building on these results, we propose a learning-rate-aware factorization that achieves improvements over prefix-sum factorizations under both MaxSE and MeanSE error metrics. Our theoretical analysis yields memory-efficient constructions suitable for practical deployment, and experiments on CIFAR-10 and IMDB datasets confirm that schedule-aware factorizations improve accuracy in private training.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 66,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18006",
        "abs_url": "https://arxiv.org/abs/2511.18006",
        "pdf_url": "https://arxiv.org/pdf/2511.18006",
        "title": "Understanding Private Learning From Feature Perspective",
        "authors": [
            "Meng Ding",
            "Mingxi Lei",
            "Shaopeng Fu",
            "Shaowei Wang",
            "Di Wang",
            "Jinhui Xu"
        ],
        "comments": "39pages",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Differentially private Stochastic Gradient Descent (DP-SGD) has become integral to privacy-preserving machine learning, ensuring robust privacy guarantees in sensitive domains. Despite notable empirical advances leveraging features from non-private, pre-trained models to enhance DP-SGD training, a theoretical understanding of feature dynamics in private learning remains underexplored. This paper presents the first theoretical framework to analyze private training through a feature learning perspective. Building on the multi-patch data structure from prior work, our analysis distinguishes between label-dependent feature signals and label-independent noise, a critical aspect overlooked by existing analyses in the DP community. Employing a two-layer CNN with polynomial ReLU activation, we theoretically characterize both feature signal learning and data noise memorization in private training via noisy gradient descent. Our findings reveal that (1) Effective private signal learning requires a higher signal-to-noise ratio (SNR) compared to non-private training, and (2) When data noise memorization occurs in non-private learning, it will also occur in private learning, leading to poor generalization despite small training loss. Our findings highlight the challenges of private learning and prove the benefit of feature enhancement to improve SNR. Experiments on synthetic and real-world datasets also validate our theoretical findings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 67,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18039",
        "abs_url": "https://arxiv.org/abs/2511.18039",
        "pdf_url": "https://arxiv.org/pdf/2511.18039",
        "title": "Curvature-Aware Safety Restoration In LLMs Fine-Tuning",
        "authors": [
            "Thong Bach",
            "Thanh Nguyen-Tang",
            "Dung Nguyen",
            "Thao Minh Le",
            "Truyen Tran"
        ],
        "comments": "19 pages, 10 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Fine-tuning Large Language Models (LLMs) for downstream tasks often compromises safety alignment, even when using parameter-efficient methods like LoRA. In this work, we uncover a notable property: fine-tuned models preserve the geometric structure of their loss landscapes concerning harmful content, regardless of the fine-tuning method employed. This suggests that safety behaviors are not erased but shifted to less influential regions of the parameter space. Building on this insight, we propose a curvature-aware alignment restoration method that leverages influence functions and second-order optimization to selectively increase loss on harmful inputs while preserving task performance. By navigating the shared geometry between base and fine-tuned models, our method discourages unsafe outputs while preserving task-relevant performance, avoiding full reversion and enabling precise, low-impact updates. Extensive evaluations across multiple model families and adversarial settings show that our approach efficiently reduces harmful responses while maintaining or even improving utility and few-shot learning performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 68,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18056",
        "abs_url": "https://arxiv.org/abs/2511.18056",
        "pdf_url": "https://arxiv.org/pdf/2511.18056",
        "title": "Hierarchical Linkage Clustering Beyond Binary Trees and Ultrametrics",
        "authors": [
            "Maximilien Dreveton",
            "Matthias Grossglauser",
            "Daichi Kuroda",
            "Patrick Thiran"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Methodology (stat.ME); Machine Learning (stat.ML)",
        "abstract": "Hierarchical clustering seeks to uncover nested structures in data by constructing a tree of clusters, where deeper levels reveal finer-grained relationships. Traditional methods, including linkage approaches, face three major limitations: (i) they always return a hierarchy, even if none exists, (ii) they are restricted to binary trees, even if the true hierarchy is non-binary, and (iii) they are highly sensitive to the choice of linkage function. In this paper, we address these issues by introducing the notion of a valid hierarchy and defining a partial order over the set of valid hierarchies. We prove the existence of a finest valid hierarchy, that is, the hierarchy that encodes the maximum information consistent with the similarity structure of the data set. In particular, the finest valid hierarchy is not constrained to binary structures and, when no hierarchical relationships exist, collapses to a star tree. We propose a simple two-step algorithm that first constructs a binary tree via a linkage method and then prunes it to enforce validity. We establish necessary and sufficient conditions on the linkage function under which this procedure exactly recovers the finest valid hierarchy, and we show that all linkage functions satisfying these conditions yield the same hierarchy after pruning. Notably, classical linkage rules such as single, complete, and average satisfy these conditions, whereas Ward's linkage fails to do so.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 69,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18084",
        "abs_url": "https://arxiv.org/abs/2511.18084",
        "pdf_url": "https://arxiv.org/pdf/2511.18084",
        "title": "The Alignment Paradox of Medical Large Language Models in Infertility Care: Decoupling Algorithmic Improvement from Clinical Decision-making Quality",
        "authors": [
            "Dou Liu",
            "Ying Long",
            "Sophia Zuoqiu",
            "Kaipeng Xie",
            "Runze Yang",
            "Di Liu",
            "Kang Li",
            "Yiting Lin",
            "Hanyi Liu",
            "Rong Yin",
            "Tian Tang"
        ],
        "comments": "22 pages 5 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Large language models (LLMs) are increasingly adopted in clinical decision support, yet aligning them with the multifaceted reasoning pathways of real-world medicine remains a major challenge. Using more than 8,000 infertility treatment records, we systematically evaluate four alignment strategies: Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), Group Relative Policy Optimization (GRPO), and In-Context Learning (ICL) through a dual-layer framework combining automatic benchmarks with blinded doctor-in-the-loop assessments. GRPO achieves the highest algorithmic accuracy across multiple decision layers, confirming the value of reinforcement-based optimization for structured prediction tasks. However, clinicians consistently prefer the SFT model, citing clearer reasoning processes (p = 0.035) and higher therapeutic feasibility (p = 0.019). In blinded pairwise comparisons, SFT attains the highest winning rate (51.2%), outperforming both GRPO (26.2%) and even physicians' original decisions (22.7%). These results reveal an alignment paradox: algorithmic improvements do not necessarily translate into higher clinical trust, and may diverge from human-centered preferences. Our findings highlight the need for alignment strategies that prioritize clinically interpretable and practically feasible reasoning, rather than solely optimizing decision-level accuracy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 70,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18107",
        "abs_url": "https://arxiv.org/abs/2511.18107",
        "pdf_url": "https://arxiv.org/pdf/2511.18107",
        "title": "Active Learning with Selective Time-Step Acquisition for PDEs",
        "authors": [
            "Yegon Kim",
            "Hyunsu Kim",
            "Gyeonghoon Ko",
            "Juho Lee"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Accurately solving partial differential equations (PDEs) is critical to understanding complex scientific and engineering phenomena, yet traditional numerical solvers are computationally expensive. Surrogate models offer a more efficient alternative, but their development is hindered by the cost of generating sufficient training data from numerical solvers. In this paper, we present a novel framework for active learning (AL) in PDE surrogate modeling that reduces this cost. Unlike the existing AL methods for PDEs that always acquire entire PDE trajectories, our approach strategically generates only the most important time steps with the numerical solver, while employing the surrogate model to approximate the remaining steps. This dramatically reduces the cost incurred by each trajectory and thus allows the active learning algorithm to try out a more diverse set of trajectories given the same budget. To accommodate this novel framework, we develop an acquisition function that estimates the utility of a set of time steps by approximating its resulting variance reduction. We demonstrate the effectiveness of our method on several benchmark PDEs, including the Burgers' equation, Korteweg-De Vries equation, Kuramoto-Sivashinsky equation, the incompressible Navier-Stokes equation, and the compressible Navier-Stokes equation. Experiments show that our approach improves performance by large margins over the best existing method. Our method not only reduces average error but also the 99\\%, 95\\%, and 50\\% quantiles of error, which is rare for an AL algorithm. All in all, our approach offers a data-efficient solution to surrogate modeling for PDEs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 71,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18138",
        "abs_url": "https://arxiv.org/abs/2511.18138",
        "pdf_url": "https://arxiv.org/pdf/2511.18138",
        "title": "Vulnerability-Aware Robust Multimodal Adversarial Training",
        "authors": [
            "Junrui Zhang",
            "Xinyu Zhao",
            "Jie Peng",
            "Chenjie Wang",
            "Jianmin Ji",
            "Tianlong Chen"
        ],
        "comments": "Accepted by AAAI26",
        "subjects": "Machine Learning (cs.LG); Cryptography and Security (cs.CR)",
        "abstract": "Multimodal learning has shown significant superiority on various tasks by integrating multiple modalities. However, the interdependencies among modalities increase the susceptibility of multimodal models to adversarial attacks. Existing methods mainly focus on attacks on specific modalities or indiscriminately attack all modalities. In this paper, we find that these approaches ignore the differences between modalities in their contribution to final robustness, resulting in suboptimal robustness performance. To bridge this gap, we introduce Vulnerability-Aware Robust Multimodal Adversarial Training (VARMAT), a probe-in-training adversarial training method that improves multimodal robustness by identifying the vulnerability of each modality. To be specific, VARMAT first explicitly quantifies the vulnerability of each modality, grounded in a first-order approximation of the attack objective (Probe). Then, we propose a targeted regularization term that penalizes modalities with high vulnerability, guiding robust learning while maintaining task accuracy (Training). We demonstrate the enhanced robustness of our method across multiple multimodal datasets involving diverse modalities. Finally, we achieve {12.73%, 22.21%, 11.19%} robustness improvement on three multimodal datasets, revealing a significant blind spot in multimodal adversarial training.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 72,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18150",
        "abs_url": "https://arxiv.org/abs/2511.18150",
        "pdf_url": "https://arxiv.org/pdf/2511.18150",
        "title": "Graph Neural Networks vs Convolutional Neural Networks for Graph Domination Number Prediction",
        "authors": [
            "Randy Davila",
            "Beyzanur Ispir"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Combinatorics (math.CO)",
        "abstract": "We investigate machine learning approaches to approximating the \\emph{domination number} of graphs, the minimum size of a dominating set. Exact computation of this parameter is NP-hard, restricting classical methods to small instances. We compare two neural paradigms: Convolutional Neural Networks (CNNs), which operate on adjacency matrix representations, and Graph Neural Networks (GNNs), which learn directly from graph structure through message passing. Across 2,000 random graphs with up to 64 vertices, GNNs achieve markedly higher accuracy ($R^2=0.987$, MAE $=0.372$) than CNNs ($R^2=0.955$, MAE $=0.500$). Both models offer substantial speedups over exact solvers, with GNNs delivering more than $200\\times$ acceleration while retaining near-perfect fidelity. Our results position GNNs as a practical surrogate for combinatorial graph invariants, with implications for scalable graph optimization and mathematical discovery.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 73,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18157",
        "abs_url": "https://arxiv.org/abs/2511.18157",
        "pdf_url": "https://arxiv.org/pdf/2511.18157",
        "title": "scipy.spatial.transform: Differentiable Framework-Agnostic 3D Transformations in Python",
        "authors": [
            "Martin Schuck",
            "Alexander von Rohr",
            "Angela P. Schoellig"
        ],
        "comments": "Accepted as oral at the 1st Workshop on Differentiable Systems and Scientific Machine Learning @ EurIPS 2025",
        "subjects": "Machine Learning (cs.LG); Robotics (cs.RO)",
        "abstract": "Three-dimensional rigid-body transforms, i.e. rotations and translations, are central to modern differentiable machine learning pipelines in robotics, vision, and simulation. However, numerically robust and mathematically correct implementations, particularly on SO(3), are error-prone due to issues such as axis conventions, normalizations, composition consistency and subtle errors that only appear in edge cases. SciPy's this http URL module is a rigorously tested Python implementation. However, it historically only supported NumPy, limiting adoption in GPU-accelerated and autodiff-based workflows. We present a complete overhaul of SciPy's this http URL functionality that makes it compatible with any array library implementing the Python array API, including JAX, PyTorch, and CuPy. The revised implementation preserves the established SciPy interface while enabling GPU/TPU execution, JIT compilation, vectorized batching, and differentiation via native autodiff of the chosen backend. We demonstrate how this foundation supports differentiable scientific computing through two case studies: (i) scalability of 3D transforms and rotations and (ii) a JAX drone simulation that leverages SciPy's Rotation for accurate integration of rotational dynamics. Our contributions have been merged into SciPy main and will ship in the next release, providing a framework-agnostic, production-grade basis for 3D spatial math in differentiable systems and ML.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 74,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18158",
        "abs_url": "https://arxiv.org/abs/2511.18158",
        "pdf_url": "https://arxiv.org/pdf/2511.18158",
        "title": "LocaGen: Low-Overhead Indoor Localization Through Spatial Augmentation",
        "authors": [
            "Abdelrahman Abdelmotlb",
            "Abdallah Taman",
            "Sherif Mostafa",
            "Moustafa Youssef"
        ],
        "comments": "Accepted at GeoIndustry @ ACM SIGSPATIAL 2025 (The 4th International Workshop on Spatial Big Data and AI for Industrial Applications)",
        "subjects": "Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)",
        "abstract": "Indoor localization systems commonly rely on fingerprinting, which requires extensive survey efforts to obtain location-tagged signal data, limiting their real-world deployability. Recent approaches that attempt to reduce this overhead either suffer from low representation ability, mode collapse issues, or require the effort of collecting data at all target locations. We present LocaGen, a novel spatial augmentation framework that significantly reduces fingerprinting overhead by generating high-quality synthetic data at completely unseen locations. LocaGen leverages a conditional diffusion model guided by a novel spatially aware optimization strategy to synthesize realistic fingerprints at unseen locations using only a subset of seen locations. To further improve our diffusion model performance, LocaGen augments seen location data based on domain-specific heuristics and strategically selects the seen and unseen locations using a novel density-based approach that ensures robust coverage. Our extensive evaluation on a real-world WiFi fingerprinting dataset shows that LocaGen maintains the same localization accuracy even with 30% of the locations unseen and achieves up to 28% improvement in accuracy over state-of-the-art augmentation methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 75,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18159",
        "abs_url": "https://arxiv.org/abs/2511.18159",
        "pdf_url": "https://arxiv.org/pdf/2511.18159",
        "title": "Bringing Stability to Diffusion: Decomposing and Reducing Variance of Training Masked Diffusion Models",
        "authors": [
            "Mengni Jia",
            "Mengyu Zhou",
            "Yihao Liu",
            "Xiaoxi Jiang",
            "Guanjun Jiang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Masked diffusion models (MDMs) are a promising alternative to autoregressive models (ARMs), but they suffer from inherently much higher training variance. High variance leads to noisier gradient estimates and unstable optimization, so even equally strong pretrained MDMs and ARMs that are competitive at initialization often diverge after task-specific training, with MDMs falling far behind. There has been no theoretical explanation or systematic solution. We derive the first decomposition of MDM training variance into three sources: (A) masking pattern noise, (B) masking rate noise, and (C) data noise, while ARMs are only affected by (C). This explains the fundamental training gap. Building on this foundation, we design six variance-reduction methods, including two core methods: (1) P-POTS, a Pareto-optimal t sampler that minimizes training variance by sampling harder t values more often with appropriately smaller update steps, and (2) MIRROR, which uses negatively correlated samples to reduce (A). Experiments show that compared to standard MDM training, our methods improve accuracy by 7-8% on complex reasoning tasks, while simultaneously reducing run-to-run variability to near ARM levels, substantially narrowing the gap with strong ARM baselines; in most settings, even the best baseline runs remain below the worst run of our method.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 76,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18178",
        "abs_url": "https://arxiv.org/abs/2511.18178",
        "pdf_url": "https://arxiv.org/pdf/2511.18178",
        "title": "Bayesian Calibration of Engine-out NOx Models for Engine-to-Engine Transferability",
        "authors": [
            "Shrenik Zinage",
            "Peter Meckl",
            "Ilias Bilionis"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Accurate prediction of engine-out NOx is essential for meeting stringent emissions regulations and optimizing engine performance. Traditional approaches rely on models trained on data from a small number of engines, which can be insufficient in generalizing across an entire population of engines due to sensor biases and variations in input conditions. In real world applications, these models require tuning or calibration to maintain acceptable error tolerance when applied to other engines. This highlights the need for models that can adapt with minimal adjustments to accommodate engine-to-engine variability and sensor discrepancies. While previous studies have explored machine learning methods for predicting engine-out NOx, these approaches often fail to generalize reliably across different engines and operating environments. To address these issues, we propose a Bayesian calibration framework that combines Gaussian processes with approximate Bayesian computation to infer and correct sensor biases. Starting with a pre-trained model developed using nominal engine data, our method identifies engine specific sensor biases and recalibrates predictions accordingly. By incorporating these inferred biases, our approach generates posterior predictive distributions for engine-out NOx on unseen test data, achieving high accuracy without retraining the model. Our results demonstrate that this transferable modeling approach significantly improves the accuracy of predictions compared to conventional non-adaptive GP models, effectively addressing engine-to-engine variability and improving model generalizability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 77,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18191",
        "abs_url": "https://arxiv.org/abs/2511.18191",
        "pdf_url": "https://arxiv.org/pdf/2511.18191",
        "title": "Accelerating Time Series Foundation Models with Speculative Decoding",
        "authors": [
            "Pranav Subbaraman",
            "Fang Sun",
            "Yue Yao",
            "Huacong Tang",
            "Xiao Luo",
            "Yizhou Sun"
        ],
        "comments": "14 pages, 7 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Modern web applications--from real-time content recommendation and dynamic pricing to CDN optimization--increasingly rely on time-series forecasting to deliver personalized experiences to billions of users. Large-scale Transformer-based models have achieved state-of-the-art performance in time-series forecasting but suffer from high computational costs, limiting their deployment in latency-sensitive web applications. To address this challenge, we propose a general inference acceleration framework that adapts speculative decoding to autoregressive time-series models. Our approach employs a smaller \"draft\" model to propose future time-series patches, which are then verified in parallel by a larger \"target\" model, reducing the number of sequential forward passes required. We address key technical challenges in adapting this technique from discrete language tokens to continuous time-series distributions, including the design of acceptance criteria for multivariate Gaussian patches and practical variants that balance efficiency with accuracy. Through experiments on time series forecasting benchmarks relevant to web applications, we demonstrate significant inference speedups while maintaining competitive accuracy. The framework requires no architectural modifications to existing foundation models, making it immediately applicable to accelerate deployed time-series forecasting systems. Our implementation can be found at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 78,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18214",
        "abs_url": "https://arxiv.org/abs/2511.18214",
        "pdf_url": "https://arxiv.org/pdf/2511.18214",
        "title": "Deep Gaussian Process Proximal Policy Optimization",
        "authors": [
            "Matthijs van der Lende",
            "Juan Cardenas-Cartagena"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Uncertainty estimation for Reinforcement Learning (RL) is a critical component in control tasks where agents must balance safe exploration and efficient learning. While deep neural networks have enabled breakthroughs in RL, they often lack calibrated uncertainty estimates. We introduce Deep Gaussian Process Proximal Policy Optimization (GPPO), a scalable, model-free actor-critic algorithm that leverages Deep Gaussian Processes (DGPs) to approximate both the policy and value function. GPPO maintains competitive performance with respect to Proximal Policy Optimization on standard high-dimensional continuous control benchmarks while providing well-calibrated uncertainty estimates that can inform safer and more effective exploration.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 79,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18225",
        "abs_url": "https://arxiv.org/abs/2511.18225",
        "pdf_url": "https://arxiv.org/pdf/2511.18225",
        "title": "Adaptive Conformal Prediction for Quantum Machine Learning",
        "authors": [
            "Douglas Spencer",
            "Samual Nicholls",
            "Michele Caprio"
        ],
        "comments": "26 pages, 5 figures",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML); Other Statistics (stat.OT)",
        "abstract": "Quantum machine learning seeks to leverage quantum computers to improve upon classical machine learning algorithms. Currently, robust uncertainty quantification methods remain underdeveloped in the quantum domain, despite the critical need for reliable and trustworthy predictions. Recent work has introduced quantum conformal prediction, a framework that produces prediction sets that are guaranteed to contain the true outcome with user-specified probability. In this work, we formalise how the time-varying noise inherent in quantum processors can undermine conformal guarantees, even when calibration and test data are exchangeable. To address this challenge, we draw on Adaptive Conformal Inference, a method which maintains validity over time via repeated recalibration. We introduce Adaptive Quantum Conformal Prediction (AQCP), an algorithm which preserves asymptotic average coverage guarantees under arbitrary hardware noise conditions. Empirical studies on an IBM quantum processor demonstrate that AQCP achieves target coverage levels and exhibits greater stability than quantum conformal prediction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 80,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18247",
        "abs_url": "https://arxiv.org/abs/2511.18247",
        "pdf_url": "https://arxiv.org/pdf/2511.18247",
        "title": "Tail Distribution of Regret in Optimistic Reinforcement Learning",
        "authors": [
            "Sajad Khodadadian",
            "Mehrdad Moharrami"
        ],
        "comments": "18 pages, 0 figures",
        "subjects": "Machine Learning (cs.LG); Optimization and Control (math.OC)",
        "abstract": "We derive instance-dependent tail bounds for the regret of optimism-based reinforcement learning in finite-horizon tabular Markov decision processes with unknown transition dynamics. Focusing on a UCBVI-type algorithm, we characterize the tail distribution of the cumulative regret $R_K$ over $K$ episodes, rather than only its expectation or a single high-probability quantile. We analyze two natural exploration-bonus schedules: (i) a $K$-dependent scheme that explicitly incorporates the total number of episodes $K$, and (ii) a $K$-independent scheme that depends only on the current episode index. For both settings, we obtain an upper bound on $\\Pr(R_K \\ge x)$ that exhibits a distinctive two-regime structure: a sub-Gaussian tail starting from an instance-dependent scale $m_K$ up to a transition threshold, followed by a sub-Weibull tail beyond that point. We further derive corresponding instance-dependent bounds on the expected regret $\\mathbb{E}[R_K]$. The proposed algorithm depends on a tuning parameter $\\alpha$, which balances the expected regret and the range over which the regret exhibits a sub-Gaussian tail. To the best of our knowledge, our results provide one of the first comprehensive tail-regret guarantees for a standard optimistic algorithm in episodic reinforcement learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 81,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18260",
        "abs_url": "https://arxiv.org/abs/2511.18260",
        "pdf_url": "https://arxiv.org/pdf/2511.18260",
        "title": "Reduced-Basis Deep Operator Learning for Parametric PDEs with Independently Varying Boundary and Source Data",
        "authors": [
            "Yueqi Wang",
            "Guang Lin"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Numerical Analysis (math.NA)",
        "abstract": "Parametric PDEs power modern simulation, design, and digital-twin systems, yet their many-query workloads still hinge on repeatedly solving large finite-element systems. Existing operator-learning approaches accelerate this process but often rely on opaque learned trunks, require extensive labeled data, or break down when boundary and source data vary independently from physical parameters. We introduce RB-DeepONet, a hybrid operator-learning framework that fuses reduced-basis (RB) numerical structure with the branch-trunk architecture of DeepONet. The trunk is fixed to a rigorously constructed RB space generated offline via Greedy selection, granting physical interpretability, stability, and certified error control. The branch network predicts only RB coefficients and is trained label-free using a projected variational residual that targets the RB-Galerkin solution. For problems with independently varying loads or boundary conditions, we develop boundary and source modal encodings that compress exogenous data into low-dimensional coordinates while preserving accuracy. Combined with affine or empirical interpolation decompositions, RB-DeepONet achieves a strict offline-online split: all heavy lifting occurs offline, and online evaluation scales only with the RB dimension rather than the full mesh. We provide convergence guarantees separating RB approximation error from statistical learning error, and numerical experiments show that RB-DeepONet attains accuracy competitive with intrusive RB-Galerkin, POD-DeepONet, and FEONet while using dramatically fewer trainable parameters and achieving significant speedups. This establishes RB-DeepONet as an efficient, stable, and interpretable operator learner for large-scale parametric PDEs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 82,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18269",
        "abs_url": "https://arxiv.org/abs/2511.18269",
        "pdf_url": "https://arxiv.org/pdf/2511.18269",
        "title": "A Fair OR-ML Framework for Resource Substitution in Large-Scale Networks",
        "authors": [
            "Ved Mohan",
            "El Mehdi Er Raqabi",
            "Pascal Van Hentenryck"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Optimization and Control (math.OC)",
        "abstract": "Ensuring that the right resource is available at the right location and time remains a major challenge for organizations operating large-scale logistics networks. The challenge comes from uneven demand patterns and the resulting asymmetric flow of resources across the arcs, which create persistent imbalances at the network nodes. Resource substitution among multiple, potentially composite and interchangeable, resource types is a cost-effective way to mitigate these imbalances. This leads to the resource substitution problem, which aims at determining the minimum number of resource substitutions from an initial assignment to minimize the overall network imbalance. In decentralized settings, achieving globally coordinated solutions becomes even more difficult. When substitution entails costs, effective prescriptions must also incorporate fairness and account for the individual preferences of schedulers. This paper presents a generic framework that combines operations research (OR) and machine learning (ML) to enable fair resource substitution in large networks. The OR component models and solves the resource substitution problem under a fairness lens. The ML component leverages historical data to learn schedulers' preferences, guide intelligent exploration of the decision space, and enhance computational efficiency by dynamically selecting the top-$\\kappa$ resources for each arc in the network. The framework produces a portfolio of high-quality solutions from which schedulers can select satisfactory trade-offs. The proposed framework is applied to the network of one of the largest package delivery companies in the world, which serves as the primary motivation for this research. Computational results demonstrate substantial improvements over state-of-the-art methods, including an 80% reduction in model size and a 90% decrease in execution time while preserving optimality.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 83,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18287",
        "abs_url": "https://arxiv.org/abs/2511.18287",
        "pdf_url": "https://arxiv.org/pdf/2511.18287",
        "title": "TRIDENT: A Trimodal Cascade Generative Framework for Drug and RNA-Conditioned Cellular Morphology Synthesis",
        "authors": [
            "Rui Peng",
            "Ziru Liu",
            "Lingyuan Ye",
            "Yuxing Lu",
            "Boxin Shi",
            "Jinzhuo Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV); Quantitative Methods (q-bio.QM)",
        "abstract": "Accurately modeling the relationship between perturbations, transcriptional responses, and phenotypic changes is essential for building an AI Virtual Cell (AIVC). However, existing methods typically constrained to modeling direct associations, such as Perturbation $\\rightarrow$ RNA or Perturbation $\\rightarrow$ Morphology, overlook the crucial causal link from RNA to morphology. To bridge this gap, we propose TRIDENT, a cascade generative framework that synthesizes realistic cellular morphology by conditioning on both the perturbation and the corresponding gene expression profile. To train and evaluate this task, we construct MorphoGene, a new dataset pairing L1000 gene expression with Cell Painting images for 98 compounds. TRIDENT significantly outperforms state-of-the-art approaches, achieving up to 7-fold improvement with strong generalization to unseen compounds. In a case study on docetaxel, we validate that RNA-guided synthesis accurately produces the corresponding phenotype. An ablation study further confirms that this RNA conditioning is essential for the model's high fidelity. By explicitly modeling transcriptome-phenome mapping, TRIDENT provides a powerful in silico tool and moves us closer to a predictive virtual cell.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 84,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18291",
        "abs_url": "https://arxiv.org/abs/2511.18291",
        "pdf_url": "https://arxiv.org/pdf/2511.18291",
        "title": "ADF-LoRA: Alternating Low-Rank Aggregation for Decentralized Federated Fine-Tuning",
        "authors": [
            "Xiaoyu Wang",
            "Xiaotian Li",
            "Zhixiang Zhou",
            "Chen Li",
            "Yong Liu"
        ],
        "comments": "10 Pages",
        "subjects": "Machine Learning (cs.LG); Distributed, Parallel, and Cluster Computing (cs.DC)",
        "abstract": "This paper revisits alternating low-rank updates for federated fine-tuning and examines their behavior in decentralized federated learning (DFL). While alternating the LoRA matrices has been shown to stabilize aggregation in centralized FL, extending this mechanism to decentralized, peer-to-peer communication introduces new challenges due to phase-state mismatch and block-wise divergence across clients. We introduce ADF-LoRA, which synchronizes the update of only one low-rank matrix per round and mixes both matrices to maintain more consistent parameter states under decentralized propagation. This design preserves the cross-term suppression effect of alternating updates while improving stability in serverless topologies. We provide a convergence analysis under standard smoothness assumptions and evaluate ADF-LoRA on multiple GLUE tasks. Experiments show that ADF-LoRA achieves faster and smoother convergence and delivers the highest average accuracy across tasks, outperforming existing LoRA variants in decentralized FL by a consistent margin.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 85,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18297",
        "abs_url": "https://arxiv.org/abs/2511.18297",
        "pdf_url": "https://arxiv.org/pdf/2511.18297",
        "title": "GROOT: Graph Edge Re-growth and Partitioning for the Verification of Large Designs in Logic Synthesis",
        "authors": [
            "Kiran Thorat",
            "Hongwu Peng",
            "Yuebo Luo",
            "Xi Xie",
            "Shaoyi Huang",
            "Amit Hasan",
            "Jiahui Zhao",
            "Yingjie Li",
            "Zhijie Shi",
            "Cunxi Yu",
            "Caiwen Ding"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Traditional verification methods in chip design are highly time-consuming and computationally demanding, especially for large scale circuits. Graph neural networks (GNNs) have gained popularity as a potential solution to improve verification efficiency. However, there lacks a joint framework that considers all chip design domain knowledge, graph theory, and GPU kernel designs. To address this challenge, we introduce GROOT, an algorithm and system co-design framework that contains chip design domain knowledge and redesigned GPU kernels, to improve verification efficiency. More specifically, we create node features utilizing the circuit node types and the polarity of the connections between the input edges to nodes in And-Inverter Graphs (AIGs). We utilize a graph partitioning algorithm to divide the large graphs into smaller sub-graphs for fast GPU processing and develop a graph edge re-growth algorithm to recover verification accuracy. We carefully profile the EDA graph workloads and observe the uniqueness of their polarized distribution of high degree (HD) nodes and low degree (LD) nodes. We redesign two GPU kernels (HD-kernel and LD-kernel), to fit the EDA graph learning workload on a single GPU. We compare the results with state-of-the-art (SOTA) methods: GAMORA, a GNN-based approach, and the traditional ABC framework. Results show that GROOT achieves a significant reduction in memory footprint (59.38 %), with high accuracy (99.96%) for a very large CSA multiplier, i.e. 1,024 bits with a batch size of 16, which consists of 134,103,040 nodes and 268,140,544 edges. We compare GROOT with GPU-based GPU Kernel designs SOTAs such as cuSPARSE, MergePath-SpMM, and GNNAdvisor. We achieve up to 1.104x, 5.796x, and 1.469x improvement in runtime, respectively.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 86,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18303",
        "abs_url": "https://arxiv.org/abs/2511.18303",
        "pdf_url": "https://arxiv.org/pdf/2511.18303",
        "title": "Hierarchical Deep Research with Local-Web RAG: Toward Automated System-Level Materials Discovery",
        "authors": [
            "Rui Ding",
            "Rodrigo Pires Ferreira",
            "Yuxin Chen",
            "Junhong Chen"
        ],
        "comments": "A preliminary version appeared in The AI for Accelerated Materials Discovery (AI4Mat) Workshop at NeurIPS 2025",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We present a long-horizon, hierarchical deep research (DR) agent designed for complex materials and device discovery problems that exceed the scope of existing Machine Learning (ML) surrogates and closed-source commercial agents. Our framework instantiates a locally deployable DR instance that integrates local retrieval-augmented generation with large language model reasoners, enhanced by a Deep Tree of Research (DToR) mechanism that adaptively expands and prunes research branches to maximize coverage, depth, and coherence. We systematically evaluate across 27 nanomaterials/device topics using a large language model (LLM)-as-judge rubric with five web-enabled state-of-the-art models as jurors. In addition, we conduct dry-lab validations on five representative tasks, where human experts use domain simulations (e.g., density functional theory, DFT) to verify whether DR-agent proposals are actionable. Results show that our DR agent produces reports with quality comparable to--and often exceeding--those of commercial systems (ChatGPT-5-thinking/o3/o4-mini-high Deep Research) at a substantially lower cost, while enabling on-prem integration with local data and tools.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 87,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18312",
        "abs_url": "https://arxiv.org/abs/2511.18312",
        "pdf_url": "https://arxiv.org/pdf/2511.18312",
        "title": "DiM-TS: Bridge the Gap between Selective State Space Models and Time Series for Generative Modeling",
        "authors": [
            "Zihao Yao",
            "Jiankai Zuo",
            "Yaying Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Time series data plays a pivotal role in a wide variety of fields but faces challenges related to privacy concerns. Recently, synthesizing data via diffusion models is viewed as a promising solution. However, existing methods still struggle to capture long-range temporal dependencies and complex channel interrelations. In this research, we aim to utilize the sequence modeling capability of a State Space Model called Mamba to extend its applicability to time series data generation. We firstly analyze the core limitations in State Space Model, namely the lack of consideration for correlated temporal lag and channel permutation. Building upon the insight, we propose Lag Fusion Mamba and Permutation Scanning Mamba, which enhance the model's ability to discern significant patterns during the denoising process. Theoretical analysis reveals that both variants exhibit a unified matrix multiplication framework with the original Mamba, offering a deeper understanding of our method. Finally, we integrate two variants and introduce Diffusion Mamba for Time Series (DiM-TS), a high-quality time series generation model that better preserves the temporal periodicity and inter-channel correlations. Comprehensive experiments on public datasets demonstrate the superiority of DiM-TS in generating realistic time series while preserving diverse properties of data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 88,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18314",
        "abs_url": "https://arxiv.org/abs/2511.18314",
        "pdf_url": "https://arxiv.org/pdf/2511.18314",
        "title": "AnyExperts: On-Demand Expert Allocation for Multimodal Language Models with Mixture of Expert",
        "authors": [
            "Yuting Gao",
            "Wang Lan",
            "Hengyuan Zhao",
            "Linjiang Huang",
            "Si Liu",
            "Qingpei Guo"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Multimodal Mixture-of-Experts (MoE) models offer a promising path toward scalable and efficient large vision-language systems. However, existing approaches rely on rigid routing strategies (typically activating a fixed number of experts per token) ignoring the inherent heterogeneity in semantic importance across modalities. This leads to suboptimal compute allocation, where redundant tokens consume as many resources as critical ones. To address this, we propose AnyExperts, a novel on-demand, budget-aware dynamic routing framework that allocates a variable total number of expert slots per token based on its semantic importance. Crucially, to prevent uncontrolled compute growth, the total slots per token are constrained within a fixed range, and each slot is filled by either a real expert or a virtual expert, with the virtual share capped at a small maximum (e.g., 20%). The model then adaptively balances the real-to-virtual ratio per token, assigning more real experts to semantically rich regions and relying more on virtual experts for redundant content. Evaluated across diverse tasks in visual understanding, audio understanding, and NLP understanding, AnyExperts improves performance under the same compute budget. Notably, on general image/video tasks, it achieves comparable accuracy with 40% fewer real expert activations; on text-dense tasks (OCR and NLP), it maintains performance while reducing real expert usage by 10%. These results demonstrate that fine-grained, importance-driven expert allocation significantly enhances both the efficiency and effectiveness of multimodal MoE models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 89,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18331",
        "abs_url": "https://arxiv.org/abs/2511.18331",
        "pdf_url": "https://arxiv.org/pdf/2511.18331",
        "title": "DynamiX: Dynamic Resource eXploration for Personalized Ad-Recommendations",
        "authors": [
            "Sohini Roychowdhury",
            "Adam Holeman",
            "Mohammad Amin",
            "Feng Wei",
            "Bhaskar Mehta",
            "Srihari Reddy"
        ],
        "comments": "9 pages, 3 Tables, 5 images. this https URL",
        "subjects": "Machine Learning (cs.LG); Software Engineering (cs.SE)",
        "abstract": "For online ad-recommendation systems, processing complete user-ad-engagement histories is both computationally intensive and noise-prone. We introduce Dynamix, a scalable, personalized sequence exploration framework that optimizes event history processing using maximum relevance principles and self-supervised learning through Event Based Features (EBFs). Dynamix categorizes users-engagements at session and surface-levels by leveraging correlations between dwell-times and ad-conversion events. This enables targeted, event-level feature removal and selective feature boosting for certain user-segments, thereby yielding training and inference efficiency wins without sacrificing engaging ad-prediction accuracy. While, dynamic resource removal increases training and inference throughput by 1.15% and 1.8%, respectively, dynamic feature boosting provides 0.033 NE gains while boosting inference QPS by 4.2% over baseline models. These results demonstrate that Dynamix achieves significant cost efficiency and performance improvements in online user-sequence based recommendation models. Self-supervised user-segmentation and resource exploration can further boost complex feature selection strategies while optimizing for workflow and compute resources.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 90,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18334",
        "abs_url": "https://arxiv.org/abs/2511.18334",
        "pdf_url": "https://arxiv.org/pdf/2511.18334",
        "title": "Clinician-in-the-Loop Smart Home System to Detect Urinary Tract Infection Flare-Ups via Uncertainty-Aware Decision Support",
        "authors": [
            "Chibuike E. Ugwu",
            "Roschelle Fritz",
            "Diane J. Cook",
            "Janardhan Rao Doppa"
        ],
        "comments": "Accepted for publication at IAAI-26 / AAAI-26",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Urinary tract infection (UTI) flare-ups pose a significant health risk for older adults with chronic conditions. These infections often go unnoticed until they become severe, making early detection through innovative smart home technologies crucial. Traditional machine learning (ML) approaches relying on simple binary classification for UTI detection offer limited utility to nurses and practitioners as they lack insight into prediction uncertainty, hindering informed clinical decision-making. This paper presents a clinician-in-the-loop (CIL) smart home system that leverages ambient sensor data to extract meaningful behavioral markers, train robust predictive ML models, and calibrate them to enable uncertainty-aware decision support. The system incorporates a statistically valid uncertainty quantification method called Conformal-Calibrated Interval (CCI), which quantifies uncertainty and abstains from making predictions (\"I don't know\") when the ML model's confidence is low. Evaluated on real-world data from eight smart homes, our method outperforms baseline methods in recall and other classification metrics while maintaining the lowest abstention proportion and interval width. A survey of 42 nurses confirms that our system's outputs are valuable for guiding clinical decision-making, underscoring their practical utility in improving informed decisions and effectively managing UTIs and other condition flare-ups in older adults.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 91,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18394",
        "abs_url": "https://arxiv.org/abs/2511.18394",
        "pdf_url": "https://arxiv.org/pdf/2511.18394",
        "title": "Future Is Unevenly Distributed: Forecasting Ability of LLMs Depends on What We're Asking",
        "authors": [
            "Chinmay Karkar",
            "Paras Chopra"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Large Language Models (LLMs) demonstrate partial forecasting competence across social, political, and economic events. Yet, their predictive ability varies sharply with domain structure and prompt framing. We investigate how forecasting performance varies with different model families on real-world questions about events that happened beyond the model cutoff date. We analyze how context, question type, and external knowledge affect accuracy and calibration, and how adding factual news context modifies belief formation and failure modes. Our results show that forecasting ability is highly variable as it depends on what, and how, we ask.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 92,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18404",
        "abs_url": "https://arxiv.org/abs/2511.18404",
        "pdf_url": "https://arxiv.org/pdf/2511.18404",
        "title": "Pre-training Graph Neural Networks on 2D and 3D Molecular Structures by using Multi-View Conditional Information Bottleneck",
        "authors": [
            "Van Thuy Hoang",
            "O-Joun Lee"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Recent pre-training strategies for molecular graphs have attempted to use 2D and 3D molecular views as both inputs and self-supervised signals, primarily aligning graph-level representations. However, existing studies remain limited in addressing two main challenges of multi-view molecular learning: (1) discovering shared information between two views while diminishing view-specific information and (2) identifying and aligning important substructures, e.g., functional groups, which are crucial for enhancing cross-view consistency and model expressiveness. To solve these challenges, we propose a Multi-View Conditional Information Bottleneck framework, called MVCIB, for pre-training graph neural networks on 2D and 3D molecular structures in a self-supervised setting. Our idea is to discover the shared information while minimizing irrelevant features from each view under the MVCIB principle, which uses one view as a contextual condition to guide the representation learning of its counterpart. To enhance semantic and structural consistency across views, we utilize key substructures, e.g., functional groups and ego-networks, as anchors between the two views. Then, we propose a cross-attention mechanism that captures fine-grained correlations between the substructures to achieve subgraph alignment across views. Extensive experiments in four molecular domains demonstrated that MVCIB consistently outperforms baselines in both predictive performance and interpretability. Moreover, MVCIB achieved the 3d Weisfeiler-Lehman expressiveness power to distinguish not only non-isomorphic graphs but also different 3D geometries that share identical 2D connectivity, such as isomers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 93,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18457",
        "abs_url": "https://arxiv.org/abs/2511.18457",
        "pdf_url": "https://arxiv.org/pdf/2511.18457",
        "title": "Radiation-Preserving Selective Imaging for Pediatric Hip Dysplasia: A Cross-Modal Ultrasound-Xray Policy with Limited Labels",
        "authors": [
            "Duncan Stothers",
            "Ben Stothers",
            "Emily Schaeffer",
            "Kishore Mulpuri"
        ],
        "comments": "Accepted (with oral presentation) to the AAAI 2026 AIMedHealth Bridge Program",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We study an ultrasound-first, radiation-preserving policy for developmental dysplasia of the hip (DDH) that requests a radiograph only when needed. We (i) pretrain modality-specific encoders (ResNet-18) with SimSiam on a large unlabelled registry (37186 ultrasound; 19546 radiographs), (ii) freeze the backbones and fit small, measurement-faithful heads on DDH relevant landmarks and measurements (iii) calibrate a one sided conformal deferral rule on ultrasound predictions that provides finite sample coverage guarantees under exchangeability, using a held-out calibration set. Ultrasound heads predict Graf alpha, beta, and femoral head coverage; X-ray heads predict acetabular index (AI), center-edge (CE) angle and IHDI grade. On our held out labeled evaluation set, ultrasound measurement error is modest (e.g., alpha MAE ~= 9.7 degrees, coverage MAE ~= 14.0%), while radiographic probes achieve AI and CE MAEs of ~= 7.6 degrees and ~= 8.9 degrees, respectively. The calibrated US-only policy is explored across rule families (alpha-only; alpha OR coverage; alpha AND coverage), uncertainty inflation factors, and per-utility trade-offs using decision-curve analysis. Conservative settings yield high coverage with near-zero US-only rates; permissive settings (e.g., alpha OR coverage at larger deltas) achieve non-zero US-only throughput with expected coverage tradeoffs. The result is a simple, reproducible pipeline that turns limited labels into interpretable measurements and tunable selective imaging curves suitable for clinical handoff and future external validation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 94,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18474",
        "abs_url": "https://arxiv.org/abs/2511.18474",
        "pdf_url": "https://arxiv.org/pdf/2511.18474",
        "title": "Adaptive Mesh-Quantization for Neural PDE Solvers",
        "authors": [
            "Winfried van den Dool",
            "Maksim Zhdanov",
            "Yuki M. Asano",
            "Max Welling"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Physical systems commonly exhibit spatially varying complexity, presenting a significant challenge for neural PDE solvers. While Graph Neural Networks can handle the irregular meshes required for complex geometries and boundary conditions, they still apply uniform computational effort across all nodes regardless of the underlying physics complexity. This leads to inefficient resource allocation where computationally simple regions receive the same treatment as complex phenomena. We address this challenge by introducing Adaptive Mesh Quantization: spatially adaptive quantization across mesh node, edge, and cluster features, dynamically adjusting the bit-width used by a quantized model. We propose an adaptive bit-width allocation strategy driven by a lightweight auxiliary model that identifies high-loss regions in the input mesh. This enables dynamic resource distribution in the main model, where regions of higher difficulty are allocated increased bit-width, optimizing computational resource utilization. We demonstrate our framework's effectiveness by integrating it with two state-of-the-art models, MP-PDE and GraphViT, to evaluate performance across multiple tasks: 2D Darcy flow, large-scale unsteady fluid dynamics in 2D, steady-state Navier-Stokes simulations in 3D, and a 2D hyper-elasticity problem. Our framework demonstrates consistent Pareto improvements over uniformly quantized baselines, yielding up to 50% improvements in performance at the same cost.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 95,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18489",
        "abs_url": "https://arxiv.org/abs/2511.18489",
        "pdf_url": "https://arxiv.org/pdf/2511.18489",
        "title": "Real-Time Personalized Content Adaptation through Matrix Factorization and Context-Aware Federated Learning",
        "authors": [
            "Sai Puppala",
            "Ismail Hossain",
            "Md Jahangir Alam",
            "Sajedul Talukder"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Our study presents a multifaceted approach to enhancing user interaction and content relevance in social media platforms through a federated learning framework. We introduce personalized LLM Federated Learning and Context-based Social Media models. In our framework, multiple client entities receive a foundational GPT model, which is fine-tuned using locally collected social media data while ensuring data privacy through federated aggregation. Key modules focus on categorizing user-generated content, computing user persona scores, and identifying relevant posts from friends networks. By integrating a sophisticated social engagement quantification method with matrix factorization techniques, our system delivers real-time personalized content suggestions tailored to individual preferences. Furthermore, an adaptive feedback loop, alongside a robust readability scoring algorithm, significantly enhances the quality and relevance of the content presented to users. This comprehensive solution not only addresses the challenges of content filtering and recommendation but also fosters a more engaging social media experience while safeguarding user privacy, setting a new standard for personalized interactions in digital platforms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 96,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18515",
        "abs_url": "https://arxiv.org/abs/2511.18515",
        "pdf_url": "https://arxiv.org/pdf/2511.18515",
        "title": "RRaPINNs: Residual Risk-Aware Physics Informed Neural Networks",
        "authors": [
            "Ange-Clément Akazan",
            "Issa Karambal",
            "Jean Medard Ngnotchouye",
            "Abebe Geletu Selassie. W"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Physics-informed neural networks (PINNs) typically minimize average residuals, which can conceal large, localized errors. We propose Residual Risk-Aware Physics-Informed Neural Networks PINNs (RRaPINNs), a single-network framework that optimizes tail-focused objectives using Conditional Value-at-Risk (CVaR), we also introduced a Mean-Excess (ME) surrogate penalty to directly control worst-case PDE residuals. This casts PINN training as risk-sensitive optimization and links it to chance-constrained formulations. The method is effective and simple to implement. Across several partial differential equations (PDEs) such as Burgers, Heat, Korteweg-de-Vries, and Poisson (including a Poisson interface problem with a source jump at x=0.5) equations, RRaPINNs reduce tail residuals while maintaining or improving mean errors compared to vanilla PINNs, Residual-Based Attention and its variant using convolution weighting; the ME surrogate yields smoother optimization than a direct CVaR hinge. The chance constraint reliability level $\\alpha$ acts as a transparent knob trading bulk accuracy (lower $\\alpha$ ) for stricter tail control (higher $\\alpha$ ). We discuss the framework limitations, including memoryless sampling, global-only tail budgeting, and residual-centric risk, and outline remedies via persistent hard-point replay, local risk budgets, and multi-objective risk over BC/IC terms. RRaPINNs offer a practical path to reliability-aware scientific ML for both smooth and discontinuous PDEs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 97,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18519",
        "abs_url": "https://arxiv.org/abs/2511.18519",
        "pdf_url": "https://arxiv.org/pdf/2511.18519",
        "title": "CHIPS: Efficient CLIP Adaptation via Curvature-aware Hybrid Influence-based Data Selection",
        "authors": [
            "Xinlin Zhuang",
            "Yichen Li",
            "Xiwei Liu",
            "Haolin Yang",
            "Yifan Lu",
            "Ziyun Zou",
            "Yulong Li",
            "Huifa Li",
            "Dongliang Chen",
            "Qinglei Wang",
            "Weiyang Liu",
            "Ying Qian",
            "Jiangming Shi",
            "Imran Razzak"
        ],
        "comments": "preprint, under-review",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Adapting CLIP to vertical domains is typically approached by novel fine-tuning strategies or by continual pre-training (CPT) on large domain-specific datasets. Yet, data itself remains an underexplored factor in this process. We revisit this task from a data-centric perspective: Can effective data selection substitute for large-scale datasets in CPT? We introduce CHIPS (Curvature-aware Hybrid Influence in Projection Subspace), which assigns each image-text pair a utility score that integrates three complementary factors aligned with three goals: faithfulness via a curvature-aware, Newton-style alignment computed in CLIP's end-point subspace; scalability via an InfoNCE-aware curvature estimator with Johnson-Lindenstrauss (JL) sketching; and retention via a selection-aware relevance weight combined with learnability to balance target adaptation against general-domain preservation. We justify this design theoretically by proving a lower-bound guarantee on the proxy's correlation with full-parameter alignment and by characterizing the bias-variance trade-offs introduced by curvature mixing and JL sketching. We evaluate CHIPS empirically across various settings: 1) CHIPS attains state-of-the-art performance among selection baselines on 17 medical benchmarks, matches full-dataset CPT with 30% of the data, and outperforms half-dataset CPT using only 10%; 2) on 31 general-domain benchmarks, CHIPS yields the smallest performance drop under 10-30% data-retention budgets. Code, data, and checkpoints will be released.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 98,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18521",
        "abs_url": "https://arxiv.org/abs/2511.18521",
        "pdf_url": "https://arxiv.org/pdf/2511.18521",
        "title": "Hyperspectral Variational Autoencoders for Joint Data Compression and Component Extraction",
        "authors": [
            "Core Francisco Park",
            "Manuel Perez-Carrasco",
            "Caroline Nowlan",
            "Cecilia Garraffo"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Earth and Planetary Astrophysics (astro-ph.EP); Instrumentation and Methods for Astrophysics (astro-ph.IM)",
        "abstract": "Geostationary hyperspectral satellites generate terabytes of data daily, creating critical challenges for storage, transmission, and distribution to the scientific community. We present a variational autoencoder (VAE) approach that achieves x514 compression of NASA's TEMPO satellite hyperspectral observations (1028 channels, 290-490nm) with reconstruction errors 1-2 orders of magnitude below the signal across all wavelengths. This dramatic data volume reduction enables efficient archival and sharing of satellite observations while preserving spectral fidelity. Beyond compression, we investigate to what extent atmospheric information is retained in the compressed latent space by training linear and nonlinear probes to extract Level-2 products (NO2, O3, HCHO, cloud fraction). Cloud fraction and total ozone achieve strong extraction performance (R^2 = 0.93 and 0.81 respectively), though these represent relatively straightforward retrievals given their distinct spectral signatures. In contrast, tropospheric trace gases pose genuine challenges for extraction (NO2 R^2 = 0.20, HCHO R^2 = 0.51) reflecting their weaker signals and complex atmospheric interactions. Critically, we find the VAE encodes atmospheric information in a semi-linear manner - nonlinear probes substantially outperform linear ones - and that explicit latent supervision during training provides minimal improvement, revealing fundamental encoding challenges for certain products. This work demonstrates that neural compression can dramatically reduce hyperspectral data volumes while preserving key atmospheric signals, addressing a critical bottleneck for next-generation Earth observation systems. Code - this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 99,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18567",
        "abs_url": "https://arxiv.org/abs/2511.18567",
        "pdf_url": "https://arxiv.org/pdf/2511.18567",
        "title": "In Search of Goodness: Large Scale Benchmarking of Goodness Functions for the Forward-Forward Algorithm",
        "authors": [
            "Arya Shah",
            "Vaibhav Tripathi"
        ],
        "comments": "24 pages, 5 tables, 17 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The Forward-Forward (FF) algorithm offers a biologically plausible alternative to backpropagation, enabling neural networks to learn through local updates. However, FF's efficacy relies heavily on the definition of \"goodness\", which is a scalar measure of neural activity. While current implementations predominantly utilize a simple sum-of-squares metric, it remains unclear if this default choice is optimal. To address this, we benchmarked 21 distinct goodness functions across four standard image datasets (MNIST, FashionMNIST, CIFAR-10, STL-10), evaluating classification accuracy, energy consumption, and carbon footprint. We found that certain alternative goodness functions inspired from various domains significantly outperform the standard baseline. Specifically, \\texttt{game\\_theoretic\\_local} achieved 97.15\\% accuracy on MNIST, \\texttt{softmax\\_energy\\_margin\\_local} reached 82.84\\% on FashionMNIST, and \\texttt{triplet\\_margin\\_local} attained 37.69\\% on STL-10. Furthermore, we observed substantial variability in computational efficiency, highlighting a critical trade-off between predictive performance and environmental cost. These findings demonstrate that the goodness function is a pivotal hyperparameter in FF design. We release our code on \\href{this https URL}{Github} for reference and reproducibility.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 100,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18571",
        "abs_url": "https://arxiv.org/abs/2511.18571",
        "pdf_url": "https://arxiv.org/pdf/2511.18571",
        "title": "SAMBA: Toward a Long-Context EEG Foundation Model via Spatial Embedding and Differential Mamba",
        "authors": [
            "Jiazhen Hong",
            "Geoffrey Mackellar",
            "Soheila Ghane"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Long-sequence electroencephalogram (EEG) modeling is essential for developing generalizable EEG representation models. This need arises from the high sampling rate of EEG data and the long recording durations required to capture extended neurological patterns in brain activity. Transformer-based models have shown promise in modeling short sequences of a few seconds; however, their quadratic complexity limits scalability to longer contexts. Moreover, variability in electrode montage across available datasets, along with inter-subject differences in brain signals, pose significant challenges to developing a generalizable and robust foundation model. We propose \\textit{SAMBA}, a self-supervised learning framework with a Mamba-based U-shaped encoder-decoder architecture, which effectively captures long-range temporal dependencies and spatial variability in EEG data. Leveraging the inherent ability of Mamba in processing long context sizes, we introduce: (1) \\textit{Temporal Semantic Random Masking} for semantic-level sequence reconstruction, (2) a \\textit{Multi-Head Differential Mamba} module to suppress redundancy and emphasize salient temporal structures, and (3) a \\textit{Spatial-Adaptive Input Embedding} that learns unified embeddings in a three-dimensional Euclidean space, enabling robustness across devices. Experiments on thirteen EEG datasets across diverse tasks, electrode configurations, and sequence durations demonstrate that SAMBA consistently outperforms state-of-the-art methods while maintaining low memory consumption and inference time. We also show the learned spatial weight maps from our embedding module align closely with task-relevant neurophysiological regions, demonstrating the learnability and interpretability of SAMBA. These results highlight SAMBA's scalability and practical potential as a foundation model for real-time brain-computer interface applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 101,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18593",
        "abs_url": "https://arxiv.org/abs/2511.18593",
        "pdf_url": "https://arxiv.org/pdf/2511.18593",
        "title": "Generative Myopia: Why Diffusion Models Fail at Structure",
        "authors": [
            "Milad Siami"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Systems and Control (eess.SY); Spectral Theory (math.SP)",
        "abstract": "Graph Diffusion Models (GDMs) optimize for statistical likelihood, implicitly acting as \\textbf{frequency filters} that favor abundant substructures over spectrally critical ones. We term this phenomenon \\textbf{Generative Myopia}. In combinatorial tasks like graph sparsification, this leads to the catastrophic removal of ``rare bridges,'' edges that are structurally mandatory ($R_{\\text{eff}} \\approx 1$) but statistically scarce. We prove theoretically and empirically that this failure is driven by \\textbf{Gradient Starvation}: the optimization landscape itself suppresses rare structural signals, rendering them unlearnable regardless of model capacity. To resolve this, we introduce \\textbf{Spectrally-Weighted Diffusion}, which re-aligns the variational objective using Effective Resistance. We demonstrate that spectral priors can be amortized into the training phase with zero inference overhead. Our method eliminates myopia, matching the performance of an optimal Spectral Oracle and achieving \\textbf{100\\% connectivity} on adversarial benchmarks where standard diffusion fails completely (0\\%).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 102,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18611",
        "abs_url": "https://arxiv.org/abs/2511.18611",
        "pdf_url": "https://arxiv.org/pdf/2511.18611",
        "title": "CycleSL: Server-Client Cyclical Update Driven Scalable Split Learning",
        "authors": [
            "Mengdi Wang",
            "Efe Bozkir",
            "Enkelejda Kasneci"
        ],
        "comments": "The IEEE/CVF Winter Conference on Applications of Computer Vision 2026 (WACV-26)",
        "subjects": "Machine Learning (cs.LG); Distributed, Parallel, and Cluster Computing (cs.DC)",
        "abstract": "Split learning emerges as a promising paradigm for collaborative distributed model training, akin to federated learning, by partitioning neural networks between clients and a server without raw data exchange. However, sequential split learning suffers from poor scalability, while parallel variants like parallel split learning and split federated learning often incur high server resource overhead due to model duplication and aggregation, and generally exhibit reduced model performance and convergence owing to factors like client drift and lag. To address these limitations, we introduce CycleSL, a novel aggregation-free split learning framework that enhances scalability and performance and can be seamlessly integrated with existing methods. Inspired by alternating block coordinate descent, CycleSL treats server-side training as an independent higher-level machine learning task, resampling client-extracted features (smashed data) to mitigate heterogeneity and drift. It then performs cyclical updates, namely optimizing the server model first, followed by client updates using the updated server for gradient computation. We integrate CycleSL into previous algorithms and benchmark them on five publicly available datasets with non-iid data distribution and partial client attendance. Our empirical findings highlight the effectiveness of CycleSL in enhancing model performance. Our source code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 103,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18615",
        "abs_url": "https://arxiv.org/abs/2511.18615",
        "pdf_url": "https://arxiv.org/pdf/2511.18615",
        "title": "Bayesian-based Online Label Shift Estimation with Dynamic Dirichlet Priors",
        "authors": [
            "Jiawei Hu",
            "Javier A. Barria"
        ],
        "comments": "13 pages, submitted to IEEE journal for possible publication",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Label shift, a prevalent challenge in supervised learning, arises when the class prior distribution of test data differs from that of training data, leading to significant degradation in classifier performance. To accurately estimate the test priors and enhance classification accuracy, we propose a Bayesian framework for label shift estimation, termed Full Maximum A Posterior Label Shift (FMAPLS), along with its online version, online-FMAPLS. Leveraging batch and online Expectation-Maximization (EM) algorithms, these methods jointly and dynamically optimize Dirichlet hyperparameters $\\boldsymbol{\\alpha}$ and class priors $\\boldsymbol{\\pi}$, thereby overcoming the rigid constraints of the existing Maximum A Posterior Label Shift (MAPLS) approach. Moreover, we introduce a linear surrogate function (LSF) to replace gradient-based hyperparameter updates, yielding closed-form solutions that reduce computational complexity while retaining asymptotic equivalence. The online variant substitutes the batch E-step with a stochastic approximation, enabling real-time adaptation to streaming data. Furthermore, our theoretical analysis reveals a fundamental trade-off between online convergence rate and estimation accuracy. Extensive experiments on CIFAR100 and ImageNet datasets under shuffled long-tail and Dirichlet test priors demonstrate that FMAPLS and online-FMAPLS respectively achieve up to 40% and 12% lower KL divergence and substantial improvements in post-shift accuracy over state-of-the-art baselines, particularly under severe class imbalance and distributional uncertainty. These results confirm the robustness, scalability, and suitability of the proposed methods for large-scale and dynamic learning scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 104,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18631",
        "abs_url": "https://arxiv.org/abs/2511.18631",
        "pdf_url": "https://arxiv.org/pdf/2511.18631",
        "title": "FOS: A Large-Scale Temporal Graph Benchmark for Scientific Interdisciplinary Link Prediction",
        "authors": [
            "Kiyan Rezaee",
            "Morteza Ziabakhsh",
            "Niloofar Nikfarjam",
            "Mohammad M. Ghassemi",
            "Yazdan Rezaee Jouryabi",
            "Sadegh Eskandari",
            "Reza Lashgari"
        ],
        "comments": "21 pages, 10 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Interdisciplinary scientific breakthroughs mostly emerge unexpectedly, and forecasting the formation of novel research fields remains a major challenge. We introduce FOS (Future Of Science), a comprehensive time-aware graph-based benchmark that reconstructs annual co-occurrence graphs of 65,027 research sub-fields (spanning 19 general domains) over the period 1827-2024. In these graphs, edges denote the co-occurrence of two fields in a single publication and are timestamped with the corresponding publication year. Nodes are enriched with semantic embeddings, and edges are characterized by temporal and topological descriptors. We formulate the prediction of new field-pair linkages as a temporal link-prediction task, emphasizing the \"first-time\" connections that signify pioneering interdisciplinary directions. Through extensive experiments, we evaluate a suite of state-of-the-art temporal graph architectures under multiple negative-sampling regimes and show that (i) embedding long-form textual descriptions of fields significantly boosts prediction accuracy, and (ii) distinct model classes excel under different evaluation settings. Case analyses show that top-ranked link predictions on FOS align with field pairings that emerge in subsequent years of academic publications. We publicly release FOS, along with its temporal data splits and evaluation code, to establish a reproducible benchmark for advancing research in predicting scientific frontiers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 105,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18632",
        "abs_url": "https://arxiv.org/abs/2511.18632",
        "pdf_url": "https://arxiv.org/pdf/2511.18632",
        "title": "The Locally Deployable Virtual Doctor: LLM Based Human Interface for Automated Anamnesis and Database Conversion",
        "authors": [
            "Jan Benedikt Ruhland",
            "Doguhan Bahcivan",
            "Jan-Peter Sowa",
            "Ali Canbay",
            "Dominik Heider"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Recent advances in large language models made it possible to achieve high conversational performance with substantially reduced computational demands, enabling practical on-site deployment in clinical environments. Such progress allows for local integration of AI systems that uphold strict data protection and patient privacy requirements, yet their secure implementation in medicine necessitates careful consideration of ethical, regulatory, and technical constraints. In this study, we introduce MedChat, a locally deployable virtual physician framework that integrates an LLM-based medical chatbot with a diffusion-driven avatar for automated and structured anamnesis. The chatbot was fine-tuned using a hybrid corpus of real and synthetically generated medical dialogues, while model efficiency was optimized via Low-Rank Adaptation. A secure and isolated database interface was implemented to ensure complete separation between patient data and the inference process. The avatar component was realized through a conditional diffusion model operating in latent space, trained on researcher video datasets and synchronized with mel-frequency audio features for realistic speech and facial animation. Unlike existing cloud-based systems, this work demonstrates the feasibility of a fully offline, locally deployable LLM-diffusion framework for clinical anamnesis. The autoencoder and diffusion networks exhibited smooth convergence, and MedChat achieved stable fine-tuning with strong generalization to unseen data. The proposed system thus provides a privacy-preserving, resource-efficient foundation for AI-assisted clinical anamnesis, also in low-cost settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 106,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18643",
        "abs_url": "https://arxiv.org/abs/2511.18643",
        "pdf_url": "https://arxiv.org/pdf/2511.18643",
        "title": "Kitty: Accurate and Efficient 2-bit KV Cache Quantization with Dynamic Channel-wise Precision Boost",
        "authors": [
            "Haojun Xia",
            "Xiaoxia Wu",
            "Jisen Li",
            "Robert Wu",
            "Junxiong Wang",
            "Jue Wang",
            "Chenxi Li",
            "Aman Singhal",
            "Alay Dilipbhai Shah",
            "Alpay Ariyak",
            "Donglin Zhuang",
            "Zhongzhu Zhou",
            "Ben Athiwaratkun",
            "Zhen Zheng",
            "Shuaiwen Leon Song"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "The KV cache is a dominant memory bottleneck for LLM inference. While 4-bit KV quantization preserves accuracy, 2-bit often degrades it, especially on long-context reasoning. We close this gap via an algorithm-system co-design for mixed-precision KV caching: Kitty. On the algorithm side, extensive experiments show that Dynamic Channel-wise Precision Boost -- which ranks Key-cache channels by sensitivity and keeps only a small fraction at higher precision -- maintains near-zero loss in accuracy drop while approaching 2-bit memory. The main challenge is handling dynamic 4-bit channel boosts while keeping the page layout coalesced and the dequantization uniform, with no scattered reads or hard-coded masks. Kitty addresses these issues by decompose each mixed-precision Key page into two tensors with unified 2-bit precision. Based on this, Kitty provides a page-centric KV layout, Triton-compatible page dequantization kernels, and a lightweight runtime pipeline that preserves coalescing and avoids divergence. Across seven tasks and two model families (Qwen3, LLaMA3), Kitty cuts KV memory by nearly 8x with negligible accuracy loss, enabling up to 8x larger batches and 2.1x-4.1x higher throughput under the same memory budget. We release the full implementation of Kitty at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 107,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18660",
        "abs_url": "https://arxiv.org/abs/2511.18660",
        "pdf_url": "https://arxiv.org/pdf/2511.18660",
        "title": "Subtract the Corruption: Training-Data-Free Corrective Machine Unlearning using Task Arithmetic",
        "authors": [
            "Mostafa Mozafari",
            "Farooq Ahmad Wani",
            "Maria Sofia Bucarelli",
            "Fabrizio Silvestri"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Corrupted training data are ubiquitous. Corrective Machine Unlearning (CMU) seeks to remove the influence of such corruption post-training. Prior CMU typically assumes access to identified corrupted training samples (a ``forget set''). However, in many real-world scenarios the training data are no longer accessible. We formalize \\emph{source-free} CMU, where the original training data are unavailable and, consequently, no forget set of identified corrupted training samples can be specified. Instead, we assume a small proxy (surrogate) set of corrupted samples that reflect the suspected corruption type without needing to be the original training samples. In this stricter setting, methods relying on forget set are ineffective or narrow in scope. We introduce \\textit{Corrective Unlearning in Task Space} (CUTS), a lightweight weight space correction method guided by the proxy set using task arithmetic principles. CUTS treats the clean and the corruption signal as distinct tasks. Specifically, we briefly fine-tune the corrupted model on the proxy to amplify the corruption mechanism in the weight space, compute the difference between the corrupted and fine-tuned weights as a proxy task vector, and subtract a calibrated multiple of this vector to cancel the corruption. Without access to clean data or a forget set, CUTS recovers a large fraction of the lost utility under label noise and, for backdoor triggers, nearly eliminates the attack with minimal damage to utility, outperforming state-of-the-art specialized CMU methods in source-free setting.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 108,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18671",
        "abs_url": "https://arxiv.org/abs/2511.18671",
        "pdf_url": "https://arxiv.org/pdf/2511.18671",
        "title": "Multi-Agent Cross-Entropy Method with Monotonic Nonlinear Critic Decomposition",
        "authors": [
            "Yan Wang",
            "Ke Deng",
            "Yongli Ren"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Multiagent Systems (cs.MA)",
        "abstract": "Cooperative multi-agent reinforcement learning (MARL) commonly adopts centralized training with decentralized execution (CTDE), where centralized critics leverage global information to guide decentralized actors. However, centralized-decentralized mismatch (CDM) arises when the suboptimal behavior of one agent degrades others' learning. Prior approaches mitigate CDM through value decomposition, but linear decompositions allow per-agent gradients at the cost of limited expressiveness, while nonlinear decompositions improve representation but require centralized gradients, reintroducing CDM. To overcome this trade-off, we propose the multi-agent cross-entropy method (MCEM), combined with monotonic nonlinear critic decomposition (NCD). MCEM updates policies by increasing the probability of high-value joint actions, thereby excluding suboptimal behaviors. For sample efficiency, we extend off-policy learning with a modified k-step return and Retrace. Analysis and experiments demonstrate that MCEM outperforms state-of-the-art methods across both continuous and discrete action benchmarks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 109,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18689",
        "abs_url": "https://arxiv.org/abs/2511.18689",
        "pdf_url": "https://arxiv.org/pdf/2511.18689",
        "title": "QuantKAN: A Unified Quantization Framework for Kolmogorov Arnold Networks",
        "authors": [
            "Kazi Ahmed Asif Fuad",
            "Lizhong Chen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Kolmogorov Arnold Networks (KANs) represent a new class of neural architectures that replace conventional linear transformations and node-based nonlinearities with spline-based function approximations distributed along network edges. Although KANs offer strong expressivity and interpretability, their heterogeneous spline and base branch parameters hinder efficient quantization, which remains unexamined compared to CNNs and Transformers. In this paper, we present QuantKAN, a unified framework for quantizing KANs across both quantization aware training (QAT) and post-training quantization (PTQ) regimes. QuantKAN extends modern quantization algorithms, such as LSQ, LSQ+, PACT, DoReFa, QIL, GPTQ, BRECQ, AdaRound, AWQ, and HAWQ-V2, to spline based layers with branch-specific quantizers for base, spline, and activation components. Through extensive experiments on MNIST, CIFAR 10, and CIFAR 100 across multiple KAN variants (EfficientKAN, FastKAN, PyKAN, and KAGN), we establish the first systematic benchmarks for low-bit spline networks. Our results show that KANs, particularly deeper KAGN variants, are compatible with low-bit quantization but exhibit strong method architecture interactions: LSQ, LSQ+, and PACT preserve near full precision accuracy at 4 bit for shallow KAN MLP and ConvNet models, while DoReFa provides the most stable behavior for deeper KAGN under aggressive low-bit settings. For PTQ, GPTQ and Uniform consistently deliver the strongest overall performance across datasets, with BRECQ highly competitive on simpler regimes such as MNIST. Our proposed QuantKAN framework thus unifies spline learning and quantization, and provides practical tools and guidelines for efficiently deploying KANs in real-world, resource-constrained environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 110,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18716",
        "abs_url": "https://arxiv.org/abs/2511.18716",
        "pdf_url": "https://arxiv.org/pdf/2511.18716",
        "title": "GRIT-LP: Graph Transformer with Long-Range Skip Connection and Partitioned Spatial Graphs for Accurate Ice Layer Thickness Prediction",
        "authors": [
            "Zesheng Liu",
            "Maryam Rahnemoonfar"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Graph transformers have demonstrated remarkable capability on complex spatio-temporal tasks, yet their depth is often limited by oversmoothing and weak long-range dependency modeling. To address these challenges, we introduce GRIT-LP, a graph transformer explicitly designed for polar ice-layer thickness estimation from polar radar imagery. Accurately estimating ice layer thickness is critical for understanding snow accumulation, reconstructing past climate patterns and reducing uncertainties in projections of future ice sheet evolution and sea level rise. GRIT-LP combines an inductive geometric graph learning framework with self-attention mechanism, and introduces two major innovations that jointly address challenges in modeling the spatio-temporal patterns of ice layers: a partitioned spatial graph construction strategy that forms overlapping, fully connected local neighborhoods to preserve spatial coherence and suppress noise from irrelevant long-range links, and a long-range skip connection mechanism within the transformer that improves information flow and mitigates oversmoothing in deeper attention layers. We conducted extensive experiments, demonstrating that GRIT-LP outperforms current state-of-the-art methods with a 24.92\\% improvement in root mean squared error. These results highlight the effectiveness of graph transformers in modeling spatiotemporal patterns by capturing both localized structural features and long-range dependencies across internal ice layers, and demonstrate their potential to advance data-driven understanding of cryospheric processes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 111,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18721",
        "abs_url": "https://arxiv.org/abs/2511.18721",
        "pdf_url": "https://arxiv.org/pdf/2511.18721",
        "title": "Towards Realistic Guarantees: A Probabilistic Certificate for SmoothLLM",
        "authors": [
            "Adarsh Kumarappan",
            "Ayushi Mehrotra"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The SmoothLLM defense provides a certification guarantee against jailbreaking attacks, but it relies on a strict `k-unstable' assumption that rarely holds in practice. This strong assumption can limit the trustworthiness of the provided safety certificate. In this work, we address this limitation by introducing a more realistic probabilistic framework, `(k, $\\varepsilon$)-unstable,' to certify defenses against diverse jailbreaking attacks, from gradient-based (GCG) to semantic (PAIR). We derive a new, data-informed lower bound on SmoothLLM's defense probability by incorporating empirical models of attack success, providing a more trustworthy and practical safety certificate. By introducing the notion of (k, $\\varepsilon$)-unstable, our framework provides practitioners with actionable safety guarantees, enabling them to set certification thresholds that better reflect the real-world behavior of LLMs. Ultimately, this work contributes a practical and theoretically-grounded mechanism to make LLMs more resistant to the exploitation of their safety alignments, a critical challenge in secure AI deployment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 112,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18727",
        "abs_url": "https://arxiv.org/abs/2511.18727",
        "pdf_url": "https://arxiv.org/pdf/2511.18727",
        "title": "LogSyn: A Few-Shot LLM Framework for Structured Insight Extraction from Unstructured General Aviation Maintenance Logs",
        "authors": [
            "Devansh Agarwal",
            "Maitreyi Chatterjee",
            "Biplab Chatterjee"
        ],
        "comments": "Accepted in Proceedings of the 3rd INCOM 2026",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Aircraft maintenance logs hold valuable safety data but remain underused due to their unstructured text format. This paper introduces LogSyn, a framework that uses Large Language Models (LLMs) to convert these logs into structured, machine-readable data. Using few-shot in-context learning on 6,169 records, LogSyn performs Controlled Abstraction Generation (CAG) to summarize problem-resolution narratives and classify events within a detailed hierarchical ontology. The framework identifies key failure patterns, offering a scalable method for semantic structuring and actionable insight extraction from maintenance logs. This work provides a practical path to improve maintenance workflows and predictive analytics in aviation and related industries.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 113,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18728",
        "abs_url": "https://arxiv.org/abs/2511.18728",
        "pdf_url": "https://arxiv.org/pdf/2511.18728",
        "title": "Reinforcement Learning for Self-Healing Material Systems",
        "authors": [
            "Maitreyi Chatterjee",
            "Devansh Agarwal",
            "Biplab Chatterjee"
        ],
        "comments": "Accepted to INCOM 2026. This is the camera-ready version",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The transition to autonomous material systems necessitates adaptive control methodologies to maximize structural longevity. This study frames the self-healing process as a Reinforcement Learning (RL) problem within a Markov Decision Process (MDP), enabling agents to autonomously derive optimal policies that efficiently balance structural integrity maintenance against finite resource consumption. A comparative evaluation of discrete-action (Q-learning, DQN) and continuous-action (TD3) agents in a stochastic simulation environment revealed that RL controllers significantly outperform heuristic baselines, achieving near-complete material recovery. Crucially, the TD3 agent utilizing continuous dosage control demonstrated superior convergence speed and stability, underscoring the necessity of fine-grained, proportional actuation in dynamic self-healing applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 114,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18730",
        "abs_url": "https://arxiv.org/abs/2511.18730",
        "pdf_url": "https://arxiv.org/pdf/2511.18730",
        "title": "Large-Scale In-Game Outcome Forecasting for Match, Team and Players in Football using an Axial Transformer Neural Network",
        "authors": [
            "Michael Horton",
            "Patrick Lucey"
        ],
        "comments": "25 pages, 7 figures, 1 table",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Football (soccer) is a sport that is characterised by complex game play, where players perform a variety of actions, such as passes, shots, tackles, fouls, in order to score goals, and ultimately win matches. Accurately forecasting the total number of each action that each player will complete during a match is desirable for a variety of applications, including tactical decision-making, sports betting, and for television broadcast commentary and analysis. Such predictions must consider the game state, the ability and skill of the players in both teams, the interactions between the players, and the temporal dynamics of the game as it develops. In this paper, we present a transformer-based neural network that jointly and recurrently predicts the expected totals for thirteen individual actions at multiple time-steps during the match, and where predictions are made for each individual player, each team and at the game-level. The neural network is based on an \\emph{axial transformer} that efficiently captures the temporal dynamics as the game progresses, and the interactions between the players at each time-step. We present a novel axial transformer design that we show is equivalent to a regular sequential transformer, and the design performs well experimentally. We show empirically that the model can make consistent and reliable predictions, and efficiently makes $\\sim$75,000 live predictions at low latency for each game.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 115,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18732",
        "abs_url": "https://arxiv.org/abs/2511.18732",
        "pdf_url": "https://arxiv.org/pdf/2511.18732",
        "title": "OceanForecastBench: A Benchmark Dataset for Data-Driven Global Ocean Forecasting",
        "authors": [
            "Haoming Jia",
            "Yi Han",
            "Xiang Wang",
            "Huizan Wang",
            "Wei Wu",
            "Jianming Zheng",
            "Peikun Xiao"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Global ocean forecasting aims to predict key ocean variables such as temperature, salinity, and currents, which is essential for understanding and describing oceanic phenomena. In recent years, data-driven deep learning-based ocean forecast models, such as XiHe, WenHai, LangYa and AI-GOMS, have demonstrated significant potential in capturing complex ocean dynamics and improving forecasting efficiency. Despite these advancements, the absence of open-source, standardized benchmarks has led to inconsistent data usage and evaluation methods. This gap hinders efficient model development, impedes fair performance comparison, and constrains interdisciplinary collaboration. To address this challenge, we propose OceanForecastBench, a benchmark offering three core contributions: (1) A high-quality global ocean reanalysis data over 28 years for model training, including 4 ocean variables across 23 depth levels and 4 sea surface variables. (2) A high-reliability satellite and in-situ observations for model evaluation, covering approximately 100 million locations in the global ocean. (3) An evaluation pipeline and a comprehensive benchmark with 6 typical baseline models, leveraging observations to evaluate model performance from multiple perspectives. OceanForecastBench represents the most comprehensive benchmarking framework currently available for data-driven ocean forecasting, offering an open-source platform for model development, evaluation, and comparison. The dataset and code are publicly available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 116,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18777",
        "abs_url": "https://arxiv.org/abs/2511.18777",
        "pdf_url": "https://arxiv.org/pdf/2511.18777",
        "title": "SAOT: An Enhanced Locality-Aware Spectral Transformer for Solving PDEs",
        "authors": [
            "Chenhong Zhou",
            "Jie Chen",
            "Zaifeng Yang"
        ],
        "comments": "Accepted to AAAI 2026 (Main Technical Track)",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Neural operators have shown great potential in solving a family of Partial Differential Equations (PDEs) by modeling the mappings between input and output functions. Fourier Neural Operator (FNO) implements global convolutions via parameterizing the integral operators in Fourier space. However, it often results in over-smoothing solutions and fails to capture local details and high-frequency components. To address these limitations, we investigate incorporating the spatial-frequency localization property of Wavelet transforms into the Transformer architecture. We propose a novel Wavelet Attention (WA) module with linear computational complexity to efficiently learn locality-aware features. Building upon WA, we further develop the Spectral Attention Operator Transformer (SAOT), a hybrid spectral Transformer framework that integrates WA's localized focus with the global receptive field of Fourier-based Attention (FA) through a gated fusion block. Experimental results demonstrate that WA significantly mitigates the limitations of FA and outperforms existing Wavelet-based neural operators by a large margin. By integrating the locality-aware and global spectral representations, SAOT achieves state-of-the-art performance on six operator learning benchmarks and exhibits strong discretization-invariant ability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 117,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18783",
        "abs_url": "https://arxiv.org/abs/2511.18783",
        "pdf_url": "https://arxiv.org/pdf/2511.18783",
        "title": "Hypergraph Contrastive Learning for both Homophilic and Heterophilic Hypergraphs",
        "authors": [
            "Renchu Guan",
            "Xuyang Li",
            "Yachao Zhang",
            "Wei Pang",
            "Fausto Giunchiglia",
            "Ximing Li",
            "Yonghao Liu",
            "Xiaoyue Feng"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Social and Information Networks (cs.SI)",
        "abstract": "Hypergraphs, as a generalization of traditional graphs, naturally capture high-order relationships. In recent years, hypergraph neural networks (HNNs) have been widely used to capture complex high-order relationships. However, most existing hypergraph neural network methods inherently rely on the homophily assumption, which often does not hold in real-world scenarios that exhibit significant heterophilic structures. To address this limitation, we propose \\textbf{HONOR}, a novel unsupervised \\textbf{H}ypergraph c\\textbf{ON}trastive learning framework suitable for both hom\\textbf{O}philic and hete\\textbf{R}ophilic hypergraphs. Specifically, HONOR explicitly models the heterophilic relationships between hyperedges and nodes through two complementary mechanisms: a prompt-based hyperedge feature construction strategy that maintains global semantic consistency while suppressing local noise, and an adaptive attention aggregation module that dynamically captures the diverse local contributions of nodes to hyperedges. Combined with high-pass filtering, these designs enable HONOR to fully exploit heterophilic connection patterns, yielding more discriminative and robust node and hyperedge representations. Theoretically, we demonstrate the superior generalization ability and robustness of HONOR. Empirically, extensive experiments further validate that HONOR consistently outperforms state-of-the-art baselines under both homophilic and heterophilic datasets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 118,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18789",
        "abs_url": "https://arxiv.org/abs/2511.18789",
        "pdf_url": "https://arxiv.org/pdf/2511.18789",
        "title": "Doubly Wild Refitting: Model-Free Evaluation of High Dimensional Black-Box Predictions under Convex Losses",
        "authors": [
            "Haichen Hu",
            "David Simchi-Levi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "We study the problem of excess risk evaluation for empirical risk minimization (ERM) under general convex loss functions. Our contribution is an efficient refitting procedure that computes the excess risk and provides high-probability upper bounds under the fixed-design setting. Assuming only black-box access to the training algorithm and a single dataset, we begin by generating two sets of artificially modified pseudo-outcomes termed wild response, created by stochastically perturbing the gradient vectors with carefully chosen scaling. Using these two pseudo-labeled datasets, we then refit the black-box procedure twice to obtain two corresponding wild predictors. Finally, leveraging the original predictor, the two wild predictors, and the constructed wild responses, we derive an efficient excess risk upper bound. A key feature of our analysis is that it requires no prior knowledge of the complexity of the underlying function class. As a result, the method is essentially model-free and holds significant promise for theoretically evaluating modern opaque machine learning system--such as deep nerral networks and generative model--where traditional capacity-based learning theory becomes infeasible due to the extreme complexity of the hypothesis class.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 119,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18829",
        "abs_url": "https://arxiv.org/abs/2511.18829",
        "pdf_url": "https://arxiv.org/pdf/2511.18829",
        "title": "Towards Characterizing Knowledge Distillation of PPG Heart Rate Estimation Models",
        "authors": [
            "Kanav Arora",
            "Girish Narayanswamy",
            "Shwetak Patel",
            "Richard Li"
        ],
        "comments": "To be published in: 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Learning from Time Series for Health",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Heart rate estimation from photoplethysmography (PPG) signals generated by wearable devices such as smartwatches and fitness trackers has significant implications for the health and well-being of individuals. Although prior work has demonstrated deep learning models with strong performance in the heart rate estimation task, in order to deploy these models on wearable devices, these models must also adhere to strict memory and latency constraints. In this work, we explore and characterize how large pre-trained PPG models may be distilled to smaller models appropriate for real-time inference on the edge. We evaluate four distillation strategies through comprehensive sweeps of teacher and student model capacities: (1) hard distillation, (2) soft distillation, (3) decoupled knowledge distillation (DKD), and (4) feature distillation. We present a characterization of the resulting scaling laws describing the relationship between model size and performance. This early investigation lays the groundwork for practical and predictable methods for building edge-deployable models for physiological sensing.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 120,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18830",
        "abs_url": "https://arxiv.org/abs/2511.18830",
        "pdf_url": "https://arxiv.org/pdf/2511.18830",
        "title": "Leveraging Duration Pseudo-Embeddings in Multilevel LSTM and GCN Hypermodels for Outcome-Oriented PPM",
        "authors": [
            "Fang Wang",
            "Paolo Ceravolo",
            "Ernesto Damiani"
        ],
        "comments": "12 pages",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Existing deep learning models for Predictive Process Monitoring (PPM) struggle with temporal irregularities, particularly stochastic event durations and overlapping timestamps, limiting their adaptability across heterogeneous datasets. We propose a dual input neural network strategy that separates event and sequence attributes, using a duration-aware pseudo-embedding matrix to transform temporal importance into compact, learnable representations. This design is implemented across two baseline families: B-LSTM and B-GCN, and their duration-aware variants D-LSTM and D-GCN. All models incorporate self-tuned hypermodels for adaptive architecture selection. Experiments on balanced and imbalanced outcome prediction tasks show that duration pseudo-embedding inputs consistently improve generalization, reduce model complexity, and enhance interpretability. Our results demonstrate the benefits of explicit temporal encoding and provide a flexible design for robust, real-world PPM applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 121,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18835",
        "abs_url": "https://arxiv.org/abs/2511.18835",
        "pdf_url": "https://arxiv.org/pdf/2511.18835",
        "title": "Auto-ML Graph Neural Network Hypermodels for Outcome Prediction in Event-Sequence Data",
        "authors": [
            "Fang Wang",
            "Lance Kosca",
            "Adrienne Kosca",
            "Marko Gacesa",
            "Ernesto Damiani"
        ],
        "comments": "6 pages",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "This paper introduces HGNN(O), an AutoML GNN hypermodel framework for outcome prediction on event-sequence data. Building on our earlier work on graph convolutional network hypermodels, HGNN(O) extends four architectures-One Level, Two Level, Two Level Pseudo Embedding, and Two Level Embedding-across six canonical GNN operators. A self-tuning mechanism based on Bayesian optimization with pruning and early stopping enables efficient adaptation over architectures and hyperparameters without manual configuration. Empirical evaluation on both balanced and imbalanced event logs shows that HGNN(O) achieves accuracy exceeding 0.98 on the Traffic Fines dataset and weighted F1 scores up to 0.86 on the Patients dataset without explicit imbalance handling. These results demonstrate that the proposed AutoML-GNN approach provides a robust and generalizable benchmark for outcome prediction in complex event-sequence data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 122,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18846",
        "abs_url": "https://arxiv.org/abs/2511.18846",
        "pdf_url": "https://arxiv.org/pdf/2511.18846",
        "title": "WaveTuner: Comprehensive Wavelet Subband Tuning for Time Series Forecasting",
        "authors": [
            "Yubo Wang",
            "Hui He",
            "Chaoxi Niu",
            "Zhendong Niu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Due to the inherent complexity, temporal patterns in real-world time series often evolve across multiple intertwined scales, including long-term periodicity, short-term fluctuations, and abrupt regime shifts. While existing literature has designed many sophisticated decomposition approaches based on the time or frequency domain to partition trend-seasonality components and high-low frequency components, an alternative line of approaches based on the wavelet domain has been proposed to provide a unified multi-resolution representation with precise time-frequency localization. However, most wavelet-based methods suffer from a persistent bias toward recursively decomposing only low-frequency components, severely underutilizing subtle yet informative high-frequency components that are pivotal for precise time series forecasting. To address this problem, we propose WaveTuner, a Wavelet decomposition framework empowered by full-spectrum subband Tuning for time series forecasting. Concretely, WaveTuner comprises two key modules: (i) Adaptive Wavelet Refinement module, that transforms time series into time-frequency coefficients, utilizes an adaptive router to dynamically assign subband weights, and generates subband-specific embeddings to support refinement; and (ii) Multi-Branch Specialization module, that employs multiple functional branches, each instantiated as a flexible Kolmogorov-Arnold Network (KAN) with a distinct functional order to model a specific spectral subband. Equipped with these modules, WaveTuner comprehensively tunes global trends and local variations within a unified time-frequency framework. Extensive experiments on eight real-world datasets demonstrate WaveTuner achieves state-of-the-art forecasting performance in time series forecasting.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 123,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18868",
        "abs_url": "https://arxiv.org/abs/2511.18868",
        "pdf_url": "https://arxiv.org/pdf/2511.18868",
        "title": "KernelBand: Boosting LLM-based Kernel Optimization with a Hierarchical and Hardware-aware Multi-armed Bandit",
        "authors": [
            "Dezhi Ran",
            "Shuxiao Xie",
            "Mingfang Ji",
            "Ziyue Hua",
            "Mengzhou Wu",
            "Yuan Cao",
            "Yuzhe Guo",
            "Yu Hao",
            "Linyi Li",
            "Yitao Hu",
            "Tao Xie"
        ],
        "comments": "Work in progress",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "High quality kernels are critical for reducing training and inference costs of Large Language Models (LLMs), yet they traditionally require significant expertise in hardware architecture and software optimization. While recent advances in LLM-based code generation show promise for complex optimization, existing methods struggle with the vast optimization space due to insufficient hardware domain knowledge, failing to effectively balance exploration and exploitation. We present KernelBand, a novel framework that formulates kernel optimization as a hierarchical multi-armed bandit problem, enabling LLM agents to strategically navigate the optimization space by treating kernel selection and optimization strategy application as sequential decision-making processes. Our approach leverages hardware profiling information to identify promising optimization strategies and employs runtime behavior clustering to reduce exploration overhead across kernel candidates. Extensive experiments on TritonBench demonstrate that KernelBand significantly outperforms state-of-the-art methods, achieving superior performance with fewer tokens while exhibiting consistent improvement without saturation as computational resources increase.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 124,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18887",
        "abs_url": "https://arxiv.org/abs/2511.18887",
        "pdf_url": "https://arxiv.org/pdf/2511.18887",
        "title": "Hi-SAFE: Hierarchical Secure Aggregation for Lightweight Federated Learning",
        "authors": [
            "Hyeong-Gun Joo",
            "Songnam Hong",
            "Seunghwan Lee",
            "Dong-Joon Shin"
        ],
        "comments": "currently submitted and awaiting review at the IEEE Internet of Things Journal",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Federated learning (FL) faces challenges in ensuring both privacy and communication efficiency, particularly in resource-constrained environments such as Internet of Things (IoT) and edge networks. While sign-based methods, such as sign stochastic gradient descent with majority voting (SIGNSGD-MV), offer substantial bandwidth savings, they remain vulnerable to inference attacks due to exposure of gradient signs. Existing secure aggregation techniques are either incompatible with sign-based methods or incur prohibitive overhead. To address these limitations, we propose Hi-SAFE, a lightweight and cryptographically secure aggregation framework for sign-based FL. Our core contribution is the construction of efficient majority vote polynomials for SIGNSGD-MV, derived from Fermat's Little Theorem. This formulation represents the majority vote as a low-degree polynomial over a finite field, enabling secure evaluation that hides intermediate values and reveals only the final result. We further introduce a hierarchical subgrouping strategy that ensures constant multiplicative depth and bounded per-user complexity, independent of the number of users n.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 125,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18902",
        "abs_url": "https://arxiv.org/abs/2511.18902",
        "pdf_url": "https://arxiv.org/pdf/2511.18902",
        "title": "VADE: Variance-Aware Dynamic Sampling via Online Sample-Level Difficulty Estimation for Multimodal RL",
        "authors": [
            "Zengjie Hu",
            "Jiantao Qiu",
            "Tianyi Bai",
            "Haojin Yang",
            "Binhang Yuan",
            "Qi Jing",
            "Conghui He",
            "Wentao Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Group-based policy optimization methods like GRPO and GSPO have become standard for training multimodal models, leveraging group-wise rollouts and relative advantage estimation. However, they suffer from a critical \\emph{gradient vanishing} problem when all responses within a group receive identical rewards, causing advantage estimates to collapse and training signals to diminish. Existing attempts to mitigate this issue fall into two paradigms: filtering-based and sampling-based methods. Filtering-based methods first generate rollouts broadly and then retroactively filter out uninformative groups, leading to substantial computational overhead. Sampling-based methods proactively select effective samples before rollout but rely on static criteria or prior dataset knowledge, lacking real-time adaptability. To address these issues, we propose \\textbf{VADE}, a \\textbf{V}ariance-\\textbf{A}ware \\textbf{D}ynamic sampling framework via online sample-level difficulty \\textbf{E}stimation. Our framework integrates three key components: online sample-level difficulty estimation using Beta distributions, a Thompson sampler that maximizes information gain through the estimated correctness probability, and a two-scale prior decay mechanism that maintains robust estimation under policy evolution. This three components design enables VADE to dynamically select the most informative samples, thereby amplifying training signals while eliminating extra rollout costs. Extensive experiments on multimodal reasoning benchmarks show that VADE consistently outperforms strong baselines in both performance and sample efficiency, while achieving a dramatic reduction in computational overhead. More importantly, our framework can serves as a plug-and-play component to be seamlessly integrated into existing group-based RL algorithms. Code and models are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 126,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18930",
        "abs_url": "https://arxiv.org/abs/2511.18930",
        "pdf_url": "https://arxiv.org/pdf/2511.18930",
        "title": "Learning Solution Operators for Partial Differential Equations via Monte Carlo-Type Approximation",
        "authors": [
            "Salah Eddine Choutri",
            "Prajwal Chauhan",
            "Othmane Mazhar",
            "Saif Eddin Jabari"
        ],
        "comments": "NeurIPS 2025 Workshop on Machine Learning and the Physical Sciences",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "The Monte Carlo-type Neural Operator (MCNO) introduces a lightweight architecture for learning solution operators for parametric PDEs by directly approximating the kernel integral using a Monte Carlo approach. Unlike Fourier Neural Operators, MCNO makes no spectral or translation-invariance assumptions. The kernel is represented as a learnable tensor over a fixed set of randomly sampled points. This design enables generalization across multiple grid resolutions without relying on fixed global basis functions or repeated sampling during training. Experiments on standard 1D PDE benchmarks show that MCNO achieves competitive accuracy with low computational cost, providing a simple and practical alternative to spectral and graph-based neural operators.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 127,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18940",
        "abs_url": "https://arxiv.org/abs/2511.18940",
        "pdf_url": "https://arxiv.org/pdf/2511.18940",
        "title": "Geometry-Aware Deep Congruence Networks for Manifold Learning in Cross-Subject Motor Imagery",
        "authors": [
            "Sanjeev Manivannan",
            "Chandrashekar Lakshminarayan"
        ],
        "comments": "10 pages, 2 figures",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Cross-subject motor-imagery decoding remains a major challenge in EEG-based brain-computer interfaces due to strong subject variability and the curved geometry of covariance matrices on the symmetric positive definite (SPD) manifold. We address the zero-shot cross-subject setting, where no target-subject labels or adaptation are allowed, by introducing novel geometry-aware preprocessing modules and deep congruence networks that operate directly on SPD covariance matrices. Our preprocessing modules, DCR and RiFU, extend Riemannian Alignment by improving action separation while reducing subject-specific distortions. We further propose two manifold classifiers, SPD-DCNet and RiFUNet, which use hierarchical congruence transforms to learn discriminative, subject-invariant covariance representations. On the BCI-IV 2a benchmark, our framework improves cross-subject accuracy by 3-4% over the strongest classical baselines, demonstrating the value of geometry-aware transformations for robust EEG decoding.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 128,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18945",
        "abs_url": "https://arxiv.org/abs/2511.18945",
        "pdf_url": "https://arxiv.org/pdf/2511.18945",
        "title": "MIST: Mutual Information Via Supervised Training",
        "authors": [
            "German Gritsai",
            "Megan Richards",
            "Maxime Méloux",
            "Kyunghyun Cho",
            "Maxime Peyrard"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Information Theory (cs.IT)",
        "abstract": "We propose a fully data-driven approach to designing mutual information (MI) estimators. Since any MI estimator is a function of the observed sample from two random variables, we parameterize this function with a neural network (MIST) and train it end-to-end to predict MI values. Training is performed on a large meta-dataset of 625,000 synthetic joint distributions with known ground-truth MI. To handle variable sample sizes and dimensions, we employ a two-dimensional attention scheme ensuring permutation invariance across input samples. To quantify uncertainty, we optimize a quantile regression loss, enabling the estimator to approximate the sampling distribution of MI rather than return a single point estimate. This research program departs from prior work by taking a fully empirical route, trading universal theoretical guarantees for flexibility and efficiency. Empirically, the learned estimators largely outperform classical baselines across sample sizes and dimensions, including on joint distributions unseen during training. The resulting quantile-based intervals are well-calibrated and more reliable than bootstrap-based confidence intervals, while inference is orders of magnitude faster than existing neural baselines. Beyond immediate empirical gains, this framework yields trainable, fully differentiable estimators that can be embedded into larger learning pipelines. Moreover, exploiting MI's invariance to invertible transformations, meta-datasets can be adapted to arbitrary data modalities via normalizing flows, enabling flexible training for diverse target meta-distributions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 129,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18960",
        "abs_url": "https://arxiv.org/abs/2511.18960",
        "pdf_url": "https://arxiv.org/pdf/2511.18960",
        "title": "AVA-VLA: Improving Vision-Language-Action models with Active Visual Attention",
        "authors": [
            "Lei Xiao",
            "Jifeng Li",
            "Juntao Gao",
            "Feiyang Ye",
            "Yan Jin",
            "Jingjing Qian",
            "Jing Zhang",
            "Yong Wu",
            "Xiaoyuan Yu"
        ],
        "comments": "18 pages, 10 figures",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
        "abstract": "Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in embodied AI tasks. However, existing VLA models, often built upon Vision-Language Models (VLMs), typically process dense visual inputs independently at each timestep. This approach implicitly models the task as a Markov Decision Process (MDP). However, this history-agnostic design is suboptimal for effective visual token processing in dynamic sequential decision-making, as it fails to leverage the context of history. To address this limitation, we reformulate the problem from a Partially Observable Markov Decision Process (POMDP) perspective and propose a novel framework named AVA-VLA. Inspired by the POMDP that the action generation should be conditioned on the belief state. AVA-VLA introduces Active Visual Attention (AVA) to dynamically modulate visual processing. It achieves this by leveraging the recurrent state, which is a neural approximation of the agent's belief state derived from the previous decision step. Specifically, the AVA module uses the recurrent state to compute the soft weights to actively process task-relevant visual tokens based on its historical context. Comprehensive evaluations demonstrate that AVA-VLA achieves state-of-the-art performance across popular robotic benchmarks, including LIBERO and CALVIN. Furthermore, real-world deployments on a dual-arm robot platform validate the framework's practical applicability and robust sim-to-real transferability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 130,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18987",
        "abs_url": "https://arxiv.org/abs/2511.18987",
        "pdf_url": "https://arxiv.org/pdf/2511.18987",
        "title": "Dynamic Mixture of Experts Against Severe Distribution Shifts",
        "authors": [
            "Donghu Kim"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "The challenge of building neural networks that can continuously learn and adapt to evolving data streams is central to the fields of continual learning (CL) and reinforcement learning (RL). This lifelong learning problem is often framed in terms of the plasticity-stability dilemma, focusing on issues like loss of plasticity and catastrophic forgetting. Unlike neural networks, biological brains maintain plasticity through capacity growth, inspiring researchers to explore similar approaches in artificial networks, such as adding capacity dynamically. Prior solutions often lack parameter efficiency or depend on explicit task indices, but Mixture-of-Experts (MoE) architectures offer a promising alternative by specializing experts for distinct distributions. This paper aims to evaluate a DynamicMoE approach for continual and reinforcement learning environments and benchmark its effectiveness against existing network expansion methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 131,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19019",
        "abs_url": "https://arxiv.org/abs/2511.19019",
        "pdf_url": "https://arxiv.org/pdf/2511.19019",
        "title": "3D Dynamic Radio Map Prediction Using Vision Transformers for Low-Altitude Wireless Networks",
        "authors": [
            "Nguyen Duc Minh Quang",
            "Chang Liu",
            "Huy-Trung Nguyen",
            "Shuangyang Li",
            "Derrick Wing Kwan Ng",
            "Wei Xiang"
        ],
        "comments": "7 pages, 4 figures, submitted to IEEE ICC 2026",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Low-altitude wireless networks (LAWN) are rapidly expanding with the growing deployment of unmanned aerial vehicles (UAVs) for logistics, surveillance, and emergency response. Reliable connectivity remains a critical yet challenging task due to three-dimensional (3D) mobility, time-varying user density, and limited power budgets. The transmit power of base stations (BSs) fluctuates dynamically according to user locations and traffic demands, leading to a highly non-stationary 3D radio environment. Radio maps (RMs) have emerged as an effective means to characterize spatial power distributions and support radio-aware network optimization. However, most existing works construct static or offline RMs, overlooking real-time power variations and spatio-temporal dependencies in multi-UAV networks. To overcome this limitation, we propose a {3D dynamic radio map (3D-DRM)} framework that learns and predicts the spatio-temporal evolution of received power. Specially, a Vision Transformer (ViT) encoder extracts high-dimensional spatial representations from 3D RMs, while a Transformer-based module models sequential dependencies to predict future power distributions. Experiments unveil that 3D-DRM accurately captures fast-varying power dynamics and substantially outperforms baseline models in both RM reconstruction and short-term prediction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 132,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19037",
        "abs_url": "https://arxiv.org/abs/2511.19037",
        "pdf_url": "https://arxiv.org/pdf/2511.19037",
        "title": "Resolving Node Identifiability in Graph Neural Processes via Laplacian Spectral Encodings",
        "authors": [
            "Zimo Yan",
            "Zheng Xie",
            "Chang Liu",
            "Yuan Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Probability (math.PR)",
        "abstract": "Message passing graph neural networks are widely used for learning on graphs, yet their expressive power is limited by the one-dimensional Weisfeiler-Lehman test and can fail to distinguish structurally different nodes. We provide rigorous theory for a Laplacian positional encoding that is invariant to eigenvector sign flips and to basis rotations within eigenspaces. We prove that this encoding yields node identifiability from a constant number of observations and establishes a sample-complexity separation from architectures constrained by the Weisfeiler-Lehman test. The analysis combines a monotone link between shortest-path and diffusion distance, spectral trilateration with a constant set of anchors, and quantitative spectral injectivity with logarithmic embedding size. As an instantiation, pairing this encoding with a neural-process style decoder yields significant gains on a drug-drug interaction task on chemical graphs, improving both the area under the ROC curve and the F1 score and demonstrating the practical benefits of resolving theoretical expressiveness limitations with principled positional information.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 133,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19066",
        "abs_url": "https://arxiv.org/abs/2511.19066",
        "pdf_url": "https://arxiv.org/pdf/2511.19066",
        "title": "Mitigating Participation Imbalance Bias in Asynchronous Federated Learning",
        "authors": [
            "Xiangyu Chang",
            "Manyi Yao",
            "Srikanth V. Krishnamurthy",
            "Christian R. Shelton",
            "Anirban Chakraborty",
            "Ananthram Swami",
            "Samet Oymak",
            "Amit Roy-Chowdhury"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "In Asynchronous Federated Learning (AFL), the central server immediately updates the global model with each arriving client's contribution. As a result, clients perform their local training on different model versions, causing information staleness (delay). In federated environments with non-IID local data distributions, this asynchronous pattern amplifies the adverse effect of client heterogeneity (due to different data distribution, local objectives, etc.), as faster clients contribute more frequent updates, biasing the global model. We term this phenomenon heterogeneity amplification. Our work provides a theoretical analysis that maps AFL design choices to their resulting error sources when heterogeneity amplification occurs. Guided by our analysis, we propose ACE (All-Client Engagement AFL), which mitigates participation imbalance through immediate, non-buffered updates that use the latest information available from all clients. We also introduce a delay-aware variant, ACED, to balance client diversity against update staleness. Experiments on different models for different tasks across diverse heterogeneity and delay settings validate our analysis and demonstrate the robust performance of our approaches.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 134,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19090",
        "abs_url": "https://arxiv.org/abs/2511.19090",
        "pdf_url": "https://arxiv.org/pdf/2511.19090",
        "title": "Optimization of Deep Learning Models for Dynamic Market Behavior Prediction",
        "authors": [
            "Shenghan Zhao",
            "Yuzhen Lin",
            "Ximeng Yang",
            "Qiaochu Lu",
            "Haozhong Xue",
            "Gaozhe Jiang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The advent of financial technology has witnessed a surge in the utilization of deep learning models to anticipate consumer conduct, a trend that has demonstrated considerable potential in enhancing lending strategies and bolstering market efficiency. We study multi-horizon demand forecasting on e-commerce transactions using the UCI Online Retail II dataset. Unlike prior versions of this manuscript that mixed financial-loan narratives with retail data, we focus exclusively on retail market behavior and define a clear prediction target: per SKU daily demand (or revenue) for horizons H=1,7,14. We present a hybrid sequence model that combines multi-scale temporal convolutions, a gated recurrent module, and time-aware self-attention. The model is trained with standard regression losses and evaluated under MAE, RMSE, sMAPE, MASE, and Theil's U_2 with strict time-based splits to prevent leakage. We benchmark against ARIMA/Prophet, LSTM/GRU, LightGBM, and state-of-the-art Transformer forecasters (TFT, Informer, Autoformer, N-BEATS). Results show consistent accuracy gains and improved robustness on peak/holiday periods. We further provide ablations and statistical significance tests to ensure the reliability of improvements, and we release implementation details to facilitate reproducibility.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 135,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19103",
        "abs_url": "https://arxiv.org/abs/2511.19103",
        "pdf_url": "https://arxiv.org/pdf/2511.19103",
        "title": "Edge-Based Predictive Data Reduction for Smart Agriculture: A Lightweight Approach to Efficient IoT Communication",
        "authors": [
            "Dora Krekovic",
            "Mario Kusek",
            "Ivana Podnar Zarko",
            "Danh Le-Phuoc"
        ],
        "comments": "Accepted for presentation and publication in the proceedings of the IEEE Annual Congress on Artificial Intelligence of Things (IEEE AIoT 2025)",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The rapid growth of IoT devices has led to an enormous amount of sensor data that requires transmission to cloud servers for processing, resulting in excessive network congestion, increased latency and high energy consumption. This is particularly problematic in resource-constrained and remote environments where bandwidth is limited, and battery-dependent devices further emphasize the problem. Moreover, in domains such as agriculture, consecutive sensor readings often have minimal variation, making continuous data transmission inefficient and unnecessarily resource intensive. To overcome these challenges, we propose an analytical prediction algorithm designed for edge computing environments and validated through simulation. The proposed solution utilizes a predictive filter at the network edge that forecasts the next sensor data point and triggers data transmission only when the deviation from the predicted value exceeds a predefined tolerance. A complementary cloud-based model ensures data integrity and overall system consistency. This dual-model strategy effectively reduces communication overhead and demonstrates potential for improving energy efficiency by minimizing redundant transmissions. In addition to reducing communication load, our approach leverages both in situ and satellite observations from the same locations to enhance model robustness. It also supports cross-site generalization, enabling models trained in one region to be effectively deployed elsewhere without retraining. This makes our solution highly scalable, energy-aware, and well-suited for optimizing sensor data transmission in remote and bandwidth-constrained IoT environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 136,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19124",
        "abs_url": "https://arxiv.org/abs/2511.19124",
        "pdf_url": "https://arxiv.org/pdf/2511.19124",
        "title": "Uncertainty-Aware Deep Learning Framework for Remaining Useful Life Prediction in Turbofan Engines with Learned Aleatoric Uncertainty",
        "authors": [
            "Krishang Sharma"
        ],
        "comments": "10 pages, 2 figures, 3 tables. Submitted to arXiv",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Accurate Remaining Useful Life (RUL) prediction coupled with uncertainty quantification remains a critical challenge in aerospace prognostics. This research introduces a novel uncertainty-aware deep learning framework that learns aleatoric uncertainty directly through probabilistic modeling, an approach unexplored in existing CMAPSS-based literature. Our hierarchical architecture integrates multi-scale Inception blocks for temporal pattern extraction, bidirectional Long Short-Term Memory networks for sequential modeling, and a dual-level attention mechanism operating simultaneously on sensor and temporal dimensions. The innovation lies in the Bayesian output layer that predicts both mean RUL and variance, enabling the model to learn data-inherent uncertainty. Comprehensive preprocessing employs condition-aware clustering, wavelet denoising, and intelligent feature selection. Experimental validation on NASA CMAPSS benchmarks (FD001-FD004) demonstrates competitive overall performance with RMSE values of 16.22, 19.29, 16.84, and 19.98 respectively. Remarkably, our framework achieves breakthrough critical zone performance (RUL <= 30 cycles) with RMSE of 5.14, 6.89, 5.27, and 7.16, representing 25-40 percent improvements over conventional approaches and establishing new benchmarks for safety-critical predictions. The learned uncertainty provides well-calibrated 95 percent confidence intervals with coverage ranging from 93.5 percent to 95.2 percent, enabling risk-aware maintenance scheduling previously unattainable in CMAPSS literature.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 137,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19152",
        "abs_url": "https://arxiv.org/abs/2511.19152",
        "pdf_url": "https://arxiv.org/pdf/2511.19152",
        "title": "Masked Diffusion Models are Secretly Learned-Order Autoregressive Models",
        "authors": [
            "Prateek Garg",
            "Bhavya Kohli",
            "Sunita Sarawagi"
        ],
        "comments": "Accepted at EurIPS 2025 Workshop on Principles of Generative Modeling (PriGM)",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Masked Diffusion Models (MDMs) have emerged as one of the most promising paradigms for generative modeling over discrete domains. It is known that MDMs effectively train to decode tokens in a random order, and that this ordering has significant performance implications in practice. This observation raises a fundamental question: can we design a training framework that optimizes for a favorable decoding order? We answer this in the affirmative, showing that the continuous-time variational objective of MDMs, when equipped with multivariate noise schedules, can identify and optimize for a decoding order during training. We establish a direct correspondence between decoding order and the multivariate noise schedule and show that this setting breaks invariance of the MDM objective to the noise schedule. Furthermore, we prove that the MDM objective decomposes precisely into a weighted auto-regressive losses over these orders, which establishes them as auto-regressive models with learnable orders.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 138,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19165",
        "abs_url": "https://arxiv.org/abs/2511.19165",
        "pdf_url": "https://arxiv.org/pdf/2511.19165",
        "title": "First-order Sobolev Reinforcement Learning",
        "authors": [
            "Fabian Schramm",
            "Nicolas Perrin-Gilbert",
            "Justin Carpentier"
        ],
        "comments": "Workshop paper at Differentiable Systems and Scientific Machine Learning, EurIPS 2025",
        "subjects": "Machine Learning (cs.LG); Robotics (cs.RO)",
        "abstract": "We propose a refinement of temporal-difference learning that enforces first-order Bellman consistency: the learned value function is trained to match not only the Bellman targets in value but also their derivatives with respect to states and actions. By differentiating the Bellman backup through differentiable dynamics, we obtain analytically consistent gradient targets. Incorporating these into the critic objective using a Sobolev-type loss encourages the critic to align with both the value and local geometry of the target function. This first-order TD matching principle can be seamlessly integrated into existing algorithms, such as Q-learning or actor-critic methods (e.g., DDPG, SAC), potentially leading to faster critic convergence and more stable policy gradients without altering their overall structure.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 139,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19168",
        "abs_url": "https://arxiv.org/abs/2511.19168",
        "pdf_url": "https://arxiv.org/pdf/2511.19168",
        "title": "RAVEN++: Pinpointing Fine-Grained Violations in Advertisement Videos with Active Reinforcement Reasoning",
        "authors": [
            "Deyi Ji",
            "Yuekui Yang",
            "Liqun Liu",
            "Peng Shu",
            "Haiyang Wu",
            "Shaogang Tang",
            "Xudong Chen",
            "Shaoping Ma",
            "Tianrun Chen",
            "Lanyun Zhu"
        ],
        "comments": "EMNLP 2025 (Oral, Industry Track)",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
        "abstract": "Advertising (Ad) is a cornerstone of the digital economy, yet the moderation of video advertisements remains a significant challenge due to their complexity and the need for precise violation localization. While recent advancements, such as the RAVEN model, have improved coarse-grained violation detection, critical gaps persist in fine-grained understanding, explainability, and generalization. To address these limitations, we propose RAVEN++, a novel framework that introduces three key innovations: 1) Active Reinforcement Learning (RL), which dynamically adapts training to samples of varying difficulty; 2) Fine-Grained Violation Understanding, achieved through hierarchical reward functions and reasoning distillation; and 3) Progressive Multi-Stage Training, which systematically combines knowledge injection, curriculum-based passive RL, and active RL. Extensive experiments on both public and proprietary datasets, on both offline scenarios and online deployed A/B Testing, demonstrate that RAVEN++ outperforms general-purpose LLMs and specialized models like RAVEN in terms of fine-grained violation understanding, reasoning capabilities, and generalization ability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 140,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19176",
        "abs_url": "https://arxiv.org/abs/2511.19176",
        "pdf_url": "https://arxiv.org/pdf/2511.19176",
        "title": "From Raw Features to Effective Embeddings: A Three-Stage Approach for Multimodal Recipe Recommendation",
        "authors": [
            "Jeeho Shin",
            "Kyungho Kim",
            "Kijung Shin"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Information Retrieval (cs.IR)",
        "abstract": "Recipe recommendation has become an essential task in web-based food platforms. A central challenge is effectively leveraging rich multimodal features beyond user-recipe interactions. Our analysis shows that even simple uses of multimodal signals yield competitive performance, suggesting that systematic enhancement of these signals is highly promising. We propose TESMR, a 3-stage framework for recipe recommendation that progressively refines raw multimodal features into effective embeddings through: (1) content-based enhancement using foundation models with multimodal comprehension, (2) relation-based enhancement via message propagation over user-recipe interactions, and (3) learning-based enhancement through contrastive learning with learnable embeddings. Experiments on two real-world datasets show that TESMR outperforms existing methods, achieving 7-15% higher Recall@10.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 141,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19240",
        "abs_url": "https://arxiv.org/abs/2511.19240",
        "pdf_url": "https://arxiv.org/pdf/2511.19240",
        "title": "Empirical Comparison of Forgetting Mechanisms for UCB-based Algorithms on a Data-Driven Simulation Platform",
        "authors": [
            "Minxin Chen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Many real-world bandit problems involve non-stationary reward distributions, where the optimal decision may shift due to evolving environments. However, the performance of some typical Multi-Armed Bandit (MAB) models such as Upper Confidence Bound (UCB) algorithms degrades significantly in non-stationary environments where reward distributions change over time. To address this limitation, this paper introduces and evaluates FDSW-UCB, a novel dual-view algorithm that integrates a discount-based long-term perspective with a sliding-window-based short-term view. A data-driven semi-synthetic simulation platform, built upon the MovieLens-1M and Open Bandit datasets, is developed to test algorithm adaptability under abrupt and gradual drift scenarios. Experimental results demonstrate that a well-configured sliding-window mechanism (SW-UCB) is robust, while the widely used discounting method (D-UCB) suffers from a fundamental learning failure, leading to linear regret. Crucially, the proposed FDSW-UCB, when employing an optimistic aggregation strategy, achieves superior performance in dynamic settings, highlighting that the ensemble strategy itself is a decisive factor for success.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 142,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19253",
        "abs_url": "https://arxiv.org/abs/2511.19253",
        "pdf_url": "https://arxiv.org/pdf/2511.19253",
        "title": "MAESTRO: Multi-Agent Environment Shaping through Task and Reward Optimization",
        "authors": [
            "Boyuan Wu"
        ],
        "comments": "Preprint. 16 pages, 6 figures. Preliminary version; extended experiments and analysis forthcoming",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Cooperative Multi-Agent Reinforcement Learning (MARL) faces two major design bottlenecks: crafting dense reward functions and constructing curricula that avoid local optima in high-dimensional, non-stationary environments. Existing approaches rely on fixed heuristics or use Large Language Models (LLMs) directly in the control loop, which is costly and unsuitable for real-time systems. We propose MAESTRO (Multi-Agent Environment Shaping through Task and Reward Optimization), a framework that moves the LLM outside the execution loop and uses it as an offline training architect. MAESTRO introduces two generative components: (i) a semantic curriculum generator that creates diverse, performance-driven traffic scenarios, and (ii) an automated reward synthesizer that produces executable Python reward functions adapted to evolving curriculum difficulty. These components guide a standard MARL backbone (MADDPG) without increasing inference cost at deployment. We evaluate MAESTRO on large-scale traffic signal control (Hangzhou, 16 intersections) and conduct controlled ablations. Results show that combining LLM-generated curricula with LLM-generated reward shaping yields improved performance and stability. Across four seeds, the full system achieves +4.0% higher mean return (163.26 vs. 156.93) and 2.2% better risk-adjusted performance (Sharpe 1.53 vs. 0.70) over a strong curriculum baseline. These findings highlight LLMs as effective high-level designers for cooperative MARL training.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 143,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19263",
        "abs_url": "https://arxiv.org/abs/2511.19263",
        "pdf_url": "https://arxiv.org/pdf/2511.19263",
        "title": "Solar-GECO: Perovskite Solar Cell Property Prediction with Geometric-Aware Co-Attention",
        "authors": [
            "Lucas Li",
            "Jean-Baptiste Puel",
            "Florence Carton",
            "Dounya Barrit",
            "Jhony H. Giraldo"
        ],
        "comments": "Accepted at the AI for Accelerated Materials Design (AI4Mat) Workshop at NeurIPS 2025. 14 pages, 4 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Perovskite solar cells are promising candidates for next-generation photovoltaics. However, their performance as multi-scale devices is determined by complex interactions between their constituent layers. This creates a vast combinatorial space of possible materials and device architectures, making the conventional experimental-based screening process slow and expensive. Machine learning models try to address this problem, but they only focus on individual material properties or neglect the important geometric information of the perovskite crystal. To address this problem, we propose to predict perovskite solar cell power conversion efficiency with a geometric-aware co-attention (Solar-GECO) model. Solar-GECO combines a geometric graph neural network (GNN) - that directly encodes the atomic structure of the perovskite absorber - with language model embeddings that process the textual strings representing the chemical compounds of the transport layers and other device components. Solar-GECO also integrates a co-attention module to capture intra-layer dependencies and inter-layer interactions, while a probabilistic regression head predicts both power conversion efficiency (PCE) and its associated uncertainty. Solar-GECO achieves state-of-the-art performance, significantly outperforming several baselines, reducing the mean absolute error (MAE) for PCE prediction from 3.066 to 2.936 compared to semantic GNN (the previous state-of-the-art model). Solar-GECO demonstrates that integrating geometric and textual information provides a more powerful and accurate framework for PCE prediction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 144,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19265",
        "abs_url": "https://arxiv.org/abs/2511.19265",
        "pdf_url": "https://arxiv.org/pdf/2511.19265",
        "title": "Unboxing the Black Box: Mechanistic Interpretability for Algorithmic Understanding of Neural Networks",
        "authors": [
            "Bianka Kowalska",
            "Halina Kwaśnicka"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The black box nature of deep neural networks poses a significant challenge for the deployment of transparent and trustworthy artificial intelligence (AI) systems. With the growing presence of AI in society, it becomes increasingly important to develop methods that can explain and interpret the decisions made by these systems. To address this, mechanistic interpretability (MI) emerged as a promising and distinctive research program within the broader field of explainable artificial intelligence (XAI). MI is the process of studying the inner computations of neural networks and translating them into human-understandable algorithms. It encompasses reverse engineering techniques aimed at uncovering the computational algorithms implemented by neural networks. In this article, we propose a unified taxonomy of MI approaches and provide a detailed analysis of key techniques, illustrated with concrete examples and pseudo-code. We contextualize MI within the broader interpretability landscape, comparing its goals, methods, and insights to other strands of XAI. Additionally, we trace the development of MI as a research area, highlighting its conceptual roots and the accelerating pace of recent work. We argue that MI holds significant potential to support a more scientific understanding of machine learning systems -- treating models not only as tools for solving tasks, but also as systems to be studied and understood. We hope to invite new researchers into the field of mechanistic interpretability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 145,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19267",
        "abs_url": "https://arxiv.org/abs/2511.19267",
        "pdf_url": "https://arxiv.org/pdf/2511.19267",
        "title": "Leveraging Spatiotemporal Graph Neural Networks for Multi-Store Sales Forecasting",
        "authors": [
            "Manish Singh",
            "Arpita Dayama"
        ],
        "comments": "6 pages, 4 figures, 1 table",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "This work evaluates the effectiveness of spatiotemporal Graph Neural Networks (GNNs) for multi-store retail sales forecasting and compares their performance against ARIMA, LSTM, and XGBoost baselines. Using weekly sales data from 45 Walmart stores, we construct a relational forecasting framework that models inter-store dependencies through a learned adaptive graph. The proposed STGNN predicts log-differenced sales and reconstructs final values through a residual path, enabling stable training and improved generalisation. Experiments show that STGNN achieves the lowest overall forecasting error, outperforming all baselines in Normalised Total Absolute Error, P90 MAPE, and variance of MAPE across stores. Analysis of the learned adjacency matrix reveals meaningful functional store clusters and high-influence nodes that emerge without geographic metadata. These results demonstrate that relational structure significantly improves forecast quality in interconnected retail environments and establishes STGNNs as a robust modelling choice for multi-store demand prediction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 146,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19269",
        "abs_url": "https://arxiv.org/abs/2511.19269",
        "pdf_url": "https://arxiv.org/pdf/2511.19269",
        "title": "CDLM: Consistency Diffusion Language Models For Faster Sampling",
        "authors": [
            "Minseo Kim",
            "Chenfeng Xu",
            "Coleman Hooper",
            "Harman Singh",
            "Ben Athiwaratkun",
            "Ce Zhang",
            "Kurt Keutzer",
            "Amir Gholami"
        ],
        "comments": "18 pages, 6 figures",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
        "abstract": "Diffusion Language Models (DLMs) offer a promising parallel generation paradigm but suffer from slow inference due to numerous refinement steps and the inability to use standard KV caching. We introduce CDLM (Consistency Diffusion Language Models), a training-based acceleration method that simultaneously tackles both bottlenecks. CDLM integrates consistency modeling to drastically reduce the number of required sampling steps by enabling multi-token finalization. Furthermore, we enforce a block-wise causal attention mask during fine-tuning, making the model fully compatible with KV caching. Experiments show CDLM achieves 3.6x-14.5x lower latency while maintaining competitive accuracy on math and coding tasks. The full training and evaluation code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 147,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19272",
        "abs_url": "https://arxiv.org/abs/2511.19272",
        "pdf_url": "https://arxiv.org/pdf/2511.19272",
        "title": "Tiny-TSM: Efficiently Training a Lightweight SOTA Time Series Foundation Model",
        "authors": [
            "Felix Birkel"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We present Tiny-TSM, a time series foundation model characterized by small scale, economical training, and state-of-the-art performance. It comprises 23M total parameters, trained on a single A100 GPU in less than a week using a new synthetic data generation and data augmentation pipeline (SynthTS). Without any neural architecture search, hyperparameter tuning, or scaling up model size, Tiny-TSM achieves state-of-the-art performance on a wide range of time series benchmark datasets, often outperforming much larger models and even matching the performance of much larger, industrial-scale, likely highly tuned foundation models. Specifically, Tiny-TSM outperforms all other time series foundation models we evaluated on medium- and long-term forecasting tasks under MSE loss, while short-term accuracy is still competitive with state-of-the-art models. We also introduce a causal input normalization scheme that enables time series models to be trained with dense next-token prediction loss, significantly accelerating convergence speed and reducing training time. All experiments were conducted on a single A100 GPU, illustrating the practicality of the proposed approach in a resource-constrained setting.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 148,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19273",
        "abs_url": "https://arxiv.org/abs/2511.19273",
        "pdf_url": "https://arxiv.org/pdf/2511.19273",
        "title": "Scalable Bayesian Network Structure Learning Using Tsetlin Machine to Constrain the Search Space",
        "authors": [
            "Kunal Dumbre",
            "Lei Jiao",
            "Ole-Christoffer Granmo"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The PC algorithm is a widely used method in causal inference for learning the structure of Bayesian networks. Despite its popularity, the PC algorithm suffers from significant time complexity, particularly as the size of the dataset increases, which limits its applicability in large-scale real-world problems. In this study, we propose a novel approach that utilises the Tsetlin Machine (TM) to construct Bayesian structures more efficiently. Our method leverages the most significant literals extracted from the TM and performs conditional independence (CI) tests on these selected literals instead of the full set of variables, resulting in a considerable reduction in computational time. We implemented our approach and compared it with various state-of-the-art methods. Our evaluation includes categorical datasets from the bnlearn repository, such as Munin1, Hepar2. The findings indicate that the proposed TM-based method not only reduces computational complexity but also maintains competitive accuracy in causal discovery, making it a viable alternative to traditional PC algorithm implementations by offering improved efficiency without compromising performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 149,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19277",
        "abs_url": "https://arxiv.org/abs/2511.19277",
        "pdf_url": "https://arxiv.org/pdf/2511.19277",
        "title": "Closing Gaps in Emissions Monitoring with Climate TRACE",
        "authors": [
            "Brittany V. Lancellotti",
            "Jordan M. Malof",
            "Aaron Davitt",
            "Gavin McCormick",
            "Shelby Anderson",
            "Pol Carbó-Mestre",
            "Gary Collins",
            "Verity Crane",
            "Zoheyr Doctor",
            "George Ebri",
            "Kevin Foster",
            "Trey M. Gowdy",
            "Michael Guzzardi",
            "John Heal",
            "Heather Hunter",
            "David Kroodsma",
            "Khandekar Mahammad Galib",
            "Paul J. Markakis",
            "Gavin McDonald",
            "Daniel P. Moore",
            "Eric D. Nguyen",
            "Sabina Parvu",
            "Michael Pekala",
            "Christine D. Piatko",
            "Amy Piscopo",
            "Mark Powell",
            "Krsna Raniga",
            "Elizabeth P. Reilly",
            "Michael Robinette",
            "Ishan Saraswat",
            "Patrick Sicurello",
            "Isabella Söldner-Rembold",
            "Raymond Song",
            "Charlotte Underwood",
            "Kyle Bradbury"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Global greenhouse gas emissions estimates are essential for monitoring and mitigation planning. Yet most datasets lack one or more characteristics that enhance their actionability, such as accuracy, global coverage, high spatial and temporal resolution, and frequent updates. To address these gaps, we present Climate TRACE (this http URL), an open-access platform delivering global emissions estimates with enhanced detail, coverage, and timeliness. Climate TRACE synthesizes existing emissions data, prioritizing accuracy, coverage, and resolution, and fills gaps using sector-specific estimation approaches. The dataset is the first to provide globally comprehensive emissions estimates for individual sources (e.g., individual power plants) for all anthropogenic emitting sectors. The dataset spans January 1, 2021, to the present, with a two-month reporting lag and monthly updates. The open-access platform enables non-technical audiences to engage with detailed emissions datasets for most subnational governments worldwide. Climate TRACE supports data-driven climate action at scales where decisions are made, representing a major breakthrough for emissions accounting and mitigation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 150,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19279",
        "abs_url": "https://arxiv.org/abs/2511.19279",
        "pdf_url": "https://arxiv.org/pdf/2511.19279",
        "title": "MapFormer: Self-Supervised Learning of Cognitive Maps with Input-Dependent Positional Embeddings",
        "authors": [
            "Victor Rambaud",
            "Salvador Mascarenhas",
            "Yair Lakretz"
        ],
        "comments": "19 pages (29 with appendix), 8 figures",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
        "abstract": "A cognitive map is an internal model which encodes the abstract relationships among entities in the world, giving humans and animals the flexibility to adapt to new situations, with a strong out-of-distribution (OOD) generalization that current AI systems still do not possess. To bridge this gap, we introduce MapFormers, new architectures based on Transformer models, which can learn cognitive maps from observational data and perform path integration in parallel, in a self-supervised manner. Cognitive maps are learned in the model by disentangling structural relationships in the inputs from their specific content, a property that can be achieved naturally by updating the positional encoding in Transformers with input-dependent matrices. We developed two variants of MapFormers that unify absolute and relative positional encoding to model episodic (EM) and working memory (WM), respectively. We tested MapFormers on several tasks, including a classic 2D navigation task, showing that our models can learn a cognitive map of the underlying space and generalize OOD (e.g., to longer sequences) with near-perfect performance, unlike current architectures. Together, these results demonstrate the superiority of models designed to learn a cognitive map, and the importance of introducing a structural bias for structure-content disentanglement, which can be achieved in Transformers with input-dependent positional encoding. MapFormers have broad applications in both neuroscience and AI, by explaining the neural mechanisms giving rise to cognitive maps, while allowing these relation models to be learned at scale.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 151,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19328",
        "abs_url": "https://arxiv.org/abs/2511.19328",
        "pdf_url": "https://arxiv.org/pdf/2511.19328",
        "title": "Understanding the Staged Dynamics of Transformers in Learning Latent Structure",
        "authors": [
            "Rohan Saha",
            "Farzane Aminmansour",
            "Alona Fyshe"
        ],
        "comments": "Preprint",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "While transformers can discover latent structure from context, the dynamics of how they acquire different components of the latent structure remain poorly understood. In this work, we use the Alchemy benchmark, to investigate the dynamics of latent structure learning. We train a small decoder-only transformer on three task variants: 1) inferring missing rules from partial contextual information, 2) composing simple rules to solve multi-step sequences, and 3) decomposing complex multi-step examples to infer intermediate steps. By factorizing each task into interpretable events, we show that the model acquires capabilities in discrete stages, first learning the coarse grained rules, before learning the complete latent structure. We also identify a crucial asymmetry, where the model can compose fundamental rules robustly, but struggles to decompose complex examples to discover the fundamental rules. These findings offer new insights into understanding how a transformer model learns latent structures, providing a granular view of how these capabilities evolve during training.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 152,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19330",
        "abs_url": "https://arxiv.org/abs/2511.19330",
        "pdf_url": "https://arxiv.org/pdf/2511.19330",
        "title": "Targeted Manipulation: Slope-Based Attacks on Financial Time-Series Data",
        "authors": [
            "Dominik Luszczynski"
        ],
        "comments": "13 pages, 6 figures, 4 tables, preprint; Total including Appendix: 21 pages, 11 figures, 7 tables",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "A common method of attacking deep learning models is through adversarial attacks, which occur when an attacker specifically modifies the input of a model to produce an incorrect result. Adversarial attacks have been deeply investigated in the image domain; however, there is less research in the time-series domain and very little for forecasting financial data. To address these concerns, this study aims to build upon previous research on adversarial attacks for time-series data by introducing two new slope-based methods aimed to alter the trends of the predicted stock forecast generated by an N-HiTS model. Compared to the normal N-HiTS predictions, the two new slope-based methods, the General Slope Attack and Least-Squares Slope Attack, can manipulate N-HiTS predictions by doubling the slope. These new slope attacks can bypass standard security mechanisms, such as a discriminator that filters real and perturbed inputs, reducing a 4-layered CNN's specificity to 28% and accuracy to 57%. Furthermore, the slope based methods were incorporated into a GAN architecture as a means of generating realistic synthetic data, while simultaneously fooling the model. Finally, this paper also proposes a sample malware designed to inject an adversarial attack in the model inference library, proving that ML-security research should not only focus on making the model safe, but also securing the entire pipeline.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 153,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19344",
        "abs_url": "https://arxiv.org/abs/2511.19344",
        "pdf_url": "https://arxiv.org/pdf/2511.19344",
        "title": "Annotation-Free Class-Incremental Learning",
        "authors": [
            "Hari Chandana Kuchibhotla",
            "K S Ananth",
            "Vineeth N Balasubramanian"
        ],
        "comments": "18 pages, 6 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Despite significant progress in continual learning ranging from architectural novelty to clever strategies for mitigating catastrophic forgetting most existing methods rest on a strong but unrealistic assumption the availability of labeled data throughout the learning process. In real-world scenarios, however, data often arrives sequentially and without annotations, rendering conventional approaches impractical. In this work, we revisit the fundamental assumptions of continual learning and ask: Can current systems adapt when labels are absent and tasks emerge incrementally over time? To this end, we introduce Annotation-Free Class-Incremental Learning (AFCIL), a more realistic and challenging paradigm where unlabeled data arrives continuously, and the learner must incrementally acquire new classes without any supervision. To enable effective learning under AFCIL, we propose CrossWorld CL, a Cross Domain World Guided Continual Learning framework that incorporates external world knowledge as a stable auxiliary source. The method retrieves semantically related ImageNet classes for each downstream category, maps downstream and ImageNet features through a cross domain alignment strategy and finally introduce a novel replay strategy. This design lets the model uncover semantic structure without annotations while keeping earlier knowledge intact. Across four datasets, CrossWorld-CL surpasses CLIP baselines and existing continual and unlabeled learning methods, underscoring the benefit of world knowledge for annotation free continual learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 154,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19350",
        "abs_url": "https://arxiv.org/abs/2511.19350",
        "pdf_url": "https://arxiv.org/pdf/2511.19350",
        "title": "Scalable Parameter-Light Spectral Method for Clustering Short Text Embeddings with a Cohesion-Based Evaluation Metric",
        "authors": [
            "Nikita Neveditsin",
            "Pawan Lingras",
            "Vijay Mago"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
        "abstract": "Clustering short text embeddings is a foundational task in natural language processing, yet remains challenging due to the need to specify the number of clusters in advance. We introduce a scalable spectral method that estimates the number of clusters directly from the structure of the Laplacian eigenspectrum, constructed using cosine similarities and guided by an adaptive sampling strategy. This sampling approach enables our estimator to efficiently scale to large datasets without sacrificing reliability. To support intrinsic evaluation of cluster quality without ground-truth labels, we propose the Cohesion Ratio, a simple and interpretable evaluation metric that quantifies how much intra-cluster similarity exceeds the global similarity background. It has an information-theoretic motivation inspired by mutual information, and in our experiments it correlates closely with extrinsic measures such as normalized mutual information and homogeneity. Extensive experiments on six short-text datasets and four modern embedding models show that standard algorithms like K-Means and HAC, when guided by our estimator, significantly outperform popular parameter-light methods such as HDBSCAN, OPTICS, and Leiden. These results demonstrate the practical value of our spectral estimator and Cohesion Ratio for unsupervised organization and evaluation of short text data. Implementation of our estimator of k and Cohesion Ratio, along with code for reproducing the experiments, is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 155,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19359",
        "abs_url": "https://arxiv.org/abs/2511.19359",
        "pdf_url": "https://arxiv.org/pdf/2511.19359",
        "title": "Enhancing Conformal Prediction via Class Similarity",
        "authors": [
            "Ariel Fargion",
            "Lahav Dabah",
            "Tom Tirer"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Conformal Prediction (CP) has emerged as a powerful statistical framework for high-stakes classification applications. Instead of predicting a single class, CP generates a prediction set, guaranteed to include the true label with a pre-specified probability. The performance of different CP methods is typically assessed by their average prediction set size. In setups where the classes can be partitioned into semantic groups, e.g., diseases that require similar treatment, users can benefit from prediction sets that are not only small on average, but also contain a small number of semantically different groups. This paper begins by addressing this problem and ultimately offers a widely applicable tool for boosting any CP method on any dataset. First, given a class partition, we propose augmenting the CP score function with a term that penalizes predictions with out-of-group errors. We theoretically analyze this strategy and prove its advantages for group-related metrics. Surprisingly, we show mathematically that, for common class partitions, it can also reduce the average set size of any CP score function. Our analysis reveals the class similarity factors behind this improvement and motivates us to propose a model-specific variant, which does not require any human semantic partition and can further reduce the prediction set size. Finally, we present an extensive empirical study, encompassing prominent CP methods, multiple models, and several datasets, which demonstrates that our class-similarity-based approach consistently enhances CP methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 156,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19364",
        "abs_url": "https://arxiv.org/abs/2511.19364",
        "pdf_url": "https://arxiv.org/pdf/2511.19364",
        "title": "Neural surrogates for designing gravitational wave detectors",
        "authors": [
            "Carlos Ruiz-Gonzalez",
            "Sören Arlt",
            "Sebastian Lehner",
            "Arturs Berzins",
            "Yehonathan Drori",
            "Rana X Adhikari",
            "Johannes Brandstetter",
            "Mario Krenn"
        ],
        "comments": "20 pages, 7 figures, 4 tables",
        "subjects": "Machine Learning (cs.LG); Instrumentation and Methods for Astrophysics (astro-ph.IM); General Relativity and Quantum Cosmology (gr-qc); Quantum Physics (quant-ph)",
        "abstract": "Physics simulators are essential in science and engineering, enabling the analysis, control, and design of complex systems. In experimental sciences, they are increasingly used to automate experimental design, often via combinatorial search and optimization. However, as the setups grow more complex, the computational cost of traditional, CPU-based simulators becomes a major limitation. Here, we show how neural surrogate models can significantly reduce reliance on such slow simulators while preserving accuracy. Taking the design of interferometric gravitational wave detectors as a representative example, we train a neural network to surrogate the gravitational wave physics simulator Finesse, which was developed by the LIGO community. Despite that small changes in physical parameters can change the output by orders of magnitudes, the model rapidly predicts the quality and feasibility of candidate designs, allowing an efficient exploration of large design spaces. Our algorithm loops between training the surrogate, inverse designing new experiments, and verifying their properties with the slow simulator for further training. Assisted by auto-differentiation and GPU parallelism, our method proposes high-quality experiments much faster than direct optimization. Solutions that our algorithm finds within hours outperform designs that take five days for the optimizer to reach. Though shown in the context of gravitational wave detectors, our framework is broadly applicable to other domains where simulator bottlenecks hinder optimization and discovery.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 157,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19368",
        "abs_url": "https://arxiv.org/abs/2511.19368",
        "pdf_url": "https://arxiv.org/pdf/2511.19368",
        "title": "LLM-Driven Stationarity-Aware Expert Demonstrations for Multi-Agent Reinforcement Learning in Mobile Systems",
        "authors": [
            "Tianyang Duan",
            "Zongyuan Zhang",
            "Zheng Lin",
            "Songxiao Guo",
            "Xiuxian Guan",
            "Guangyu Wu",
            "Zihan Fang",
            "Haotian Meng",
            "Xia Du",
            "Ji-Zhe Zhou",
            "Heming Cui",
            "Jun Luo",
            "Yue Gao"
        ],
        "comments": "15 pages, 9 figures",
        "subjects": "Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)",
        "abstract": "Multi-agent reinforcement learning (MARL) has been increasingly adopted in many real-world applications. While MARL enables decentralized deployment on resource-constrained edge devices, it suffers from severe non-stationarity due to the synchronous updates of agent policies. This non stationarity results in unstable training and poor policy con vergence, especially as the number of agents increases. In this paper, we propose RELED, a scalable MARL framework that integrates large language model (LLM)-driven expert demonstrations with autonomous agent exploration. RELED incorporates a Stationarity-Aware Expert Demonstration module, which leverages theoretical non-stationarity bounds to enhance the quality of LLM-generated expert trajectories, thus providing high reward and training-stable samples for each agent. Moreover, a Hybrid Expert-Agent Policy Optimization module adaptively balances each agent's learning from both expert-generated and agent-generated trajectories, accelerating policy convergence and improving generalization. Extensive experiments with real city networks based on OpenStreetMap demonstrate that RELED achieves superior performance compared to state-of-the-art MARL methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 158,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19379",
        "abs_url": "https://arxiv.org/abs/2511.19379",
        "pdf_url": "https://arxiv.org/pdf/2511.19379",
        "title": "Efficiency vs. Fidelity: A Comparative Analysis of Diffusion Probabilistic Models and Flow Matching on Low-Resource Hardware",
        "authors": [
            "Srishti Gupta",
            "Yashasvee Taiwade"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Denoising Diffusion Probabilistic Models (DDPMs) have established a new state-of-the-art in generative image synthesis, yet their deployment is hindered by significant computational overhead during inference, often requiring up to 1,000 iterative steps. This study presents a rigorous comparative analysis of DDPMs against the emerging Flow Matching (Rectified Flow) paradigm, specifically isolating their geometric and efficiency properties on low-resource hardware. By implementing both frameworks on a shared Time-Conditioned U-Net backbone using the MNIST dataset, we demonstrate that Flow Matching significantly outperforms Diffusion in efficiency. Our geometric analysis reveals that Flow Matching learns a highly rectified transport path (Curvature $\\mathcal{C} \\approx 1.02$), which is near-optimal, whereas Diffusion trajectories remain stochastic and tortuous ($\\mathcal{C} \\approx 3.45$). Furthermore, we establish an ``efficiency frontier'' at $N=10$ function evaluations, where Flow Matching retains high fidelity while Diffusion collapses. Finally, we show via numerical sensitivity analysis that the learned vector field is sufficiently linear to render high-order ODE solvers (Runge-Kutta 4) unnecessary, validating the use of lightweight Euler solvers for edge deployment. \\textbf{This work concludes that Flow Matching is the superior algorithmic choice for real-time, resource-constrained generative tasks.}",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 159,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19405",
        "abs_url": "https://arxiv.org/abs/2511.19405",
        "pdf_url": "https://arxiv.org/pdf/2511.19405",
        "title": "Learning Robust Social Strategies with Large Language Models",
        "authors": [
            "Dereck Piche",
            "Mohammed Muqeeth",
            "Milad Aghajohari",
            "Juan Duque",
            "Michael Noukhovitch",
            "Aaron Courville"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "As agentic AI becomes more widespread, agents with distinct and possibly conflicting goals will interact in complex ways. These multi-agent interactions pose a fundamental challenge, particularly in social dilemmas, where agents' individual incentives can undermine collective welfare. While reinforcement learning (RL) has been effective for aligning large language models (LLMs) in the single-agent regime, prior small-network results suggest that standard RL in multi-agent settings often converges to defecting, self-interested policies. We show the same effect in LLMs: despite cooperative priors, RL-trained LLM agents develop opportunistic behavior that can exploit even advanced closed-source models. To address this tendency of RL to converge to poor equilibria, we adapt a recent opponent-learning awareness algorithm, Advantage Alignment, to fine-tune LLMs toward multi-agent cooperation and non-exploitability. We then introduce a group-relative baseline that simplifies advantage computation in iterated games, enabling multi-agent training at LLM scale. We also contribute a novel social dilemma environment, Trust and Split, which requires natural language communication to achieve high collective welfare. Across a wide range of social dilemmas, policies learned with Advantage Alignment achieve higher collective payoffs while remaining robust against exploitation by greedy agents.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 160,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19428",
        "abs_url": "https://arxiv.org/abs/2511.19428",
        "pdf_url": "https://arxiv.org/pdf/2511.19428",
        "title": "Flow Map Distillation Without Data",
        "authors": [
            "Shangyuan Tong",
            "Nanye Ma",
            "Saining Xie",
            "Tommi Jaakkola"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "State-of-the-art flow models achieve remarkable quality but require slow, iterative sampling. To accelerate this, flow maps can be distilled from pre-trained teachers, a procedure that conventionally requires sampling from an external dataset. We argue that this data-dependency introduces a fundamental risk of Teacher-Data Mismatch, as a static dataset may provide an incomplete or even misaligned representation of the teacher's full generative capabilities. This leads us to question whether this reliance on data is truly necessary for successful flow map distillation. In this work, we explore a data-free alternative that samples only from the prior distribution, a distribution the teacher is guaranteed to follow by construction, thereby circumventing the mismatch risk entirely. To demonstrate the practical viability of this philosophy, we introduce a principled framework that learns to predict the teacher's sampling path while actively correcting for its own compounding errors to ensure high fidelity. Our approach surpasses all data-based counterparts and establishes a new state-of-the-art by a significant margin. Specifically, distilling from SiT-XL/2+REPA, our method reaches an impressive FID of 1.45 on ImageNet 256x256, and 1.49 on ImageNet 512x512, both with only 1 sampling step. We hope our work establishes a more robust paradigm for accelerating generative models and motivates the broader adoption of flow map distillation without data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 161,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.12135",
        "abs_url": "https://arxiv.org/abs/2511.12135",
        "pdf_url": "https://arxiv.org/pdf/2511.12135",
        "title": "RTMol: Rethinking Molecule-text Alignment in a Round-trip View",
        "authors": [
            "Letian Chen",
            "Runhan Shi",
            "Gufeng Yu",
            "Yang Yang"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Biomolecules (q-bio.BM)",
        "abstract": "Aligning molecular sequence representations (e.g., SMILES notations) with textual descriptions is critical for applications spanning drug discovery, materials design, and automated chemical literature analysis. Existing methodologies typically treat molecular captioning (molecule-to-text) and text-based molecular design (text-to-molecule) as separate tasks, relying on supervised fine-tuning or contrastive learning pipelines. These approaches face three key limitations: (i) conventional metrics like BLEU prioritize linguistic fluency over chemical accuracy, (ii) training datasets frequently contain chemically ambiguous narratives with incomplete specifications, and (iii) independent optimization of generation directions leads to bidirectional inconsistency. To address these issues, we propose RTMol, a bidirectional alignment framework that unifies molecular captioning and text-to-SMILES generation through self-supervised round-trip learning. The framework introduces novel round-trip evaluation metrics and enables unsupervised training for molecular captioning without requiring paired molecule-text corpora. Experiments demonstrate that RTMol enhances bidirectional alignment performance by up to 47% across various LLMs, establishing an effective paradigm for joint molecule-text understanding and generation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 162,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17505",
        "abs_url": "https://arxiv.org/abs/2511.17505",
        "pdf_url": "https://arxiv.org/pdf/2511.17505",
        "title": "Causal Intervention Sequence Analysis for Fault Tracking in Radio Access Networks",
        "authors": [
            "Chenhua Shi",
            "Joji Philip",
            "Subhadip Bandyopadhyay",
            "Jayanta Choudhury"
        ],
        "comments": "6 pages, 7 figures, 1 table, accepted by IEEE FMLDS 2025",
        "subjects": "Networking and Internet Architecture (cs.NI); Machine Learning (cs.LG)",
        "abstract": "To keep modern Radio Access Networks (RAN) running smoothly, operators need to spot the real-world triggers behind Service-Level Agreement (SLA) breaches well before customers feel them. We introduce an AI/ML pipeline that does two things most tools miss: (1) finds the likely root-cause indicators and (2) reveals the exact order in which those events unfold. We start by labeling network data: records linked to past SLA breaches are marked `abnormal', and everything else `normal'. Our model then learns the causal chain that turns normal behavior into a fault. In Monte Carlo tests the approach pinpoints the correct trigger sequence with high precision and scales to millions of data points without loss of speed. These results show that high-resolution, causally ordered insights can move fault management from reactive troubleshooting to proactive prevention.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 163,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17523",
        "abs_url": "https://arxiv.org/abs/2511.17523",
        "pdf_url": "https://arxiv.org/pdf/2511.17523",
        "title": "DyPBP: Dynamic Peer Beneficialness Prediction for Cryptocurrency P2P Networking",
        "authors": [
            "Nazmus Sakib",
            "Simeon Wuthier",
            "Amanul Islam",
            "Xiaobo Zhou",
            "Jinoh Kim",
            "Ikkyun Kim",
            "Sang-Yoon Chang"
        ],
        "comments": "",
        "subjects": "Networking and Internet Architecture (cs.NI); Machine Learning (cs.LG)",
        "abstract": "Distributed peer-to-peer (P2P) networking delivers the new blocks and transactions and is critical for the cryptocurrency blockchain system operations. Having poor P2P connectivity reduces the financial rewards from the mining consensus protocol. Previous research defines beneficalness of each Bitcoin peer connection and estimates the beneficialness based on the observations of the blocks and transactions delivery, which are after they are delivered. However, due to the infrequent block arrivals and the sporadic and unstable peer connections, the peers do not stay connected long enough to have the beneficialness score to converge to its expected beneficialness. We design and build Dynamic Peer Beneficialness Prediction (DyPBP) which predicts a peer's beneficialness by using networking behavior observations beyond just the block and transaction arrivals. DyPBP advances the previous research by estimating the beneficialness of a peer connection before it delivers new blocks and transactions. To achieve such goal, DyPBP introduces a new feature for remembrance to address the dynamic connectivity issue, as Bitcoin's peers using distributed networking often disconnect and re-connect. We implement DyPBP on an active Bitcoin node connected to the Mainnet and use machine learning for the beneficialness prediction. Our experimental results validate and evaluate the effectiveness of DyPBP; for example, the error performance improves by 2 to 13 orders of magnitude depending on the machine-learning model selection. DyPBP's use of the remembrance feature also informs our model selection. DyPBP enables the P2P connection's beneficialness estimation from the connection start before a new block arrives.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 164,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17531",
        "abs_url": "https://arxiv.org/abs/2511.17531",
        "pdf_url": "https://arxiv.org/pdf/2511.17531",
        "title": "Q-Learning-Based Time-Critical Data Aggregation Scheduling in IoT",
        "authors": [
            "Van-Vi Vo",
            "Tien-Dung Nguyen",
            "Duc-Tai Le",
            "Hyunseung Choo"
        ],
        "comments": "7 pages, 6 figures",
        "subjects": "Networking and Internet Architecture (cs.NI); Machine Learning (cs.LG)",
        "abstract": "Time-critical data aggregation in Internet of Things (IoT) networks demands efficient, collision-free scheduling to minimize latency for applications like smart cities and industrial automation. Traditional heuristic methods, with two-phase tree construction and scheduling, often suffer from high computational overhead and suboptimal delays due to their static nature. To address this, we propose a novel Q-learning framework that unifies aggregation tree construction and scheduling, modeling the process as a Markov Decision Process (MDP) with hashed states for scalability. By leveraging a reward function that promotes large, interference-free batch transmissions, our approach dynamically learns optimal scheduling policies. Simulations on static networks with up to 300 nodes demonstrate up to 10.87% lower latency compared to a state-of-the-art heuristic algorithm, highlighting its robustness for delay-sensitive IoT applications. This framework enables timely insights in IoT environments, paving the way for scalable, low-latency data aggregation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 165,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17550",
        "abs_url": "https://arxiv.org/abs/2511.17550",
        "pdf_url": "https://arxiv.org/pdf/2511.17550",
        "title": "Gate-level boolean evolutionary geometric attention neural networks",
        "authors": [
            "Xianshuai Shi",
            "Jianfeng Zhu",
            "Leibo Liu"
        ],
        "comments": "",
        "subjects": "Neural and Evolutionary Computing (cs.NE); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "This paper presents a gate-level Boolean evolutionary geometric attention neural network that models images as Boolean fields governed by logic gates. Each pixel is a Boolean variable (0 or 1) embedded on a two-dimensional geometric manifold (for example, a discrete toroidal lattice), which defines adjacency and information propagation among pixels. The network updates image states through a Boolean reaction-diffusion mechanism: pixels receive Boolean diffusion from neighboring pixels (diffusion process) and perform local logic updates via trainable gate-level logic kernels (reaction process), forming a reaction-diffusion logic network. A Boolean self-attention mechanism is introduced, using XNOR-based Boolean Query-Key (Q-K) attention to modulate neighborhood diffusion pathways and realize logic attention. We also propose Boolean Rotary Position Embedding (RoPE), which encodes relative distances by parity-bit flipping to simulate Boolean ``phase'' offsets. The overall structure resembles a Transformer but operates entirely in the Boolean domain. Trainable parameters include Q-K pattern bits and gate-level kernel configurations. Because outputs are discrete, continuous relaxation methods (such as sigmoid approximation or soft-logic operators) ensure differentiable training. Theoretical analysis shows that the network achieves universal expressivity, interpretability, and hardware efficiency, capable of reproducing convolutional and attention mechanisms. Applications include high-speed image processing, interpretable artificial intelligence, and digital hardware acceleration, offering promising future research directions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 166,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17592",
        "abs_url": "https://arxiv.org/abs/2511.17592",
        "pdf_url": "https://arxiv.org/pdf/2511.17592",
        "title": "GigaEvo: An Open Source Optimization Framework Powered By LLMs And Evolution Algorithms",
        "authors": [
            "Valentin Khrulkov",
            "Andrey Galichin",
            "Denis Bashkirov",
            "Dmitry Vinichenko",
            "Oleg Travkin",
            "Roman Alferov",
            "Andrey Kuznetsov",
            "Ivan Oseledets"
        ],
        "comments": "",
        "subjects": "Neural and Evolutionary Computing (cs.NE); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Recent advances in LLM-guided evolutionary computation, particularly AlphaEvolve (Novikov et al., 2025; Georgiev et al., 2025), have demonstrated remarkable success in discovering novel mathematical constructions and solving challenging optimization problems. However, the high-level descriptions in published work leave many implementation details unspecified, hindering reproducibility and further research. In this report we present GigaEvo, an extensible open-source framework that enables researchers to study and experiment with hybrid LLM-evolution approaches inspired by AlphaEvolve. Our system provides modular implementations of key components: MAP-Elites quality-diversity algorithms, asynchronous DAG-based evaluation pipelines, LLM-driven mutation operators with insight generation and bidirectional lineage tracking, and flexible multi-island evolutionary strategies. In order to assess reproducibility and validate our implementation we evaluate GigaEvo on challenging problems from the AlphaEvolve paper: Heilbronn triangle placement, circle packing in squares, and high-dimensional kissing numbers. The framework emphasizes modularity, concurrency, and ease of experimentation, enabling rapid prototyping through declarative configuration. We provide detailed descriptions of system architecture, implementation decisions, and experimental methodology to support further research in LLM driven evolutionary methods. The GigaEvo framework and all experimental code are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 167,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17600",
        "abs_url": "https://arxiv.org/abs/2511.17600",
        "pdf_url": "https://arxiv.org/pdf/2511.17600",
        "title": "SALPA: Spaceborne LiDAR Point Adjustment for Enhanced GEDI Footprint Geolocation",
        "authors": [
            "Narumasa Tsutsumida",
            "Rei Mitsuhashi",
            "Yoshito Sawada",
            "Akira Kato"
        ],
        "comments": "21 pages, 2 figures",
        "subjects": "Image and Video Processing (eess.IV); Machine Learning (cs.LG)",
        "abstract": "Spaceborne Light Detection and Ranging (LiDAR) systems, such as NASA's Global Ecosystem Dynamics Investigation (GEDI), provide forest structure for global carbon assessments. However, geolocation uncertainties (typically 5-15 m) propagate systematically through derived products, undermining forest profile estimates, including carbon stock assessments. Existing correction methods face critical limitations: waveform simulation approaches achieve meter-level accuracy but require high-resolution LiDAR data unavailable in most regions, while terrain-based methods employ deterministic grid searches that may overlook optimal solutions in continuous solution spaces. We present SALPA (Spaceborne LiDAR Point Adjustment), a multi-algorithm optimization framework integrating three optimization paradigms with five distance metrics. Operating exclusively with globally available digital elevation models and geoid data, SALPA explores continuous solution spaces through gradient-based, evolutionary, and swarm intelligence approaches. Validation across contrasting sites: topographically complex Nikko, Japan, and flat Landes, France, demonstrates 15-16% improvements over original GEDI positions and 0.5-2% improvements over the state-of-the-art GeoGEDI algorithm. L-BFGS-B with Area-based metrics achieves optimal accuracy-efficiency trade-offs, while population-based algorithms (genetic algorithms, particle swarm optimization) excel in complex terrain. The platform-agnostic framework facilitates straightforward adaptation to emerging spaceborne LiDAR missions, providing a generalizable foundation for universal geolocation correction essential for reliable global forest monitoring and climate policy decisions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 168,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17614",
        "abs_url": "https://arxiv.org/abs/2511.17614",
        "pdf_url": "https://arxiv.org/pdf/2511.17614",
        "title": "HSMix: Hard and Soft Mixing Data Augmentation for Medical Image Segmentation",
        "authors": [
            "Danyang Sun",
            "Fadi Dornaika",
            "Nagore Barrena"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Due to the high cost of annotation or the rarity of some diseases, medical image segmentation is often limited by data scarcity and the resulting overfitting problem. Self-supervised learning and semi-supervised learning can mitigate the data scarcity challenge to some extent. However, both of these paradigms are complex and require either hand-crafted pretexts or well-defined pseudo-labels. In contrast, data augmentation represents a relatively simple and straightforward approach to addressing data scarcity issues. It has led to significant improvements in image recognition tasks. However, the effectiveness of local image editing augmentation techniques in the context of segmentation has been less explored. We propose HSMix, a novel approach to local image editing data augmentation involving hard and soft mixing for medical semantic segmentation. In our approach, a hard-augmented image is created by combining homogeneous regions (superpixels) from two source images. A soft mixing method further adjusts the brightness of these composed regions with brightness mixing based on locally aggregated pixel-wise saliency coefficients. The ground-truth segmentation masks of the two source images undergo the same mixing operations to generate the associated masks for the augmented images. Our method fully exploits both the prior contour and saliency information, thus preserving local semantic information in the augmented images while enriching the augmentation space with more diversity. Our method is a plug-and-play solution that is model agnostic and applicable to a range of medical imaging modalities. Extensive experimental evidence has demonstrated its effectiveness in a variety of medical segmentation tasks. The source code is available in this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 169,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17627",
        "abs_url": "https://arxiv.org/abs/2511.17627",
        "pdf_url": "https://arxiv.org/pdf/2511.17627",
        "title": "An Ecologically-Informed Deep Learning Framework for Interpretable and Validatable Habitat Mapping",
        "authors": [
            "Iván Felipe Benavides-Martínez",
            "Cristiam Victoriano Portilla-Cabrera",
            "Katherine E. Mills",
            "Claire Enterline",
            "José Garcés-Vargas",
            "Andrew J. Allyn",
            "Auroop R Ganguly"
        ],
        "comments": "",
        "subjects": "Populations and Evolution (q-bio.PE); Machine Learning (cs.LG)",
        "abstract": "Benthic habitat is challenging due to the environmental complexity of the seafloor, technological limitations, and elevated operational costs, especially in under-explored regions. This generates knowledge gaps for the sustainable management of hydrobiological resources and their nexus with society. We developed ECOSAIC (Ecological Compression via Orthogonal Specialized Autoencoders for Interpretable Classification), an Artificial Intelligence framework for automatic classification of benthic habitats through interpretable latent representations using a customizable autoencoder. ECOSAIC compresses n-dimensional feature space by optimizing specialization and orthogonality between domain-informed features. We employed two domain-informed categories: biogeochemical and hydrogeomorphological, that together integrate biological, physicochemical, hydrological and geomorphological, features, whose constraints on habitats have been recognized in ecology for a century. We applied the model to the Colombian Pacific Ocean and the results revealed 16 benthic habitats, expanding from mangroves to deep rocky areas up to 1000 m depth. The candidate habitats exhibited a strong correspondence between their environmental constraints, represented in latent space, and their expected species composition. This correspondence reflected meaningful ecological associations rather than purely statistical correlations, where the habitat's environmental offerings align semantically with the species' requirements. This approach could improve the management and conservation of benthic habitats, facilitating the development of functional maps that support marine planning, biodiversity conservation and fish stock assessment. We also hope it provides new insights into how ecological principles can inform AI frameworks, particularly given the substantial data limitations that characterize ecological research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 170,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17643",
        "abs_url": "https://arxiv.org/abs/2511.17643",
        "pdf_url": "https://arxiv.org/pdf/2511.17643",
        "title": "Fluid Grey 2: How Well Does Generative Adversarial Network Learn Deeper Topology Structure in Architecture That Matches Images?",
        "authors": [
            "Yayan Qiu",
            "Sean Hanna"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Taking into account the regional characteristics of intrinsic and extrinsic properties of space is an essential issue in architectural design and urban renewal, which is often achieved step by step using image and graph-based GANs. However, each model nesting and data conversion may cause information loss, and it is necessary to streamline the tools to facilitate architects and users to participate in the design. Therefore, this study hopes to prove that I2I GAN also has the potential to recognize topological relationships autonomously. Therefore, this research proposes a method for quickly detecting the ability of pix2pix to learn topological relationships, which is achieved by adding two Grasshopper-based detection modules before and after GAN. At the same time, quantitative data is provided and its learning process is visualized, and changes in different input modes such as greyscale and RGB affect its learning efficiency. There are two innovations in this paper: 1) It proves that pix2pix can automatically learn spatial topological relationships and apply them to architectural design. 2) It fills the gap in detecting the performance of Image-based Generation GAN from a topological perspective. Moreover, the detection method proposed in this study takes a short time and is simple to operate. The two detection modules can be widely used for customizing image datasets with the same topological structure and for batch detection of topological relationships of images. In the future, this paper may provide a theoretical foundation and data support for the application of architectural design and urban renewal that use GAN to preserve spatial topological characteristics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 171,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17656",
        "abs_url": "https://arxiv.org/abs/2511.17656",
        "pdf_url": "https://arxiv.org/pdf/2511.17656",
        "title": "Multi-Agent Coordination in Autonomous Vehicle Routing: A Simulation-Based Study of Communication, Memory, and Routing Loops",
        "authors": [
            "KM Khalid Saifullah",
            "Daniel Palmer"
        ],
        "comments": "",
        "subjects": "Multiagent Systems (cs.MA); Machine Learning (cs.LG); Robotics (cs.RO)",
        "abstract": "Multi-agent coordination is critical for next-generation autonomous vehicle (AV) systems, yet naive implementations of communication-based rerouting can lead to catastrophic performance degradation. This study investigates a fundamental problem in decentralized multi-agent navigation: routing loops, where vehicles without persistent obstacle memory become trapped in cycles of inefficient path recalculation. Through systematic simulation experiments involving 72 unique configurations across varying vehicle densities (15, 35, 55 vehicles) and obstacle frequencies (6, 20 obstacles), we demonstrate that memory-less reactive rerouting increases average travel time by up to 682% compared to baseline conditions. To address this, we introduce Object Memory Management (OMM), a lightweight mechanism enabling agents to retain and share knowledge of previously encountered obstacles. OMM operates by maintaining a distributed blacklist of blocked nodes, which each agent consults during Dijkstra-based path recalculation, effectively preventing redundant routing attempts. Our results show that OMM-enabled coordination reduces average travel time by 75.7% and wait time by 88% compared to memory-less systems, while requiring only 1.67 route recalculations per vehicle versus 9.83 in memory-less scenarios. This work provides empirical evidence that persistent, shared memory is not merely beneficial but essential for robust multi-agent coordination in dynamic environments. The findings have implications beyond autonomous vehicles, informing the design of decentralized systems in robotics, network routing, and distributed AI. We provide a comprehensive experimental analysis, including detailed scenario breakdowns, scalability assessments, and visual documentation of the routing loop phenomenon, demonstrating OMM's critical role in preventing detrimental feedback cycles in cooperative multi-agent systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 172,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17658",
        "abs_url": "https://arxiv.org/abs/2511.17658",
        "pdf_url": "https://arxiv.org/pdf/2511.17658",
        "title": "Predicting Healthcare Provider Engagement in SMS Campaigns",
        "authors": [
            "Daanish Aleem Qureshi",
            "Rafay Chaudhary",
            "Kok Seng Tan",
            "Or Maoz",
            "Scott Burian",
            "Michael Gelber",
            "Phillip Hoon Kang",
            "Alan George Labouseur"
        ],
        "comments": "",
        "subjects": "Physics and Society (physics.soc-ph); Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "As digital communication grows in importance when connecting with healthcare providers, traditional behavioral and content message features are imbued with renewed significance. If one is to meaningfully connect with them, it is crucial to understand what drives them to engage and respond. In this study, the authors analyzed several million text messages sent through the Impiricus platform to learn which factors influenced whether or not a doctor clicked on a link in a message. Several key insights came to light through the use of logistic regression, random forest, and neural network models, the details of which the authors discuss in this paper.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 173,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17698",
        "abs_url": "https://arxiv.org/abs/2511.17698",
        "pdf_url": "https://arxiv.org/pdf/2511.17698",
        "title": "Quantum Fourier Transform Based Kernel for Solar Irrandiance Forecasting",
        "authors": [
            "Nawfel Mechiche-Alami",
            "Eduardo Rodriguez",
            "Jose M. Cardemil",
            "Enrique Lopez Droguett"
        ],
        "comments": "33 pages, 13 figures",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "This study proposes a Quantum Fourier Transform (QFT)-enhanced quantum kernel for short-term time-series forecasting. Each signal is windowed, amplitude-encoded, transformed by a QFT, then passed through a protective rotation layer to avoid the QFT/QFT adjoint cancellation; the resulting kernel is used in kernel ridge regression (KRR). Exogenous predictors are incorporated by convexly fusing feature-specific kernels. On multi-station solar irradiance data across Koppen climate classes, the proposed kernel consistently improves median R2 and nRMSE over reference classical RBF and polynomials kernels, while also reducing bias (nMBE); complementary MAE/ERMAX analyses indicate tighter average errors with remaining headroom under sharp transients. For both quantum and classical models, the only tuned quantities are the feature-mixing weights and the KRR ridge alpha; classical hyperparameters (gamma, r, d) are fixed, with the same validation set size for all models. Experiments are conducted on a noiseless simulator (5 qubits; window length L=32). Limitations and ablations are discussed, and paths toward NISQ execution are outlined.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 174,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17721",
        "abs_url": "https://arxiv.org/abs/2511.17721",
        "pdf_url": "https://arxiv.org/pdf/2511.17721",
        "title": "Prequential posteriors",
        "authors": [
            "Shreya Sinha-Roy",
            "Richard G. Everitt",
            "Christian P. Robert",
            "Ritabrata Dutta"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Data assimilation is a fundamental task in updating forecasting models upon observing new data, with applications ranging from weather prediction to online reinforcement learning. Deep generative forecasting models (DGFMs) have shown excellent performance in these areas, but assimilating data into such models is challenging due to their intractable likelihood functions. This limitation restricts the use of standard Bayesian data assimilation methodologies for DGFMs. To overcome this, we introduce prequential posteriors, based upon a predictive-sequential (prequential) loss function; an approach naturally suited for temporally dependent data which is the focus of forecasting tasks. Since the true data-generating process often lies outside the assumed model class, we adopt an alternative notion of consistency and prove that, under mild conditions, both the prequential loss minimizer and the prequential posterior concentrate around parameters with optimal predictive performance. For scalable inference, we employ easily parallelizable wastefree sequential Monte Carlo (SMC) samplers with preconditioned gradient-based kernels, enabling efficient exploration of high-dimensional parameter spaces such as those in DGFMs. We validate our method on both a synthetic multi-dimensional time series and a real-world meteorological dataset; highlighting its practical utility for data assimilation for complex dynamical systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 175,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17760",
        "abs_url": "https://arxiv.org/abs/2511.17760",
        "pdf_url": "https://arxiv.org/pdf/2511.17760",
        "title": "When Active Learning Fails, Uncalibrated Out of Distribution Uncertainty Quantification Might Be the Problem",
        "authors": [
            "Ashley S. Dale",
            "Kangming Li",
            "Brian DeCost",
            "Hao Wan",
            "Yuchen Han",
            "Yao Fehlis",
            "Jason Hattrick-Simpers"
        ],
        "comments": "",
        "subjects": "Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG)",
        "abstract": "Efficiently and meaningfully estimating prediction uncertainty is important for exploration in active learning campaigns in materials discovery, where samples with high uncertainty are interpreted as containing information missing from the model. In this work, the effect of different uncertainty estimation and calibration methods are evaluated for active learning when using ensembles of ALIGNN, eXtreme Gradient Boost, Random Forest, and Neural Network model architectures. We compare uncertainty estimates from ALIGNN deep ensembles to loss landscape uncertainty estimates obtained for solubility, bandgap, and formation energy prediction tasks. We then evaluate how the quality of the uncertainty estimate impacts an active learning campaign that seeks model generalization to out-of-distribution data. Uncertainty calibration methods were found to variably generalize from in-domain data to out-of-domain data. Furthermore, calibrated uncertainties were generally unsuccessful in reducing the amount of data required by a model to improve during an active learning campaign on out-of-distribution data when compared to random sampling and uncalibrated uncertainties. The impact of poor-quality uncertainty persists for random forest and eXtreme Gradient Boosting models trained on the same data for the same tasks, indicating that this is at least partially intrinsic to the data and not due to model capacity alone. Analysis of the target, in-distribution uncertainty, out-of-distribution uncertainty, and training residual distributions suggest that future work focus on understanding empirical uncertainties in the feature input space for cases where ensemble prediction variances do not accurately capture the missing information required for the model to generalize.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 176,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17765",
        "abs_url": "https://arxiv.org/abs/2511.17765",
        "pdf_url": "https://arxiv.org/pdf/2511.17765",
        "title": "LEARN: Learning End-to-End Aerial Resource-Constrained Multi-Robot Navigation",
        "authors": [
            "Darren Chiu",
            "Zhehui Huang",
            "Ruohai Ge",
            "Gaurav S. Sukhatme"
        ],
        "comments": "20 pages, 15 figures",
        "subjects": "Robotics (cs.RO); Machine Learning (cs.LG); Multiagent Systems (cs.MA)",
        "abstract": "Nano-UAV teams offer great agility yet face severe navigation challenges due to constrained onboard sensing, communication, and computation. Existing approaches rely on high-resolution vision or compute-intensive planners, rendering them infeasible for these platforms. We introduce LEARN, a lightweight, two-stage safety-guided reinforcement learning (RL) framework for multi-UAV navigation in cluttered spaces. Our system combines low-resolution Time-of-Flight (ToF) sensors and a simple motion planner with a compact, attention-based RL policy. In simulation, LEARN outperforms two state-of-the-art planners by $10\\%$ while using substantially fewer resources. We demonstrate LEARN's viability on six Crazyflie quadrotors, achieving fully onboard flight in diverse indoor and outdoor environments at speeds up to $2.0 m/s$ and traversing $0.2 m$ gaps.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 177,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17772",
        "abs_url": "https://arxiv.org/abs/2511.17772",
        "pdf_url": "https://arxiv.org/pdf/2511.17772",
        "title": "Weighted Birkhoff Averages Accelerate Data-Driven Methods",
        "authors": [
            "Maria Bou-Sakr-El-Tayar",
            "Jason J. Bramburger",
            "Matthew J. Colbrook"
        ],
        "comments": "",
        "subjects": "Dynamical Systems (math.DS); Machine Learning (cs.LG); Chaotic Dynamics (nlin.CD)",
        "abstract": "Many data-driven algorithms in dynamical systems rely on ergodic averages that converge painfully slowly. One simple idea changes this: taper the ends. Weighted Birkhoff averages can converge much faster (sometimes superpolynomially, even exponentially) and can be incorporated seamlessly into existing methods. We demonstrate this with five weighted algorithms: weighted Dynamic Mode Decomposition (wtDMD), weighted Extended DMD (wtEDMD), weighted Sparse Identification of Nonlinear Dynamics (wtSINDy), weighted spectral measure estimation, and weighted diffusion forecasting. Across examples ranging from fluid flows to El Niño data, the message is clear: weighting costs nothing, is easy to implement, and often delivers markedly better results from the same data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 178,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17783",
        "abs_url": "https://arxiv.org/abs/2511.17783",
        "pdf_url": "https://arxiv.org/pdf/2511.17783",
        "title": "Variational Estimators for Node Popularity Models",
        "authors": [
            "Jony Karki",
            "Dongzhou Huang",
            "Yunpeng Zhao"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Node popularity is recognized as a key factor in modeling real-world networks, capturing heterogeneity in connectivity across communities. This concept is equally important in bipartite networks, where nodes in different partitions may exhibit varying popularity patterns, motivating models such as the Two-Way Node Popularity Model (TNPM). Existing methods, such as the Two-Stage Divided Cosine (TSDC) algorithm, provide a scalable estimation approach but may have limitations in terms of accuracy or applicability across different types of networks. In this paper, we develop a computationally efficient and theoretically justified variational expectation-maximization (VEM) framework for the TNPM. We establish label consistency for the estimated community assignments produced by the proposed variational estimator in bipartite networks. Through extensive simulation studies, we show that our method achieves superior estimation accuracy across a range of bipartite as well as undirected networks compared to existing algorithms. Finally, we evaluate our method on real-world bipartite and undirected networks, further demonstrating its practical effectiveness and robustness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 179,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17806",
        "abs_url": "https://arxiv.org/abs/2511.17806",
        "pdf_url": "https://arxiv.org/pdf/2511.17806",
        "title": "REXO: Indoor Multi-View Radar Object Detection via 3D Bounding Box Diffusion",
        "authors": [
            "Ryoma Yataka",
            "Pu Perry Wang",
            "Petros Boufounos",
            "Ryuhei Takahashi"
        ],
        "comments": "26 pages, Accepted to AAAI 2026; Code to be released",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Signal Processing (eess.SP)",
        "abstract": "Multi-view indoor radar perception has drawn attention due to its cost-effectiveness and low privacy risks. Existing methods often rely on {implicit} cross-view radar feature association, such as proposal pairing in RFMask or query-to-feature cross-attention in RETR, which can lead to ambiguous feature matches and degraded detection in complex indoor scenes. To address these limitations, we propose \\textbf{REXO} (multi-view Radar object dEtection with 3D bounding boX diffusiOn), which lifts the 2D bounding box (BBox) diffusion process of DiffusionDet into the 3D radar space. REXO utilizes these noisy 3D BBoxes to guide an {explicit} cross-view radar feature association, enhancing the cross-view radar-conditioned denoising process. By accounting for prior knowledge that the person is in contact with the ground, REXO reduces the number of diffusion parameters by determining them from this prior. Evaluated on two open indoor radar datasets, our approach surpasses state-of-the-art methods by a margin of +4.22 AP on the HIBER dataset and +11.02 AP on the MMVR dataset.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 180,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17813",
        "abs_url": "https://arxiv.org/abs/2511.17813",
        "pdf_url": "https://arxiv.org/pdf/2511.17813",
        "title": "Point of Order: Action-Aware LLM Persona Modeling for Realistic Civic Simulation",
        "authors": [
            "Scott Merrill",
            "Shashank Srivastava"
        ],
        "comments": "8 pages (29 pages including appendix), 18 figures. Code and datasets are available at this https URL. Submitted to ACL 2026",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Sound (cs.SD)",
        "abstract": "Large language models offer opportunities to simulate multi-party deliberation, but realistic modeling remains limited by a lack of speaker-attributed data. Transcripts produced via automatic speech recognition (ASR) assign anonymous speaker labels (e.g., Speaker_1), preventing models from capturing consistent human behavior. This work introduces a reproducible pipeline to transform public Zoom recordings into speaker-attributed transcripts with metadata like persona profiles and pragmatic action tags (e.g., [propose_motion]). We release three local government deliberation datasets: Appellate Court hearings, School Board meetings, and Municipal Council sessions. Fine-tuning LLMs to model specific participants using this \"action-aware\" data produces a 67% reduction in perplexity and nearly doubles classifier-based performance metrics for speaker fidelity and realism. Turing-style human evaluations show our simulations are often indistinguishable from real deliberations, providing a practical and scalable method for complex realistic civic simulations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 181,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17825",
        "abs_url": "https://arxiv.org/abs/2511.17825",
        "pdf_url": "https://arxiv.org/pdf/2511.17825",
        "title": "Analog Physical Systems Can Exhibit Double Descent",
        "authors": [
            "Sam Dillavou",
            "Jason W Rocks",
            "Jacob F Wycoff",
            "Andrea J Liu",
            "Douglas J Durian"
        ],
        "comments": "11 pages 7 figures",
        "subjects": "Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning (cs.LG)",
        "abstract": "An important component of the success of large AI models is double descent, in which networks avoid overfitting as they grow relative to the amount of training data, instead improving their performance on unseen data. Here we demonstrate double descent in a decentralized analog network of self-adjusting resistive elements. This system trains itself and performs tasks without a digital processor, offering potential gains in energy efficiency and speed -- but must endure component non-idealities. We find that standard training fails to yield double descent, but a modified protocol that accommodates this inherent imperfection succeeds. Our findings show that analog physical systems, if appropriately trained, can exhibit behaviors underlying the success of digital AI. Further, they suggest that biological systems might similarly benefit from over-parameterization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 182,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17850",
        "abs_url": "https://arxiv.org/abs/2511.17850",
        "pdf_url": "https://arxiv.org/pdf/2511.17850",
        "title": "Efficient Dynamic and Momentum Aperture Optimization for Lattice Design Using Multipoint Bayesian Algorithm Execution",
        "authors": [
            "Z. Zhang",
            "I. Agapov",
            "S. Gasiorowski",
            "T. Hellert",
            "W. Neiswanger",
            "X. Huang",
            "D. Ratner"
        ],
        "comments": "10 pages, 8 figures",
        "subjects": "Accelerator Physics (physics.acc-ph); Machine Learning (cs.LG)",
        "abstract": "We demonstrate that multipoint Bayesian algorithm execution can overcome fundamental computational challenges in storage ring design optimization. Dynamic (DA) and momentum (MA) optimization is a multipoint, multiobjective design task for storage rings, ultimately informing the flux of x-ray sources and luminosity of colliders. Current state-of-art black-box optimization methods require extensive particle-tracking simulations for each trial configuration; the high computational cost restricts the extent of the search to $\\sim 10^3$ configurations, and therefore limits the quality of the final design. We remove this bottleneck using multipointBAX, which selects, simulates, and models each trial configuration at the single particle level. We demonstrate our approach on a novel design for a fourth-generation light source, with neural-network powered multipointBAX achieving equivalent Pareto front results using more than two orders of magnitude fewer tracking computations compared to genetic algorithms. The significant reduction in cost positions multipointBAX as a promising alternative to black-box optimization, and we anticipate multipointBAX will be instrumental in the design of future light sources, colliders, and large-scale scientific facilities.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 183,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17865",
        "abs_url": "https://arxiv.org/abs/2511.17865",
        "pdf_url": "https://arxiv.org/pdf/2511.17865",
        "title": "Generative Model Predictive Control in Manufacturing Processes: A Review",
        "authors": [
            "Suk Ki Lee",
            "Ronnie F. P. Stone",
            "Max Gao",
            "Wenlong Zhang",
            "Zhenghui Sha",
            "Hyunwoong Ko"
        ],
        "comments": "24 pages, 5 figures, Review article",
        "subjects": "Systems and Control (eess.SY); Machine Learning (cs.LG)",
        "abstract": "Manufacturing processes are inherently dynamic and uncertain, with varying parameters and nonlinear behaviors, making robust control essential for maintaining quality and reliability. Traditional control methods often fail under these conditions due to their reactive nature. Model Predictive Control (MPC) has emerged as a more advanced framework, leveraging process models to predict future states and optimize control actions. However, MPC relies on simplified models that often fail to capture complex dynamics, and it struggles with accurate state estimation and handling the propagation of uncertainty in manufacturing environments. Machine learning (ML) has been introduced to enhance MPC by modeling nonlinear dynamics and learning latent representations that support predictive modeling, state estimation, and optimization. Yet existing ML-driven MPC approaches remain deterministic and correlation-focused, motivating the exploration of generative. Generative ML offers new opportunities by learning data distributions, capturing hidden patterns, and inherently managing uncertainty, thereby complementing MPC. This review highlights five representative methods and examines how each has been integrated into MPC components, including predictive modeling, state estimation, and optimization. By synthesizing these cases, we outline the common ways generative ML can systematically enhance MPC and provide a framework for understanding its potential in diverse manufacturing processes. We identify key research gaps, propose future directions, and use a representative case to illustrate how generative ML-driven MPC can extend broadly across manufacturing. Taken together, this review positions generative ML not as an incremental add-on but as a transformative approach to reshape predictive control for next-generation manufacturing systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 184,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17892",
        "abs_url": "https://arxiv.org/abs/2511.17892",
        "pdf_url": "https://arxiv.org/pdf/2511.17892",
        "title": "Arbitrage-Free Bond and Yield Curve Forecasting with Neural Filters under HJM Constraints",
        "authors": [
            "Xiang Gao",
            "Cody Hyndman"
        ],
        "comments": "31 pages, 17 figures",
        "subjects": "Mathematical Finance (q-fin.MF); Machine Learning (cs.LG); Computational Finance (q-fin.CP); Machine Learning (stat.ML)",
        "abstract": "We develop an arbitrage-free deep learning framework for yield curve and bond price forecasting based on the Heath-Jarrow-Morton (HJM) term-structure model and a dynamic Nelson-Siegel parameterization of forward rates. Our approach embeds a no-arbitrage drift restriction into a neural state-space architecture by combining Kalman, extended Kalman, and particle filters with recurrent neural networks (LSTM/CLSTM), and introduces an explicit arbitrage error regularization (AER) term during training. The model is applied to U.S. Treasury and corporate bond data, and its performance is evaluated for both yield-space and price-space predictions at 1-day and 5-day horizons. Empirically, arbitrage regularization leads to its strongest improvements at short maturities, particularly in 5-day-ahead forecasts, increasing market-consistency as measured by bid-ask hit rates and reducing dollar-denominated prediction errors.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 185,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17913",
        "abs_url": "https://arxiv.org/abs/2511.17913",
        "pdf_url": "https://arxiv.org/pdf/2511.17913",
        "title": "Token-Controlled Re-ranking for Sequential Recommendation via LLMs",
        "authors": [
            "Wenxi Dai",
            "Wujiang Xu",
            "Pinhuan Wang",
            "Dimitris N. Metaxas"
        ],
        "comments": "",
        "subjects": "Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "The widespread adoption of Large Language Models (LLMs) as re-rankers is shifting recommender systems towards a user-centric paradigm. However, a significant gap remains: current re-rankers often lack mechanisms for fine-grained user control. They struggle to balance inherent user preferences with multiple attribute-based constraints, often resorting to simplistic hard filtering that can excessively narrow the recommendation pool and yield suboptimal results. This limitation leaves users as passive recipients rather than active collaborators in the recommendation process. To bridge this gap, we propose COREC, a novel token-augmented re-ranking framework that incorporates specific user requirements in co-creating the recommendation outcome. COREC empowers users to steer re-ranking results with precise and flexible control via explicit, attribute-based signals. The framework learns to balance these commands against latent preferences, yielding rankings that adhere to user instructions without sacrificing personalization. Experiments show that COREC: (1) exceeds state-of-the-art baselines on standard recommendation effectiveness and (2) demonstrates superior adherence to specific attribute requirements, proving that COREC enables fine-grained and predictable manipulation of the rankings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 186,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17931",
        "abs_url": "https://arxiv.org/abs/2511.17931",
        "pdf_url": "https://arxiv.org/pdf/2511.17931",
        "title": "A Reinforcement Learning Framework for Resource Allocation in Uplink Carrier Aggregation in the Presence of Self Interference",
        "authors": [
            "Jaswanth Bodempudi",
            "Batta Siva Sairam",
            "Madepalli Haritha",
            "Sandesh Rao Mattu",
            "Ananthanarayanan Chockalingam"
        ],
        "comments": "Accepted in IEEE Trans. on Machine Learning in Communications and Networking",
        "subjects": "Information Theory (cs.IT); Machine Learning (cs.LG); Signal Processing (eess.SP)",
        "abstract": "Carrier aggregation (CA) is a technique that allows mobile networks to combine multiple carriers to increase user data rate. On the uplink, for power constrained users, this translates to the need for an efficient resource allocation scheme, where each user distributes its available power among its assigned uplink carriers. Choosing a good set of carriers and allocating appropriate power on the carriers is important. If the carrier allocation on the uplink is such that a harmonic of a user's uplink carrier falls on the downlink frequency of that user, it leads to a self coupling-induced sensitivity degradation of that user's downlink receiver. In this paper, we model the uplink carrier aggregation problem as an optimal resource allocation problem with the associated constraints of non-linearities induced self interference (SI). This involves optimization over a discrete variable (which carriers need to be turned on) and a continuous variable (what power needs to be allocated on the selected carriers) in dynamic environments, a problem which is hard to solve using traditional methods owing to the mixed nature of the optimization variables and the additional need to consider the SI constraint. We adopt a reinforcement learning (RL) framework involving a compound-action actor-critic (CA2C) algorithm for the uplink carrier aggregation problem. We propose a novel reward function that is critical for enabling the proposed CA2C algorithm to efficiently handle SI. The CA2C algorithm along with the proposed reward function learns to assign and activate suitable carriers in an online fashion. Numerical results demonstrate that the proposed RL based scheme is able to achieve higher sum throughputs compared to naive schemes. The results also demonstrate that the proposed reward function allows the CA2C algorithm to adapt the optimization both in the presence and absence of SI.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 187,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17938",
        "abs_url": "https://arxiv.org/abs/2511.17938",
        "pdf_url": "https://arxiv.org/pdf/2511.17938",
        "title": "SPINE: Token-Selective Test-Time Reinforcement Learning with Entropy-Band Regularization",
        "authors": [
            "Jianghao Wu",
            "Yasmeen George",
            "Jin Ye",
            "Yicheng Wu",
            "Daniel F. Schmidt",
            "Jianfei Cai"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Large language models (LLMs) and multimodal LLMs (MLLMs) excel at chain-of-thought reasoning but face distribution shift at test-time and a lack of verifiable supervision. Recent test-time reinforcement learning (TTRL) methods derive label-free pseudo-rewards from self-consistency voting over sampled trajectories, yet they often collapse: the majority-vote reward prevails, responses shorten, and Pass@1 declines. We trace this to uniform sequence updates in which most tokens are low-entropy followers, while a small high-entropy subset determines the reasoning branches. Thus we propose SPINE, a token-selective test-time reinforcement learning framework that (i) updates only forking tokens, the high-entropy branch points identified from forward-pass statistics, and (ii) applies an entropy-band regularizer at those tokens to sustain exploration when entropy is too low and to suppress noisy supervision when it is too high. SPINE plugs into GRPO-style objectives, optionally with a KL anchor, and requires no labels or reward models. Across ten benchmarks spanning multimodal VQA, general and expert QA, mathematical reasoning, and medical QA, SPINE consistently improves Pass@1 over TTRL while avoiding response-length collapse and yielding more stable training dynamics on both LLM and MLLM backbones. These results indicate that aligning updates with chain-of-thought branch points is a simple and label-free mechanism for stable and effective test-time adaptation in reasoning models. Code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 188,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17954",
        "abs_url": "https://arxiv.org/abs/2511.17954",
        "pdf_url": "https://arxiv.org/pdf/2511.17954",
        "title": "A multi-view contrastive learning framework for spatial embeddings in risk modelling",
        "authors": [
            "Freek Holvoet",
            "Christopher Blier-Wong",
            "Katrien Antonio"
        ],
        "comments": "",
        "subjects": "Risk Management (q-fin.RM); Machine Learning (cs.LG)",
        "abstract": "Incorporating spatial information, particularly those influenced by climate, weather, and demographic factors, is crucial for improving underwriting precision and enhancing risk management in insurance. However, spatial data are often unstructured, high-dimensional, and difficult to integrate into predictive models. Embedding methods are needed to convert spatial data into meaningful representations for modelling tasks. We propose a novel multi-view contrastive learning framework for generating spatial embeddings that combine information from multiple spatial data sources. To train the model, we construct a spatial dataset that merges satellite imagery and OpenStreetMap features across Europe. The framework aligns these spatial views with coordinate-based encodings, producing low-dimensional embeddings that capture both spatial structure and contextual similarity. Once trained, the model generates embeddings directly from latitude-longitude pairs, enabling any dataset with coordinates to be enriched with meaningful spatial features without requiring access to the original spatial inputs. In a case study on French real estate prices, we compare models trained on raw coordinates against those using our spatial embeddings as inputs. The embeddings consistently improve predictive accuracy across generalised linear, additive, and boosting models, while providing interpretable spatial effects and demonstrating transferability to unseen regions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 189,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.17977",
        "abs_url": "https://arxiv.org/abs/2511.17977",
        "pdf_url": "https://arxiv.org/pdf/2511.17977",
        "title": "Synthesizing Precise Protocol Specs from Natural Language for Effective Test Generation",
        "authors": [
            "Kuangxiangzi Liu",
            "Dhiman Chakraborty",
            "Alexander Liggesmeyer",
            "Andreas Zeller"
        ],
        "comments": "",
        "subjects": "Software Engineering (cs.SE); Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)",
        "abstract": "Safety- and security-critical systems have to be thoroughly tested against their specifications. The state of practice is to have _natural language_ specifications, from which test cases are derived manually - a process that is slow, error-prone, and difficult to scale. _Formal_ specifications, on the other hand, are well-suited for automated test generation, but are tedious to write and maintain. In this work, we propose a two-stage pipeline that uses large language models (LLMs) to bridge the gap: First, we extract _protocol elements_ from natural-language specifications; second, leveraging a protocol implementation, we synthesize and refine a formal _protocol specification_ from these elements, which we can then use to massively test further implementations. We see this two-stage approach to be superior to end-to-end LLM-based test generation, as 1. it produces an _inspectable specification_ that preserves traceability to the original text; 2. the generation of actual test cases _no longer requires an LLM_; 3. the resulting formal specs are _human-readable_, and can be reviewed, version-controlled, and incrementally refined; and 4. over time, we can build a _corpus_ of natural-language-to-formal-specification mappings that can be used to further train and refine LLMs for more automatic translations. Our prototype, AUTOSPEC, successfully demonstrated the feasibility of our approach on five widely used _internet protocols_ (SMTP, POP3, IMAP, FTP, and ManageSieve) by applying its methods on their _RFC specifications_ written in natural-language, and the recent _I/O grammar_ formalism for protocol specification and fuzzing. In its evaluation, AUTOSPEC recovers on average 92.8% of client and 80.2% of server message types, and achieves 81.5% message acceptance across diverse, real-world systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 190,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18024",
        "abs_url": "https://arxiv.org/abs/2511.18024",
        "pdf_url": "https://arxiv.org/pdf/2511.18024",
        "title": "Extracting Interaction-Aware Monosemantic Concepts in Recommender Systems",
        "authors": [
            "Dor Arviv",
            "Yehonatan Elisha",
            "Oren Barkan",
            "Noam Koenigstein"
        ],
        "comments": "",
        "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "We present a method for extracting \\emph{monosemantic} neurons, defined as latent dimensions that align with coherent and interpretable concepts, from user and item embeddings in recommender systems. Our approach employs a Sparse Autoencoder (SAE) to reveal semantic structure within pretrained representations. In contrast to work on language models, monosemanticity in recommendation must preserve the interactions between separate user and item embeddings. To achieve this, we introduce a \\emph{prediction aware} training objective that backpropagates through a frozen recommender and aligns the learned latent structure with the model's user-item affinity predictions. The resulting neurons capture properties such as genre, popularity, and temporal trends, and support post hoc control operations including targeted filtering and content promotion without modifying the base model. Our method generalizes across different recommendation models and datasets, providing a practical tool for interpretable and controllable personalization. Code and evaluation resources are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 191,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18025",
        "abs_url": "https://arxiv.org/abs/2511.18025",
        "pdf_url": "https://arxiv.org/pdf/2511.18025",
        "title": "Correlated-Sequence Differential Privacy",
        "authors": [
            "Yifan Luo",
            "Meng Zhang",
            "Jin Xu",
            "Junting Chen",
            "Jianwei Huang"
        ],
        "comments": "11 pages, 5 figures. Published in 2025 34th International Conference on Computer Communications and Networks (ICCCN), IEEE, August 2025",
        "subjects": "Cryptography and Security (cs.CR); Information Theory (cs.IT); Machine Learning (cs.LG)",
        "abstract": "Data streams collected from multiple sources are rarely independent. Values evolve over time and influence one another across sequences. These correlations improve prediction in healthcare, finance, and smart-city control yet violate the record-independence assumption built into most Differential Privacy (DP) mechanisms. To restore rigorous privacy guarantees without sacrificing utility, we introduce Correlated-Sequence Differential Privacy (CSDP), a framework specifically designed for preserving privacy in correlated sequential data. CSDP addresses two linked challenges: quantifying the extra information an attacker gains from joint temporal and cross-sequence links, and adding just enough noise to hide that information while keeping the data useful. We model multivariate streams as a Coupling Markov Chain, yielding the derived loose leakage bound expressed with a few spectral terms and revealing a counterintuitive result: stronger coupling can actually decrease worst-case leakage by dispersing perturbations across sequences. Guided by these bounds, we build the Freshness-Regulated Adaptive Noise (FRAN) mechanism--combining data aging, correlation-aware sensitivity scaling, and Laplace noise--that runs in linear time. Tests on two-sequence datasets show that CSDP improves the privacy-utility trade-off by approximately 50% over existing correlated-DP methods and by two orders of magnitude compared to the standard DP approach.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 192,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18035",
        "abs_url": "https://arxiv.org/abs/2511.18035",
        "pdf_url": "https://arxiv.org/pdf/2511.18035",
        "title": "On a Reinforcement Learning Methodology for Epidemic Control, with application to COVID-19",
        "authors": [
            "Giacomo Iannucci",
            "Petros Barmpounakis",
            "Alexandros Beskos",
            "Nikolaos Demiris"
        ],
        "comments": "Submitted to Statistics and Computing. Approx. 26 pages, 10 figures",
        "subjects": "Methodology (stat.ME); Machine Learning (cs.LG); Computation (stat.CO); Machine Learning (stat.ML)",
        "abstract": "This paper presents a real time, data driven decision support framework for epidemic control. We combine a compartmental epidemic model with sequential Bayesian inference and reinforcement learning (RL) controllers that adaptively choose intervention levels to balance disease burden, such as intensive care unit (ICU) load, against socio economic costs. We construct a context specific cost function using empirical experiments and expert feedback. We study two RL policies: an ICU threshold rule computed via Monte Carlo grid search, and a policy based on a posterior averaged Q learning agent. We validate the framework by fitting the epidemic model to publicly available ICU occupancy data from the COVID 19 pandemic in England and then generating counterfactual roll out scenarios under each RL controller, which allows us to compare the RL policies to the historical government strategy. Over a 300 day period and for a range of cost parameters, both controllers substantially reduce ICU burden relative to the observed interventions, illustrating how Bayesian sequential learning combined with RL can support the design of epidemic control policies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 193,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18047",
        "abs_url": "https://arxiv.org/abs/2511.18047",
        "pdf_url": "https://arxiv.org/pdf/2511.18047",
        "title": "Fidelity-Aware Recommendation Explanations via Stochastic Path Integration",
        "authors": [
            "Oren Barkan",
            "Yahlly Schein",
            "Yehonatan Elisha",
            "Veronika Bogina",
            "Mikhail Baklanov",
            "Noam Koenigstein"
        ],
        "comments": "",
        "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Explanation fidelity, which measures how accurately an explanation reflects a model's true reasoning, remains critically underexplored in recommender systems. We introduce SPINRec (Stochastic Path Integration for Neural Recommender Explanations), a model-agnostic approach that adapts path-integration techniques to the sparse and implicit nature of recommendation data. To overcome the limitations of prior methods, SPINRec employs stochastic baseline sampling: instead of integrating from a fixed or unrealistic baseline, it samples multiple plausible user profiles from the empirical data distribution and selects the most faithful attribution path. This design captures the influence of both observed and unobserved interactions, yielding more stable and personalized explanations. We conduct the most comprehensive fidelity evaluation to date across three models (MF, VAE, NCF), three datasets (ML1M, Yahoo! Music, Pinterest), and a suite of counterfactual metrics, including AUC-based perturbation curves and fixed-length diagnostics. SPINRec consistently outperforms all baselines, establishing a new benchmark for faithful explainability in recommendation. Code and evaluation tools are publicly available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 194,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18051",
        "abs_url": "https://arxiv.org/abs/2511.18051",
        "pdf_url": "https://arxiv.org/pdf/2511.18051",
        "title": "Sparse Kalman Identification for Partially Observable Systems via Adaptive Bayesian Learning",
        "authors": [
            "Jilan Mei",
            "Tengjie Zheng",
            "Lin Cheng",
            "Shengping Gong",
            "Xu Huang"
        ],
        "comments": "",
        "subjects": "Systems and Control (eess.SY); Machine Learning (cs.LG)",
        "abstract": "Sparse dynamics identification is an essential tool for discovering interpretable physical models and enabling efficient control in engineering systems. However, existing methods rely on batch learning with full historical data, limiting their applicability to real-time scenarios involving sequential and partially observable data. To overcome this limitation, this paper proposes an online Sparse Kalman Identification (SKI) method by integrating the Augmented Kalman Filter (AKF) and Automatic Relevance Determination (ARD). The main contributions are: (1) a theoretically grounded Bayesian sparsification scheme that is seamlessly integrated into the AKF framework and adapted to sequentially collected data in online scenarios; (2) an update mechanism that adapts the Kalman posterior to reflect the updated selection of the basis functions that define the model structure; (3) an explicit gradient-descent formulation that enhances computational efficiency. Consequently, the SKI method achieves accurate model structure selection with millisecond-level efficiency and higher identification accuracy, as demonstrated by extensive simulations and real-world experiments (showing an 84.21\\% improvement in accuracy over the baseline AKF).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 195,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18054",
        "abs_url": "https://arxiv.org/abs/2511.18054",
        "pdf_url": "https://arxiv.org/pdf/2511.18054",
        "title": "Blu-WERP (Web Extraction and Refinement Pipeline): A Scalable Pipeline for Preprocessing Large Language Model Datasets",
        "authors": [
            "Gowtham",
            "Sai Rupesh",
            "Sanjay Kumar",
            "Saravanan",
            "Venkata Chaithanya"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "High-quality training data is fundamental to large language model (LLM) performance, yet existing preprocessing pipelines often struggle to effectively remove noise and unstructured content from web-scale corpora. This paper presents Blu-WERP, a novel data preprocessing pipeline designed to optimize the quality of Common Crawl WARC files for LLM training. We demonstrate that Blu-WERP significantly outperforms established baselines including DCLM across multiple model scales and evaluation benchmarks. Our pipeline processes CC WARC dumps, implementing advanced filtering and quality assessment mechanisms. We conducted comprehensive evaluations using models with 150M, 400M, 530M, 750M, and 1B parameters, testing against nine standard benchmarks categorized as World Knowledge & Reasoning, Language Understanding, and Commonsense Reasoning. Results show Blu-WERP consistently achieved superior performance across all model scales. At the 1B parameter scale, Relatively Blu-WERP demonstrates a 4.0% and 9.5% aggregate improvement over DCLM and Fineweb respectively, while achieving quality-per-token efficiency gain. Categorical analysis reveals 2.4% improvement in World Knowledge & Reasoning, 6.2% improvement in Language Understanding, and 4.2% improvement in Commonsense Reasoning. These results establish Blu-WERP as a state-of-the-art preprocessing pipeline that substantially improves LLM training data quality and downstream model performance with reduced computational cost. Our findings contribute to the growing body of research on data-centric AI, demonstrating that preprocessing pipeline design significantly impacts LLM capabilities. The Blu-WERP pipeline represents a practical advancement in data quality optimization, offering researchers and practitioners an effective solution for improving LLM training efficiency and model performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 196,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18060",
        "abs_url": "https://arxiv.org/abs/2511.18060",
        "pdf_url": "https://arxiv.org/pdf/2511.18060",
        "title": "An operator splitting analysis of Wasserstein--Fisher--Rao gradient flows",
        "authors": [
            "Francesca Romana Crucinio",
            "Sahani Pathiraja"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Methodology (stat.ME)",
        "abstract": "Wasserstein-Fisher-Rao (WFR) gradient flows have been recently proposed as a powerful sampling tool that combines the advantages of pure Wasserstein (W) and pure Fisher-Rao (FR) gradient flows. Existing algorithmic developments implicitly make use of operator splitting techniques to numerically approximate the WFR partial differential equation, whereby the W flow is evaluated over a given step size and then the FR flow (or vice versa). This works investigates the impact of the order in which the W and FR operator are evaluated and aims to provide a quantitative analysis. Somewhat surprisingly, we show that with a judicious choice of step size and operator ordering, the split scheme can converge to the target distribution faster than the exact WFR flow (in terms of model time). We obtain variational formulae describing the evolution over one time step of both sequential splitting schemes and investigate in which settings the W-FR split should be preferred to the FR-W split. As a step towards this goal we show that the WFR gradient flow preserves log-concavity and obtain the first sharp decay bound for WFR.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 197,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18076",
        "abs_url": "https://arxiv.org/abs/2511.18076",
        "pdf_url": "https://arxiv.org/pdf/2511.18076",
        "title": "Reinforcement Learning for Portfolio Optimization with a Financial Goal and Defined Time Horizons",
        "authors": [
            "Fermat Leukam",
            "Rock Stephane Koffi",
            "Prudence Djagba"
        ],
        "comments": "",
        "subjects": "Portfolio Management (q-fin.PM); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "This research proposes an enhancement to the innovative portfolio optimization approach using the G-Learning algorithm, combined with parametric optimization via the GIRL algorithm (G-learning approach to the setting of Inverse Reinforcement Learning) as presented by. The goal is to maximize portfolio value by a target date while minimizing the investor's periodic contributions. Our model operates in a highly volatile market with a well-diversified portfolio, ensuring a low-risk level for the investor, and leverages reinforcement learning to dynamically adjust portfolio positions over time. Results show that we improved the Sharpe Ratio from 0.42, as suggested by recent studies using the same approach, to a value of 0.483 a notable achievement in highly volatile markets with diversified portfolios. The comparison between G-Learning and GIRL reveals that while GIRL optimizes the reward function parameters (e.g., lambda = 0.0012 compared to 0.002), its impact on portfolio performance remains marginal. This suggests that reinforcement learning methods, like G-Learning, already enable robust optimization. This research contributes to the growing development of reinforcement learning applications in financial decision-making, demonstrating that probabilistic learning algorithms can effectively align portfolio management strategies with investor needs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 198,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18098",
        "abs_url": "https://arxiv.org/abs/2511.18098",
        "pdf_url": "https://arxiv.org/pdf/2511.18098",
        "title": "Towards Harnessing the Power of LLMs for ABAC Policy Mining",
        "authors": [
            "More Aayush Babasaheb",
            "Shamik Sural"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
        "abstract": "This paper presents an empirical investigation into the capabilities of Large Language Models (LLMs) to perform automated Attribute-based Access Control (ABAC) policy mining. While ABAC provides fine-grained, context-aware access management, the increasing number and complexity of access policies can make their formulation and evaluation rather challenging. To address the task of synthesizing concise yet accurate policies, we evaluate the performance of some of the state-of-the-art LLMs, specifically Google Gemini (Flash and Pro) and OpenAI ChatGPT, as potential policy mining engines. An experimental framework was developed in Python to generate randomized access data parameterized by varying numbers of subjects, objects, and initial policy sets. The baseline policy sets, which govern permission decisions between subjects and objects, serve as the ground truth for comparison. Each LLM-generated policy was evaluated against the baseline policy using standard performance metrics. The results indicate that LLMs can effectively infer compact and valid ABAC policies for small-scale scenarios. However, as the system size increases, characterized by higher numbers of subjects and objects, LLM outputs exhibit declining accuracy and precision, coupled with significant increase in the size of policy generated, which is beyond the optimal size. These findings highlight both the promise and limitations of current LLM architectures for scalable policy mining in access control domains. Future work will explore hybrid approaches that combine prompt optimization with classical rule mining algorithms to improve scalability and interpretability in complex ABAC environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 199,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18123",
        "abs_url": "https://arxiv.org/abs/2511.18123",
        "pdf_url": "https://arxiv.org/pdf/2511.18123",
        "title": "Bias Is a Subspace, Not a Coordinate: A Geometric Rethinking of Post-hoc Debiasing in Vision-Language Models",
        "authors": [
            "Dachuan Zhao",
            "Weiyue Li",
            "Zhenda Shen",
            "Yushu Qiu",
            "Bowen Xu",
            "Haoyu Chen",
            "Yongchao Chen"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Vision-Language Models (VLMs) have become indispensable for multimodal reasoning, yet their representations often encode and amplify demographic biases, resulting in biased associations and misaligned predictions in downstream tasks. Such behavior undermines fairness and distorts the intended alignment between vision and language. Recent post-hoc approaches attempt to mitigate bias by replacing the most attribute-correlated embedding coordinates with neutral values. However, our systematic analysis reveals three critical failures of this coordinate-wise approach: feature entanglement, poor cross-dataset generalization, and incomplete bias removal. We find that bias is not localized to a few coordinates but is instead distributed across a few linear subspaces. To address these limitations, we propose $\\textbf{S}$ubspace $\\textbf{P}$rojection $\\textbf{D}$ebiasing ($\\textbf{SPD}$), a geometrically principled framework that identifies and removes the entire subspace of linearly decodable bias while reinserting a neutral mean component to preserve semantic fidelity. Extensive experiments across zero-shot classification, text-to-image retrieval, and image generation validate the effectiveness of SPD: our method achieves more robust debiasing with an average improvement of $18.5\\%$ across four fairness metrics, while maintaining minimal loss in task performance compared to the best debiasing baseline.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 200,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18141",
        "abs_url": "https://arxiv.org/abs/2511.18141",
        "pdf_url": "https://arxiv.org/pdf/2511.18141",
        "title": "Conformal Prediction for Compositional Data",
        "authors": [
            "Lucas P. Amaral",
            "Luben M. C. Cabezas",
            "Thiago R. Ramos",
            "Gustavo H. G. A. Pereira"
        ],
        "comments": "18 pages, 4 figures",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "In this work, we propose a set of conformal prediction procedures tailored to compositional responses, where outcomes are proportions that must be positive and sum to one. Building on Dirichlet regression, we introduce a split conformal approach based on quantile residuals and a highest-density region strategy that combines a fast coordinate-floor approximation with an internal grid refinement to restore sharpness. Both constructions are model-agnostic at the conformal layer and guarantee finite-sample marginal coverage under exchangeability, while respecting the geometry of the simplex. A comprehensive Monte Carlo study spanning homoscedastic and heteroscedastic designs shows that the quantile residual and grid-refined HDR methods achieve empirical coverage close to the nominal 90\\% level and produce substantially narrower regions than the coordinate-floor approximation, which tends to be conservative. We further demonstrate the methods on household budget shares from the BudgetItaly dataset, using standardized socioeconomic and price covariates with a train, calibration, and test split. In this application, the grid-refined HDR attains coverage closest to the target with the smallest average widths, closely followed by the quantile residual approach, while the simple triangular HDR yields wider, less informative sets. Overall, the results indicate that conformal prediction on the simplex can be both calibrated and efficient, providing practical uncertainty quantification for compositional prediction tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 201,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18153",
        "abs_url": "https://arxiv.org/abs/2511.18153",
        "pdf_url": "https://arxiv.org/pdf/2511.18153",
        "title": "A Coordinated Dual-Arm Framework for Delicate Snap-Fit Assemblies",
        "authors": [
            "Shreyas Kumar",
            "Barat S",
            "Debojit Das",
            "Yug Desai",
            "Siddhi Jain",
            "Rajesh Kumar",
            "Harish J. Palanthandalam-Madapusi"
        ],
        "comments": "10 pages, 9 figures",
        "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
        "abstract": "Delicate snap-fit assemblies, such as inserting a lens into an eye-wear frame or during electronics assembly, demand timely engagement detection and rapid force attenuation to prevent overshoot-induced component damage or assembly failure. We address these challenges with two key contributions. First, we introduce SnapNet, a lightweight neural network that detects snap-fit engagement from joint-velocity transients in real-time, showing that reliable detection can be achieved using proprioceptive signals without external sensors. Second, we present a dynamical-systems-based dual-arm coordination framework that integrates SnapNet driven detection with an event-triggered impedance modulation, enabling accurate alignment and compliant insertion during delicate snap-fit assemblies. Experiments across diverse geometries on a heterogeneous bimanual platform demonstrate high detection accuracy (over 96% recall) and up to a 30% reduction in peak impact forces compared to standard impedance control.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 202,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18167",
        "abs_url": "https://arxiv.org/abs/2511.18167",
        "pdf_url": "https://arxiv.org/pdf/2511.18167",
        "title": "Sparse Polyak with optimal thresholding operators for high-dimensional M-estimation",
        "authors": [
            "Tianqi Qiao",
            "Marie Maros"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Optimization and Control (math.OC)",
        "abstract": "We propose and analyze a variant of Sparse Polyak for high dimensional M-estimation problems. Sparse Polyak proposes a novel adaptive step-size rule tailored to suitably estimate the problem's curvature in the high-dimensional setting, guaranteeing that the algorithm's performance does not deteriorate when the ambient dimension increases. However, convergence guarantees can only be obtained by sacrificing solution sparsity and statistical accuracy. In this work, we introduce a variant of Sparse Polyak that retains its desirable scaling properties with respect to the ambient dimension while obtaining sparser and more accurate solutions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 203,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18199",
        "abs_url": "https://arxiv.org/abs/2511.18199",
        "pdf_url": "https://arxiv.org/pdf/2511.18199",
        "title": "Improving Forecasts of Suicide Attempts for Patients with Little Data",
        "authors": [
            "Genesis Hang",
            "Annie Chen",
            "Hope Neveux",
            "Matthew K. Nock",
            "Yaniv Yacoby"
        ],
        "comments": "Accepted at the TS4H Workshop at NeurIPS 2025",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Ecological Momentary Assessment provides real-time data on suicidal thoughts and behaviors, but predicting suicide attempts remains challenging due to their rarity and patient heterogeneity. We show that single models fit to all patients perform poorly, while individualized models improve performance but still overfit to patients with limited data. To address this, we introduce Latent Similarity Gaussian Processes (LSGPs) to capture patient heterogeneity, enabling those with little data to leverage similar patients' trends. Preliminary results show promise: even without kernel-design, we outperform all but one baseline while offering a new understanding of patient similarity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 204,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18207",
        "abs_url": "https://arxiv.org/abs/2511.18207",
        "pdf_url": "https://arxiv.org/pdf/2511.18207",
        "title": "ProHD: Projection-Based Hausdorff Distance Approximation",
        "authors": [
            "Jiuzhou Fu",
            "Luanzheng Guo",
            "Nathan R. Tallent",
            "Dongfang Zhao"
        ],
        "comments": "",
        "subjects": "Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "The Hausdorff distance (HD) is a robust measure of set dissimilarity, but computing it exactly on large, high-dimensional datasets is prohibitively expensive. We propose \\textbf{ProHD}, a projection-guided approximation algorithm that dramatically accelerates HD computation while maintaining high accuracy. ProHD identifies a small subset of candidate \"extreme\" points by projecting the data onto a few informative directions (such as the centroid axis and top principal components) and computing the HD on this subset. This approach guarantees an underestimate of the true HD with a bounded additive error and typically achieves results within a few percent of the exact value. In extensive experiments on image, physics, and synthetic datasets (up to two million points in $D=256$), ProHD runs 10--100$\\times$ faster than exact algorithms while attaining 5--20$\\times$ lower error than random sampling-based approximations. Our method enables practical HD calculations in scenarios like large vector databases and streaming data, where quick and reliable set distance estimation is needed.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 205,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18213",
        "abs_url": "https://arxiv.org/abs/2511.18213",
        "pdf_url": "https://arxiv.org/pdf/2511.18213",
        "title": "Typing Reinvented: Towards Hands-Free Input via sEMG",
        "authors": [
            "Kunwoo Lee",
            "Dhivya Sreedhar",
            "Pushkar Saraf",
            "Chaeeun Lee",
            "Kateryna Shapovalenko"
        ],
        "comments": "",
        "subjects": "Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)",
        "abstract": "We explore surface electromyography (sEMG) as a non-invasive input modality for mapping muscle activity to keyboard inputs, targeting immersive typing in next-generation human-computer interaction (HCI). This is especially relevant for spatial computing and virtual reality (VR), where traditional keyboards are impractical. Using attention-based architectures, we significantly outperform the existing convolutional baselines, reducing online generic CER from 24.98% -> 20.34% and offline personalized CER from 10.86% -> 10.10%, while remaining fully causal. We further incorporate a lightweight decoding pipeline with language-model-based correction, demonstrating the feasibility of accurate, real-time muscle-driven text input for future wearable and spatial interfaces.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 206,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18258",
        "abs_url": "https://arxiv.org/abs/2511.18258",
        "pdf_url": "https://arxiv.org/pdf/2511.18258",
        "title": "Hybrid Agentic AI and Multi-Agent Systems in Smart Manufacturing",
        "authors": [
            "Mojtaba A. Farahani",
            "Md Irfan Khan",
            "Thorsten Wuest"
        ],
        "comments": "",
        "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "The convergence of Agentic AI and MAS enables a new paradigm for intelligent decision making in SMS. Traditional MAS architectures emphasize distributed coordination and specialized autonomy, while recent advances in agentic AI driven by LLMs introduce higher order reasoning, planning, and tool orchestration capabilities. This paper presents a hybrid agentic AI and multi agent framework for a Prescriptive Maintenance use case, where LLM based agents provide strategic orchestration and adaptive reasoning, complemented by rule based and SLMs agents performing efficient, domain specific tasks on the edge. The proposed framework adopts a layered architecture that consists of perception, preprocessing, analytics, and optimization layers, coordinated through an LLM Planner Agent that manages workflow decisions and context retention. Specialized agents autonomously handle schema discovery, intelligent feature analysis, model selection, and prescriptive optimization, while a HITL interface ensures transparency and auditability of generated maintenance recommendations. This hybrid design supports dynamic model adaptation, cost efficient maintenance scheduling, and interpretable decision making. An initial proof of concept implementation is validated on two industrial manufacturing datasets. The developed framework is modular and extensible, supporting seamless integration of new agents or domain modules as capabilities evolve. The results demonstrate the system capability to automatically detect schema, adapt preprocessing pipelines, optimize model performance through adaptive intelligence, and generate actionable, prioritized maintenance recommendations. The framework shows promise in achieving improved robustness, scalability, and explainability for RxM in smart manufacturing, bridging the gap between high level agentic reasoning and low level autonomous execution.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 207,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18313",
        "abs_url": "https://arxiv.org/abs/2511.18313",
        "pdf_url": "https://arxiv.org/pdf/2511.18313",
        "title": "Path-Constrained Retrieval: A Structural Approach to Reliable LLM Agent Reasoning Through Graph-Scoped Semantic Search",
        "authors": [
            "Joseph Oladokun"
        ],
        "comments": "10 pages",
        "subjects": "Computation and Language (cs.CL); Databases (cs.DB); Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "Large Language Model agents often retrieve context from knowledge bases that lack structural consistency with the agent's current reasoning state, leading to incoherent reasoning chains. We introduce Path-Constrained Retrieval (PCR), a retrieval method that combines structural graph constraints with semantic search to ensure retrieved information maintains logical relationships within a knowledge graph. PCR restricts the search space to nodes reachable from an anchor node, preventing retrieval of structurally disconnected information that may lead to inconsistent reasoning. We evaluate PCR on PathRAG-6, a benchmark spanning six domains with 180 nodes and 360 edges. Our results show that PCR achieves full structural consistency compared to 24-32 percent in baseline methods, while maintaining strong relevance scores. On the technology domain, PCR obtains full relevance at rank 10 with full structural consistency, significantly outperforming vector search and hybrid retrieval. PCR reduces the average graph distance of retrieved context by 78 percent compared to baselines, demonstrating retrieval of more structurally consistent information. These findings suggest that path-constrained retrieval is an effective approach for improving the reliability and coherence of LLM agent reasoning systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 208,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18322",
        "abs_url": "https://arxiv.org/abs/2511.18322",
        "pdf_url": "https://arxiv.org/pdf/2511.18322",
        "title": "Learning Visually Interpretable Oscillator Networks for Soft Continuum Robots from Video",
        "authors": [
            "Henrik Krauss",
            "Johann Licher",
            "Naoya Takeishi",
            "Annika Raatz",
            "Takehisa Yairi"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Data-driven learning of soft continuum robot (SCR) dynamics from high-dimensional observations offers flexibility but often lacks physical interpretability, while model-based approaches require prior knowledge and can be computationally expensive. We bridge this gap by introducing (1) the Attention Broadcast Decoder (ABCD), a plug-and-play module for autoencoder-based latent dynamics learning that generates pixel-accurate attention maps localizing each latent dimension's contribution while filtering static backgrounds. (2) By coupling these attention maps to 2D oscillator networks, we enable direct on-image visualization of learned dynamics (masses, stiffness, and forces) without prior knowledge. We validate our approach on single- and double-segment SCRs, demonstrating that ABCD-based models significantly improve multi-step prediction accuracy: 5.7x error reduction for Koopman operators and 3.5x for oscillator networks on the two-segment robot. The learned oscillator network autonomously discovers a chain structure of oscillators. Unlike standard methods, ABCD models enable smooth latent space extrapolation beyond training data. This fully data-driven approach yields compact, physically interpretable models suitable for control applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 209,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18323",
        "abs_url": "https://arxiv.org/abs/2511.18323",
        "pdf_url": "https://arxiv.org/pdf/2511.18323",
        "title": "Crash-Consistent Checkpointing for AI Training on macOS/APFS",
        "authors": [
            "Juha Jeon"
        ],
        "comments": "18 pages, 6 figures. Independent mini-research report; not submitted to a conference or journal",
        "subjects": "Operating Systems (cs.OS); Machine Learning (cs.LG)",
        "abstract": "Deep learning training relies on periodic checkpoints to recover from failures, but unsafe checkpoint installation can leave corrupted files on disk. This paper presents an experimental study of checkpoint installation protocols and integrity validation for AI training on macOS/APFS. We implement three write modes with increasing durability guarantees: unsafe (baseline, no fsync), atomic_nodirsync (file-level durability via fsync()), and atomic_dirsync (file + directory durability). We design a format-agnostic integrity guard using SHA-256 checksums with automatic rollback. Through controlled experiments including crash injection (430 unsafe-mode trials) and corruption injection (1,600 atomic-mode trials), we demonstrate that the integrity guard detects 99.8-100% of corruptions with zero false positives. Performance overhead is 56.5-108.4% for atomic_nodirsync and 84.2-570.6% for atomic_dirsync relative to the unsafe baseline. Our findings quantify the reliability-performance trade-offs and provide deployment guidance for production AI infrastructure.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 210,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18325",
        "abs_url": "https://arxiv.org/abs/2511.18325",
        "pdf_url": "https://arxiv.org/pdf/2511.18325",
        "title": "Brain-MGF: Multimodal Graph Fusion Network for EEG-fMRI Brain Connectivity Analysis Under Psilocybin",
        "authors": [
            "Sin-Yee Yap",
            "Fuad Noman",
            "Junn Yong Loo",
            "Devon Stoliker",
            "Moein Khajehnejad",
            "Raphaël C.-W. Phan",
            "David L. Dowe",
            "Adeel Razi",
            "Chee-Ming Ting"
        ],
        "comments": "5 pages",
        "subjects": "Neurons and Cognition (q-bio.NC); Machine Learning (cs.LG)",
        "abstract": "Psychedelics, such as psilocybin, reorganise large-scale brain connectivity, yet how these changes are reflected across electrophysiological (electroencephalogram, EEG) and haemodynamic (functional magnetic resonance imaging, fMRI) networks remains unclear. We present Brain-MGF, a multimodal graph fusion network for joint EEG-fMRI connectivity analysis. For each modality, we construct graphs with partial-correlation edges and Pearson-profile node features, and learn subject-level embeddings via graph convolution. An adaptive softmax gate then fuses modalities with sample-specific weights to capture context-dependent contributions. Using the world's largest single-site psilocybin dataset, PsiConnect, Brain-MGF distinguishes psilocybin from no-psilocybin conditions in meditation and rest. Fusion improves over unimodal and non-adaptive variants, achieving 74.0% accuracy and 76.5% F1 score on meditation, and 76.0% accuracy with 85.8% ROC-AUC on rest. UMAP visualisations reveal clearer class separation for fused embeddings. These results indicate that adaptive graph fusion effectively integrates complementary EEG-fMRI information, providing an interpretable framework for characterising psilocybin-induced alterations in large-scale neural organisation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 211,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18364",
        "abs_url": "https://arxiv.org/abs/2511.18364",
        "pdf_url": "https://arxiv.org/pdf/2511.18364",
        "title": "KGpipe: Generation and Evaluation of Pipelines for Data Integration into Knowledge Graphs",
        "authors": [
            "Marvin Hofer",
            "Erhard Rahm"
        ],
        "comments": "15 KG pipelines (9 single source, 6 multi source)",
        "subjects": "Artificial Intelligence (cs.AI); Databases (cs.DB); Machine Learning (cs.LG)",
        "abstract": "Building high-quality knowledge graphs (KGs) from diverse sources requires combining methods for information extraction, data transformation, ontology mapping, entity matching, and data fusion. Numerous methods and tools exist for each of these tasks, but support for combining them into reproducible and effective end-to-end pipelines is still lacking. We present a new framework, KGpipe for defining and executing integration pipelines that can combine existing tools or LLM (Large Language Model) functionality. To evaluate different pipelines and the resulting KGs, we propose a benchmark to integrate heterogeneous data of different formats (RDF, JSON, text) into a seed KG. We demonstrate the flexibility of KGpipe by running and comparatively evaluating several pipelines integrating sources of the same or different formats using selected performance and quality metrics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 212,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18421",
        "abs_url": "https://arxiv.org/abs/2511.18421",
        "pdf_url": "https://arxiv.org/pdf/2511.18421",
        "title": "DHAuDS: A Dynamic and Heterogeneous Audio Benchmark for Test-Time Adaptation",
        "authors": [
            "Weichuang Shao",
            "Iman Yi Liao",
            "Tomas Henrique Bode Maul",
            "Tissa Chandesa"
        ],
        "comments": "",
        "subjects": "Sound (cs.SD); Machine Learning (cs.LG)",
        "abstract": "Audio classifiers frequently face domain shift, when models trained on one dataset lose accuracy on data recorded in acoustically different conditions. Previous Test-Time Adaptation (TTA) research in speech and sound analysis often evaluates models under fixed or mismatched noise settings, that fail to mimic real-world variability. To overcome these limitations, this paper presents DHAuDS (Dynamic and Heterogeneous Audio Domain Shift), a benchmark designed to assess TTA approaches under more realistic and diverse acoustic shifts. DHAuDS comprises four standardized benchmarks: UrbanSound8K-C, SpeechCommandsV2-C, VocalSound-C, and ReefSet-C, each constructed with dynamic corruption severity levels and heterogeneous noise types to simulate authentic audio degradation scenarios. The framework defines 14 evaluation criteria for each benchmark (8 for UrbanSound8K-C), resulting in 50 unrepeated criteria (124 experiments) that collectively enable fair, reproducible, and cross-domain comparison of TTA algorithms. Through the inclusion of dynamic and mixed-domain noise settings, DHAuDS offers a consistent and publicly reproducible testbed to support ongoing studies in robust and adaptive audio modeling.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 213,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18423",
        "abs_url": "https://arxiv.org/abs/2511.18423",
        "pdf_url": "https://arxiv.org/pdf/2511.18423",
        "title": "General Agentic Memory Via Deep Research",
        "authors": [
            "B.Y. Yan",
            "Chaofan Li",
            "Hongjin Qian",
            "Shuqi Lu",
            "Zheng Liu"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called \\textbf{general agentic memory (GAM)}. GAM follows the principle of \"\\textbf{just-in time (JIT) compilation}\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) \\textbf{Memorizer}, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) \\textbf{Researcher}, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 214,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18464",
        "abs_url": "https://arxiv.org/abs/2511.18464",
        "pdf_url": "https://arxiv.org/pdf/2511.18464",
        "title": "Reliable Selection of Heterogeneous Treatment Effect Estimators",
        "authors": [
            "Jiayi Guo",
            "Zijun Gao"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "We study the problem of selecting the best heterogeneous treatment effect (HTE) estimator from a collection of candidates in settings where the treatment effect is fundamentally unobserved. We cast estimator selection as a multiple testing problem and introduce a ground-truth-free procedure based on a cross-fitted, exponentially weighted test statistic. A key component of our method is a two-way sample splitting scheme that decouples nuisance estimation from weight learning and ensures the stability required for valid inference. Leveraging a stability-based central limit theorem, we establish asymptotic familywise error rate control under mild regularity conditions. Empirically, our procedure provides reliable error control while substantially reducing false selections compared with commonly used methods across ACIC 2016, IHDP, and Twins benchmarks, demonstrating that our method is feasible and powerful even without ground-truth treatment effects.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 215,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18530",
        "abs_url": "https://arxiv.org/abs/2511.18530",
        "pdf_url": "https://arxiv.org/pdf/2511.18530",
        "title": "Transforming Conditional Density Estimation Into a Single Nonparametric Regression Task",
        "authors": [
            "Alexander G. Reisach",
            "Olivier Collier",
            "Alex Luedtke",
            "Antoine Chambaz"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "We propose a way of transforming the problem of conditional density estimation into a single nonparametric regression task via the introduction of auxiliary samples. This allows leveraging regression methods that work well in high dimensions, such as neural networks and decision trees. Our main theoretical result characterizes and establishes the convergence of our estimator to the true conditional density in the data limit. We develop condensité, a method that implements this approach. We demonstrate the benefit of the auxiliary samples on synthetic data and showcase that condensité can achieve good out-of-the-box results. We evaluate our method on a large population survey dataset and on a satellite imaging dataset. In both cases, we find that condensité matches or outperforms the state of the art and yields conditional densities in line with established findings in the literature on each dataset. Our contribution opens up new possibilities for regression-based conditional density estimation and the empirical results indicate strong promise for applied research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 216,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18554",
        "abs_url": "https://arxiv.org/abs/2511.18554",
        "pdf_url": "https://arxiv.org/pdf/2511.18554",
        "title": "Online Smoothed Demand Management",
        "authors": [
            "Adam Lechowicz",
            "Nicolas Christianson",
            "Mohammad Hajiesmaili",
            "Adam Wierman",
            "Prashant Shenoy"
        ],
        "comments": "69 pages, 12 figures",
        "subjects": "Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG); Systems and Control (eess.SY)",
        "abstract": "We introduce and study a class of online problems called online smoothed demand management $(\\texttt{OSDM})$, motivated by paradigm shifts in grid integration and energy storage for large energy consumers such as data centers. In $\\texttt{OSDM}$, an operator makes two decisions at each time step: an amount of energy to be purchased, and an amount of energy to be delivered (i.e., used for computation). The difference between these decisions charges (or discharges) the operator's energy storage (e.g., a battery). Two types of demand arrive online: base demand, which must be covered at the current time, and flexible demand, which can be satisfied at any time steps before a demand-specific deadline $\\Delta_t$. The operator's goal is to minimize a cost (subject to the constraints above) that combines a cost of purchasing energy, a cost for delivering energy (if applicable), and smoothness penalties on the purchasing and delivery rates to discourage fluctuations and encourage ``grid healthy'' decisions. $\\texttt{OSDM}$ generalizes several problems in the online algorithms literature while being the first to fully model applications of interest. We propose a competitive algorithm called $\\texttt{PAAD}$ (partitioned accounting \\& aggregated decisions) and show it achieves the optimal competitive ratio. To overcome the pessimism typical of worst-case analysis, we also propose a novel learning framework that provides guarantees on the worst-case competitive ratio (i.e., to provide robustness against nonstationarity) while allowing end-to-end differentiable learning of the best algorithm on historical instances of the problem. We evaluate our algorithms in a case study of a grid-integrated data center with battery storage, showing that $\\texttt{PAAD}$ effectively solves the problem and end-to-end learning achieves substantial performance improvements compared to $\\texttt{PAAD}$.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 217,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18555",
        "abs_url": "https://arxiv.org/abs/2511.18555",
        "pdf_url": "https://arxiv.org/pdf/2511.18555",
        "title": "A joint optimization approach to identifying sparse dynamics using least squares kernel collocation",
        "authors": [
            "Alexander W. Hsu",
            "Ike W. Griss Salas",
            "Jacob M. Stevens-Haas",
            "J. Nathan Kutz",
            "Aleksandr Aravkin",
            "Bamdad Hosseini"
        ],
        "comments": "",
        "subjects": "Methodology (stat.ME); Machine Learning (cs.LG); Dynamical Systems (math.DS); Machine Learning (stat.ML)",
        "abstract": "We develop an all-at-once modeling framework for learning systems of ordinary differential equations (ODE) from scarce, partial, and noisy observations of the states. The proposed methodology amounts to a combination of sparse recovery strategies for the ODE over a function library combined with techniques from reproducing kernel Hilbert space (RKHS) theory for estimating the state and discretizing the ODE. Our numerical experiments reveal that the proposed strategy leads to significant gains in terms of accuracy, sample efficiency, and robustness to noise, both in terms of learning the equation and estimating the unknown states. This work demonstrates capabilities well beyond existing and widely used algorithms while extending the modeling flexibility of other recent developments in equation discovery.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 218,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18562",
        "abs_url": "https://arxiv.org/abs/2511.18562",
        "pdf_url": "https://arxiv.org/pdf/2511.18562",
        "title": "Ensuring Calibration Robustness in Split Conformal Prediction Under Adversarial Attacks",
        "authors": [
            "Xunlei Qian",
            "Yue Xing"
        ],
        "comments": "Submitted to AISTATS 2026",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Conformal prediction (CP) provides distribution-free, finite-sample coverage guarantees but critically relies on exchangeability, a condition often violated under distribution shift. We study the robustness of split conformal prediction under adversarial perturbations at test time, focusing on both coverage validity and the resulting prediction set size. Our theoretical analysis characterizes how the strength of adversarial perturbations during calibration affects coverage guarantees under adversarial test conditions. We further examine the impact of adversarial training at the model-training stage. Extensive experiments support our theory: (i) Prediction coverage varies monotonically with the calibration-time attack strength, enabling the use of nonzero calibration-time attack to predictably control coverage under adversarial tests; (ii) target coverage can hold over a range of test-time attacks: with a suitable calibration attack, coverage stays within any chosen tolerance band across a contiguous set of perturbation levels; and (iii) adversarial training at the training stage produces tighter prediction sets that retain high informativeness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 219,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18583",
        "abs_url": "https://arxiv.org/abs/2511.18583",
        "pdf_url": "https://arxiv.org/pdf/2511.18583",
        "title": "Differential privacy with dependent data",
        "authors": [
            "Valentin Roth",
            "Marco Avella-Medina"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Statistics Theory (math.ST)",
        "abstract": "Dependent data underlies many statistical studies in the social and health sciences, which often involve sensitive or private information. Differential privacy (DP) and in particular \\textit{user-level} DP provide a natural formalization of privacy requirements for processing dependent data where each individual provides multiple observations to the dataset. However, dependence introduced, e.g., through repeated measurements challenges the existing statistical theory under DP-constraints. In \\iid{} settings, noisy Winsorized mean estimators have been shown to be minimax optimal for standard (\\textit{item-level}) and \\textit{user-level} DP estimation of a mean $\\mu \\in \\R^d$. Yet, their behavior on potentially dependent observations has not previously been studied. We fill this gap and show that Winsorized mean estimators can also be used under dependence for bounded and unbounded data, and can lead to asymptotic and finite sample guarantees that resemble their \\iid{} counterparts under a weak notion of dependence. For this, we formalize dependence via log-Sobolev inequalities on the joint distribution of observations. This enables us to adapt the stable histogram by Karwa and Vadhan (2018) to a non-\\iid{} setting, which we then use to estimate the private projection intervals of the Winsorized estimator. The resulting guarantees for our item-level mean estimator extend to \\textit{user-level} mean estimation and transfer to the local model via a randomized response histogram. Using the mean estimators as building blocks, we provide extensions to random effects models, longitudinal linear regression and nonparametric regression. Therefore, our work constitutes a first step towards a systematic study of DP for dependent data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 220,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18590",
        "abs_url": "https://arxiv.org/abs/2511.18590",
        "pdf_url": "https://arxiv.org/pdf/2511.18590",
        "title": "From Simulations to Surveys: Domain Adaptation for Galaxy Observations",
        "authors": [
            "Kaley Brauer",
            "Aditya Prasad Dash",
            "Meet J. Vyas",
            "Ahmed Salim",
            "Stiven Briand Massala"
        ],
        "comments": "8 pages, 4 figures. Will be presented at NeurIPS 2025 ML4PS",
        "subjects": "Astrophysics of Galaxies (astro-ph.GA); Machine Learning (cs.LG)",
        "abstract": "Large photometric surveys will image billions of galaxies, but we currently lack quick, reliable automated ways to infer their physical properties like morphology, stellar mass, and star formation rates. Simulations provide galaxy images with ground-truth physical labels, but domain shifts in PSF, noise, backgrounds, selection, and label priors degrade transfer to real surveys. We present a preliminary domain adaptation pipeline that trains on simulated TNG50 galaxies and evaluates on real SDSS galaxies with morphology labels (elliptical/spiral/irregular). We train three backbones (CNN, $E(2)$-steerable CNN, ResNet-18) with focal loss and effective-number class weighting, and a feature-level domain loss $L_D$ built from GeomLoss (entropic Sinkhorn OT, energy distance, Gaussian MMD, and related metrics). We show that a combination of these losses with an OT-based \"top_$k$ soft matching\" loss that focuses $L_D$ on the worst-matched source-target pairs can further enhance domain alignment. With Euclidean distance, scheduled alignment weights, and top-$k$ matching, target accuracy (macro F1) rises from $\\sim$46% ($\\sim$30%) at no adaptation to $\\sim$87% ($\\sim$62.6%), with a domain AUC near 0.5, indicating strong latent-space mixing.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 221,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18594",
        "abs_url": "https://arxiv.org/abs/2511.18594",
        "pdf_url": "https://arxiv.org/pdf/2511.18594",
        "title": "Autoencoder for Position-Assisted Beam Prediction in mmWave ISAC Systems",
        "authors": [
            "Ahmad A. Aziz El-Banna",
            "Octavia A. Dobre"
        ],
        "comments": "",
        "subjects": "Signal Processing (eess.SP); Machine Learning (cs.LG)",
        "abstract": "Integrated sensing and communication and millimeter wave (mmWave) have emerged as pivotal technologies for 6G networks. However, the narrow nature of mmWave beams requires precise alignments that typically necessitate large training overhead. This overhead can be reduced by incorporating the position information with beam adjustments. This letter proposes a lightweight autorencoder (LAE) model that addresses the position-assisted beam prediction problem while significantly reducing computational complexity compared to the conventional baseline method, i.e., deep fully connected neural network. The proposed LAE is designed as a three-layer undercomplete network to exploit its dimensionality reduction capabilities and thereby mitigate the computational requirements of the trained model. Simulation results show that the proposed model achieves a similar beam prediction accuracy to the baseline with an 83% complexity reduction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 222,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18606",
        "abs_url": "https://arxiv.org/abs/2511.18606",
        "pdf_url": "https://arxiv.org/pdf/2511.18606",
        "title": "How to Train Your Latent Control Barrier Function: Smooth Safety Filtering Under Hard-to-Model Constraints",
        "authors": [
            "Kensuke Nakamura",
            "Arun L. Bishop",
            "Steven Man",
            "Aaron M. Johnson",
            "Zachary Manchester",
            "Andrea Bajcsy"
        ],
        "comments": "3 figures, 10 tables, 22 pages",
        "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
        "abstract": "Latent safety filters extend Hamilton-Jacobi (HJ) reachability to operate on latent state representations and dynamics learned directly from high-dimensional observations, enabling safe visuomotor control under hard-to-model constraints. However, existing methods implement \"least-restrictive\" filtering that discretely switch between nominal and safety policies, potentially undermining the task performance that makes modern visuomotor policies valuable. While reachability value functions can, in principle, be adapted to be control barrier functions (CBFs) for smooth optimization-based filtering, we theoretically and empirically show that current latent-space learning methods produce fundamentally incompatible value functions. We identify two sources of incompatibility: First, in HJ reachability, failures are encoded via a \"margin function\" in latent space, whose sign indicates whether or not a latent is in the constraint set. However, representing the margin function as a classifier yields saturated value functions that exhibit discontinuous jumps. We prove that the value function's Lipschitz constant scales linearly with the margin function's Lipschitz constant, revealing that smooth CBFs require smooth margins. Second, reinforcement learning (RL) approximations trained solely on safety policy data yield inaccurate value estimates for nominal policy actions, precisely where CBF filtering needs them. We propose the LatentCBF, which addresses both challenges through gradient penalties that lead to smooth margin functions without additional labeling, and a value-training procedure that mixes data from both nominal and safety policy distributions. Experiments on simulated benchmarks and hardware with a vision-based manipulation policy demonstrate that LatentCBF enables smooth safety filtering while doubling the task-completion rate over prior switching methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 223,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18633",
        "abs_url": "https://arxiv.org/abs/2511.18633",
        "pdf_url": "https://arxiv.org/pdf/2511.18633",
        "title": "Bridging Philosophy and Machine Learning: A Structuralist Framework for Classifying Neural Network Representations",
        "authors": [
            "Yildiz Culcu"
        ],
        "comments": "7 pages, 1 figure, 1 table. Developed from the author's bachelor thesis but substantially revised and reformulated for research publication",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Machine learning models increasingly function as representational systems, yet the philosoph- ical assumptions underlying their internal structures remain largely unexamined. This paper develops a structuralist decision framework for classifying the implicit ontological commitments made in machine learning research on neural network representations. Using a modified PRISMA protocol, a systematic review of the last two decades of literature on representation learning and interpretability is conducted. Five influential papers are analysed through three hierarchical criteria derived from structuralist philosophy of science: entity elimination, source of structure, and mode of existence. The results reveal a pronounced tendency toward structural idealism, where learned representations are treated as model-dependent constructions shaped by architec- ture, data priors, and training dynamics. Eliminative and non-eliminative structuralist stances appear selectively, while structural realism is notably absent. The proposed framework clarifies conceptual tensions in debates on interpretability, emergence, and epistemic trust in machine learning, and offers a rigorous foundation for future interdisciplinary work between philosophy of science and machine learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 224,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18651",
        "abs_url": "https://arxiv.org/abs/2511.18651",
        "pdf_url": "https://arxiv.org/pdf/2511.18651",
        "title": "Lean 5.0: A Predictive, Human-AI, and Ethically Grounded Paradigm for Construction Management",
        "authors": [
            "Atena Khoshkonesh",
            "Mohsen Mohammadagha",
            "Navid Ebrahimi",
            "Narges Sadeghigolshan"
        ],
        "comments": "",
        "subjects": "Computational Engineering, Finance, and Science (cs.CE); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "This paper introduces Lean 5.0, a human-centric evolution of Lean-Digital integration that connects predictive analytics, AI collaboration, and continuous learning within Industry 5.0 and Construction 5.0 contexts. A systematic literature review (2019-2024) and a 12-week empirical validation study demonstrate measurable performance gains, including a 13% increase in Plan Percent Complete (PPC), 22% reduction in rework, and 42% improvement in forecast accuracy. The study adopts a mixed-method Design Science Research (DSR) approach aligned with PRISMA 2020 guidelines. The paper also examines integration with digital twin and blockchain technologies to improve traceability, auditability, and lifecycle transparency. Despite limitations related to sample size, single-case design, and study duration, the findings show that Lean 5.0 provides a transformative paradigm connecting human cognition with predictive control in construction management.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 225,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18661",
        "abs_url": "https://arxiv.org/abs/2511.18661",
        "pdf_url": "https://arxiv.org/pdf/2511.18661",
        "title": "Fast Escape, Slow Convergence: Learning Dynamics of Phase Retrieval under Power-Law Data",
        "authors": [
            "Guillaume Braun",
            "Bruno Loureiro",
            "Ha Quang Minh",
            "Masaaki Imaizumi"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Scaling laws describe how learning performance improves with data, compute, or training time, and have become a central theme in modern deep learning. We study this phenomenon in a canonical nonlinear model: phase retrieval with anisotropic Gaussian inputs whose covariance spectrum follows a power law. Unlike the isotropic case, where dynamics collapse to a two-dimensional system, anisotropy yields a qualitatively new regime in which an infinite hierarchy of coupled equations governs the evolution of the summary statistics. We develop a tractable reduction that reveals a three-phase trajectory: (i) fast escape from low alignment, (ii) slow convergence of the summary statistics, and (iii) spectral-tail learning in low-variance directions. From this decomposition, we derive explicit scaling laws for the mean-squared error, showing how spectral decay dictates convergence times and error curves. Experiments confirm the predicted phases and exponents. These results provide the first rigorous characterization of scaling laws in nonlinear regression with anisotropic data, highlighting how anisotropy reshapes learning dynamics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 226,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18667",
        "abs_url": "https://arxiv.org/abs/2511.18667",
        "pdf_url": "https://arxiv.org/pdf/2511.18667",
        "title": "Equivariant Deep Equilibrium Models for Imaging Inverse Problems",
        "authors": [
            "Alexander Mehta",
            "Ruangrawee Kitichotkul",
            "Vivek K Goyal",
            "Julián Tachella"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Machine Learning (cs.LG); Signal Processing (eess.SP)",
        "abstract": "Equivariant imaging (EI) enables training signal reconstruction models without requiring ground truth data by leveraging signal symmetries. Deep equilibrium models (DEQs) are a powerful class of neural networks where the output is a fixed point of a learned operator. However, training DEQs with complex EI losses requires implicit differentiation through fixed-point computations, whose implementation can be challenging. We show that backpropagation can be implemented modularly, simplifying training. Experiments demonstrate that DEQs trained with implicit differentiation outperform those trained with Jacobian-free backpropagation and other baseline methods. Additionally, we find evidence that EI-trained DEQs approximate the proximal map of an invariant prior.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 227,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18674",
        "abs_url": "https://arxiv.org/abs/2511.18674",
        "pdf_url": "https://arxiv.org/pdf/2511.18674",
        "title": "Low-Rank GEMM: Efficient Matrix Multiplication via Low-Rank Approximation with FP8 Acceleration",
        "authors": [
            "Alfredo Metere"
        ],
        "comments": "",
        "subjects": "Performance (cs.PF); Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)",
        "abstract": "Large matrix multiplication is a cornerstone of modern machine learning workloads, yet traditional approaches suffer from cubic computational complexity (e.g., $\\mathcal{O}(n^3)$ for a matrix of size $n\\times n$). We present Low-Rank GEMM, a novel approach that leverages low-rank matrix approximations to achieve sub-quadratic complexity while maintaining hardware-accelerated performance through FP8 precision and intelligent kernel selection. On a NVIDIA RTX 4090, our implementation achieves up to 378 TFLOPS on matrices up to $N=20480$, providing 75\\% memory savings and $7.8\\times$ speedup over PyTorch FP32 for large matrices. The system automatically adapts to hardware capabilities, selecting optimal decomposition methods (SVD, randomized SVD) and precision levels based on matrix characteristics and available accelerators. Comprehensive benchmarking on NVIDIA RTX 4090 demonstrates that Low-Rank GEMM becomes the fastest approach for matrices $N\\geq10240$, surpassing traditional cuBLAS implementations through memory bandwidth optimization rather than computational shortcuts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 228,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18699",
        "abs_url": "https://arxiv.org/abs/2511.18699",
        "pdf_url": "https://arxiv.org/pdf/2511.18699",
        "title": "Dendritic Convolution for Noise Image Recognition",
        "authors": [
            "Jiarui Xue",
            "Dongjian Yang",
            "Ye Sun",
            "Gang Liu"
        ],
        "comments": "11 pages, 8 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "In real-world scenarios of image recognition, there exists substantial noise interference. Existing works primarily focus on methods such as adjusting networks or training strategies to address noisy image recognition, and the anti-noise performance has reached a bottleneck. However, little is known about the exploration of anti-interference solutions from a neuronal this http URL paper proposes an anti-noise neuronal convolution. This convolution mimics the dendritic structure of neurons, integrates the neighborhood interaction computation logic of dendrites into the underlying design of convolutional operations, and simulates the XOR logic preprocessing function of biological dendrites through nonlinear interactions between input features, thereby fundamentally reconstructing the mathematical paradigm of feature extraction. Unlike traditional convolution where noise directly interferes with feature extraction and exerts a significant impact, DDC mitigates the influence of noise by focusing on the interaction of neighborhood information. Experimental results demonstrate that in image classification tasks (using YOLOv11-cls, VGG16, and EfficientNet-B0) and object detection tasks (using YOLOv11, YOLOv8, and YOLOv5), after replacing traditional convolution with the dendritic convolution, the accuracy of the EfficientNet-B0 model on noisy datasets is relatively improved by 11.23%, and the mean Average Precision (mAP) of YOLOv8 is increased by 19.80%. The consistency between the computation method of this convolution and the dendrites of biological neurons enables it to perform significantly better than traditional convolution in complex noisy environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 229,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18717",
        "abs_url": "https://arxiv.org/abs/2511.18717",
        "pdf_url": "https://arxiv.org/pdf/2511.18717",
        "title": "When and What to Recommend: Joint Modeling of Timing and Content for Active Sequential Recommendation",
        "authors": [
            "Jin Chai",
            "Xiaoxiao Ma",
            "Jian Yang",
            "Jia Wu"
        ],
        "comments": "10 pages, 5 figures. Submitted to arXiv",
        "subjects": "Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "Sequential recommendation models user preferences to predict the next target item. Most existing work is passive, where the system responds only when users open the application, missing chances after closure. We investigate active recommendation, which predicts the next interaction time and actively delivers items. Two challenges: accurately estimating the Time of Interest (ToI) and generating Item of Interest (IoI) conditioned on the predicted ToI. We propose PASRec, a diffusion-based framework that aligns ToI and IoI via a joint objective. Experiments on five benchmarks show superiority over eight state-of-the-art baselines under leave-one-out and temporal splits.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 230,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18742",
        "abs_url": "https://arxiv.org/abs/2511.18742",
        "pdf_url": "https://arxiv.org/pdf/2511.18742",
        "title": "ProxT2I: Efficient Reward-Guided Text-to-Image Generation via Proximal Diffusion",
        "authors": [
            "Zhenghan Fang",
            "Jian Zheng",
            "Qiaozi Gao",
            "Xiaofeng Gao",
            "Jeremias Sulam"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Diffusion models have emerged as a dominant paradigm for generative modeling across a wide range of domains, including prompt-conditional generation. The vast majority of samplers, however, rely on forward discretization of the reverse diffusion process and use score functions that are learned from data. Such forward and explicit discretizations can be slow and unstable, requiring a large number of sampling steps to produce good-quality samples. In this work we develop a text-to-image (T2I) diffusion model based on backward discretizations, dubbed ProxT2I, relying on learned and conditional proximal operators instead of score functions. We further leverage recent advances in reinforcement learning and policy optimization to optimize our samplers for task-specific rewards. Additionally, we develop a new large-scale and open-source dataset comprising 15 million high-quality human images with fine-grained captions, called LAION-Face-T2I-15M, for training and evaluation. Our approach consistently enhances sampling efficiency and human-preference alignment compared to score-based baselines, and achieves results on par with existing state-of-the-art and open-source text-to-image models while requiring lower compute and smaller model size, offering a lightweight yet performant solution for human text-to-image generation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 231,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18750",
        "abs_url": "https://arxiv.org/abs/2511.18750",
        "pdf_url": "https://arxiv.org/pdf/2511.18750",
        "title": "On Instability of Minimax Optimal Optimism-Based Bandit Algorithms",
        "authors": [
            "Samya Praharaj",
            "Koulik Khamaru"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Information Theory (cs.IT); Machine Learning (cs.LG); Statistics Theory (math.ST)",
        "abstract": "Statistical inference from data generated by multi-armed bandit (MAB) algorithms is challenging due to their adaptive, non-i.i.d. nature. A classical manifestation is that sample averages of arm rewards under bandit sampling may fail to satisfy a central limit theorem. Lai and Wei's stability condition provides a sufficient, and essentially necessary criterion, for asymptotic normality in bandit problems. While the celebrated Upper Confidence Bound (UCB) algorithm satisfies this stability condition, it is not minimax optimal, raising the question of whether minimax optimality and statistical stability can be achieved simultaneously. In this paper, we analyze the stability properties of a broad class of bandit algorithms that are based on the optimism principle. We establish general structural conditions under which such algorithms violate the Lai-Wei stability criterion. As a consequence, we show that widely used minimax-optimal UCB-style algorithms, including MOSS, Anytime-MOSS, Vanilla-MOSS, ADA-UCB, OC-UCB, KL-MOSS, KL-UCB++, KL-UCB-SWITCH, and Anytime KL-UCB-SWITCH, are unstable. We further complement our theoretical results with numerical simulations demonstrating that, in all these cases, the sample means fail to exhibit asymptotic normality. Overall, our findings suggest a fundamental tension between stability and minimax optimal regret, raising the question of whether it is possible to design bandit algorithms that achieve both. Understanding whether such simultaneously stable and minimax optimal strategies exist remains an important open direction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 232,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18793",
        "abs_url": "https://arxiv.org/abs/2511.18793",
        "pdf_url": "https://arxiv.org/pdf/2511.18793",
        "title": "NEZHA: A Zero-sacrifice and Hyperspeed Decoding Architecture for Generative Recommendations",
        "authors": [
            "Yejing Wang",
            "Shengyu Zhou",
            "Jinyu Lu",
            "Ziwei Liu",
            "Langming Liu",
            "Maolin Wang",
            "Wenlin Zhang",
            "Feng Li",
            "Wenbo Su",
            "Pengjie Wang",
            "Jian Xu",
            "Xiangyu Zhao"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Generative Recommendation (GR), powered by Large Language Models (LLMs), represents a promising new paradigm for industrial recommender systems. However, their practical application is severely hindered by high inference latency, which makes them infeasible for high-throughput, real-time services and limits their overall business impact. While Speculative Decoding (SD) has been proposed to accelerate the autoregressive generation process, existing implementations introduce new bottlenecks: they typically require separate draft models and model-based verifiers, requiring additional training and increasing the latency overhead. In this paper, we address these challenges with NEZHA, a novel architecture that achieves hyperspeed decoding for GR systems without sacrificing recommendation quality. Specifically, NEZHA integrates a nimble autoregressive draft head directly into the primary model, enabling efficient self-drafting. This design, combined with a specialized input prompt structure, preserves the integrity of sequence-to-sequence generation. Furthermore, to tackle the critical problem of hallucination, a major source of performance degradation, we introduce an efficient, model-free verifier based on a hash set. We demonstrate the effectiveness of NEZHA through extensive experiments on public datasets and have successfully deployed the system on Taobao since October 2025, driving the billion-level advertising revenue and serving hundreds of millions of daily active users.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 233,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18813",
        "abs_url": "https://arxiv.org/abs/2511.18813",
        "pdf_url": "https://arxiv.org/pdf/2511.18813",
        "title": "Uncertainty of Network Topology with Applications to Out-of-Distribution Detection",
        "authors": [
            "Sing-Yuan Yeh",
            "Chun-Hao Yang"
        ],
        "comments": "Submitted for journal publication",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Methodology (stat.ME)",
        "abstract": "Persistent homology (PH) is a crucial concept in computational topology, providing a multiscale topological description of a space. It is particularly significant in topological data analysis, which aims to make statistical inference from a topological perspective. In this work, we introduce a new topological summary for Bayesian neural networks, termed the predictive topological uncertainty (pTU). The proposed pTU measures the uncertainty in the interaction between the model and the inputs. It provides insights from the model perspective: if two samples interact with a model in a similar way, then they are considered identically distributed. We also show that the pTU is insensitive to the model architecture. As an application, pTU is used to solve the out-of-distribution (OOD) detection problem, which is critical to ensure model reliability. Failure to detect OOD input can lead to incorrect and unreliable predictions. To address this issue, we propose a significance test for OOD based on the pTU, providing a statistical framework for this issue. The effectiveness of the framework is validated through various experiments, in terms of its statistical power, sensitivity, and robustness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 234,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18820",
        "abs_url": "https://arxiv.org/abs/2511.18820",
        "pdf_url": "https://arxiv.org/pdf/2511.18820",
        "title": "Solution of Incompressible Flow Equations with Physics and Equality Constrained Artificial Neural Networks",
        "authors": [
            "Qifeng Hu",
            "Inanc Senocak"
        ],
        "comments": "21 pages, 13 figures",
        "subjects": "Fluid Dynamics (physics.flu-dyn); Machine Learning (cs.LG)",
        "abstract": "We present a meshless method for the solution of incompressible Navier-Stokes equations in advection-dominated regimes using physics- and equality-constrained artificial neural networks combined with a conditionally adaptive augmented Lagrangian formulation. A single neural network parameterizes both the velocity and pressure fields, and is trained by minimizing the residual of a Poisson's equation for pressure, constrained by the momentum and continuity equations, together with boundary conditions on the velocity field. No boundary conditions are imposed on the pressure field aside from anchoring the pressure at a point to prevent its unbounded development. The training is performed from scratch without labeled data, relying solely on the governing equations and constraints. To enhance accuracy in advection-dominated flows, we employ a single Fourier feature mapping of the input coordinates. The proposed method is demonstrated for the canonical lid-driven cavity flow up to a Reynolds number of 7,500 and for laminar flow over a circular cylinder with inflow-outflow boundary conditions, achieving excellent agreement with benchmark solutions. We further compare the present formulation against alternative objective-function constructions based on different arrangements of the flow equations, thereby highlighting the algorithmic advantages of the proposed formulation centered around the Poisson's equation for pressure.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 235,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18828",
        "abs_url": "https://arxiv.org/abs/2511.18828",
        "pdf_url": "https://arxiv.org/pdf/2511.18828",
        "title": "Solving a Research Problem in Mathematical Statistics with AI Assistance",
        "authors": [
            "Edgar Dobriban"
        ],
        "comments": "",
        "subjects": "Statistics Theory (math.ST); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Over the last few months, AI models including large language models have improved greatly. There are now several documented examples where they have helped professional mathematical scientists prove new results, sometimes even helping resolve known open problems. In this short note, we add another example to the list, by documenting how we were able to solve a previously unsolved research problem in robust mathematical statistics with crucial help from GPT-5. Our problem concerns robust density estimation, where the observations are perturbed by Wasserstein-bounded this http URL a previous preprint (Chao and Dobriban, 2023, arXiv:2308.01853v2), we have obtained upper and lower bounds on the minimax optimal estimation error; which were, however, not sharp. Starting in October 2025, making significant use of GPT-5 Pro, we were able to derive the minimax optimal error rate (reported in version 3 of the above arxiv preprint). GPT-5 provided crucial help along the way, including by suggesting calculations that we did not think of, and techniques that were not familiar to us, such as the dynamic Benamou-Brenier formulation, for key steps in the analysis. Working with GPT-5 took a few weeks of effort, and we estimate that it could have taken several months to get the same results otherwise. At the same time, there are still areas where working with GPT-5 was challenging: it sometimes provided incorrect references, and glossed over details that sometimes took days of work to fill in. We outline our workflow and steps taken to mitigate issues. Overall, our work can serve as additional documentation for a new age of human-AI collaborative work in mathematical science.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 236,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18843",
        "abs_url": "https://arxiv.org/abs/2511.18843",
        "pdf_url": "https://arxiv.org/pdf/2511.18843",
        "title": "A Reproducible Framework for Neural Topic Modeling in Focus Group Analysis",
        "authors": [
            "Heger Arfaoui",
            "Mohammed Iheb Hergli",
            "Beya Benzina",
            "Slimane BenMiled"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)",
        "abstract": "Focus group discussions generate rich qualitative data but their analysis traditionally relies on labor-intensive manual coding that limits scalability and reproducibility. We present a rigorous, reproducible computational framework for applying neural topic modeling to focus group transcripts, addressing fundamental methodological challenges: hyperparameter sensitivity, model stability, and validation of interpretability. Using BERTopic applied to ten focus groups exploring HPV vaccine perceptions in Tunisia (1,076 utterances), we conducted systematic evaluation across 27 hyperparameter configurations, assessed stability through bootstrap resampling with 30 replicates per configuration, and validated interpretability through formal human evaluation by three domain experts. Our analysis demonstrates substantial sensitivity to hyperparameter choices and reveals that metric selection for stability assessment must align with analytical goals. A hierarchical merging strategy (extracting fine-grained topics for stability then consolidating for interpretability) effectively navigates the stability-coherence tradeoff, achieving coherence of 0.558 compared to 0.539 for direct extraction. Human validation confirmed topic quality with very good inter-rater reliability (ICC = 0.79, weighted Cohen's kappa = 0.578). Our framework provides practical guidelines that researchers can adapt to their own qualitative research contexts. All code, data processing scripts, and evaluation protocols are publicly available to support reproduction and extension of this work.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 237,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18876",
        "abs_url": "https://arxiv.org/abs/2511.18876",
        "pdf_url": "https://arxiv.org/pdf/2511.18876",
        "title": "Fairness Meets Privacy: Integrating Differential Privacy and Demographic Parity in Multi-class Classification",
        "authors": [
            "Lilian Say",
            "Christophe Denis",
            "Rafael Pinot"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "The increasing use of machine learning in sensitive applications demands algorithms that simultaneously preserve data privacy and ensure fairness across potentially sensitive sub-populations. While privacy and fairness have each been extensively studied, their joint treatment remains poorly understood. Existing research often frames them as conflicting objectives, with multiple studies suggesting that strong privacy notions such as differential privacy inevitably compromise fairness. In this work, we challenge that perspective by showing that differential privacy can be integrated into a fairness-enhancing pipeline with minimal impact on fairness guarantees. We design a postprocessing algorithm, called DP2DP, that enforces both demographic parity and differential privacy. Our analysis reveals that our algorithm converges towards its demographic parity objective at essentially the same rate (up logarithmic factor) as the best non-private methods from the literature. Experiments on both synthetic and real datasets confirm our theoretical results, showing that the proposed algorithm achieves state-of-the-art accuracy/fairness/privacy trade-offs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 238,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.18992",
        "abs_url": "https://arxiv.org/abs/2511.18992",
        "pdf_url": "https://arxiv.org/pdf/2511.18992",
        "title": "Classification EM-PCA for clustering and embedding",
        "authors": [
            "Zineddine Tighidet",
            "Lazhar Labiod",
            "Mohamed Nadif"
        ],
        "comments": "Accepted at the IEEE conference on Big Data (Special Session on Machine Learning)",
        "subjects": "Machine Learning (stat.ML); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "The mixture model is undoubtedly one of the greatest contributions to clustering. For continuous data, Gaussian models are often used and the Expectation-Maximization (EM) algorithm is particularly suitable for estimating parameters from which clustering is inferred. If these models are particularly popular in various domains including image clustering, they however suffer from the dimensionality and also from the slowness of convergence of the EM algorithm. However, the Classification EM (CEM) algorithm, a classifying version, offers a fast convergence solution while dimensionality reduction still remains a challenge. Thus we propose in this paper an algorithm combining simultaneously and non-sequentially the two tasks --Data embedding and Clustering-- relying on Principal Component Analysis (PCA) and CEM. We demonstrate the interest of such approach in terms of clustering and data embedding. We also establish different connections with other clustering approaches.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 239,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19067",
        "abs_url": "https://arxiv.org/abs/2511.19067",
        "pdf_url": "https://arxiv.org/pdf/2511.19067",
        "title": "DynaMix: Generalizable Person Re-identification via Dynamic Relabeling and Mixed Data Sampling",
        "authors": [
            "Timur Mamedov",
            "Anton Konushin",
            "Vadim Konushin"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Generalizable person re-identification (Re-ID) aims to recognize individuals across unseen cameras and environments. While existing methods rely heavily on limited labeled multi-camera data, we propose DynaMix, a novel method that effectively combines manually labeled multi-camera and large-scale pseudo-labeled single-camera data. Unlike prior works, DynaMix dynamically adapts to the structure and noise of the training data through three core components: (1) a Relabeling Module that refines pseudo-labels of single-camera identities on-the-fly; (2) an Efficient Centroids Module that maintains robust identity representations under a large identity space; and (3) a Data Sampling Module that carefully composes mixed data mini-batches to balance learning complexity and intra-batch diversity. All components are specifically designed to operate efficiently at scale, enabling effective training on millions of images and hundreds of thousands of identities. Extensive experiments demonstrate that DynaMix consistently outperforms state-of-the-art methods in generalizable person Re-ID.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 240,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19075",
        "abs_url": "https://arxiv.org/abs/2511.19075",
        "pdf_url": "https://arxiv.org/pdf/2511.19075",
        "title": "Structured Matching via Cost-Regularized Unbalanced Optimal Transport",
        "authors": [
            "Emanuele Pardini",
            "Katerina Papagiannouli"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Applications (stat.AP)",
        "abstract": "Unbalanced optimal transport (UOT) provides a flexible way to match or compare nonnegative finite Radon measures. However, UOT requires a predefined ground transport cost, which may misrepresent the data's underlying geometry. Choosing such a cost is particularly challenging when datasets live in heterogeneous spaces, often motivating practitioners to adopt Gromov-Wasserstein formulations. To address this challenge, we introduce cost-regularized unbalanced optimal transport (CR-UOT), a framework that allows the ground cost to vary while allowing mass creation and removal. We show that CR-UOT incorporates unbalanced Gromov-Wasserstein type problems through families of inner-product costs parameterized by linear transformations, enabling the matching of measures or point clouds across Euclidean spaces. We develop algorithms for such CR-UOT problems using entropic regularization and demonstrate that this approach improves the alignment of heterogeneous single-cell omics profiles, especially when many cells lack direct matches.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 241,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19147",
        "abs_url": "https://arxiv.org/abs/2511.19147",
        "pdf_url": "https://arxiv.org/pdf/2511.19147",
        "title": "Collaborative Learning with Multiple Foundation Models for Source-Free Domain Adaptation",
        "authors": [
            "Huisoo Lee",
            "Jisu Han",
            "Hyunsouk Cho",
            "Wonjun Hwang"
        ],
        "comments": "15 pages, 8 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained source model to an unlabeled target domain without access to source data. Recent advances in Foundation Models (FMs) have introduced new opportunities for leveraging external semantic knowledge to guide SFDA. However, relying on a single FM is often insufficient, as it tends to bias adaptation toward a restricted semantic coverage, failing to capture diverse contextual cues under domain shift. To overcome this limitation, we propose a Collaborative Multi-foundation Adaptation (CoMA) framework that jointly leverages two different FMs (e.g., CLIP and BLIP) with complementary properties to capture both global semantics and local contextual cues. Specifically, we employ a bidirectional adaptation mechanism that (1) aligns different FMs with the target model for task adaptation while maintaining their semantic distinctiveness, and (2) transfers complementary knowledge from the FMs to the target model. To ensure stable adaptation under mini-batch training, we introduce Decomposed Mutual Information (DMI) that selectively enhances true dependencies while suppressing false dependencies arising from incomplete class coverage. Extensive experiments demonstrate that our method consistently outperforms existing state-of-the-art SFDA methods across four benchmarks, including Office-31, Office-Home, DomainNet-126, and VisDA, under the closed-set setting, while also achieving best results on partial-set and open-set variants.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 242,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19150",
        "abs_url": "https://arxiv.org/abs/2511.19150",
        "pdf_url": "https://arxiv.org/pdf/2511.19150",
        "title": "Feature Ranking in Credit-Risk with Qudit-Based Networks",
        "authors": [
            "Georgios Maragkopoulos",
            "Lazaros Chavatzoglou",
            "Aikaterini Mandilara",
            "Dimitris Syvridis"
        ],
        "comments": "",
        "subjects": "Quantum Physics (quant-ph); Machine Learning (cs.LG)",
        "abstract": "In finance, predictive models must balance accuracy and interpretability, particularly in credit risk assessment, where model decisions carry material consequences. We present a quantum neural network (QNN) based on a single qudit, in which both data features and trainable parameters are co-encoded within a unified unitary evolution generated by the full Lie algebra. This design explores the entire Hilbert space while enabling interpretability through the magnitudes of the learned coefficients. We benchmark our model on a real-world, imbalanced credit-risk dataset from Taiwan. The proposed QNN consistently outperforms LR and reaches the results of random forest models in macro-F1 score while preserving a transparent correspondence between learned parameters and input feature importance. To quantify the interpretability of the proposed model, we introduce two complementary metrics: (i) the edit distance between the model's feature ranking and that of LR, and (ii) a feature-poisoning test where selected features are replaced with noise. Results indicate that the proposed quantum model achieves competitive performance while offering a tractable path toward interpretable quantum learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 243,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19157",
        "abs_url": "https://arxiv.org/abs/2511.19157",
        "pdf_url": "https://arxiv.org/pdf/2511.19157",
        "title": "A Robust State Filter Against Unmodeled Process And Measurement Noise",
        "authors": [
            "Weitao Liu"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "This paper introduces a novel Kalman filter framework designed to achieve robust state estimation under both process and measurement noise. Inspired by the Weighted Observation Likelihood Filter (WoLF), which provides robustness against measurement outliers, we applied generalized Bayesian approach to build a framework considering both process and measurement noise outliers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 244,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19162",
        "abs_url": "https://arxiv.org/abs/2511.19162",
        "pdf_url": "https://arxiv.org/pdf/2511.19162",
        "title": "BioArtlas: Computational Clustering of Multi-Dimensional Complexity in Bioart",
        "authors": [
            "Joonhyung Bae"
        ],
        "comments": "",
        "subjects": "Information Retrieval (cs.IR); Computers and Society (cs.CY); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG); Multimedia (cs.MM)",
        "abstract": "Bioart's hybrid nature spanning art, science, technology, ethics, and politics defies traditional single-axis categorization. I present BioArtlas, analyzing 81 bioart works across thirteen curated dimensions using novel axis-aware representations that preserve semantic distinctions while enabling cross-dimensional comparison. Our codebook-based approach groups related concepts into unified clusters, addressing polysemy in cultural terminology. Comprehensive evaluation of up to 800 representation-space-algorithm combinations identifies Agglomerative clustering at k=15 on 4D UMAP as optimal (silhouette 0.664 +/- 0.008, trustworthiness/continuity 0.805/0.812). The approach reveals four organizational patterns: artist-specific methodological cohesion, technique-based segmentation, temporal artistic evolution, and trans-temporal conceptual affinities. By separating analytical optimization from public communication, I provide rigorous analysis and accessible exploration through an interactive web interface (this https URL) with the dataset publicly available (this https URL).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 245,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19199",
        "abs_url": "https://arxiv.org/abs/2511.19199",
        "pdf_url": "https://arxiv.org/pdf/2511.19199",
        "title": "CLASH: A Benchmark for Cross-Modal Contradiction Detection",
        "authors": [
            "Teodora Popordanoska",
            "Jiameng Li",
            "Matthew B. Blaschko"
        ],
        "comments": "First two authors contributed equally",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Contradictory multimodal inputs are common in real-world settings, yet existing benchmarks typically assume input consistency and fail to evaluate cross-modal contradiction detection - a fundamental capability for preventing hallucinations and ensuring reliability. We introduce CLASH, a novel benchmark for multimodal contradiction detection, featuring COCO images paired with contradictory captions containing controlled object-level or attribute-level contradictions. The samples include targeted questions evaluated in both multiple-choice and open-ended formats. The benchmark provides an extensive fine-tuning set filtered through automated quality checks, alongside a smaller human-verified diagnostic set. Our analysis of state-of-the-art models reveals substantial limitations in recognizing cross-modal conflicts, exposing systematic modality biases and category-specific weaknesses. Furthermore, we empirically demonstrate that targeted fine-tuning on CLASH substantially enhances conflict detection capabilities.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 246,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19256",
        "abs_url": "https://arxiv.org/abs/2511.19256",
        "pdf_url": "https://arxiv.org/pdf/2511.19256",
        "title": "SimDiff: Simpler Yet Better Diffusion Model for Time Series Point Forecasting",
        "authors": [
            "Hang Ding",
            "Xue Wang",
            "Tian Zhou",
            "Tao Yao"
        ],
        "comments": "Accepted by AAAI 2026",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Diffusion models have recently shown promise in time series forecasting, particularly for probabilistic predictions. However, they often fail to achieve state-of-the-art point estimation performance compared to regression-based methods. This limitation stems from difficulties in providing sufficient contextual bias to track distribution shifts and in balancing output diversity with the stability and precision required for point forecasts. Existing diffusion-based approaches mainly focus on full-distribution modeling under probabilistic frameworks, often with likelihood maximization objectives, while paying little attention to dedicated strategies for high-accuracy point estimation. Moreover, other existing point prediction diffusion methods frequently rely on pre-trained or jointly trained mature models for contextual bias, sacrificing the generative flexibility of diffusion models. To address these challenges, we propose SimDiff, a single-stage, end-to-end framework. SimDiff employs a single unified Transformer network carefully tailored to serve as both denoiser and predictor, eliminating the need for external pre-trained or jointly trained regressors. It achieves state-of-the-art point estimation performance by leveraging intrinsic output diversity and improving mean squared error accuracy through multiple inference ensembling. Key innovations, including normalization independence and the median-of-means estimator, further enhance adaptability and stability. Extensive experiments demonstrate that SimDiff significantly outperforms existing methods in time series point forecasting.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 247,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19257",
        "abs_url": "https://arxiv.org/abs/2511.19257",
        "pdf_url": "https://arxiv.org/pdf/2511.19257",
        "title": "Medusa: Cross-Modal Transferable Adversarial Attacks on Multimodal Medical Retrieval-Augmented Generation",
        "authors": [
            "Yingjia Shang",
            "Yi Liu",
            "Huimin Wang",
            "Furong Li",
            "Wenfang Sun",
            "Wu Chengyu",
            "Yefeng Zheng"
        ],
        "comments": "Accepted at KDD 2026 First Cycle (full version). Authors marked with * contributed equally. Yi Liu is the lead author",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "With the rapid advancement of retrieval-augmented vision-language models, multimodal medical retrieval-augmented generation (MMed-RAG) systems are increasingly adopted in clinical decision support. These systems enhance medical applications by performing cross-modal retrieval to integrate relevant visual and textual evidence for tasks, e.g., report generation and disease diagnosis. However, their complex architecture also introduces underexplored adversarial vulnerabilities, particularly via visual input perturbations. In this paper, we propose Medusa, a novel framework for crafting cross-modal transferable adversarial attacks on MMed-RAG systems under a black-box setting. Specifically, Medusa formulates the attack as a perturbation optimization problem, leveraging a multi-positive InfoNCE loss (MPIL) to align adversarial visual embeddings with medically plausible but malicious textual targets, thereby hijacking the retrieval process. To enhance transferability, we adopt a surrogate model ensemble and design a dual-loop optimization strategy augmented with invariant risk minimization (IRM). Extensive experiments on two real-world medical tasks, including medical report generation and disease diagnosis, demonstrate that Medusa achieves over 90% average attack success rate across various generation models and retrievers under appropriate parameter configuration, while remaining robust against four mainstream defenses, outperforming state-of-the-art baselines. Our results reveal critical vulnerabilities in the MMed-RAG systems and highlight the necessity of robustness benchmarking in safety-critical medical applications. The code and data are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 248,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19284",
        "abs_url": "https://arxiv.org/abs/2511.19284",
        "pdf_url": "https://arxiv.org/pdf/2511.19284",
        "title": "The Unified Non-Convex Framework for Robust Causal Inference: Overcoming the Gaussian Barrier and Optimization Fragility",
        "authors": [
            "Eichi Uehara"
        ],
        "comments": "10 pages, 1 table",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Methodology (stat.ME)",
        "abstract": "This document proposes a Unified Robust Framework that re-engineers the estimation of the Average Treatment Effect on the Overlap (ATO). It synthesizes gamma-Divergence for outlier robustness, Graduated Non-Convexity (GNC) for global optimization, and a \"Gatekeeper\" mechanism to address the impossibility of higher-order orthogonality in Gaussian regimes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 249,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19289",
        "abs_url": "https://arxiv.org/abs/2511.19289",
        "pdf_url": "https://arxiv.org/pdf/2511.19289",
        "title": "Performance Guarantees for Quantum Neural Estimation of Entropies",
        "authors": [
            "Sreejith Sreekumar",
            "Ziv Goldfeld",
            "Mark M. Wilde"
        ],
        "comments": "42+4 pages",
        "subjects": "Quantum Physics (quant-ph); Information Theory (cs.IT); Machine Learning (cs.LG)",
        "abstract": "Estimating quantum entropies and divergences is an important problem in quantum physics, information theory, and machine learning. Quantum neural estimators (QNEs), which utilize a hybrid classical-quantum architecture, have recently emerged as an appealing computational framework for estimating these measures. Such estimators combine classical neural networks with parametrized quantum circuits, and their deployment typically entails tedious tuning of hyperparameters controlling the sample size, network architecture, and circuit topology. This work initiates the study of formal guarantees for QNEs of measured (Rényi) relative entropies in the form of non-asymptotic error risk bounds. We further establish exponential tail bounds showing that the error is sub-Gaussian, and thus sharply concentrates about the ground truth value. For an appropriate sub-class of density operator pairs on a space of dimension $d$ with bounded Thompson metric, our theory establishes a copy complexity of $O(|\\Theta(\\mathcal{U})|d/\\epsilon^2)$ for QNE with a quantum circuit parameter set $\\Theta(\\mathcal{U})$, which has minimax optimal dependence on the accuracy $\\epsilon$. Additionally, if the density operator pairs are permutation invariant, we improve the dimension dependence above to $O(|\\Theta(\\mathcal{U})|\\mathrm{polylog}(d)/\\epsilon^2)$. Our theory aims to facilitate principled implementation of QNEs for measured relative entropies and guide hyperparameter tuning in practice.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 250,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19291",
        "abs_url": "https://arxiv.org/abs/2511.19291",
        "pdf_url": "https://arxiv.org/pdf/2511.19291",
        "title": "TorchQuantumDistributed",
        "authors": [
            "Oliver Knitter",
            "Jonathan Mei",
            "Masako Yamada",
            "Martin Roetteler"
        ],
        "comments": "12 pages, 4 figures, to appear in the AI for Science Workshop at NeurIPS 2025",
        "subjects": "Quantum Physics (quant-ph); Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)",
        "abstract": "TorchQuantumDistributed (tqd) is a PyTorch-based [Paszke et al., 2019] library for accelerator-agnostic differentiable quantum state vector simulation at scale. This enables studying the behavior of learnable parameterized near-term and fault- tolerant quantum circuits with high qubit counts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 251,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19314",
        "abs_url": "https://arxiv.org/abs/2511.19314",
        "pdf_url": "https://arxiv.org/pdf/2511.19314",
        "title": "PRInTS: Reward Modeling for Long-Horizon Information Seeking",
        "authors": [
            "Jaewoo Lee",
            "Archiki Prasad",
            "Justin Chih-Yao Chen",
            "Zaid Khan",
            "Elias Stengel-Eskin",
            "Mohit Bansal"
        ],
        "comments": "18 pages, code: this https URL",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Information-seeking is a core capability for AI agents, requiring them to gather and reason over tool-generated information across long trajectories. However, such multi-step information-seeking tasks remain challenging for agents backed by language models. While process reward models (PRMs) can guide agents by ranking candidate steps at test-time, existing PRMs, designed for short reasoning with binary judgment, cannot capture richer dimensions of information-seeking steps, such as tool interactions and reasoning over tool outputs, nor handle the rapidly growing context in long-horizon tasks. To address these limitations, we introduce PRInTS, a generative PRM trained with dual capabilities: (1) dense scoring based on the PRM's reasoning across multiple step quality dimensions (e.g., interpretation of tool outputs, tool call informativeness) and (2) trajectory summarization that compresses the growing context while preserving essential information for step evaluation. Extensive evaluations across FRAMES, GAIA (levels 1-3), and WebWalkerQA (easy-hard) benchmarks on multiple models, along with ablations, reveal that best-of-n sampling with PRInTS enhances information-seeking abilities of open-source models as well as specialized agents, matching or surpassing the performance of frontier models with a much smaller backbone agent and outperforming other strong reward modeling baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 252,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19335",
        "abs_url": "https://arxiv.org/abs/2511.19335",
        "pdf_url": "https://arxiv.org/pdf/2511.19335",
        "title": "High-throughput validation of phase formability and simulation accuracy of Cantor alloys",
        "authors": [
            "Changjun Cheng",
            "Daniel Persaud",
            "Kangming Li",
            "Michael J. Moorehead",
            "Natalie Page",
            "Christian Lavoie",
            "Beatriz Diaz Moreno",
            "Adrien Couet",
            "Samuel E Lofland",
            "Jason Hattrick-Simpers"
        ],
        "comments": "",
        "subjects": "Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG)",
        "abstract": "High-throughput methods enable accelerated discovery of novel materials in complex systems such as high-entropy alloys, which exhibit intricate phase stability across vast compositional spaces. Computational approaches, including Density Functional Theory (DFT) and calculation of phase diagrams (CALPHAD), facilitate screening of phase formability as a function of composition and temperature. However, the integration of computational predictions with experimental validation remains challenging in high-throughput studies. In this work, we introduce a quantitative confidence metric to assess the agreement between predictions and experimental observations, providing a quantitative measure of the confidence of machine learning models trained on either DFT or CALPHAD input in accounting for experimental evidence. The experimental dataset was generated via high-throughput in-situ synchrotron X-ray diffraction on compositionally varied FeNiMnCr alloy libraries, heated from room temperature to ~1000 °C. Agreement between the observed and predicted phases was evaluated using either temperature-independent phase classification or a model that incorporates a temperature-dependent probability of phase formation. This integrated approach demonstrates where strong overall agreement between computation and experiment exists, while also identifying key discrepancies, particularly in FCC/BCC predictions at Mn-rich regions to inform future model refinement.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 253,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19347",
        "abs_url": "https://arxiv.org/abs/2511.19347",
        "pdf_url": "https://arxiv.org/pdf/2511.19347",
        "title": "Artificial Intelligence Driven Workflow for Accelerating Design of Novel Photosensitizers",
        "authors": [
            "Hongyi Wang",
            "Xiuli Zheng",
            "Weimin Liu",
            "Zitian Tang",
            "Sheng Gong"
        ],
        "comments": "",
        "subjects": "Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG); Chemical Physics (physics.chem-ph)",
        "abstract": "The discovery of high-performance photosensitizers has long been hindered by the time-consuming and resource-intensive nature of traditional trial-and-error approaches. Here, we present \\textbf{A}I-\\textbf{A}ccelerated \\textbf{P}hoto\\textbf{S}ensitizer \\textbf{I}nnovation (AAPSI), a closed-loop workflow that integrates expert knowledge, scaffold-based molecule generation, and Bayesian optimization to accelerate the design of novel photosensitizers. The scaffold-driven generation in AAPSI ensures structural novelty and synthetic feasibility, while the iterative AI-experiment loop accelerates the discovery of novel photosensitizers. AAPSI leverages a curated database of 102,534 photosensitizer-solvent pairs and generate 6,148 synthetically accessible candidates. These candidates are screened via graph transformers trained to predict singlet oxygen quantum yield ($\\phi_\\Delta$) and absorption maxima ($\\lambda_{max}$), following experimental validation. This work generates several novel candidates for photodynamic therapy (PDT), among which the hypocrellin-based candidate HB4Ph exhibits exceptional performance at the Pareto frontier of high quantum yield of singlet oxygen and long absorption maxima among current photosensitizers ($\\phi_\\Delta$=0.85, $\\lambda_{max}$=650nm).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 254,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19398",
        "abs_url": "https://arxiv.org/abs/2511.19398",
        "pdf_url": "https://arxiv.org/pdf/2511.19398",
        "title": "PTF Testing Lower Bounds for Non-Gaussian Component Analysis",
        "authors": [
            "Ilias Diakonikolas",
            "Daniel M. Kane",
            "Sihan Liu",
            "Thanasis Pittas"
        ],
        "comments": "",
        "subjects": "Data Structures and Algorithms (cs.DS); Information Theory (cs.IT); Machine Learning (cs.LG); Statistics Theory (math.ST); Machine Learning (stat.ML)",
        "abstract": "This work studies information-computation gaps for statistical problems. A common approach for providing evidence of such gaps is to show sample complexity lower bounds (that are stronger than the information-theoretic optimum) against natural models of computation. A popular such model in the literature is the family of low-degree polynomial tests. While these tests are defined in such a way that make them easy to analyze, the class of algorithms that they rule out is somewhat restricted. An important goal in this context has been to obtain lower bounds against the stronger and more natural class of low-degree Polynomial Threshold Function (PTF) tests, i.e., any test that can be expressed as comparing some low-degree polynomial of the data to a threshold. Proving lower bounds against PTF tests has turned out to be challenging. Indeed, we are not aware of any non-trivial PTF testing lower bounds in the literature. In this paper, we establish the first non-trivial PTF testing lower bounds for a range of statistical tasks. Specifically, we prove a near-optimal PTF testing lower bound for Non-Gaussian Component Analysis (NGCA). Our NGCA lower bound implies similar lower bounds for a number of other statistical problems. Our proof leverages a connection to recent work on pseudorandom generators for PTFs and recent techniques developed in that context. At the technical level, we develop several tools of independent interest, including novel structural results for analyzing the behavior of low-degree polynomials restricted to random directions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 255,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19399",
        "abs_url": "https://arxiv.org/abs/2511.19399",
        "pdf_url": "https://arxiv.org/pdf/2511.19399",
        "title": "DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research",
        "authors": [
            "Rulin Shao",
            "Akari Asai",
            "Shannon Zejiang Shen",
            "Hamish Ivison",
            "Varsha Kishore",
            "Jingming Zhuo",
            "Xinran Zhao",
            "Molly Park",
            "Samuel G. Finlayson",
            "David Sontag",
            "Tyler Murray",
            "Sewon Min",
            "Pradeep Dasigi",
            "Luca Soldaini",
            "Faeze Brahman",
            "Wen-tau Yih",
            "Tongshuang Wu",
            "Luke Zettlemoyer",
            "Yoon Kim",
            "Hannaneh Hajishirzi",
            "Pang Wei Koh"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Deep research models perform multi-step research to produce long-form, well-attributed answers. However, most open deep research models are trained on easily verifiable short-form QA tasks via reinforcement learning with verifiable rewards (RLVR), which does not extend to realistic long-form tasks. We address this with Reinforcement Learning with Evolving Rubrics (RLER), in which we construct and maintain rubrics that co-evolve with the policy model during training; this allows the rubrics to incorporate information that the model has newly explored and to provide discriminative, on-policy feedback. Using RLER, we develop Deep Research Tulu (DR Tulu-8B), the first open model that is directly trained for open-ended, long-form deep research. Across four long-form deep research benchmarks in science, healthcare and general domains, DR Tulu substantially outperforms existing open deep research models, and matches or exceeds proprietary deep research systems, while being significantly smaller and cheaper per query. To facilitate future research, we release all data, models, and code, including our new MCP-based agent infrastructure for deep research systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 256,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19404",
        "abs_url": "https://arxiv.org/abs/2511.19404",
        "pdf_url": "https://arxiv.org/pdf/2511.19404",
        "title": "Nonparametric Instrumental Variable Regression with Observed Covariates",
        "authors": [
            "Zikai Shen",
            "Zonghao Chen",
            "Dimitri Meunier",
            "Ingo Steinwart",
            "Arthur Gretton",
            "Zhu Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Statistics Theory (math.ST)",
        "abstract": "We study the problem of nonparametric instrumental variable regression with observed covariates, which we refer to as NPIV-O. Compared with standard nonparametric instrumental variable regression (NPIV), the additional observed covariates facilitate causal identification and enables heterogeneous causal effect estimation. However, the presence of observed covariates introduces two challenges for its theoretical analysis. First, it induces a partial identity structure, which renders previous NPIV analyses - based on measures of ill-posedness, stability conditions, or link conditions - inapplicable. Second, it imposes anisotropic smoothness on the structural function. To address the first challenge, we introduce a novel Fourier measure of partial smoothing; for the second challenge, we extend the existing kernel 2SLS instrumental variable algorithm with observed covariates, termed KIV-O, to incorporate Gaussian kernel lengthscales adaptive to the anisotropic smoothness. We prove upper $L^2$-learning rates for KIV-O and the first $L^2$-minimax lower learning rates for NPIV-O. Both rates interpolate between known optimal rates of NPIV and nonparametric regression (NPR). Interestingly, we identify a gap between our upper and lower bounds, which arises from the choice of kernel lengthscales tuned to minimize a projected risk. Our theoretical analysis also applies to proximal causal inference, an emerging framework for causal effect estimation that shares the same conditional moment restriction as NPIV-O.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 257,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19418",
        "abs_url": "https://arxiv.org/abs/2511.19418",
        "pdf_url": "https://arxiv.org/pdf/2511.19418",
        "title": "Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens",
        "authors": [
            "Yiming Qin",
            "Bomin Wei",
            "Jiaxin Ge",
            "Konstantinos Kallidromitis",
            "Stephanie Fu",
            "Trevor Darrell",
            "Xudong Wang"
        ],
        "comments": "Project page: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Vision-Language Models (VLMs) excel at reasoning in linguistic space but struggle with perceptual understanding that requires dense visual perception, e.g., spatial reasoning and geometric awareness. This limitation stems from the fact that current VLMs have limited mechanisms to capture dense visual information across spatial dimensions. We introduce Chain-of-Visual-Thought (COVT), a framework that enables VLMs to reason not only in words but also through continuous visual tokens-compact latent representations that encode rich perceptual cues. Within a small budget of roughly 20 tokens, COVT distills knowledge from lightweight vision experts, capturing complementary properties such as 2D appearance, 3D geometry, spatial layout, and edge structure. During training, the VLM with COVT autoregressively predicts these visual tokens to reconstruct dense supervision signals (e.g., depth, segmentation, edges, and DINO features). At inference, the model reasons directly in the continuous visual token space, preserving efficiency while optionally decoding dense predictions for interpretability. Evaluated across more than ten diverse perception benchmarks, including CV-Bench, MMVP, RealWorldQA, MMStar, WorldMedQA, and HRBench, integrating COVT into strong VLMs such as Qwen2.5-VL and LLaVA consistently improves performance by 3% to 16% and demonstrates that compact continuous visual thinking enables more precise, grounded, and interpretable multimodal intelligence.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 258,
        "date": "2025-11-25",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-25?abs=True",
        "arxiv_id": "2511.19436",
        "abs_url": "https://arxiv.org/abs/2511.19436",
        "pdf_url": "https://arxiv.org/pdf/2511.19436",
        "title": "VDC-Agent: When Video Detailed Captioners Evolve Themselves via Agentic Self-Reflection",
        "authors": [
            "Qiang Wang",
            "Xinyuan Gao",
            "SongLin Dong",
            "Jizhou Han",
            "Jiangyang Li",
            "Yuhang He",
            "Yihong Gong"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multimedia (cs.MM)",
        "abstract": "We present VDC-Agent, a self-evolving framework for Video Detailed Captioning that requires neither human annotations nor larger teacher models. The agent forms a closed loop of caption generation, principle-guided scoring (score and textual suggestions), and prompt refinement. When caption quality regresses, a self-reflection path leverages the previous chain-of-thought to amend the update. Running this process on unlabeled videos produces trajectories of (caption, score) pairs. We convert the trajectories into preference tuples and filter out samples with JSON parsing errors, resulting in VDC-Agent-19K, which contains 18,886 automatically constructed pairs. We then fine-tune the base MLLM on this dataset using an easy-to-hard curriculum direct preference optimization. Built on Qwen2.5-VL-7B-Instruct, our VDC-Agent-7B attains state-of-the-art performance on the VDC benchmark with 49.08% average accuracy and 2.50 score, surpassing specialized video captioners and improving over the base model by +5.13% accuracy and +0.27 score at similar inference cost.",
        "gemini2.5flash": "",
        "overall_idea": ""
    }
]