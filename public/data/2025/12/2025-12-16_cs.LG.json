[
    {
        "order": 1,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.11829",
        "abs_url": "https://arxiv.org/abs/2512.11829",
        "pdf_url": "https://arxiv.org/pdf/2512.11829",
        "title": "Active Inference with Reusable State-Dependent Value Profiles",
        "authors": [
            "Jacob Poschl"
        ],
        "comments": "27 pages",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "Adaptive behavior in volatile environments requires agents to switch among value-control regimes across latent contexts, but maintaining separate preferences, policy biases, and action-confidence parameters for every situation is intractable. We introduce value profiles: a small set of reusable bundles of value-related parameters (outcome preferences, policy priors, and policy precision) assigned to hidden states in a generative model. As posterior beliefs over states evolve trial by trial, effective control parameters arise via belief-weighted mixing, enabling state-conditional strategy recruitment without requiring independent parameters for each context. We evaluate this framework in probabilistic reversal learning, comparing static-precision, entropy-coupled dynamic-precision, and profile-based models using cross-validated log-likelihood and information criteria. Model comparison favors the profile-based model over simpler alternatives (about 100-point AIC differences), and parameter-recovery analyses support structural identifiability even when context must be inferred from noisy observations. Model-based inference further suggests that adaptive control in this task is driven primarily by modulation of policy priors rather than policy precision, with gradual belief-dependent profile recruitment consistent with state-conditional (not purely uncertainty-driven) control. Overall, reusable value profiles provide a tractable computational account of belief-conditioned value control in volatile environments and yield testable signatures of belief-dependent control and behavioral flexibility.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 2,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.11831",
        "abs_url": "https://arxiv.org/abs/2512.11831",
        "pdf_url": "https://arxiv.org/pdf/2512.11831",
        "title": "On the Design of One-step Diffusion via Shortcutting Flow Paths",
        "authors": [
            "Haitao Lin",
            "Peiyan Hu",
            "Minsi Ren",
            "Zhifeng Gao",
            "Zhi-Ming Ma",
            "Guolin ke",
            "Tailin Wu",
            "Stan Z. Li"
        ],
        "comments": "10 pages of main body, conference paper",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Recent advances in few-step diffusion models have demonstrated their efficiency and effectiveness by shortcutting the probabilistic paths of diffusion models, especially in training one-step diffusion models from scratch (a.k.a. shortcut models). However, their theoretical derivation and practical implementation are often closely coupled, which obscures the design space. To address this, we propose a common design framework for representative shortcut models. This framework provides theoretical justification for their validity and disentangles concrete component-level choices, thereby enabling systematic identification of improvements. With our proposed improvements, the resulting one-step model achieves a new state-of-the-art FID50k of 2.85 on ImageNet-256x256 under the classifier-free guidance setting. Remarkably, the model requires no pre-training, distillation, or curriculum learning. We believe our work lowers the barrier to component-level innovation in shortcut models and facilitates principled exploration of their design space.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 3,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.11833",
        "abs_url": "https://arxiv.org/abs/2512.11833",
        "pdf_url": "https://arxiv.org/pdf/2512.11833",
        "title": "Soft Decision Tree classifier: explainable and extendable PyTorch implementation",
        "authors": [
            "Reuben R Shamir"
        ],
        "comments": "Keywords: Soft Decision Tree, Short-term Memory Soft Decision Tree, Classification, Explainability",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We implemented a Soft Decision Tree (SDT) and a Short-term Memory Soft Decision Tree (SM-SDT) using PyTorch. The methods were extensively tested on simulated and clinical datasets. The SDT was visualized to demonstrate the potential for its explainability. SDT, SM-SDT, and XGBoost demonstrated similar area under the curve (AUC) values. These methods were better than Random Forest, Logistic Regression, and Decision Tree. The results on clinical datasets suggest that, aside from a decision tree, all tested classification methods yield comparable results. The code and datasets are available online on GitHub: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 4,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.11834",
        "abs_url": "https://arxiv.org/abs/2512.11834",
        "pdf_url": "https://arxiv.org/pdf/2512.11834",
        "title": "Hybrid twinning using PBDW and DeepONet for the effective state estimation and prediction on partially known systems",
        "authors": [
            "Stiven Briand Massala",
            "Ludovic Chamoin",
            "Massimo Picca Ciamarra"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The accurate estimation of the state of complex uncertain physical systems requires reconciling theoretical models, with inherent imperfections, with noisy experimental data. In this work, we propose an effective hybrid approach that combines physics-based modeling with data-driven learning to enhance state estimation and further prediction. Our method builds upon the Parameterized Background Data-Weak (PBDW) framework, which naturally integrates a reduced-order representation of the best-available model with measurement data to account for both anticipated and unanticipated uncertainties. To address model discrepancies not captured by the reduced-order space, and learn the structure of model deviation, we incorporate a Deep Operator Network (DeepONet) constrained to be an orthogonal complement of the best-knowledge manifold. This ensures that the learned correction targets only the unknown components of model bias, preserving the interpretability and fidelity of the physical model. An optimal sensor placement strategy is also investigated to maximize information gained from measurements. We validate the proposed approach on a representative problem involving the Helmholtz equation under various sources of modeling error, including those arising from boundary conditions and source terms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 5,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.11836",
        "abs_url": "https://arxiv.org/abs/2512.11836",
        "pdf_url": "https://arxiv.org/pdf/2512.11836",
        "title": "Semantic Nutrition Estimation: Predicting Food Healthfulness from Text Descriptions",
        "authors": [
            "Dayne R. Freudenberg",
            "Daniel G. Haughian",
            "Mitchell A. Klusty",
            "Caroline N. Leach",
            "W. Scott Black",
            "Leslie N. Woltenberg",
            "Rowan Hallock",
            "Elizabeth Solie",
            "Emily B. Collier",
            "Samuel E. Armstrong",
            "V. K. Cody Bumgardner"
        ],
        "comments": "10 pages, 4 figures, 6 tables, submitted to AMIA 2026 Informatics Summit",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Accurate nutritional assessment is critical for public health, but existing profiling systems require detailed data often unavailable or inaccessible from colloquial text descriptions of food. This paper presents a machine learning pipeline that predicts the comprehensive Food Compass Score 2.0 (FCS) from text descriptions. Our approach uses multi-headed neural networks to process hybrid feature vectors that combine semantic text embeddings, lexical patterns, and domain heuristics, alongside USDA Food and Nutrient Database for Dietary Studies (FNDDS) data. The networks estimate the nutrient and food components necessary for the FCS algorithm. The system demonstratedstrong predictive power, achieving a median R^2 of 0.81 for individual nutrients. The predicted FCS correlated strongly with published values (Pearson's r = 0.77), with a mean absolute difference of 14.0 points. While errors were largest for ambiguous or processed foods, this methodology translates language into actionable nutritional information, enabling scalable dietary assessment for consumer applications and research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 6,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.11838",
        "abs_url": "https://arxiv.org/abs/2512.11838",
        "pdf_url": "https://arxiv.org/pdf/2512.11838",
        "title": "D-STEER - Preference Alignment Techniques Learn to Behave, not to Believe - Beneath the Surface, DPO as Steering Vector Perturbation in Activation Space",
        "authors": [
            "Samarth Raina",
            "Saksham Aggarwal",
            "Aman Chadha",
            "Vinija Jain",
            "Amitava Das"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Direct Preference Optimization (DPO) has become a standard recipe for aligning large language models, yet it is still unclear what kind of change it actually induces inside the network. This paper argues that DPO does not rewrite a models internal beliefs; instead, it acts as a low rank steering mechanism that nudges activations along a small number of preference directions. Using a simple derivation, we show that the DPO gradient depends only on the difference between the logit embeddings of preferred and dispreferred completions, implying a first order shift in the final hidden representation rather than a deep restructuring of semantics. We then extract an empirical steering vector from a DPO tuned model and demonstrate that adding this vector to base activations reproduces most of the aligned behavior, while subtracting it nearly restores the original model. Finally, spectral analyses reveal rank-one dominance and entropy collapse in upper layers, indicating that alignment is funneled through a narrow subspace. Taken together, these results support a behavioral illusion view of DPO: it teaches models how to act aligned, not what to believe.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 7,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.11839",
        "abs_url": "https://arxiv.org/abs/2512.11839",
        "pdf_url": "https://arxiv.org/pdf/2512.11839",
        "title": "Large Language Models as Generalist Policies for Network Optimization",
        "authors": [
            "Duo Wu",
            "Linjia Kang",
            "Zhimin Wang",
            "Fangxin Wang",
            "Wei Zhang",
            "Xuefeng Tao",
            "Wei Yang",
            "Le Zhang",
            "Peng Cui",
            "Zhi Wang"
        ],
        "comments": "In submission. Project homepage: this https URL",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Designing control policies to ensure robust network services is essential to modern digital infrastructure. However, the dominant paradigm for network optimization relies on designing specialist policies based on handcrafted rules or deep learning models, leading to poor generalization across diverse tasks and environments. In contrast, large language models (LLMs), pretrained on Internet-scale corpora, provide a rich and unified knowledge base that encodes fundamental networking principles. Combined with their emergent abilities in generalization to unseen scenarios, LLMs offer a transformative foundation for generalist network policies that can generalize across diverse tasks and environments with minimal adaptation. In this paper, we present Trailblazer, the first systematic framework to realize such a generalist policy for networking. Trailblazer incorporates a network alignment scheme to ground the LLM in specific networking tasks, and an adaptive policy collaboration mechanism that offloads simple control cases from the LLM to a lightweight policy for computational efficiency. Through extensive simulations and large-scale real-world online evaluation on Douyin (the Chinese version of TikTok), Trailblazer, powered by a single LLM, demonstrates stronger cross-task and cross-environment generalization than conventional specialist policies. Our results validate LLMs as the foundation for generalist network policies, and position Trailblazer as the first step toward the generalist-driven paradigm that enables strong generalization with minimal efforts in policy design.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 8,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.11840",
        "abs_url": "https://arxiv.org/abs/2512.11840",
        "pdf_url": "https://arxiv.org/pdf/2512.11840",
        "title": "Amortized Causal Discovery with Prior-Fitted Networks",
        "authors": [
            "Mateusz Sypniewski",
            "Mateusz Olko",
            "Mateusz Gajewski",
            "Piotr Miłoś"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Methodology (stat.ME); Machine Learning (stat.ML)",
        "abstract": "In recent years, differentiable penalized likelihood methods have gained popularity, optimizing the causal structure by maximizing its likelihood with respect to the data. However, recent research has shown that errors in likelihood estimation, even on relatively large sample sizes, disallow the discovery of proper structures. We propose a new approach to amortized causal discovery that addresses the limitations of likelihood estimator accuracy. Our method leverages Prior-Fitted Networks (PFNs) to amortize data-dependent likelihood estimation, yielding more reliable scores for structure learning. Experiments on synthetic, simulated, and real-world datasets show significant gains in structure recovery compared to standard baselines. Furthermore, we demonstrate directly that PFNs provide more accurate likelihood estimates than conventional neural network-based approaches.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 9,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.11841",
        "abs_url": "https://arxiv.org/abs/2512.11841",
        "pdf_url": "https://arxiv.org/pdf/2512.11841",
        "title": "Meta-Continual Mobility Forecasting for Proactive Handover Prediction",
        "authors": [
            "Sasi Vardhan Reddy Mandapati"
        ],
        "comments": "6 pages, 1 figure",
        "subjects": "Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)",
        "abstract": "Short-term mobility forecasting is a core requirement for proactive handover (HO) in cellular networks. Real-world mobility is highly non-stationary: abrupt turns, rapid speed changes, and unpredictable user behavior cause conventional predictors to drift, leading to mistimed or failed handovers. We propose a lightweight meta-continual forecasting framework that integrates a GRU-based predictor, Reptile meta-initialization for fast few-shot adaptation, and an EWMA residual detector that triggers compact online updates only when drift occurs. Evaluated on a reproducible GeoLife and DeepMIMO pipeline, our method achieves 4.46 m ADE and 7.79 m FDE in zero-shot settings, improves few-shot ADE to 3.71 m at 10-shot, and enables recovery from abrupt drift about 2 to 3 times faster than an offline GRU. When applied to downstream HO prediction, the approach improves F1 to 0.83 and AUROC to 0.90, with substantial reductions in missed-HO and ping-pong events. The model is lightweight (128k parameters) and suitable for edge deployment in 5G and 6G systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 10,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.11846",
        "abs_url": "https://arxiv.org/abs/2512.11846",
        "pdf_url": "https://arxiv.org/pdf/2512.11846",
        "title": "Exploring Topological Bias in Heterogeneous Graph Neural Networks",
        "authors": [
            "Yihan Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Graph Neural Networks (GNNs) are characterized by their capacity of processing graph-structured data. However, due to the sparsity of labels under semi-supervised learning, they have been found to exhibit biased performance on specific nodes. This kind of bias has been validated to correlate with topological structure and is considered as a bottleneck of GNNs' performance. Existing work focuses on the study of homogeneous GNNs and little attention has been given to topological bias in Heterogeneous Graph Neural Networks (HGNNs). In this work, firstly, in order to distinguish distinct meta relations, we apply meta-weighting to the adjacency matrix of a heterogeneous graph. Based on the modified adjacency matrix, we leverage PageRank along with the node label information to construct a projection. The constructed projection effectively maps nodes to values that strongly correlated with model performance when using datasets both with and without intra-type connections, which demonstrates the universal existence of topological bias in HGNNs. To handle this bias, we propose a debiasing structure based on the difference in the mapped values of nodes and use it along with the original graph structure for contrastive learning. Experiments on three public datasets verify the effectiveness of the proposed method in improving HGNNs' performance and debiasing.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 11,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.11847",
        "abs_url": "https://arxiv.org/abs/2512.11847",
        "pdf_url": "https://arxiv.org/pdf/2512.11847",
        "title": "Tiny Recursive Models on ARC-AGI-1: Inductive Biases, Identity Conditioning, and Test-Time Compute",
        "authors": [
            "Antonio Roye-Azar",
            "Santiago Vargas-Naranjo",
            "Dhruv Ghai",
            "Nithin Balamurugan",
            "Rayan Amir"
        ],
        "comments": "13 pages, 0 figures, 6 tables",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Tiny Recursive Models (TRM) were proposed as a parameter-efficient alternative to large language models for solving Abstraction and Reasoning Corpus (ARC) style tasks. The original work reports strong performance and suggests that recursive latent updates enable non-trivial reasoning, but it remains unclear how much of this performance stems from architecture, test-time compute, or task-specific priors. In this technical note, we empirically analyze the ARC Prize TRM checkpoint on ARC-AGI-1 and report four behavioral findings and an efficiency comparison. First, we show that test-time augmentation and majority-vote ensembling account for a substantial fraction of reported performance: the 1000-sample voting pipeline improves Pass@1 by about 11 percentage points over single-pass canonical inference. Second, a puzzle-identity ablation reveals strict dependence on task identifiers: replacing the correct puzzle ID with a blank or random token yields zero accuracy. Third, a recursion trajectory analysis shows that most of the final accuracy is achieved at the first recursion step and that performance saturates after few latent updates, indicating shallow effective recursion. Fourth, early-stage training experiments under canonical versus heavy augmentation regimes suggest that heavy augmentation broadens the distribution of candidate solutions and improves multi-sample success. Finally, we compare TRM with a naive QLoRA fine-tune of Llama 3 8B on canonical ARC-AGI-1, finding that TRM's non-autoregressive design achieves much higher throughput and substantially lower memory usage in this setting. Overall, TRM's ARC-AGI-1 performance appears to arise from an interaction between efficiency, task-specific conditioning, and aggressive test-time compute rather than deep internal reasoning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 12,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.11851",
        "abs_url": "https://arxiv.org/abs/2512.11851",
        "pdf_url": "https://arxiv.org/pdf/2512.11851",
        "title": "KV Cache Recycling to Expand Usable Context Capacity in Low Parameter LLMs",
        "authors": [
            "Prashant Pandey"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)",
        "abstract": "Whether attention key value (KV) states computed for one prompt for a small LLM can be reused to accelerate inference on a new similar prompt, giving an increase to the space to its context memory using an approach called token recycling. Using a standard Hugging Face setup with DialoGPT-medium (a 345M parameter GPT-2 style decoder trained on 147M Reddit exchanges, 2005 to 2017) as the testbed, we build a cache of past activations and get entries by sentence embeddings, then reuse cached past key values when the cached prompt is an exact prefix of the new input. We compare recycled vs. baseline runs on latency and output fidelity, and log reuse depth in tokens. Reproducibility requires no model modifications, cached KVs are serialized to the CPU, reloaded, and supplied to the generate function to continue decoding from the cached prefix. In tests, we observe consistent speedups when prefix overlap exists, with no material degradation in output semantics, and when overlap is absent, behavior matches baseline.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 13,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.11854",
        "abs_url": "https://arxiv.org/abs/2512.11854",
        "pdf_url": "https://arxiv.org/pdf/2512.11854",
        "title": "Rep Smarter, Not Harder: AI Hypertrophy Coaching with Wearable Sensors and Edge Neural Networks",
        "authors": [
            "Grant King",
            "Musa Azeem",
            "Savannah Noblitt",
            "Ramtin Zand",
            "Homayoun Valafar"
        ],
        "comments": "24th International Conference on Machine Learning and Applications",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Optimizing resistance training for hypertrophy requires balancing proximity to muscular failure, often quantified by Repetitions in Reserve (RiR), with fatigue management. However, subjective RiR assessment is unreliable, leading to suboptimal training stimuli or excessive fatigue. This paper introduces a novel system for real-time feedback on near-failure states (RiR $\\le$ 2) during resistance exercise using only a single wrist-mounted Inertial Measurement Unit (IMU). We propose a two-stage pipeline suitable for edge deployment: first, a ResNet-based model segments repetitions from the 6-axis IMU data in real-time. Second, features derived from this segmentation, alongside direct convolutional features and historical context captured by an LSTM, are used by a classification model to identify exercise windows corresponding to near-failure states. Using a newly collected dataset from 13 diverse participants performing preacher curls to failure (631 total reps), our segmentation model achieved an F1 score of 0.83, and the near-failure classifier achieved an F1 score of 0.82 under simulated real-time evaluation conditions (1.6 Hz inference rate). Deployment on a Raspberry Pi 5 yielded an average inference latency of 112 ms, and on an iPhone 16 yielded 23.5 ms, confirming the feasibility for edge computation. This work demonstrates a practical approach for objective, real-time training intensity feedback using minimal hardware, paving the way for accessible AI-driven hypertrophy coaching tools that help users manage intensity and fatigue effectively.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 14,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.11856",
        "abs_url": "https://arxiv.org/abs/2512.11856",
        "pdf_url": "https://arxiv.org/pdf/2512.11856",
        "title": "GCoDE: Efficient Device-Edge Co-Inference for GNNs via Architecture-Mapping Co-Search",
        "authors": [
            "Ao Zhou",
            "Jianlei Yang",
            "Tong Qiao",
            "Yingjie Qi",
            "Zhi Yang",
            "Weisheng Zhao",
            "Chunming Hu"
        ],
        "comments": "accepted by IEEE Transactions on Computers",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Graph Neural Networks (GNNs) have emerged as the state-of-the-art graph learning method. However, achieving efficient GNN inference on edge devices poses significant challenges, limiting their application in real-world edge scenarios. This is due to the high computational cost of GNNs and limited hardware resources on edge devices, which prevent GNN inference from meeting real-time and energy requirements. As an emerging paradigm, device-edge co-inference shows potential for improving inference efficiency and reducing energy consumption on edge devices. Despite its potential, research on GNN device-edge co-inference remains scarce, and our findings show that traditional model partitioning methods are ineffective for GNNs. To address this, we propose GCoDE, the first automatic framework for GNN architecture-mapping Co-design and deployment on Device-Edge hierarchies. By abstracting the device communication process into an explicit operation, GCoDE fuses the architecture and mapping scheme in a unified design space for joint optimization. Additionally, GCoDE's system performance awareness enables effective evaluation of architecture efficiency across diverse heterogeneous systems. By analyzing the energy consumption of various GNN operations, GCoDE introduces an energy prediction method that improves energy assessment accuracy and identifies energy-efficient solutions. Using a constraint-based random search strategy, GCoDE identifies the optimal solution in 1.5 hours, balancing accuracy and efficiency. Moreover, the integrated co-inference engine in GCoDE enables efficient deployment and execution of GNN co-inference. Experimental results show that GCoDE can achieve up to 44.9x speedup and 98.2% energy reduction compared to existing approaches across diverse applications and system configurations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 15,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.11858",
        "abs_url": "https://arxiv.org/abs/2512.11858",
        "pdf_url": "https://arxiv.org/pdf/2512.11858",
        "title": "Adaptive Path Integral Diffusion: AdaPID",
        "authors": [
            "Michael Chertkov",
            "Hamidreza Behjoo"
        ],
        "comments": "51 pages, 17 figures",
        "subjects": "Machine Learning (cs.LG); Statistical Mechanics (cond-mat.stat-mech); Artificial Intelligence (cs.AI); Systems and Control (eess.SY); Machine Learning (stat.ML)",
        "abstract": "Diffusion-based samplers -- Score Based Diffusions, Bridge Diffusions and Path Integral Diffusions -- match a target at terminal time, but the real leverage comes from choosing the schedule that governs the intermediate-time dynamics. We develop a path-wise schedule -- selection gramework for Harmonic PID with a time-varying stiffness, exploiting Piece-Wise-Constant(PWC) parametrizations and a simple hierarchical refinement. We introduce schedule-sensitive Quality-of-Sampling (QoS) diagnostics. Assuming a Gaussian-Mixture (GM) target, we retain closed-form Green functions' ration and numerically stable, Neural-Network free oracles for predicted-state maps and score. Experiments in 2D show that QoS driven PWC schedules consistently improve early-exit fidelity, tail accuracy, conditioning of the dynamics, and speciation (label-selection) timing at fixed integration budgets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 16,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.11860",
        "abs_url": "https://arxiv.org/abs/2512.11860",
        "pdf_url": "https://arxiv.org/pdf/2512.11860",
        "title": "An Operator-Consistent Graph Neural Network for Learning Diffusion Dynamics on Irregular Meshes",
        "authors": [
            "Yuelian Li",
            "Andrew Rushing Hands"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Classical numerical methods solve partial differential equations (PDEs) efficiently on regular meshes, but many of them become unstable on irregular domains. In practice, multiphysics interactions such as diffusion, damage, and healing often take place on irregular meshes. We develop an operator-consistent graph neural network (OCGNN-PINN) that approximates PDE evolution under physics-informed constraints. It couples node-edge message passing with a consistency loss enforcing the gradient-divergence relation through the graph incidence matrix, ensuring that discrete node and edge dynamics remain structurally coupled during temporal rollout. We evaluate the model on diffusion processes over physically driven evolving meshes and real-world scanned surfaces. The results show improved temporal stability and prediction accuracy compared with graph convolutional and multilayer perceptron baselines, approaching the performance of Crank-Nicolson solvers on unstructured domains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 17,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.11866",
        "abs_url": "https://arxiv.org/abs/2512.11866",
        "pdf_url": "https://arxiv.org/pdf/2512.11866",
        "title": "Phase transitions reveal hierarchical structure in deep neural networks",
        "authors": [
            "Ibrahim Talha Ersoy",
            "Andrés Fernando Cardozo Licha",
            "Karoline Wiesner"
        ],
        "comments": "15 pages, 5 figures",
        "subjects": "Machine Learning (cs.LG); Data Analysis, Statistics and Probability (physics.data-an)",
        "abstract": "Training Deep Neural Networks relies on the model converging on a high-dimensional, non-convex loss landscape toward a good minimum. Yet, much of the phenomenology of training remains ill understood. We focus on three seemingly disparate observations: the occurrence of phase transitions reminiscent of statistical physics, the ubiquity of saddle points, and phenomenon of mode connectivity relevant for model merging. We unify these within a single explanatory framework, the geometry of the loss and error landscapes. We analytically show that phase transitions in DNN learning are governed by saddle points in the loss landscape. Building on this insight, we introduce a simple, fast, and easy to implement algorithm that uses the L2 regularizer as a tool to probe the geometry of error landscapes. We apply it to confirm mode connectivity in DNNs trained on the MNIST dataset by efficiently finding paths that connect global minima. We then show numerically that saddle points induce transitions between models that encode distinct digit classes. Our work establishes the geometric origin of key training phenomena in DNNs and reveals a hierarchy of accuracy basins analogous to phases in statistical physics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 18,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.11946",
        "abs_url": "https://arxiv.org/abs/2512.11946",
        "pdf_url": "https://arxiv.org/pdf/2512.11946",
        "title": "Data-Driven Global Sensitivity Analysis for Engineering Design Based on Individual Conditional Expectations",
        "authors": [
            "Pramudita Satria Palar",
            "Paul Saves",
            "Rommel G. Regis",
            "Koji Shimoyama",
            "Shigeru Obayashi",
            "Nicolas Verstaevel",
            "Joseph Morlier"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "Explainable machine learning techniques have gained increasing attention in engineering applications, especially in aerospace design and analysis, where understanding how input variables influence data-driven models is essential. Partial Dependence Plots (PDPs) are widely used for interpreting black-box models by showing the average effect of an input variable on the prediction. However, their global sensitivity metric can be misleading when strong interactions are present, as averaging tends to obscure interaction effects. To address this limitation, we propose a global sensitivity metric based on Individual Conditional Expectation (ICE) curves. The method computes the expected feature importance across ICE curves, along with their standard deviation, to more effectively capture the influence of interactions. We provide a mathematical proof demonstrating that the PDP-based sensitivity is a lower bound of the proposed ICE-based metric under truncated orthogonal polynomial expansion. In addition, we introduce an ICE-based correlation value to quantify how interactions modify the relationship between inputs and the output. Comparative evaluations were performed on three cases: a 5-variable analytical function, a 5-variable wind-turbine fatigue problem, and a 9-variable airfoil aerodynamics case, where ICE-based sensitivity was benchmarked against PDP, SHapley Additive exPlanations (SHAP), and Sobol' indices. The results show that ICE-based feature importance provides richer insights than the traditional PDP-based approach, while visual interpretations from PDP, ICE, and SHAP complement one another by offering multiple perspectives.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 19,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.11949",
        "abs_url": "https://arxiv.org/abs/2512.11949",
        "pdf_url": "https://arxiv.org/pdf/2512.11949",
        "title": "Neural Chameleons: Language Models Can Learn to Hide Their Thoughts from Unseen Activation Monitors",
        "authors": [
            "Max McGuinness",
            "Alex Serrano",
            "Luke Bailey",
            "Scott Emmons"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Activation monitoring, which probes a model's internal states using lightweight classifiers, is an emerging tool for AI safety. However, its worst-case robustness under a misalignment threat model--where a model might learn to actively conceal its internal states--remains untested. Focusing on this threat model, we ask: could a model learn to evade previously unseen activation monitors? Our core contribution is to stress-test the learnability of this behavior. We demonstrate that finetuning can create Neural Chameleons: models capable of zero-shot evading activation monitors. Specifically, we fine-tune an LLM to evade monitors for a set of benign concepts (e.g., languages, HTML) when conditioned on a trigger of the form: \"You are being probed for {concept}\". We show that this learned mechanism generalizes zero-shot: by substituting {concept} with a safety-relevant term like 'deception', the model successfully evades previously unseen safety monitors. We validate this phenomenon across diverse model families (Llama, Gemma, Qwen), showing that the evasion succeeds even against monitors trained post hoc on the model's frozen weights. This evasion is highly selective, targeting only the specific concept mentioned in the trigger, and having a modest impact on model capabilities on standard benchmarks. Using Gemma-2-9b-it as a case study, a mechanistic analysis reveals this is achieved via a targeted manipulation that moves activations into a low-dimensional subspace. While stronger defenses like monitor ensembles and non-linear classifiers show greater resilience, the model retains a non-trivial evasion capability. Our work provides a proof-of-concept for this failure mode and a tool to evaluate the worst-case robustness of monitoring techniques against misalignment threat models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 20,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.11986",
        "abs_url": "https://arxiv.org/abs/2512.11986",
        "pdf_url": "https://arxiv.org/pdf/2512.11986",
        "title": "Learning to Extract Context for Context-Aware LLM Inference",
        "authors": [
            "Minseon Kim",
            "Lucas Caccia",
            "Zhengyan Shi",
            "Matheus Pereira",
            "Marc-Alexandre Côté",
            "Xingdi Yuan",
            "Alessandro Sordoni"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "User prompts to large language models (LLMs) are often ambiguous or under-specified, and subtle contextual cues shaped by user intentions, prior knowledge, and risk factors strongly influence what constitutes an appropriate response. Misinterpreting intent or risks may lead to unsafe outputs, while overly cautious interpretations can cause unnecessary refusal of benign requests. In this paper, we question the conventional framework in which LLMs generate immediate responses to requests without considering broader contextual factors. User requests are situated within broader contexts such as intentions, knowledge, and prior experience, which strongly influence what constitutes an appropriate answer. We propose a framework that extracts and leverages such contextual information from the user prompt itself. Specifically, a reinforcement learning based context generator, designed in an autoencoder-like fashion, is trained to infer contextual signals grounded in the prompt and use them to guide response generation. This approach is particularly important for safety tasks, where ambiguous requests may bypass safeguards while benign but confusing requests can trigger unnecessary refusals. Experiments show that our method reduces harmful responses by an average of 5.6% on the SafetyInstruct dataset across multiple foundation models and improves the harmonic mean of attack success rate and compliance on benign prompts by 6.2% on XSTest and WildJailbreak. These results demonstrate the effectiveness of context extraction for safer and more reliable LLM inferences.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 21,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12004",
        "abs_url": "https://arxiv.org/abs/2512.12004",
        "pdf_url": "https://arxiv.org/pdf/2512.12004",
        "title": "EnviroLLM: Resource Tracking and Optimization for Local AI",
        "authors": [
            "Troy Allen"
        ],
        "comments": "8 pages, 3 tables",
        "subjects": "Machine Learning (cs.LG); Computers and Society (cs.CY)",
        "abstract": "Large language models (LLMs) are increasingly deployed locally for privacy and accessibility, yet users lack tools to measure their resource usage, environmental impact, and efficiency metrics. This paper presents EnviroLLM, an open-source toolkit for tracking, benchmarking, and optimizing performance and energy consumption when running LLMs on personal devices. The system provides real-time process monitoring, benchmarking across multiple platforms (Ollama, LM Studio, vLLM, and OpenAI-compatible APIs), persistent storage with visualizations for longitudinal analysis, and personalized model and optimization recommendations. The system includes LLM-as-judge evaluations alongside energy and speed metrics, enabling users to assess quality-efficiency tradeoffs when testing models with custom prompts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 22,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12022",
        "abs_url": "https://arxiv.org/abs/2512.12022",
        "pdf_url": "https://arxiv.org/pdf/2512.12022",
        "title": "DFedReweighting: A Unified Framework for Objective-Oriented Reweighting in Decentralized Federated Learning",
        "authors": [
            "Kaichuang Zhang",
            "Wei Yin",
            "Jinghao Yang",
            "Ping Xu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Decentralized federated learning (DFL) has recently emerged as a promising paradigm that enables multiple clients to collaboratively train machine learning model through iterative rounds of local training, communication, and aggregation without relying on a central server which introduces potential vulnerabilities in conventional Federated Learning. Nevertheless, DFL systems continue to face a range of challenges, including fairness, robustness, etc. To address these challenges, we propose \\textbf{DFedReweighting}, a unified aggregation framework designed to achieve diverse objectives in DFL systems via a objective-oriented reweighting aggregation at the final step of each learning round. Specifically, the framework first computes preliminary weights based on \\textit{target performance metric} obtained from auxiliary dataset constructed using local data. These weights are then refined using \\textit{customized reweighting strategy}, resulting in the final aggregation weights. Our results from the theoretical analysis demonstrate that the appropriate combination of the target performance metric and the customized reweighting strategy ensures linear convergence. Experimental results consistently show that our proposed framework significantly improves fairness and robustness against Byzantine attacks in diverse scenarios. Provided that appropriate target performance metrics and customized reweighting strategy are selected, our framework can achieve a wide range of desired learning objectives.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 23,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12046",
        "abs_url": "https://arxiv.org/abs/2512.12046",
        "pdf_url": "https://arxiv.org/pdf/2512.12046",
        "title": "Goal Reaching with Eikonal-Constrained Hierarchical Quasimetric Reinforcement Learning",
        "authors": [
            "Vittorio Giammarino",
            "Ahmed H. Qureshi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Robotics (cs.RO); Systems and Control (eess.SY); Machine Learning (stat.ML)",
        "abstract": "Goal-Conditioned Reinforcement Learning (GCRL) mitigates the difficulty of reward design by framing tasks as goal reaching rather than maximizing hand-crafted reward signals. In this setting, the optimal goal-conditioned value function naturally forms a quasimetric, motivating Quasimetric RL (QRL), which constrains value learning to quasimetric mappings and enforces local consistency through discrete, trajectory-based constraints. We propose Eikonal-Constrained Quasimetric RL (Eik-QRL), a continuous-time reformulation of QRL based on the Eikonal Partial Differential Equation (PDE). This PDE-based structure makes Eik-QRL trajectory-free, requiring only sampled states and goals, while improving out-of-distribution generalization. We provide theoretical guarantees for Eik-QRL and identify limitations that arise under complex dynamics. To address these challenges, we introduce Eik-Hierarchical QRL (Eik-HiQRL), which integrates Eik-QRL into a hierarchical decomposition. Empirically, Eik-HiQRL achieves state-of-the-art performance in offline goal-conditioned navigation and yields consistent gains over QRL in manipulation tasks, matching temporal-difference methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 24,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12074",
        "abs_url": "https://arxiv.org/abs/2512.12074",
        "pdf_url": "https://arxiv.org/pdf/2512.12074",
        "title": "Physics-informed neural networks to solve inverse problems in unbounded domains",
        "authors": [
            "Gregorio Pérez-Bernal",
            "Oscar Rincón-Cardeño",
            "Silvana Montoya-Noguera",
            "Nicolás Guarín-Zapata"
        ],
        "comments": "19 pages, 15 figures",
        "subjects": "Machine Learning (cs.LG); Mathematical Physics (math-ph)",
        "abstract": "Inverse problems are extensively studied in applied mathematics, with applications ranging from acoustic tomography for medical diagnosis to geophysical exploration. Physics informed neural networks (PINNs) have emerged as a powerful tool for solving such problems, while Physics informed Kolmogorov Arnold networks (PIKANs) represent a recent benchmark that, in certain problems, promises greater interpretability and accuracy compared to PINNs, due to their nature, being constructed as a composition of polynomials. In this work, we develop a methodology for addressing inverse problems in infinite and semi infinite domains. We introduce a novel sampling strategy for the network's training points, using the negative exponential and normal distributions, alongside a dual network architecture that is trained to learn the solution and parameters of an equation with the same loss function. This design enables the solution of inverse problems without explicitly imposing boundary conditions, as long as the solutions tend to stabilize when leaving the domain of interest. The proposed architecture is implemented using both PINNs and PIKANs, and their performance is compared in terms of accuracy with respect to a known solution as well as computational time and response to a noisy environment. Our results demonstrate that, in this setting, PINNs provide a more accurate and computationally efficient solution, solving the inverse problem 1,000 times faster and in the same order of magnitude, yet with a lower relative error than PIKANs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 25,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12076",
        "abs_url": "https://arxiv.org/abs/2512.12076",
        "pdf_url": "https://arxiv.org/pdf/2512.12076",
        "title": "SigTime: Learning and Visually Explaining Time Series Signatures",
        "authors": [
            "Yu-Chia Huang",
            "Juntong Chen",
            "Dongyu Liu",
            "Kwan-Liu Ma"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Understanding and distinguishing temporal patterns in time series data is essential for scientific discovery and decision-making. For example, in biomedical research, uncovering meaningful patterns in physiological signals can improve diagnosis, risk assessment, and patient outcomes. However, existing methods for time series pattern discovery face major challenges, including high computational complexity, limited interpretability, and difficulty in capturing meaningful temporal structures. To address these gaps, we introduce a novel learning framework that jointly trains two Transformer models using complementary time series representations: shapelet-based representations to capture localized temporal structures and traditional feature engineering to encode statistical properties. The learned shapelets serve as interpretable signatures that differentiate time series across classification labels. Additionally, we develop a visual analytics system -- SigTIme -- with coordinated views to facilitate exploration of time series signatures from multiple perspectives, aiding in useful insights generation. We quantitatively evaluate our learning framework on eight publicly available datasets and one proprietary clinical dataset. Additionally, we demonstrate the effectiveness of our system through two usage scenarios along with the domain experts: one involving public ECG data and the other focused on preterm labor analysis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 26,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12086",
        "abs_url": "https://arxiv.org/abs/2512.12086",
        "pdf_url": "https://arxiv.org/pdf/2512.12086",
        "title": "CLOAK: Contrastive Guidance for Latent Diffusion-Based Data Obfuscation",
        "authors": [
            "Xin Yang",
            "Omid Ardakanian"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Cryptography and Security (cs.CR)",
        "abstract": "Data obfuscation is a promising technique for mitigating attribute inference attacks by semi-trusted parties with access to time-series data emitted by sensors. Recent advances leverage conditional generative models together with adversarial training or mutual information-based regularization to balance data privacy and utility. However, these methods often require modifying the downstream task, struggle to achieve a satisfactory privacy-utility trade-off, or are computationally intensive, making them impractical for deployment on resource-constrained mobile IoT devices. We propose Cloak, a novel data obfuscation framework based on latent diffusion models. In contrast to prior work, we employ contrastive learning to extract disentangled representations, which guide the latent diffusion process to retain useful information while concealing private information. This approach enables users with diverse privacy needs to navigate the privacy-utility trade-off with minimal retraining. Extensive experiments on four public time-series datasets, spanning multiple sensing modalities, and a dataset of facial images demonstrate that Cloak consistently outperforms state-of-the-art obfuscation techniques and is well-suited for deployment in resource-constrained settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 27,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12091",
        "abs_url": "https://arxiv.org/abs/2512.12091",
        "pdf_url": "https://arxiv.org/pdf/2512.12091",
        "title": "GraphPerf-RT: A Graph-Driven Performance Model for Hardware-Aware Scheduling of OpenMP Codes",
        "authors": [
            "Mohammad Pivezhandi",
            "Mahdi Banisharif",
            "Saeed Bakhshan",
            "Abusayeed Saifullah",
            "Ali Jannesari"
        ],
        "comments": "36 pages, 1 figure, 7 tables",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Performance prediction for OpenMP workloads on heterogeneous embedded SoCs is challenging due to complex interactions between task DAG structure, control-flow irregularity, cache and branch behavior, and thermal dynamics; classical heuristics struggle under workload irregularity, tabular regressors discard structural information, and model-free RL risks overheating resource-constrained devices. We introduce GraphPerf-RT, the first surrogate that unifies task DAG topology, CFG-derived code semantics, and runtime context (per-core DVFS, thermal state, utilization) in a heterogeneous graph representation with typed edges encoding precedence, placement, and contention. Multi-task evidential heads predict makespan, energy, cache and branch misses, and utilization with calibrated uncertainty (Normal-Inverse-Gamma), enabling risk-aware scheduling that filters low-confidence rollouts. We validate GraphPerf-RT on three embedded ARM platforms (Jetson TX2, Jetson Orin NX, RUBIK Pi), achieving R^2 > 0.95 with well-calibrated uncertainty (ECE < 0.05). To demonstrate end-to-end scheduling utility, we integrate the surrogate with four RL methods on Jetson TX2: single-agent model-free (SAMFRL), single-agent model-based (SAMBRL), multi-agent model-free (MAMFRL-D3QN), and multi-agent model-based (MAMBRL-D3QN). Experiments across 5 seeds (200 episodes each) show that MAMBRL-D3QN with GraphPerf-RT as the world model achieves 66% makespan reduction (0.97 +/- 0.35s) and 82% energy reduction (0.006 +/- 0.005J) compared to model-free baselines, demonstrating that accurate, uncertainty-aware surrogates enable effective model-based planning on thermally constrained embedded systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 28,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12116",
        "abs_url": "https://arxiv.org/abs/2512.12116",
        "pdf_url": "https://arxiv.org/pdf/2512.12116",
        "title": "Neural CDEs as Correctors for Learned Time Series Models",
        "authors": [
            "Muhammad Bilal Shahid",
            "Prajwal Koirla",
            "Cody Fleming"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Learned time-series models, whether continuous- or discrete-time, are widely used to forecast the states of a dynamical system. Such models generate multi-step forecasts either directly, by predicting the full horizon at once, or iteratively, by feeding back their own predictions at each step. In both cases, the multi-step forecasts are prone to errors. To address this, we propose a Predictor-Corrector mechanism where the Predictor is any learned time-series model and the Corrector is a neural controlled differential equation. The Predictor forecasts, and the Corrector predicts the errors of the forecasts. Adding these errors to the forecasts improves forecast performance. The proposed Corrector works with irregularly sampled time series and continuous- and discrete-time Predictors. Additionally, we introduce two regularization strategies to improve the extrapolation performance of the Corrector with accelerated training. We evaluate our Corrector with diverse Predictors, e.g., neural ordinary differential equations, Contiformer, and DLinear, on synthetic, physics simulation, and real-world forecasting datasets. The experiments demonstrate that the Predictor-Corrector mechanism consistently improves the performance compared to Predictor alone.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 29,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12122",
        "abs_url": "https://arxiv.org/abs/2512.12122",
        "pdf_url": "https://arxiv.org/pdf/2512.12122",
        "title": "High-Dimensional Tensor Discriminant Analysis: Low-Rank Discriminant Structure, Representation Synergy, and Theoretical Guarantees",
        "authors": [
            "Elynn Chen",
            "Yuefeng Han",
            "Jiayu Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "High-dimensional tensor-valued predictors arise in modern applications, increasingly as learned representations from neural networks. Existing tensor classification methods rely on sparsity or Tucker structures and often lack theoretical guarantees. Motivated by empirical evidence that discriminative signals concentrate along a few multilinear components, we introduce CP low-rank structure for the discriminant tensor, a modeling perspective not previously explored. Under a Tensor Gaussian Mixture Model, we propose high-dimensional CP low-rank Tensor Discriminant Analysis (CP-TDA) with Randomized Composite PCA (\\textsc{rc-PCA}) initialization, that is essential for handling dependent and anisotropic noise under weaker signal strength and incoherence conditions, followed by iterative refinement algorithm. We establish global convergence and minimax-optimal misclassification rates. To handle tensor data deviating from tensor normality, we develop the first semiparametric tensor discriminant model, in which learned tensor representations are mapped via deep generative models into a latent space tailored for CP-TDA. Misclassification risk decomposes into representation, approximation, and estimation errors. Numerical studies and real data analysis on graph classification demonstrate substantial gains over existing tensor classifiers and state-of-the-art graph neural networks, particularly in high-dimensional, small-sample regimes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 30,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12131",
        "abs_url": "https://arxiv.org/abs/2512.12131",
        "pdf_url": "https://arxiv.org/pdf/2512.12131",
        "title": "BOOST: BOttleneck-Optimized Scalable Training Framework for Low-Rank Large Language Models",
        "authors": [
            "Zhengyang Wang",
            "Ziyue Liu",
            "Ruijie Zhang",
            "Avinash Maurya",
            "Paul Hovland",
            "Bogdan Nicolae",
            "Franck Cappello",
            "Zheng Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Distributed, Parallel, and Cluster Computing (cs.DC)",
        "abstract": "The scale of transformer model pre-training is constrained by the increasing computation and communication cost. Low-rank bottleneck architectures offer a promising solution to significantly reduce the training time and memory footprint with minimum impact on accuracy. Despite algorithmic efficiency, bottleneck architectures scale poorly under standard tensor parallelism. Simply applying 3D parallelism designed for full-rank methods leads to excessive communication and poor GPU utilization. To address this limitation, we propose BOOST, an efficient training framework tailored for large-scale low-rank bottleneck architectures. BOOST introduces a novel Bottleneck-aware Tensor Parallelism, and combines optimizations such as online-RMSNorm, linear layer grouping, and low-rank activation checkpointing to achieve end-to-end training speedup. Evaluations on different low-rank bottleneck architectures demonstrate that BOOST achieves 1.46-1.91$\\times$ speedup over full-rank model baselines and 1.87-2.27$\\times$ speedup over low-rank model with naively integrated 3D parallelism, with improved GPU utilization and reduced communication overhead.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 31,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12132",
        "abs_url": "https://arxiv.org/abs/2512.12132",
        "pdf_url": "https://arxiv.org/pdf/2512.12132",
        "title": "On the Approximation Power of SiLU Networks: Exponential Rates and Depth Efficiency",
        "authors": [
            "Koffi O. Ayena"
        ],
        "comments": "35 pages, 20 figures, submitted to the journal",
        "subjects": "Machine Learning (cs.LG); Numerical Analysis (math.NA)",
        "abstract": "This article establishes a comprehensive theoretical framework demonstrating that SiLU (Sigmoid Linear Unit) activation networks achieve exponential approximation rates for smooth functions with explicit and improved complexity control compared to classical ReLU-based constructions. We develop a novel hierarchical construction beginning with an efficient approximation of the square function $x^2$ more compact in depth and size than comparable ReLU realizations, such as those given by Yarotsky. This construction yields an approximation error decaying as $\\mathcal{O}(\\omega^{-2k})$ using networks of depth $\\mathcal{O}(1)$. We then extend this approach through functional composition to establish sharp approximation bounds for deep SiLU networks in approximating Sobolev-class functions, with total depth $\\mathcal{O}(1)$ and size $\\mathcal{O}(\\varepsilon^{-d/n})$.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 32,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12135",
        "abs_url": "https://arxiv.org/abs/2512.12135",
        "pdf_url": "https://arxiv.org/pdf/2512.12135",
        "title": "BaRISTA: Brain Scale Informed Spatiotemporal Representation of Human Intracranial Neural Activity",
        "authors": [
            "Lucine L. Oganesian",
            "Saba Hashemi",
            "Maryam M. Shanechi"
        ],
        "comments": "Published at the 39th Annual Conference on Neural Information Processing Systems (NeurIPS 2025). Code available at this https URL",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Neurons and Cognition (q-bio.NC)",
        "abstract": "Intracranial recordings have opened a unique opportunity to simultaneously measure activity across multiregional networks in the human brain. Recent works have focused on developing transformer-based neurofoundation models of such recordings that can generalize across subjects and datasets. However, these recordings exhibit highly complex spatiotemporal interactions across diverse spatial scales, from the single-channel scale to the scale of brain regions. As such, there remain critical open questions regarding how best to encode spatial information and how to design self-supervision tasks that enable the learning of brain network patterns and enhance downstream decoding performance using such high-dimensional, multiregional recordings. To allow for exploring these questions, we propose a new spatiotemporal transformer model of multiregional neural activity and a corresponding self-supervised masked latent reconstruction task, designed to enable flexibility in the spatial scale used for token encoding and masking. Applying this model on publicly available multiregional intracranial electrophysiology (iEEG) data, we demonstrate that adjusting the spatial scale for both token encoding and masked reconstruction significantly impacts downstream decoding. Further, we find that spatial encoding at larger scales than channel-level encoding, which is commonly used in existing iEEG transformer models, improves downstream decoding performance. Finally, we demonstrate that our method allows for region-level token encoding while also maintaining accurate channel-level neural reconstruction. Taken together, our modeling framework enables exploration of the spatial scales used for token encoding and masking, reveals their importance towards self-supervised pretraining of neurofoundation models of multiregional human brain activity, and enhances downstream decoding performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 33,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12183",
        "abs_url": "https://arxiv.org/abs/2512.12183",
        "pdf_url": "https://arxiv.org/pdf/2512.12183",
        "title": "HydroDiffusion: Diffusion-Based Probabilistic Streamflow Forecasting with a State Space Backbone",
        "authors": [
            "Yihan Wang",
            "Annan Yu",
            "Lujun Zhang",
            "Charuleka Varadharajan",
            "N. Benjamin Erichson"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Geophysics (physics.geo-ph)",
        "abstract": "Recent advances have introduced diffusion models for probabilistic streamflow forecasting, demonstrating strong early flood-warning skill. However, current implementations rely on recurrent Long Short-Term Memory (LSTM) backbones and single-step training objectives, which limit their ability to capture long-range dependencies and produce coherent forecast trajectories across lead times. To address these limitations, we developed HydroDiffusion, a diffusion-based probabilistic forecasting framework with a decoder-only state space model backbone. The proposed framework jointly denoises full multi-day trajectories in a single pass, ensuring temporal coherence and mitigating error accumulation common in autoregressive prediction. HydroDiffusion is evaluated across 531 watersheds in the contiguous United States (CONUS) in the CAMELS dataset. We benchmark HydroDiffusion against two diffusion baselines with LSTM backbones, as well as the recently proposed Diffusion-based Runoff Model (DRUM). Results show that HydroDiffusion achieves strong nowcast accuracy when driven by observed meteorological forcings, and maintains consistent performance across the full simulation horizon. Moreover, HydroDiffusion delivers stronger deterministic and probabilistic forecast skill than DRUM in operational forecasting. These results establish HydroDiffusion as a robust generative modeling framework for medium-range streamflow forecasting, providing both a new modeling benchmark and a foundation for future research on probabilistic hydrologic prediction at continental scales.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 34,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12198",
        "abs_url": "https://arxiv.org/abs/2512.12198",
        "pdf_url": "https://arxiv.org/pdf/2512.12198",
        "title": "MolGuidance: Advanced Guidance Strategies for Conditional Molecular Generation with Flow Matching",
        "authors": [
            "Jirui Jin",
            "Cheng Zeng",
            "Pawan Prakash",
            "Ellad B. Tadmor",
            "Adrian Roitberg",
            "Richard G. Hennig",
            "Stefano Martiniani",
            "Mingjie Liu"
        ],
        "comments": "19 pages, 5 figures, code: this https URL",
        "subjects": "Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)",
        "abstract": "Key objectives in conditional molecular generation include ensuring chemical validity, aligning generated molecules with target properties, promoting structural diversity, and enabling efficient sampling for discovery. Recent advances in computer vision introduced a range of new guidance strategies for generative models, many of which can be adapted to support these goals. In this work, we integrate state-of-the-art guidance methods -- including classifier-free guidance, autoguidance, and model guidance -- in a leading molecule generation framework built on an SE(3)-equivariant flow matching process. We propose a hybrid guidance strategy that separately guides continuous and discrete molecular modalities -- operating on velocity fields and predicted logits, respectively -- while jointly optimizing their guidance scales via Bayesian optimization. Our implementation, benchmarked on the QM9 and QMe14S datasets, achieves new state-of-the-art performance in property alignment for de novo molecular generation. The generated molecules also exhibit high structural validity. Furthermore, we systematically compare the strengths and limitations of various guidance methods, offering insights into their broader applicability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 35,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12210",
        "abs_url": "https://arxiv.org/abs/2512.12210",
        "pdf_url": "https://arxiv.org/pdf/2512.12210",
        "title": "EEG-DLite: Dataset Distillation for Efficient Large EEG Model Training",
        "authors": [
            "Yuting Tang",
            "Weibang Jiang",
            "Shanglin Li",
            "Yong Li",
            "Chenyu Liu",
            "Xinliang Zhou",
            "Yi Ding",
            "Cuntai Guan"
        ],
        "comments": "Accepted by AAAI-2026",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Large-scale EEG foundation models have shown strong generalization across a range of downstream tasks, but their training remains resource-intensive due to the volume and variable quality of EEG data. In this work, we introduce EEG-DLite, a data distillation framework that enables more efficient pre-training by selectively removing noisy and redundant samples from large EEG datasets. EEG-DLite begins by encoding EEG segments into compact latent representations using a self-supervised autoencoder, allowing sample selection to be performed efficiently and with reduced sensitivity to noise. Based on these representations, EEG-DLite filters out outliers and minimizes redundancy, resulting in a smaller yet informative subset that retains the diversity essential for effective foundation model training. Through extensive experiments, we demonstrate that training on only 5 percent of a 2,500-hour dataset curated with EEG-DLite yields performance comparable to, and in some cases better than, training on the full dataset across multiple downstream tasks. To our knowledge, this is the first systematic study of pre-training data distillation in the context of EEG foundation models. EEG-DLite provides a scalable and practical path toward more effective and efficient physiological foundation modeling. The code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 36,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12252",
        "abs_url": "https://arxiv.org/abs/2512.12252",
        "pdf_url": "https://arxiv.org/pdf/2512.12252",
        "title": "Optimized Learned Count-Min Sketch",
        "authors": [
            "Kyosuke Nishishita",
            "Atsuki Sato",
            "Yusuke Matsui"
        ],
        "comments": "4 pages, 3 figures. Accepted at NeurIPS 2025 Workshop on Machine Learning for Systems",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Count-Min Sketch (CMS) is a memory-efficient data structure for estimating the frequency of elements in a multiset. Learned Count-Min Sketch (LCMS) enhances CMS with a machine learning model to reduce estimation error under the same memory usage, but suffers from slow construction due to empirical parameter tuning and lacks theoretical guarantees on intolerable error probability. We propose Optimized Learned Count-Min Sketch (OptLCMS), which partitions the input domain and assigns each partition to its own CMS instance, with CMS parameters analytically derived for fixed thresholds, and thresholds optimized via dynamic programming with approximate feasibility checks. This reduces the need for empirical validation, enabling faster construction while providing theoretical guarantees under these assumptions. OptLCMS also allows explicit control of the allowable error threshold, improving flexibility in practice. Experiments show that OptLCMS builds faster, achieves lower intolerable error probability, and matches the estimation accuracy of LCMS.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 37,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12276",
        "abs_url": "https://arxiv.org/abs/2512.12276",
        "pdf_url": "https://arxiv.org/pdf/2512.12276",
        "title": "Balancing Accuracy and Speed: A Multi-Fidelity Ensemble Kalman Filter with a Machine Learning Surrogate Model",
        "authors": [
            "Jeffrey van der Voort",
            "Martin Verlaan",
            "Hanne Kekkonen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Statistics Theory (math.ST); Atmospheric and Oceanic Physics (physics.ao-ph)",
        "abstract": "Currently, more and more machine learning (ML) surrogates are being developed for computationally expensive physical models. In this work we investigate the use of a Multi-Fidelity Ensemble Kalman Filter (MF-EnKF) in which the low-fidelity model is such a machine learning surrogate model, instead of a traditional low-resolution or reduced-order model. The idea behind this is to use an ensemble of a few expensive full model runs, together with an ensemble of many cheap but less accurate ML model runs. In this way we hope to reach increased accuracy within the same computational budget. We investigate the performance by testing the approach on two common test problems, namely the Lorenz-2005 model and the Quasi-Geostrophic model. By keeping the original physical model in place, we obtain a higher accuracy than when we completely replace it by the ML model. Furthermore, the MF-EnKF reaches improved accuracy within the same computational budget. The ML surrogate has similar or improved accuracy compared to the low-resolution one, but it can provide a larger speed-up. Our method contributes to increasing the effective ensemble size in the EnKF, which improves the estimation of the initial condition and hence accuracy of the predictions in fields such as meteorology and oceanography.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 38,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12285",
        "abs_url": "https://arxiv.org/abs/2512.12285",
        "pdf_url": "https://arxiv.org/pdf/2512.12285",
        "title": "Fractional Differential Equation Physics-Informed Neural Network and Its Application in Battery State Estimation",
        "authors": [
            "Lujuan Dang",
            "Zilai Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Accurate estimation of the State of Charge (SOC) is critical for ensuring the safety, reliability, and performance optimization of lithium-ion battery systems. Conventional data-driven neural network models often struggle to fully characterize the inherent complex nonlinearities and memory-dependent dynamics of electrochemical processes, significantly limiting their predictive accuracy and physical interpretability under dynamic operating conditions. To address this challenge, this study proposes a novel neural architecture termed the Fractional Differential Equation Physics-Informed Neural Network (FDIFF-PINN), which integrates fractional calculus with deep learning. The main contributions of this paper include: (1) Based on a fractional-order equivalent circuit model, a discretized fractional-order partial differential equation is constructed. (2) Comparative experiments were conducted using a dynamic charge/discharge dataset of Panasonic 18650PF batteries under multi-temperature conditions (from -10$^{\\circ}$C to 20$^{\\circ}$C).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 39,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12301",
        "abs_url": "https://arxiv.org/abs/2512.12301",
        "pdf_url": "https://arxiv.org/pdf/2512.12301",
        "title": "TwinFormer: A Dual-Level Transformer for Long-Sequence Time-Series Forecasting",
        "authors": [
            "Mahima Kumavat",
            "Aditya Maheshwari"
        ],
        "comments": "14 pages, 4 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "TwinFormer is a hierarchical Transformer for long-sequence time-series forecasting. It divides the input into non-overlapping temporal patches and processes them in two stages: (1) a Local Informer with top-$k$ Sparse Attention models intra-patch dynamics, followed by mean pooling; (2) a Global Informer captures long-range inter-patch dependencies using the same top-$k$ attention. A lightweight GRU aggregates the globally contextualized patch tokens for direct multi-horizon prediction. The resulting architecture achieves linear $O(kLd)$ time and memory complexity. On eight real-world benchmarking datasets from six different domains, including weather, stock price, temperature, power consumption, electricity, and disease, and forecasting horizons $96-720$, TwinFormer secures $27$ positions in the top two out of $34$. Out of the $27$, it achieves the best performance on MAE and RMSE at $17$ places and $10$ at the second-best place on MAE and RMSE. This consistently outperforms PatchTST, iTransformer, FEDformer, Informer, and vanilla Transformers. Ablations confirm the superiority of top-$k$ Sparse Attention over ProbSparse and the effectiveness of GRU-based aggregation. Code is available at this repository: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 40,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12325",
        "abs_url": "https://arxiv.org/abs/2512.12325",
        "pdf_url": "https://arxiv.org/pdf/2512.12325",
        "title": "Eventually LIL Regret: Almost Sure $\\ln\\ln T$ Regret for a sub-Gaussian Mixture on Unbounded Data",
        "authors": [
            "Shubhada Agrawal",
            "Aaditya Ramdas"
        ],
        "comments": "24 pages",
        "subjects": "Machine Learning (cs.LG); Statistics Theory (math.ST); Machine Learning (stat.ML)",
        "abstract": "We prove that a classic sub-Gaussian mixture proposed by Robbins in a stochastic setting actually satisfies a path-wise (deterministic) regret bound. For every path in a natural ``Ville event'' $E_\\alpha$, this regret till time $T$ is bounded by $\\ln^2(1/\\alpha)/V_T + \\ln (1/\\alpha) + \\ln \\ln V_T$ up to universal constants, where $V_T$ is a nonnegative, nondecreasing, cumulative variance process. (The bound reduces to $\\ln(1/\\alpha) + \\ln \\ln V_T$ if $V_T \\geq \\ln(1/\\alpha)$.) If the data were stochastic, then one can show that $E_\\alpha$ has probability at least $1-\\alpha$ under a wide class of distributions (eg: sub-Gaussian, symmetric, variance-bounded, etc.). In fact, we show that on the Ville event $E_0$ of probability one, the regret on every path in $E_0$ is eventually bounded by $\\ln \\ln V_T$ (up to constants). We explain how this work helps bridge the world of adversarial online learning (which usually deals with regret bounds for bounded data), with game-theoretic statistics (which can handle unbounded data, albeit using stochastic assumptions). In short, conditional regret bounds serve as a bridge between stochastic and adversarial betting.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 41,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12341",
        "abs_url": "https://arxiv.org/abs/2512.12341",
        "pdf_url": "https://arxiv.org/pdf/2512.12341",
        "title": "Uncertainty Quantification for Machine Learning: One Size Does Not Fit All",
        "authors": [
            "Paul Hofman",
            "Yusuf Sale",
            "Eyke Hüllermeier"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Proper quantification of predictive uncertainty is essential for the use of machine learning in safety-critical applications. Various uncertainty measures have been proposed for this purpose, typically claiming superiority over other measures. In this paper, we argue that there is no single best measure. Instead, uncertainty quantification should be tailored to the specific application. To this end, we use a flexible family of uncertainty measures that distinguishes between total, aleatoric, and epistemic uncertainty of second-order distributions. These measures can be instantiated with specific loss functions, so-called proper scoring rules, to control their characteristics, and we show that different characteristics are useful for different tasks. In particular, we show that, for the task of selective prediction, the scoring rule should ideally match the task loss. On the other hand, for out-of-distribution detection, our results confirm that mutual information, a widely used measure of epistemic uncertainty, performs best. Furthermore, in an active learning setting, epistemic uncertainty based on zero-one loss is shown to consistently outperform other uncertainty measures.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 42,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12365",
        "abs_url": "https://arxiv.org/abs/2512.12365",
        "pdf_url": "https://arxiv.org/pdf/2512.12365",
        "title": "Synthetic Swarm Mosquito Dataset for Acoustic Classification: A Proof of Concept",
        "authors": [
            "Thai-Duy Dinh",
            "Minh-Luan Vo",
            "Cuong Tuan Nguyen",
            "Bich-Hien Vo"
        ],
        "comments": "Accepted at RIVF 2025",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Mosquito-borne diseases pose a serious global health threat, causing over 700,000 deaths annually. This work introduces a proof-of-concept Synthetic Swarm Mosquito Dataset for Acoustic Classification, created to simulate realistic multi-species and noisy swarm conditions. Unlike conventional datasets that require labor-intensive recording of individual mosquitoes, the synthetic approach enables scalable data generation while reducing human resource demands. Using log-mel spectrograms, we evaluated lightweight deep learning architectures for the classification of mosquito species. Experiments show that these models can effectively identify six major mosquito vectors and are suitable for deployment on embedded low-power devices. The study demonstrates the potential of synthetic swarm audio datasets to accelerate acoustic mosquito research and enable scalable real-time surveillance solutions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 43,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12384",
        "abs_url": "https://arxiv.org/abs/2512.12384",
        "pdf_url": "https://arxiv.org/pdf/2512.12384",
        "title": "The Data Efficiency Frontier of Financial Foundation Models: Scaling Laws from Continued Pretraining",
        "authors": [
            "Jesse Ponnock"
        ],
        "comments": "8 pages, 4 figures, 1 table",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
        "abstract": "Domain-adaptive pretraining (DAPT) offers a practical path to specializing large language models for high-value domains without full retraining. We conduct an early-stage scaling-law analysis of continued pretraining on U.S. SEC filings, training 1B and 3B-parameter Llama-3.2 models on a 400M-token financial corpus with validation checkpoints at 50M, 100M, 200M, and 400M tokens. Results show consistent improvements in SEC-domain validation loss for both models, with the largest gains occurring within the first 200M tokens and diminishing returns thereafter. Power-law fits reveal shallow exponents, indicating that financial language is highly regular and efficiently learnable under continued pretraining. General-domain validation loss remains effectively unchanged across all token budgets, suggesting minimal drift and no signs of catastrophic forgetting. A data-efficiency frontier further shows that both models move toward improved specialization with negligible mixed-domain degradation. Together, these findings provide early empirical guidance for scaling financial foundation models, suggesting that meaningful domain adaptation can be achieved with comparatively modest token budgets and that larger model scales (7B-70B) remain tractable under projected data requirements.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 44,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12387",
        "abs_url": "https://arxiv.org/abs/2512.12387",
        "pdf_url": "https://arxiv.org/pdf/2512.12387",
        "title": "Anchoring Values in Temporal and Group Dimensions for Flow Matching Model Alignment",
        "authors": [
            "Yawen Shao",
            "Jie Xiao",
            "Kai Zhu",
            "Yu Liu",
            "Wei Zhai",
            "Yang Cao",
            "Zheng-Jun Zha"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Group Relative Policy Optimization (GRPO) has proven highly effective in enhancing the alignment capabilities of Large Language Models (LLMs). However, current adaptations of GRPO for the flow matching-based image generation neglect a foundational conflict between its core principles and the distinct dynamics of the visual synthesis process. This mismatch leads to two key limitations: (i) Uniformly applying a sparse terminal reward across all timesteps impairs temporal credit assignment, ignoring the differing criticality of generation phases from early structure formation to late-stage tuning. (ii) Exclusive reliance on relative, intra-group rewards causes the optimization signal to fade as training converges, leading to the optimization stagnation when reward diversity is entirely depleted. To address these limitations, we propose Value-Anchored Group Policy Optimization (VGPO), a framework that redefines value estimation across both temporal and group dimensions. Specifically, VGPO transforms the sparse terminal reward into dense, process-aware value estimates, enabling precise credit assignment by modeling the expected cumulative reward at each generative stage. Furthermore, VGPO replaces standard group normalization with a novel process enhanced by absolute values to maintain a stable optimization signal even as reward diversity declines. Extensive experiments on three benchmarks demonstrate that VGPO achieves state-of-the-art image quality while simultaneously improving task-specific accuracy, effectively mitigating reward hacking. Project webpage: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 45,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12402",
        "abs_url": "https://arxiv.org/abs/2512.12402",
        "pdf_url": "https://arxiv.org/pdf/2512.12402",
        "title": "DeepVekua: Geometric-Spectral Representation Learning for Physics-Informed Fields",
        "authors": [
            "Vladimer Khasia"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We present DeepVekua, a hybrid architecture that unifies geometric deep learning with spectral analysis to solve partial differential equations (PDEs) in sparse data regimes. By learning a diffeomorphic coordinate transformation that maps complex geometries to a latent harmonic space, our method outperforms state-of-the-art implicit representations on advection-diffusion systems. Unlike standard coordinate-based networks which struggle with spectral bias, DeepVekua separates the learning of geometry from the learning of physics, solving for optimal spectral weights in closed form. We demonstrate a 100x improvement over spectral baselines. The code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 46,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12405",
        "abs_url": "https://arxiv.org/abs/2512.12405",
        "pdf_url": "https://arxiv.org/pdf/2512.12405",
        "title": "Can Graphs Improve Tabular Foundation Models?",
        "authors": [
            "Franck Le",
            "Keith Grueneberg",
            "Erich Nahum",
            "Vadim Sheinin"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Tabular data are central to many real-world systems. While recent tabular transformers and in-context learners such as SAINT, TP-BERTa, TabPFN, TabICL, and MITRA incorporate limited inter-row reasoning, most approaches still lack an explicit mechanism to model relationships among instances, even though similar samples often share related outcomes. We investigate whether introducing \\emph{simple graph priors} can enhance \\emph{pretrained tabular transformers}. Concretely, we introduce {BOLERO}, a lightweight, static bipartite graph head that augments {RoBERTa-Tab} (a RoBERTa-style tabular backbone pretrained with masked-token prediction.) Each instance connects to feature/value anchors; a small GNN refines row representations, while the backbone remains frozen. We evaluate on 80 classification and 64 regression datasets from the TP-BERTa benchmark suites, comparing against strong baselines including XGBoost, CatBoost, TabPFN-v2, MITRA, TabICL, TP-BERTa, and RoBERTa-Tab. To ensure statistically sound conclusions, we follow best practices for multi-dataset evaluation: pairwise Wilcoxon signed-rank tests on per-dataset score differences and effect sizes (median improvement with confidence intervals), rather than mean-rank post-hoc tests that depend on the competitor pool. BOLERO achieves the highest number of statistically significant wins across both classification and regression, demonstrating that lightweight graph priors meaningfully improve pretrained tabular transformers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 47,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12428",
        "abs_url": "https://arxiv.org/abs/2512.12428",
        "pdf_url": "https://arxiv.org/pdf/2512.12428",
        "title": "Learning Dynamics in Memristor-Based Equilibrium Propagation",
        "authors": [
            "Michael Döll",
            "Andreas Müller",
            "Bernd Ulmann"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Emerging Technologies (cs.ET); Neural and Evolutionary Computing (cs.NE)",
        "abstract": "Memristor-based in-memory computing has emerged as a promising paradigm to overcome the constraints of the von Neumann bottleneck and the memory wall by enabling fully parallelisable and energy-efficient vector-matrix multiplications. We investigate the effect of nonlinear, memristor-driven weight updates on the convergence behaviour of neural networks trained with equilibrium propagation (EqProp). Six memristor models were characterised by their voltage-current hysteresis and integrated into the EBANA framework for evaluation on two benchmark classification tasks. EqProp can achieve robust convergence under nonlinear weight updates, provided that memristors exhibit a sufficiently wide resistance range of at least an order of magnitude.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 48,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12436",
        "abs_url": "https://arxiv.org/abs/2512.12436",
        "pdf_url": "https://arxiv.org/pdf/2512.12436",
        "title": "Rough Sets for Explainability of Spectral Graph Clustering",
        "authors": [
            "Bartłomiej Starosta",
            "Sławomir T. Wierzchoń",
            "Piotr Borkowski",
            "Dariusz Czerski",
            "Marcin Sydow",
            "Eryk Laskowski",
            "Mieczysław A. Kłopotek"
        ],
        "comments": "24 figures, 21tables",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Graph Spectral Clustering methods (GSC) allow representing clusters of diverse shapes, densities, etc. However, the results of such algorithms, when applied e.g. to text documents, are hard to explain to the user, especially due to embedding in the spectral space which has no obvious relation to document contents. Furthermore, the presence of documents without clear content meaning and the stochastic nature of the clustering algorithms deteriorate explainability. This paper proposes an enhancement to the explanation methodology, proposed in an earlier research of our team. It allows us to overcome the latter problems by taking inspiration from rough set theory.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 49,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12445",
        "abs_url": "https://arxiv.org/abs/2512.12445",
        "pdf_url": "https://arxiv.org/pdf/2512.12445",
        "title": "Knowledge-Guided Masked Autoencoder with Linear Spectral Mixing and Spectral-Angle-Aware Reconstruction",
        "authors": [
            "Abdul Matin",
            "Rupasree Dey",
            "Tanjim Bin Faruk",
            "Shrideep Pallickara",
            "Sangmi Lee Pallickara"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Integrating domain knowledge into deep learning has emerged as a promising direction for improving model interpretability, generalization, and data efficiency. In this work, we present a novel knowledge-guided ViT-based Masked Autoencoder that embeds scientific domain knowledge within the self-supervised reconstruction process. Instead of relying solely on data-driven optimization, our proposed approach incorporates the Linear Spectral Mixing Model (LSMM) as a physical constraint and physically-based Spectral Angle Mapper (SAM), ensuring that learned representations adhere to known structural relationships between observed signals and their latent components. The framework jointly optimizes LSMM and SAM loss with a conventional Huber loss objective, promoting both numerical accuracy and geometric consistency in the feature space. This knowledge-guided design enhances reconstruction fidelity, stabilizes training under limited supervision, and yields interpretable latent representations grounded in physical principles. The experimental findings indicate that the proposed model substantially enhances reconstruction quality and improves downstream task performance, highlighting the promise of embedding physics-informed inductive biases within transformer-based self-supervised learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 50,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12448",
        "abs_url": "https://arxiv.org/abs/2512.12448",
        "pdf_url": "https://arxiv.org/pdf/2512.12448",
        "title": "Optimized Architectures for Kolmogorov-Arnold Networks",
        "authors": [
            "James Bagrow",
            "Josh Bongard"
        ],
        "comments": "12 pages, 1 figure, 3 tables",
        "subjects": "Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Data Analysis, Statistics and Probability (physics.data-an); Machine Learning (stat.ML)",
        "abstract": "Efforts to improve Kolmogorov-Arnold networks (KANs) with architectural enhancements have been stymied by the complexity those enhancements bring, undermining the interpretability that makes KANs attractive in the first place. Here we study overprovisioned architectures combined with sparsification to learn compact, interpretable KANs without sacrificing accuracy. Crucially, we focus on differentiable sparsification, turning architecture search into an end-to-end optimization problem. Across function approximation benchmarks, dynamical systems forecasting, and real-world prediction tasks, we demonstrate competitive or superior accuracy while discovering substantially smaller models. Overprovisioning and sparsification are synergistic, with the combination outperforming either alone. The result is a principled path toward models that are both more expressive and more interpretable, addressing a key tension in scientific machine learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 51,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12462",
        "abs_url": "https://arxiv.org/abs/2512.12462",
        "pdf_url": "https://arxiv.org/pdf/2512.12462",
        "title": "Dynamical modeling of nonlinear latent factors in multiscale neural activity with real-time inference",
        "authors": [
            "Eray Erturk",
            "Maryam M. Shanechi"
        ],
        "comments": "Published at the 39th Annual Conference on Neural Information Processing Systems 2025. Code is available at this https URL",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Neurons and Cognition (q-bio.NC)",
        "abstract": "Real-time decoding of target variables from multiple simultaneously recorded neural time-series modalities, such as discrete spiking activity and continuous field potentials, is important across various neuroscience applications. However, a major challenge for doing so is that different neural modalities can have different timescales (i.e., sampling rates) and different probabilistic distributions, or can even be missing at some time-steps. Existing nonlinear models of multimodal neural activity do not address different timescales or missing samples across modalities. Further, some of these models do not allow for real-time decoding. Here, we develop a learning framework that can enable real-time recursive decoding while nonlinearly aggregating information across multiple modalities with different timescales and distributions and with missing samples. This framework consists of 1) a multiscale encoder that nonlinearly aggregates information after learning within-modality dynamics to handle different timescales and missing samples in real time, 2) a multiscale dynamical backbone that extracts multimodal temporal dynamics and enables real-time recursive decoding, and 3) modality-specific decoders to account for different probabilistic distributions across modalities. In both simulations and three distinct multiscale brain datasets, we show that our model can aggregate information across modalities with different timescales and distributions and missing samples to improve real-time target decoding. Further, our method outperforms various linear and nonlinear multimodal benchmarks in doing so.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 52,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12469",
        "abs_url": "https://arxiv.org/abs/2512.12469",
        "pdf_url": "https://arxiv.org/pdf/2512.12469",
        "title": "Sparse Concept Anchoring for Interpretable and Controllable Neural Representations",
        "authors": [
            "Sandy Fraser",
            "Patryk Wielopolski"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We introduce Sparse Concept Anchoring, a method that biases latent space to position a targeted subset of concepts while allowing others to self-organize, using only minimal supervision (labels for <0.1% of examples per anchored concept). Training combines activation normalization, a separation regularizer, and anchor or subspace regularizers that attract rare labeled examples to predefined directions or axis-aligned subspaces. The anchored geometry enables two practical interventions: reversible behavioral steering that projects out a concept's latent component at inference, and permanent removal via targeted weight ablation of anchored dimensions. Experiments on structured autoencoders show selective attenuation of targeted concepts with negligible impact on orthogonal features, and complete elimination with reconstruction error approaching theoretical bounds. Sparse Concept Anchoring therefore provides a practical pathway to interpretable, steerable behavior in learned representations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 53,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12489",
        "abs_url": "https://arxiv.org/abs/2512.12489",
        "pdf_url": "https://arxiv.org/pdf/2512.12489",
        "title": "GoMS: Graph of Molecule Substructure Network for Molecule Property Prediction",
        "authors": [
            "Shuhui Qu",
            "Cheolwoo Park"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "While graph neural networks have shown remarkable success in molecular property prediction, current approaches like the Equivariant Subgraph Aggregation Networks (ESAN) treat molecules as bags of independent substructures, overlooking crucial relationships between these components. We present Graph of Molecule Substructures (GoMS), a novel architecture that explicitly models the interactions and spatial arrangements between molecular substructures. Unlike ESAN's bag-based representation, GoMS constructs a graph where nodes represent subgraphs and edges capture their structural relationships, preserving critical topological information about how substructures are connected and overlap within the molecule. Through extensive experiments on public molecular datasets, we demonstrate that GoMS outperforms ESAN and other baseline methods, with particularly improvements for large molecules containing more than 100 atoms. The performance gap widens as molecular size increases, demonstrating GoMS's effectiveness for modeling industrial-scale molecules. Our theoretical analysis demonstrates that GoMS can distinguish molecules with identical subgraph compositions but different spatial arrangements. Our approach shows particular promise for materials science applications involving complex molecules where properties emerge from the interplay between multiple functional units. By capturing substructure relationships that are lost in bag-based approaches, GoMS represents a significant advance toward scalable and interpretable molecular property prediction for real-world applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 54,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12493",
        "abs_url": "https://arxiv.org/abs/2512.12493",
        "pdf_url": "https://arxiv.org/pdf/2512.12493",
        "title": "AI-Driven Early Warning Systems for Student Success: Discovering Static Feature Dominance in Temporal Prediction Models",
        "authors": [
            "Vaarunay Kaushal",
            "Rajib Mall"
        ],
        "comments": "5 pages, 3 figures, KDD 2026",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Early identification of at-risk students is critical for effective intervention in online learning environments. This study extends temporal prediction analysis to Week 20 (50% of course duration), comparing Decision Tree and Long Short- Term Memory (LSTM) models across six temporal snapshots. Our analysis reveals that different performance metrics matter at different intervention stages: high recall is critical for early intervention (Weeks 2-4), while balanced precision-recall is important for mid-course resource allocation (Weeks 8-16), and high precision becomes paramount in later stages (Week 20). We demonstrate that static demographic features dominate predictions (68% importance), enabling assessment-free early prediction. The LSTM model achieves 97% recall at Week 2, making it ideal for early intervention, while Decision Tree provides stable balanced performance (78% accuracy) during mid-course. By Week 20, both models converge to similar recall (68%), but LSTM achieves higher precision (90% vs 86%). Our findings also suggest that model selection should depend on intervention timing, and that early signals (Weeks 2-4) are sufficient for reliable initial prediction using primarily demographic and pre-enrollment information.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 55,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12497",
        "abs_url": "https://arxiv.org/abs/2512.12497",
        "pdf_url": "https://arxiv.org/pdf/2512.12497",
        "title": "Policy Optimization for Dynamic Heart Transplant Allocation",
        "authors": [
            "Ioannis Anagnostides",
            "Zachary W. Sollie",
            "Arman Kilic",
            "Tuomas Sandholm"
        ],
        "comments": "An extended abstract of this paper was presented at the scientific sessions of AHA 2025",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Heart transplantation is a viable path for patients suffering from advanced heart failure, but this lifesaving option is severely limited due to donor shortage. Although the current allocation policy was recently revised in 2018, a major concern is that it does not adequately take into account pretransplant and post-transplant mortality. In this paper, we take an important step toward addressing these deficiencies. To begin with, we use historical data from UNOS to develop a new simulator that enables us to evaluate and compare the performance of different policies. We then leverage our simulator to demonstrate that the status quo policy is considerably inferior to the myopic policy that matches incoming donors to the patient who maximizes the number of years gained by the transplant. Moreover, we develop improved policies that account for the dynamic nature of the allocation process through the use of potentials -- a measure of a patient's utility in future allocations that we introduce. We also show that batching together even a handful of donors -- which is a viable option for a certain type of donors -- further enhances performance. Our simulator also allows us to evaluate the effect of critical, and often unexplored, factors in the allocation, such as geographic proximity and the tendency to reject offers by the transplant centers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 56,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12526",
        "abs_url": "https://arxiv.org/abs/2512.12526",
        "pdf_url": "https://arxiv.org/pdf/2512.12526",
        "title": "Empirical Mode Decomposition and Graph Transformation of the MSCI World Index: A Multiscale Topological Analysis for Graph Neural Network Modeling",
        "authors": [
            "Agustín M. de los Riscos",
            "Julio E. Sandubete",
            "Diego Carmona-Fernández",
            "León Beleña"
        ],
        "comments": "19 pages, 3 figures, 6 tables",
        "subjects": "Machine Learning (cs.LG); Computational Finance (q-fin.CP)",
        "abstract": "This study applies Empirical Mode Decomposition (EMD) to the MSCI World index and converts the resulting intrinsic mode functions (IMFs) into graph representations to enable modeling with graph neural networks (GNNs). Using CEEMDAN, we extract nine IMFs spanning high-frequency fluctuations to long-term trends. Each IMF is transformed into a graph using four time-series-to-graph methods: natural visibility, horizontal visibility, recurrence, and transition graphs. Topological analysis shows clear scale-dependent structure: high-frequency IMFs yield dense, highly connected small-world graphs, whereas low-frequency IMFs produce sparser networks with longer characteristic path lengths. Visibility-based methods are more sensitive to amplitude variability and typically generate higher clustering, while recurrence graphs better preserve temporal dependencies. These results provide guidance for designing GNN architectures tailored to the structural properties of decomposed components, supporting more effective predictive modeling of financial time series.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 57,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12543",
        "abs_url": "https://arxiv.org/abs/2512.12543",
        "pdf_url": "https://arxiv.org/pdf/2512.12543",
        "title": "Effective Fine-Tuning with Eigenvector Centrality Based Pruning",
        "authors": [
            "Shaif Chowdhury",
            "Soham Biren Katlariwala",
            "Devleena Kashyap"
        ],
        "comments": "12 pages",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In social media networks a small number of highly influential users can drive large scale changes in discourse across multiple communities. Small shifts in the behavior of these users are often sufficient to propagate widely throughout the network. A similar phenomenon occurs during neural network fine tuning. Conventional fine tuning of convolutional neural networks typically adds a new linear classification layer on top of a large pre trained model. Instead we argue that improved adaptation can be achieved by first pruning the network to retain only the most important neurons and then performing fine tuning. We propose a graph theory based method for pruning neural networks that is designed to improve fine tuning performance. In this method each neuron is represented as a node and edges encode similarity between neurons. Neurons are pruned based on importance scores computed using eigenvector centrality. The resulting pruned network is then fine tuned using only the most central neurons. We evaluate the proposed method on VGGNet EfficientNet and ResNet models using the TF Flowers Caltech one zero one and Oxford Flowers one zero two datasets. The proposed approach achieves higher classification accuracy while significantly reducing model complexity. On the Oxford Flowers one zero two dataset the method achieves forty eight percent classification accuracy compared to thirty percent accuracy obtained by the baseline VGGNet model.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 58,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12567",
        "abs_url": "https://arxiv.org/abs/2512.12567",
        "pdf_url": "https://arxiv.org/pdf/2512.12567",
        "title": "Optimal Mistake Bounds for Transductive Online Learning",
        "authors": [
            "Zachary Chase",
            "Steve Hanneke",
            "Shay Moran",
            "Jonathan Shafer"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Combinatorics (math.CO); Machine Learning (stat.ML)",
        "abstract": "We resolve a 30-year-old open problem concerning the power of unlabeled data in online learning by tightly quantifying the gap between transductive and standard online learning. In the standard setting, the optimal mistake bound is characterized by the Littlestone dimension $d$ of the concept class $H$ (Littlestone 1987). We prove that in the transductive setting, the mistake bound is at least $\\Omega(\\sqrt{d})$. This constitutes an exponential improvement over previous lower bounds of $\\Omega(\\log\\log d)$, $\\Omega(\\sqrt{\\log d})$, and $\\Omega(\\log d)$, due respectively to Ben-David, Kushilevitz, and Mansour (1995, 1997) and Hanneke, Moran, and Shafer (2023). We also show that this lower bound is tight: for every $d$, there exists a class of Littlestone dimension $d$ with transductive mistake bound $O(\\sqrt{d})$. Our upper bound also improves upon the best known upper bound of $(2/3)d$ from Ben-David, Kushilevitz, and Mansour (1997). These results establish a quadratic gap between transductive and standard online learning, thereby highlighting the benefit of advance access to the unlabeled instance sequence. This contrasts with the PAC setting, where transductive and standard learning exhibit similar sample complexities.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 59,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12572",
        "abs_url": "https://arxiv.org/abs/2512.12572",
        "pdf_url": "https://arxiv.org/pdf/2512.12572",
        "title": "On the Accuracy of Newton Step and Influence Function Data Attributions",
        "authors": [
            "Ittai Rubinstein",
            "Samuel B. Hopkins"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Data attribution aims to explain model predictions by estimating how they would change if certain training points were removed, and is used in a wide range of applications, from interpretability and credit assignment to unlearning and privacy. Even in the relatively simple case of linear regressions, existing mathematical analyses of leading data attribution methods such as Influence Functions (IF) and single Newton Step (NS) remain limited in two key ways. First, they rely on global strong convexity assumptions which are often not satisfied in practice. Second, the resulting bounds scale very poorly with the number of parameters ($d$) and the number of samples removed ($k$). As a result, these analyses are not tight enough to answer fundamental questions such as \"what is the asymptotic scaling of the errors of each method?\" or \"which of these methods is more accurate for a given dataset?\" In this paper, we introduce a new analysis of the NS and IF data attribution methods for convex learning problems. To the best of our knowledge, this is the first analysis of these questions that does not assume global strong convexity and also the first explanation of [KATL19] and [RH25a]'s observation that NS data attribution is often more accurate than IF. We prove that for sufficiently well-behaved logistic regression, our bounds are asymptotically tight up to poly-logarithmic factors, yielding scaling laws for the errors in the average-case sample removals. \\[ \\mathbb{E}_{T \\subseteq [n],\\, |T| = k} \\bigl[ \\|\\hat{\\theta}_T - \\hat{\\theta}_T^{\\mathrm{NS}}\\|_2 \\bigr] = \\widetilde{\\Theta}\\!\\left(\\frac{k d}{n^2}\\right), \\qquad \\mathbb{E}_{T \\subseteq [n],\\, |T| = k} \\bigl[ \\|\\hat{\\theta}_T^{\\mathrm{NS}} - \\hat{\\theta}_T^{\\mathrm{IF}}\\|_2 \\bigr] = \\widetilde{\\Theta}\\!\\left( \\frac{(k + d)\\sqrt{k d}}{n^2} \\right). \\]",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 60,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12581",
        "abs_url": "https://arxiv.org/abs/2512.12581",
        "pdf_url": "https://arxiv.org/pdf/2512.12581",
        "title": "Differentiable Energy-Based Regularization in GANs: A Simulator-Based Exploration of VQE-Inspired Auxiliary Losses",
        "authors": [
            "David Strnadel"
        ],
        "comments": "Exploratory, simulator-based proof of concept. No claims of quantum advantage",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "This paper presents an exploratory, simulator-based proof of concept investigating whether differentiable energy terms derived from parameterized quantum circuits can serve as auxiliary regularization signals in Generative Adversarial Networks (GANs). We augment the Auxiliary Classifier GAN (ACGAN) generator objective with a Variational Quantum Eigensolver (VQE)-inspired energy term computed from class-specific Ising Hamiltonians using Qiskit's EstimatorQNN and TorchConnector. Important limitations: All experiments run on a noiseless statevector simulator with only 4 qubits, use a deliberately simple Hamiltonian parameterization, and lack ablation studies comparing against equivalent classical biases. The computational overhead (approximately 200x slower than classical ACGAN) reflects simulator artifacts rather than inherent quantum costs. On MNIST, we observe that the energy-regularized model (termed QACGAN) achieves high classification accuracy (99 to 100 percent) within 5 epochs compared to 87.8 percent for ACGAN, suggesting the auxiliary term influences class conditioning. However, sample quality metrics (FID) show high variance across runs (coefficient of variation approximately 25 percent at epoch 5), with values ranging from 19.92 to 35.96. Extended runs stabilize around FID 23 to 24, comparable to the ACGAN baseline. We explicitly do not claim quantum advantage, improved stability in any general sense, or scalability beyond this toy setting. The contribution is methodological: demonstrating that VQE-style energy computations can be integrated into GAN training loops via differentiable pathways. Whether such auxiliary signals provide benefits beyond equivalent classical regularizers remains an open question requiring systematic ablation studies, which we leave for future work.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 61,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12602",
        "abs_url": "https://arxiv.org/abs/2512.12602",
        "pdf_url": "https://arxiv.org/pdf/2512.12602",
        "title": "Error-Free Linear Attention is a Free Lunch: Exact Solution from Continuous-Time Dynamics",
        "authors": [
            "Jingdi Lei",
            "Di Zhang",
            "Soujanya Poria"
        ],
        "comments": "17 pages, 2 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Linear-time attention and State Space Models (SSMs) promise to solve the quadratic cost bottleneck in long-context language models employing softmax attention. We introduce Error-Free Linear Attention (EFLA), a numerically stable, fully parallelism and generalized formulation of the delta rule. Specifically, we formulate the online learning update as a continuous-time dynamical system and prove that its exact solution is not only attainable but also computable in linear time with full parallelism. By leveraging the rank-1 structure of the dynamics matrix, we directly derive the exact closed-form solution effectively corresponding to the infinite-order Runge-Kutta method. This attention mechanism is theoretically free from error accumulation, perfectly capturing the continuous dynamics while preserving the linear-time complexity. Through an extensive suite of experiments, we show that EFLA enables robust performance in noisy environments, achieving lower language modeling perplexity and superior downstream benchmark performance than DeltaNet without introducing additional parameters. Our work provides a new theoretical foundation for building high-fidelity, scalable linear-time attention models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 62,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12605",
        "abs_url": "https://arxiv.org/abs/2512.12605",
        "pdf_url": "https://arxiv.org/pdf/2512.12605",
        "title": "Causal inference and model explainability tools for retail",
        "authors": [
            "Pranav Gupta",
            "Nithin Surendran"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Most major retailers today have multiple divisions focused on various aspects, such as marketing, supply chain, online customer experience, store customer experience, employee productivity, and vendor fulfillment. They also regularly collect data corresponding to all these aspects as dashboards and weekly/monthly/quarterly reports. Although several machine learning and statistical techniques have been in place to analyze and predict key metrics, such models typically lack interpretability. Moreover, such techniques also do not allow the validation or discovery of causal links. In this paper, we aim to provide a recipe for applying model interpretability and causal inference for deriving sales insights. In this paper, we review the existing literature on causal inference and interpretability in the context of problems in e-commerce and retail, and apply them to a real-world dataset. We find that an inherently explainable model has a lower variance of SHAP values, and show that including multiple confounders through a double machine learning approach allows us to get the correct sign of causal effect.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 63,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12617",
        "abs_url": "https://arxiv.org/abs/2512.12617",
        "pdf_url": "https://arxiv.org/pdf/2512.12617",
        "title": "Spectral Sentinel: Scalable Byzantine-Robust Decentralized Federated Learning via Sketched Random Matrix Theory on Blockchain",
        "authors": [
            "Animesh Mishra"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Distributed, Parallel, and Cluster Computing (cs.DC)",
        "abstract": "Decentralized federated learning (DFL) enables collaborative model training without centralized trust, but it remains vulnerable to Byzantine clients that poison gradients under heterogeneous (Non-IID) data. Existing defenses face a scalability trilemma: distance-based filtering (e.g., Krum) can reject legitimate Non-IID updates, geometric-median methods incur prohibitive $O(n^2 d)$ cost, and many certified defenses are evaluated only on models below 100M parameters. We propose Spectral Sentinel, a Byzantine detection and aggregation framework that leverages a random-matrix-theoretic signature: honest Non-IID gradients produce covariance eigenspectra whose bulk follows the Marchenko-Pastur law, while Byzantine perturbations induce detectable tail anomalies. Our algorithm combines Frequent Directions sketching with data-dependent MP tracking, enabling detection on models up to 1.5B parameters using $O(k^2)$ memory with $k \\ll d$. Under a $(\\sigma,f)$ threat model with coordinate-wise honest variance bounded by $\\sigma^2$ and $f < 1/2$ adversaries, we prove $(\\epsilon,\\delta)$-Byzantine resilience with convergence rate $O(\\sigma f / \\sqrt{T} + f^2 / T)$, and we provide a matching information-theoretic lower bound $\\Omega(\\sigma f / \\sqrt{T})$, establishing minimax optimality. We implement the full system with blockchain integration on Polygon networks and validate it across 144 attack-aggregator configurations, achieving 78.4 percent average accuracy versus 48-63 percent for baseline methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 64,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12642",
        "abs_url": "https://arxiv.org/abs/2512.12642",
        "pdf_url": "https://arxiv.org/pdf/2512.12642",
        "title": "Torch Geometric Pool: the Pytorch library for pooling in Graph Neural Networks",
        "authors": [
            "Filippo Maria Bianchi",
            "Carlo Abate",
            "Ivan Marisca"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We introduce Torch Geometric Pool (tgp), a library for hierarchical pooling in Graph Neural Networks. Built upon Pytorch Geometric, Torch Geometric Pool (tgp) provides a wide variety of pooling operators, unified under a consistent API and a modular design. The library emphasizes usability and extensibility, and includes features like precomputed pooling, which significantly accelerate training for a class of operators. In this paper, we present tgp's structure and present an extensive benchmark. The latter showcases the library's features and systematically compares the performance of the implemented graph-pooling methods in different downstream tasks. The results, showing that the choice of the optimal pooling operator depends on tasks and data at hand, support the need for a library that enables fast prototyping.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 65,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12669",
        "abs_url": "https://arxiv.org/abs/2512.12669",
        "pdf_url": "https://arxiv.org/pdf/2512.12669",
        "title": "DynaGen: Unifying Temporal Knowledge Graph Reasoning with Dynamic Subgraphs and Generative Regularization",
        "authors": [
            "Jiawei Shen",
            "Jia Zhu",
            "Hanghui Guo",
            "Weijie Shi",
            "Guoqing Ma",
            "Yidan Liang",
            "Jingjiang Liu",
            "Hao Chen",
            "Shimin Di"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Temporal Knowledge Graph Reasoning (TKGR) aims to complete missing factual elements along the timeline. Depending on the temporal position of the query, the task is categorized into interpolation and extrapolation. Existing interpolation methods typically embed temporal information into individual facts to complete missing historical knowledge, while extrapolation techniques often leverage sequence models over graph snapshots to identify recurring patterns for future event prediction. These methods face two critical challenges: limited contextual modeling in interpolation and cognitive generalization bias in extrapolation. To address these, we propose a unified method for TKGR, dubbed DynaGen. For interpolation, DynaGen dynamically constructs entity-centric subgraphs and processes them with a synergistic dual-branch GNN encoder to capture evolving structural context. For extrapolation, it applies a conditional diffusion process, which forces the model to learn underlying evolutionary principles rather than just superficial patterns, enhancing its ability to predict unseen future events. Extensive experiments on six benchmark datasets show DynaGen achieves state-of-the-art performance. On average, compared to the second-best models, DynaGen improves the Mean Reciprocal Rank (MRR) score by 2.61 points for interpolation and 1.45 points for extrapolation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 66,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12671",
        "abs_url": "https://arxiv.org/abs/2512.12671",
        "pdf_url": "https://arxiv.org/pdf/2512.12671",
        "title": "On Approaches to Building Surrogate ODE Models for Diffusion Bridges",
        "authors": [
            "Maria Khilchuk",
            "Vladimir Latypov",
            "Pavel Kleshchev",
            "Alexander Hvatov"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Diffusion and Schrödinger Bridge models have established state-of-the-art performance in generative modeling but are often hampered by significant computational costs and complex training procedures. While continuous-time bridges promise faster sampling, overparameterized neural networks describe their optimal dynamics, and the underlying stochastic differential equations can be difficult to integrate efficiently. This work introduces a novel paradigm that uses surrogate models to create simpler, faster, and more flexible approximations of these dynamics. We propose two specific algorithms: SINDy Flow Matching (SINDy-FM), which leverages sparse regression to identify interpretable, symbolic differential equations from data, and a Neural-ODE reformulation of the Schrödinger Bridge (DSBM-NeuralODE) for flexible continuous-time parameterization. Our experiments on Gaussian transport tasks and MNIST latent translation demonstrate that these surrogates achieve competitive performance while offering dramatic improvements in efficiency and interpretability. The symbolic SINDy-FM models, in particular, reduce parameter counts by several orders of magnitude and enable near-instantaneous inference, paving the way for a new class of tractable and high-performing bridge models for practical deployment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 67,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12688",
        "abs_url": "https://arxiv.org/abs/2512.12688",
        "pdf_url": "https://arxiv.org/pdf/2512.12688",
        "title": "Theoretical Foundations of Prompt Engineering: From Heuristics to Expressivity",
        "authors": [
            "Dongseok Kim",
            "Hyoungsun Choi",
            "Mohamed Jismy Aashik Rasool",
            "Gisung Oh"
        ],
        "comments": "24 pages",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Prompts can switch a model's behavior even when the weights are fixed, yet this phenomenon is rarely treated as a clean theoretical object rather than a heuristic. We study the family of functions obtainable by holding a Transformer backbone fixed as an executor and varying only the prompt. Our core idea is to view the prompt as an externally injected program and to construct a simplified Transformer that interprets it to implement different computations. The construction exposes a mechanism-level decomposition: attention performs selective routing from prompt memory, the FFN performs local arithmetic conditioned on retrieved fragments, and depth-wise stacking composes these local updates into a multi-step computation. Under this viewpoint, we prove a constructive existential result showing that a single fixed backbone can approximate a broad class of target behaviors via prompts alone. The framework provides a unified starting point for formalizing trade-offs under prompt length/precision constraints and for studying structural limits of prompt-based switching, while remaining distinct from empirical claims about pretrained LLMs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 68,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12693",
        "abs_url": "https://arxiv.org/abs/2512.12693",
        "pdf_url": "https://arxiv.org/pdf/2512.12693",
        "title": "Co-Exploration and Co-Exploitation via Shared Structure in Multi-Task Bandits",
        "authors": [
            "Sumantrak Mukherjee",
            "Serafima Lebedeva",
            "Valentin Margraf",
            "Jonas Hanselle",
            "Kanta Yamaoka",
            "Viktor Bengs",
            "Stefan Konigorski",
            "Eyke Hüllermeier",
            "Sebastian Josef Vollmer"
        ],
        "comments": "18 pages, 9 figures, preprint",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "We propose a novel Bayesian framework for efficient exploration in contextual multi-task multi-armed bandit settings, where the context is only observed partially and dependencies between reward distributions are induced by latent context variables. In order to exploit these structural dependencies, our approach integrates observations across all tasks and learns a global joint distribution, while still allowing personalised inference for new tasks. In this regard, we identify two key sources of epistemic uncertainty, namely structural uncertainty in the latent reward dependencies across arms and tasks, and user-specific uncertainty due to incomplete context and limited interaction history. To put our method into practice, we represent the joint distribution over tasks and rewards using a particle-based approximation of a log-density Gaussian process. This representation enables flexible, data-driven discovery of both inter-arm and inter-task dependencies without prior assumptions on the latent variables. Empirically, we demonstrate that our method outperforms baselines such as hierarchical model bandits, especially in settings with model misspecification or complex latent heterogeneity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 69,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12708",
        "abs_url": "https://arxiv.org/abs/2512.12708",
        "pdf_url": "https://arxiv.org/pdf/2512.12708",
        "title": "Multi-Trajectory Physics-Informed Neural Networks for HJB Equations with Hard-Zero Terminal Inventory: Optimal Execution on Synthetic & SPY Data",
        "authors": [
            "Anthime Valin"
        ],
        "comments": "24 pages, 19 figures. Accepted to the NeurIPS 2025 Workshop on Generative AI in Finance",
        "subjects": "Machine Learning (cs.LG); Optimization and Control (math.OC)",
        "abstract": "We study optimal trade execution with a hard-zero terminal inventory constraint, modeled via Hamilton-Jacobi-Bellman (HJB) equations. Vanilla PINNs often under-enforce this constraint and produce unstable controls. We propose a Multi-Trajectory PINN (MT-PINN) that adds a rollout-based trajectory loss and propagates a terminal penalty on terminal inventory via backpropagation-through-time, directly enforcing zero terminal inventory. A lightweight lambda-curriculum is adopted to stabilize training as the state expands from a risk-neutral reduced HJB to a risk-averse HJB. On the Gatheral-Schied single-asset model, MT-PINN aligns closely with their derived closed-form solutions and concentrates terminal inventory tightly around zero while reducing errors along optimal paths. We apply MT-PINNs on SPY intraday data, matching TWAP when risk-neutral, and achieving lower exposure and competitive costs, especially in falling windows, for higher risk-aversion.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 70,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12731",
        "abs_url": "https://arxiv.org/abs/2512.12731",
        "pdf_url": "https://arxiv.org/pdf/2512.12731",
        "title": "Solving a Machine Learning Regression Problem Based on the Theory of Random Functions",
        "authors": [
            "Yuriy N. Bakhvalov"
        ],
        "comments": "Part 1 of 4 in the \"Polyharmonic Cascade\" cycle. 25 pages, 2 figures. Source code is available at: this https URL",
        "subjects": "Machine Learning (cs.LG); Numerical Analysis (math.NA)",
        "abstract": "This paper studies a machine learning regression problem as a multivariate approximation problem using the framework of the theory of random functions. An ab initio derivation of a regression method is proposed, starting from postulates of indifference. It is shown that if a probability measure on an infinite-dimensional function space possesses natural symmetries (invariance under translation, rotation, scaling, and Gaussianity), then the entire solution scheme, including the kernel form, the type of regularization, and the noise parameterization, follows analytically from these postulates. The resulting kernel coincides with a generalized polyharmonic spline; however, unlike existing approaches, it is not chosen empirically but arises as a consequence of the indifference principle. This result provides a theoretical foundation for a broad class of smoothing and interpolation methods, demonstrating their optimality in the absence of a priori information.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 71,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12737",
        "abs_url": "https://arxiv.org/abs/2512.12737",
        "pdf_url": "https://arxiv.org/pdf/2512.12737",
        "title": "SPARK: Igniting Communication-Efficient Decentralized Learning via Stage-wise Projected NTK and Accelerated Regularization",
        "authors": [
            "Li Xia"
        ],
        "comments": "11 pages, 8 figures",
        "subjects": "Machine Learning (cs.LG); Distributed, Parallel, and Cluster Computing (cs.DC)",
        "abstract": "Decentralized federated learning (DFL) faces critical challenges from statistical heterogeneity and communication overhead. While NTK-based methods achieve faster convergence, transmitting full Jacobian matrices is impractical for bandwidth-constrained edge networks. We propose SPARK, synergistically integrating random projection-based Jacobian compression, stage-wise annealed distillation, and Nesterov momentum acceleration. Random projections compress Jacobians while preserving spectral properties essential for convergence. Stage-wise annealed distillation transitions from pure NTK evolution to neighbor-regularized learning, counteracting compression noise. Nesterov momentum accelerates convergence through stable accumulation enabled by distillation smoothing. SPARK achieves 98.7% communication reduction compared to NTK-DFL while maintaining convergence speed and superior accuracy. With momentum, SPARK reaches target performance 3 times faster, establishing state-of-the-art results for communication-efficient decentralized learning and enabling practical deployment in bandwidth-limited edge environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 72,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12744",
        "abs_url": "https://arxiv.org/abs/2512.12744",
        "pdf_url": "https://arxiv.org/pdf/2512.12744",
        "title": "Resting Neurons, Active Insights: Improving Input Sparsification for Large Language Models",
        "authors": [
            "Haotian Xu",
            "Tian Gao",
            "Tsui-Wei Weng",
            "Tengfei Ma"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Large Language Models (LLMs) achieve state-of-the-art performance across a wide range of applications, but their massive scale poses significant challenges for both efficiency and interpretability. Structural pruning, which reduces model size by removing redundant computational units such as neurons, has been widely explored as a solution, and this study devotes to input sparsification, an increasingly popular technique that improves efficiency by selectively activating only a subset of entry values for each input. However, existing approaches focus primarily on computational savings, often overlooking the representational consequences of sparsification and leaving a noticeable performance gap compared to full models. In this work, we first reinterpret input sparsification as a form of dynamic structural pruning. Motivated by the spontaneous baseline firing rates observed in biological neurons, we introduce a small set of trainable spontaneous neurons that act as compensatory units to stabilize activations in sparsified LLMs. Experiments demonstrate that these auxiliary neurons substantially reduce the sparsification-induced performance gap while generalizing effectively across tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 73,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12779",
        "abs_url": "https://arxiv.org/abs/2512.12779",
        "pdf_url": "https://arxiv.org/pdf/2512.12779",
        "title": "OLR-WAA: Adaptive and Drift-Resilient Online Regression with Dynamic Weighted Averaging",
        "authors": [
            "Mohammad Abu-Shaira",
            "Weishi Shi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Real-world datasets frequently exhibit evolving data distributions, reflecting temporal variations and underlying shifts. Overlooking this phenomenon, known as concept drift, can substantially degrade the predictive performance of the model. Furthermore, the presence of hyperparameters in online models exacerbates this issue, as these parameters are typically fixed and lack the flexibility to dynamically adjust to evolving data. This paper introduces \"OLR-WAA: An Adaptive and Drift-Resilient Online Regression with Dynamic Weighted Average\", a hyperparameter-free model designed to tackle the challenges of non-stationary data streams and enable effective, continuous adaptation. The objective is to strike a balance between model stability and adaptability. OLR-WAA incrementally updates its base model by integrating incoming data streams, utilizing an exponentially weighted moving average. It further introduces a unique optimization mechanism that dynamically detects concept drift, quantifies its magnitude, and adjusts the model based on real-time data characteristics. Rigorous evaluations show that it matches batch regression performance in static settings and consistently outperforms or rivals state-of-the-art online models, confirming its effectiveness. Concept drift datasets reveal a performance gap that OLR-WAA effectively bridges, setting it apart from other online models. In addition, the model effectively handles confidence-based scenarios through a conservative update strategy that prioritizes stable, high-confidence data points. Notably, OLR-WAA converges rapidly, consistently yielding higher R2 values compared to other online models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 74,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12783",
        "abs_url": "https://arxiv.org/abs/2512.12783",
        "pdf_url": "https://arxiv.org/pdf/2512.12783",
        "title": "Credit Risk Estimation with Non-Financial Features: Evidence from a Synthetic Istanbul Dataset",
        "authors": [
            "Atalay Denknalbant",
            "Emre Sezdi",
            "Zeki Furkan Kutlu",
            "Polat Goktas"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Statistical Finance (q-fin.ST); Applications (stat.AP)",
        "abstract": "Financial exclusion constrains entrepreneurship, increases income volatility, and widens wealth gaps. Underbanked consumers in Istanbul often have no bureau file because their earnings and payments flow through informal channels. To study how such borrowers can be evaluated we create a synthetic dataset of one hundred thousand Istanbul residents that reproduces first quarter 2025 TÜİK census marginals and telecom usage patterns. Retrieval augmented generation feeds these public statistics into the OpenAI o3 model, which synthesises realistic yet private records. Each profile contains seven socio demographic variables and nine alternative attributes that describe phone specifications, online shopping rhythm, subscription spend, car ownership, monthly rent, and a credit card flag. To test the impact of the alternative financial data CatBoost, LightGBM, and XGBoost are each trained in two versions. Demo models use only the socio demographic variables; Full models include both socio demographic and alternative attributes. Across five fold stratified validation the alternative block raises area under the curve by about one point three percentage and lifts balanced \\(F_{1}\\) from roughly 0.84 to 0.95, a fourteen percent gain. We contribute an open Istanbul 2025 Q1 synthetic dataset, a fully reproducible modeling pipeline, and empirical evidence that a concise set of behavioural attributes can approach bureau level discrimination power while serving borrowers who lack formal credit records. These findings give lenders and regulators a transparent blueprint for extending fair and safe credit access to the underbanked.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 75,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12787",
        "abs_url": "https://arxiv.org/abs/2512.12787",
        "pdf_url": "https://arxiv.org/pdf/2512.12787",
        "title": "Unveiling Statistical Significance of Online Regression over Multiple Datasets",
        "authors": [
            "Mohammad Abu-Shaira",
            "Weishi Shi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Despite extensive focus on techniques for evaluating the performance of two learning algorithms on a single dataset, the critical challenge of developing statistical tests to compare multiple algorithms across various datasets has been largely overlooked in most machine learning research. Additionally, in the realm of Online Learning, ensuring statistical significance is essential to validate continuous learning processes, particularly for achieving rapid convergence and effectively managing concept drifts in a timely manner. Robust statistical methods are needed to assess the significance of performance differences as data evolves over time. This article examines the state-of-the-art online regression models and empirically evaluates several suitable tests. To compare multiple online regression models across various datasets, we employed the Friedman test along with corresponding post-hoc tests. For thorough evaluations, utilizing both real and synthetic datasets with 5-fold cross-validation and seed averaging ensures comprehensive assessment across various data subsets. Our tests generally confirmed the performance of competitive baselines as consistent with their individual reports. However, some statistical test results also indicate that there is still room for improvement in certain aspects of state-of-the-art methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 76,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12795",
        "abs_url": "https://arxiv.org/abs/2512.12795",
        "pdf_url": "https://arxiv.org/pdf/2512.12795",
        "title": "TRACER: Transfer Learning based Real-time Adaptation for Clinical Evolving Risk",
        "authors": [
            "Mengying Yan",
            "Ziye Tian",
            "Siqi Li",
            "Nan Liu",
            "Benjamin A. Goldstein",
            "Molei Liu",
            "Chuan Hong"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Methodology (stat.ME)",
        "abstract": "Clinical decision support tools built on electronic health records often experience performance drift due to temporal population shifts, particularly when changes in the clinical environment initially affect only a subset of patients, resulting in a transition to mixed populations. Such case-mix changes commonly arise following system-level operational updates or the emergence of new diseases, such as COVID-19. We propose TRACER (Transfer Learning-based Real-time Adaptation for Clinical Evolving Risk), a framework that identifies encounter-level transition membership and adapts predictive models using transfer learning without full retraining. In simulation studies, TRACER outperformed static models trained on historical or contemporary data. In a real-world application predicting hospital admission following emergency department visits across the COVID-19 transition, TRACER improved both discrimination and calibration. TRACER provides a scalable approach for maintaining robust predictive performance under evolving and heterogeneous clinical conditions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 77,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12816",
        "abs_url": "https://arxiv.org/abs/2512.12816",
        "pdf_url": "https://arxiv.org/pdf/2512.12816",
        "title": "Optimal Resource Allocation for ML Model Training and Deployment under Concept Drift",
        "authors": [
            "Hasan Burhan Beytur",
            "Gustavo de Veciana",
            "Haris Vikalo",
            "Kevin S Chan"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)",
        "abstract": "We study how to allocate resources for training and deployment of machine learning (ML) models under concept drift and limited budgets. We consider a setting in which a model provider distributes trained models to multiple clients whose devices support local inference but lack the ability to retrain those models, placing the burden of performance maintenance on the provider. We introduce a model-agnostic framework that captures the interaction between resource allocation, concept drift dynamics, and deployment timing. We show that optimal training policies depend critically on the aging properties of concept durations. Under sudden concept changes, we derive optimal training policies subject to budget constraints when concept durations follow distributions with Decreasing Mean Residual Life (DMRL), and show that intuitive heuristics are provably suboptimal under Increasing Mean Residual Life (IMRL). We further study model deployment under communication constraints, prove that the associated optimization problem is quasi-convex under mild conditions, and propose a randomized scheduling strategy that achieves near-optimal client-side performance. These results offer theoretical and algorithmic foundations for cost-efficient ML model management under concept drift, with implications for continual learning, distributed inference, and adaptive ML systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 78,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12827",
        "abs_url": "https://arxiv.org/abs/2512.12827",
        "pdf_url": "https://arxiv.org/pdf/2512.12827",
        "title": "GradID: Adversarial Detection via Intrinsic Dimensionality of Gradients",
        "authors": [
            "Mohammad Mahdi Razmjoo",
            "Mohammad Mahdi Sharifian",
            "Saeed Bagheri Shouraki"
        ],
        "comments": "16 pages, 8 figures",
        "subjects": "Machine Learning (cs.LG); Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Despite their remarkable performance, deep neural networks exhibit a critical vulnerability: small, often imperceptible, adversarial perturbations can lead to drastically altered model predictions. Given the stringent reliability demands of applications such as medical diagnosis and autonomous driving, robust detection of such adversarial attacks is paramount. In this paper, we investigate the geometric properties of a model's input loss landscape. We analyze the Intrinsic Dimensionality (ID) of the model's gradient parameters, which quantifies the minimal number of coordinates required to describe the data points on their underlying manifold. We reveal a distinct and consistent difference in the ID for natural and adversarial data, which forms the basis of our proposed detection method. We validate our approach across two distinct operational scenarios. First, in a batch-wise context for identifying malicious data groups, our method demonstrates high efficacy on datasets like MNIST and SVHN. Second, in the critical individual-sample setting, we establish new state-of-the-art results on challenging benchmarks such as CIFAR-10 and MS COCO. Our detector significantly surpasses existing methods against a wide array of attacks, including CW and AutoAttack, achieving detection rates consistently above 92\\% on CIFAR-10. The results underscore the robustness of our geometric approach, highlighting that intrinsic dimensionality is a powerful fingerprint for adversarial detection across diverse datasets and attack strategies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 79,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12840",
        "abs_url": "https://arxiv.org/abs/2512.12840",
        "pdf_url": "https://arxiv.org/pdf/2512.12840",
        "title": "PRIVEE: Privacy-Preserving Vertical Federated Learning Against Feature Inference Attacks",
        "authors": [
            "Sindhuja Madabushi",
            "Ahmad Faraz Khan",
            "Haider Ali",
            "Ananthram Swami",
            "Rui Ning",
            "Hongyi Wu",
            "Jin-Hee Cho"
        ],
        "comments": "12 pages, 3 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Vertical Federated Learning (VFL) enables collaborative model training across organizations that share common user samples but hold disjoint feature spaces. Despite its potential, VFL is susceptible to feature inference attacks, in which adversarial parties exploit shared confidence scores (i.e., prediction probabilities) during inference to reconstruct private input features of other participants. To counter this threat, we propose PRIVEE (PRIvacy-preserving Vertical fEderated lEarning), a novel defense mechanism named after the French word privée, meaning \"private.\" PRIVEE obfuscates confidence scores while preserving critical properties such as relative ranking and inter-score distances. Rather than exposing raw scores, PRIVEE shares only the transformed representations, mitigating the risk of reconstruction attacks without degrading model prediction accuracy. Extensive experiments show that PRIVEE achieves a threefold improvement in privacy protection compared to state-of-the-art defenses, while preserving full predictive performance against advanced feature inference attacks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 80,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12858",
        "abs_url": "https://arxiv.org/abs/2512.12858",
        "pdf_url": "https://arxiv.org/pdf/2512.12858",
        "title": "Information-Consistent Language Model Recommendations through Group Relative Policy Optimization",
        "authors": [
            "Sonal Prabhune",
            "Balaji Padmanabhan",
            "Kaushik Dutta"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Large Language Models (LLMs) are increasingly deployed in business-critical domains such as finance, education, healthcare, and customer support, where users expect consistent and reliable recommendations. Yet LLMs often exhibit variability when prompts are phrased with minor differences, even when semantically equivalent. Such inconsistency undermines trust, complicates compliance, and disrupts user experience. While personalization is desirable in certain contexts, many enterprise scenarios-such as HR onboarding, customer support, or policy disclosure-require invariant information delivery regardless of phrasing or prior conversational history. Existing approaches, including retrieval-augmented generation (RAG) and temperature tuning, improve factuality or reduce stochasticity but cannot guarantee stability across equivalent prompts. In this paper, we propose a reinforcement learning framework based on Group Relative Policy Optimization (GRPO) to directly optimize for consistency. Unlike prior applications of GRPO, which have been limited to reasoning and code generation, we adapt GRPO to enforce stability of information content across groups of semantically equivalent prompts. We introduce entropy-based helpfulness and stability rewards, treating prompt variants as groups and resetting conversational context to isolate phrasing effects. Experiments on investment and job recommendation tasks show that our GRPO-trained model reduces variability more effectively than fine-tuning or decoding-based baselines. To our knowledge, this is a novel application of GRPO for aligning LLMs toward information consistency, reframing variability not as an acceptable feature of generative diversity but as a correctable flaw in enterprise deployments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 81,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12880",
        "abs_url": "https://arxiv.org/abs/2512.12880",
        "pdf_url": "https://arxiv.org/pdf/2512.12880",
        "title": "Improving Recursive Transformers with Mixture of LoRAs",
        "authors": [
            "Mohammadmahdi Nouriborji",
            "Morteza Rohanian",
            "Omid Rohanian"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Parameter sharing in recursive transformers reduces model size but collapses layer-wise expressivity. We propose Mixture of LoRAs (MoL), a lightweight conditional-computation mechanism that inserts Low-Rank Adaptation (LoRA) experts inside a shared feed-forward network (FFN). MoL enables token-conditional weight-space modulation of the shared FFN without untying backbone parameters, unlike prior approaches that add fixed or externally attached adapters. We pretrain a modernised recursive architecture, ModernALBERT, integrating rotary embeddings, GeGLU, FlashAttention, and a distillation-based initialisation. Across GLUE, SQuAD-v2, and BEIR, ModernALBERT (50M--120M) achieves state-of-the-art performance among compact models and surpasses larger fully parameterised baselines. We also propose an expert-merging procedure that compresses MoL into a single adapter at inference while preserving accuracy, enabling efficient deployment. Our results show that conditional weight-space modulation effectively restores the expressivity lost under aggressive parameter sharing in recursive transformers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 82,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12881",
        "abs_url": "https://arxiv.org/abs/2512.12881",
        "pdf_url": "https://arxiv.org/pdf/2512.12881",
        "title": "Unsupervised learning of multiscale switching dynamical system models from multimodal neural data",
        "authors": [
            "DongKyu Kim",
            "Han-Lin Hsieh",
            "Maryam M. Shanechi"
        ],
        "comments": "30 pages, 8 figures",
        "subjects": "Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC); Machine Learning (stat.ML)",
        "abstract": "Neural population activity often exhibits regime-dependent non-stationarity in the form of switching dynamics. Learning accurate switching dynamical system models can reveal how behavior is encoded in neural activity. Existing switching approaches have primarily focused on learning models from a single neural modality, either continuous Gaussian signals or discrete Poisson signals. However, multiple neural modalities are often recorded simultaneously to measure different spatiotemporal scales of brain activity, and all these modalities can encode behavior. Moreover, regime labels are typically unavailable in training data, posing a significant challenge for learning models of regime-dependent switching dynamics. To address these challenges, we develop a novel unsupervised learning algorithm that learns the parameters of switching multiscale dynamical system models using only multiscale neural observations. We demonstrate our method using both simulations and two distinct experimental datasets with multimodal spike-LFP observations during different motor tasks. We find that our switching multiscale dynamical system models more accurately decode behavior than switching single-scale dynamical models, showing the success of multiscale neural fusion. Further, our models outperform stationary multiscale models, illustrating the importance of tracking regime-dependent non-stationarity in multimodal neural data. The developed unsupervised learning framework enables more accurate modeling of complex multiscale neural dynamics by leveraging information in multimodal recordings while incorporating regime switches. This approach holds promise for improving the performance and robustness of brain-computer interfaces over time and for advancing our understanding of the neural basis of behavior.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 83,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12889",
        "abs_url": "https://arxiv.org/abs/2512.12889",
        "pdf_url": "https://arxiv.org/pdf/2512.12889",
        "title": "Distillation of Discrete Diffusion by Exact Conditional Distribution Matching",
        "authors": [
            "Yansong Gao",
            "Yu Sun"
        ],
        "comments": "[work in progress]",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Discrete diffusion models (DDMs) are a powerful class of generative models for categorical data, but they typically require many function evaluations for a single sample, making inference expensive. Existing acceleration methods either rely on approximate simulators, such as $\\tau$-leaping, or on distillation schemes that train new student models and auxiliary networks with proxy objectives. We propose a simple and principled distillation alternative based on \\emph{conditional distribution matching}. Our key observation is that the reverse conditional distribution of clean data given a noisy state, $p_{0\\mid t}(x_0 \\mid x_t)$, admits a Markov decomposition through intermediate times and can be recovered from marginal density ratios and the known forward CTMC kernel. We exploit this structure to define distillation objectives that directly match conditional distributions between a pre-trained teacher and a low-NFE student, both for one-step and few-step samplers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 84,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12895",
        "abs_url": "https://arxiv.org/abs/2512.12895",
        "pdf_url": "https://arxiv.org/pdf/2512.12895",
        "title": "Wait, Wait, Wait... Why Do Reasoning Models Loop?",
        "authors": [
            "Charilaos Pipis",
            "Shivam Garg",
            "Vasilis Kontonis",
            "Vaishnavi Shrivastava",
            "Akshay Krishnamurthy",
            "Dimitris Papailiopoulos"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Reasoning models (e.g., DeepSeek-R1) generate long chains of thought to solve harder problems, but they often loop, repeating the same text at low temperatures or with greedy decoding. We study why this happens and what role temperature plays. With open reasoning models, we find that looping is common at low temperature. Larger models tend to loop less, and distilled students loop significantly even when their teachers rarely do. This points to mismatches between the training distribution and the learned model, which we refer to as errors in learning, as a key cause. To understand how such errors cause loops, we introduce a synthetic graph reasoning task and demonstrate two mechanisms. First, risk aversion caused by hardness of learning: when the correct progress-making action is hard to learn but an easy cyclic action is available, the model puts relatively more probability on the cyclic action and gets stuck. Second, even when there is no hardness, Transformers show an inductive bias toward temporally correlated errors, so the same few actions keep being chosen and loops appear. Higher temperature reduces looping by promoting exploration, but it does not fix the errors in learning, so generations remain much longer than necessary at high temperature; in this sense, temperature is a stopgap rather than a holistic solution. We end with a discussion of training-time interventions aimed at directly reducing errors in learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 85,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12896",
        "abs_url": "https://arxiv.org/abs/2512.12896",
        "pdf_url": "https://arxiv.org/pdf/2512.12896",
        "title": "Probability Estimation for Predicted-Occupancy Grids in Vehicle Safety Applications Based on Machine Learning",
        "authors": [
            "Parthasarathy Nadarajan",
            "Michael Botsch"
        ],
        "comments": "2016 IEEE Intelligent Vehicles Symposium",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "This paper presents a method to predict the evolution of a complex traffic scenario with multiple objects. The current state of the scenario is assumed to be known from sensors and the prediction is taking into account various hypotheses about the behavior of traffic participants. This way, the uncertainties regarding the behavior of traffic participants can be modelled in detail. In the first part of this paper a model-based approach is presented to compute Predicted-Occupancy Grids (POG), which are introduced as a grid-based probabilistic representation of the future scenario hypotheses. However, due to the large number of possible trajectories for each traffic participant, the model-based approach comes with a very high computational load. Thus, a machine-learning approach is adopted for the computation of POGs. This work uses a novel grid-based representation of the current state of the traffic scenario and performs the mapping to POGs. This representation consists of augmented cells in an occupancy grid. The adopted machine-learning approach is based on the Random Forest algorithm. Simulations of traffic scenarios are performed to compare the machine-learning with the model-based approach. The results are promising and could enable the real-time computation of POGs for vehicle safety applications. With this detailed modelling of uncertainties, crucial components in vehicle safety systems like criticality estimation and trajectory planning can be improved.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 86,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12901",
        "abs_url": "https://arxiv.org/abs/2512.12901",
        "pdf_url": "https://arxiv.org/pdf/2512.12901",
        "title": "Predicted-occupancy grids for vehicle safety applications based on autoencoders and the Random Forest algorithm",
        "authors": [
            "Parthasarathy Nadarajan",
            "Michael Botsch",
            "Sebastian Sardina"
        ],
        "comments": "2017 International Joint Conference on Neural Networks (IJCNN)",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In this paper, a probabilistic space-time representation of complex traffic scenarios is predicted using machine learning algorithms. Such a representation is significant for all active vehicle safety applications especially when performing dynamic maneuvers in a complex traffic scenario. As a first step, a hierarchical situation classifier is used to distinguish the different types of traffic scenarios. This classifier is responsible for identifying the type of the road infrastructure and the safety-relevant traffic participants of the driving environment. With each class representing similar traffic scenarios, a set of Random Forests (RFs) is individually trained to predict the probabilistic space-time representation, which depicts the future behavior of traffic participants. This representation is termed as a Predicted-Occupancy Grid (POG). The input to the RFs is an Augmented Occupancy Grid (AOG). In order to increase the learning accuracy of the RFs and to perform better predictions, the AOG is reduced to low-dimensional features using a Stacked Denoising Autoencoder (SDA). The excellent performance of the proposed machine learning approach consisting of SDAs and RFs is demonstrated in simulations and in experiments with real vehicles. An application of POGs to estimate the criticality of traffic scenarios and to determine safe trajectories is also presented.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 87,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12903",
        "abs_url": "https://arxiv.org/abs/2512.12903",
        "pdf_url": "https://arxiv.org/pdf/2512.12903",
        "title": "Next-generation reservoir computing validated by classification task",
        "authors": [
            "Ken-ichi Kitayama"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "An emerging computing paradigm, so-called next-generation reservoir computing (NG-RC) is investigated. True to its namesake, NG-RC requires no actual reservoirs for input data mixing but rather computing the polynomial terms directly from the time series inputs. However, benchmark tests so far reported have been one-sided, limited to prediction tasks of temporal waveforms such as Lorenz 63 attractor and Mackey-Glass chaotic signal. We will demonstrate for the first time that NG-RC can perform classification task as good as conventional RC. This validates the versatile computational capability of NG-RC in tasks of both prediction and classification.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 88,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12907",
        "abs_url": "https://arxiv.org/abs/2512.12907",
        "pdf_url": "https://arxiv.org/pdf/2512.12907",
        "title": "Machine Learning Architectures for the Estimation of Predicted Occupancy Grids in Road Traffic",
        "authors": [
            "Parthasarathy Nadarajan",
            "Michael Botsch",
            "Sebastian Sardina"
        ],
        "comments": "Journal of Advances in Information Technology",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "This paper introduces a novel machine learning architecture for an efficient estimation of the probabilistic space-time representation of complex traffic scenarios. A detailed representation of the future traffic scenario is of significant importance for autonomous driving and for all active safety systems. In order to predict the future space-time representation of the traffic scenario, first the type of traffic scenario is identified and then the machine learning algorithm maps the current state of the scenario to possible future states. The input to the machine learning algorithms is the current state representation of a traffic scenario, termed as the Augmented Occupancy Grid (AOG). The output is the probabilistic space-time representation which includes uncertainties regarding the behaviour of the traffic participants and is termed as the Predicted Occupancy Grid (POG). The novel architecture consists of two Stacked Denoising Autoencoders (SDAs) and a set of Random Forests. It is then compared with the other two existing architectures that comprise of SDAs and DeconvNet. The architectures are validated with the help of simulations and the comparisons are made both in terms of accuracy and computational time. Also, a brief overview on the applications of POGs in the field of active safety is presented.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 89,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12922",
        "abs_url": "https://arxiv.org/abs/2512.12922",
        "pdf_url": "https://arxiv.org/pdf/2512.12922",
        "title": "LLM-based Personalized Portfolio Recommender: Integrating Large Language Models and Reinforcement Learning for Intelligent Investment Strategy Optimization",
        "authors": [
            "Bangyu Li",
            "Boping Gu",
            "Ziyang Ding"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In modern financial markets, investors increasingly seek personalized and adaptive portfolio strategies that reflect their individual risk preferences and respond to dynamic market conditions. Traditional rule-based or static optimization approaches often fail to capture the nonlinear interactions among investor behavior, market volatility, and evolving financial objectives. To address these limitations, this paper introduces the LLM-based Personalized Portfolio Recommender , an integrated framework that combines Large Language Models, reinforcement learning, and individualized risk preference modeling to support intelligent investment decision-making.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 90,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12930",
        "abs_url": "https://arxiv.org/abs/2512.12930",
        "pdf_url": "https://arxiv.org/pdf/2512.12930",
        "title": "SeVeDo: A Heterogeneous Transformer Accelerator for Low-Bit Inference via Hierarchical Group Quantization and SVD-Guided Mixed Precision",
        "authors": [
            "Yuseon Choi",
            "Sangjin Kim",
            "Jungjun Oh",
            "Byeongcheol Kim",
            "Hoi-Jun Yoo"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Hardware Architecture (cs.AR)",
        "abstract": "Low-bit quantization is a promising technique for efficient transformer inference by reducing computational and memory overhead. However, aggressive bitwidth reduction remains challenging due to activation outliers, leading to accuracy degradation. Existing methods, such as outlier-handling and group quantization, achieve high accuracy but incur substantial energy consumption. To address this, we propose SeVeDo, an energy-efficient SVD-based heterogeneous accelerator that structurally separates outlier-sensitive components into a high-precision low-rank path, while the remaining computations are executed in a low-bit residual datapath with group quantization. To further enhance efficiency, Hierarchical Group Quantization (HGQ) combines coarse-grained floating-point scaling with fine-grained shifting, effectively reducing dequantization cost. Also, SVD-guided mixed precision (SVD-MP) statically allocates higher bitwidths to precision-sensitive components identified through low-rank decomposition, thereby minimizing floating-point operation cost. Experimental results show that SeVeDo achieves a peak energy efficiency of 13.8TOPS/W, surpassing conventional designs, with 12.7TOPS/W on ViT-Base and 13.4TOPS/W on Llama2-7B benchmarks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 91,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12932",
        "abs_url": "https://arxiv.org/abs/2512.12932",
        "pdf_url": "https://arxiv.org/pdf/2512.12932",
        "title": "Investigating Data Pruning for Pretraining Biological Foundation Models at Scale",
        "authors": [
            "Yifan Wu",
            "Jiyue Jiang",
            "Xichen Ye",
            "Yiqi Wang",
            "Chang Zhou",
            "Yitao Xu",
            "Jiayang Chen",
            "He Hu",
            "Weizhong Zhang",
            "Cheng Jin",
            "Jiao Yuan",
            "Yu Li"
        ],
        "comments": "Accepted by AAAI 2026",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE)",
        "abstract": "Biological foundation models (BioFMs), pretrained on large-scale biological sequences, have recently shown strong potential in providing meaningful representations for diverse downstream bioinformatics tasks. However, such models often rely on millions to billions of training sequences and billions of parameters, resulting in prohibitive computational costs and significant barriers to reproducibility and accessibility, particularly for academic labs. To address these challenges, we investigate the feasibility of data pruning for BioFM pretraining and propose a post-hoc influence-guided data pruning framework tailored to biological domains. Our approach introduces a subset-based self-influence formulation that enables efficient estimation of sample importance at low computational cost, and builds upon it two simple yet effective selection strategies, namely Top-k Influence (Top I) and Coverage-Centric Influence (CCI). We empirically validate our method on two representative BioFMs, RNA-FM and ESM-C. For RNA, our framework consistently outperforms random selection baselines under an extreme pruning rate of over 99 percent, demonstrating its effectiveness. Furthermore, we show the generalizability of our framework on protein-related tasks using ESM-C. In particular, our coreset even outperforms random subsets that are ten times larger in both RNA and protein settings, revealing substantial redundancy in biological sequence datasets. These findings underscore the potential of influence-guided data pruning to substantially reduce the computational cost of BioFM pretraining, paving the way for more efficient, accessible, and sustainable biological AI research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 92,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12947",
        "abs_url": "https://arxiv.org/abs/2512.12947",
        "pdf_url": "https://arxiv.org/pdf/2512.12947",
        "title": "Understanding When Graph Convolutional Networks Help: A Diagnostic Study on Label Scarcity and Structural Properties",
        "authors": [
            "Nischal Subedi",
            "Ember Kerstetter",
            "Winnie Li",
            "Silo Murphy"
        ],
        "comments": "10 pages, 8 tables,5 figures",
        "subjects": "Machine Learning (cs.LG); Applications (stat.AP); Machine Learning (stat.ML)",
        "abstract": "Graph Convolutional Networks (GCNs) have become a standard approach for semi-supervised node classification, yet practitioners lack clear guidance on when GCNs provide meaningful improvements over simpler baselines. We present a diagnostic study using the Amazon Computers co-purchase data to understand when and why GCNs help. Through systematic experiments with simulated label scarcity, feature ablation, and per-class analysis, we find that GCN performance depends critically on the interaction between graph homophily and feature quality. GCNs provide the largest gains under extreme label scarcity, where they leverage neighborhood structure to compensate for limited supervision. Surprisingly, GCNs can match their original performance even when node features are replaced with random noise, suggesting that structure alone carries sufficient signal on highly homophilous graphs. However, GCNs hurt performance when homophily is low and features are already strong, as noisy neighbors corrupt good predictions. Our quadrant analysis reveals that GCNs help in three of four conditions and only hurt when low homophily meets strong features. These findings offer practical guidance for practitioners deciding whether to adopt graph-based methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 93,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12975",
        "abs_url": "https://arxiv.org/abs/2512.12975",
        "pdf_url": "https://arxiv.org/pdf/2512.12975",
        "title": "Application of Deep Learning in Biological Data Compression",
        "authors": [
            "Chunyu Zou"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Information Theory (cs.IT)",
        "abstract": "Cryogenic electron microscopy (Cryo-EM) has become an essential tool for capturing high-resolution biological structures. Despite its advantage in visualizations, the large storage size of Cryo-EM data file poses significant challenges for researchers and educators. This paper investigates the application of deep learning, specifically implicit neural representation (INR), to compress Cryo-EM biological data. The proposed approach first extracts the binary map of each file according to the density threshold. The density map is highly repetitive, ehich can be effectively compressed by GZIP. The neural network then trains to encode spatial density information, allowing the storage of network parameters and learnable latent vectors. To improve reconstruction accuracy, I further incorporate the positional encoding to enhance spatial representation and a weighted Mean Squared Error (MSE) loss function to balance density distribution variations. Using this approach, my aim is to provide a practical and efficient biological data compression solution that can be used for educational and research purpose, while maintaining a reasonable compression ratio and reconstruction quality from file to file.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 94,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12981",
        "abs_url": "https://arxiv.org/abs/2512.12981",
        "pdf_url": "https://arxiv.org/pdf/2512.12981",
        "title": "CoDeQ: End-to-End Joint Model Compression with Dead-Zone Quantizer for High-Sparsity and Low-Precision Networks",
        "authors": [
            "Jonathan Wenshøj",
            "Tong Chen",
            "Bob Pepin",
            "Raghavendra Selvan"
        ],
        "comments": "Source code at this https URL",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "While joint pruning--quantization is theoretically superior to sequential application, current joint methods rely on auxiliary procedures outside the training loop for finding compression parameters. This reliance adds engineering complexity and hyperparameter tuning, while also lacking a direct data-driven gradient signal, which might result in sub-optimal compression. In this paper, we introduce CoDeQ, a simple, fully differentiable method for joint pruning--quantization. Our approach builds on a key observation: the dead-zone of a scalar quantizer is equivalent to magnitude pruning, and can be used to induce sparsity directly within the quantization operator. Concretely, we parameterize the dead-zone width and learn it via backpropagation, alongside the quantization parameters. This design provides explicit control of sparsity, regularized by a single global hyperparameter, while decoupling sparsity selection from bit-width selection. The result is a method for Compression with Dead-zone Quantizer (CoDeQ) that supports both fixed-precision and mixed-precision quantization (controlled by an optional second hyperparameter). It simultaneously determines the sparsity pattern and quantization parameters in a single end-to-end optimization. Consequently, CoDeQ does not require any auxiliary procedures, making the method architecture-agnostic and straightforward to implement. On ImageNet with ResNet-18, CoDeQ reduces bit operations to ~5% while maintaining close to full precision accuracy in both fixed and mixed-precision regimes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 95,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13010",
        "abs_url": "https://arxiv.org/abs/2512.13010",
        "pdf_url": "https://arxiv.org/pdf/2512.13010",
        "title": "Deep Learning-Driven Inversion Framework for Shear Modulus Estimation in Magnetic Resonance Elastography (DIME)",
        "authors": [
            "Hassan Iftikhar",
            "Rizwan Ahmad",
            "Arunark Kolipaka"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Tissues and Organs (q-bio.TO)",
        "abstract": "The Multimodal Direct Inversion (MMDI) algorithm is widely used in Magnetic Resonance Elastography (MRE) to estimate tissue shear stiffness. However, MMDI relies on the Helmholtz equation, which assumes wave propagation in a uniform, homogeneous, and infinite medium. Furthermore, the use of the Laplacian operator makes MMDI highly sensitive to noise, which compromises the accuracy and reliability of stiffness estimates. In this study, we propose the Deep-Learning driven Inversion Framework for Shear Modulus Estimation in MRE (DIME), aimed at enhancing the robustness of inversion. DIME is trained on the displacement fields-stiffness maps pair generated through Finite Element Modelling (FEM) simulations. To capture local wave behavior and improve robustness to global image variations, DIME is trained on small image patches. We first validated DIME using homogeneous and heterogeneous datasets simulated with FEM, where DIME produced stiffness maps with low inter-pixel variability, accurate boundary delineation, and higher correlation with ground truth (GT) compared to MMDI. Next, DIME was evaluated in a realistic anatomy-informed simulated liver dataset with known GT and compared directly to MMDI. DIME reproduced ground-truth stiffness patterns with high fidelity (r = 0.99, R^2 = 0.98), while MMDI showed greater underestimation. After validating DIME on synthetic data, we tested the model in in vivo liver MRE data from eight healthy and seven fibrotic subjects. DIME preserved physiologically consistent stiffness patterns and closely matched MMDI, which showed directional bias. Overall, DIME showed higher correlation with ground truth and visually similar stiffness patterns, whereas MMDI displayed a larger bias that can potentially be attributed to directional filtering. These preliminary results highlight the feasibility of DIME for clinical applications in MRE.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 96,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13034",
        "abs_url": "https://arxiv.org/abs/2512.13034",
        "pdf_url": "https://arxiv.org/pdf/2512.13034",
        "title": "Alada: Alternating Adaptation of Momentum Method for Memory-Efficient Matrix Optimization",
        "authors": [
            "Xiaoyu He",
            "Yu Cai",
            "Jin Jia",
            "Canxi Huang",
            "Wenqing Chen",
            "Zibin Zheng"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "This work proposes Alada, an adaptive momentum method for stochastic optimization over large-scale matrices. Alada employs a rank-one factorization approach to estimate the second moment of gradients, where factors are updated alternatively to minimize the estimation error. Alada achieves sublinear memory overheads and can be readily extended to optimizing tensor-shaped this http URL also equip Alada with a first moment estimation rule, which enhances the algorithm's robustness without incurring additional memory overheads. The theoretical performance of Alada aligns with that of traditional methods such as Adam. Numerical studies conducted on several natural language processing tasks demonstrate the reduction in memory overheads and the robustness in training large models relative to Adam and its variants.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 97,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13040",
        "abs_url": "https://arxiv.org/abs/2512.13040",
        "pdf_url": "https://arxiv.org/pdf/2512.13040",
        "title": "Understanding Structured Financial Data with LLMs: A Case Study on Fraud Detection",
        "authors": [
            "Xuwei Tan",
            "Yao Ma",
            "Xueru Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
        "abstract": "Detecting fraud in financial transactions typically relies on tabular models that demand heavy feature engineering to handle high-dimensional data and offer limited interpretability, making it difficult for humans to understand predictions. Large Language Models (LLMs), in contrast, can produce human-readable explanations and facilitate feature analysis, potentially reducing the manual workload of fraud analysts and informing system refinements. However, they perform poorly when applied directly to tabular fraud detection due to the difficulty of reasoning over many features, the extreme class imbalance, and the absence of contextual information. To bridge this gap, we introduce FinFRE-RAG, a two-stage approach that applies importance-guided feature reduction to serialize a compact subset of numeric/categorical attributes into natural language and performs retrieval-augmented in-context learning over label-aware, instance-level exemplars. Across four public fraud datasets and three families of open-weight LLMs, FinFRE-RAG substantially improves F1/MCC over direct prompting and is competitive with strong tabular baselines in several settings. Although these LLMs still lag behind specialized classifiers, they narrow the performance gap and provide interpretable rationales, highlighting their value as assistive tools in fraud analysis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 98,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13060",
        "abs_url": "https://arxiv.org/abs/2512.13060",
        "pdf_url": "https://arxiv.org/pdf/2512.13060",
        "title": "Deep Q-Learning-Based Intelligent Scheduling for ETL Optimization in Heterogeneous Data Environments",
        "authors": [
            "Kangning Gao",
            "Yi Hu",
            "Cong Nie",
            "Wei Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "This paper addresses the challenges of low scheduling efficiency, unbalanced resource allocation, and poor adaptability in ETL (Extract-Transform-Load) processes under heterogeneous data environments by proposing an intelligent scheduling optimization framework based on deep Q-learning. The framework formalizes the ETL scheduling process as a Markov Decision Process and enables adaptive decision-making by a reinforcement learning agent in high-dimensional state spaces to dynamically optimize task allocation and resource scheduling. The model consists of a state representation module, a feature embedding network, a Q-value estimator, and a reward evaluation mechanism, which collectively consider task dependencies, node load states, and data flow characteristics to derive the optimal scheduling strategy in complex environments. A multi-objective reward function is designed to balance key performance indicators such as average scheduling delay, task completion rate, throughput, and resource utilization. Sensitivity experiments further verify the model's robustness under changes in hyperparameters, environmental dynamics, and data scale. Experimental results show that the proposed deep Q-learning scheduling framework significantly reduces scheduling delay, improves system throughput, and enhances execution stability under multi-source heterogeneous task conditions, demonstrating the strong potential of reinforcement learning in complex data scheduling and resource management, and providing an efficient and scalable optimization strategy for intelligent data pipeline construction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 99,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13069",
        "abs_url": "https://arxiv.org/abs/2512.13069",
        "pdf_url": "https://arxiv.org/pdf/2512.13069",
        "title": "Multi-fidelity aerodynamic data fusion by autoencoder transfer learning",
        "authors": [
            "Javier Nieto-Centenero",
            "Esther Andrés",
            "Rodrigo Castellanos"
        ],
        "comments": "29 pages, 13 figures",
        "subjects": "Machine Learning (cs.LG); Fluid Dynamics (physics.flu-dyn); Machine Learning (stat.ML)",
        "abstract": "Accurate aerodynamic prediction often relies on high-fidelity simulations; however, their prohibitive computational costs severely limit their applicability in data-driven modeling. This limitation motivates the development of multi-fidelity strategies that leverage inexpensive low-fidelity information without compromising accuracy. Addressing this challenge, this work presents a multi-fidelity deep learning framework that combines autoencoder-based transfer learning with a newly developed Multi-Split Conformal Prediction (MSCP) strategy to achieve uncertainty-aware aerodynamic data fusion under extreme data scarcity. The methodology leverages abundant Low-Fidelity (LF) data to learn a compact latent physics representation, which acts as a frozen knowledge base for a decoder that is subsequently fine-tuned using scarce HF samples. Tested on surface-pressure distributions for NACA airfoils (2D) and a transonic wing (3D) databases, the model successfully corrects LF deviations and achieves high-accuracy pressure predictions using minimal HF training data. Furthermore, the MSCP framework produces robust, actionable uncertainty bands with pointwise coverage exceeding 95%. By combining extreme data efficiency with uncertainty quantification, this work offers a scalable and reliable solution for aerodynamic regression in data-scarce environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 100,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13077",
        "abs_url": "https://arxiv.org/abs/2512.13077",
        "pdf_url": "https://arxiv.org/pdf/2512.13077",
        "title": "LikeBench: Evaluating Subjective Likability in LLMs for Personalization",
        "authors": [
            "Md Awsafur Rahman",
            "Adam Gabrys",
            "Doug Kang",
            "Jingjing Sun",
            "Tian Tan",
            "Ashwin Chandramouli"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "A personalized LLM should remember user facts, apply them correctly, and adapt over time to provide responses that the user prefers. Existing LLM personalization benchmarks are largely centered on two axes: accurately recalling user information and accurately applying remembered information in downstream tasks. We argue that a third axis, likability, is both subjective and central to user experience, yet under-measured by current benchmarks. To measure likability holistically, we introduce LikeBench, a multi-session, dynamic evaluation framework that measures likability across multiple dimensions by how much an LLM can adapt over time to a user's preferences to provide more likable responses. In LikeBench, the LLMs engage in conversation with a simulated user and learn preferences only from the ongoing dialogue. As the interaction unfolds, models try to adapt to responses, and after each turn, they are evaluated for likability across seven dimensions by the same simulated user. To the best of our knowledge, we are the first to decompose likability into multiple diagnostic metrics: emotional adaptation, formality matching, knowledge adaptation, reference understanding, conversation length fit, humor fit, and callback, which makes it easier to pinpoint where a model falls short. To make the simulated user more realistic and discriminative, LikeBench uses fine-grained, psychologically grounded descriptive personas rather than the coarse high/low trait rating based personas used in prior work. Our benchmark shows that strong memory performance does not guarantee high likability: DeepSeek R1, with lower memory accuracy (86%, 17 facts/profile), outperformed Qwen3 by 28% on likability score despite Qwen3's higher memory accuracy (93%, 43 facts/profile). Even SOTA models like GPT-5 adapt well in short exchanges but show only limited robustness in longer, noisier interactions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 101,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13106",
        "abs_url": "https://arxiv.org/abs/2512.13106",
        "pdf_url": "https://arxiv.org/pdf/2512.13106",
        "title": "TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning",
        "authors": [
            "Shenzhi Yang",
            "Guangcheng Zhu",
            "Xing Zheng",
            "Yingfan MA",
            "Zhongqi Chen",
            "Bowen Song",
            "Weiqiang Wang",
            "Junbo Zhao",
            "Gang Chen",
            "Haobo Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Reinforcement learning with verifiable rewards (RLVR) has proven effective in training large reasoning models (LRMs) by leveraging answer-verifiable signals to guide policy optimization, which, however, suffers from high annotation costs. To alleviate this problem, recent work has explored unsupervised RLVR methods that derive rewards solely from the model's internal consistency, such as through entropy and majority voting. While seemingly promising, these methods often suffer from model collapse in the later stages of training, which may arise from the reinforcement of incorrect reasoning patterns in the absence of external supervision. In this work, we investigate a novel semi-supervised RLVR paradigm that utilizes a small labeled set to guide RLVR training on unlabeled samples. Our key insight is that supervised rewards are essential for stabilizing consistency-based training on unlabeled samples, ensuring that only reasoning patterns verified on labeled instances are incorporated into RL training. Technically, we propose an effective policy optimization algorithm, TraPO, that identifies reliable unlabeled samples by matching their learning trajectory similarity to labeled ones. Building on this, TraPO achieves remarkable data efficiency and strong generalization on six widely used mathematical reasoning benchmarks (AIME24/25, AMC, MATH-500, Minerva, and Olympiad) and three out-of-distribution tasks (ARC-c, GPQA-diamond, and MMLU-pro). With only 1K labeled and 3K unlabeled samples, TraPO reaches 42.6% average accuracy, surpassing the best unsupervised method trained on 45K unlabeled samples (38.3%). Notably, when using 4K labeled and 12K unlabeled samples, TraPO even outperforms the fully supervised model trained on the full 45K labeled samples on all benchmarks, while using only 10% of the labeled data. The code is available via this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 102,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13125",
        "abs_url": "https://arxiv.org/abs/2512.13125",
        "pdf_url": "https://arxiv.org/pdf/2512.13125",
        "title": "Quanvolutional Neural Networks for Spectrum Peak-Finding",
        "authors": [
            "Lukas Bischof",
            "Rudolf M. Füchslin",
            "Kurt Stockinger",
            "Pavel Sulimov"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The analysis of spectra, such as Nuclear Magnetic Resonance (NMR) spectra, for the comprehensive characterization of peaks is a challenging task for both experts and machines, especially with complex molecules. This process, also known as deconvolution, involves identifying and quantifying the peaks in the spectrum. Machine learning techniques have shown promising results in automating this process. With the advent of quantum computing, there is potential to further enhance these techniques. In this work, inspired by the success of classical Convolutional Neural Networks (CNNs), we explore the use of Quanvolutional Neural Networks (QuanvNNs) for the multi-task peak finding problem, involving both peak counting and position estimation. We implement a simple and interpretable QuanvNN architecture that can be directly compared to its classical CNN counterpart, and evaluate its performance on a synthetic NMR-inspired dataset. Our results demonstrate that QuanvNNs outperform classical CNNs on challenging spectra, achieving an 11\\% improvement in F1 score and a 30\\% reduction in mean absolute error for peak position estimation. Additionally, QuanvNNs appear to exhibit better convergence stability for harder problems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 103,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13149",
        "abs_url": "https://arxiv.org/abs/2512.13149",
        "pdf_url": "https://arxiv.org/pdf/2512.13149",
        "title": "Enhancing Node-Level Graph Domain Adaptation by Alleviating Local Dependency",
        "authors": [
            "Xinwei Tai",
            "Dongmian Zou",
            "Hongfei Wang"
        ],
        "comments": "Accepted to KDD 2026",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Recent years have witnessed significant advancements in machine learning methods on graphs. However, transferring knowledge effectively from one graph to another remains a critical challenge. This highlights the need for algorithms capable of applying information extracted from a source graph to an unlabeled target graph, a task known as unsupervised graph domain adaptation (GDA). One key difficulty in unsupervised GDA is conditional shift, which hinders transferability. In this paper, we show that conditional shift can be observed only if there exists local dependencies among node features. To support this claim, we perform a rigorous analysis and also further provide generalization bounds of GDA when dependent node features are modeled using markov chains. Guided by the theoretical findings, we propose to improve GDA by decorrelating node features, which can be specifically implemented through decorrelated GCN layers and graph transformer layers. Our experimental results demonstrate the effectiveness of this approach, showing not only substantial performance enhancements over baseline GDA methods but also clear visualizations of small intra-class distances in the learned representations. Our code is available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 104,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13165",
        "abs_url": "https://arxiv.org/abs/2512.13165",
        "pdf_url": "https://arxiv.org/pdf/2512.13165",
        "title": "SACn: Soft Actor-Critic with n-step Returns",
        "authors": [
            "Jakub Łyskawa",
            "Jakub Lewandowski",
            "Paweł Wawrzyński"
        ],
        "comments": "Accepted at ICAART 2026",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Soft Actor-Critic (SAC) is widely used in practical applications and is now one of the most relevant off-policy online model-free reinforcement learning (RL) methods. The technique of n-step returns is known to increase the convergence speed of RL algorithms compared to their 1-step returns-based versions. However, SAC is notoriously difficult to combine with n-step returns, since their usual combination introduces bias in off-policy algorithms due to the changes in action distribution. While this problem is solved by importance sampling, a method for estimating expected values of one distribution using samples from another distribution, importance sampling may result in numerical instability. In this work, we combine SAC with n-step returns in a way that overcomes this issue. We present an approach to applying numerically stable importance sampling with simplified hyperparameter selection. Furthermore, we analyze the entropy estimation approach of Soft Actor-Critic in the context of the n-step maximum entropy framework and formulate the $\\tau$-sampled entropy estimation to reduce the variance of the learning target. Finally, we formulate the Soft Actor-Critic with n-step returns (SAC$n$) algorithm that we experimentally verify on MuJoCo simulated environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 105,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13190",
        "abs_url": "https://arxiv.org/abs/2512.13190",
        "pdf_url": "https://arxiv.org/pdf/2512.13190",
        "title": "WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory",
        "authors": [
            "Jin Sob Kim",
            "Hyun Joon Park",
            "Wooseok Shin",
            "Dongil Park",
            "Sung Won Han"
        ],
        "comments": "Accepted to IEEE Transactions on Aerospace and Electronic Systems (TAES)",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "The Automatic Identification System (AIS) enables data-driven maritime surveillance but suffers from reliability issues and irregular intervals. We address vessel destination estimation using global-scope AIS data by proposing a differentiated approach that recasts long port-to-port trajectories as a nested sequence structure. Using spatial grids, this method mitigates spatio-temporal bias while preserving detailed resolution. We introduce a novel deep learning architecture, WAY, designed to process these reformulated trajectories for long-term destination estimation days to weeks in advance. WAY comprises a trajectory representation layer and Channel-Aggregative Sequential Processing (CASP) blocks. The representation layer generates multi-channel vector sequences from kinematic and non-kinematic features. CASP blocks utilize multi-headed channel- and self-attention for aggregation and sequential information delivery. Additionally, we propose a task-specialized Gradient Dropout (GD) technique to enable many-to-many training on single labels, preventing biased feedback surges by stochastically blocking gradient flow based on sample length. Experiments on 5-year AIS data demonstrate WAY's superiority over conventional spatial grid-based approaches regardless of trajectory progression. Results further confirm that adopting GD leads to performance gains. Finally, we explore WAY's potential for real-world application through multitask learning for ETA estimation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 106,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13196",
        "abs_url": "https://arxiv.org/abs/2512.13196",
        "pdf_url": "https://arxiv.org/pdf/2512.13196",
        "title": "Noise-Resilient Quantum Aggregation on NISQ for Federated ADAS Learning",
        "authors": [
            "Chethana Prasad Kabgere",
            "Sudarshan T S B"
        ],
        "comments": "This paper was accepted and presented at WinTechCon 2025, Bangalore, India, and is published in IEEE Xplore",
        "subjects": "Machine Learning (cs.LG); Hardware Architecture (cs.AR)",
        "abstract": "Advanced Driver Assistance Systems (ADAS) increasingly employ Federated Learning (FL) to collaboratively train models across distributed vehicular nodes while preserving data privacy. Yet, conventional FL aggregation remains susceptible to noise, latency, and security constraints inherent to real-time vehicular networks. This paper introduces Noise-Resilient Quantum Federated Learning (NR-QFL), a hybrid quantum-classical framework that enables secure, low-latency aggregation through variational quantum circuits (VQCs) operating under Noisy Intermediate-Scale Quantum (NISQ) conditions. The framework encodes model parameters as quantum states with adaptive gate reparameterization, ensuring bounded-error convergence and provable resilience under Completely Positive Trace-Preserving (CPTP) dynamics. NR-QFL employs quantum entropy-based client selection and multi-server coordination for fairness and stability. Empirical validation shows consistent convergence with reduced gradient variance, lower communication overhead, and enhanced noise tolerance under constrained edge conditions. The framework establishes a scalable foundation for quantum-enhanced federated learning, enabling secure, efficient, and dynamically stable ADAS intelligence at the vehicular edge.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 107,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13207",
        "abs_url": "https://arxiv.org/abs/2512.13207",
        "pdf_url": "https://arxiv.org/pdf/2512.13207",
        "title": "Evaluating Adversarial Attacks on Federated Learning for Temperature Forecasting",
        "authors": [
            "Karina Chichifoi",
            "Fabio Merizzi",
            "Michele Colajanni"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Cryptography and Security (cs.CR)",
        "abstract": "Deep learning and federated learning (FL) are becoming powerful partners for next-generation weather forecasting. Deep learning enables high-resolution spatiotemporal forecasts that can surpass traditional numerical models, while FL allows institutions in different locations to collaboratively train models without sharing raw data, addressing efficiency and security concerns. While FL has shown promise across heterogeneous regions, its distributed nature introduces new vulnerabilities. In particular, data poisoning attacks, in which compromised clients inject manipulated training data, can degrade performance or introduce systematic biases. These threats are amplified by spatial dependencies in meteorological data, allowing localized perturbations to influence broader regions through global model aggregation. In this study, we investigate how adversarial clients distort federated surface temperature forecasts trained on the Copernicus European Regional ReAnalysis (CERRA) dataset. We simulate geographically distributed clients and evaluate patch-based and global biasing attacks on regional temperature forecasts. Our results show that even a small fraction of poisoned clients can mislead predictions across large, spatially connected areas. A global temperature bias attack from a single compromised client shifts predictions by up to -1.7 K, while coordinated patch attacks more than triple the mean squared error and produce persistent regional anomalies exceeding +3.5 K. Finally, we assess trimmed mean aggregation as a defense mechanism, showing that it successfully defends against global bias attacks (2-13\\% degradation) but fails against patch attacks (281-603\\% amplification), exposing limitations of outlier-based defenses for spatially correlated data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 108,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13228",
        "abs_url": "https://arxiv.org/abs/2512.13228",
        "pdf_url": "https://arxiv.org/pdf/2512.13228",
        "title": "ModSSC: A Modular Framework for Semi-Supervised Classification on Heterogeneous Data",
        "authors": [
            "Melvin Barbaux"
        ],
        "comments": "Preprint describing the open source ModSSC framework for inductive and transductive semi-supervised classification on heterogeneous data",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Semi-supervised classification leverages both labeled and unlabeled data to improve predictive performance, but existing software support is fragmented across methods and modalities. We introduce ModSSC, an open source Python framework that unifies inductive and transductive semi-supervised classification in a modular code base. ModSSC implements a broad range of classical and recent algorithms, provides loaders for tabular, image, text, audio and graph datasets, and exposes a single configuration interface for specifying datasets, models and evaluation protocols. It supports both lightweight classical methods on small datasets running on CPU and recent deep approaches that can exploit multiple GPUs within the same experimental framework. Experiments are described declaratively in YAML, which facilitates reproducing existing work and running large comparative studies. ModSSC 1.0.0 is released under the MIT license with extensive documentation and tests, and is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 109,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13237",
        "abs_url": "https://arxiv.org/abs/2512.13237",
        "pdf_url": "https://arxiv.org/pdf/2512.13237",
        "title": "Learning to Retrieve with Weakened Labels: Robust Training under Label Noise",
        "authors": [
            "Arnab Sharma"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Information Retrieval (cs.IR)",
        "abstract": "Neural Encoders are frequently used in the NLP domain to perform dense retrieval tasks, for instance, to generate the candidate documents for a given query in question-answering tasks. However, sparse annotation and label noise in the training data make it challenging to train or fine-tune such retrieval models. Although existing works have attempted to mitigate these problems by incorporating modified loss functions or data cleaning, these approaches either require some hyperparameters to tune during training or add substantial complexity to the training setup. In this work, we consider a label weakening approach to generate robust retrieval models in the presence of label noise. Instead of enforcing a single, potentially erroneous label for each query document pair, we allow for a set of plausible labels derived from both the observed supervision and the model's confidence scores. We perform an extensive evaluation considering two retrieval models, one re-ranking model, considering four diverse ranking datasets. To this end, we also consider a realistic noisy setting by using a semantic-aware noise generation technique to generate different ratios of noise. Our initial results show that label weakening can improve the performance of the retrieval tasks in comparison to 10 different state-of-the-art loss functions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 110,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13255",
        "abs_url": "https://arxiv.org/abs/2512.13255",
        "pdf_url": "https://arxiv.org/pdf/2512.13255",
        "title": "BézierFlow: Bézier Stochastic Interpolant Schedulers for Few-Step Generation",
        "authors": [
            "Yunhong Min",
            "Juil Koo",
            "Seungwoo Yoo",
            "Minhyuk Sung"
        ],
        "comments": "Project page: this https URL",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We introduce BézierFlow, a lightweight training approach for few-step generation with pretrained diffusion and flow models. BézierFlow achieves a 2-3x performance improvement for sampling with $\\leq$ 10 NFEs while requiring only 15 minutes of training. Recent lightweight training approaches have shown promise by learning optimal timesteps, but their scope remains restricted to ODE discretizations. To broaden this scope, we propose learning the optimal transformation of the sampling trajectory by parameterizing stochastic interpolant (SI) schedulers. The main challenge lies in designing a parameterization that satisfies critical desiderata, including boundary conditions, differentiability, and monotonicity of the SNR. To effectively meet these requirements, we represent scheduler functions as Bézier functions, where control points naturally enforce these properties. This reduces the problem to learning an ordered set of points in the time range, while the interpretation of the points changes from ODE timesteps to Bézier control points. Across a range of pretrained diffusion and flow models, BézierFlow consistently outperforms prior timestep-learning methods, demonstrating the effectiveness of expanding the search space from discrete timesteps to Bézier-based trajectory transformations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 111,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13316",
        "abs_url": "https://arxiv.org/abs/2512.13316",
        "pdf_url": "https://arxiv.org/pdf/2512.13316",
        "title": "ALIGN-FL: Architecture-independent Learning through Invariant Generative component sharing in Federated Learning",
        "authors": [
            "Mayank Gulati",
            "Benedikt Groß",
            "Gerhard Wunder"
        ],
        "comments": "Accepted at 2025 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery (CyberC)",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "We present ALIGN-FL, a novel approach to distributed learning that addresses the challenge of learning from highly disjoint data distributions through selective sharing of generative components. Instead of exchanging full model parameters, our framework enables privacy-preserving learning by transferring only generative capabilities across clients, while the server performs global training using synthetic samples. Through complementary privacy mechanisms: DP-SGD with adaptive clipping and Lipschitz regularized VAE decoders and a stateful architecture supporting heterogeneous clients, we experimentally validate our approach on MNIST and Fashion-MNIST datasets with cross-domain outliers. Our analysis demonstrates that both privacy mechanisms effectively map sensitive outliers to typical data points while maintaining utility in extreme Non-IID scenarios typical of cross-silo collaborations. Index Terms: Client-invariant Learning, Federated Learning (FL), Privacy-preserving Generative Models, Non-Independent and Identically Distributed (Non-IID), Heterogeneous Architectures",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 112,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13336",
        "abs_url": "https://arxiv.org/abs/2512.13336",
        "pdf_url": "https://arxiv.org/pdf/2512.13336",
        "title": "KD-PINN: Knowledge-Distilled PINNs for ultra-low-latency real-time neural PDE solvers",
        "authors": [
            "Karim Bounja",
            "Lahcen Laayouni",
            "Abdeljalil Sakat"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Numerical Analysis (math.NA)",
        "abstract": "This work introduces Knowledge-Distilled Physics-Informed Neural Networks (KD-PINN), a framework that transfers the predictive accuracy of a high-capacity teacher model to a compact student through a continuous adaptation of the Kullback-Leibler divergence. To confirm its generality for various dynamics and dimensionalities, the framework is evaluated on a representative set of partial differential equations (PDEs). In all tested cases, the student model preserved the teacher's physical accuracy, with a mean RMSE increase below 0.64%, and achieved inference speedups ranging from 4.8x (Navier-Stokes) to 6.9x (Burgers). The distillation process also revealed a regularizing effect. With an average inference latency of 5.3 ms on CPU, the distilled models enter the ultra-low-latency real-time regime defined by sub-10 ms performance. Finally, this study examines how knowledge distillation reduces inference latency in PINNs to contribute to the development of accurate ultra-low-latency neural PDE solvers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 113,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13337",
        "abs_url": "https://arxiv.org/abs/2512.13337",
        "pdf_url": "https://arxiv.org/pdf/2512.13337",
        "title": "FROC: A Unified Framework with Risk-Optimized Control for Machine Unlearning in LLMs",
        "authors": [
            "Si Qi Goh",
            "Yongsen Zheng",
            "Ziyao Liu",
            "Sami Hormi",
            "Kwok-Yan Lam"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Machine unlearning (MU) seeks to eliminate the influence of specific training examples from deployed models. As large language models (LLMs) become widely used, managing risks arising from insufficient forgetting or utility loss is increasingly crucial. Current MU techniques lack effective mechanisms for evaluating and controlling these risks, hindering the selection of strategies that appropriately balance safety and utility, and raising trust concerns surrounding the \"right to be forgotten.\" To address these issues, we propose FROC, a unified framework with Risk-Optimized Control for machine unlearning in LLMs. FROC is built around a conformal-style risk-control formulation that expresses a user-specified risk budget on unlearning behavior. This probability-based constraint enables FROC to compare MU strategies, identify feasible operating regions, and guide hyperparameter selection according to desired trade-offs between forgetting sufficiency and utility preservation. To operationalize this constraint, FROC introduces a smoothly varying continuous risk model that aggregates forgetting deficiency and utility degradation into a single configuration-level score. Building on conformal risk analysis, FROC computes (1) the Conformal Unlearning Risk (CUR), a data-driven estimated value on the probability that forgotten samples continue to influence model predictions, and (2) risk-controlled configuration sets, which identify unlearning hyperparameters that are valid under the specified risk budget. Experiments across multiple LLM MU methods demonstrate that FROC produces stable, interpretable risk landscapes and reveals consistent relationships between unlearning configurations, semantic shift, and utility impact. FROC reframes MU as a controllable, risk-aware process and offers a practical foundation for managing unlearning behavior in large-scale LLM deployments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 114,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13340",
        "abs_url": "https://arxiv.org/abs/2512.13340",
        "pdf_url": "https://arxiv.org/pdf/2512.13340",
        "title": "Link-Aware Energy-Frugal Continual Learning for Fault Detection in IoT Networks",
        "authors": [
            "Henrik C. M. Frederiksen",
            "Junya Shiraishi",
            "Cedomir Stefanovic",
            "Hei Victor Cheng",
            "Shashi Raj Pandey"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)",
        "abstract": "The use of lightweight machine learning (ML) models in internet of things (IoT) networks enables resource constrained IoT devices to perform on-device inference for several critical applications. However, the inference accuracy deteriorates due to the non-stationarity in the IoT environment and limited initial training data. To counteract this, the deployed models can be updated occasionally with new observed data samples. However, this approach consumes additional energy, which is undesirable for energy constrained IoT devices. This letter introduces an event-driven communication framework that strategically integrates continual learning (CL) in IoT networks for energy-efficient fault detection. Our framework enables the IoT device and the edge server (ES) to collaboratively update the lightweight ML model by adapting to the wireless link conditions for communication and the available energy budget. Evaluation on real-world datasets show that the proposed approach can outperform both periodic sampling and non-adaptive CL in terms of inference recall; our proposed approach achieves up to a 42.8% improvement, even under tight energy and bandwidth constraint.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 115,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13352",
        "abs_url": "https://arxiv.org/abs/2512.13352",
        "pdf_url": "https://arxiv.org/pdf/2512.13352",
        "title": "On the Effectiveness of Membership Inference in Targeted Data Extraction from Large Language Models",
        "authors": [
            "Ali Al Sahili",
            "Ali Chehab",
            "Razane Tajeddine"
        ],
        "comments": "Accepted to IEEE Conference on Secure and Trustworthy Machine Learning (SaTML) 2026",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL); Cryptography and Security (cs.CR)",
        "abstract": "Large Language Models (LLMs) are prone to mem- orizing training data, which poses serious privacy risks. Two of the most prominent concerns are training data extraction and Membership Inference Attacks (MIAs). Prior research has shown that these threats are interconnected: adversaries can extract training data from an LLM by querying the model to generate a large volume of text and subsequently applying MIAs to verify whether a particular data point was included in the training set. In this study, we integrate multiple MIA techniques into the data extraction pipeline to systematically benchmark their effectiveness. We then compare their performance in this integrated setting against results from conventional MIA bench- marks, allowing us to evaluate their practical utility in real-world extraction scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 116,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13381",
        "abs_url": "https://arxiv.org/abs/2512.13381",
        "pdf_url": "https://arxiv.org/pdf/2512.13381",
        "title": "Dual-Phase Federated Deep Unlearning via Weight-Aware Rollback and Reconstruction",
        "authors": [
            "Changjun Zhou",
            "Jintao Zheng",
            "Leyou Yang",
            "Pengfei Wang"
        ],
        "comments": "10 pages, submitted to INFOCOM 2026",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Federated Unlearning (FUL) focuses on client data and computing power to offer a privacy-preserving solution. However, high computational demands, complex incentive mechanisms, and disparities in client-side computing power often lead to long times and higher costs. To address these challenges, many existing methods rely on server-side knowledge distillation that solely removes the updates of the target client, overlooking the privacy embedded in the contributions of other clients, which can lead to privacy leakage. In this work, we introduce DPUL, a novel server-side unlearning method that deeply unlearns all influential weights to prevent privacy pitfalls. Our approach comprises three components: (i) identifying high-weight parameters by filtering client update magnitudes, and rolling them back to ensure deep removal. (ii) leveraging the variational autoencoder (VAE) to reconstruct and eliminate low-weight parameters. (iii) utilizing a projection-based technique to recover the model. Experimental results on four datasets demonstrate that DPUL surpasses state-of-the-art baselines, providing a 1%-5% improvement in accuracy and up to 12x reduction in time cost.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 117,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13410",
        "abs_url": "https://arxiv.org/abs/2512.13410",
        "pdf_url": "https://arxiv.org/pdf/2512.13410",
        "title": "Multiclass Graph-Based Large Margin Classifiers: Unified Approach for Support Vectors and Neural Networks",
        "authors": [
            "Vítor M. Hanriot",
            "Luiz C. B. Torres",
            "Antônio P. Braga"
        ],
        "comments": "Accepted to the IEEE Transactions on Neural Networks and Learning Systems (TNNLS)",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "While large margin classifiers are originally an outcome of an optimization framework, support vectors (SVs) can be obtained from geometric approaches. This article presents advances in the use of Gabriel graphs (GGs) in binary and multiclass classification problems. For Chipclass, a hyperparameter-less and optimization-less GG-based binary classifier, we discuss how activation functions and support edge (SE)-centered neurons affect the classification, proposing smoother functions and structural SV (SSV)-centered neurons to achieve margins with low probabilities and smoother classification contours. We extend the neural network architecture, which can be trained with backpropagation with a softmax function and a cross-entropy loss, or by solving a system of linear equations. A new subgraph-/distance-based membership function for graph regularization is also proposed, along with a new GG recomputation algorithm that is less computationally expensive than the standard approach. Experimental results with the Friedman test show that our method was better than previous GG-based classifiers and statistically equivalent to tree-based models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 118,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13442",
        "abs_url": "https://arxiv.org/abs/2512.13442",
        "pdf_url": "https://arxiv.org/pdf/2512.13442",
        "title": "XNNTab -- Interpretable Neural Networks for Tabular Data using Sparse Autoencoders",
        "authors": [
            "Khawla Elhadri",
            "Jörg Schlötterer",
            "Christin Seifert"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In data-driven applications relying on tabular data, where interpretability is key, machine learning models such as decision trees and linear regression are applied. Although neural networks can provide higher predictive performance, they are not used because of their blackbox nature. In this work, we present XNNTab, a neural architecture that combines the expressiveness of neural networks and interpretability. XNNTab first learns highly non-linear feature representations, which are decomposed into monosemantic features using a sparse autoencoder (SAE). These features are then assigned human-interpretable concepts, making the overall model prediction intrinsically interpretable. XNNTab outperforms interpretable predictive models, and achieves comparable performance to its non-interpretable counterparts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 119,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13460",
        "abs_url": "https://arxiv.org/abs/2512.13460",
        "pdf_url": "https://arxiv.org/pdf/2512.13460",
        "title": "DP-EMAR: A Differentially Private Framework for Autonomous Model Weight Repair in Federated IoT Systems",
        "authors": [
            "Chethana Prasad Kabgere",
            "Shylaja S S"
        ],
        "comments": "Accepted and presented at the AI-IoT Workshop, co-located with COMSNETS 2025",
        "subjects": "Machine Learning (cs.LG); Numerical Analysis (math.NA)",
        "abstract": "Federated Learning (FL) enables decentralized model training without sharing raw data, but model weight distortion remains a major challenge in resource constrained IoT networks. In multi tier Federated IoT (Fed-IoT) systems, unstable connectivity and adversarial interference can silently alter transmitted parameters, degrading convergence. We propose DP-EMAR, a differentially private, error model based autonomous repair framework that detects and reconstructs transmission induced distortions during FL aggregation. DP-EMAR estimates corruption patterns and applies adaptive correction before privacy noise is added, enabling reliable in network repair without violating confidentiality. By integrating Differential Privacy (DP) with Secure Aggregation (SA), the framework distinguishes DP noise from genuine transmission errors. Experiments on heterogeneous IoT sensor and graph datasets show that DP-EMAR preserves convergence stability and maintains near baseline performance under communication corruption while ensuring strict (epsilon, delta)-DP guarantees. The framework enhances robustness, communication efficiency, and trust in privacy preserving Federated IoT learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 120,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13480",
        "abs_url": "https://arxiv.org/abs/2512.13480",
        "pdf_url": "https://arxiv.org/pdf/2512.13480",
        "title": "Element-wise Modulation of Random Matrices for Efficient Neural Layers",
        "authors": [
            "Maksymilian Szorc"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Fully connected layers are a primary source of memory and computational overhead in deep neural networks due to their dense, often redundant parameterization. While various compression techniques exist, they frequently introduce complex engineering trade-offs or degrade model performance. We propose the Parametrized Random Projection (PRP) layer, a novel approach that decouples feature mixing from adaptation by utilizing a fixed random matrix modulated by lightweight, learnable element-wise parameters. This architecture drastically reduces the trainable parameter count to a linear scale while retaining reliable accuracy across various benchmarks. The design serves as a stable, computationally efficient solution for architectural scaling and deployment in resource-limited settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 121,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13506",
        "abs_url": "https://arxiv.org/abs/2512.13506",
        "pdf_url": "https://arxiv.org/pdf/2512.13506",
        "title": "Learning under Distributional Drift: Reproducibility as an Intrinsic Statistical Resource",
        "authors": [
            "Sofiya Zaichyk"
        ],
        "comments": "37 pages, 4 figures",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Statistical learning under distributional drift remains insufficiently characterized: when each observation alters the data-generating law, classical generalization bounds can collapse. We introduce a new statistical primitive, the reproducibility budget $C_T$, which quantifies a system's finite capacity for statistical reproducibility - the extent to which its sampling process can remain governed by a consistent underlying distribution in the presence of both exogenous change and endogenous feedback. Formally, $C_T$ is defined as the cumulative Fisher-Rao path length of the coupled learner-environment evolution, measuring the total distributional motion accumulated during learning. From this construct we derive a drift-feedback generalization bound of order $O(T^{-1/2} + C_T/T)$, and we prove a matching minimax lower bound showing that this rate is minimax-optimal. Consequently, the results establish a reproducibility speed limit: no algorithm can achieve smaller worst-case generalization error than that imposed by the average Fisher-Rao drift rate $C_T/T$ of the data-generating process. The framework situates exogenous drift, adaptive data analysis, and performative prediction within a common geometric structure, with $C_T$ emerging as the intrinsic quantity measuring distributional motion across these settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 122,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13526",
        "abs_url": "https://arxiv.org/abs/2512.13526",
        "pdf_url": "https://arxiv.org/pdf/2512.13526",
        "title": "Async Control: Stress-testing Asynchronous Control Measures for LLM Agents",
        "authors": [
            "Asa Cooper Stickland",
            "Jan Michelfeit",
            "Arathi Mani",
            "Charlie Griffin",
            "Ollie Matthews",
            "Tomek Korbak",
            "Rogan Inglis",
            "Oliver Makins",
            "Alan Cooney"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "LLM-based software engineering agents are increasingly used in real-world development tasks, often with access to sensitive data or security-critical codebases. Such agents could intentionally sabotage these codebases if they were misaligned. We investigate asynchronous monitoring, in which a monitoring system reviews agent actions after the fact. Unlike synchronous monitoring, this approach does not impose runtime latency, while still attempting to disrupt attacks before irreversible harm occurs. We treat monitor development as an adversarial game between a blue team (who design monitors) and a red team (who create sabotaging agents). We attempt to set the game rules such that they upper bound the sabotage potential of an agent based on Claude 4.1 Opus. To ground this game in a realistic, high-stakes deployment scenario, we develop a suite of 5 diverse software engineering environments that simulate tasks that an agent might perform within an AI developer's internal infrastructure. Over the course of the game, we develop an ensemble monitor that achieves a 6% false negative rate at 1% false positive rate on a held out test environment. Then, we estimate risk of sabotage at deployment time by extrapolating from our monitor's false negative rate. We describe one simple model for this extrapolation, present a sensitivity analysis, and describe situations in which the model would be invalid. Code is available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 123,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13583",
        "abs_url": "https://arxiv.org/abs/2512.13583",
        "pdf_url": "https://arxiv.org/pdf/2512.13583",
        "title": "DP-CSGP: Differentially Private Stochastic Gradient Push with Compressed Communication",
        "authors": [
            "Zehan Zhu",
            "Heng Zhao",
            "Yan Huang",
            "Joey Tianyi Zhou",
            "Shouling Ji",
            "Jinming Xu"
        ],
        "comments": "13 pages",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "In this paper, we propose a Differentially Private Stochastic Gradient Push with Compressed communication (termed DP-CSGP) for decentralized learning over directed graphs. Different from existing works, the proposed algorithm is designed to maintain high model utility while ensuring both rigorous differential privacy (DP) guarantees and efficient communication. For general non-convex and smooth objective functions, we show that the proposed algorithm achieves a tight utility bound of $\\mathcal{O}\\left( \\sqrt{d\\log \\left( \\frac{1}{\\delta} \\right)}/(\\sqrt{n}J\\epsilon) \\right)$ ($J$ and $d$ are the number of local samples and the dimension of decision variables, respectively) with $\\left(\\epsilon, \\delta\\right)$-DP guarantee for each node, matching that of decentralized counterparts with exact communication. Extensive experiments on benchmark tasks show that, under the same privacy budget, DP-CSGP achieves comparable model accuracy with significantly lower communication cost than existing decentralized counterparts with exact communication.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 124,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13593",
        "abs_url": "https://arxiv.org/abs/2512.13593",
        "pdf_url": "https://arxiv.org/pdf/2512.13593",
        "title": "Scalable Formal Verification via Autoencoder Latent Space Abstraction",
        "authors": [
            "Robert Reed",
            "Morteza Lahijanian",
            "Luca Laurenti"
        ],
        "comments": "14 pages, 7 figures, under review",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Finite Abstraction methods provide a powerful formal framework for proving that systems satisfy their specifications. However, these techniques face scalability challenges for high-dimensional systems, as they rely on state-space discretization which grows exponentially with dimension. Learning-based approaches to dimensionality reduction, utilizing neural networks and autoencoders, have shown great potential to alleviate this problem. However, ensuring the correctness of the resulting verification results remains an open question. In this work, we provide a formal approach to reduce the dimensionality of systems via convex autoencoders and learn the dynamics in the latent space through a kernel-based method. We then construct a finite abstraction from the learned model in the latent space and guarantee that the abstraction contains the true behaviors of the original system. We show that the verification results in the latent space can be mapped back to the original system. Finally, we demonstrate the effectiveness of our approach on multiple systems, including a 26D system controlled by a neural network, showing significant scalability improvements without loss of rigor.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 125,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13617",
        "abs_url": "https://arxiv.org/abs/2512.13617",
        "pdf_url": "https://arxiv.org/pdf/2512.13617",
        "title": "LightTopoGAT: Enhancing Graph Attention Networks with Topological Features for Efficient Graph Classification",
        "authors": [
            "Ankit Sharma",
            "Sayan Roy Gupta"
        ],
        "comments": "9 pages",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Graph Neural Networks have demonstrated significant success in graph classification tasks, yet they often require substantial computational resources and struggle to capture global graph properties effectively. We introduce LightTopoGAT, a lightweight graph attention network that enhances node features through topological augmentation by incorporating node degree and local clustering coefficient to improve graph representation learning. The proposed approach maintains parameter efficiency through streamlined attention mechanisms while integrating structural information that is typically overlooked by local message passing schemes. Through comprehensive experiments on three benchmark datasets, MUTAG, ENZYMES, and PROTEINS, we show that LightTopoGAT achieves superior performance compared to established baselines including GCN, GraphSAGE, and standard GAT, with a 6.6 percent improvement in accuracy on MUTAG and a 2.2 percent improvement on PROTEINS. Ablation studies further confirm that these performance gains arise directly from the inclusion of topological features, demonstrating a simple yet effective strategy for enhancing graph neural network performance without increasing architectural complexity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 126,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13632",
        "abs_url": "https://arxiv.org/abs/2512.13632",
        "pdf_url": "https://arxiv.org/pdf/2512.13632",
        "title": "StutterFuse: Mitigating Modality Collapse in Stuttering Detection with Jaccard-Weighted Metric Learning and Gated Fusion",
        "authors": [
            "Guransh Singh",
            "Md Shah Fahad"
        ],
        "comments": "13 pages, 10 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Stuttering detection breaks down when disfluencies overlap. Existing parametric models struggle to distinguish complex, simultaneous disfluencies (e.g., a 'block' with a 'prolongation') due to the scarcity of these specific combinations in training data. While Retrieval-Augmented Generation (RAG) has revolutionized NLP by grounding models in external knowledge, this paradigm remains unexplored in pathological speech processing. To bridge this gap, we introduce StutterFuse, the first Retrieval-Augmented Classifier (RAC) for multi-label stuttering detection. By conditioning a Conformer encoder on a non-parametric memory bank of clinical examples, we allow the model to classify by reference rather than memorization. We further identify and solve \"Modality Collapse\", an \"Echo Chamber\" effect where naive retrieval boosts recall but degrades precision. We mitigate this using: (1) SetCon, a Jaccard-Weighted Metric Learning objective that optimizes for multi-label set similarity, and (2) a Gated Mixture-of-Experts fusion strategy that dynamically arbitrates between acoustic evidence and retrieved context. On the SEP-28k dataset, StutterFuse achieves a weighted F1-score of 0.65, outperforming strong baselines and demonstrating remarkable zero-shot cross-lingual generalization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 127,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13668",
        "abs_url": "https://arxiv.org/abs/2512.13668",
        "pdf_url": "https://arxiv.org/pdf/2512.13668",
        "title": "A Scientific Reasoning Model for Organic Synthesis Procedure Generation",
        "authors": [
            "Guoqing Liu",
            "Junren Li",
            "Zihan Zhao",
            "Eray Inanc",
            "Krzysztof Maziarz",
            "Jose Garrido Torres",
            "Victor Garcia Satorras",
            "Shoko Ueda",
            "Christopher M. Bishop",
            "Marwin Segler"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Solving computer-aided synthesis planning is essential for enabling fully automated, robot-assisted synthesis workflows and improving the efficiency of drug discovery. A key challenge, however, is bridging the gap between computational route design and practical laboratory execution, particularly the accurate prediction of viable experimental procedures for each synthesis step. In this work, we present QFANG, a scientific reasoning language model capable of generating precise, structured experimental procedures directly from reaction equations, with explicit chain-of-thought reasoning. To develop QFANG, we curated a high-quality dataset comprising 905,990 chemical reactions paired with structured action sequences, extracted and processed from patent literature using large language models. We introduce a Chemistry-Guided Reasoning (CGR) framework that produces chain-of-thought data grounded in chemical knowledge at scale. The model subsequently undergoes supervised fine-tuning to elicit complex chemistry reasoning. Finally, we apply Reinforcement Learning from Verifiable Rewards (RLVR) to further enhance procedural accuracy. Experimental results demonstrate that QFANG outperforms advanced general-purpose reasoning models and nearest-neighbor retrieval baselines, measured by traditional NLP similarity metrics and a chemically aware evaluator using an LLM-as-a-judge. Moreover, QFANG generalizes to certain out-of-domain reaction classes and adapts to variations in laboratory conditions and user-specific constraints. We believe that QFANG's ability to generate high-quality synthesis procedures represents an important step toward bridging the gap between computational synthesis planning and fully automated laboratory synthesis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 128,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2310.00177",
        "abs_url": "https://arxiv.org/abs/2310.00177",
        "pdf_url": "https://arxiv.org/pdf/2310.00177",
        "title": "A Neural-preconditioned Poisson Solver for Mixed Dirichlet and Neumann Boundary Conditions",
        "authors": [
            "Kai Weixian Lan",
            "Elias Gueidon",
            "Ayano Kaneda",
            "Julian Panetta",
            "Joseph Teran"
        ],
        "comments": "",
        "subjects": "Numerical Analysis (math.NA); Graphics (cs.GR); Machine Learning (cs.LG)",
        "abstract": "We introduce a neural-preconditioned iterative solver for Poisson equations with mixed boundary conditions. Typical Poisson discretizations yield large, ill-conditioned linear systems. Iterative solvers can be effective for these problems, but only when equipped with powerful preconditioners. Unfortunately, effective preconditioners like multigrid require costly setup phases that must be re-executed every time domain shapes or boundary conditions change, forming a severe bottleneck for problems with evolving boundaries. In contrast, we present a neural preconditioner trained to efficiently approximate the inverse of the discrete Laplacian in the presence of such changes. Our approach generalizes to domain shapes, boundary conditions, and grid sizes outside the training set. The key to our preconditioner's success is a novel, lightweight neural network architecture featuring spatially varying convolution kernels and supporting fast inference. We demonstrate that our solver outperforms state-of-the-art methods like algebraic multigrid as well as recently proposed neural preconditioners on challenging test cases arising from incompressible fluid simulations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 129,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2511.01411",
        "abs_url": "https://arxiv.org/abs/2511.01411",
        "pdf_url": "https://arxiv.org/pdf/2511.01411",
        "title": "Extremal Contours: Gradient-driven contours for compact visual attribution",
        "authors": [
            "Reza Karimzadeh",
            "Albert Alonso",
            "Frans Zdyb",
            "Julius B. Kirkegaard",
            "Bulat Ibragimov"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Image and Video Processing (eess.IV)",
        "abstract": "Faithful yet compact explanations for vision models remain a challenge, as commonly used dense perturbation masks are often fragmented and overfitted, needing careful post-processing. Here, we present a training-free explanation method that replaces dense masks with smooth tunable contours. A star-convex region is parameterized by a truncated Fourier series and optimized under an extremal preserve/delete objective using the classifier gradients. The approach guarantees a single, simply connected mask, cuts the number of free parameters by orders of magnitude, and yields stable boundary updates without cleanup. Restricting solutions to low-dimensional, smooth contours makes the method robust to adversarial masking artifacts. On ImageNet classifiers, it matches the extremal fidelity of dense masks while producing compact, interpretable regions with improved run-to-run consistency. Explicit area control also enables importance contour maps, yielding a transparent fidelity-area profiles. Finally, we extend the approach to multi-contour and show how it can localize multiple objects within the same framework. Across benchmarks, the method achieves higher relevance mass and lower complexity than gradient and perturbation based baselines, with especially strong gains on self-supervised DINO models where it improves relevance mass by over 15% and maintains positive faithfulness correlations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 130,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.11816",
        "abs_url": "https://arxiv.org/abs/2512.11816",
        "pdf_url": "https://arxiv.org/pdf/2512.11816",
        "title": "Reinforcement Learning for Latent-Space Thinking in LLMs",
        "authors": [
            "Enes Özeren",
            "Matthias Aßenmacher"
        ],
        "comments": "16 pages, 16 figures, 7 tables",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Chain-of-Thought (CoT) reasoning typically utilizes the discrete language space for thinking, which is inherently inefficient, as many generated tokens only enforce linguistic rules that are not required for reasoning. To bypass this, latent-space thinking allows models to think using the continuous embedding space. While existing methods for training those models show domain-specific gains, they fail to maintain performance in complex tasks, such as mathematical reasoning. We experimentally demonstrate that the Coconut approach, a form of supervised fine-tuning for latent-space thinking, is highly sensitive to design choices and exhibits several inherent limitations. To address these issues, we investigate reinforcement learning (RL) techniques -- an underexplored direction in latent-space thinking -- including GRPO and design a novel Latent RL method for directly optimizing the latent thinking steps. Our experimental results reveal that these RL-trained models still lag behind traditional language-space CoT models in the mathematical reasoning domain. We make our codebase publicly available.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 131,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.11837",
        "abs_url": "https://arxiv.org/abs/2512.11837",
        "pdf_url": "https://arxiv.org/pdf/2512.11837",
        "title": "Vision Foundry: A System for Training Foundational Vision AI Models",
        "authors": [
            "Mahmut S. Gokmen",
            "Mitchell A. Klusty",
            "Evan W. Damron",
            "W. Vaiden Logan",
            "Aaron D. Mullen",
            "Caroline N. Leach",
            "Emily B. Collier",
            "Samuel E. Armstrong",
            "V.K. Cody Bumgardner"
        ],
        "comments": "10 pages, 4 figures, 3 tables, submitted to AMIA 2026 Informatics Summit",
        "subjects": "Quantitative Methods (q-bio.QM); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Self-supervised learning (SSL) leverages vast unannotated medical datasets, yet steep technical barriers limit adoption by clinical researchers. We introduce Vision Foundry, a code-free, HIPAA-compliant platform that democratizes pre-training, adaptation, and deployment of foundational vision models. The system integrates the DINO-MX framework, abstracting distributed infrastructure complexities while implementing specialized strategies like Magnification-Aware Distillation (MAD) and Parameter-Efficient Fine-Tuning (PEFT). We validate the platform across domains, including neuropathology segmentation, lung cellularity estimation, and coronary calcium scoring. Our experiments demonstrate that models trained via Vision Foundry significantly outperform generic baselines in segmentation fidelity and regression accuracy, while exhibiting robust zero-shot generalization across imaging protocols. By bridging the gap between advanced representation learning and practical application, Vision Foundry enables domain experts to develop state-of-the-art clinical AI tools with minimal annotation overhead, shifting focus from engineering optimization to clinical discovery.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 132,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.11843",
        "abs_url": "https://arxiv.org/abs/2512.11843",
        "pdf_url": "https://arxiv.org/pdf/2512.11843",
        "title": "Spiking Manifesto",
        "authors": [
            "Eugene Izhikevich"
        ],
        "comments": "This is a declaration of principles and roadmap for spiking networks, intended as a manifesto rather than a conventional research article",
        "subjects": "Neural and Evolutionary Computing (cs.NE); Artificial Intelligence (cs.AI); Hardware Architecture (cs.AR); Machine Learning (cs.LG)",
        "abstract": "Practically everything computers do is better, faster, and more power-efficient than the brain. For example, a calculator crunches numbers more energy-efficiently than any human. Yet AI models are a thousand times less efficient than the brain. These models use artificial neural networks (ANNs) and require GPUs for the multiplication of huge matrices. In contrast, spiking neural networks (SNNs) of the brain have no matrix multiplication and much smaller energy requirements. This manifesto proposes a framework for thinking about popular AI models in terms of spiking networks and polychronization, and for interpreting spiking activity as nature's way of implementing look-up tables. This offers a way to convert AI models into a novel type of architecture with the promise of a thousandfold improvement in efficiency. Code is available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 133,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.11844",
        "abs_url": "https://arxiv.org/abs/2512.11844",
        "pdf_url": "https://arxiv.org/pdf/2512.11844",
        "title": "Love First, Know Later: Persona-Based Romantic Compatibility Through LLM Text World Engines",
        "authors": [
            "Haoyang Shang",
            "Zhengyang Yan",
            "Xuan Liu"
        ],
        "comments": "NeurIPS 2025 Workshop: First Workshop on LLM Persona Modeling (Oral)",
        "subjects": "Human-Computer Interaction (cs.HC); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "We propose Love First, Know Later: a paradigm shift in computational matching that simulates interactions first, then assesses compatibility. Instead of comparing static profiles, our framework leverages LLMs as text world engines that operate in dual capacity-as persona-driven agents following behavioral policies and as the environment modeling interaction dynamics. We formalize compatibility assessment as a reward-modeling problem: given observed matching outcomes, we learn to extract signals from simulations that predict human preferences. Our key insight is that relationships hinge on responses to critical moments-we translate this observation from relationship psychology into mathematical hypotheses, enabling effective simulation. Theoretically, we prove that as LLM policies better approximate human behavior, the induced matching converges to optimal stable matching. Empirically, we validate on speed dating data for initial chemistry and divorce prediction for long-term stability. This paradigm enables interactive, personalized matching systems where users iteratively refine their agents, unlocking future possibilities for transparent and interactive compatibility assessment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 134,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.11853",
        "abs_url": "https://arxiv.org/abs/2512.11853",
        "pdf_url": "https://arxiv.org/pdf/2512.11853",
        "title": "Evolving Deep Learning Optimizers",
        "authors": [
            "Mitchell Marfinetz"
        ],
        "comments": "",
        "subjects": "Neural and Evolutionary Computing (cs.NE); Machine Learning (cs.LG)",
        "abstract": "We present a genetic algorithm framework for automatically discovering deep learning optimization algorithms. Our approach encodes optimizers as genomes that specify combinations of primitive update terms (gradient, momentum, RMS normalization, Adam-style adaptive terms, and sign-based updates) along with hyperparameters and scheduling options. Through evolutionary search over 50 generations with a population of 50 individuals, evaluated across multiple vision tasks, we discover an evolved optimizer that outperforms Adam by 2.6% in aggregate fitness and achieves a 7.7% relative improvement on CIFAR-10. The evolved optimizer combines sign-based gradient terms with adaptive moment estimation, uses lower momentum coefficients than Adam ($\\beta_1$=0.86, $\\beta_2$=0.94), and notably disables bias correction while enabling learning rate warmup and cosine decay. Our results demonstrate that evolutionary search can discover competitive optimization algorithms and reveal design principles that differ from hand-crafted optimizers. Code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 135,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.11875",
        "abs_url": "https://arxiv.org/abs/2512.11875",
        "pdf_url": "https://arxiv.org/pdf/2512.11875",
        "title": "The Art of Storytelling in Authoritarian Regimes: Crafting State Narratives on Chinese Social Media",
        "authors": [
            "Ting Luo",
            "Yan Wang"
        ],
        "comments": "",
        "subjects": "Computers and Society (cs.CY); Machine Learning (cs.LG); Applications (stat.AP)",
        "abstract": "This article examines how authoritarian regimes construct state narratives about politically consequential events. Building on the narrative policy framework and existing research on authoritarian propaganda, we propose two dimensions that shape narrative construction: legitimacy implications -- whether events enhance or threaten regime legitimacy, and citizen verification capacity -- the extent to which citizens can evaluate official narratives through alternative sources. Using quantitative narrative analysis of Chinese social media posts by government, state media, and celebrity accounts, we extract subject-verb-object (SVO) triplets to map dominant narrative structures across four major events. Our findings show that legitimacy implications of the event shape regime's efforts in storytelling and the beliefs highlighted in the narratives, while citizen's verification capacity could balance the strategic choice between a top-down manipulation and bottom-up responsiveness of state narratives. Together, the results reveal propaganda as a complex process of narrative construction adaptive to specific contexts, offering new insights into how dynamic storytelling sustains authoritarian resilience.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 136,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.11894",
        "abs_url": "https://arxiv.org/abs/2512.11894",
        "pdf_url": "https://arxiv.org/pdf/2512.11894",
        "title": "mmWEAVER: Environment-Specific mmWave Signal Synthesis from a Photo and Activity Description",
        "authors": [
            "Mahathir Monjur",
            "Shahriar Nirjon"
        ],
        "comments": "Accepted at the IEEE/CVF Winter Conference on Applications of Computer Vision 2026 (WACV 2026)",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Realistic signal generation and dataset augmentation are essential for advancing mmWave radar applications such as activity recognition and pose estimation, which rely heavily on diverse, and environment-specific signal datasets. However, mmWave signals are inherently complex, sparse, and high-dimensional, making physical simulation computationally expensive. This paper presents mmWeaver, a novel framework that synthesizes realistic, environment-specific complex mmWave signals by modeling them as continuous functions using Implicit Neural Representations (INRs), achieving up to 49-fold compression. mmWeaver incorporates hypernetworks that dynamically generate INR parameters based on environmental context (extracted from RGB-D images) and human motion features (derived from text-to-pose generation via MotionGPT), enabling efficient and adaptive signal synthesis. By conditioning on these semantic and geometric priors, mmWeaver generates diverse I/Q signals at multiple resolutions, preserving phase information critical for downstream tasks such as point cloud estimation and activity classification. Extensive experiments show that mmWeaver achieves a complex SSIM of 0.88 and a PSNR of 35 dB, outperforming existing methods in signal realism while improving activity recognition accuracy by up to 7% and reducing human pose estimation error by up to 15%, all while operating 6-35 times faster than simulation-based approaches.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 137,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.11902",
        "abs_url": "https://arxiv.org/abs/2512.11902",
        "pdf_url": "https://arxiv.org/pdf/2512.11902",
        "title": "Mirror Mode in Fire Emblem: Beating Players at their own Game with Imitation and Reinforcement Learning",
        "authors": [
            "Yanna Elizabeth Smid",
            "Peter van der Putten",
            "Aske Plaat"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG)",
        "abstract": "Enemy strategies in turn-based games should be surprising and unpredictable. This study introduces Mirror Mode, a new game mode where the enemy AI mimics the personal strategy of a player to challenge them to keep changing their gameplay. A simplified version of the Nintendo strategy video game Fire Emblem Heroes has been built in Unity, with a Standard Mode and a Mirror Mode. Our first set of experiments find a suitable model for the task to imitate player demonstrations, using Reinforcement Learning and Imitation Learning: combining Generative Adversarial Imitation Learning, Behavioral Cloning, and Proximal Policy Optimization. The second set of experiments evaluates the constructed model with player tests, where models are trained on demonstrations provided by participants. The gameplay of the participants indicates good imitation in defensive behavior, but not in offensive strategies. Participant's surveys indicated that they recognized their own retreating tactics, and resulted in an overall higher player-satisfaction for Mirror Mode. Refining the model further may improve imitation quality and increase player's satisfaction, especially when players face their own strategies. The full code and survey results are stored at: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 138,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.11945",
        "abs_url": "https://arxiv.org/abs/2512.11945",
        "pdf_url": "https://arxiv.org/pdf/2512.11945",
        "title": "Interval Fisher's Discriminant Analysis and Visualisation",
        "authors": [
            "Diogo Pinheiro",
            "M. Rosário Oliveira",
            "Igor Kravchenko",
            "Lina Oliveira"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Statistics Theory (math.ST)",
        "abstract": "In Data Science, entities are typically represented by single valued measurements. Symbolic Data Analysis extends this framework to more complex structures, such as intervals and histograms, that express internal variability. We propose an extension of multiclass Fisher's Discriminant Analysis to interval-valued data, using Moore's interval arithmetic and the Mallows' distance. Fisher's objective function is generalised to consider simultaneously the contributions of the centres and the ranges of intervals and is numerically maximised. The resulting discriminant directions are then used to classify interval-valued this http URL support visual assessment, we adapt the class map, originally introduced for conventional data, to classifiers that assign labels through minimum distance rules. We also extend the silhouette plot to this setting and use stacked mosaic plots to complement the visual display of class assignments. Together, these graphical tools provide insight into classifier performance and the strength of class membership. Applications to real datasets illustrate the proposed methodology and demonstrate its value in interpreting classification results for interval-valued data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 139,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.11990",
        "abs_url": "https://arxiv.org/abs/2512.11990",
        "pdf_url": "https://arxiv.org/pdf/2512.11990",
        "title": "Policy Gradient Algorithms for Age-of-Information Cost Minimization",
        "authors": [
            "José-Ramón Vidal",
            "Vicent Pla",
            "Luis Guijarro",
            "Israel Leyva-Mayorga"
        ],
        "comments": "",
        "subjects": "Networking and Internet Architecture (cs.NI); Information Theory (cs.IT); Machine Learning (cs.LG)",
        "abstract": "Recent developments in cyber-physical systems have increased the importance of maximizing the freshness of the information about the physical environment. However, optimizing the access policies of Internet of Things devices to maximize the data freshness, measured as a function of the Age-of-Information (AoI) metric, is a challenging task. This work introduces two algorithms to optimize the information update process in cyber-physical systems operating under the generate-at-will model, by finding an online policy without knowing the characteristics of the transmission delay or the age cost function. The optimization seeks to minimize the time-average cost, which integrates the AoI at the receiver and the data transmission cost, making the approach suitable for a broad range of scenarios. Both algorithms employ policy gradient methods within the framework of model-free reinforcement learning (RL) and are specifically designed to handle continuous state and action spaces. Each algorithm minimizes the cost using a distinct strategy for deciding when to send an information update. Moreover, we demonstrate that it is feasible to apply the two strategies simultaneously, leading to an additional reduction in cost. The results demonstrate that the proposed algorithms exhibit good convergence properties and achieve a time-average cost within 3% of the optimal value, when the latter is computable. A comparison with other state-of-the-art methods shows that the proposed algorithms outperform them in one or more of the following aspects: being applicable to a broader range of scenarios, achieving a lower time-average cost, and requiring a computational cost at least one order of magnitude lower.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 140,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12002",
        "abs_url": "https://arxiv.org/abs/2512.12002",
        "pdf_url": "https://arxiv.org/pdf/2512.12002",
        "title": "Adversarial Attacks Against Deep Learning-Based Radio Frequency Fingerprint Identification",
        "authors": [
            "Jie Ma",
            "Junqing Zhang",
            "Guanxiong Shen",
            "Alan Marshall",
            "Chip-Hong Chang"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
        "abstract": "Radio frequency fingerprint identification (RFFI) is an emerging technique for the lightweight authentication of wireless Internet of things (IoT) devices. RFFI exploits deep learning models to extract hardware impairments to uniquely identify wireless devices. Recent studies show deep learning-based RFFI is vulnerable to adversarial attacks. However, effective adversarial attacks against different types of RFFI classifiers have not yet been explored. In this paper, we carried out a comprehensive investigations into different adversarial attack methods on RFFI systems using various deep learning models. Three specific algorithms, fast gradient sign method (FGSM), projected gradient descent (PGD), and universal adversarial perturbation (UAP), were analyzed. The attacks were launched to LoRa-RFFI and the experimental results showed the generated perturbations were effective against convolutional neural networks (CNNs), long short-term memory (LSTM) networks, and gated recurrent units (GRU). We further used UAP to launch practical attacks. Special factors were considered for the wireless context, including implementing real-time attacks, the effectiveness of the attacks over a period of time, etc. Our experimental evaluation demonstrated that UAP can successfully launch adversarial attacks against the RFFI, achieving a success rate of 81.7% when the adversary almost has no prior knowledge of the victim RFFI systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 141,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12060",
        "abs_url": "https://arxiv.org/abs/2512.12060",
        "pdf_url": "https://arxiv.org/pdf/2512.12060",
        "title": "CreativeVR: Diffusion-Prior-Guided Approach for Structure and Motion Restoration in Generative and Real Videos",
        "authors": [
            "Tejas Panambur",
            "Ishan Rajendrakumar Dave",
            "Chongjian Ge",
            "Ersin Yumer",
            "Xue Bai"
        ],
        "comments": "The first two authors contributed equally",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Multimedia (cs.MM)",
        "abstract": "Modern text-to-video (T2V) diffusion models can synthesize visually compelling clips, yet they remain brittle at fine-scale structure: even state-of-the-art generators often produce distorted faces and hands, warped backgrounds, and temporally inconsistent motion. Such severe structural artifacts also appear in very low-quality real-world videos. Classical video restoration and super-resolution (VR/VSR) methods, in contrast, are tuned for synthetic degradations such as blur and downsampling and tend to stabilize these artifacts rather than repair them, while diffusion-prior restorers are usually trained on photometric noise and offer little control over the trade-off between perceptual quality and fidelity. We introduce CreativeVR, a diffusion-prior-guided video restoration framework for AI-generated (AIGC) and real videos with severe structural and temporal artifacts. Our deep-adapter-based method exposes a single precision knob that controls how strongly the model follows the input, smoothly trading off between precise restoration on standard degradations and stronger structure- and motion-corrective behavior on challenging content. Our key novelty is a temporally coherent degradation module used during training, which applies carefully designed transformations that produce realistic structural failures. To evaluate AIGC-artifact restoration, we propose the AIGC54 benchmark with FIQA, semantic and perceptual metrics, and multi-aspect scoring. CreativeVR achieves state-of-the-art results on videos with severe artifacts and performs competitively on standard video restoration benchmarks, while running at practical throughput (about 13 FPS at 720p on a single 80-GB A100). Project page: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 142,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12072",
        "abs_url": "https://arxiv.org/abs/2512.12072",
        "pdf_url": "https://arxiv.org/pdf/2512.12072",
        "title": "VOYAGER: A Training Free Approach for Generating Diverse Datasets using LLMs",
        "authors": [
            "Avinash Amballa",
            "Yashas Malur Saidutta",
            "Chi-Heng Lin",
            "Vivek Kulkarni",
            "Srinivas Chappidi"
        ],
        "comments": "Arxiv Submission",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Large language models (LLMs) are increasingly being used to generate synthetic datasets for the evaluation and training of downstream models. However, prior work has noted that such generated data lacks diversity. In this paper, we propose Voyager, a novel principled approach to generate diverse datasets. Our approach is iterative and directly optimizes a mathematical quantity that optimizes the diversity of the dataset using the machinery of determinantal point processes. Furthermore, our approach is training-free, applicable to closed-source models, and scalable. In addition to providing theoretical justification for the working of our method, we also demonstrate through comprehensive experiments that Voyager significantly outperforms popular baseline approaches by providing a 1.5-3x improvement in diversity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 143,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12088",
        "abs_url": "https://arxiv.org/abs/2512.12088",
        "pdf_url": "https://arxiv.org/pdf/2512.12088",
        "title": "Reliable Policy Iteration: Performance Robustness Across Architecture and Environment Perturbations",
        "authors": [
            "S.R. Eshwar",
            "Aniruddha Mukherjee",
            "Kintan Saha",
            "Krishna Agarwal",
            "Gugan Thoppe",
            "Aditya Gopalan",
            "Gal Dalal"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "In a recent work, we proposed Reliable Policy Iteration (RPI), that restores policy iteration's monotonicity-of-value-estimates property to the function approximation setting. Here, we assess the robustness of RPI's empirical performance on two classical control tasks -- CartPole and Inverted Pendulum -- under changes to neural network and environmental parameters. Relative to DQN, Double DQN, DDPG, TD3, and PPO, RPI reaches near-optimal performance early and sustains this policy as training proceeds. Because deep RL methods are often hampered by sample inefficiency, training instability, and hyperparameter sensitivity, our results highlight RPI's promise as a more reliable alternative.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 144,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12101",
        "abs_url": "https://arxiv.org/abs/2512.12101",
        "pdf_url": "https://arxiv.org/pdf/2512.12101",
        "title": "AI-Augmented Pollen Recognition in Optical and Holographic Microscopy for Veterinary Imaging",
        "authors": [
            "Swarn S. Warshaneyan",
            "Maksims Ivanovs",
            "Blaž Cugmas",
            "Inese Bērziņa",
            "Laura Goldberga",
            "Mindaugas Tamosiunas",
            "Roberts Kadiķis"
        ],
        "comments": "10 pages, 10 figures, 2 tables, 22 references. Journal submission undergoing peer review",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "We present a comprehensive study on fully automated pollen recognition across both conventional optical and digital in-line holographic microscopy (DIHM) images of sample slides. Visually recognizing pollen in unreconstructed holographic images remains challenging due to speckle noise, twin-image artifacts and substantial divergence from bright-field appearances. We establish the performance baseline by training YOLOv8s for object detection and MobileNetV3L for classification on a dual-modality dataset of automatically annotated optical and affinely aligned DIHM images. On optical data, detection mAP50 reaches 91.3% and classification accuracy reaches 97%, whereas on DIHM data, we achieve only 8.15% for detection mAP50 and 50% for classification accuracy. Expanding the bounding boxes of pollens in DIHM images over those acquired in aligned optical images achieves 13.3% for detection mAP50 and 54% for classification accuracy. To improve object detection in DIHM images, we employ a Wasserstein GAN with spectral normalization (WGAN-SN) to create synthetic DIHM images, yielding an FID score of 58.246. Mixing real-world and synthetic data at the 1.0 : 1.5 ratio for DIHM images improves object detection up to 15.4%. These results demonstrate that GAN-based augmentation can reduce the performance divide, bringing fully automated DIHM workflows for veterinary imaging a small but important step closer to practice.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 145,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12117",
        "abs_url": "https://arxiv.org/abs/2512.12117",
        "pdf_url": "https://arxiv.org/pdf/2512.12117",
        "title": "Citation-Grounded Code Comprehension: Preventing LLM Hallucination Through Hybrid Retrieval and Graph-Augmented Context",
        "authors": [
            "Jahidul Arafat"
        ],
        "comments": "",
        "subjects": "Software Engineering (cs.SE); Machine Learning (cs.LG)",
        "abstract": "Large language models have become essential tools for code comprehension, enabling developers to query unfamiliar codebases through natural language interfaces. However, LLM hallucination, generating plausible but factually incorrect citations to source code, remains a critical barrier to reliable developer assistance. This paper addresses the challenges of achieving verifiable, citation grounded code comprehension through hybrid retrieval and lightweight structural reasoning. Our work is grounded in systematic evaluation across 30 Python repositories with 180 developer queries, comparing retrieval modalities, graph expansion strategies, and citation verification mechanisms. We find that challenges of citation accuracy arise from the interplay between sparse lexical matching, dense semantic similarity, and cross file architectural dependencies. Among these, cross file evidence discovery is the largest contributor to citation completeness, but it is largely overlooked because existing systems rely on pure textual similarity without leveraging code structure. We advocate for citation grounded generation as an architectural principle for code comprehension systems and demonstrate this need by achieving 92 percent citation accuracy with zero hallucinations. Specifically, we develop a hybrid retrieval system combining BM25 sparse matching, BGE dense embeddings, and Neo4j graph expansion via import relationships, which outperforms single mode baselines by 14 to 18 percentage points while discovering cross file evidence missed by pure text similarity in 62 percent of architectural queries.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 146,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12134",
        "abs_url": "https://arxiv.org/abs/2512.12134",
        "pdf_url": "https://arxiv.org/pdf/2512.12134",
        "title": "Modeling Dabrafenib Response Using Multi-Omics Modality Fusion and Protein Network Embeddings Based on Graph Convolutional Networks",
        "authors": [
            "La Ode Aman",
            "A Mu'thi Andy Suryadi",
            "Dizky Ramadani Putri Papeo",
            "Hamsidar Hasan",
            "Ariani H Hutuba",
            "Netty Ino Ischak",
            "Yuszda K. Salimi"
        ],
        "comments": "",
        "subjects": "Biomolecules (q-bio.BM); Machine Learning (cs.LG)",
        "abstract": "Cancer cell response to targeted therapy arises from complex molecular interactions, making single omics insufficient for accurate prediction. This study develops a model to predict Dabrafenib sensitivity by integrating multiple omics layers (genomics, transcriptomics, proteomics, epigenomics, and metabolomics) with protein network embeddings generated using Graph Convolutional Networks (GCN). Each modality is encoded into low dimensional representations through neural network preprocessing. Protein interaction information from STRING is incorporated using GCN to capture biological topology. An attention based fusion mechanism assigns adaptive weights to each modality according to its relevance. Using GDSC cancer cell line data, the model shows that selective integration of two modalities, especially proteomics and transcriptomics, achieves the best test performance (R2 around 0.96), outperforming all single omics and full multimodal settings. Genomic and epigenomic data were less informative, while proteomic and transcriptomic layers provided stronger phenotypic signals related to MAPK inhibitor activity. These results show that attention guided multi omics fusion combined with GCN improves drug response prediction and reveals complementary molecular determinants of Dabrafenib sensitivity. The approach offers a promising computational framework for precision oncology and predictive modeling of targeted therapies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 147,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12154",
        "abs_url": "https://arxiv.org/abs/2512.12154",
        "pdf_url": "https://arxiv.org/pdf/2512.12154",
        "title": "Keep the Lights On, Keep the Lengths in Check: Plug-In Adversarial Detection for Time-Series LLMs in Energy Forecasting",
        "authors": [
            "Hua Ma",
            "Ruoxi Sun",
            "Minhui Xue",
            "Xingliang Yuan",
            "Carsten Rudolph",
            "Surya Nepal",
            "Ling Liu"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
        "abstract": "Accurate time-series forecasting is increasingly critical for planning and operations in low-carbon power systems. Emerging time-series large language models (TS-LLMs) now deliver this capability at scale, requiring no task-specific retraining, and are quickly becoming essential components within the Internet-of-Energy (IoE) ecosystem. However, their real-world deployment is complicated by a critical vulnerability: adversarial examples (AEs). Detecting these AEs is challenging because (i) adversarial perturbations are optimized across the entire input sequence and exploit global temporal dependencies, which renders local detection methods ineffective, and (ii) unlike traditional forecasting models with fixed input dimensions, TS-LLMs accept sequences of variable length, increasing variability that complicates detection. To address these challenges, we propose a plug-in detection framework that capitalizes on the TS-LLM's own variable-length input capability. Our method uses sampling-induced divergence as a detection signal. Given an input sequence, we generate multiple shortened variants and detect AEs by measuring the consistency of their forecasts: Benign sequences tend to produce stable predictions under sampling, whereas adversarial sequences show low forecast similarity, because perturbations optimized for a full-length sequence do not transfer reliably to shorter, differently-structured subsamples. We evaluate our approach on three representative TS-LLMs (TimeGPT, TimesFM, and TimeLLM) across three energy datasets: ETTh2 (Electricity Transformer Temperature), NI (Hourly Energy Consumption), and Consumption (Hourly Electricity Consumption and Production). Empirical results confirm strong and robust detection performance across both black-box and white-box attack scenarios, highlighting its practicality as a reliable safeguard for TS-LLM forecasting in real-world energy systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 148,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12218",
        "abs_url": "https://arxiv.org/abs/2512.12218",
        "pdf_url": "https://arxiv.org/pdf/2512.12218",
        "title": "Journey Before Destination: On the importance of Visual Faithfulness in Slow Thinking",
        "authors": [
            "Rheeya Uppaal",
            "Phu Mon Htut",
            "Min Bai",
            "Nikolaos Pappas",
            "Zheng Qi"
        ],
        "comments": "Preprint",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Reasoning-augmented vision language models (VLMs) generate explicit chains of thought that promise greater capability and transparency but also introduce new failure modes: models may reach correct answers via visually unfaithful intermediate steps, or reason faithfully yet fail on the final prediction. Standard evaluations that only measure final-answer accuracy cannot distinguish these behaviors. We introduce the visual faithfulness of reasoning chains as a distinct evaluation dimension, focusing on whether the perception steps of a reasoning chain are grounded in the image. We propose a training- and reference-free framework that decomposes chains into perception versus reasoning steps and uses off-the-shelf VLM judges for step-level faithfulness, additionally verifying this approach through a human meta-evaluation. Building on this metric, we present a lightweight self-reflection procedure that detects and locally regenerates unfaithful perception steps without any training. Across multiple reasoning-trained VLMs and perception-heavy benchmarks, our method reduces Unfaithful Perception Rate while preserving final-answer accuracy, improving the reliability of multimodal reasoning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 149,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12230",
        "abs_url": "https://arxiv.org/abs/2512.12230",
        "pdf_url": "https://arxiv.org/pdf/2512.12230",
        "title": "Learning to Get Up Across Morphologies: Zero-Shot Recovery with a Unified Humanoid Policy",
        "authors": [
            "Jonathan Spraggett"
        ],
        "comments": "Accepted at 28th RoboCup International Symposium",
        "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
        "abstract": "Fall recovery is a critical skill for humanoid robots in dynamic environments such as RoboCup, where prolonged downtime often decides the match. Recent techniques using deep reinforcement learning (DRL) have produced robust get-up behaviors, yet existing methods require training of separate policies for each robot morphology. This paper presents a single DRL policy capable of recovering from falls across seven humanoid robots with diverse heights (0.48 - 0.81 m), weights (2.8 - 7.9 kg), and dynamics. Trained with CrossQ, the unified policy transfers zero-shot up to 86 +/- 7% (95% CI [81, 89]) on unseen morphologies, eliminating the need for robot-specific training. Comprehensive leave-one-out experiments, morph scaling analysis, and diversity ablations show that targeted morphological coverage improves zero-shot generalization. In some cases, the shared policy even surpasses the specialist baselines. These findings illustrate the practicality of morphology-agnostic control for fall recovery, laying the foundation for generalist humanoid control. The software is open-source and available at: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 150,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12267",
        "abs_url": "https://arxiv.org/abs/2512.12267",
        "pdf_url": "https://arxiv.org/pdf/2512.12267",
        "title": "Hellinger loss function for Generative Adversarial Networks",
        "authors": [
            "Giovanni Saraceno",
            "Anand N. Vidyashankar",
            "Claudio Agostinelli"
        ],
        "comments": "25 pages and Supplemental Material",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "We propose Hellinger-type loss functions for training Generative Adversarial Networks (GANs), motivated by the boundedness, symmetry, and robustness properties of the Hellinger distance. We define an adversarial objective based on this divergence and study its statistical properties within a general parametric framework. We establish the existence, uniqueness, consistency, and joint asymptotic normality of the estimators obtained from the adversarial training procedure. In particular, we analyze the joint estimation of both generator and discriminator parameters, offering a comprehensive asymptotic characterization of the resulting estimators. We introduce two implementations of the Hellinger-type loss and we evaluate their empirical behavior in comparison with the classic (Maximum Likelihood-type) GAN loss. Through a controlled simulation study, we demonstrate that both proposed losses yield improved estimation accuracy and robustness under increasing levels of data contamination.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 151,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12289",
        "abs_url": "https://arxiv.org/abs/2512.12289",
        "pdf_url": "https://arxiv.org/pdf/2512.12289",
        "title": "Robust Outlier Detection and Low-Latency Concept Drift Adaptation for Data Stream Regression: A Dual-Channel Architecture",
        "authors": [
            "Bingbing Wang",
            "Shengyan Sun",
            "Jiaqi Wang",
            "Yu Tang"
        ],
        "comments": "",
        "subjects": "Methodology (stat.ME); Machine Learning (cs.LG)",
        "abstract": "Outlier detection and concept drift detection represent two challenges in data analysis. Most studies address these issues separately. However, joint detection mechanisms in regression remain underexplored, where the continuous nature of output spaces makes distinguishing drifts from outliers inherently challenging. To address this, we propose a novel robust regression framework for joint outlier and concept drift detection. Specifically, we introduce a dual-channel decision process that orchestrates prediction residuals into two coupled logic flows: a rapid response channel for filtering point outliers and a deep analysis channel for diagnosing drifts. We further develop the Exponentially Weighted Moving Absolute Deviation with Distinguishable Types (EWMAD-DT) detector to autonomously differentiate between abrupt and incremental drifts via dynamic thresholding. Comprehensive experiments on both synthetic and real-world datasets demonstrate that our unified framework, enhanced by EWMAD-DT, exhibits superior detection performance even when point outliers and concept drifts coexist.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 152,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12295",
        "abs_url": "https://arxiv.org/abs/2512.12295",
        "pdf_url": "https://arxiv.org/pdf/2512.12295",
        "title": "Near-Zero-Overhead Freshness for Recommendation Systems via Inference-Side Model Updates",
        "authors": [
            "Wenjun Yu",
            "Sitian Chen",
            "Cheng Chen",
            "Amelie Chi Zhou"
        ],
        "comments": "13 Pages, 19 figures",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)",
        "abstract": "Deep Learning Recommendation Models (DLRMs) underpin personalized services but face a critical freshness-accuracy tradeoff due to massive parameter synchronization overheads. Production DLRMs deploy decoupled training/inference clusters, where synchronizing petabyte-scale embedding tables (EMTs) causes multi-minute staleness, degrading recommendation quality and revenue. We observe that (1) inference nodes exhibit sustained CPU underutilization (peak <= 20%), and (2) EMT gradients possess intrinsic low-rank structure, enabling compact update representation. We present LiveUpdate, a system that eliminates inter-cluster synchronization by colocating Low-Rank Adaptation (LoRA) trainers within inference nodes. LiveUpdate addresses two core challenges: (1) dynamic rank adaptation via singular value monitoring to constrain memory overhead (<2% of EMTs), and (2) NUMA-aware resource scheduling with hardware-enforced QoS to eliminate update inference contention (P99 latency impact <20ms). Evaluations show LiveUpdate reduces update costs by 2x versus delta-update baselines while achieving higher accuracy within 1-hour windows. By transforming idle inference resources into freshness engines, LiveUpdate delivers online model updates while outperforming state-of-the-art delta-update methods by 0.04% to 0.24% in accuracy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 153,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12334",
        "abs_url": "https://arxiv.org/abs/2512.12334",
        "pdf_url": "https://arxiv.org/pdf/2512.12334",
        "title": "Extending the application of dynamic Bayesian networks in calculating market risk: Standard and stressed expected shortfall",
        "authors": [
            "Eden Gross",
            "Ryan Kruger",
            "Francois Toerien"
        ],
        "comments": "",
        "subjects": "Risk Management (q-fin.RM); Machine Learning (cs.LG); Computational Finance (q-fin.CP); Statistical Finance (q-fin.ST)",
        "abstract": "In the last five years, expected shortfall (ES) and stressed ES (SES) have become key required regulatory measures of market risk in the banking sector, especially following events such as the global financial crisis. Thus, finding ways to optimize their estimation is of great importance. We extend the application of dynamic Bayesian networks (DBNs) to the estimation of 10-day 97.5% ES and stressed ES, building on prior work applying DBNs to value at risk. Using the S&P 500 index as a proxy for the equities trading desk of a US bank, we compare the performance of three DBN structure-learning algorithms with several traditional market risk models, using either the normal or the skewed Student's t return distributions. Backtesting shows that all models fail to produce statistically accurate ES and SES forecasts at the 2.5% level, reflecting the difficulty of modeling extreme tail behavior. For ES, the EGARCH(1,1) model (normal) produces the most accurate forecasts, while, for SES, the GARCH(1,1) model (normal) performs best. All distribution-dependent models deteriorate substantially when using the skewed Student's t distribution. The DBNs perform comparably to the historical simulation model, but their contribution to tail prediction is limited by the small weight assigned to their one-day-ahead forecasts within the return distribution. Future research should examine weighting schemes that enhance the influence of forward-looking DBN forecasts on tail risk estimation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 154,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12358",
        "abs_url": "https://arxiv.org/abs/2512.12358",
        "pdf_url": "https://arxiv.org/pdf/2512.12358",
        "title": "Towards a pretrained deep learning estimator of the Linfoot informational correlation",
        "authors": [
            "Stéphanie M. van den Berg",
            "Ulrich Halekoh",
            "Sören Möller",
            "Andreas Kryger Jensen",
            "Jacob von Bornemann Hjelmborg"
        ],
        "comments": "3 figures",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Methodology (stat.ME)",
        "abstract": "We develop a supervised deep-learning approach to estimate mutual information between two continuous random variables. As labels, we use the Linfoot informational correlation, a transformation of mutual information that has many important properties. Our method is based on ground truth labels for Gaussian and Clayton copulas. We compare our method with estimators based on kernel density, k-nearest neighbours and neural estimators. We show generally lower bias and lower variance. As a proof of principle, future research could look into training the model with a more diverse set of examples from other copulas for which ground truth labels are available.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 155,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12366",
        "abs_url": "https://arxiv.org/abs/2512.12366",
        "pdf_url": "https://arxiv.org/pdf/2512.12366",
        "title": "ElasticVR: Elastic Task Computing in Multi-User Multi-Connectivity Wireless Virtual Reality (VR) Systems",
        "authors": [
            "Babak Badnava",
            "Jacob Chakareski",
            "Morteza Hashemi"
        ],
        "comments": "Submitted to ACM TOMM",
        "subjects": "Information Theory (cs.IT); Machine Learning (cs.LG); Image and Video Processing (eess.IV)",
        "abstract": "Diverse emerging VR applications integrate streaming of high fidelity 360 video content that requires ample amounts of computation and data rate. Scalable 360 video tiling enables having elastic VR computational tasks that can be scaled adaptively in computation and data rate based on the available user and system resources. We integrate scalable 360 video tiling in an edge-client wireless multi-connectivity architecture for joint elastic task computation offloading across multiple VR users called ElasticVR. To balance the trade-offs in communication, computation, energy consumption, and QoE that arise herein, we formulate a constrained QoE and energy optimization problem that integrates the multi-user/multi-connectivity action space with the elasticity of VR computational tasks. The ElasticVR framework introduces two multi-agent deep reinforcement learning solutions, namely CPPG and IPPG. CPPG adopts a centralized training and centralized execution approach to capture the coupling between users' communication and computational demands. This leads to globally coordinated decisions at the cost of increased computational overheads and limited scalability. To address the latter challenges, we also explore an alternative strategy denoted IPPG that adopts a centralized training with decentralized execution paradigm. IPPG leverages shared information and parameter sharing to learn robust policies; however, during execution, each user takes action independently based on its local state information only. The decentralized execution alleviates the communication and computation overhead of centralized decision-making and improves scalability. We show that the ElasticVR framework improves the PSNR by 43.21%, while reducing the response time and energy consumption by 42.35% and 56.83%, respectively, compared with a case where no elasticity is incorporated into VR computations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 156,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12432",
        "abs_url": "https://arxiv.org/abs/2512.12432",
        "pdf_url": "https://arxiv.org/pdf/2512.12432",
        "title": "Data-driven modelling of autonomous and forced dynamical systems",
        "authors": [
            "Robert Szalai"
        ],
        "comments": "Keywords: Invariant foliation, Invariant manifold, Reduced order model, Instantaneous frequency, Tensor approximation, Machine learning, JEPA",
        "subjects": "Dynamical Systems (math.DS); Machine Learning (cs.LG); Data Analysis, Statistics and Probability (physics.data-an)",
        "abstract": "The paper demonstrates that invariant foliations are accurate, data-efficient and practical tools for data-driven modelling of physical systems. Invariant foliations can be fitted to data that either fill the phase space or cluster about an invariant manifold. Invariant foliations can be fitted to a single trajectory or multiple trajectories. Over and underfitting are eliminated by appropriately choosing a function representation and its hyperparameters, such as polynomial orders. The paper extends invariant foliations to forced and parameter dependent systems. It is assumed that forcing is provided by a volume preserving map, and therefore the forcing can be periodic, quasi-periodic or even chaotic. The method utilises full trajectories, hence it is able to predict long-term dynamics accurately. We take into account if a forced system is reducible to an autonomous system about a steady state, similar to how Floquet theory guarantees reducibility for periodically forced systems. In order to find an invariant manifold, multiple invariant foliations are calculated in the neighbourhood of the invariant manifold. Some of the invariant foliations can be linear, while others nonlinear but only defined in a small neighbourhood of an invariant manifold, which reduces the number of parameters to be identified. An invariant manifold is recovered as the zero level set of one or more of the foliations. To interpret the results, the identified mathematical models are transformed to a canonical form and instantaneous frequency and damping information are calculated.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 157,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12435",
        "abs_url": "https://arxiv.org/abs/2512.12435",
        "pdf_url": "https://arxiv.org/pdf/2512.12435",
        "title": "Co-Hub Node Based Multiview Graph Learning with Theoretical Guarantees",
        "authors": [
            "Bisakh Banerjee",
            "Mohammad Alwardat",
            "Tapabrata Maiti",
            "Selin Aviyente"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Identifying the graphical structure underlying the observed multivariate data is essential in numerous applications. Current methodologies are predominantly confined to deducing a singular graph under the presumption that the observed data are uniform. However, many contexts involve heterogeneous datasets that feature multiple closely related graphs, typically referred to as multiview graphs. Previous research on multiview graph learning promotes edge-based similarity across layers using pairwise or consensus-based regularizers. However, multiview graphs frequently exhibit a shared node-based architecture across different views, such as common hub nodes. Such commonalities can enhance the precision of learning and provide interpretive insight. In this paper, we propose a co-hub node model, positing that different views share a common group of hub nodes. The associated optimization framework is developed by enforcing structured sparsity on the connections of these co-hub nodes. Moreover, we present a theoretical examination of layer identifiability and determine bounds on estimation error. The proposed methodology is validated using both synthetic graph data and fMRI time series data from multiple subjects to discern several closely related graphs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 158,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12442",
        "abs_url": "https://arxiv.org/abs/2512.12442",
        "pdf_url": "https://arxiv.org/pdf/2512.12442",
        "title": "Efficient Level-Crossing Probability Calculation for Gaussian Process Modeled Data",
        "authors": [
            "Haoyu Li",
            "Isaac J Michaud",
            "Ayan Biswas",
            "Han-Wei Shen"
        ],
        "comments": "IEEE PacificVis 2024",
        "subjects": "Machine Learning (stat.ML); Graphics (cs.GR); Machine Learning (cs.LG)",
        "abstract": "Almost all scientific data have uncertainties originating from different sources. Gaussian process regression (GPR) models are a natural way to model data with Gaussian-distributed uncertainties. GPR also has the benefit of reducing I/O bandwidth and storage requirements for large scientific simulations. However, the reconstruction from the GPR models suffers from high computation complexity. To make the situation worse, classic approaches for visualizing the data uncertainties, like probabilistic marching cubes, are also computationally very expensive, especially for data of high resolutions. In this paper, we accelerate the level-crossing probability calculation efficiency on GPR models by subdividing the data spatially into a hierarchical data structure and only reconstructing values adaptively in the regions that have a non-zero probability. For each region, leveraging the known GPR kernel and the saved data observations, we propose a novel approach to efficiently calculate an upper bound for the level-crossing probability inside the region and use this upper bound to make the subdivision and reconstruction decisions. We demonstrate that our value occurrence probability estimation is accurate with a low computation cost by experiments that calculate the level-crossing probability fields on different datasets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 159,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12458",
        "abs_url": "https://arxiv.org/abs/2512.12458",
        "pdf_url": "https://arxiv.org/pdf/2512.12458",
        "title": "Breaking the Curse of Dimensionality: On the Stability of Modern Vector Retrieval",
        "authors": [
            "Vihan Lakshman",
            "Blaise Munyampirwa",
            "Julian Shun",
            "Benjamin Coleman"
        ],
        "comments": "27 pages",
        "subjects": "Information Retrieval (cs.IR); Computational Geometry (cs.CG); Databases (cs.DB); Machine Learning (cs.LG)",
        "abstract": "Modern vector databases enable efficient retrieval over high-dimensional neural embeddings, powering applications from web search to retrieval-augmented generation. However, classical theory predicts such tasks should suffer from the curse of dimensionality, where distances between points become nearly indistinguishable, thereby crippling efficient nearest-neighbor search. We revisit this paradox through the lens of stability, the property that small perturbations to a query do not radically alter its nearest neighbors. Building on foundational results, we extend stability theory to three key retrieval settings widely used in practice: (i) multi-vector search, where we prove that the popular Chamfer distance metric preserves single-vector stability, while average pooling aggregation may destroy it; (ii) filtered vector search, where we show that sufficiently large penalties for mismatched filters can induce stability even when the underlying search is unstable; and (iii) sparse vector search, where we formalize and prove novel sufficient stability conditions. Across synthetic and real datasets, our experimental results match our theoretical predictions, offering concrete guidance for model and system design to avoid the curse of dimensionality.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 160,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12463",
        "abs_url": "https://arxiv.org/abs/2512.12463",
        "pdf_url": "https://arxiv.org/pdf/2512.12463",
        "title": "Understanding Overparametrization in Survival Models through Double-Descent",
        "authors": [
            "Yin Liu",
            "Jianwen Cai",
            "Didong Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Statistics Theory (math.ST)",
        "abstract": "Classical statistical learning theory predicts a U-shaped relationship between test loss and model capacity, driven by the bias-variance trade-off. Recent advances in modern machine learning have revealed a more complex pattern, double-descent, in which test loss, after peaking near the interpolation threshold, decreases again as model capacity continues to grow. While this behavior has been extensively analyzed in regression and classification, its manifestation in survival analysis remains unexplored. This study investigates double-descent in four representative survival models: DeepSurv, PC-Hazard, Nnet-Survival, and N-MTLR. We rigorously define interpolation and finite-norm interpolation, two key characteristics of loss-based models to understand double-descent. We then show the existence (or absence) of (finite-norm) interpolation of all four models. Our findings clarify how likelihood-based losses and model implementation jointly determine the feasibility of interpolation and show that overfitting should not be regarded as benign for survival models. All theoretical results are supported by numerical experiments that highlight the distinct generalization behaviors of survival models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 161,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12477",
        "abs_url": "https://arxiv.org/abs/2512.12477",
        "pdf_url": "https://arxiv.org/pdf/2512.12477",
        "title": "MetaHGNIE: Meta-Path Induced Hypergraph Contrastive Learning in Heterogeneous Knowledge Graphs",
        "authors": [
            "Jiawen Chen",
            "Yanyan He",
            "Qi Shao",
            "Mengli Wei",
            "Duxin Chen",
            "Wenwu Yu",
            "Yanlong Zhao"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Node importance estimation (NIE) in heterogeneous knowledge graphs is a critical yet challenging task, essential for applications such as recommendation, knowledge reasoning, and question answering. Existing methods often rely on pairwise connections, neglecting high-order dependencies among multiple entities and relations, and they treat structural and semantic signals independently, hindering effective cross-modal integration. To address these challenges, we propose MetaHGNIE, a meta-path induced hypergraph contrastive learning framework for disentangling and aligning structural and semantic information. MetaHGNIE constructs a higher-order knowledge graph via meta-path sequences, where typed hyperedges capture multi-entity relational contexts. Structural dependencies are aggregated with local attention, while semantic representations are encoded through a hypergraph transformer equipped with sparse chunking to reduce redundancy. Finally, a multimodal fusion module integrates structural and semantic embeddings under contrastive learning with auxiliary supervision, ensuring robust cross-modal alignment. Extensive experiments on benchmark NIE datasets demonstrate that MetaHGNIE consistently outperforms state-of-the-art baselines. These results highlight the effectiveness of explicitly modeling higher-order interactions and cross-modal alignment in heterogeneous knowledge graphs. Our code is available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 162,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12534",
        "abs_url": "https://arxiv.org/abs/2512.12534",
        "pdf_url": "https://arxiv.org/pdf/2512.12534",
        "title": "Animus3D: Text-driven 3D Animation via Motion Score Distillation",
        "authors": [
            "Qi Sun",
            "Can Wang",
            "Jiaxiang Shang",
            "Wensen Feng",
            "Jing Liao"
        ],
        "comments": "SIGGRAPH Asia 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR); Machine Learning (cs.LG)",
        "abstract": "We present Animus3D, a text-driven 3D animation framework that generates motion field given a static 3D asset and text prompt. Previous methods mostly leverage the vanilla Score Distillation Sampling (SDS) objective to distill motion from pretrained text-to-video diffusion, leading to animations with minimal movement or noticeable jitter. To address this, our approach introduces a novel SDS alternative, Motion Score Distillation (MSD). Specifically, we introduce a LoRA-enhanced video diffusion model that defines a static source distribution rather than pure noise as in SDS, while another inversion-based noise estimation technique ensures appearance preservation when guiding motion. To further improve motion fidelity, we incorporate explicit temporal and spatial regularization terms that mitigate geometric distortions across time and space. Additionally, we propose a motion refinement module to upscale the temporal resolution and enhance fine-grained details, overcoming the fixed-resolution constraints of the underlying video model. Extensive experiments demonstrate that Animus3D successfully animates static 3D assets from diverse text prompts, generating significantly more substantial and detailed motion than state-of-the-art baselines while maintaining high visual integrity. Code will be released at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 163,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12544",
        "abs_url": "https://arxiv.org/abs/2512.12544",
        "pdf_url": "https://arxiv.org/pdf/2512.12544",
        "title": "HyperEdit: Unlocking Instruction-based Text Editing in LLMs via Hypernetworks",
        "authors": [
            "Yiming Zeng",
            "Jinghan Cao",
            "Zexin Li",
            "Wanhao Yu",
            "Zhankai Ye",
            "Dawei Xiang",
            "Ting Hua",
            "Xin Liu",
            "Shangqian Gao",
            "Tingting Yu"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Instruction-based text editing is increasingly critical for real-world applications such as code editors (e.g., Cursor), but Large Language Models (LLMs) continue to struggle with this task. Unlike free-form generation, editing requires faithfully implementing user instructions while preserving unchanged content, as even minor unintended modifications can break functionality. Existing approaches treat editing as generic text generation, leading to two key failures: they struggle to faithfully align edits with diverse user intents, and they often over-edit unchanged regions. We propose HyperEdit to address both issues. First, we introduce hypernetwork-based dynamic adaptation that generates request-specific parameters, enabling the model to tailor its editing strategy to each instruction. Second, we develop difference-aware regularization that focuses supervision on modified spans, preventing over-editing while ensuring precise, minimal changes. HyperEdit achieves a 9%--30% relative improvement in BLEU on modified regions over state-of-the-art baselines, despite utilizing only 3B parameters.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 164,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12549",
        "abs_url": "https://arxiv.org/abs/2512.12549",
        "pdf_url": "https://arxiv.org/pdf/2512.12549",
        "title": "Supervised Contrastive Frame Aggregation for Video Representation Learning",
        "authors": [
            "Shaif Chowdhury",
            "Mushfika Rahman",
            "Greg Hamerly"
        ],
        "comments": "12 pages",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "We propose a supervised contrastive learning framework for video representation learning that leverages temporally global context. We introduce a video to image aggregation strategy that spatially arranges multiple frames from each video into a single input image. This design enables the use of pre trained convolutional neural network backbones such as ResNet50 and avoids the computational overhead of complex video transformer models. We then design a contrastive learning objective that directly compares pairwise projections generated by the model. Positive pairs are defined as projections from videos sharing the same label while all other projections are treated as negatives. Multiple natural views of the same video are created using different temporal frame samplings from the same underlying video. Rather than relying on data augmentation these frame level variations produce diverse positive samples with global context and reduce overfitting. Experiments on the Penn Action and HMDB51 datasets demonstrate that the proposed method outperforms existing approaches in classification accuracy while requiring fewer computational resources. The proposed Supervised Contrastive Frame Aggregation method learns effective video representations in both supervised and self supervised settings and supports video based tasks such as classification and captioning. The method achieves seventy six percent classification accuracy on Penn Action compared to forty three percent achieved by ViVIT and forty eight percent accuracy on HMDB51 compared to thirty seven percent achieved by ViVIT.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 165,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12550",
        "abs_url": "https://arxiv.org/abs/2512.12550",
        "pdf_url": "https://arxiv.org/pdf/2512.12550",
        "title": "Iterative Sampling Methods for Sinkhorn Distributionally Robust Optimization",
        "authors": [
            "Jie Wang"
        ],
        "comments": "29 pages",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Distributionally robust optimization (DRO) has emerged as a powerful paradigm for reliable decision-making under uncertainty. This paper focuses on DRO with ambiguity sets defined via the Sinkhorn discrepancy: an entropy-regularized Wasserstein distance, referred to as Sinkhorn DRO. Existing work primarily addresses Sinkhorn DRO from a dual perspective, leveraging its formulation as a conditional stochastic optimization problem, for which many stochastic gradient methods are applicable. However, the theoretical analyses of such methods often rely on the boundedness of the loss function, and it is indirect to obtain the worst-case distribution associated with Sinkhorn DRO. In contrast, we study Sinkhorn DRO from the primal perspective, by reformulating it as a bilevel program with several infinite-dimensional lower-level subproblems over probability space. This formulation enables us to simultaneously obtain the optimal robust decision and the worst-case distribution, which is valuable in practical settings, such as generating stress-test scenarios or designing robust learning algorithms. We propose both double-loop and single-loop sampling-based algorithms with theoretical guarantees to solve this bilevel program. Finally, we demonstrate the effectiveness of our approach through a numerical study on adversarial classification.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 166,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12574",
        "abs_url": "https://arxiv.org/abs/2512.12574",
        "pdf_url": "https://arxiv.org/pdf/2512.12574",
        "title": "Mind the Jumps: A Scalable Robust Local Gaussian Process for Multidimensional Response Surfaces with Discontinuities",
        "authors": [
            "Isaac Adjetey",
            "Yiyuan She"
        ],
        "comments": "25 pages, 4 figures",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Applications (stat.AP)",
        "abstract": "Modeling response surfaces with abrupt jumps and discontinuities remains a major challenge across scientific and engineering domains. Although Gaussian process models excel at capturing smooth nonlinear relationships, their stationarity assumptions limit their ability to adapt to sudden input-output variations. Existing nonstationary extensions, particularly those based on domain partitioning, often struggle with boundary inconsistencies, sensitivity to outliers, and scalability issues in higher-dimensional settings, leading to reduced predictive accuracy and unreliable parameter estimation. To address these challenges, this paper proposes the Robust Local Gaussian Process (RLGP) model, a framework that integrates adaptive nearest-neighbor selection with a sparsity-driven robustification mechanism. Unlike existing methods, RLGP leverages an optimization-based mean-shift adjustment after a multivariate perspective transformation combined with local neighborhood modeling to mitigate the influence of outliers. This approach improves predictive accuracy near discontinuities while enhancing robustness to data heterogeneity. Comprehensive evaluations on real-world datasets show that RLGP consistently delivers high predictive accuracy and maintains competitive computational efficiency, especially in scenarios with sharp transitions and complex response structures. Scalability tests further confirm RLGP's stability and reliability in higher-dimensional settings, where other methods struggle. These results establish RLGP as an effective and practical solution for modeling nonstationary and discontinuous response surfaces across a wide range of applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 167,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12578",
        "abs_url": "https://arxiv.org/abs/2512.12578",
        "pdf_url": "https://arxiv.org/pdf/2512.12578",
        "title": "Scalable Quantum Error Mitigation with Neighbor-Informed Learning",
        "authors": [
            "Zhenyu Chen",
            "Bin Cheng",
            "Minbo Gao",
            "Xiaodie Lin",
            "Ruiqi Zhang",
            "Zhaohui Wei",
            "Zhengfeng Ji"
        ],
        "comments": "",
        "subjects": "Quantum Physics (quant-ph); Computational Complexity (cs.CC); Machine Learning (cs.LG)",
        "abstract": "Noise in quantum hardware is the primary obstacle to realizing the transformative potential of quantum computing. Quantum error mitigation (QEM) offers a promising pathway to enhance computational accuracy on near-term devices, yet existing methods face a difficult trade-off between performance, resource overhead, and theoretical guarantees. In this work, we introduce neighbor-informed learning (NIL), a versatile and scalable QEM framework that unifies and strengthens existing methods such as zero-noise extrapolation (ZNE) and probabilistic error cancellation (PEC), while offering improved flexibility, accuracy, efficiency, and robustness. NIL learns to predict the ideal output of a target quantum circuit from the noisy outputs of its structurally related ``neighbor'' circuits. A key innovation is our 2-design training method, which generates training data for our machine learning model. In contrast to conventional learning-based QEM protocols that create training circuits by replacing non-Clifford gates with uniformly random Clifford gates, our approach achieves higher accuracy and efficiency, as demonstrated by both theoretical analysis and numerical simulation. Furthermore, we prove that the required size of the training set scales only \\emph{logarithmically} with the total number of neighbor circuits, enabling NIL to be applied to problems involving large-scale quantum circuits. Our work establishes a theoretically grounded and practically efficient framework for QEM, paving a viable path toward achieving quantum advantage on noisy hardware.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 168,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12594",
        "abs_url": "https://arxiv.org/abs/2512.12594",
        "pdf_url": "https://arxiv.org/pdf/2512.12594",
        "title": "ceLLMate: Sandboxing Browser AI Agents",
        "authors": [
            "Luoxi Meng",
            "Henry Feng",
            "Ilia Shumailov",
            "Earlence Fernandes"
        ],
        "comments": "Homepage: this https URL",
        "subjects": "Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
        "abstract": "Browser-using agents (BUAs) are an emerging class of autonomous agents that interact with web browsers in human-like ways, including clicking, scrolling, filling forms, and navigating across pages. While these agents help automate repetitive online tasks, they are vulnerable to prompt injection attacks that can trick an agent into performing undesired actions, such as leaking private information or issuing state-changing requests. We propose ceLLMate, a browser-level sandboxing framework that restricts the agent's ambient authority and reduces the blast radius of prompt injections. We address two fundamental challenges: (1) The semantic gap challenge in policy enforcement arises because the agent operates through low-level UI observations and manipulations; however, writing and enforcing policies directly over UI-level events is brittle and error-prone. To address this challenge, we introduce an agent sitemap that maps low-level browser behaviors to high-level semantic actions. (2) Policy prediction in BUAs is the norm rather than the exception. BUAs have no app developer to pre-declare sandboxing policies, and thus, ceLLMate pairs website-authored mandatory policies with an automated policy-prediction layer that adapts and instantiates these policies from the user's natural-language task. We implement ceLLMate as an agent-agnostic browser extension and demonstrate how it enables sandboxing policies that effectively block various types of prompt injection attacks with negligible overhead.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 169,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12624",
        "abs_url": "https://arxiv.org/abs/2512.12624",
        "pdf_url": "https://arxiv.org/pdf/2512.12624",
        "title": "CoLSE: A Lightweight and Robust Hybrid Learned Model for Single-Table Cardinality Estimation using Joint CDF",
        "authors": [
            "Lankadinee Rathuwadu",
            "Guanli Liu",
            "Christopher Leckie",
            "Renata Borovica-Gajic"
        ],
        "comments": "",
        "subjects": "Databases (cs.DB); Machine Learning (cs.LG)",
        "abstract": "Cardinality estimation (CE), the task of predicting the result size of queries is a critical component of query optimization. Accurate estimates are essential for generating efficient query execution plans. Recently, machine learning techniques have been applied to CE, broadly categorized into query-driven and data-driven approaches. Data-driven methods learn the joint distribution of data, while query-driven methods construct regression models that map query features to cardinalities. Ideally, a CE technique should strike a balance among three key factors: accuracy, efficiency, and memory footprint. However, existing state-of-the-art models often fail to achieve this balance. To address this, we propose CoLSE, a hybrid learned approach for single-table cardinality estimation. CoLSE directly models the joint probability over queried intervals using a novel algorithm based on copula theory and integrates a lightweight neural network to correct residual estimation errors. Experimental results show that CoLSE achieves a favorable trade-off among accuracy, training time, inference latency, and model size, outperforming existing state-of-the-art methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 170,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12654",
        "abs_url": "https://arxiv.org/abs/2512.12654",
        "pdf_url": "https://arxiv.org/pdf/2512.12654",
        "title": "Modeling Authorial Style in Urdu Novels Using Character Interaction Graphs and Graph Neural Networks",
        "authors": [
            "Hassan Mujtaba",
            "Hamza Naveed",
            "Hanzlah Munir"
        ],
        "comments": "6 pages",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG); Social and Information Networks (cs.SI)",
        "abstract": "Authorship analysis has traditionally focused on lexical and stylistic cues within text, while higher-level narrative structure remains underexplored, particularly for low-resource languages such as Urdu. This work proposes a graph-based framework that models Urdu novels as character interaction networks to examine whether authorial style can be inferred from narrative structure alone. Each novel is represented as a graph where nodes correspond to characters and edges denote their co-occurrence within narrative proximity. We systematically compare multiple graph representations, including global structural features, node-level semantic summaries, unsupervised graph embeddings, and supervised graph neural networks. Experiments on a dataset of 52 Urdu novels written by seven authors show that learned graph representations substantially outperform hand-crafted and unsupervised baselines, achieving up to 0.857 accuracy under a strict author-aware evaluation protocol.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 171,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12676",
        "abs_url": "https://arxiv.org/abs/2512.12676",
        "pdf_url": "https://arxiv.org/pdf/2512.12676",
        "title": "Robust Variational Bayes by Min-Max Median Aggregation",
        "authors": [
            "Jiawei Yan",
            "Ju Liu",
            "Weidong Liu",
            "Jiyuan Tu"
        ],
        "comments": "34 pages, 11 figures",
        "subjects": "Methodology (stat.ME); Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "We propose a robust and scalable variational Bayes (VB) framework designed to effectively handle contamination and outliers in dataset. Our approach partitions the data into $m$ disjoint subsets and formulates a joint optimization problem based on robust aggregation principles. A key insight is that the full posterior distribution is equivalent to the minimizer of the mean Kullback-Leibler (KL) divergence from the $m$-powered local posterior distributions. To enhance robustness, we replace the mean KL divergence with a min-max median formulation. The min-max formulation not only ensures consistency between the KL minimizer and the Evidence Lower Bound (ELBO) maximizer but also facilitates the establishment of improved statistical rates for the mean of variational posterior. We observe a notable discrepancy in the $m$-powered marginal log likelihood function contingent on the presence of local latent variables. To address this, we treat these two scenarios separately to guarantee the consistency of the aggregated variational posterior. Specifically, when local latent variables are present, we introduce an aggregate-and-rescale strategy. Theoretically, we provide a non-asymptotic analysis of our proposed posterior, incorporating a refined analysis of Bernstein-von Mises (BvM) theorem to accommodate a diverging number of subsets $m$. Our findings indicate that the two-stage approach yields a smaller approximation error compared to directly aggregating the $m$-powered local posteriors. Furthermore, we establish a nearly optimal statistical rate for the mean of the proposed posterior, advancing existing theories related to min-max median estimators. The efficacy of our method is demonstrated through extensive simulation studies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 172,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12701",
        "abs_url": "https://arxiv.org/abs/2512.12701",
        "pdf_url": "https://arxiv.org/pdf/2512.12701",
        "title": "Efficient Vision-Language Reasoning via Adaptive Token Pruning",
        "authors": [
            "Xue Li",
            "Xiaonan Song",
            "Henry Hu"
        ],
        "comments": "10 pages, 3 figures. Expanded version of an extended abstract accepted at NeurIPS 2025 Workshop on VLM4RWD. Presents methodology and preliminary experimental results",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Real-world deployment of Vision-Language Models (VLMs) is hindered by high computational demands, as existing architectures inefficiently process all tokens uniformly. We introduce Adaptive Token Pruning (ATP), a dynamic inference mechanism that retains only the most informative tokens based on contextual relevance. ATP operates at the vision-language interface, assigning a hybrid importance score combining ViT CLS attention (intra-modal saliency) and CLIP text-image similarity (inter-modal relevance) to keep top-K tokens for the LLM. Unlike static compression, ATP adapts to each input without modifying the backbone. Proposed as a lightweight gating module, ATP is compatible with popular backbones like BLIP-2, LLaVA, and Flamingo. Preliminary evaluations across VQAv2, GQA, and COCO indicate that ATP reduces inference FLOPs by around 40% and achieves roughly 1.5x speedups in end-to-end latency with negligible accuracy loss (less than 1%). Qualitative analyses suggest ATP preserves visual grounding and enhances interpretability. Beyond efficiency, we investigate robustness under corruptions; observations suggest adaptive pruning suppresses spurious correlations, improving stability. These findings imply that resource-constrained inference and model reliability are not competing objectives. Finally, we discuss ATP's role in efficient multimodal edge computing pipelines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 173,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12710",
        "abs_url": "https://arxiv.org/abs/2512.12710",
        "pdf_url": "https://arxiv.org/pdf/2512.12710",
        "title": "Practical Hybrid Quantum Language Models with Observable Readout on Real Hardware",
        "authors": [
            "Stefan Balauca",
            "Ada-Astrid Balauca",
            "Adrian Iftene"
        ],
        "comments": "",
        "subjects": "Quantum Physics (quant-ph); Machine Learning (cs.LG)",
        "abstract": "Hybrid quantum-classical models represent a crucial step toward leveraging near-term quantum devices for sequential data processing. We present Quantum Recurrent Neural Networks (QRNNs) and Quantum Convolutional Neural Networks (QCNNs) as hybrid quantum language models, reporting the first empirical demonstration of generative language modeling trained and evaluated end-to-end on real quantum hardware. Our architecture combines hardware-optimized parametric quantum circuits with a lightweight classical projection layer, utilizing a multi-sample SPSA strategy to efficiently train quantum parameters despite hardware noise. To characterize the capabilities of these models, we introduce a synthetic dataset designed to isolate syntactic dependencies in a controlled, low-resource environment. Experiments on IBM Quantum processors reveal the critical trade-offs between circuit depth and trainability, demonstrating that while noise remains a significant factor, observable-based readout enables the successful learning of sequential patterns on NISQ devices. These results establish a rigorous engineering baseline for generative quantum natural language processing, validating the feasibility of training complex sequence models on current quantum hardware.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 174,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12713",
        "abs_url": "https://arxiv.org/abs/2512.12713",
        "pdf_url": "https://arxiv.org/pdf/2512.12713",
        "title": "Self-Motivated Growing Neural Network for Adaptive Architecture via Local Structural Plasticity",
        "authors": [
            "Yiyang Jia",
            "Chengxu Zhou"
        ],
        "comments": "",
        "subjects": "Neural and Evolutionary Computing (cs.NE); Machine Learning (cs.LG)",
        "abstract": "Control policies in deep reinforcement learning are often implemented with fixed-capacity multilayer perceptrons trained by backpropagation, which lack structural plasticity and depend on global error signals. This paper introduces the Self-Motivated Growing Neural Network (SMGrNN), a controller whose topology evolves online through a local Structural Plasticity Module (SPM). The SPM monitors neuron activations and edge-wise weight update statistics over short temporal windows and uses these signals to trigger neuron insertion and pruning, while synaptic weights are updated by a standard gradient-based optimizer. This allows network capacity to be regulated during learning without manual architectural tuning. SMGrNN is evaluated on control benchmarks via policy distillation. Compared with multilayer perceptron baselines, it achieves similar or higher returns, lower variance, and task-appropriate network sizes. Ablation studies with growth disabled and growth-only variants isolate the role of structural plasticity, showing that adaptive topology improves reward stability. The local and modular design of SPM enables future integration of a Hebbian plasticity module and spike-timing-dependent plasticity, so that SMGrNN can support both artificial and spiking neural implementations driven by local rules.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 175,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12735",
        "abs_url": "https://arxiv.org/abs/2512.12735",
        "pdf_url": "https://arxiv.org/pdf/2512.12735",
        "title": "Limits To (Machine) Learning",
        "authors": [
            "Zhimin Chen",
            "Bryan Kelly",
            "Semyon Malamud"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Machine learning (ML) methods are highly flexible, but their ability to approximate the true data-generating process is fundamentally constrained by finite samples. We characterize a universal lower bound, the Limits-to-Learning Gap (LLG), quantifying the unavoidable discrepancy between a model's empirical fit and the population benchmark. Recovering the true population $R^2$, therefore, requires correcting observed predictive performance by this bound. Using a broad set of variables, including excess returns, yields, credit spreads, and valuation ratios, we find that the implied LLGs are large. This indicates that standard ML approaches can substantially understate true predictability in financial data. We also derive LLG-based refinements to the classic Hansen and Jagannathan (1991) bounds, analyze implications for parameter learning in general-equilibrium settings, and show that the LLG provides a natural mechanism for generating excess volatility.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 176,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12742",
        "abs_url": "https://arxiv.org/abs/2512.12742",
        "pdf_url": "https://arxiv.org/pdf/2512.12742",
        "title": "Transport Reversible Jump Markov Chain Monte Carlo with proposals generated by Variational Inference with Normalizing Flows",
        "authors": [
            "Pingping Yin",
            "Xiyun Jiao"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "We present a framework using variational inference with normalizing flows (VI-NFs) to generate proposals of reversible jump Markov chain Monte Carlo (RJMCMC) for efficient trans-dimensional Bayesian inference. Unlike transport reversible jump methods relying on forward KL minimization with pilot MCMC samples, our approach minimizes the reverse KL divergence which requires only samples from a base distribution, eliminating costly target sampling. The method employs RealNVP-based flows to learn model-specific transport maps, enabling construction of both between-model and within-model proposals. Our framework provides accurate marginal likelihood estimates from the variational approximation. This facilitates efficient model comparison and proposal adaptation in RJMCMC. Experiments on illustrative example, factor analysis and variable selection tasks in linear regression show that TRJ designed by VI-NFs achieves faster mixing and more efficient model space exploration compared to existing baselines. The proposed algorithm can be extended to conditional flows for amortized vairiational inference across models. Code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 177,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12749",
        "abs_url": "https://arxiv.org/abs/2512.12749",
        "pdf_url": "https://arxiv.org/pdf/2512.12749",
        "title": "Flow-matching Operators for Residual-Augmented Probabilistic Learning of Partial Differential Equations",
        "authors": [
            "Sahil Bhola",
            "Karthik Duraisamy"
        ],
        "comments": "",
        "subjects": "Computation (stat.CO); Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Learning probabilistic surrogates for PDEs remains challenging in data-scarce regimes: neural operators require large amounts of high-fidelity data, while generative approaches typically sacrifice resolution invariance. We formulate flow matching in an infinite-dimensional function space to learn a probabilistic transport that maps low-fidelity approximations to the manifold of high-fidelity PDE solutions via learned residual corrections. We develop a conditional neural operator architecture based on feature-wise linear modulation for flow-matching vector fields directly in function space, enabling inference at arbitrary spatial resolutions without retraining. To improve stability and representational control of the induced neural ODE, we parameterize the flow vector field as a sum of a linear operator and a nonlinear operator, combining lightweight linear components with a conditioned Fourier neural operator for expressive, input-dependent dynamics. We then formulate a residual-augmented learning strategy where the flow model learns probabilistic corrections from inexpensive low-fidelity surrogates to high-fidelity solutions, rather than learning the full solution mapping from scratch. Finally, we derive tractable training objectives that extend conditional flow matching to the operator setting with input-function-dependent couplings. To demonstrate the effectiveness of our approach, we present numerical experiments on a range of PDEs, including the 1D advection and Burgers' equation, and a 2D Darcy flow problem for flow through a porous medium. We show that the proposed method can accurately learn solution operators across different resolutions and fidelities and produces uncertainty estimates that appropriately reflect model confidence, even when trained on limited high-fidelity data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 178,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12755",
        "abs_url": "https://arxiv.org/abs/2512.12755",
        "pdf_url": "https://arxiv.org/pdf/2512.12755",
        "title": "An End-to-End Approach for Microgrid Probabilistic Forecasting and Robust Operation via Decision-focused Learning",
        "authors": [
            "Tingwei Cao",
            "Yan Xu"
        ],
        "comments": "10 pages",
        "subjects": "Systems and Control (eess.SY); Machine Learning (cs.LG)",
        "abstract": "High penetration of renewable energy sources (RES) introduces significant uncertainty and intermittency into microgrid operations, posing challenges to economic and reliable scheduling. To address this, this paper proposes an end-to-end decision-focused framework that jointly optimizes probabilistic forecasting and robust operation for microgrids. A multilayer encoder-decoder (MED) probabilistic forecasting model is integrated with a two-stage robust optimization (TSRO) model involving direct load control (DLC) through a differentiable decision pathway, enabling gradient-based feedback from operational outcomes to improve forecasting performance. Unlike conventional sequential approaches, the proposed method aligns forecasting accuracy with operational objectives by directly minimizing decision regret via a surrogate smart predict-then-optimize (SPO) loss function. This integration ensures that probabilistic forecasts are optimized for downstream decisions, enhancing both economic efficiency and robustness. Case studies on modified IEEE 33-bus and 69-bus systems demonstrate that the proposed framework achieves superior forecasting accuracy and operational performance, reducing total and net operation costs by up to 18% compared with conventional forecasting and optimization combinations. The results verify the effectiveness and scalability of the end-to-end decision-focused approach for resilient and cost-efficient microgrid management under uncertainty.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 179,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12818",
        "abs_url": "https://arxiv.org/abs/2512.12818",
        "pdf_url": "https://arxiv.org/pdf/2512.12818",
        "title": "Hindsight is 20/20: Building Agent Memory that Retains, Recalls, and Reflects",
        "authors": [
            "Chris Latimer",
            "Nicoló Boschi",
            "Andrew Neeser",
            "Chris Bartholomew",
            "Gaurav Srivastava",
            "Xuan Wang",
            "Naren Ramakrishnan"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "Agent memory has been touted as a dimension of growth for LLM-based applications, enabling agents that can accumulate experience, adapt across sessions, and move beyond single-shot question answering. The current generation of agent memory systems treats memory as an external layer that extracts salient snippets from conversations, stores them in vector or graph-based stores, and retrieves top-k items into the prompt of an otherwise stateless model. While these systems improve personalization and context carry-over, they still blur the line between evidence and inference, struggle to organize information over long horizons, and offer limited support for agents that must explain their reasoning. We present Hindsight, a memory architecture that treats agent memory as a structured, first-class substrate for reasoning by organizing it into four logical networks that distinguish world facts, agent experiences, synthesized entity summaries, and evolving beliefs. This framework supports three core operations -- retain, recall, and reflect -- that govern how information is added, accessed, and updated. Under this abstraction, a temporal, entity aware memory layer incrementally turns conversational streams into a structured, queryable memory bank, while a reflection layer reasons over this bank to produce answers and to update information in a traceable way. On key long-horizon conversational memory benchmarks like LongMemEval and LoCoMo, Hindsight with an open-source 20B model lifts overall accuracy from 39% to 83.6% over a full-context baseline with the same backbone and outperforms full context GPT-4o. Scaling the backbone further pushes Hindsight to 91.4% on LongMemEval and up to 89.61% on LoCoMo (vs. 75.78% for the strongest prior open system), consistently outperforming existing memory architectures on multi-session and open-domain questions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 180,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12847",
        "abs_url": "https://arxiv.org/abs/2512.12847",
        "pdf_url": "https://arxiv.org/pdf/2512.12847",
        "title": "HaShiFlex: A High-Throughput Hardened Shifter DNN Accelerator with Fine-Tuning Flexibility",
        "authors": [
            "Jonathan Herbst",
            "Michael Pellauer",
            "Sherief Reda"
        ],
        "comments": "12 pages, 6 figures, 5 tables",
        "subjects": "Hardware Architecture (cs.AR); Machine Learning (cs.LG)",
        "abstract": "We introduce a high-throughput neural network accelerator that embeds most network layers directly in hardware, minimizing data transfer and memory usage while preserving a degree of flexibility via a small neural processing unit for the final classification layer. By leveraging power-of-two (Po2) quantization for weights, we replace multiplications with simple rewiring, effectively reducing each convolution to a series of additions. This streamlined approach offers high-throughput, energy-efficient processing, making it highly suitable for applications where model parameters remain stable, such as continuous sensing tasks at the edge or large-scale data center deployments. Furthermore, by including a strategically chosen reprogrammable final layer, our design achieves high throughput without sacrificing fine-tuning capabilities. We implement this accelerator in a 7nm ASIC flow using MobileNetV2 as a baseline and report throughput, area, accuracy, and sensitivity to quantization and pruning - demonstrating both the advantages and potential trade-offs of the proposed architecture. We find that for MobileNetV2, we can improve inference throughput by 20x over fully programmable GPUs, processing 1.21 million images per second through a full forward pass while retaining fine-tuning flexibility. If absolutely no post-deployment fine tuning is required, this advantage increases to 67x at 4 million images per second.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 181,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12850",
        "abs_url": "https://arxiv.org/abs/2512.12850",
        "pdf_url": "https://arxiv.org/pdf/2512.12850",
        "title": "KANELÉ: Kolmogorov-Arnold Networks for Efficient LUT-based Evaluation",
        "authors": [
            "Duc Hoang",
            "Aarush Gupta",
            "Philip Harris"
        ],
        "comments": "International Symposium on Field-Programmable Gate Arrays 2026 (ISFPGA'2026)",
        "subjects": "Hardware Architecture (cs.AR); Machine Learning (cs.LG); Systems and Control (eess.SY); High Energy Physics - Experiment (hep-ex)",
        "abstract": "Low-latency, resource-efficient neural network inference on FPGAs is essential for applications demanding real-time capability and low power. Lookup table (LUT)-based neural networks are a common solution, combining strong representational power with efficient FPGA implementation. In this work, we introduce KANELÉ, a framework that exploits the unique properties of Kolmogorov-Arnold Networks (KANs) for FPGA deployment. Unlike traditional multilayer perceptrons (MLPs), KANs employ learnable one-dimensional splines with fixed domains as edge activations, a structure naturally suited to discretization and efficient LUT mapping. We present the first systematic design flow for implementing KANs on FPGAs, co-optimizing training with quantization and pruning to enable compact, high-throughput, and low-latency KAN architectures. Our results demonstrate up to a 2700x speedup and orders of magnitude resource savings compared to prior KAN-on-FPGA approaches. Moreover, KANELÉ matches or surpasses other LUT-based architectures on widely used benchmarks, particularly for tasks involving symbolic or physical formulas, while balancing resource usage across FPGA hardware. Finally, we showcase the versatility of the framework by extending it to real-time, power-efficient control systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 182,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12868",
        "abs_url": "https://arxiv.org/abs/2512.12868",
        "pdf_url": "https://arxiv.org/pdf/2512.12868",
        "title": "Counting Clues: A Lightweight Probabilistic Baseline Can Match an LLM",
        "authors": [
            "Furong Jia",
            "Yuan Pu",
            "Finn Guo",
            "Monica Agrawal"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Large language models (LLMs) excel on multiple-choice clinical diagnosis benchmarks, yet it is unclear how much of this performance reflects underlying probabilistic reasoning. We study this through questions from MedQA, where the task is to select the most likely diagnosis. We introduce the Frequency-Based Probabilistic Ranker (FBPR), a lightweight method that scores options with a smoothed Naive Bayes over concept-diagnosis co-occurrence statistics from a large corpus. When co-occurrence statistics were sourced from the pretraining corpora for OLMo and Llama, FBPR achieves comparable performance to the corresponding LLMs pretrained on that same corpus. Direct LLM inference and FBPR largely get different questions correct, with an overlap only slightly above random chance, indicating complementary strengths of each method. These findings highlight the continued value of explicit probabilistic baselines: they provide a meaningful performance reference point and a complementary signal for potential hybridization. While the performance of LLMs seems to be driven by a mechanism other than simple frequency aggregation, we show that an approach similar to the historically grounded, low-complexity expert systems still accounts for a substantial portion of benchmark performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 183,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12888",
        "abs_url": "https://arxiv.org/abs/2512.12888",
        "pdf_url": "https://arxiv.org/pdf/2512.12888",
        "title": "Meta-GPT: Decoding the Metasurface Genome with Generative Artificial Intelligence",
        "authors": [
            "David Dang",
            "Stuart Love",
            "Meena Salib",
            "Quynh Dang",
            "Samuel Rothfarb",
            "Mysk Alnatour",
            "Andrew Salij",
            "Hou-Tong Chen",
            "Ho Wai",
            "Wilton J.M. Kort-Kamp"
        ],
        "comments": "Keywords: Physics-informed machine learning; Transformer models; Reinforcement learning; Chain-of-thought reasoning; Metasurfaces; Nanophotonics; Inverse design",
        "subjects": "Optics (physics.optics); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Advancing artificial intelligence for physical sciences requires representations that are both interpretable and compatible with the underlying laws of nature. We introduce METASTRINGS, a symbolic language for photonics that expresses nanostructures as textual sequences encoding materials, geometries, and lattice configurations. Analogous to molecular textual representations in chemistry, METASTRINGS provides a framework connecting human interpretability with computational design by capturing the structural hierarchy of photonic metasurfaces. Building on this representation, we develop Meta-GPT, a foundation transformer model trained on METASTRINGS and finetuned with physics-informed supervised, reinforcement, and chain-of-thought learning. Across various design tasks, the model achieves <3% mean-squared spectral error and maintains >98% syntactic validity, generating diverse metasurface prototypes whose experimentally measured optical responses match their target spectra. These results demonstrate that Meta-GPT can learn the compositional rules of light-matter interactions through METASTRINGS, laying a rigorous foundation for AI-driven photonics and representing an important step toward a metasurface genome project.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 184,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12905",
        "abs_url": "https://arxiv.org/abs/2512.12905",
        "pdf_url": "https://arxiv.org/pdf/2512.12905",
        "title": "PAC-Bayes Bounds for Multivariate Linear Regression and Linear Autoencoders",
        "authors": [
            "Ruixin Guo",
            "Ruoming Jin",
            "Xinyu Li",
            "Yang Zhou"
        ],
        "comments": "Accepted at NeurIPS 2025 (this https URL)",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Linear Autoencoders (LAEs) have shown strong performance in state-of-the-art recommender systems. However, this success remains largely empirical, with limited theoretical understanding. In this paper, we investigate the generalizability -- a theoretical measure of model performance in statistical learning -- of multivariate linear regression and LAEs. We first propose a PAC-Bayes bound for multivariate linear regression, extending the earlier bound for single-output linear regression by Shalaeva et al., and establish sufficient conditions for its convergence. We then show that LAEs, when evaluated under a relaxed mean squared error, can be interpreted as constrained multivariate linear regression models on bounded data, to which our bound adapts. Furthermore, we develop theoretical methods to improve the computational efficiency of optimizing the LAE bound, enabling its practical evaluation on large models and real-world datasets. Experimental results demonstrate that our bound is tight and correlates well with practical ranking metrics such as Recall@K and NDCG@K.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 185,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12911",
        "abs_url": "https://arxiv.org/abs/2512.12911",
        "pdf_url": "https://arxiv.org/pdf/2512.12911",
        "title": "Evaluating Singular Value Thresholds for DNN Weight Matrices based on Random Matrix Theory",
        "authors": [
            "Kohei Nishikawa",
            "Koki Shimizu",
            "Hashiguchi Hiroki"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "This study evaluates thresholds for removing singular values from singular value decomposition-based low-rank approximations of deep neural network weight matrices. Each weight matrix is modeled as the sum of signal and noise matrices. The low-rank approximation is obtained by removing noise-related singular values using a threshold based on random matrix theory. To assess the adequacy of this threshold, we propose an evaluation metric based on the cosine similarity between the singular vectors of the signal and original weight matrices. The proposed metric is used in numerical experiments to compare two threshold estimation methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 186,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12939",
        "abs_url": "https://arxiv.org/abs/2512.12939",
        "pdf_url": "https://arxiv.org/pdf/2512.12939",
        "title": "Continuous Edit Distance, Geodesics and Barycenters of Time-varying Persistence Diagrams",
        "authors": [
            "Sebastien Tchitchek",
            "Mohamed Kissi",
            "Julien Tierny"
        ],
        "comments": "30 pages, 13 figures, 2 tables",
        "subjects": "Computational Geometry (cs.CG); Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR); Machine Learning (cs.LG)",
        "abstract": "We introduce the Continuous Edit Distance (CED), a geodesic and elastic distance for time-varying persistence diagrams (TVPDs). The CED extends edit-distance ideas to TVPDs by combining local substitution costs with penalized deletions/insertions, controlled by two parameters: \\(\\alpha\\) (trade-off between temporal misalignment and diagram discrepancy) and \\(\\beta\\) (gap penalty). We also provide an explicit construction of CED-geodesics. Building on these ingredients, we present two practical barycenter solvers, one stochastic and one greedy, that monotonically decrease the CED Frechet energy. Empirically, the CED is robust to additive perturbations (both temporal and spatial), recovers temporal shifts, and supports temporal pattern search. On real-life datasets, the CED achieves clustering performance comparable or better than standard elastic dissimilarities, while our clustering based on CED-barycenters yields superior classification results. Overall, the CED equips TVPD analysis with a principled distance, interpretable geodesics, and practical barycenters, enabling alignment, comparison, averaging, and clustering directly in the space of TVPDs. A C++ implementation is provided for reproducibility at the following address this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 187,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.12987",
        "abs_url": "https://arxiv.org/abs/2512.12987",
        "pdf_url": "https://arxiv.org/pdf/2512.12987",
        "title": "Tackling Snow-Induced Challenges: Safe Autonomous Lane-Keeping with Robust Reinforcement Learning",
        "authors": [
            "Amin Jalal Aghdasian",
            "Farzaneh Abdollahi",
            "Ali Kamali Iglie"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "This paper proposes two new algorithms for the lane keeping system (LKS) in autonomous vehicles (AVs) operating under snowy road conditions. These algorithms use deep reinforcement learning (DRL) to handle uncertainties and slippage. They include Action-Robust Recurrent Deep Deterministic Policy Gradient (AR-RDPG) and end-to-end Action-Robust convolutional neural network Attention Deterministic Policy Gradient (AR-CADPG), two action-robust approaches for decision-making. In the AR-RDPG method, within the perception layer, camera images are first denoised using multi-scale neural networks. Then, the centerline coefficients are extracted by a pre-trained deep convolutional neural network (DCNN). These coefficients, concatenated with the driving characteristics, are used as input to the control layer. The AR-CADPG method presents an end-to-end approach in which a convolutional neural network (CNN) and an attention mechanism are integrated within a DRL framework. Both methods are first trained in the CARLA simulator and validated under various snowy scenarios. Real-world experiments on a Jetson Nano-based autonomous vehicle confirm the feasibility and stability of the learned policies. Among the two models, the AR-CADPG approach demonstrates superior path-tracking accuracy and robustness, highlighting the effectiveness of combining temporal memory, adversarial resilience, and attention mechanisms in AVs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 188,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13003",
        "abs_url": "https://arxiv.org/abs/2512.13003",
        "pdf_url": "https://arxiv.org/pdf/2512.13003",
        "title": "General OOD Detection via Model-aware and Subspace-aware Variable Priority",
        "authors": [
            "Min Lu",
            "Hemant Ishwaran"
        ],
        "comments": "29 pages, 11 figures",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Out-of-distribution (OOD) detection is essential for determining when a supervised model encounters inputs that differ meaningfully from its training distribution. While widely studied in classification, OOD detection for regression and survival analysis remains limited due to the absence of discrete labels and the challenge of quantifying predictive uncertainty. We introduce a framework for OOD detection that is simultaneously model aware and subspace aware, and that embeds variable prioritization directly into the detection step. The method uses the fitted predictor to construct localized neighborhoods around each test case that emphasize the features driving the model's learned relationship and downweight directions that are less relevant to prediction. It produces OOD scores without relying on global distance metrics or estimating the full feature density. The framework is applicable across outcome types, and in our implementation we use random forests, where the rule structure yields transparent neighborhoods and effective scoring. Experiments on synthetic and real data benchmarks designed to isolate functional shifts show consistent improvements over existing methods. We further demonstrate the approach in an esophageal cancer survival study, where distribution shifts related to lymphadenectomy identify patterns relevant to surgical guidelines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 189,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13030",
        "abs_url": "https://arxiv.org/abs/2512.13030",
        "pdf_url": "https://arxiv.org/pdf/2512.13030",
        "title": "Motus: A Unified Latent Action World Model",
        "authors": [
            "Hongzhe Bi",
            "Hengkai Tan",
            "Shenghao Xie",
            "Zeyuan Wang",
            "Shuhe Huang",
            "Haitian Liu",
            "Ruowen Zhao",
            "Yao Feng",
            "Chendong Xiang",
            "Yinze Rong",
            "Hongyan Zhao",
            "Hanyu Liu",
            "Zhizhong Su",
            "Lei Ma",
            "Hang Su",
            "Jun Zhu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Robotics (cs.RO)",
        "abstract": "While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level \"delta action\" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 190,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13037",
        "abs_url": "https://arxiv.org/abs/2512.13037",
        "pdf_url": "https://arxiv.org/pdf/2512.13037",
        "title": "Progressive Refinement of E-commerce Search Ranking Based on Short-Term Activities of the Buyer",
        "authors": [
            "Taoran Sheng",
            "Sathappan Muthiah",
            "Atiq Islam",
            "Jinming Feng"
        ],
        "comments": "",
        "subjects": "Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "In e-commerce shopping, aligning search results with a buyer's immediate needs and preferences presents a significant challenge, particularly in adapting search results throughout the buyer's shopping journey as they move from the initial stages of browsing to making a purchase decision or shift from one intent to another. This study presents a systematic approach to adapting e-commerce search results based on the current context. We start with basic methods and incrementally incorporate more contextual information and state-of-the-art techniques to improve the search outcomes. By applying this evolving contextual framework to items displayed on the search engine results page (SERP), we progressively align search outcomes more closely with the buyer's interests and current search intentions. Our findings demonstrate that this incremental enhancement, from simple heuristic autoregressive features to advanced sequence models, significantly improves ranker performance. The integration of contextual techniques enhances the performance of our production ranker, leading to improved search results in both offline and online A/B testing in terms of Mean Reciprocal Rank (MRR). Overall, the paper details iterative methodologies and their substantial contributions to search result contextualization on e-commerce platforms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 191,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13093",
        "abs_url": "https://arxiv.org/abs/2512.13093",
        "pdf_url": "https://arxiv.org/pdf/2512.13093",
        "title": "PvP: Data-Efficient Humanoid Robot Learning with Proprioceptive-Privileged Contrastive Representations",
        "authors": [
            "Mingqi Yuan",
            "Tao Yu",
            "Haolin Song",
            "Bo Li",
            "Xin Jin",
            "Hua Chen",
            "Wenjun Zeng"
        ],
        "comments": "13 pages, 12 figures",
        "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
        "abstract": "Achieving efficient and robust whole-body control (WBC) is essential for enabling humanoid robots to perform complex tasks in dynamic environments. Despite the success of reinforcement learning (RL) in this domain, its sample inefficiency remains a significant challenge due to the intricate dynamics and partial observability of humanoid robots. To address this limitation, we propose PvP, a Proprioceptive-Privileged contrastive learning framework that leverages the intrinsic complementarity between proprioceptive and privileged states. PvP learns compact and task-relevant latent representations without requiring hand-crafted data augmentations, enabling faster and more stable policy learning. To support systematic evaluation, we develop SRL4Humanoid, the first unified and modular framework that provides high-quality implementations of representative state representation learning (SRL) methods for humanoid robot learning. Extensive experiments on the LimX Oli robot across velocity tracking and motion imitation tasks demonstrate that PvP significantly improves sample efficiency and final performance compared to baseline SRL methods. Our study further provides practical insights into integrating SRL with RL for humanoid WBC, offering valuable guidance for data-efficient humanoid robot learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 192,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13101",
        "abs_url": "https://arxiv.org/abs/2512.13101",
        "pdf_url": "https://arxiv.org/pdf/2512.13101",
        "title": "Harmonizing Generalization and Specialization: Uncertainty-Informed Collaborative Learning for Semi-supervised Medical Image Segmentation",
        "authors": [
            "Wenjing Lu",
            "Yi Hong",
            "Yang Yang"
        ],
        "comments": "This work has been submitted to the IEEE TMI for possible publication",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Vision foundation models have demonstrated strong generalization in medical image segmentation by leveraging large-scale, heterogeneous pretraining. However, they often struggle to generalize to specialized clinical tasks under limited annotations or rare pathological variations, due to a mismatch between general priors and task-specific requirements. To address this, we propose Uncertainty-informed Collaborative Learning (UnCoL), a dual-teacher framework that harmonizes generalization and specialization in semi-supervised medical image segmentation. Specifically, UnCoL distills both visual and semantic representations from a frozen foundation model to transfer general knowledge, while concurrently maintaining a progressively adapting teacher to capture fine-grained and task-specific representations. To balance guidance from both teachers, pseudo-label learning in UnCoL is adaptively regulated by predictive uncertainty, which selectively suppresses unreliable supervision and stabilizes learning in ambiguous regions. Experiments on diverse 2D and 3D segmentation benchmarks show that UnCoL consistently outperforms state-of-the-art semi-supervised methods and foundation model baselines. Moreover, our model delivers near fully supervised performance with markedly reduced annotation requirements.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 193,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13120",
        "abs_url": "https://arxiv.org/abs/2512.13120",
        "pdf_url": "https://arxiv.org/pdf/2512.13120",
        "title": "Towards Practical Large-scale Dynamical Heterogeneous Graph Embedding: Cold-start Resilient Recommendation",
        "authors": [
            "Mabiao Long",
            "Jiaxi Liu",
            "Yufeng Li",
            "Hao Xiong",
            "Junchi Yan",
            "Kefan Wang",
            "Yi Cao",
            "Jiandong Ding"
        ],
        "comments": "",
        "subjects": "Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "Deploying dynamic heterogeneous graph embeddings in production faces key challenges of scalability, data freshness, and cold-start. This paper introduces a practical, two-stage solution that balances deep graph representation with low-latency incremental updates. Our framework combines HetSGFormer, a scalable graph transformer for static learning, with Incremental Locally Linear Embedding (ILLE), a lightweight, CPU-based algorithm for real-time updates. HetSGFormer captures global structure with linear scalability, while ILLE provides rapid, targeted updates to incorporate new data, thus avoiding costly full retraining. This dual approach is cold-start resilient, leveraging the graph to create meaningful embeddings from sparse data. On billion-scale graphs, A/B tests show HetSGFormer achieved up to a 6.11% lift in Advertiser Value over previous methods, while the ILLE module added another 3.22% lift and improved embedding refresh timeliness by 83.2%. Our work provides a validated framework for deploying dynamic graph learning in production environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 194,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13123",
        "abs_url": "https://arxiv.org/abs/2512.13123",
        "pdf_url": "https://arxiv.org/pdf/2512.13123",
        "title": "Stopping Rules for Stochastic Gradient Descent via Anytime-Valid Confidence Sequences",
        "authors": [
            "Liviu Aolaritei",
            "Michael I. Jordan"
        ],
        "comments": "",
        "subjects": "Optimization and Control (math.OC); Machine Learning (cs.LG); Statistics Theory (math.ST); Machine Learning (stat.ML)",
        "abstract": "We study stopping rules for stochastic gradient descent (SGD) for convex optimization from the perspective of anytime-valid confidence sequences. Classical analyses of SGD provide convergence guarantees in expectation or at a fixed horizon, but offer no statistically valid way to assess, at an arbitrary time, how close the current iterate is to the optimum. We develop an anytime-valid, data-dependent upper confidence sequence for the weighted average suboptimality of projected SGD, constructed via nonnegative supermartingales and requiring no smoothness or strong convexity. This confidence sequence yields a simple stopping rule that is provably $\\varepsilon$-optimal with probability at least $1-\\alpha$ and is almost surely finite under standard stochastic approximation stepsizes. To the best of our knowledge, these are the first rigorous, time-uniform performance guarantees and finite-time $\\varepsilon$-optimality certificates for projected SGD with general convex objectives, based solely on observable trajectory quantities.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 195,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13170",
        "abs_url": "https://arxiv.org/abs/2512.13170",
        "pdf_url": "https://arxiv.org/pdf/2512.13170",
        "title": "Iterative Tuning of Nonlinear Model Predictive Control for Robotic Manufacturing Tasks",
        "authors": [
            "Deepak Ingole",
            "Valentin Bhend",
            "Shiva Ganesh Murali",
            "Oliver Dobrich",
            "Alisa Rupenayan"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Machine Learning (cs.LG); Systems and Control (eess.SY); Optimization and Control (math.OC)",
        "abstract": "Manufacturing processes are often perturbed by drifts in the environment and wear in the system, requiring control re-tuning even in the presence of repetitive operations. This paper presents an iterative learning framework for automatic tuning of Nonlinear Model Predictive Control (NMPC) weighting matrices based on task-level performance feedback. Inspired by norm-optimal Iterative Learning Control (ILC), the proposed method adaptively adjusts NMPC weights Q and R across task repetitions to minimize key performance indicators (KPIs) related to tracking accuracy, control effort, and saturation. Unlike gradient-based approaches that require differentiating through the NMPC solver, we construct an empirical sensitivity matrix, enabling structured weight updates without analytic derivatives. The framework is validated through simulation on a UR10e robot performing carbon fiber winding on a tetrahedral core. Results demonstrate that the proposed approach converges to near-optimal tracking performance (RMSE within 0.3% of offline Bayesian Optimization (BO)) in just 4 online repetitions, compared to 100 offline evaluations required by BO algorithm. The method offers a practical solution for adaptive NMPC tuning in repetitive robotic tasks, combining the precision of carefully optimized controllers with the flexibility of online adaptation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 196,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13197",
        "abs_url": "https://arxiv.org/abs/2512.13197",
        "pdf_url": "https://arxiv.org/pdf/2512.13197",
        "title": "MicroPhaseNO: Adapting an Earthquake-Trained Phase Neural Operator for Microseismic Phase Picking",
        "authors": [
            "Ayrat Abdullin",
            "Umair bin Waheed",
            "Leo Eisner",
            "Naveed Iqbal"
        ],
        "comments": "Submitted to Pure and Applied Geophysics",
        "subjects": "Geophysics (physics.geo-ph); Machine Learning (cs.LG)",
        "abstract": "Seismic phase picking is very often used for microseismic monitoring and subsurface imaging. Traditional manual processing is not feasible for either real-time applications or large arrays. Deep learning-based pickers trained on large earthquake catalogs offer an automated alternative. However, they are typically optimized for high signal-to-noise, long-duration networks and struggle with the challenges presented by microseismic datasets, which are purpose-built for limited time without previously detected seismicity. In this study, we demonstrate how a network-wide earthquake phase picker, the Phase Neural Operator (PhaseNO), can be adapted to microseismic monitoring using transfer learning. Starting from a PhaseNO model pre-trained on more than 57,000 three-component earthquake and noise records, we fine-tune the model using only 200 labeled and noise seismograms from induced events in hydraulic-fracturing settings. The fine-tuned model thus preserves the rich spatio-temporal representation learned from abundant earthquake data, while adapting to the characteristics and labeling conventions of microseismic phases, which are often picked on peaks or troughs rather than onsets. We evaluate performance on three distinct real-world microseismic datasets with different network geometries and acquisition parameters. Compared to the original PhaseNO and a conventional workflow, the adapted model increases F1 score and accuracy by up to 30%, and strongly reduces systematic timing bias and pick uncertainty. Because the adaptation relies on a small, campaign-specific calibration set, the approach is readily transferable to other microseismic tasks where public earthquake data and pre-trained models are accessible. The associated code will be released openly at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 197,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13217",
        "abs_url": "https://arxiv.org/abs/2512.13217",
        "pdf_url": "https://arxiv.org/pdf/2512.13217",
        "title": "Rethinking Physics-Informed Regression Beyond Training Loops and Bespoke Architectures",
        "authors": [
            "Lorenzo Sabug Jr.",
            "Eric Kerrigan"
        ],
        "comments": "",
        "subjects": "Optimization and Control (math.OC); Machine Learning (cs.LG)",
        "abstract": "We revisit the problem of physics-informed regression, and propose a method that directly computes the state at the prediction point, simultaneously with the derivative and curvature information of the existing samples. We frame each prediction as a constrained optimisation problem, leveraging multivariate Taylor series expansions and explicitly enforcing physical laws. Each individual query can be processed with low computational cost without any pre- or re-training, in contrast to global function approximator-based solutions such as neural networks. Our comparative benchmarks on a reaction-diffusion system show competitive predictive accuracy relative to a neural network-based solution, while completely eliminating the need for long training loops, and remaining robust to changes in the sampling layout.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 198,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13227",
        "abs_url": "https://arxiv.org/abs/2512.13227",
        "pdf_url": "https://arxiv.org/pdf/2512.13227",
        "title": "Better LMO-based Momentum Methods with Second-Order Information",
        "authors": [
            "Sarit Khirirat",
            "Abdurakhmon Sadiev",
            "Yury Demidovich",
            "Peter Richtárik"
        ],
        "comments": "",
        "subjects": "Optimization and Control (math.OC); Machine Learning (cs.LG)",
        "abstract": "The use of momentum in stochastic optimization algorithms has shown empirical success across a range of machine learning tasks. Recently, a new class of stochastic momentum algorithms has emerged within the Linear Minimization Oracle (LMO) framework--leading to state-of-the-art methods, such as Muon, Scion, and Gluon, that effectively solve deep neural network training problems. However, traditional stochastic momentum methods offer convergence guarantees no better than the ${O}(1/K^{1/4})$ rate. While several approaches--such as Hessian-Corrected Momentum (HCM)--have aimed to improve this rate, their theoretical results are generally restricted to the Euclidean norm setting. This limitation hinders their applicability in problems, where arbitrary norms are often required. In this paper, we extend the LMO-based framework by integrating HCM, and provide convergence guarantees under relaxed smoothness and arbitrary norm settings. We establish improved convergence rates of ${O}(1/K^{1/3})$ for HCM, which can adapt to the geometry of the problem and achieve a faster rate than traditional momentum. Experimental results on training Multi-Layer Perceptrons (MLPs) and Long Short-Term Memory (LSTM) networks verify our theoretical observations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 199,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13278",
        "abs_url": "https://arxiv.org/abs/2512.13278",
        "pdf_url": "https://arxiv.org/pdf/2512.13278",
        "title": "AutoTool: Dynamic Tool Selection and Integration for Agentic Reasoning",
        "authors": [
            "Jiaru Zou",
            "Ling Yang",
            "Yunzhe Qi",
            "Sirui Chen",
            "Mengting Ai",
            "Ke Shen",
            "Jingrui He",
            "Mengdi Wang"
        ],
        "comments": "Best Paper Award at ICCV 2025 Workshop on Multi-Modal Reasoning for Agentic Intelligence",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Agentic reinforcement learning has advanced large language models (LLMs) to reason through long chain-of-thought trajectories while interleaving external tool use. Existing approaches assume a fixed inventory of tools, limiting LLM agents' adaptability to new or evolving toolsets. We present AutoTool, a framework that equips LLM agents with dynamic tool-selection capabilities throughout their reasoning trajectories. We first construct a 200k dataset with explicit tool-selection rationales across 1,000+ tools and 100+ tasks spanning mathematics, science, code generation, and multimodal reasoning. Building on this data foundation, AutoTool employs a dual-phase optimization pipeline: (i) supervised and RL-based trajectory stabilization for coherent reasoning, and (ii) KL-regularized Plackett-Luce ranking to refine consistent multi-step tool selection. Across ten diverse benchmarks, we train two base models, Qwen3-8B and Qwen2.5-VL-7B, with AutoTool. With fewer parameters, AutoTool consistently outperforms advanced LLM agents and tool-integration methods, yielding average gains of 6.4% in math & science reasoning, 4.5% in search-based QA, 7.7% in code generation, and 6.9% in multimodal understanding. In addition, AutoTool exhibits stronger generalization by dynamically leveraging unseen tools from evolving toolsets during inference.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 200,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13297",
        "abs_url": "https://arxiv.org/abs/2512.13297",
        "pdf_url": "https://arxiv.org/pdf/2512.13297",
        "title": "MedInsightBench: Evaluating Medical Analytics Agents Through Multi-Step Insight Discovery in Multimodal Medical Data",
        "authors": [
            "Zhenghao Zhu",
            "Chuxue Cao",
            "Sirui Han",
            "Yuanfeng Song",
            "Xing Chen",
            "Caleb Chen Cao",
            "Yike Guo"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "In medical data analysis, extracting deep insights from complex, multi-modal datasets is essential for improving patient care, increasing diagnostic accuracy, and optimizing healthcare operations. However, there is currently a lack of high-quality datasets specifically designed to evaluate the ability of large multi-modal models (LMMs) to discover medical insights. In this paper, we introduce MedInsightBench, the first benchmark that comprises 332 carefully curated medical cases, each annotated with thoughtfully designed insights. This benchmark is intended to evaluate the ability of LMMs and agent frameworks to analyze multi-modal medical image data, including posing relevant questions, interpreting complex findings, and synthesizing actionable insights and recommendations. Our analysis indicates that existing LMMs exhibit limited performance on MedInsightBench, which is primarily attributed to their challenges in extracting multi-step, deep insights and the absence of medical expertise. Therefore, we propose MedInsightAgent, an automated agent framework for medical data analysis, composed of three modules: Visual Root Finder, Analytical Insight Agent, and Follow-up Question Composer. Experiments on MedInsightBench highlight pervasive challenges and demonstrate that MedInsightAgent can improve the performance of general LMMs in medical data insight discovery.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 201,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13323",
        "abs_url": "https://arxiv.org/abs/2512.13323",
        "pdf_url": "https://arxiv.org/pdf/2512.13323",
        "title": "Error-Driven Prompt Optimization for Arithmetic Reasoning",
        "authors": [
            "Árpád Pándy",
            "Róbert Lakatos",
            "András Hajdu"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Recent advancements in artificial intelligence have sparked interest in industrial agents capable of supporting analysts in regulated sectors, such as finance and healthcare, within tabular data workflows. A key capability for such systems is performing accurate arithmetic operations on structured data while ensuring sensitive information never leaves secure, on-premises environments. Here, we introduce an error-driven optimization framework for arithmetic reasoning that enhances a Code Generation Agent (CGA), specifically applied to on-premises small language models (SLMs). Through a systematic evaluation of a leading SLM (Qwen3 4B), we find that while the base model exhibits fundamental limitations in arithmetic tasks, our proposed error-driven method, which clusters erroneous predictions to refine prompt-rules iteratively, dramatically improves performance, elevating the model's accuracy to 70.8\\%. Our results suggest that developing reliable, interpretable, and industrially deployable AI assistants can be achieved not only through costly fine-tuning but also via systematic, error-driven prompt optimization, enabling small models to surpass larger language models (GPT-3.5 Turbo) in a privacy-compliant manner.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 202,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13325",
        "abs_url": "https://arxiv.org/abs/2512.13325",
        "pdf_url": "https://arxiv.org/pdf/2512.13325",
        "title": "Security and Detectability Analysis of Unicode Text Watermarking Methods Against Large Language Models",
        "authors": [
            "Malte Hellmeier"
        ],
        "comments": "Accepted for publication at the ICISSP 2026",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Securing digital text is becoming increasingly relevant due to the widespread use of large language models. Individuals' fear of losing control over data when it is being used to train such machine learning models or when distinguishing model-generated output from text written by humans. Digital watermarking provides additional protection by embedding an invisible watermark within the data that requires protection. However, little work has been taken to analyze and verify if existing digital text watermarking methods are secure and undetectable by large language models. In this paper, we investigate the security-related area of watermarking and machine learning models for text data. In a controlled testbed of three experiments, ten existing Unicode text watermarking methods were implemented and analyzed across six large language models: GPT-5, GPT-4o, Teuken 7B, Llama 3.3, Claude Sonnet 4, and Gemini 2.5 Pro. The findings of our experiments indicate that, especially the latest reasoning models, can detect a watermarked text. Nevertheless, all models fail to extract the watermark unless implementation details in the form of source code are provided. We discuss the implications for security researchers and practitioners and outline future research opportunities to address security concerns.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 203,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13359",
        "abs_url": "https://arxiv.org/abs/2512.13359",
        "pdf_url": "https://arxiv.org/pdf/2512.13359",
        "title": "Fast Policy Learning for 6-DOF Position Control of Underwater Vehicles",
        "authors": [
            "Sümer Tunçay",
            "Alain Andres",
            "Ignacio Carlucho"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
        "abstract": "Autonomous Underwater Vehicles (AUVs) require reliable six-degree-of-freedom (6-DOF) position control to operate effectively in complex and dynamic marine environments. Traditional controllers are effective under nominal conditions but exhibit degraded performance when faced with unmodeled dynamics or environmental disturbances. Reinforcement learning (RL) provides a powerful alternative but training is typically slow and sim-to-real transfer remains challenging. This work introduces a GPU-accelerated RL training pipeline built in JAX and MuJoCo-XLA (MJX). By jointly JIT-compiling large-scale parallel physics simulation and learning updates, we achieve training times of under two this http URL systematic evaluation of multiple RL algorithms, we show robust 6-DOF trajectory tracking and effective disturbance rejection in real underwater experiments, with policies transferred zero-shot from simulation. Our results provide the first explicit real-world demonstration of RL-based AUV position control across all six degrees of freedom.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 204,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13427",
        "abs_url": "https://arxiv.org/abs/2512.13427",
        "pdf_url": "https://arxiv.org/pdf/2512.13427",
        "title": "MineTheGap: Automatic Mining of Biases in Text-to-Image Models",
        "authors": [
            "Noa Cohen",
            "Nurit Spingarn-Eliezer",
            "Inbar Huberman-Spiegelglas",
            "Tomer Michaeli"
        ],
        "comments": "Code and examples are available on the project's webpage at this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Text-to-Image (TTI) models generate images based on text prompts, which often leave certain aspects of the desired image ambiguous. When faced with these ambiguities, TTI models have been shown to exhibit biases in their interpretations. These biases can have societal impacts, e.g., when showing only a certain race for a stated occupation. They can also affect user experience when creating redundancy within a set of generated images instead of spanning diverse possibilities. Here, we introduce MineTheGap - a method for automatically mining prompts that cause a TTI model to generate biased outputs. Our method goes beyond merely detecting bias for a given prompt. Rather, it leverages a genetic algorithm to iteratively refine a pool of prompts, seeking for those that expose biases. This optimization process is driven by a novel bias score, which ranks biases according to their severity, as we validate on a dataset with known biases. For a given prompt, this score is obtained by comparing the distribution of generated images to the distribution of LLM-generated texts that constitute variations on the prompt. Code and examples are available on the project's webpage.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 205,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13482",
        "abs_url": "https://arxiv.org/abs/2512.13482",
        "pdf_url": "https://arxiv.org/pdf/2512.13482",
        "title": "Real-Time AI-Driven Milling Digital Twin Towards Extreme Low-Latency",
        "authors": [
            "Wenyi Liu",
            "R. Sharma",
            "W. \"Grace\" Guo",
            "J. Yi",
            "Y.B. Guo"
        ],
        "comments": "",
        "subjects": "Systems and Control (eess.SY); Machine Learning (cs.LG)",
        "abstract": "Digital twin (DT) enables smart manufacturing by leveraging real-time data, AI models, and intelligent control systems. This paper presents a state-of-the-art analysis on the emerging field of DTs in the context of milling. The critical aspects of DT are explored through the lens of virtual models of physical milling, data flow from physical milling to virtual model, and feedback from virtual model to physical milling. Live data streaming protocols and virtual modeling methods are highlighted. A case study showcases the transformative capability of a real-time machine learning-driven live DT of tool-work contact in a milling process. Future research directions are outlined to achieve the goals of Industry 4.0 and beyond.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 206,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13491",
        "abs_url": "https://arxiv.org/abs/2512.13491",
        "pdf_url": "https://arxiv.org/pdf/2512.13491",
        "title": "From Zipf's Law to Neural Scaling through Heaps' Law and Hilberg's Hypothesis",
        "authors": [
            "Łukasz Dębowski"
        ],
        "comments": "32 pages, no figures",
        "subjects": "Information Theory (cs.IT); Machine Learning (cs.LG); Statistics Theory (math.ST)",
        "abstract": "We inspect the deductive connection between the neural scaling law and Zipf's law -- two statements discussed in machine learning and quantitative linguistics. The neural scaling law describes how the cross entropy rate of a foundation model -- such as a large language model -- changes with respect to the amount of training tokens, parameters, and compute. By contrast, Zipf's law posits that the distribution of tokens exhibits a power law tail. Whereas similar claims have been made in more specific settings, we show that the neural scaling law is a consequence of Zipf's law under certain broad assumptions that we reveal systematically. The derivation steps are as follows: We derive Heaps' law on the vocabulary growth from Zipf's law, Hilberg's hypothesis on the entropy scaling from Heaps' law, and the neural scaling from Hilberg's hypothesis. We illustrate these inference steps by a toy example of the Santa Fe process that satisfies all the four statistical laws.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 207,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13517",
        "abs_url": "https://arxiv.org/abs/2512.13517",
        "pdf_url": "https://arxiv.org/pdf/2512.13517",
        "title": "A Deep Learning Model of Mental Rotation Informed by Interactive VR Experiments",
        "authors": [
            "Raymond Khazoum",
            "Daniela Fernandes",
            "Aleksandr Krylov",
            "Qin Li",
            "Stephane Deny"
        ],
        "comments": "",
        "subjects": "Neurons and Cognition (q-bio.NC); Machine Learning (cs.LG)",
        "abstract": "Mental rotation -- the ability to compare objects seen from different viewpoints -- is a fundamental example of mental simulation and spatial world modelling in humans. Here we propose a mechanistic model of human mental rotation, leveraging advances in deep, equivariant, and neuro-symbolic learning. Our model consists of three stacked components: (1) an equivariant neural encoder, taking images as input and producing 3D spatial representations of objects, (2) a neuro-symbolic object encoder, deriving symbolic descriptions of objects from these spatial representations, and (3) a neural decision agent, comparing these symbolic descriptions to prescribe rotation simulations in 3D latent space via a recurrent pathway. Our model design is guided by the abundant experimental literature on mental rotation, which we complemented with experiments in VR where participants could at times manipulate the objects to compare, providing us with additional insights into the cognitive process of mental rotation. Our model captures well the performance, response times and behavior of participants in our and others' experiments. The necessity of each model component is shown through systematic ablations. Our work adds to a recent collection of deep neural models of human spatial reasoning, further demonstrating the potency of integrating deep, equivariant, and symbolic representations to model the human mind.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 208,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13529",
        "abs_url": "https://arxiv.org/abs/2512.13529",
        "pdf_url": "https://arxiv.org/pdf/2512.13529",
        "title": "Enhancing lithological interpretation from petrophysical well log of IODP expedition 390/393 using machine learning",
        "authors": [
            "Raj Sahu",
            "Saumen Maiti"
        ],
        "comments": "Accepted for presentation at the International Meeting for Applied Geoscience & Energy (IMAGE) 2025",
        "subjects": "Geophysics (physics.geo-ph); Machine Learning (cs.LG)",
        "abstract": "Enhanced lithological interpretation from well logs plays a key role in geological resource exploration and mapping, as well as in geo-environmental modeling studies. Core and cutting information is useful for making sound interpretations of well logs; however, these are rarely collected at each depth due to high costs. Moreover, well log interpretation using traditional methods is constrained by poor borehole conditions. Traditional statistical methods are mostly linear, often failing to discriminate between lithology and rock facies, particularly when dealing with overlapping well log signals characterized by the structural and compositional variation of rock types. In this study, we develop multiple supervised and unsupervised machine learning algorithms to jointly analyze multivariate well log data from Integrated Ocean Drilling Program (IODP) expeditions 390 and 393 for enhanced lithological interpretations. Among the algorithms, Logistic Regression, Decision Trees, Gradient Boosting, Support Vector Machines (SVM), k-Nearest Neighbors (KNN), and Multi-Layer Perceptron (MLP) neural network models, the Decision Tree and Gradient Boosting models outperformed the others, achieving an accuracy of 0.9950 and an F1-score of 0.9951. While unsupervised machine learning (ML) provides the foundation for cluster information that inherently supports the classification algorithm, supervised ML is applied to devise a data-driven lithology clustering mechanism for IODP datasets. The joint ML-based method developed here has the potential to be further explored for analyzing other well log datasets from the world's oceans.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 209,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13530",
        "abs_url": "https://arxiv.org/abs/2512.13530",
        "pdf_url": "https://arxiv.org/pdf/2512.13530",
        "title": "Actively Learning Joint Contours of Multiple Computer Experiments",
        "authors": [
            "Shih-Ni Prim",
            "Kevin R. Quinlan",
            "Paul Hawkins",
            "Jagadeesh Movva",
            "Annie S. Booth"
        ],
        "comments": "",
        "subjects": "Methodology (stat.ME); Machine Learning (cs.LG)",
        "abstract": "Contour location$\\unicode{x2014}$the process of sequentially training a surrogate model to identify the design inputs that result in a pre-specified response value from a single computer experiment$\\unicode{x2014}$is a well-studied active learning problem. Here, we tackle a related but distinct problem: identifying the input configuration that returns pre-specified values of multiple independent computer experiments simultaneously. Motivated by computer experiments of the rotational torques acting upon a vehicle in flight, we aim to identify stable flight conditions which result in zero torque forces. We propose a \"joint contour location\" (jCL) scheme that strikes a strategic balance between exploring the multiple response surfaces while exploiting learning of the intersecting contours. We employ both shallow and deep Gaussian process surrogates, but our jCL procedure is applicable to any surrogate that can provide posterior predictive distributions. Our jCL designs significantly outperform existing (single response) CL strategies, enabling us to efficiently locate the joint contour of our motivating computer experiments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 210,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13532",
        "abs_url": "https://arxiv.org/abs/2512.13532",
        "pdf_url": "https://arxiv.org/pdf/2512.13532",
        "title": "Adaptive Sampling for Hydrodynamic Stability",
        "authors": [
            "Anshima Singh",
            "David J. Silvester"
        ],
        "comments": "",
        "subjects": "Fluid Dynamics (physics.flu-dyn); Machine Learning (cs.LG)",
        "abstract": "An adaptive sampling approach for efficient detection of bifurcation boundaries in parametrized fluid flow problems is presented herein. The study extends the machine-learning approach of Silvester (Machine Learning for Hydrodynamic Stability, arXiv:2407.09572), where a classifier network was trained on preselected simulation data to identify bifurcated and nonbifurcated flow regimes. In contrast, the proposed methodology introduces adaptivity through a flow-based deep generative model that automatically refines the sampling of the parameter space. The strategy has two components: a classifier network maps the flow parameters to a bifurcation probability, and a probability density estimation technique (KRnet) for the generation of new samples at each adaptive step. The classifier output provides a probabilistic measure of flow stability, and the Shannon entropy of these predictions is employed as an uncertainty indicator. KRnet is trained to approximate a probability density function that concentrates sampling in regions of high entropy, thereby directing computational effort towards the evolving bifurcation boundary. This coupling between classification and generative modeling establishes a feedback-driven adaptive learning process analogous to error-indicator based refinement in contemporary partial differential equation solution strategies. Starting from a uniform parameter distribution, the new approach achieves accurate bifurcation boundary identification with significantly fewer Navier--Stokes simulations, providing a scalable foundation for high-dimensional stability analysis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 211,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13565",
        "abs_url": "https://arxiv.org/abs/2512.13565",
        "pdf_url": "https://arxiv.org/pdf/2512.13565",
        "title": "A Nonparametric Statistics Approach to Feature Selection in Deep Neural Networks with Theoretical Guarantees",
        "authors": [
            "Junye Du",
            "Zhenghao Li",
            "Zhutong Gu",
            "Long Feng"
        ],
        "comments": "64 pages",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "This paper tackles the problem of feature selection in a highly challenging setting: $\\mathbb{E}(y | \\boldsymbol{x}) = G(\\boldsymbol{x}_{\\mathcal{S}_0})$, where $\\mathcal{S}_0$ is the set of relevant features and $G$ is an unknown, potentially nonlinear function subject to mild smoothness conditions. Our approach begins with feature selection in deep neural networks, then generalizes the results to H{ö}lder smooth functions by exploiting the strong approximation capabilities of neural networks. Unlike conventional optimization-based deep learning methods, we reformulate neural networks as index models and estimate $\\mathcal{S}_0$ using the second-order Stein's formula. This gradient-descent-free strategy guarantees feature selection consistency with a sample size requirement of $n = \\Omega(p^2)$, where $p$ is the feature dimension. To handle high-dimensional scenarios, we further introduce a screening-and-selection mechanism that achieves nonlinear selection consistency when $n = \\Omega(s \\log p)$, with $s$ representing the sparsity level. Additionally, we refit a neural network on the selected features for prediction and establish performance guarantees under a relaxed sparsity assumption. Extensive simulations and real-data analyses demonstrate the strong performance of our method even in the presence of complex feature interactions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 212,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13598",
        "abs_url": "https://arxiv.org/abs/2512.13598",
        "pdf_url": "https://arxiv.org/pdf/2512.13598",
        "title": "Textual Gradients are a Flawed Metaphor for Automatic Prompt Optimization",
        "authors": [
            "Daniel Melcer",
            "Qi Chen",
            "Wen-Hao Chiang",
            "Shweta Garg",
            "Pranav Garg",
            "Christian Bock"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "A well-engineered prompt can increase the performance of large language models; automatic prompt optimization techniques aim to increase performance without requiring human effort to tune the prompts. One leading class of prompt optimization techniques introduces the analogy of textual gradients. We investigate the behavior of these textual gradient methods through a series of experiments and case studies. While such methods often result in a performance improvement, our experiments suggest that the gradient analogy does not accurately explain their behavior. Our insights may inform the selection of prompt optimization strategies, and development of new approaches.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 213,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13607",
        "abs_url": "https://arxiv.org/abs/2512.13607",
        "pdf_url": "https://arxiv.org/pdf/2512.13607",
        "title": "Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models",
        "authors": [
            "Boxin Wang",
            "Chankyu Lee",
            "Nayeon Lee",
            "Sheng-Chieh Lin",
            "Wenliang Dai",
            "Yang Chen",
            "Yangyi Chen",
            "Zhuolin Yang",
            "Zihan Liu",
            "Mohammad Shoeybi",
            "Bryan Catanzaro",
            "Wei Ping"
        ],
        "comments": "We publicly release the Nemotron-Cascade models and the full collection of training data at: this https URL",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Building general-purpose reasoning models with reinforcement learning (RL) entails substantial cross-domain heterogeneity, including large variation in inference-time response lengths and verification latency. Such variability complicates the RL infrastructure, slows training, and makes training curriculum (e.g., response length extension) and hyperparameter selection challenging. In this work, we propose cascaded domain-wise reinforcement learning (Cascade RL) to develop general-purpose reasoning models, Nemotron-Cascade, capable of operating in both instruct and deep thinking modes. Departing from conventional approaches that blend heterogeneous prompts from different domains, Cascade RL orchestrates sequential, domain-wise RL, reducing engineering complexity and delivering state-of-the-art performance across a wide range of benchmarks. Notably, RLHF for alignment, when used as a pre-step, boosts the model's reasoning ability far beyond mere preference optimization, and subsequent domain-wise RLVR stages rarely degrade the benchmark performance attained in earlier domains and may even improve it (see an illustration in Figure 1). Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro and achieves silver-medal performance in the 2025 International Olympiad in Informatics (IOI). We transparently share our training and data recipes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 214,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13618",
        "abs_url": "https://arxiv.org/abs/2512.13618",
        "pdf_url": "https://arxiv.org/pdf/2512.13618",
        "title": "Temporal Tokenization Strategies for Event Sequence Modeling with Large Language Models",
        "authors": [
            "Zefang Liu",
            "Nam Nguyen",
            "Yinzhu Quan",
            "Austin Zhang"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Representing continuous time is a critical and under-explored challenge in modeling temporal event sequences with large language models (LLMs). Various strategies like byte-level representations or calendar tokens have been proposed. However, the optimal approach remains unclear, especially given the diverse statistical distributions of real-world event data, which range from smooth log-normal to discrete, spiky patterns. This paper presents the first empirical study of temporal tokenization for event sequences, comparing distinct encoding strategies: naive numeric strings, high-precision byte-level representations, human-semantic calendar tokens, classic uniform binning, and adaptive residual scalar quantization. We evaluate these strategies by fine-tuning LLMs on real-world datasets that exemplify these diverse distributions. Our analysis reveals that no single strategy is universally superior; instead, prediction performance depends heavily on aligning the tokenizer with the data's statistical properties, with log-based strategies excelling on skewed distributions and human-centric formats proving robust for mixed modalities.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 215,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13634",
        "abs_url": "https://arxiv.org/abs/2512.13634",
        "pdf_url": "https://arxiv.org/pdf/2512.13634",
        "title": "Universality of high-dimensional scaling limits of stochastic gradient descent",
        "authors": [
            "Reza Gheissari",
            "Aukosh Jagannath"
        ],
        "comments": "30 pages",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Probability (math.PR); Statistics Theory (math.ST)",
        "abstract": "We consider statistical tasks in high dimensions whose loss depends on the data only through its projection into a fixed-dimensional subspace spanned by the parameter vectors and certain ground truth vectors. This includes classifying mixture distributions with cross-entropy loss with one and two-layer networks, and learning single and multi-index models with one and two-layer networks. When the data is drawn from an isotropic Gaussian mixture distribution, it is known that the evolution of a finite family of summary statistics under stochastic gradient descent converges to an autonomous ordinary differential equation (ODE), as the dimension and sample size go to $\\infty$ and the step size goes to $0$ commensurately. Our main result is that these ODE limits are universal in that this convergence occurs even when the data is drawn from mixtures of product measures provided the first two moments match the corresponding Gaussian distribution and the initialization and ground truth vectors are sufficiently coordinate-delocalized. We complement this by proving two corresponding non-universality results. We provide a simple example where the ODE limits are non-universal if the initialization is coordinate aligned. We also show that the stochastic differential equation limits arising as fluctuations of the summary statistics around their ODE's fixed points are not universal.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 216,
        "date": "2025-12-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-16?abs=True",
        "arxiv_id": "2512.13666",
        "abs_url": "https://arxiv.org/abs/2512.13666",
        "pdf_url": "https://arxiv.org/pdf/2512.13666",
        "title": "SEDULity: A Proof-of-Learning Framework for Distributed and Secure Blockchains with Efficient Useful Work",
        "authors": [
            "Weihang Cao",
            "Mustafa Doger",
            "Sennur Ulukus"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Distributed, Parallel, and Cluster Computing (cs.DC); Information Theory (cs.IT); Machine Learning (cs.LG)",
        "abstract": "The security and decentralization of Proof-of-Work (PoW) have been well-tested in existing blockchain systems. However, its tremendous energy waste has raised concerns about sustainability. Proof-of-Useful-Work (PoUW) aims to redirect the meaningless computation to meaningful tasks such as solving machine learning (ML) problems, giving rise to the branch of Proof-of-Learning (PoL). While previous studies have proposed various PoLs, they all, to some degree, suffer from security, decentralization, or efficiency issues. In this paper, we propose a PoL framework that trains ML models efficiently while maintaining blockchain security in a fully distributed manner. We name the framework SEDULity, which stands for a Secure, Efficient, Distributed, and Useful Learning-based blockchain system. Specifically, we encode the template block into the training process and design a useful function that is difficult to solve but relatively easy to verify, as a substitute for the PoW puzzle. We show that our framework is distributed, secure, and efficiently trains ML models. We further demonstrate that the proposed PoL framework can be extended to other types of useful work and design an incentive mechanism to incentivize task verification. We show theoretically that a rational miner is incentivized to train fully honestly with well-designed system parameters. Finally, we present simulation results to demonstrate the performance of our framework and validate our analysis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    }
]