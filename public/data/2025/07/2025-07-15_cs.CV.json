[
    {
        "order": 1,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.08831",
        "abs_url": "https://arxiv.org/abs/2507.08831",
        "pdf_url": "https://arxiv.org/pdf/2507.08831",
        "title": "View Invariant Learning for Vision-Language Navigation in Continuous Environments",
        "authors": [
            "Josh Qixuan Sun",
            "Xiaoying Xing",
            "Huaiyuan Weng",
            "Chul Min Yeum",
            "Mark Crowley"
        ],
        "comments": "Under review",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Robotics (cs.RO)",
        "abstract": "Vision-Language Navigation in Continuous Environments (VLNCE), where an agent follows instructions and moves freely to reach a destination, is a key research problem in embodied AI. However, most navigation policies are sensitive to viewpoint changes, i.e., variations in camera height and viewing angle that alter the agent's observation. In this paper, we introduce a generalized scenario, V2-VLNCE (VLNCE with Varied Viewpoints), and propose VIL (View Invariant Learning), a view-invariant post-training strategy that enhances the robustness of existing navigation policies to changes in camera viewpoint. VIL employs a contrastive learning framework to learn sparse and view-invariant features. Additionally, we introduce a teacher-student framework for the Waypoint Predictor Module, a core component of most VLNCE baselines, where a view-dependent teacher model distills knowledge into a view-invariant student model. We employ an end-to-end training paradigm to jointly optimize these components, thus eliminating the cost for individual module training. Empirical results show that our method outperforms state-of-the-art approaches on V2-VLNCE by 8-15% measured on Success Rate for two standard benchmark datasets R2R-CE and RxR-CE. Furthermore, we evaluate VIL under the standard VLNCE setting and find that, despite being trained for varied viewpoints, it often still improves performance. On the more challenging RxR-CE dataset, our method also achieved state-of-the-art performance across all metrics when compared to other map-free methods. This suggests that adding VIL does not diminish the standard viewpoint performance and can serve as a plug-and-play post-training method.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 2,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.08917",
        "abs_url": "https://arxiv.org/abs/2507.08917",
        "pdf_url": "https://arxiv.org/pdf/2507.08917",
        "title": "Detecting Deepfake Talking Heads from Facial Biometric Anomalies",
        "authors": [
            "Justin D. Norman",
            "Hany Farid"
        ],
        "comments": "10 pages, 3 figures, 3 tables",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The combination of highly realistic voice cloning, along with visually compelling avatar, face-swap, or lip-sync deepfake video generation, makes it relatively easy to create a video of anyone saying anything. Today, such deepfake impersonations are often used to power frauds, scams, and political disinformation. We propose a novel forensic machine learning technique for the detection of deepfake video impersonations that leverages unnatural patterns in facial biometrics. We evaluate this technique across a large dataset of deepfake techniques and impersonations, as well as assess its reliability to video laundering and its generalization to previously unseen video deepfake generators.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 3,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.08979",
        "abs_url": "https://arxiv.org/abs/2507.08979",
        "pdf_url": "https://arxiv.org/pdf/2507.08979",
        "title": "PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with LLM-Guided Embedding Projection",
        "authors": [
            "Mahdiyar Molahasani",
            "Azadeh Motamedi",
            "Michael Greenspan",
            "Il-Min Kim",
            "Ali Etemad"
        ],
        "comments": "Accepted to ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "We introduce Projection-based Reduction of Implicit Spurious bias in vision-language Models (PRISM), a new data-free and task-agnostic solution for bias mitigation in VLMs like CLIP. VLMs often inherit and amplify biases in their training data, leading to skewed predictions. PRISM is designed to debias VLMs without relying on predefined bias categories or additional external data. It operates in two stages: first, an LLM is prompted with simple class prompts to generate scene descriptions that contain spurious correlations. Next, PRISM uses our novel contrastive-style debiasing loss to learn a projection that maps the embeddings onto a latent space that minimizes spurious correlations while preserving the alignment between image and text this http URL experiments demonstrate that PRISM outperforms current debiasing methods on the commonly used Waterbirds and CelebA datasets We make our code public at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 4,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.08981",
        "abs_url": "https://arxiv.org/abs/2507.08981",
        "pdf_url": "https://arxiv.org/pdf/2507.08981",
        "title": "Video Inference for Human Mesh Recovery with Vision Transformer",
        "authors": [
            "Hanbyel Cho",
            "Jaesung Ahn",
            "Yooshin Cho",
            "Junmo Kim"
        ],
        "comments": "Accepted to IEEE FG 2023",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Human Mesh Recovery (HMR) from an image is a challenging problem because of the inherent ambiguity of the task. Existing HMR methods utilized either temporal information or kinematic relationships to achieve higher accuracy, but there is no method using both. Hence, we propose \"Video Inference for Human Mesh Recovery with Vision Transformer (HMR-ViT)\" that can take into account both temporal and kinematic information. In HMR-ViT, a Temporal-kinematic Feature Image is constructed using feature vectors obtained from video frames by an image encoder. When generating the feature image, we use a Channel Rearranging Matrix (CRM) so that similar kinematic features could be located spatially close together. The feature image is then further encoded using Vision Transformer, and the SMPL pose and shape parameters are finally inferred using a regression network. Extensive evaluation on the 3DPW and Human3.6M datasets indicates that our method achieves a competitive performance in HMR.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 5,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09005",
        "abs_url": "https://arxiv.org/abs/2507.09005",
        "pdf_url": "https://arxiv.org/pdf/2507.09005",
        "title": "From images to properties: a NeRF-driven framework for granular material parameter inversion",
        "authors": [
            "Cheng-Hsi Hsiao",
            "Krishna Kumar"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Geophysics (physics.geo-ph)",
        "abstract": "We introduce a novel framework that integrates Neural Radiance Fields (NeRF) with Material Point Method (MPM) simulation to infer granular material properties from visual observations. Our approach begins by generating synthetic experimental data, simulating an plow interacting with sand. The experiment is rendered into realistic images as the photographic observations. These observations include multi-view images of the experiment's initial state and time-sequenced images from two fixed cameras. Using NeRF, we reconstruct the 3D geometry from the initial multi-view images, leveraging its capability to synthesize novel viewpoints and capture intricate surface details. The reconstructed geometry is then used to initialize material point positions for the MPM simulation, where the friction angle remains unknown. We render images of the simulation under the same camera setup and compare them to the observed images. By employing Bayesian optimization, we minimize the image loss to estimate the best-fitting friction angle. Our results demonstrate that friction angle can be estimated with an error within 2 degrees, highlighting the effectiveness of inverse analysis through purely visual observations. This approach offers a promising solution for characterizing granular materials in real-world scenarios where direct measurement is impractical or impossible.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 6,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09008",
        "abs_url": "https://arxiv.org/abs/2507.09008",
        "pdf_url": "https://arxiv.org/pdf/2507.09008",
        "title": "VISTA: A Visual Analytics Framework to Enhance Foundation Model-Generated Data Labels",
        "authors": [
            "Xiwei Xuan",
            "Xiaoqi Wang",
            "Wenbin He",
            "Jorge Piazentin Ono",
            "Liang Gou",
            "Kwan-Liu Ma",
            "Liu Ren"
        ],
        "comments": "IEEE Transactions on Visualization and Computer Graphics (2025)",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The advances in multi-modal foundation models (FMs) (e.g., CLIP and LLaVA) have facilitated the auto-labeling of large-scale datasets, enhancing model performance in challenging downstream tasks such as open-vocabulary object detection and segmentation. However, the quality of FM-generated labels is less studied as existing approaches focus more on data quantity over quality. This is because validating large volumes of data without ground truth presents a considerable challenge in practice. Existing methods typically rely on limited metrics to identify problematic data, lacking a comprehensive perspective, or apply human validation to only a small data fraction, failing to address the full spectrum of potential issues. To overcome these challenges, we introduce VISTA, a visual analytics framework that improves data quality to enhance the performance of multi-modal models. Targeting the complex and demanding domain of open-vocabulary image segmentation, VISTA integrates multi-phased data validation strategies with human expertise, enabling humans to identify, understand, and correct hidden issues within FM-generated labels. Through detailed use cases on two benchmark datasets and expert reviews, we demonstrate VISTA's effectiveness from both quantitative and qualitative perspectives.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 7,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09036",
        "abs_url": "https://arxiv.org/abs/2507.09036",
        "pdf_url": "https://arxiv.org/pdf/2507.09036",
        "title": "BrainLesion Suite: A Flexible and User-Friendly Framework for Modular Brain Lesion Image Analysis",
        "authors": [
            "Florian Kofler",
            "Marcel Rosier",
            "Mehdi Astaraki",
            "Hendrik Möller",
            "Ilhem Isra Mekki",
            "Josef A. Buchner",
            "Anton Schmick",
            "Arianna Pfiffer",
            "Eva Oswald",
            "Lucas Zimmer",
            "Ezequiel de la Rosa",
            "Sarthak Pati",
            "Julian Canisius",
            "Arianna Piffer",
            "Ujjwal Baid",
            "Mahyar Valizadeh",
            "Akis Linardos",
            "Jan C. Peeken",
            "Surprosanna Shit",
            "Felix Steinbauer",
            "Daniel Rueckert",
            "Rolf Heckemann",
            "Spyridon Bakas",
            "Jan Kirschke",
            "Constantin von See",
            "Ivan Ezhov",
            "Marie Piraud",
            "Benedikt Wiestler",
            "Bjoern Menze"
        ],
        "comments": "16p, 3f",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Software Engineering (cs.SE)",
        "abstract": "BrainLesion Suite is a versatile toolkit for building modular brain lesion image analysis pipelines in Python. Following Pythonic principles, BrainLesion Suite is designed to provide a 'brainless' development experience, minimizing cognitive effort and streamlining the creation of complex workflows for clinical and scientific practice. At its core is an adaptable preprocessing module that performs co-registration, atlas registration, and optional skull-stripping and defacing on arbitrary multi-modal input images. BrainLesion Suite leverages algorithms from the BraTS challenge to synthesize missing modalities, inpaint lesions, and generate pathology-specific tumor segmentations. BrainLesion Suite also enables quantifying segmentation model performance, with tools such as panoptica to compute lesion-wise metrics. Although BrainLesion Suite was originally developed for image analysis pipelines of brain lesions such as glioma, metastasis, and multiple sclerosis, it can be adapted for other biomedical image analysis applications. The individual BrainLesion Suite packages and tutorials are accessible on GitHub.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 8,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09052",
        "abs_url": "https://arxiv.org/abs/2507.09052",
        "pdf_url": "https://arxiv.org/pdf/2507.09052",
        "title": "Can Contrastive Learning Improve Class-Imbalanced Diffusion Model?",
        "authors": [
            "Fang Chen",
            "Alex Villa",
            "Gongbo Liang",
            "Xiaoyi Lu",
            "Meng Tang"
        ],
        "comments": "20 pages, 11 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Training data for class-conditional image synthesis often exhibit a long-tailed distribution with limited images for tail classes. Such an imbalance causes mode collapse and reduces the diversity of synthesized images for tail classes. For class-conditional diffusion models trained on imbalanced data, we aim to improve the diversity of tail class images without compromising the fidelity and diversity of head class images. We achieve this by introducing two deceptively simple but highly effective contrastive loss functions. Firstly, we employ an unsupervised InfoNCE loss utilizing negative samples to increase the distance/dissimilarity among synthetic images, particularly for tail classes. To further enhance the diversity of tail classes, our second loss is an MSE loss that contrasts class-conditional generation with unconditional generation at large timesteps. This second loss makes the denoising process insensitive to class conditions for the initial steps, which enriches tail classes through knowledge sharing from head classes. Conditional-unconditional alignment has been shown to enhance the performance of long-tailed GAN. We are the first to adapt such alignment to diffusion models. We successfully leveraged contrastive learning for class-imbalanced diffusion models. Our contrastive learning framework is easy to implement and outperforms standard DDPM and alternative methods for class-imbalanced diffusion models across various datasets, including CIFAR10/100-LT, PlacesLT, TinyImageNetLT, and ImageNetLT.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 9,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09068",
        "abs_url": "https://arxiv.org/abs/2507.09068",
        "pdf_url": "https://arxiv.org/pdf/2507.09068",
        "title": "Infinite Video Understanding",
        "authors": [
            "Dell Zhang",
            "Xiangyu Chen",
            "Jixiang Luo",
            "Mengxi Jia",
            "Changzhi Sun",
            "Ruilong Ren",
            "Jingren Liu",
            "Hao Sun",
            "Xuelong Li"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG); Multimedia (cs.MM)",
        "abstract": "The rapid advancements in Large Language Models (LLMs) and their multimodal extensions (MLLMs) have ushered in remarkable progress in video understanding. However, a fundamental challenge persists: effectively processing and comprehending video content that extends beyond minutes or hours. While recent efforts like Video-XL-2 have demonstrated novel architectural solutions for extreme efficiency, and advancements in positional encoding such as HoPE and VideoRoPE++ aim to improve spatio-temporal understanding over extensive contexts, current state-of-the-art models still encounter significant computational and memory constraints when faced with the sheer volume of visual tokens from lengthy sequences. Furthermore, maintaining temporal coherence, tracking complex events, and preserving fine-grained details over extended periods remain formidable hurdles, despite progress in agentic reasoning systems like Deep Video Discovery. This position paper posits that a logical, albeit ambitious, next frontier for multimedia research is Infinite Video Understanding -- the capability for models to continuously process, understand, and reason about video data of arbitrary, potentially never-ending duration. We argue that framing Infinite Video Understanding as a blue-sky research objective provides a vital north star for the multimedia, and the wider AI, research communities, driving innovation in areas such as streaming architectures, persistent memory mechanisms, hierarchical and adaptive representations, event-centric reasoning, and novel evaluation paradigms. Drawing inspiration from recent work on long/ultra-long video understanding and several closely related fields, we outline the core challenges and key research directions towards achieving this transformative capability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 10,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09071",
        "abs_url": "https://arxiv.org/abs/2507.09071",
        "pdf_url": "https://arxiv.org/pdf/2507.09071",
        "title": "BlindSight: Harnessing Sparsity for Efficient VLMs",
        "authors": [
            "Tharun Adithya Srikrishnan",
            "Deval Shah",
            "Steven K. Reinhardt"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Large vision-language models (VLMs) enable the joint processing of text and images. However, the inclusion of vision data significantly expands the prompt length. Along with the quadratic complexity of the attention computation, this results in a longer prefill duration. An approach to mitigate this bottleneck is to leverage the inherent sparsity in the attention computation. In our analysis of attention patterns in VLMs, we observe that a substantial portion of layers exhibit minimal cross-image attention, except through attention-sink tokens per image. These sparse attention patterns fall into distinct categories: sink-only, document mask and a hybrid document-sink mask. Based on this, we propose BlindSight: a training-free approach to optimize VLM inference using a input template-aware attention sparsity mask. We utilize samples from a dataset to derive a prompt-agnostic sparsity categorization for every attention head. We evaluate the proposed technique using VLMs such as Qwen2-VL, Qwen2.5-VL and Gemma-3. BlindSight results in a 32%-41% reduction in FLOPs on average with -2%-+2% accuracy compared to the original model in most evaluated multi-image understanding benchmarks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 11,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09081",
        "abs_url": "https://arxiv.org/abs/2507.09081",
        "pdf_url": "https://arxiv.org/pdf/2507.09081",
        "title": "From Physics to Foundation Models: A Review of AI-Driven Quantitative Remote Sensing Inversion",
        "authors": [
            "Zhenyu Yu",
            "Mohd Yamani Idna Idris",
            "Hua Wang",
            "Pei Wang",
            "Junyi Chen",
            "Kun Wang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Quantitative remote sensing inversion aims to estimate continuous surface variables-such as biomass, vegetation indices, and evapotranspiration-from satellite observations, supporting applications in ecosystem monitoring, carbon accounting, and land management. With the evolution of remote sensing systems and artificial intelligence, traditional physics-based paradigms are giving way to data-driven and foundation model (FM)-based approaches. This paper systematically reviews the methodological evolution of inversion techniques, from physical models (e.g., PROSPECT, SCOPE, DART) to machine learning methods (e.g., deep learning, multimodal fusion), and further to foundation models (e.g., SatMAE, GFM, mmEarth). We compare the modeling assumptions, application scenarios, and limitations of each paradigm, with emphasis on recent FM advances in self-supervised pretraining, multi-modal integration, and cross-task adaptation. We also highlight persistent challenges in physical interpretability, domain generalization, limited supervision, and uncertainty quantification. Finally, we envision the development of next-generation foundation models for remote sensing inversion, emphasizing unified modeling capacity, cross-domain generalization, and physical interpretability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 12,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09082",
        "abs_url": "https://arxiv.org/abs/2507.09082",
        "pdf_url": "https://arxiv.org/pdf/2507.09082",
        "title": "Taming generative video models for zero-shot optical flow extraction",
        "authors": [
            "Seungwoo Kim",
            "Khai Loong Aw",
            "Klemen Kotar",
            "Cristobal Eyzaguirre",
            "Wanhee Lee",
            "Yunong Liu",
            "Jared Watrous",
            "Stefan Stojanov",
            "Juan Carlos Niebles",
            "Jiajun Wu",
            "Daniel L. K. Yamins"
        ],
        "comments": "Project webpage: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Extracting optical flow from videos remains a core computer vision problem. Motivated by the success of large general-purpose models, we ask whether frozen self-supervised video models trained only for future frame prediction can be prompted, without fine-tuning, to output flow. Prior work reading out depth or illumination from video generators required fine-tuning, which is impractical for flow where labels are scarce and synthetic datasets suffer from a sim-to-real gap. Inspired by the Counterfactual World Model (CWM) paradigm, which can obtain point-wise correspondences by injecting a small tracer perturbation into a next-frame predictor and tracking its propagation, we extend this idea to generative video models. We explore several popular architectures and find that successful zero-shot flow extraction in this manner is aided by three model properties: (1) distributional prediction of future frames (avoiding blurry or noisy outputs); (2) factorized latents that treat each spatio-temporal patch independently; and (3) random-access decoding that can condition on any subset of future pixels. These properties are uniquely present in the recent Local Random Access Sequence (LRAS) architecture. Building on LRAS, we propose KL-tracing: a novel test-time procedure that injects a localized perturbation into the first frame, rolls out the model one step, and computes the Kullback-Leibler divergence between perturbed and unperturbed predictive distributions. Without any flow-specific fine-tuning, our method outperforms state-of-the-art models on real-world TAP-Vid DAVIS dataset (16.6% relative improvement for endpoint error) and synthetic TAP-Vid Kubric (4.7% relative improvement). Our results indicate that counterfactual prompting of controllable generative video models is a scalable and effective alternative to supervised or photometric-loss approaches for high-quality flow.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 13,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09092",
        "abs_url": "https://arxiv.org/abs/2507.09092",
        "pdf_url": "https://arxiv.org/pdf/2507.09092",
        "title": "MI CAM: Mutual Information Weighted Activation Mapping for Causal Visual Explanations of Convolutional Neural Networks",
        "authors": [
            "Ram S Iyer",
            "Narayan S Iyer",
            "Rugmini Ammal P"
        ],
        "comments": "12 pages, 10 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "With the intervention of machine vision in our crucial day to day necessities including healthcare and automated power plants, attention has been drawn to the internal mechanisms of convolutional neural networks, and the reason why the network provides specific inferences. This paper proposes a novel post-hoc visual explanation method called MI CAM based on activation mapping. Differing from previous class activation mapping based approaches, MI CAM produces saliency visualizations by weighing each feature map through its mutual information with the input image and the final result is generated by a linear combination of weights and activation maps. It also adheres to producing causal interpretations as validated with the help of counterfactual analysis. We aim to exhibit the visual performance and unbiased justifications for the model inferencing procedure achieved by MI CAM. Our approach works at par with all state-of-the-art methods but particularly outperforms some in terms of qualitative and quantitative measures. The implementation of proposed method can be found on this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 14,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09097",
        "abs_url": "https://arxiv.org/abs/2507.09097",
        "pdf_url": "https://arxiv.org/pdf/2507.09097",
        "title": "RadEyeVideo: Enhancing general-domain Large Vision Language Model for chest X-ray analysis with video representations of eye gaze",
        "authors": [
            "Yunsoo Kim",
            "Jinge Wu",
            "Honghan Wu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Large Vision-Language Models (LVLMs) have demonstrated promising performance in chest X-ray (CXR) analysis. To enhance human-computer interaction, several studies have incorporated radiologists' eye gaze, typically through heatmaps or textual prompts. However, these methods often overlook the sequential order of eye movements, which could provide valuable insights by highlighting both the areas of interest and the order in which they are examined. In this work, we propose a novel approach called RadEyeVideo that integrates radiologists' eye-fixation data as a video sequence, capturing both the temporal and spatial dynamics of their gaze. We evaluate this method in CXR report generation and disease diagnosis using three general-domain, open-source LVLMs with video input capabilities. When prompted with eye-gaze videos, model performance improves by up to 24.6% in the report generation task and on average 15.2% for both tasks using scaled evaluation metrics. Notably, RadEyeVideo enhanced an open-domain LVLM model, LLaVA-OneVision, to surpass task-specific medical LVLMs such as MAIRA-2 and CheXagent, trained on large Chest X-ray data. This work highlights that domain expert's knowledge (eye-gaze information in this case), when effectively integrated with LVLMs, can significantly enhance general-domain models' capabilities in clinical tasks. RadEyeVideo is a step toward a scalable human-centered approach of utilizing LVLMs in medical image analytics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 15,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09102",
        "abs_url": "https://arxiv.org/abs/2507.09102",
        "pdf_url": "https://arxiv.org/pdf/2507.09102",
        "title": "Harnessing Text-to-Image Diffusion Models for Point Cloud Self-Supervised Learning",
        "authors": [
            "Yiyang Chen",
            "Shanshan Zhao",
            "Lunhao Duan",
            "Changxing Ding",
            "Dacheng Tao"
        ],
        "comments": "Accepted by ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Diffusion-based models, widely used in text-to-image generation, have proven effective in 2D representation learning. Recently, this framework has been extended to 3D self-supervised learning by constructing a conditional point generator for enhancing 3D representations. However, its performance remains constrained by the 3D diffusion model, which is trained on the available 3D datasets with limited size. We hypothesize that the robust capabilities of text-to-image diffusion models, particularly Stable Diffusion (SD), which is trained on large-scale datasets, can help overcome these limitations. To investigate this hypothesis, we propose PointSD, a framework that leverages the SD model for 3D self-supervised learning. By replacing the SD model's text encoder with a 3D encoder, we train a point-to-image diffusion model that allows point clouds to guide the denoising of rendered noisy images. With the trained point-to-image diffusion model, we use noise-free images as the input and point clouds as the condition to extract SD features. Next, we train a 3D backbone by aligning its features with these SD features, thereby facilitating direct semantic learning. Comprehensive experiments on downstream point cloud tasks and ablation studies demonstrate that the SD model can enhance point cloud self-supervised learning. Code is publicly available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 16,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09105",
        "abs_url": "https://arxiv.org/abs/2507.09105",
        "pdf_url": "https://arxiv.org/pdf/2507.09105",
        "title": "Hybrid Autoregressive-Diffusion Model for Real-Time Streaming Sign Language Production",
        "authors": [
            "Maoxiao Ye",
            "Xinfeng Ye",
            "Mano Manoharan"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Earlier Sign Language Production (SLP) models typically relied on autoregressive methods that generate output tokens one by one, which inherently provide temporal alignment. Although techniques like Teacher Forcing can prevent model collapse during training, they still cannot solve the problem of error accumulation during inference, since ground truth is unavailable at that stage. In contrast, more recent approaches based on diffusion models leverage step-by-step denoising to enable high-quality generation. However, the iterative nature of these models and the requirement to denoise entire sequences limit their applicability in real-time tasks like SLP. To address it, we apply a hybrid approach combining autoregressive and diffusion models to SLP for the first time, leveraging the strengths of both models in sequential dependency modeling and output refinement. To capture fine-grained body movements, we design a Multi-Scale Pose Representation module that separately extracts detailed features from distinct articulators and integrates them via a Multi-Scale Fusion module. Furthermore, we introduce a Confidence-Aware Causal Attention mechanism that utilizes joint-level confidence scores to dynamically guide the pose generation process, improving accuracy and robustness. Extensive experiments on the PHOENIX14T and How2Sign datasets demonstrate the effectiveness of our method in both generation quality and real-time streaming efficiency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 17,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09111",
        "abs_url": "https://arxiv.org/abs/2507.09111",
        "pdf_url": "https://arxiv.org/pdf/2507.09111",
        "title": "RoHOI: Robustness Benchmark for Human-Object Interaction Detection",
        "authors": [
            "Di Wen",
            "Kunyu Peng",
            "Kailun Yang",
            "Yufan Chen",
            "Ruiping Liu",
            "Junwei Zheng",
            "Alina Roitberg",
            "Rainer Stiefelhagen"
        ],
        "comments": "Benchmarks, datasets, and code will be made publicly available at this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC); Robotics (cs.RO); Image and Video Processing (eess.IV)",
        "abstract": "Human-Object Interaction (HOI) detection is crucial for robot-human assistance, enabling context-aware support. However, models trained on clean datasets degrade in real-world conditions due to unforeseen corruptions, leading to inaccurate prediction. To address this, we introduce the first robustness benchmark for HOI detection, evaluating model resilience under diverse challenges. Despite advances, current models struggle with environmental variability, occlusion, and noise. Our benchmark, RoHOI, includes 20 corruption types based on HICO-DET and V-COCO datasets and a new robustness-focused metric. We systematically analyze existing models in the related field, revealing significant performance drops under corruptions. To improve robustness, we propose a Semantic-Aware Masking-based Progressive Learning (SAMPL) strategy to guide the model to be optimized based on holistic and partial cues, dynamically adjusting the model's optimization to enhance robust feature learning. Extensive experiments show our approach outperforms state-of-the-art methods, setting a new standard for robust HOI detection. Benchmarks, datasets, and code will be made publicly available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 18,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09118",
        "abs_url": "https://arxiv.org/abs/2507.09118",
        "pdf_url": "https://arxiv.org/pdf/2507.09118",
        "title": "Mind the Gap: Preserving and Compensating for the Modality Gap in CLIP-Based Continual Learning",
        "authors": [
            "Linlan Huang",
            "Xusheng Cao",
            "Haori Lu",
            "Yifan Meng",
            "Fei Yang",
            "Xialei Liu"
        ],
        "comments": "Accepted at ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Continual learning aims to enable models to learn sequentially from continuously incoming data while retaining performance on previously learned tasks. With the Contrastive Language-Image Pre-trained model (CLIP) exhibiting strong capabilities across various downstream tasks, there has been growing interest in leveraging CLIP for continual learning in such scenarios. Most existing works overlook the inherent modality gap in CLIP, a key factor in its generalization and adaptability. In this paper, we analyze the variations in the modality gap during the fine-tuning of vision-language pre-trained models. Our observations reveal that the modality gap effectively reflects the extent to which pre-trained knowledge is preserved. Based on these insights, we propose a simple yet effective method, MG-CLIP, that improves CLIP's performance in class-incremental learning. Our approach leverages modality gap preservation to mitigate forgetting and modality gap compensation to enhance the capacity for new data, introducing a novel modality-gap-based perspective for continual learning. Extensive experiments on multiple benchmarks demonstrate that our method outperforms existing approaches without requiring additional replay data. Our code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 19,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09122",
        "abs_url": "https://arxiv.org/abs/2507.09122",
        "pdf_url": "https://arxiv.org/pdf/2507.09122",
        "title": "SnapMoGen: Human Motion Generation from Expressive Texts",
        "authors": [
            "Chuan Guo",
            "Inwoo Hwang",
            "Jian Wang",
            "Bing Zhou"
        ],
        "comments": "Project Webpage: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Text-to-motion generation has experienced remarkable progress in recent years. However, current approaches remain limited to synthesizing motion from short or general text prompts, primarily due to dataset constraints. This limitation undermines fine-grained controllability and generalization to unseen prompts. In this paper, we introduce SnapMoGen, a new text-motion dataset featuring high-quality motion capture data paired with accurate, expressive textual annotations. The dataset comprises 20K motion clips totaling 44 hours, accompanied by 122K detailed textual descriptions averaging 48 words per description (vs. 12 words of HumanML3D). Importantly, these motion clips preserve original temporal continuity as they were in long sequences, facilitating research in long-term motion generation and blending. We also improve upon previous generative masked modeling approaches. Our model, MoMask++, transforms motion into multi-scale token sequences that better exploit the token capacity, and learns to generate all tokens using a single generative masked transformer. MoMask++ achieves state-of-the-art performance on both HumanML3D and SnapMoGen benchmarks. Additionally, we demonstrate the ability to process casual user prompts by employing an LLM to reformat inputs to align with the expressivity and narration style of SnapMoGen. Project webpage: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 20,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09139",
        "abs_url": "https://arxiv.org/abs/2507.09139",
        "pdf_url": "https://arxiv.org/pdf/2507.09139",
        "title": "PoseLLM: Enhancing Language-Guided Human Pose Estimation with MLP Alignment",
        "authors": [
            "Dewen Zhang",
            "Tahir Hussain",
            "Wangpeng An",
            "Hayaru Shouno"
        ],
        "comments": "Preprint",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Human pose estimation traditionally relies on architectures that encode keypoint priors, limiting their generalization to novel poses or unseen keypoints. Recent language-guided approaches like LocLLM reformulate keypoint localization as a vision-language task, enabling zero-shot generalization through textual descriptions. However, LocLLM's linear projector fails to capture complex spatial-textual interactions critical for high-precision localization. To address this, we propose PoseLLM, the first Large Language Model (LLM)-based pose estimation framework that replaces the linear projector with a nonlinear MLP vision-language connector. This lightweight two-layer MLP with GELU activation enables hierarchical cross-modal feature transformation, enhancing the fusion of visual patches and textual keypoint descriptions. Trained exclusively on COCO data, PoseLLM achieves 77.8 AP on the COCO validation set, outperforming LocLLM by +0.4 AP, while maintaining strong zero-shot generalization on Human-Art and MPII. Our work demonstrates that a simple yet powerful nonlinear connector significantly boosts localization accuracy without sacrificing generalization, advancing the state-of-the-art in language-guided pose estimation. Code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 21,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09144",
        "abs_url": "https://arxiv.org/abs/2507.09144",
        "pdf_url": "https://arxiv.org/pdf/2507.09144",
        "title": "$I^{2}$-World: Intra-Inter Tokenization for Efficient Dynamic 4D Scene Forecasting",
        "authors": [
            "Zhimin Liao",
            "Ping Wei",
            "Ruijie Zhang",
            "Shuaijia Chen",
            "Haoxuan Wang",
            "Ziyang Ren"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Forecasting the evolution of 3D scenes and generating unseen scenarios via occupancy-based world models offers substantial potential for addressing corner cases in autonomous driving systems. While tokenization has revolutionized image and video generation, efficiently tokenizing complex 3D scenes remains a critical challenge for 3D world models. To address this, we propose $I^{2}$-World, an efficient framework for 4D occupancy forecasting. Our method decouples scene tokenization into intra-scene and inter-scene tokenizers. The intra-scene tokenizer employs a multi-scale residual quantization strategy to hierarchically compress 3D scenes while preserving spatial details. The inter-scene tokenizer residually aggregates temporal dependencies across timesteps. This dual design preserves the compactness of 3D tokenizers while retaining the dynamic expressiveness of 4D tokenizers. Unlike decoder-only GPT-style autoregressive models, $I^{2}$-World adopts an encoder-decoder architecture. The encoder aggregates spatial context from the current scene and predicts a transformation matrix to enable high-level control over scene generation. The decoder, conditioned on this matrix and historical tokens, ensures temporal consistency during generation. Experiments demonstrate that $I^{2}$-World achieves state-of-the-art performance, outperforming existing methods by 25.1\\% in mIoU and 36.9\\% in IoU for 4D occupancy forecasting while exhibiting exceptional computational efficiency: it requires merely 2.9 GB of training memory and achieves real-time inference at 37.0 FPS. Our code is available on this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 22,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09168",
        "abs_url": "https://arxiv.org/abs/2507.09168",
        "pdf_url": "https://arxiv.org/pdf/2507.09168",
        "title": "Stable Score Distillation",
        "authors": [
            "Haiming Zhu",
            "Yangyang Xu",
            "Chenshu Xu",
            "Tingrui Shen",
            "Wenxi Liu",
            "Yong Du",
            "Jun Yu",
            "Shengfeng He"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Text-guided image and 3D editing have advanced with diffusion-based models, yet methods like Delta Denoising Score often struggle with stability, spatial control, and editing strength. These limitations stem from reliance on complex auxiliary structures, which introduce conflicting optimization signals and restrict precise, localized edits. We introduce Stable Score Distillation (SSD), a streamlined framework that enhances stability and alignment in the editing process by anchoring a single classifier to the source prompt. Specifically, SSD utilizes Classifier-Free Guidance (CFG) equation to achieves cross-prompt alignment, and introduces a constant term null-text branch to stabilize the optimization process. This approach preserves the original content's structure and ensures that editing trajectories are closely aligned with the source prompt, enabling smooth, prompt-specific modifications while maintaining coherence in surrounding regions. Additionally, SSD incorporates a prompt enhancement branch to boost editing strength, particularly for style transformations. Our method achieves state-of-the-art results in 2D and 3D editing tasks, including NeRF and text-driven style edits, with faster convergence and reduced complexity, providing a robust and efficient solution for text-guided editing.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 23,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09180",
        "abs_url": "https://arxiv.org/abs/2507.09180",
        "pdf_url": "https://arxiv.org/pdf/2507.09180",
        "title": "Learning and Transferring Better with Depth Information in Visual Reinforcement Learning",
        "authors": [
            "Zichun Xu",
            "Yuntao Li",
            "Zhaomin Wang",
            "Lei Zhuang",
            "Guocai Yang",
            "Jingdong Zhao"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
        "abstract": "Depth information is robust to scene appearance variations and inherently carries 3D spatial details. In this paper, a visual backbone based on the vision transformer is proposed to fuse RGB and depth modalities for enhancing generalization. Different modalities are first processed by separate CNN stems, and the combined convolutional features are delivered to the scalable vision transformer to obtain visual representations. Moreover, a contrastive unsupervised learning scheme is designed with masked and unmasked tokens to accelerate the sample efficiency during the reinforcement learning progress. For sim2real transfer, a flexible curriculum learning schedule is developed to deploy domain randomization over training processes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 24,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09183",
        "abs_url": "https://arxiv.org/abs/2507.09183",
        "pdf_url": "https://arxiv.org/pdf/2507.09183",
        "title": "Revisiting Pool-based Prompt Learning for Few-shot Class-incremental Learning",
        "authors": [
            "Yongwei Jiang",
            "Yixiong Zou",
            "Yuhua Li",
            "Ruixuan Li"
        ],
        "comments": "Accepted to ICCV 2025, 11 pages",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Few-Shot Class-Incremental Learning (FSCIL) faces dual challenges of data scarcity and incremental learning in real-world scenarios. While pool-based prompting methods have demonstrated success in traditional incremental learning, their effectiveness in FSCIL settings remains unexplored. This paper presents the first study of current prompt pool methods in FSCIL tasks, revealing an unanticipated performance degradation in incremental sessions. Through comprehensive analysis, we identify that this phenomenon stems from token-dimension saturation: with limited data, excessive prompts compete for task-relevant information, leading to model overfitting. Based on this finding, we propose LGSP-Prompt (Local-Global Spatial Prompting), which innovatively shifts pool-based prompt learning from the token dimension to the spatial dimension. LGSP-Prompt generates spatial prompts by synergistically combining local spatial features and global frequency-domain representations to highlight key patterns in input images. We construct two spatial prompt pools enabling dynamic prompt selection to maintain acquired knowledge while effectively learning novel sessions. Extensive experiments demonstrate that our approach achieves state-of-the-art performance across multiple FSCIL benchmarks, showing significant advantages in both base knowledge preservation and incremental learning. Our implementation is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 25,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09184",
        "abs_url": "https://arxiv.org/abs/2507.09184",
        "pdf_url": "https://arxiv.org/pdf/2507.09184",
        "title": "MCA-LLaVA: Manhattan Causal Attention for Reducing Hallucination in Large Vision-Language Models",
        "authors": [
            "Qiyan Zhao",
            "Xiaofeng Zhang",
            "Yiheng Li",
            "Yun Xing",
            "Xiaosong Yuan",
            "Feilong Tang",
            "Sinan Fan",
            "Xuhang Chen",
            "Xuyao Zhang",
            "Dahan Wang"
        ],
        "comments": "Accepted in ACM MM 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Hallucinations pose a significant challenge in Large Vision Language Models (LVLMs), with misalignment between multimodal features identified as a key contributing factor. This paper reveals the negative impact of the long-term decay in Rotary Position Encoding (RoPE), used for positional modeling in LVLMs, on multimodal alignment. Concretely, under long-term decay, instruction tokens exhibit uneven perception of image tokens located at different positions within the two-dimensional space: prioritizing image tokens from the bottom-right region since in the one-dimensional sequence, these tokens are positionally closer to the instruction tokens. This biased perception leads to insufficient image-instruction interaction and suboptimal multimodal alignment. We refer to this phenomenon as image alignment bias. To enhance instruction's perception of image tokens at different spatial locations, we propose MCA-LLaVA, based on Manhattan distance, which extends the long-term decay to a two-dimensional, multi-directional spatial decay. MCA-LLaVA integrates the one-dimensional sequence order and two-dimensional spatial position of image tokens for positional modeling, mitigating hallucinations by alleviating image alignment bias. Experimental results of MCA-LLaVA across various hallucination and general benchmarks demonstrate its effectiveness and generality. The code can be accessed in this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 26,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09200",
        "abs_url": "https://arxiv.org/abs/2507.09200",
        "pdf_url": "https://arxiv.org/pdf/2507.09200",
        "title": "THYME: Temporal Hierarchical-Cyclic Interactivity Modeling for Video Scene Graphs in Aerial Footage",
        "authors": [
            "Trong-Thuan Nguyen",
            "Pha Nguyen",
            "Jackson Cothren",
            "Alper Yilmaz",
            "Minh-Triet Tran",
            "Khoa Luu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The rapid proliferation of video in applications such as autonomous driving, surveillance, and sports analytics necessitates robust methods for dynamic scene understanding. Despite advances in static scene graph generation and early attempts at video scene graph generation, previous methods often suffer from fragmented representations, failing to capture fine-grained spatial details and long-range temporal dependencies simultaneously. To address these limitations, we introduce the Temporal Hierarchical Cyclic Scene Graph (THYME) approach, which synergistically integrates hierarchical feature aggregation with cyclic temporal refinement to address these limitations. In particular, THYME effectively models multi-scale spatial context and enforces temporal consistency across frames, yielding more accurate and coherent scene graphs. In addition, we present AeroEye-v1.0, a novel aerial video dataset enriched with five types of interactivity that overcome the constraints of existing datasets and provide a comprehensive benchmark for dynamic scene graph generation. Empirically, extensive experiments on ASPIRe and AeroEye-v1.0 demonstrate that the proposed THYME approach outperforms state-of-the-art methods, offering improved scene understanding in ground-view and aerial scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 27,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09207",
        "abs_url": "https://arxiv.org/abs/2507.09207",
        "pdf_url": "https://arxiv.org/pdf/2507.09207",
        "title": "Visual Surface Wave Elastography: Revealing Subsurface Physical Properties via Visible Surface Waves",
        "authors": [
            "Alexander C. Ogren",
            "Berthy T. Feng",
            "Jihoon Ahn",
            "Katherine L. Bouman",
            "Chiara Daraio"
        ],
        "comments": "ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Wave propagation on the surface of a material contains information about physical properties beneath its surface. We propose a method for inferring the thickness and stiffness of a structure from just a video of waves on its surface. Our method works by extracting a dispersion relation from the video and then solving a physics-based optimization problem to find the best-fitting thickness and stiffness parameters. We validate our method on both simulated and real data, in both cases showing strong agreement with ground-truth measurements. Our technique provides a proof-of-concept for at-home health monitoring of medically-informative tissue properties, and it is further applicable to fields such as human-computer interaction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 28,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09209",
        "abs_url": "https://arxiv.org/abs/2507.09209",
        "pdf_url": "https://arxiv.org/pdf/2507.09209",
        "title": "Uncertainty-Driven Expert Control: Enhancing the Reliability of Medical Vision-Language Models",
        "authors": [
            "Xiao Liang",
            "Di Wang",
            "Zhicheng Jiao",
            "Ronghan Li",
            "Pengfei Yang",
            "Quan Wang",
            "Tat-Seng Chua"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The rapid advancements in Vision Language Models (VLMs) have prompted the development of multi-modal medical assistant systems. Despite this progress, current models still have inherent probabilistic uncertainties, often producing erroneous or unverified responses-an issue with serious implications in medical applications. Existing methods aim to enhance the performance of Medical Vision Language Model (MedVLM) by adjusting model structure, fine-tuning with high-quality data, or through preference fine-tuning. However, these training-dependent strategies are costly and still lack sufficient alignment with clinical expertise. To address these issues, we propose an expert-in-the-loop framework named Expert-Controlled Classifier-Free Guidance (Expert-CFG) to align MedVLM with clinical expertise without additional training. This framework introduces an uncertainty estimation strategy to identify unreliable outputs. It then retrieves relevant references to assist experts in highlighting key terms and applies classifier-free guidance to refine the token embeddings of MedVLM, ensuring that the adjusted outputs are correct and align with expert highlights. Evaluations across three medical visual question answering benchmarks demonstrate that the proposed Expert-CFG, with 4.2B parameters and limited expert annotations, outperforms state-of-the-art models with 13B parameters. The results demonstrate the feasibility of deploying such a system in resource-limited settings for clinical use.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 29,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09214",
        "abs_url": "https://arxiv.org/abs/2507.09214",
        "pdf_url": "https://arxiv.org/pdf/2507.09214",
        "title": "Stereo-based 3D Anomaly Object Detection for Autonomous Driving: A New Dataset and Baseline",
        "authors": [
            "Shiyi Mu",
            "Zichong Gu",
            "Hanqi Lyu",
            "Yilin Gao",
            "Shugong Xu"
        ],
        "comments": "under review",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "3D detection technology is widely used in the field of autonomous driving, with its application scenarios gradually expanding from enclosed highways to open conventional roads. For rare anomaly categories that appear on the road, 3D detection models trained on closed sets often misdetect or fail to detect anomaly objects. To address this risk, it is necessary to enhance the generalization ability of 3D detection models for targets of arbitrary shapes and to possess the capability to filter out anomalies. The generalization of 3D detection is limited by two factors: the coupled training of 2D and 3D, and the insufficient diversity in the scale distribution of training samples. This paper proposes a Stereo-based 3D Anomaly object Detection (S3AD) algorithm, which decouples the training strategy of 3D and 2D to release the generalization ability for arbitrary 3D foreground detection, and proposes an anomaly scoring algorithm based on foreground confidence prediction, achieving target-level anomaly scoring. In order to further verify and enhance the generalization of anomaly detection, we use a 3D rendering method to synthesize two augmented reality binocular stereo 3D detection datasets which named KITTI-AR. KITTI-AR extends upon KITTI by adding 97 new categories, totaling 6k pairs of stereo images. The KITTI-AR-ExD subset includes 39 common categories as extra training data to address the sparse sample distribution issue. Additionally, 58 rare categories form the KITTI-AR-OoD subset, which are not used in training to simulate zero-shot scenarios in real-world settings, solely for evaluating 3D anomaly detection. Finally, the performance of the algorithm and the dataset is verified in the experiments. (Code and dataset can be obtained at this https URL).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 30,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09216",
        "abs_url": "https://arxiv.org/abs/2507.09216",
        "pdf_url": "https://arxiv.org/pdf/2507.09216",
        "title": "360-Degree Full-view Image Segmentation by Spherical Convolution compatible with Large-scale Planar Pre-trained Models",
        "authors": [
            "Jingguo Liu",
            "Han Yu",
            "Shigang Li",
            "Jianfeng Li"
        ],
        "comments": "This paper is accecpted by ICMEW 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Due to the current lack of large-scale datasets at the million-scale level, tasks involving panoramic images predominantly rely on existing two-dimensional pre-trained image benchmark models as backbone networks. However, these networks are not equipped to recognize the distortions and discontinuities inherent in panoramic images, which adversely affects their performance in such tasks. In this paper, we introduce a novel spherical sampling method for panoramic images that enables the direct utilization of existing pre-trained models developed for two-dimensional images. Our method employs spherical discrete sampling based on the weights of the pre-trained models, effectively mitigating distortions while achieving favorable initial training values. Additionally, we apply the proposed sampling method to panoramic image segmentation, utilizing features obtained from the spherical model as masks for specific channel attentions, which yields commendable results on commonly used indoor datasets, Stanford2D3D.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 31,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09217",
        "abs_url": "https://arxiv.org/abs/2507.09217",
        "pdf_url": "https://arxiv.org/pdf/2507.09217",
        "title": "Online Long-term Point Tracking in the Foundation Model Era",
        "authors": [
            "Görkay Aydemir"
        ],
        "comments": "arXiv admin note: substantial text overlap with arXiv:2501.18487",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Point tracking aims to identify the same physical point across video frames and serves as a geometry-aware representation of motion. This representation supports a wide range of applications, from robotics to augmented reality, by enabling accurate modeling of dynamic environments. Most existing long-term tracking approaches operate in an offline setting, where future frames are available to refine predictions and recover from occlusions. However, real-world scenarios often demand online predictions: the model must operate causally, using only current and past frames. This constraint is critical in streaming video and embodied AI, where decisions must be made immediately based on past observations. Under such constraints, viewpoint invariance becomes essential. Visual foundation models, trained on diverse large-scale datasets, offer the potential for robust geometric representations. While they lack temporal reasoning on their own, they can be integrated into tracking pipelines to enrich spatial features. In this thesis, we address the problem of long-term point tracking in an online setting, where frames are processed sequentially without access to future information or sliding windows. We begin by evaluating the suitability of visual foundation models for this task and find that they can serve as useful initializations and be integrated into tracking pipelines. However, to enable long-term tracking in an online setting, a dedicated design is still required. In particular, maintaining coherence over time in this causal regime requires memory to propagate appearance and context across frames. To address this, we introduce Track-On, a transformer-based model that treats each tracked point as a query and processes video frames one at a time. Track-On sets a new state of the art across seven public benchmarks, demonstrating the feasibility of long-term tracking without future access.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 32,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09222",
        "abs_url": "https://arxiv.org/abs/2507.09222",
        "pdf_url": "https://arxiv.org/pdf/2507.09222",
        "title": "Calibrated and Robust Foundation Models for Vision-Language and Medical Image Tasks Under Distribution Shift",
        "authors": [
            "Behraj Khan",
            "Tahir Syed"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Foundation models like CLIP and SAM have transformed computer vision and medical imaging via low-shot transfer learning. However, deployment of these models hindered by two key challenges: \\textit{distribution shift} between training and test data, and \\textit{confidence misalignment} that leads to overconfident incorrect predictions. These issues manifest differently in vision-language classification and medical segmentation tasks, yet existing solutions remain domain-specific. We propose \\textit{StaRFM}, a unified framework addressing both challenges. It introduces a Fisher information penalty (FIP), extended to 3D medical data via patch-wise regularization, to reduce covariate shift in CLIP and SAM embeddings. Additionally, a confidence misalignment penalty (CMP), reformulated for voxel-level predictions, calibrates uncertainty in segmentation tasks. We theoretically derive PAC-Bayes bounds showing FIP controls generalization via the Fisher-Rao norm, while CMP minimizes calibration error through Brier score optimization. StaRFM shows consistent performance like \\texttt{+}3.5\\% accuracy and 28\\% lower ECE on 19 vision datasets (e.g., ImageNet, Office-Home), 84.7\\% DSC and 4.8mm HD95 in medical segmentation (e.g., BraTS, ATLAS), and 40\\% lower cross-domain performance gap compared to prior benchmarking methods. The framework is plug-and-play, requiring minimal architectural changes for seamless integration with foundation models. Code and models will be released at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 33,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09230",
        "abs_url": "https://arxiv.org/abs/2507.09230",
        "pdf_url": "https://arxiv.org/pdf/2507.09230",
        "title": "EgoAnimate: Generating Human Animations from Egocentric top-down Views",
        "authors": [
            "G. Kutay Türkoglu",
            "Julian Tanke",
            "Iheb Belgacem",
            "Lev Markhasin"
        ],
        "comments": "10 pages, 5 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "An ideal digital telepresence experience requires accurate replication of a person's body, clothing, and movements. To capture and transfer these movements into virtual reality, the egocentric (first-person) perspective can be adopted, which enables the use of a portable and cost-effective device without front-view cameras. However, this viewpoint introduces challenges such as occlusions and distorted body proportions. There are few works reconstructing human appearance from egocentric views, and none use a generative prior-based approach. Some methods create avatars from a single egocentric image during inference, but still rely on multi-view datasets during training. To our knowledge, this is the first study using a generative backbone to reconstruct animatable avatars from egocentric inputs. Based on Stable Diffusion, our method reduces training burden and improves generalizability. Inspired by methods such as SiTH and MagicMan, which perform 360-degree reconstruction from a frontal image, we introduce a pipeline that generates realistic frontal views from occluded top-down images using ControlNet and a Stable Diffusion backbone. Our goal is to convert a single top-down egocentric image into a realistic frontal representation and feed it into an image-to-motion model. This enables generation of avatar motions from minimal input, paving the way for more accessible and generalizable telepresence systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 34,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09242",
        "abs_url": "https://arxiv.org/abs/2507.09242",
        "pdf_url": "https://arxiv.org/pdf/2507.09242",
        "title": "PPJudge: Towards Human-Aligned Assessment of Artistic Painting Process",
        "authors": [
            "Shiqi Jiang",
            "Xinpeng Li",
            "Xi Mao",
            "Changbo Wang",
            "Chenhui Li"
        ],
        "comments": "ACM International Conference on Multimedia 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Artistic image assessment has become a prominent research area in computer vision. In recent years, the field has witnessed a proliferation of datasets and methods designed to evaluate the aesthetic quality of paintings. However, most existing approaches focus solely on static final images, overlooking the dynamic and multi-stage nature of the artistic painting process. To address this gap, we propose a novel framework for human-aligned assessment of painting processes. Specifically, we introduce the Painting Process Assessment Dataset (PPAD), the first large-scale dataset comprising real and synthetic painting process images, annotated by domain experts across eight detailed attributes. Furthermore, we present PPJudge (Painting Process Judge), a Transformer-based model enhanced with temporally-aware positional encoding and a heterogeneous mixture-of-experts architecture, enabling effective assessment of the painting process. Experimental results demonstrate that our method outperforms existing baselines in accuracy, robustness, and alignment with human judgment, offering new insights into computational creativity and art education.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 35,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09248",
        "abs_url": "https://arxiv.org/abs/2507.09248",
        "pdf_url": "https://arxiv.org/pdf/2507.09248",
        "title": "AGCD-Net: Attention Guided Context Debiasing Network for Emotion Recognition",
        "authors": [
            "Varsha Devi",
            "Amine Bohi",
            "Pardeep Kumar"
        ],
        "comments": "13 Pages, 4 figures, 2 tables ICIAP 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Context-aware emotion recognition (CAER) enhances affective computing in real-world scenarios, but traditional methods often suffer from context bias-spurious correlation between background context and emotion labels (e.g. associating ``garden'' with ``happy''). In this paper, we propose \\textbf{AGCD-Net}, an Attention Guided Context Debiasing model that introduces \\textit{Hybrid ConvNeXt}, a novel convolutional encoder that extends the ConvNeXt backbone by integrating Spatial Transformer Network and Squeeze-and-Excitation layers for enhanced feature recalibration. At the core of AGCD-Net is the Attention Guided - Causal Intervention Module (AG-CIM), which applies causal theory, perturbs context features, isolates spurious correlations, and performs an attention-driven correction guided by face features to mitigate context bias. Experimental results on the CAER-S dataset demonstrate the effectiveness of AGCD-Net, achieving state-of-the-art performance and highlighting the importance of causal debiasing for robust emotion recognition in complex settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 36,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09256",
        "abs_url": "https://arxiv.org/abs/2507.09256",
        "pdf_url": "https://arxiv.org/pdf/2507.09256",
        "title": "Ambiguity-Aware and High-Order Relation Learning for Multi-Grained Image-Text Matching",
        "authors": [
            "Junyu Chen",
            "Yihua Gao",
            "Mingyuan Ge",
            "Mingyong Li"
        ],
        "comments": "Accepted by the Knowledge-Based Systems(KBS), 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Information Retrieval (cs.IR); Multimedia (cs.MM)",
        "abstract": "Image-text matching is crucial for bridging the semantic gap between computer vision and natural language processing. However, existing methods still face challenges in handling high-order associations and semantic ambiguities among similar instances. These ambiguities arise from subtle differences between soft positive samples (semantically similar but incorrectly labeled) and soft negative samples (locally matched but globally inconsistent), creating matching uncertainties. Furthermore, current methods fail to fully utilize the neighborhood relationships among semantically similar instances within training batches, limiting the model's ability to learn high-order shared knowledge. This paper proposes the Ambiguity-Aware and High-order Relation learning framework (AAHR) to address these issues. AAHR constructs a unified representation space through dynamic clustering prototype contrastive learning, effectively mitigating the soft positive sample problem. The framework introduces global and local feature extraction mechanisms and an adaptive aggregation network, significantly enhancing full-grained semantic understanding capabilities. Additionally, AAHR employs intra-modal and inter-modal correlation matrices to investigate neighborhood relationships among sample instances thoroughly. It incorporates GNN to enhance semantic interactions between instances. Furthermore, AAHR integrates momentum contrastive learning to expand the negative sample set. These combined strategies significantly improve the model's ability to discriminate between features. Experimental results demonstrate that AAHR outperforms existing state-of-the-art methods on Flickr30K, MSCOCO, and ECCV Caption datasets, considerably improving the accuracy and efficiency of image-text matching. The code and model checkpoints for this research are available at this https URL .",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 37,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09266",
        "abs_url": "https://arxiv.org/abs/2507.09266",
        "pdf_url": "https://arxiv.org/pdf/2507.09266",
        "title": "SAGE: Segment-Aware Gloss-Free Encoding for Token-Efficient Sign Language Translation",
        "authors": [
            "JianHe Low",
            "Ozge Mercanoglu Sincan",
            "Richard Bowden"
        ],
        "comments": "Accepted in International Conference on Computer Vision (ICCV) Workshops",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Gloss-free Sign Language Translation (SLT) has advanced rapidly, achieving strong performances without relying on gloss annotations. However, these gains have often come with increased model complexity and high computational demands, raising concerns about scalability, especially as large-scale sign language datasets become more common. We propose a segment-aware visual tokenization framework that leverages sign segmentation to convert continuous video into discrete, sign-informed visual tokens. This reduces input sequence length by up to 50% compared to prior methods, resulting in up to 2.67x lower memory usage and better scalability on larger datasets. To bridge the visual and linguistic modalities, we introduce a token-to-token contrastive alignment objective, along with a dual-level supervision that aligns both language embeddings and intermediate hidden states. This improves fine-grained cross-modal alignment without relying on gloss-level supervision. Our approach notably exceeds the performance of state-of-the-art methods on the PHOENIX14T benchmark, while significantly reducing sequence length. Further experiments also demonstrate our improved performance over prior work under comparable sequence-lengths, validating the potential of our tokenization and alignment strategies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 38,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09269",
        "abs_url": "https://arxiv.org/abs/2507.09269",
        "pdf_url": "https://arxiv.org/pdf/2507.09269",
        "title": "Cross Knowledge Distillation between Artificial and Spiking Neural Networks",
        "authors": [
            "Shuhan Ye",
            "Yuanbin Qian",
            "Chong Wang",
            "Sunqi Lin",
            "Jiazhen Xu",
            "Jiangbo Qian",
            "Yuqi Li"
        ],
        "comments": "This paper has been accepted by ICME2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Recently, Spiking Neural Networks (SNNs) have demonstrated rich potential in computer vision domain due to their high biological plausibility, event-driven characteristic and energy-saving efficiency. Still, limited annotated event-based datasets and immature SNN architectures result in their performance inferior to that of Artificial Neural Networks (ANNs). To enhance the performance of SNNs on their optimal data format, DVS data, we explore using RGB data and well-performing ANNs to implement knowledge distillation. In this case, solving cross-modality and cross-architecture challenges is necessary. In this paper, we propose cross knowledge distillation (CKD), which not only leverages semantic similarity and sliding replacement to mitigate the cross-modality challenge, but also uses an indirect phased knowledge distillation to mitigate the cross-architecture challenge. We validated our method on main-stream neuromorphic datasets, including N-Caltech101 and CEP-DVS. The experimental results show that our method outperforms current State-of-the-Art methods. The code will be available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 39,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09279",
        "abs_url": "https://arxiv.org/abs/2507.09279",
        "pdf_url": "https://arxiv.org/pdf/2507.09279",
        "title": "Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models",
        "authors": [
            "Anita Kriz",
            "Elizabeth Laura Janes",
            "Xing Shen",
            "Tal Arbel"
        ],
        "comments": "Accepted to ICCV 2025 Workshop CVAMD",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Multimodal large language models (MLLMs) hold considerable promise for applications in healthcare. However, their deployment in safety-critical settings is hindered by two key limitations: (i) sensitivity to prompt design, and (ii) a tendency to generate incorrect responses with high confidence. As clinicians may rely on a model's stated confidence to gauge the reliability of its predictions, it is especially important that when a model expresses high confidence, it is also highly accurate. We introduce Prompt4Trust, the first reinforcement learning (RL) framework for prompt augmentation targeting confidence calibration in MLLMs. A lightweight LLM is trained to produce context-aware auxiliary prompts that guide a downstream task MLLM to generate responses in which the expressed confidence more accurately reflects predictive accuracy. Unlike conventional calibration techniques, Prompt4Trust specifically prioritizes aspects of calibration most critical for safe and trustworthy clinical decision-making. Beyond improvements driven by this clinically motivated calibration objective, our proposed method also improves task accuracy, achieving state-of-the-art medical visual question answering (VQA) performance on the PMC-VQA benchmark, which is composed of multiple-choice questions spanning diverse medical imaging modalities. Moreover, our framework trained with a small downstream task MLLM showed promising zero-shot generalization to larger MLLMs in our experiments, suggesting the potential for scalable calibration without the associated computational costs. This work demonstrates the potential of automated yet human-aligned prompt engineering for improving the the trustworthiness of MLLMs in safety critical settings. Our codebase can be found at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 40,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09285",
        "abs_url": "https://arxiv.org/abs/2507.09285",
        "pdf_url": "https://arxiv.org/pdf/2507.09285",
        "title": "Generative Latent Kernel Modeling for Blind Motion Deblurring",
        "authors": [
            "Chenhao Ding",
            "Jiangtao Zhang",
            "Zongsheng Yue",
            "Hui Wang",
            "Qian Zhao",
            "Deyu Meng"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Deep prior-based approaches have demonstrated remarkable success in blind motion deblurring (BMD) recently. These methods, however, are often limited by the high non-convexity of the underlying optimization process in BMD, which leads to extreme sensitivity to the initial blur kernel. To address this issue, we propose a novel framework for BMD that leverages a deep generative model to encode the kernel prior and induce a better initialization for the blur kernel. Specifically, we pre-train a kernel generator based on a generative adversarial network (GAN) to aptly characterize the kernel's prior distribution, as well as a kernel initializer to provide a well-informed and high-quality starting point for kernel estimation. By combining these two components, we constrain the BMD solution within a compact latent kernel manifold, thus alleviating the aforementioned sensitivity for kernel initialization. Notably, the kernel generator and initializer are designed to be easily integrated with existing BMD methods in a plug-and-play manner, enhancing their overall performance. Furthermore, we extend our approach to tackle blind non-uniform motion deblurring without the need for additional priors, achieving state-of-the-art performance on challenging benchmark datasets. The source code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 41,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09291",
        "abs_url": "https://arxiv.org/abs/2507.09291",
        "pdf_url": "https://arxiv.org/pdf/2507.09291",
        "title": "Supercharging Floorplan Localization with Semantic Rays",
        "authors": [
            "Yuval Grader",
            "Hadar Averbuch-Elor"
        ],
        "comments": "Accepted at ICCV 2025. this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Floorplans provide a compact representation of the building's structure, revealing not only layout information but also detailed semantics such as the locations of windows and doors. However, contemporary floorplan localization techniques mostly focus on matching depth-based structural cues, ignoring the rich semantics communicated within floorplans. In this work, we introduce a semantic-aware localization framework that jointly estimates depth and semantic rays, consolidating over both for predicting a structural-semantic probability volume. Our probability volume is constructed in a coarse-to-fine manner: We first sample a small set of rays to obtain an initial low-resolution probability volume. We then refine these probabilities by performing a denser sampling only in high-probability regions and process the refined values for predicting a 2D location and orientation angle. We conduct an evaluation on two standard floorplan localization benchmarks. Our experiments demonstrate that our approach substantially outperforms state-of-the-art methods, achieving significant improvements in recall metrics compared to prior works. Moreover, we show that our framework can easily incorporate additional metadata such as room labels, enabling additional gains in both accuracy and efficiency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 42,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09294",
        "abs_url": "https://arxiv.org/abs/2507.09294",
        "pdf_url": "https://arxiv.org/pdf/2507.09294",
        "title": "Geo-RepNet: Geometry-Aware Representation Learning for Surgical Phase Recognition in Endoscopic Submucosal Dissection",
        "authors": [
            "Rui Tang",
            "Haochen Yin",
            "Guankun Wang",
            "Long Bai",
            "An Wang",
            "Huxin Gao",
            "Jiazheng Wang",
            "Hongliang Ren"
        ],
        "comments": "IEEE ICIA 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
        "abstract": "Surgical phase recognition plays a critical role in developing intelligent assistance systems for minimally invasive procedures such as Endoscopic Submucosal Dissection (ESD). However, the high visual similarity across different phases and the lack of structural cues in RGB images pose significant challenges. Depth information offers valuable geometric cues that can complement appearance features by providing insights into spatial relationships and anatomical structures. In this paper, we pioneer the use of depth information for surgical phase recognition and propose Geo-RepNet, a geometry-aware convolutional framework that integrates RGB image and depth information to enhance recognition performance in complex surgical scenes. Built upon a re-parameterizable RepVGG backbone, Geo-RepNet incorporates the Depth-Guided Geometric Prior Generation (DGPG) module that extracts geometry priors from raw depth maps, and the Geometry-Enhanced Multi-scale Attention (GEMA) to inject spatial guidance through geometry-aware cross-attention and efficient multi-scale aggregation. To evaluate the effectiveness of our approach, we construct a nine-phase ESD dataset with dense frame-level annotations from real-world ESD videos. Extensive experiments on the proposed dataset demonstrate that Geo-RepNet achieves state-of-the-art performance while maintaining robustness and high computational efficiency under complex and low-texture surgical environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 43,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09299",
        "abs_url": "https://arxiv.org/abs/2507.09299",
        "pdf_url": "https://arxiv.org/pdf/2507.09299",
        "title": "ViT-ProtoNet for Few-Shot Image Classification: A Multi-Benchmark Evaluation",
        "authors": [
            "Abdulvahap Mutlu",
            "Şengül Doğan",
            "Türker Tuncer"
        ],
        "comments": "All codes are available at this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "The remarkable representational power of Vision Transformers (ViTs) remains underutilized in few-shot image classification. In this work, we introduce ViT-ProtoNet, which integrates a ViT-Small backbone into the Prototypical Network framework. By averaging class conditional token embeddings from a handful of support examples, ViT-ProtoNet constructs robust prototypes that generalize to novel categories under 5-shot settings. We conduct an extensive empirical evaluation on four standard benchmarks: Mini-ImageNet, FC100, CUB-200, and CIFAR-FS, including overlapped support variants to assess robustness. Across all splits, ViT-ProtoNet consistently outperforms CNN-based prototypical counterparts, achieving up to a 3.2\\% improvement in 5-shot accuracy and demonstrating superior feature separability in latent space. Furthermore, it outperforms or is competitive with transformer-based competitors using a more lightweight backbone. Comprehensive ablations examine the impact of transformer depth, patch size, and fine-tuning strategy. To foster reproducibility, we release code and pretrained weights. Our results establish ViT-ProtoNet as a powerful, flexible approach for few-shot classification and set a new baseline for transformer-based meta-learners.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 44,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09305",
        "abs_url": "https://arxiv.org/abs/2507.09305",
        "pdf_url": "https://arxiv.org/pdf/2507.09305",
        "title": "DAA*: Deep Angular A Star for Image-based Path Planning",
        "authors": [
            "Zhiwei Xu"
        ],
        "comments": "International Conference on Computer Vision (ICCV), 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Image and Video Processing (eess.IV)",
        "abstract": "Path smoothness is often overlooked in path imitation learning from expert demonstrations. In this paper, we introduce a novel learning method, termed deep angular A* (DAA*), by incorporating the proposed path angular freedom (PAF) into A* to improve path similarity through adaptive path smoothness. The PAF aims to explore the effect of move angles on path node expansion by finding the trade-off between their minimum and maximum values, allowing for high adaptiveness for imitation learning. DAA* improves path optimality by closely aligning with the reference path through joint optimization of path shortening and smoothing, which correspond to heuristic distance and PAF, respectively. Throughout comprehensive evaluations on 7 datasets, including 4 maze datasets, 2 video-game datasets, and a real-world drone-view dataset containing 2 scenarios, we demonstrate remarkable improvements of our DAA* over neural A* in path similarity between the predicted and reference paths with a shorter path length when the shortest path is plausible, improving by 9.0% SPR, 6.9% ASIM, and 3.9% PSIM. Furthermore, when jointly learning pathfinding with both path loss and path probability map loss, DAA* significantly outperforms the state-of-the-art TransPath by 6.7% SPR, 6.5% PSIM, and 3.7% ASIM. We also discuss the minor trade-off between path optimality and search efficiency where applicable.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 45,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09308",
        "abs_url": "https://arxiv.org/abs/2507.09308",
        "pdf_url": "https://arxiv.org/pdf/2507.09308",
        "title": "AlphaVAE: Unified End-to-End RGBA Image Reconstruction and Generation with Alpha-Aware Representation Learning",
        "authors": [
            "Zile Wang",
            "Hao Yu",
            "Jiabo Zhan",
            "Chun Yuan"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Recent advances in latent diffusion models have achieved remarkable results in high-fidelity RGB image synthesis by leveraging pretrained VAEs to compress and reconstruct pixel data at low computational cost. However, the generation of transparent or layered content (RGBA image) remains largely unexplored, due to the lack of large-scale benchmarks. In this work, we propose ALPHA, the first comprehensive RGBA benchmark that adapts standard RGB metrics to four-channel images via alpha blending over canonical backgrounds. We further introduce ALPHAVAE, a unified end-to-end RGBA VAE that extends a pretrained RGB VAE by incorporating a dedicated alpha channel. The model is trained with a composite objective that combines alpha-blended pixel reconstruction, patch-level fidelity, perceptual consistency, and dual KL divergence constraints to ensure latent fidelity across both RGB and alpha representations. Our RGBA VAE, trained on only 8K images in contrast to 1M used by prior methods, achieves a +4.9 dB improvement in PSNR and a +3.2% increase in SSIM over LayerDiffuse in reconstruction. It also enables superior transparent image generation when fine-tuned within a latent diffusion framework. Our code, data, and models are released on this https URL for reproducibility.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 46,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09313",
        "abs_url": "https://arxiv.org/abs/2507.09313",
        "pdf_url": "https://arxiv.org/pdf/2507.09313",
        "title": "ProactiveVideoQA: A Comprehensive Benchmark Evaluating Proactive Interactions in Video Large Language Models",
        "authors": [
            "Yueqian Wang",
            "Xiaojun Meng",
            "Yifan Wang",
            "Huishuai Zhang",
            "Dongyan Zhao"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "With the growing research focus on multimodal dialogue systems, the capability for proactive interaction is gradually gaining recognition. As an alternative to conventional turn-by-turn dialogue, users increasingly expect multimodal systems to be more initiative, for example, by autonomously determining the timing of multi-turn responses in real time during video playback. To facilitate progress in this emerging area, we introduce ProactiveVideoQA, the first comprehensive benchmark to evaluate a system's ability to engage in proactive interaction. Since model responses are generated at varying timestamps, we further propose PAUC, the first metric that accounts for the temporal dynamics of model responses. This enables a more accurate evaluation of systems operating in proactive settings. Through extensive benchmarking of various baseline systems on ProactiveVideoQA and a user study of human preferences, we show that PAUC is in better agreement with human preferences than traditional evaluation metrics, which typically only consider the textual content of responses. These findings demonstrate that PAUC provides a more faithful assessment of user experience in proactive interaction scenarios. Project homepage: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 47,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09323",
        "abs_url": "https://arxiv.org/abs/2507.09323",
        "pdf_url": "https://arxiv.org/pdf/2507.09323",
        "title": "Dynamic Inter-Class Confusion-Aware Encoder for Audio-Visual Fusion in Human Activity Recognition",
        "authors": [
            "Kaixuan Cong",
            "Yifan Wang",
            "Rongkun Xue",
            "Yuyang Jiang",
            "Yiming Feng",
            "Jing Yang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Humans do not understand individual events in isolation; rather, they generalize concepts within classes and compare them to others. Existing audio-video pre-training paradigms only focus on the alignment of the overall audio-video modalities, without considering the reinforcement of distinguishing easily confused classes through cognitive induction and contrast during training. This paper proposes the Dynamic Inter-Class Confusion-Aware Encoder (DICCAE), an encoder that aligns audio-video representations at a fine-grained, category-level. DICCAE addresses category confusion by dynamically adjusting the confusion loss based on inter-class confusion degrees, thereby enhancing the model's ability to distinguish between similar activities. To further extend the application of DICCAE, we also introduce a novel training framework that incorporates both audio and video modalities, as well as their fusion. To mitigate the scarcity of audio-video data in the human activity recognition task, we propose a cluster-guided audio-video self-supervised pre-training strategy for DICCAE. DICCAE achieves near state-of-the-art performance on the VGGSound dataset, with a top-1 accuracy of 65.5%. We further evaluate its feature representation quality through extensive ablation studies, validating the necessity of each module.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 48,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09334",
        "abs_url": "https://arxiv.org/abs/2507.09334",
        "pdf_url": "https://arxiv.org/pdf/2507.09334",
        "title": "Fast3D: Accelerating 3D Multi-modal Large Language Models for Efficient 3D Scene Understanding",
        "authors": [
            "Wencan Huang",
            "Daizong Liu",
            "Wei Hu"
        ],
        "comments": "Accepted by ACM MM 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "While 3D Multi-modal Large Language Models (MLLMs) demonstrate remarkable scene understanding capabilities, their practical deployment faces critical challenges due to computational inefficiency. The key bottleneck stems from processing excessive object-centric visual tokens required for comprehensive 3D scene representation. Although visual token pruning has shown promise in accelerating 2D MLLMs, its applicability to 3D domains remains largely unexplored due to fundamental disparities in token structures. In this paper, we reveal two critical insights: (1) Significant redundancy exists in object-level 3D token representations, analogous to patch-level redundancy in 2D systems; (2) Global attention patterns exhibit strong predictive power for identifying non-essential tokens in 3D contexts. Building on these observations, we propose Fast3D, a plug-and-play visual token pruning framework for 3D MLLMs featuring two technical innovations: (1) Global Attention Prediction (GAP), where a lightweight neural network learns to predict the global attention distributions of the target model, enabling efficient token importance estimation for precise pruning guidance; (2) Sample-Adaptive visual token Pruning (SAP), which introduces dynamic token budgets through attention-based complexity assessment, automatically adjusting layer-wise pruning ratios based on input characteristics. Both of these two techniques operate without modifying the parameters of the target model. Extensive evaluations across five benchmarks validate the effectiveness of Fast3D, particularly under high visual token pruning ratios. Code is available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 49,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09338",
        "abs_url": "https://arxiv.org/abs/2507.09338",
        "pdf_url": "https://arxiv.org/pdf/2507.09338",
        "title": "Simplifying Traffic Anomaly Detection with Video Foundation Models",
        "authors": [
            "Svetlana Orlova",
            "Tommie Kerssies",
            "Brunó B. Englert",
            "Gijs Dubbelman"
        ],
        "comments": "ICCVW 2025 accepted. Code: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Recent methods for ego-centric Traffic Anomaly Detection (TAD) often rely on complex multi-stage or multi-representation fusion architectures, yet it remains unclear whether such complexity is necessary. Recent findings in visual perception suggest that foundation models, enabled by advanced pre-training, allow simple yet flexible architectures to outperform specialized designs. Therefore, in this work, we investigate an architecturally simple encoder-only approach using plain Video Vision Transformers (Video ViTs) and study how pre-training enables strong TAD performance. We find that: (i) strong pre-training enables simple encoder-only models to match or even surpass the performance of specialized state-of-the-art TAD methods, while also being significantly more efficient; (ii) although weakly- and fully-supervised pre-training are advantageous on standard benchmarks, we find them less effective for TAD. Instead, self-supervised Masked Video Modeling (MVM) provides the strongest signal; and (iii) Domain-Adaptive Pre-Training (DAPT) on unlabeled driving videos further improves downstream performance, without requiring anomalous examples. Our findings highlight the importance of pre-training and show that effective, efficient, and scalable TAD models can be built with minimal architectural complexity. We release our code, domain-adapted encoders, and fine-tuned models to support future work: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 50,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09375",
        "abs_url": "https://arxiv.org/abs/2507.09375",
        "pdf_url": "https://arxiv.org/pdf/2507.09375",
        "title": "Automated Multi-Class Crop Pathology Classification via Convolutional Neural Networks: A Deep Learning Approach for Real-Time Precision Agriculture",
        "authors": [
            "Sourish Suri",
            "Yifei Shao"
        ],
        "comments": "29 pages, 10 figures, 1 table. Code available at: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Crop diseases present a significant barrier to agricultural productivity and global food security, especially in large-scale farming where early identification is often delayed or inaccurate. This research introduces a Convolutional Neural Network (CNN)-based image classification system designed to automate the detection and classification of eight common crop diseases using leaf imagery. The methodology involves a complete deep learning pipeline: image acquisition from a large, labeled dataset, preprocessing via resizing, normalization, and augmentation, and model training using TensorFlow with Keras' Sequential API. The CNN architecture comprises three convolutional layers with increasing filter sizes and ReLU activations, followed by max pooling, flattening, and fully connected layers, concluding with a softmax output for multi-class classification. The system achieves high training accuracy (~90%) and demonstrates reliable performance on unseen data, although a validation accuracy of ~60% suggests minor overfitting. Notably, the model integrates a treatment recommendation module, providing actionable guidance by mapping each detected disease to suitable pesticide or fungicide interventions. Furthermore, the solution is deployed on an open-source, mobile-compatible platform, enabling real-time image-based diagnostics for farmers in remote areas. This research contributes a scalable and accessible tool to the field of precision agriculture, reducing reliance on manual inspection and promoting sustainable disease management practices. By merging deep learning with practical agronomic support, this work underscores the potential of CNNs to transform crop health monitoring and enhance food production resilience on a global scale.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 51,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09410",
        "abs_url": "https://arxiv.org/abs/2507.09410",
        "pdf_url": "https://arxiv.org/pdf/2507.09410",
        "title": "GreenCrossingAI: A Camera Trap/Computer Vision Pipeline for Environmental Science Research Groups",
        "authors": [
            "Bernie Boscoe",
            "Shawn Johnson",
            "Andrea Osborn",
            "Chandler Campbell",
            "Karen Mager"
        ],
        "comments": "This is the preprint version of the paper in Practice and Experience in Advanced Research Computing, PEARC25",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Camera traps have long been used by wildlife researchers to monitor and study animal behavior, population dynamics, habitat use, and species diversity in a non-invasive and efficient manner. While data collection from the field has increased with new tools and capabilities, methods to develop, process, and manage the data, especially the adoption of ML/AI tools, remain challenging. These challenges include the sheer volume of data generated, the need for accurate labeling and annotation, variability in environmental conditions affecting data quality, and the integration of ML/AI tools into existing workflows that often require domain-specific customization and computational resources. This paper provides a guide to a low-resource pipeline to process camera trap data on-premise, incorporating ML/AI capabilities tailored for small research groups with limited resources and computational expertise. By focusing on practical solutions, the pipeline offers accessible approaches for data transmission, inference, and evaluation, enabling researchers to discover meaningful insights from their ever-increasing camera trap datasets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 52,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09420",
        "abs_url": "https://arxiv.org/abs/2507.09420",
        "pdf_url": "https://arxiv.org/pdf/2507.09420",
        "title": "Domain Adaptation and Multi-view Attention for Learnable Landmark Tracking with Sparse Data",
        "authors": [
            "Timothy Chase Jr",
            "Karthik Dantu"
        ],
        "comments": "Presented at the RSS Space Robotics Workshop 2025. Poster available online at this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)",
        "abstract": "The detection and tracking of celestial surface terrain features are crucial for autonomous spaceflight applications, including Terrain Relative Navigation (TRN), Entry, Descent, and Landing (EDL), hazard analysis, and scientific data collection. Traditional photoclinometry-based pipelines often rely on extensive a priori imaging and offline processing, constrained by the computational limitations of radiation-hardened systems. While historically effective, these approaches typically increase mission costs and duration, operate at low processing rates, and have limited generalization. Recently, learning-based computer vision has gained popularity to enhance spacecraft autonomy and overcome these limitations. While promising, emerging techniques frequently impose computational demands exceeding the capabilities of typical spacecraft hardware for real-time operation and are further challenged by the scarcity of labeled training data for diverse extraterrestrial environments. In this work, we present novel formulations for in-situ landmark tracking via detection and description. We utilize lightweight, computationally efficient neural network architectures designed for real-time execution on current-generation spacecraft flight processors. For landmark detection, we propose improved domain adaptation methods that enable the identification of celestial terrain features with distinct, cheaply acquired training data. Concurrently, for landmark description, we introduce a novel attention alignment formulation that learns robust feature representations that maintain correspondence despite significant landmark viewpoint variations. Together, these contributions form a unified system for landmark tracking that demonstrates superior performance compared to existing state-of-the-art techniques.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 53,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09446",
        "abs_url": "https://arxiv.org/abs/2507.09446",
        "pdf_url": "https://arxiv.org/pdf/2507.09446",
        "title": "Efficient Multi-Person Motion Prediction by Lightweight Spatial and Temporal Interactions",
        "authors": [
            "Yuanhong Zheng",
            "Ruixuan Yu",
            "Jian Sun"
        ],
        "comments": "ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "3D multi-person motion prediction is a highly complex task, primarily due to the dependencies on both individual past movements and the interactions between agents. Moreover, effectively modeling these interactions often incurs substantial computational costs. In this work, we propose a computationally efficient model for multi-person motion prediction by simplifying spatial and temporal interactions. Our approach begins with the design of lightweight dual branches that learn local and global representations for individual and multiple persons separately. Additionally, we introduce a novel cross-level interaction block to integrate the spatial and temporal representations from both branches. To further enhance interaction modeling, we explicitly incorporate the spatial inter-person distance embedding. With above efficient temporal and spatial design, we achieve state-of-the-art performance for multiple metrics on standard datasets of CMU-Mocap, MuPoTS-3D, and 3DPW, while significantly reducing the computational cost. Code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 54,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09459",
        "abs_url": "https://arxiv.org/abs/2507.09459",
        "pdf_url": "https://arxiv.org/pdf/2507.09459",
        "title": "SegVec3D: A Method for Vector Embedding of 3D Objects Oriented Towards Robot manipulation",
        "authors": [
            "Zhihan Kang",
            "Boyu Wang"
        ],
        "comments": "Undergraduate Theis; 12 pages, 6 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
        "abstract": "We propose SegVec3D, a novel framework for 3D point cloud instance segmentation that integrates attention mechanisms, embedding learning, and cross-modal alignment. The approach builds a hierarchical feature extractor to enhance geometric structure modeling and enables unsupervised instance segmentation via contrastive clustering. It further aligns 3D data with natural language queries in a shared semantic space, supporting zero-shot retrieval. Compared to recent methods like Mask3D and ULIP, our method uniquely unifies instance segmentation and multimodal understanding with minimal supervision and practical deployability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 55,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09471",
        "abs_url": "https://arxiv.org/abs/2507.09471",
        "pdf_url": "https://arxiv.org/pdf/2507.09471",
        "title": "CKAA: Cross-subspace Knowledge Alignment and Aggregation for Robust Continual Learning",
        "authors": [
            "Lingfeng He",
            "De Cheng",
            "Zhiheng Ma",
            "Huaijie Wang",
            "Dingwen Zhang",
            "Nannan Wang",
            "Xinbo Gao"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Continual Learning (CL) empowers AI models to continuously learn from sequential task streams. Recently, parameter-efficient fine-tuning (PEFT)-based CL methods have garnered increasing attention due to their superior performance. They typically allocate a unique sub-module for learning each task, with a task recognizer to select the appropriate sub-modules for testing images. However, due to the feature subspace misalignment from independently trained sub-modules, these methods tend to produce ambiguous decisions under misleading task-ids. To address this, we propose Cross-subspace Knowledge Alignment and Aggregation (CKAA), a novel framework that enhances model robustness against misleading task-ids through two key innovations: (1) Dual-level Knowledge Alignment (DKA): By aligning intra-class feature distributions across different subspaces and learning a robust global classifier through a feature simulation process, DKA enables the model to distinguish features from both correct and incorrect subspaces during training. (2) Task-Confidence-guided Mixture of Adapters (TC-MoA): A robust inference scheme that adaptively aggregates task-specific knowledge from relevant sub-modules based on task-confidence scores, avoiding overconfidence in misleading task-id predictions. Extensive experiments demonstrate that CKAA outperforms existing PEFT-based CL methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 56,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09487",
        "abs_url": "https://arxiv.org/abs/2507.09487",
        "pdf_url": "https://arxiv.org/pdf/2507.09487",
        "title": "HMID-Net: An Exploration of Masked Image Modeling and Knowledge Distillation in Hyperbolic Space",
        "authors": [
            "Changli Wang",
            "Fang Yin",
            "Jiafeng Liu",
            "Rui Wu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Visual and semantic concepts are often structured in a hierarchical manner. For instance, textual concept `cat' entails all images of cats. A recent study, MERU, successfully adapts multimodal learning techniques from Euclidean space to hyperbolic space, effectively capturing the visual-semantic hierarchy. However, a critical question remains: how can we more efficiently train a model to capture and leverage this hierarchy? In this paper, we propose the \\textit{Hyperbolic Masked Image and Distillation Network} (HMID-Net), a novel and efficient method that integrates Masked Image Modeling (MIM) and knowledge distillation techniques within hyperbolic space. To the best of our knowledge, this is the first approach to leverage MIM and knowledge distillation in hyperbolic space to train highly efficient models. In addition, we introduce a distillation loss function specifically designed to facilitate effective knowledge transfer in hyperbolic space. Our experiments demonstrate that MIM and knowledge distillation techniques in hyperbolic space can achieve the same remarkable success as in Euclidean space. Extensive evaluations show that our method excels across a wide range of downstream tasks, significantly outperforming existing models like MERU and CLIP in both image classification and retrieval.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 57,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09491",
        "abs_url": "https://arxiv.org/abs/2507.09491",
        "pdf_url": "https://arxiv.org/pdf/2507.09491",
        "title": "GLIMPSE: Do Large Vision-Language Models Truly Think With Videos or Just Glimpse at Them?",
        "authors": [
            "Yiyang Zhou",
            "Linjie Li",
            "Shi Qiu",
            "Zhengyuan Yang",
            "Yuyang Zhao",
            "Siwei Han",
            "Yangfan He",
            "Kangqi Li",
            "Haonian Ji",
            "Zihao Zhao",
            "Haibo Tong",
            "Lijuan Wang",
            "Huaxiu Yao"
        ],
        "comments": "15 pages, 10 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Existing video benchmarks often resemble image-based benchmarks, with question types like \"What actions does the person perform throughout the video?\" or \"What color is the woman's dress in the video?\" For these, models can often answer by scanning just a few key frames, without deep temporal reasoning. This limits our ability to assess whether large vision-language models (LVLMs) can truly think with videos rather than perform superficial frame-level analysis. To address this, we introduce GLIMPSE, a benchmark specifically designed to evaluate whether LVLMs can genuinely think with videos. Unlike prior benchmarks, GLIMPSE emphasizes comprehensive video understanding beyond static image cues. It consists of 3,269 videos and over 4,342 highly visual-centric questions across 11 categories, including Trajectory Analysis, Temporal Reasoning, and Forensics Detection. All questions are carefully crafted by human annotators and require watching the entire video and reasoning over full video context-this is what we mean by thinking with video. These questions cannot be answered by scanning selected frames or relying on text alone. In human evaluations, GLIMPSE achieves 94.82% accuracy, but current LVLMs face significant challenges. Even the best-performing model, GPT-o3, reaches only 66.43%, highlighting that LVLMs still struggle to move beyond surface-level reasoning to truly think with videos.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 58,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09492",
        "abs_url": "https://arxiv.org/abs/2507.09492",
        "pdf_url": "https://arxiv.org/pdf/2507.09492",
        "title": "SDTN and TRN: Adaptive Spectral-Spatial Feature Extraction for Hyperspectral Image Classification",
        "authors": [
            "Fuyin Ye",
            "Erwen Yao",
            "Jianyong Chen",
            "Fengmei He",
            "Junxiang Zhang",
            "Lihao Ni"
        ],
        "comments": "4 pages, 2 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Hyperspectral image classification plays a pivotal role in precision agriculture, providing accurate insights into crop health monitoring, disease detection, and soil analysis. However, traditional methods struggle with high-dimensional data, spectral-spatial redundancy, and the scarcity of labeled samples, often leading to suboptimal performance. To address these challenges, we propose the Self-Adaptive Tensor- Regularized Network (SDTN), which combines tensor decomposition with regularization mechanisms to dynamically adjust tensor ranks, ensuring optimal feature representation tailored to the complexity of the data. Building upon SDTN, we propose the Tensor-Regularized Network (TRN), which integrates the features extracted by SDTN into a lightweight network capable of capturing spectral-spatial features at multiple scales. This approach not only maintains high classification accuracy but also significantly reduces computational complexity, making the framework highly suitable for real-time deployment in resource-constrained environments. Experiments on PaviaU datasets demonstrate significant improvements in accuracy and reduced model parameters compared to state-of-the-art methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 59,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09500",
        "abs_url": "https://arxiv.org/abs/2507.09500",
        "pdf_url": "https://arxiv.org/pdf/2507.09500",
        "title": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under Visual Variations",
        "authors": [
            "Yiwen Liang",
            "Hui Chen",
            "Yizhe Xiong",
            "Zihan Zhou",
            "Mengyao Lyu",
            "Zijia Lin",
            "Shuaicheng Niu",
            "Sicheng Zhao",
            "Jungong Han",
            "Guiguang Ding"
        ],
        "comments": "Accepted at the 33rd ACM International Conference on Multimedia(ACM MM 2025)",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but struggle with distribution shifts in downstream tasks when labeled data is unavailable, which has motivated the development of Test-Time Adaptation (TTA) to improve VLMs' performance during inference without annotations. Among various TTA approaches, cache-based methods show promise by preserving historical knowledge from low-entropy samples in a dynamic cache and fostering efficient adaptation. However, these methods face two critical reliability challenges: (1) entropy often becomes unreliable under distribution shifts, causing error accumulation in the cache and degradation in adaptation performance; (2) the final predictions may be unreliable due to inflexible decision boundaries that fail to accommodate large downstream shifts. To address these challenges, we propose a Reliable Test-time Adaptation (ReTA) method that integrates two complementary strategies to enhance reliability from two perspectives. First, to mitigate the unreliability of entropy as a sample selection criterion for cache construction, we introduce Consistency-aware Entropy Reweighting (CER), which incorporates consistency constraints to weight entropy during cache updating. While conventional approaches rely solely on low entropy for cache prioritization and risk introducing noise, our method leverages predictive consistency to maintain a high-quality cache and facilitate more robust adaptation. Second, we present Diversity-driven Distribution Calibration (DDC), which models class-wise text embeddings as multivariate Gaussian distributions, enabling adaptive decision boundaries for more accurate predictions across visually diverse content. Extensive experiments demonstrate that ReTA consistently outperforms state-of-the-art methods, particularly under challenging real-world distribution shifts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 60,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09512",
        "abs_url": "https://arxiv.org/abs/2507.09512",
        "pdf_url": "https://arxiv.org/pdf/2507.09512",
        "title": "Online Micro-gesture Recognition Using Data Augmentation and Spatial-Temporal Attention",
        "authors": [
            "Pengyu Liu",
            "Kun Li",
            "Fei Wang",
            "Yanyan Wei",
            "Junhui She",
            "Dan Guo"
        ],
        "comments": "11 pages, 4 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In this paper, we introduce the latest solution developed by our team, HFUT-VUT, for the Micro-gesture Online Recognition track of the IJCAI 2025 MiGA Challenge. The Micro-gesture Online Recognition task is a highly challenging problem that aims to locate the temporal positions and recognize the categories of multiple micro-gesture instances in untrimmed videos. Compared to traditional temporal action detection, this task places greater emphasis on distinguishing between micro-gesture categories and precisely identifying the start and end times of each instance. Moreover, micro-gestures are typically spontaneous human actions, with greater differences than those found in other human actions. To address these challenges, we propose hand-crafted data augmentation and spatial-temporal attention to enhance the model's ability to classify and localize micro-gestures more accurately. Our solution achieved an F1 score of 38.03, outperforming the previous state-of-the-art by 37.9%. As a result, our method ranked first in the Micro-gesture Online Recognition track.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 61,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09514",
        "abs_url": "https://arxiv.org/abs/2507.09514",
        "pdf_url": "https://arxiv.org/pdf/2507.09514",
        "title": "QuarterMap: Efficient Post-Training Token Pruning for Visual State Space Models",
        "authors": [
            "Tien-Yu Chi",
            "Hung-Yueh Chiang",
            "Diana Marculescu",
            "Kai-Chiang Wu"
        ],
        "comments": "Accepted by Efficient Systems for Foundation Models Workshop at the International Conference on Machine Learning (ICML) 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "State space models (SSMs) reduce the quadratic complexity of transformers by leveraging linear recurrence. Recently, VMamba has emerged as a strong SSM-based vision backbone, yet remains bottlenecked by spatial redundancy in its four-directional scan. We propose QuarterMap, a post-training activation pruning method that removes redundant spatial activations before scanning and restores dimensions via nearest-neighbor upsampling. Our method improves throughput without retraining. On ImageNet-1K, QuarterMap achieves up to 11% speedup on VMamba with less than 0.9% accuracy drop, and yields similar gains on ADE20K segmentation. Beyond VMamba, we validate QuarterMap on MedMamba, a domain-specific model that shares the same four-directional scanning structure, where it consistently improves throughput while preserving accuracy across multiple medical imaging tasks. Compared to token merging methods like ToMe, QuarterMap is tailored for SSMs and avoids costly merge-unmerge operations. Our method offers a plug-and-play tool for deployment-time efficiency without compromising transferability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 62,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09524",
        "abs_url": "https://arxiv.org/abs/2507.09524",
        "pdf_url": "https://arxiv.org/pdf/2507.09524",
        "title": "When Schrödinger Bridge Meets Real-World Image Dehazing with Unpaired Training",
        "authors": [
            "Yunwei Lan",
            "Zhigao Cui",
            "Xin Luo",
            "Chang Liu",
            "Nian Wang",
            "Menglin Zhang",
            "Yanzhao Su",
            "Dong Liu"
        ],
        "comments": "Accepted by ICCV2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Recent advancements in unpaired dehazing, particularly those using GANs, show promising performance in processing real-world hazy images. However, these methods tend to face limitations due to the generator's limited transport mapping capability, which hinders the full exploitation of their effectiveness in unpaired training paradigms. To address these challenges, we propose DehazeSB, a novel unpaired dehazing framework based on the Schrödinger Bridge. By leveraging optimal transport (OT) theory, DehazeSB directly bridges the distributions between hazy and clear images. This enables optimal transport mappings from hazy to clear images in fewer steps, thereby generating high-quality results. To ensure the consistency of structural information and details in the restored images, we introduce detail-preserving regularization, which enforces pixel-level alignment between hazy inputs and dehazed outputs. Furthermore, we propose a novel prompt learning to leverage pre-trained CLIP models in distinguishing hazy images and clear ones, by learning a haze-aware vision-language alignment. Extensive experiments on multiple real-world datasets demonstrate our method's superiority. Code: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 63,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09531",
        "abs_url": "https://arxiv.org/abs/2507.09531",
        "pdf_url": "https://arxiv.org/pdf/2507.09531",
        "title": "VDInstruct: Zero-Shot Key Information Extraction via Content-Aware Vision Tokenization",
        "authors": [
            "Son Nguyen",
            "Giang Nguyen",
            "Hung Dao",
            "Thao Do",
            "Daeyoung Kim"
        ],
        "comments": "Under Review",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Key Information Extraction (KIE) underpins the understanding of visual documents (e.g., receipts and contracts) by extracting precise semantic content and accurately capturing spatial structure. Yet existing multimodal large language models (MLLMs) often perform poorly on dense documents and rely on vision tokenization approaches that scale with image size, leading to redundant computation and memory inefficiency. To address these challenges, we introduce VDInstruct, an MLLM that separates spatial region detection from semantic feature extraction. Central to our model is a content-aware tokenization strategy: rather than fragmenting the entire image uniformly, it generates tokens in proportion to document complexity, preserving critical structure while eliminating wasted tokens. Leveraging a three-stage training paradigm, our model achieves state-of-the-art (SOTA) results on KIE benchmarks, matching or exceeding the accuracy of leading approaches while reducing the number of image tokens by roughly 3.6x. In zero-shot evaluations, VDInstruct surpasses strong baselines-such as DocOwl 1.5-by +5.5 F1 points, highlighting its robustness to unseen documents. These findings show that content-aware tokenization combined with explicit layout modeling offers a promising direction forward for document understanding. Data, source code, and model weights will be made publicly available.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 64,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09541",
        "abs_url": "https://arxiv.org/abs/2507.09541",
        "pdf_url": "https://arxiv.org/pdf/2507.09541",
        "title": "DRPCA-Net: Make Robust PCA Great Again for Infrared Small Target Detection",
        "authors": [
            "Zihao Xiong",
            "Fei Zhou",
            "Fengyi Wu",
            "Shuai Yuan",
            "Maixia Fu",
            "Zhenming Peng",
            "Jian Yang",
            "Yimian Dai"
        ],
        "comments": "Accepted by TGRS",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Infrared small target detection plays a vital role in remote sensing, industrial monitoring, and various civilian applications. Despite recent progress powered by deep learning, many end-to-end convolutional models tend to pursue performance by stacking increasingly complex architectures, often at the expense of interpretability, parameter efficiency, and generalization. These models typically overlook the intrinsic sparsity prior of infrared small targets--an essential cue that can be explicitly modeled for both performance and efficiency gains. To address this, we revisit the model-based paradigm of Robust Principal Component Analysis (RPCA) and propose Dynamic RPCA Network (DRPCA-Net), a novel deep unfolding network that integrates the sparsity-aware prior into a learnable architecture. Unlike conventional deep unfolding methods that rely on static, globally learned parameters, DRPCA-Net introduces a dynamic unfolding mechanism via a lightweight hypernetwork. This design enables the model to adaptively generate iteration-wise parameters conditioned on the input scene, thereby enhancing its robustness and generalization across diverse backgrounds. Furthermore, we design a Dynamic Residual Group (DRG) module to better capture contextual variations within the background, leading to more accurate low-rank estimation and improved separation of small targets. Extensive experiments on multiple public infrared datasets demonstrate that DRPCA-Net significantly outperforms existing state-of-the-art methods in detection accuracy. Code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 65,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09556",
        "abs_url": "https://arxiv.org/abs/2507.09556",
        "pdf_url": "https://arxiv.org/pdf/2507.09556",
        "title": "SeqCSIST: Sequential Closely-Spaced Infrared Small Target Unmixing",
        "authors": [
            "Ximeng Zhai",
            "Bohan Xu",
            "Yaohong Chen",
            "Hao Wang",
            "Kehua Guo",
            "Yimian Dai"
        ],
        "comments": "Accepted by TGRS",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Due to the limitation of the optical lens focal length and the resolution of the infrared detector, distant Closely-Spaced Infrared Small Target (CSIST) groups typically appear as mixing spots in the infrared image. In this paper, we propose a novel task, Sequential CSIST Unmixing, namely detecting all targets in the form of sub-pixel localization from a highly dense CSIST group. However, achieving such precise detection is an extremely difficult challenge. In addition, the lack of high-quality public datasets has also restricted the research progress. To this end, firstly, we contribute an open-source ecosystem, including SeqCSIST, a sequential benchmark dataset, and a toolkit that provides objective evaluation metrics for this special task, along with the implementation of 23 relevant methods. Furthermore, we propose the Deformable Refinement Network (DeRefNet), a model-driven deep learning framework that introduces a Temporal Deformable Feature Alignment (TDFA) module enabling adaptive inter-frame information aggregation. To the best of our knowledge, this work is the first endeavor to address the CSIST Unmixing task within a multi-frame paradigm. Experiments on the SeqCSIST dataset demonstrate that our method outperforms the state-of-the-art approaches with mean Average Precision (mAP) metric improved by 5.3\\%. Our dataset and toolkit are available from this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 66,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09560",
        "abs_url": "https://arxiv.org/abs/2507.09560",
        "pdf_url": "https://arxiv.org/pdf/2507.09560",
        "title": "EHPE: A Segmented Architecture for Enhanced Hand Pose Estimation",
        "authors": [
            "Bolun Zheng",
            "Xinjie Liu",
            "Qianyu Zhang",
            "Canjin Wang",
            "Fangni Chen",
            "Mingen Xu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "3D hand pose estimation has garnered great attention in recent years due to its critical applications in human-computer interaction, virtual reality, and related fields. The accurate estimation of hand joints is essential for high-quality hand pose estimation. However, existing methods neglect the importance of Distal Phalanx Tip (TIP) and Wrist in predicting hand joints overall and often fail to account for the phenomenon of error accumulation for distal joints in gesture estimation, which can cause certain joints to incur larger errors, resulting in misalignments and artifacts in the pose estimation and degrading the overall reconstruction quality. To address this challenge, we propose a novel segmented architecture for enhanced hand pose estimation (EHPE). We perform local extraction of TIP and wrist, thus alleviating the effect of error accumulation on TIP prediction and further reduce the predictive errors for all joints on this basis. EHPE consists of two key stages: In the TIP and Wrist Joints Extraction stage (TW-stage), the positions of the TIP and wrist joints are estimated to provide an initial accurate joint configuration; In the Prior Guided Joints Estimation stage (PG-stage), a dual-branch interaction network is employed to refine the positions of the remaining joints. Extensive experiments on two widely used benchmarks demonstrate that EHPE achieves state-of-the-arts performance. Code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 67,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09562",
        "abs_url": "https://arxiv.org/abs/2507.09562",
        "pdf_url": "https://arxiv.org/pdf/2507.09562",
        "title": "Prompt Engineering in Segment Anything Model: Methodologies, Applications, and Emerging Challenges",
        "authors": [
            "Yidong Jiang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "The Segment Anything Model (SAM) has revolutionized image segmentation through its innovative prompt-based approach, yet the critical role of prompt engineering in its success remains underexplored. This paper presents the first comprehensive survey focusing specifically on prompt engineering techniques for SAM and its variants. We systematically organize and analyze the rapidly growing body of work in this emerging field, covering fundamental methodologies, practical applications, and key challenges. Our review reveals how prompt engineering has evolved from simple geometric inputs to sophisticated multimodal approaches, enabling SAM's adaptation across diverse domains including medical imaging and remote sensing. We identify unique challenges in prompt optimization and discuss promising research directions. This survey fills an important gap in the literature by providing a structured framework for understanding and advancing prompt engineering in foundation models for segmentation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 68,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09573",
        "abs_url": "https://arxiv.org/abs/2507.09573",
        "pdf_url": "https://arxiv.org/pdf/2507.09573",
        "title": "WordCraft: Interactive Artistic Typography with Attention Awareness and Noise Blending",
        "authors": [
            "Zhe Wang",
            "Jingbo Zhang",
            "Tianyi Wei",
            "Wanchao Su",
            "Can Wang"
        ],
        "comments": "14 pages, 16 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Artistic typography aims to stylize input characters with visual effects that are both creative and legible. Traditional approaches rely heavily on manual design, while recent generative models, particularly diffusion-based methods, have enabled automated character stylization. However, existing solutions remain limited in interactivity, lacking support for localized edits, iterative refinement, multi-character composition, and open-ended prompt interpretation. We introduce WordCraft, an interactive artistic typography system that integrates diffusion models to address these limitations. WordCraft features a training-free regional attention mechanism for precise, multi-region generation and a noise blending that supports continuous refinement without compromising visual quality. To support flexible, intent-driven generation, we incorporate a large language model to parse and structure both concrete and abstract user prompts. These components allow our framework to synthesize high-quality, stylized typography across single- and multi-character inputs across multiple languages, supporting diverse user-centered workflows. Our system significantly enhances interactivity in artistic typography synthesis, opening up creative possibilities for artists and designers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 69,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09574",
        "abs_url": "https://arxiv.org/abs/2507.09574",
        "pdf_url": "https://arxiv.org/pdf/2507.09574",
        "title": "MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive Vision Generation Models",
        "authors": [
            "Haozhe Zhao",
            "Zefan Cai",
            "Shuzheng Si",
            "Liang Chen",
            "Jiuxiang Gu",
            "Wen Xiao",
            "Junjie Hu"
        ],
        "comments": "24 pages,12 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Recent text-to-image models produce high-quality results but still struggle with precise visual control, balancing multimodal inputs, and requiring extensive training for complex multimodal image generation. To address these limitations, we propose MENTOR, a novel autoregressive (AR) framework for efficient Multimodal-conditioned Tuning for Autoregressive multimodal image generation. MENTOR combines an AR image generator with a two-stage training paradigm, enabling fine-grained, token-level alignment between multimodal inputs and image outputs without relying on auxiliary adapters or cross-attention modules. The two-stage training consists of: (1) a multimodal alignment stage that establishes robust pixel- and semantic-level alignment, followed by (2) a multimodal instruction tuning stage that balances the integration of multimodal inputs and enhances generation controllability. Despite modest model size, suboptimal base components, and limited training resources, MENTOR achieves strong performance on the DreamBench++ benchmark, outperforming competitive baselines in concept preservation and prompt following. Additionally, our method delivers superior image reconstruction fidelity, broad task adaptability, and improved training efficiency compared to diffusion-based methods. Dataset, code, and models are available at: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 70,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09577",
        "abs_url": "https://arxiv.org/abs/2507.09577",
        "pdf_url": "https://arxiv.org/pdf/2507.09577",
        "title": "Memory-Augmented SAM2 for Training-Free Surgical Video Segmentation",
        "authors": [
            "Ming Yin",
            "Fu Wang",
            "Xujiong Ye",
            "Yanda Meng",
            "Zeyu Fu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Surgical video segmentation is a critical task in computer-assisted surgery, essential for enhancing surgical quality and patient outcomes. Recently, the Segment Anything Model 2 (SAM2) framework has demonstrated remarkable advancements in both image and video segmentation. However, the inherent limitations of SAM2's greedy selection memory design are amplified by the unique properties of surgical videos-rapid instrument movement, frequent occlusion, and complex instrument-tissue interaction-resulting in diminished performance in the segmentation of complex, long videos. To address these challenges, we introduce Memory Augmented (MA)-SAM2, a training-free video object segmentation strategy, featuring novel context-aware and occlusion-resilient memory models. MA-SAM2 exhibits strong robustness against occlusions and interactions arising from complex instrument movements while maintaining accuracy in segmenting objects throughout videos. Employing a multi-target, single-loop, one-prompt inference further enhances the efficiency of the tracking process in multi-instrument videos. Without introducing any additional parameters or requiring further training, MA-SAM2 achieved performance improvements of 4.36% and 6.1% over SAM2 on the EndoVis2017 and EndoVis2018 datasets, respectively, demonstrating its potential for practical surgical applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 71,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09595",
        "abs_url": "https://arxiv.org/abs/2507.09595",
        "pdf_url": "https://arxiv.org/pdf/2507.09595",
        "title": "Demystifying Flux Architecture",
        "authors": [
            "Or Greenberg"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "FLUX.1 is a diffusion-based text-to-image generation model developed by Black Forest Labs, designed to achieve faithful text-image alignment while maintaining high image quality and diversity. FLUX is considered state-of-the-art in text-to-image generation, outperforming popular models such as Midjourney, DALL-E 3, Stable Diffusion 3 (SD3), and SDXL. Although publicly available as open source, the authors have not released official technical documentation detailing the model's architecture or training setup. This report summarizes an extensive reverse-engineering effort aimed at demystifying FLUX's architecture directly from its source code, to support its adoption as a backbone for future research and development. This document is an unofficial technical report and is not published or endorsed by the original developers or their affiliated institutions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 72,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09612",
        "abs_url": "https://arxiv.org/abs/2507.09612",
        "pdf_url": "https://arxiv.org/pdf/2507.09612",
        "title": "Inter2Former: Dynamic Hybrid Attention for Efficient High-Precision Interactive",
        "authors": [
            "You Huang",
            "Lichao Chen",
            "Jiayi Ji",
            "Liujuan Cao",
            "Shengchuan Zhang",
            "Rongrong Ji"
        ],
        "comments": "Accepted by ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Interactive segmentation (IS) improves annotation efficiency by segmenting target regions from user prompts, with widespread applications in real-world scenarios. Current approaches face a critical trade-off: dense-token methods achieve superior accuracy and detail preservation but suffer from prohibitively slow processing on CPU devices, while the Segment Anything Model (SAM) advances the field with sparse prompt tokens for fast inference but compromises segmentation quality. In this paper, we propose Inter2Former to address this challenge by optimizing computation allocation in dense-token processing, which introduces four key enhancements. First, we propose Dynamic Prompt Embedding (DPE) that adaptively processes only regions of interest while avoiding additional overhead from background tokens. Second, we introduce Dynamic Hybrid Attention (DHA), which leverages previous segmentation masks to route tokens through either full attention (O(N2)) for boundary regions or our proposed efficient BSQ attention (O(N)) for non-boundary regions. Third, we develop Hybrid Mixture of Experts (HMoE), which applies similar adaptive computation strategies in FFN modules with CPU-optimized parallel processing. Finally, we present Dynamic Local Upsampling (DLU), a reverse operation of DPE, which localizes objects with a lightweight MLP and performs fine-grained upsampling only in detected regions. Experimental results on high-precision IS benchmarks demonstrate that Inter2Former achieves SOTA performance with high efficiency on CPU devices.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 73,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09615",
        "abs_url": "https://arxiv.org/abs/2507.09615",
        "pdf_url": "https://arxiv.org/pdf/2507.09615",
        "title": "Towards Fine-Grained Adaptation of CLIP via a Self-Trained Alignment Score",
        "authors": [
            "Eman Ali",
            "Sathira Silva",
            "Chetan Arora",
            "Muhammad Haris Khan"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Vision-language models (VLMs) like CLIP excel in zero-shot learning by aligning image and text representations through contrastive pretraining. Existing approaches to unsupervised adaptation (UA) for fine-grained classification with VLMs either rely on fixed alignment scores that cannot capture evolving, subtle class distinctions or use computationally expensive pseudo-labeling strategies that limit scalability. In contrast, we show that modeling fine-grained cross-modal interactions during adaptation produces more accurate, class-discriminative pseudo-labels and substantially improves performance over state-of-the-art (SOTA) methods. We introduce Fine-grained Alignment and Interaction Refinement (FAIR), an innovative approach that dynamically aligns localized image features with descriptive language embeddings through a set of Class Description Anchors (CDA). This enables the definition of a Learned Alignment Score (LAS), which incorporates CDA as an adaptive classifier, facilitating cross-modal interactions to improve self-training in unsupervised adaptation. Furthermore, we propose a self-training weighting mechanism designed to refine pseudo-labels in the presence of inter-class ambiguities. Our approach, FAIR, delivers a substantial performance boost in fine-grained unsupervised adaptation, achieving a notable overall gain of 2.78% across 13 fine-grained datasets compared to SOTA methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 74,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09619",
        "abs_url": "https://arxiv.org/abs/2507.09619",
        "pdf_url": "https://arxiv.org/pdf/2507.09619",
        "title": "Generate Aligned Anomaly: Region-Guided Few-Shot Anomaly Image-Mask Pair Synthesis for Industrial Inspection",
        "authors": [
            "Yilin Lu",
            "Jianghang Lin",
            "Linhuang Xie",
            "Kai Zhao",
            "Yansong Qu",
            "Shengchuan Zhang",
            "Liujuan Cao",
            "Rongrong Ji"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Anomaly inspection plays a vital role in industrial manufacturing, but the scarcity of anomaly samples significantly limits the effectiveness of existing methods in tasks such as localization and classification. While several anomaly synthesis approaches have been introduced for data augmentation, they often struggle with low realism, inaccurate mask alignment, and poor generalization. To overcome these limitations, we propose Generate Aligned Anomaly (GAA), a region-guided, few-shot anomaly image-mask pair generation framework. GAA leverages the strong priors of a pretrained latent diffusion model to generate realistic, diverse, and semantically aligned anomalies using only a small number of samples. The framework first employs Localized Concept Decomposition to jointly model the semantic features and spatial information of anomalies, enabling flexible control over the type and location of anomalies. It then utilizes Adaptive Multi-Round Anomaly Clustering to perform fine-grained semantic clustering of anomaly concepts, thereby enhancing the consistency of anomaly representations. Subsequently, a region-guided mask generation strategy ensures precise alignment between anomalies and their corresponding masks, while a low-quality sample filtering module is introduced to further improve the overall quality of the generated samples. Extensive experiments on the MVTec AD and LOCO datasets demonstrate that GAA achieves superior performance in both anomaly synthesis quality and downstream tasks such as localization and classification.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 75,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09630",
        "abs_url": "https://arxiv.org/abs/2507.09630",
        "pdf_url": "https://arxiv.org/pdf/2507.09630",
        "title": "Brain Stroke Detection and Classification Using CT Imaging with Transformer Models and Explainable AI",
        "authors": [
            "Shomukh Qari",
            "Maha A. Thafar"
        ],
        "comments": "5 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Stroke is one of the leading causes of death globally, making early and accurate diagnosis essential for improving patient outcomes, particularly in emergency settings where timely intervention is critical. CT scans are the key imaging modality because of their speed, accessibility, and cost-effectiveness. This study proposed an artificial intelligence framework for multiclass stroke classification (ischemic, hemorrhagic, and no stroke) using CT scan images from a dataset provided by the Republic of Turkey's Ministry of Health. The proposed method adopted MaxViT, a state-of-the-art Vision Transformer, as the primary deep learning model for image-based stroke classification, with additional transformer variants (vision transformer, transformer-in-transformer, and ConvNext). To enhance model generalization and address class imbalance, we applied data augmentation techniques, including synthetic image generation. The MaxViT model trained with augmentation achieved the best performance, reaching an accuracy and F1-score of 98.00%, outperforming all other evaluated models and the baseline methods. The primary goal of this study was to distinguish between stroke types with high accuracy while addressing crucial issues of transparency and trust in artificial intelligence models. To achieve this, Explainable Artificial Intelligence (XAI) was integrated into the framework, particularly Grad-CAM++. It provides visual explanations of the model's decisions by highlighting relevant stroke regions in the CT scans and establishing an accurate, interpretable, and clinically applicable solution for early stroke detection. This research contributed to the development of a trustworthy AI-assisted diagnostic tool for stroke, facilitating its integration into clinical practice and enhancing access to timely and optimal stroke diagnosis in emergency departments, thereby saving more lives.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 76,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09640",
        "abs_url": "https://arxiv.org/abs/2507.09640",
        "pdf_url": "https://arxiv.org/pdf/2507.09640",
        "title": "Disentanglement and Assessment of Shortcuts in Ophthalmological Retinal Imaging Exams",
        "authors": [
            "Leonor Fernandes",
            "Tiago Gonçalves",
            "João Matos",
            "Luis Filipe Nakayama",
            "Jaime S. Cardoso"
        ],
        "comments": "10 pages. Under review",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Diabetic retinopathy (DR) is a leading cause of vision loss in working-age adults. While screening reduces the risk of blindness, traditional imaging is often costly and inaccessible. Artificial intelligence (AI) algorithms present a scalable diagnostic solution, but concerns regarding fairness and generalization persist. This work evaluates the fairness and performance of image-trained models in DR prediction, as well as the impact of disentanglement as a bias mitigation technique, using the diverse mBRSET fundus dataset. Three models, ConvNeXt V2, DINOv2, and Swin V2, were trained on macula images to predict DR and sensitive attributes (SAs) (e.g., age and gender/sex). Fairness was assessed between subgroups of SAs, and disentanglement was applied to reduce bias. All models achieved high DR prediction performance in diagnosing (up to 94% AUROC) and could reasonably predict age and gender/sex (91% and 77% AUROC, respectively). Fairness assessment suggests disparities, such as a 10% AUROC gap between age groups in DINOv2. Disentangling SAs from DR prediction had varying results, depending on the model selected. Disentanglement improved DINOv2 performance (2% AUROC gain), but led to performance drops in ConvNeXt V2 and Swin V2 (7% and 3%, respectively). These findings highlight the complexity of disentangling fine-grained features in fundus imaging and emphasize the importance of fairness in medical imaging AI to ensure equitable and reliable healthcare solutions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 77,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09649",
        "abs_url": "https://arxiv.org/abs/2507.09649",
        "pdf_url": "https://arxiv.org/pdf/2507.09649",
        "title": "EyeSeg: An Uncertainty-Aware Eye Segmentation Framework for AR/VR",
        "authors": [
            "Zhengyuan Peng",
            "Jianqing Xu",
            "Shen Li",
            "Jiazhen Ji",
            "Yuge Huang",
            "Jingyun Zhang",
            "Jinmin Li",
            "Shouhong Ding",
            "Rizen Guo",
            "Xin Tan",
            "Lizhuang Ma"
        ],
        "comments": "Accepted to IJCAI",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Human-machine interaction through augmented reality (AR) and virtual reality (VR) is increasingly prevalent, requiring accurate and efficient gaze estimation which hinges on the accuracy of eye segmentation to enable smooth user experiences. We introduce EyeSeg, a novel eye segmentation framework designed to overcome key challenges that existing approaches struggle with: motion blur, eyelid occlusion, and train-test domain gaps. In these situations, existing models struggle to extract robust features, leading to suboptimal performance. Noting that these challenges can be generally quantified by uncertainty, we design EyeSeg as an uncertainty-aware eye segmentation framework for AR/VR wherein we explicitly model the uncertainties by performing Bayesian uncertainty learning of a posterior under the closed set prior. Theoretically, we prove that a statistic of the learned posterior indicates segmentation uncertainty levels and empirically outperforms existing methods in downstream tasks, such as gaze estimation. EyeSeg outputs an uncertainty score and the segmentation result, weighting and fusing multiple gaze estimates for robustness, which proves to be effective especially under motion blur, eyelid occlusion and cross-domain challenges. Moreover, empirical results suggest that EyeSeg achieves segmentation improvements of MIoU, E1, F1, and ACC surpassing previous approaches. The code is publicly available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 78,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09672",
        "abs_url": "https://arxiv.org/abs/2507.09672",
        "pdf_url": "https://arxiv.org/pdf/2507.09672",
        "title": "VST-Pose: A Velocity-Integrated Spatiotem-poral Attention Network for Human WiFi Pose Estimation",
        "authors": [
            "Xinyu Zhang",
            "Zhonghao Ye",
            "Jingwei Zhang",
            "Xiang Tian",
            "Zhisheng Liang",
            "Shipeng Yu"
        ],
        "comments": "8 pages, 7 figures, 8 tables. WiFi CSI, VST-Pose framework + ViSTA-Former dual-stream attention backbone. Code: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "WiFi-based human pose estimation has emerged as a promising non-visual alternative approaches due to its pene-trability and privacy advantages. This paper presents VST-Pose, a novel deep learning framework for accurate and continuous pose estimation using WiFi channel state information. The proposed method introduces ViSTA-Former, a spatiotemporal attention backbone with dual-stream architecture that adopts a dual-stream architecture to separately capture temporal dependencies and structural relationships among body joints. To enhance sensitivity to subtle human motions, a velocity modeling branch is integrated into the framework, which learns short-term keypoint dis-placement patterns and improves fine-grained motion representation. We construct a 2D pose dataset specifically designed for smart home care scenarios and demonstrate that our method achieves 92.2% accuracy on the PCK@50 metric, outperforming existing methods by 8.3% in PCK@50 on the self-collected dataset. Further evaluation on the public MMFi dataset confirms the model's robustness and effectiveness in 3D pose estimation tasks. The proposed system provides a reliable and privacy-aware solution for continuous human motion analysis in indoor environments. Our codes are available in this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 79,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09681",
        "abs_url": "https://arxiv.org/abs/2507.09681",
        "pdf_url": "https://arxiv.org/pdf/2507.09681",
        "title": "Prompt2DEM: High-Resolution DEMs for Urban and Open Environments from Global Prompts Using a Monocular Foundation Model",
        "authors": [
            "Osher Rafaeli",
            "Tal Svoray",
            "Ariel Nahlieli"
        ],
        "comments": "18 pages",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)",
        "abstract": "High-resolution elevation estimations are essential to understand catchment and hillslope hydrology, study urban morphology and dynamics, and monitor the growth, decline, and mortality of terrestrial ecosystems. Various deep learning approaches (e.g., super-resolution techniques, monocular depth estimation) have been developed to create high-resolution Digital Elevation Models (DEMs). However, super-resolution techniques are limited by the upscaling factor, and monocular depth estimation lacks global elevation context, making its conversion to a seamless DEM restricted. The recently introduced technique of prompt-based monocular depth estimation has opened new opportunities to extract estimates of absolute elevation in a global context. We present here a framework for the estimation of high-resolution DEMs as a new paradigm for absolute global elevation mapping. It is exemplified using low-resolution Shuttle Radar Topography Mission (SRTM) elevation data as prompts and high-resolution RGB imagery from the National Agriculture Imagery Program (NAIP). The approach fine-tunes a vision transformer encoder with LiDAR-derived DEMs and employs a versatile prompting strategy, enabling tasks such as DEM estimation, void filling, and updating. Our framework achieves a 100x resolution gain (from 30-m to 30-cm), surpassing prior methods by an order of magnitude. Evaluations across three diverse U.S. landscapes show robust generalization, capturing urban structures and fine-scale terrain features with < 5 m MAE relative to LiDAR, improving over SRTM by up to 18%. Hydrological analysis confirms suitability for hazard and environmental studies. We demonstrate scalability by applying the framework to large regions in the U.S. and Israel. All code and pretrained models are publicly available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 80,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09693",
        "abs_url": "https://arxiv.org/abs/2507.09693",
        "pdf_url": "https://arxiv.org/pdf/2507.09693",
        "title": "ExpStar: Towards Automatic Commentary Generation for Multi-discipline Scientific Experiments",
        "authors": [
            "Jiali Chen",
            "Yujie Jia",
            "Zihan Wu",
            "Jinyu Yang",
            "Jianpeng Chen",
            "Xusen Hei",
            "Jiayuan Xie",
            "Yi Cai",
            "Qing Li"
        ],
        "comments": "Accepted by ACM MM 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Experiment commentary is crucial in describing the experimental procedures, delving into underlying scientific principles, and incorporating content-related safety guidelines. In practice, human teachers rely heavily on subject-specific expertise and invest significant time preparing such commentary. To address this challenge, we introduce the task of automatic commentary generation across multi-discipline scientific experiments. While recent progress in large multimodal models (LMMs) has demonstrated promising capabilities in video understanding and reasoning, their ability to generate fine-grained and insightful experiment commentary remains largely underexplored. In this paper, we make the following contributions: (i) We construct \\textit{ExpInstruct}, the first dataset tailored for experiment commentary generation, featuring over 7\\textit{K} step-level commentaries across 21 scientific subjects from 3 core disciplines (\\ie, science, healthcare and engineering). Each sample includes procedural descriptions along with potential scientific principles (\\eg, chemical equations and physical laws) and safety guidelines. (ii) We propose ExpStar, an automatic experiment commentary generation model that leverages a retrieval-augmented mechanism to adaptively access, evaluate, and utilize external knowledge. (iii) Extensive experiments show that our ExpStar substantially outperforms 14 leading LMMs, which highlights the superiority of our dataset and model. We believe that ExpStar holds great potential for advancing AI-assisted scientific experiment instruction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 81,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09702",
        "abs_url": "https://arxiv.org/abs/2507.09702",
        "pdf_url": "https://arxiv.org/pdf/2507.09702",
        "title": "Token Compression Meets Compact Vision Transformers: A Survey and Comparative Evaluation for Edge AI",
        "authors": [
            "Phat Nguyen",
            "Ngai-Man Cheung"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Token compression techniques have recently emerged as powerful tools for accelerating Vision Transformer (ViT) inference in computer vision. Due to the quadratic computational complexity with respect to the token sequence length, these methods aim to remove less informative tokens before the attention layers to improve inference throughput. While numerous studies have explored various accuracy-efficiency trade-offs on large-scale ViTs, two critical gaps remain. First, there is a lack of unified survey that systematically categorizes and compares token compression approaches based on their core strategies (e.g., pruning, merging, or hybrid) and deployment settings (e.g., fine-tuning vs. plug-in). Second, most benchmarks are limited to standard ViT models (e.g., ViT-B, ViT-L), leaving open the question of whether such methods remain effective when applied to structurally compressed transformers, which are increasingly deployed on resource-constrained edge devices. To address these gaps, we present the first systematic taxonomy and comparative study of token compression methods, and we evaluate representative techniques on both standard and compact ViT architectures. Our experiments reveal that while token compression methods are effective for general-purpose ViTs, they often underperform when directly applied to compact designs. These findings not only provide practical insights but also pave the way for future research on adapting token optimization techniques to compact transformer-based networks for edge AI and AI agent applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 82,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09748",
        "abs_url": "https://arxiv.org/abs/2507.09748",
        "pdf_url": "https://arxiv.org/pdf/2507.09748",
        "title": "Advancing Text-to-3D Generation with Linearized Lookahead Variational Score Distillation",
        "authors": [
            "Yu Lei",
            "Bingde Liu",
            "Qingsong Xie",
            "Haonan Lu",
            "Zhijie Deng"
        ],
        "comments": "Accepted by ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Text-to-3D generation based on score distillation of pre-trained 2D diffusion models has gained increasing interest, with variational score distillation (VSD) as a remarkable example. VSD proves that vanilla score distillation can be improved by introducing an extra score-based model, which characterizes the distribution of images rendered from 3D models, to correct the distillation gradient. Despite the theoretical foundations, VSD, in practice, is likely to suffer from slow and sometimes ill-posed convergence. In this paper, we perform an in-depth investigation of the interplay between the introduced score model and the 3D model, and find that there exists a mismatching problem between LoRA and 3D distributions in practical implementation. We can simply adjust their optimization order to improve the generation quality. By doing so, the score model looks ahead to the current 3D state and hence yields more reasonable corrections. Nevertheless, naive lookahead VSD may suffer from unstable training in practice due to the potential over-fitting. To address this, we propose to use a linearized variant of the model for score distillation, giving rise to the Linearized Lookahead Variational Score Distillation ($L^2$-VSD). $L^2$-VSD can be realized efficiently with forward-mode autodiff functionalities of existing deep learning libraries. Extensive experiments validate the efficacy of $L^2$-VSD, revealing its clear superiority over prior score distillation-based methods. We also show that our method can be seamlessly incorporated into any other VSD-based text-to-3D framework.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 83,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09767",
        "abs_url": "https://arxiv.org/abs/2507.09767",
        "pdf_url": "https://arxiv.org/pdf/2507.09767",
        "title": "Pairwise Alignment & Compatibility for Arbitrarily Irregular Image Fragments",
        "authors": [
            "Ofir Itzhak Shahar",
            "Gur Elkin",
            "Ohad Ben-Shahar"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Pairwise compatibility calculation is at the core of most fragments-reconstruction algorithms, in particular those designed to solve different types of the jigsaw puzzle problem. However, most existing approaches fail, or aren't designed to deal with fragments of realistic geometric properties one encounters in real-life puzzles. And in all other cases, compatibility methods rely strongly on the restricted shapes of the fragments. In this paper, we propose an efficient hybrid (geometric and pictorial) approach for computing the optimal alignment for pairs of fragments, without any assumptions about their shapes, dimensions, or pictorial content. We introduce a new image fragments dataset generated via a novel method for image fragmentation and a formal erosion model that mimics real-world archaeological erosion, along with evaluation metrics for the compatibility task. We then embed our proposed compatibility into an archaeological puzzle-solving framework and demonstrate state-of-the-art neighborhood-level precision and recall on the RePAIR 2D dataset, directly reflecting compatibility performance improvements.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 84,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09795",
        "abs_url": "https://arxiv.org/abs/2507.09795",
        "pdf_url": "https://arxiv.org/pdf/2507.09795",
        "title": "NegRefine: Refining Negative Label-Based Zero-Shot OOD Detection",
        "authors": [
            "Amirhossein Ansari",
            "Ke Wang",
            "Pulei Xiong"
        ],
        "comments": "Accepted to ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Recent advancements in Vision-Language Models like CLIP have enabled zero-shot OOD detection by leveraging both image and textual label information. Among these, negative label-based methods such as NegLabel and CSP have shown promising results by utilizing a lexicon of words to define negative labels for distinguishing OOD samples. However, these methods suffer from detecting in-distribution samples as OOD due to negative labels that are subcategories of in-distribution labels or proper nouns. They also face limitations in handling images that match multiple in-distribution and negative labels. We propose NegRefine, a novel negative label refinement framework for zero-shot OOD detection. By introducing a filtering mechanism to exclude subcategory labels and proper nouns from the negative label set and incorporating a multi-matching-aware scoring function that dynamically adjusts the contributions of multiple labels matching an image, NegRefine ensures a more robust separation between in-distribution and OOD samples. We evaluate NegRefine on large-scale benchmarks, including ImageNet-1K. Source code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 85,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09815",
        "abs_url": "https://arxiv.org/abs/2507.09815",
        "pdf_url": "https://arxiv.org/pdf/2507.09815",
        "title": "VRU-Accident: A Vision-Language Benchmark for Video Question Answering and Dense Captioning for Accident Scene Understanding",
        "authors": [
            "Younggun Kim",
            "Ahmed S. Abdelrahman",
            "Mohamed Abdel-Aty"
        ],
        "comments": "22 pages, 11 figures, 5 tables",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and cyclists, is a critical challenge for autonomous driving systems, as crashes involving VRUs often result in severe or fatal consequences. While multimodal large language models (MLLMs) have shown promise in enhancing scene understanding and decision making in autonomous vehicles, there is currently no standardized benchmark to quantitatively evaluate their reasoning abilities in complex, safety-critical scenarios involving VRUs. To address this gap, we present VRU-Accident, a large-scale vision-language benchmark designed to evaluate MLLMs in high-risk traffic scenarios involving VRUs. VRU-Accident comprises 1K real-world dashcam accident videos, annotated with 6K multiple-choice question-answer pairs across six safety-critical categories (with 24K candidate options and 3.4K unique answer choices), as well as 1K dense scene descriptions. Unlike prior works, our benchmark focuses explicitly on VRU-vehicle accidents, providing rich, fine-grained annotations that capture both spatial-temporal dynamics and causal semantics of accidents. To assess the current landscape of MLLMs, we conduct a comprehensive evaluation of 17 state-of-the-art models on the multiple-choice VQA task and on the dense captioning task. Our findings reveal that while MLLMs perform reasonably well on visually grounded attributes, they face significant challenges in reasoning and describing accident causes, types, and preventability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 86,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09830",
        "abs_url": "https://arxiv.org/abs/2507.09830",
        "pdf_url": "https://arxiv.org/pdf/2507.09830",
        "title": "Hierarchical Abstraction Enables Human-Like 3D Object Recognition in Deep Learning Models",
        "authors": [
            "Shuhao Fu",
            "Philip J. Kellman",
            "Hongjing Lu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Both humans and deep learning models can recognize objects from 3D shapes depicted with sparse visual information, such as a set of points randomly sampled from the surfaces of 3D objects (termed a point cloud). Although deep learning models achieve human-like performance in recognizing objects from 3D shapes, it remains unclear whether these models develop 3D shape representations similar to those used by human vision for object recognition. We hypothesize that training with 3D shapes enables models to form representations of local geometric structures in 3D shapes. However, their representations of global 3D object shapes may be limited. We conducted two human experiments systematically manipulating point density and object orientation (Experiment 1), and local geometric structure (Experiment 2). Humans consistently performed well across all experimental conditions. We compared two types of deep learning models, one based on a convolutional neural network (DGCNN) and the other on visual transformers (point transformer), with human performance. We found that the point transformer model provided a better account of human performance than the convolution-based model. The advantage mainly results from the mechanism in the point transformer model that supports hierarchical abstraction of 3D shapes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 87,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09861",
        "abs_url": "https://arxiv.org/abs/2507.09861",
        "pdf_url": "https://arxiv.org/pdf/2507.09861",
        "title": "A Survey on MLLM-based Visually Rich Document Understanding: Methods, Challenges, and Emerging Trends",
        "authors": [
            "Yihao Ding",
            "Siwen Luo",
            "Yue Dai",
            "Yanbei Jiang",
            "Zechuan Li",
            "Geoffrey Martin",
            "Yifan Peng"
        ],
        "comments": "Work in progress",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Visually-Rich Document Understanding (VRDU) has emerged as a critical field, driven by the need to automatically process documents containing complex visual, textual, and layout information. Recently, Multimodal Large Language Models (MLLMs) have shown remarkable potential in this domain, leveraging both Optical Character Recognition (OCR)-dependent and OCR-free frameworks to extract and interpret information in document images. This survey reviews recent advancements in MLLM-based VRDU, highlighting three core components: (1) methods for encoding and fusing textual, visual, and layout features; (2) training paradigms, including pretraining strategies, instruction-response tuning, and the trainability of different model modules; and (3) datasets utilized for pretraining, instruction-tuning, and supervised fine-tuning. Finally, we discuss the challenges and opportunities in this evolving field and propose future directions to advance the efficiency, generalizability, and robustness of VRDU systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 88,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09862",
        "abs_url": "https://arxiv.org/abs/2507.09862",
        "pdf_url": "https://arxiv.org/pdf/2507.09862",
        "title": "SpeakerVid-5M: A Large-Scale High-Quality Dataset for Audio-Visual Dyadic Interactive Human Generation",
        "authors": [
            "Youliang Zhang",
            "Zhaoyang Li",
            "Duomin Wang",
            "Jiahe Zhang",
            "Deyu Zhou",
            "Zixin Yin",
            "Xili Dai",
            "Gang Yu",
            "Xiu Li"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Audio and Speech Processing (eess.AS)",
        "abstract": "The rapid development of large-scale models has catalyzed significant breakthroughs in the digital human domain. These advanced methodologies offer high-fidelity solutions for avatar driving and rendering, leading academia to focus on the next major challenge: audio-visual dyadic interactive virtual human. To facilitate research in this emerging area, we present SpeakerVid-5M dataset, the first large-scale, high-quality dataset designed for audio-visual dyadic interactive virtual human generation. Totaling over 8,743 hours, SpeakerVid-5M contains more than 5.2 million video clips of human portraits. It covers diverse scales and interaction types, including monadic talking, listening, and dyadic conversations. Crucially, the dataset is structured along two key dimensions: interaction type and data quality. First, it is categorized into four types (dialogue branch, single branch, listening branch and multi-turn branch) based on the interaction scenario. Second, it is stratified into a large-scale pre-training subset and a curated, high-quality subset for Supervised Fine-Tuning (SFT). This dual structure accommodates a wide array of 2D virtual human tasks. In addition, we provide an autoregressive (AR)-based video chat baseline trained on this data, accompanied by a dedicated set of metrics and test data to serve as a benchmark VidChatBench for future work. Both the dataset and the corresponding data processing code will be publicly released. Project page: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 89,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09876",
        "abs_url": "https://arxiv.org/abs/2507.09876",
        "pdf_url": "https://arxiv.org/pdf/2507.09876",
        "title": "ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models",
        "authors": [
            "Yongheng Zhang",
            "Xu Liu",
            "Ruihan Tao",
            "Qiguang Chen",
            "Hao Fei",
            "Wanxiang Che",
            "Libo Qin"
        ],
        "comments": "Accepted by ACM MM 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Video understanding plays a vital role in bridging low-level visual signals with high-level cognitive reasoning, and is fundamental to applications such as autonomous driving, embodied AI, and the broader pursuit of AGI. The rapid development of large language models (LLMs), particularly those utilizing Chain-of-Thought (CoT) technology, has significantly advanced video reasoning capabilities. However, current approaches primarily depend on textual information for reasoning, overlooking the visual modality in the actual video reasoning process. In contrast, humans naturally re-examine visual content while reasoning. Motivated by this, we introduce a novel video reasoning paradigm: Video-Text Interleaved CoT (ViTCoT), which facilitates more intuitive and cognitively aligned reasoning. To the end, first, we construct the Video-Text Interleaved Benchmark (ViTIB), which is created using MLLMs for key-video selection and manually verified. Furthermore, we extensively explore the potential of the ViTCoT paradigm in the video understanding field. Extensive experiments demonstrate that ViTCoT significantly enhances performance compared to the traditional text-only CoT paradigm and effectively activates more neuron values in MLLMs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 90,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09880",
        "abs_url": "https://arxiv.org/abs/2507.09880",
        "pdf_url": "https://arxiv.org/pdf/2507.09880",
        "title": "OpenHuman4D: Open-Vocabulary 4D Human Parsing",
        "authors": [
            "Keito Suzuki",
            "Bang Du",
            "Runfa Blark Li",
            "Kunyao Chen",
            "Lei Wang",
            "Peng Liu",
            "Ning Bi",
            "Truong Nguyen"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Understanding dynamic 3D human representation has become increasingly critical in virtual and extended reality applications. However, existing human part segmentation methods are constrained by reliance on closed-set datasets and prolonged inference times, which significantly restrict their applicability. In this paper, we introduce the first 4D human parsing framework that simultaneously addresses these challenges by reducing the inference time and introducing open-vocabulary capabilities. Building upon state-of-the-art open-vocabulary 3D human parsing techniques, our approach extends the support to 4D human-centric video with three key innovations: 1) We adopt mask-based video object tracking to efficiently establish spatial and temporal correspondences, avoiding the necessity of segmenting all frames. 2) A novel Mask Validation module is designed to manage new target identification and mitigate tracking failures. 3) We propose a 4D Mask Fusion module, integrating memory-conditioned attention and logits equalization for robust embedding fusion. Extensive experiments demonstrate the effectiveness and flexibility of the proposed method on 4D human-centric parsing tasks, achieving up to 93.3% acceleration compared to the previous state-of-the-art method, which was limited to parsing fixed classes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 91,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09881",
        "abs_url": "https://arxiv.org/abs/2507.09881",
        "pdf_url": "https://arxiv.org/pdf/2507.09881",
        "title": "Counterfactual Visual Explanation via Causally-Guided Adversarial Steering",
        "authors": [
            "Yiran Qiao",
            "Disheng Liu",
            "Yiren Lu",
            "Yu Yin",
            "Mengnan Du",
            "Jing Ma"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Recent work on counterfactual visual explanations has contributed to making artificial intelligence models more explainable by providing visual perturbation to flip the prediction. However, these approaches neglect the causal relationships and the spurious correlations behind the image generation process, which often leads to unintended alterations in the counterfactual images and renders the explanations with limited quality. To address this challenge, we introduce a novel framework CECAS, which first leverages a causally-guided adversarial method to generate counterfactual explanations. It innovatively integrates a causal perspective to avoid unwanted perturbations on spurious factors in the counterfactuals. Extensive experiments demonstrate that our method outperforms existing state-of-the-art approaches across multiple benchmark datasets and ultimately achieves a balanced trade-off among various aspects of validity, sparsity, proximity, and realism.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 92,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09885",
        "abs_url": "https://arxiv.org/abs/2507.09885",
        "pdf_url": "https://arxiv.org/pdf/2507.09885",
        "title": "MCGA: Mixture of Codebooks Hyperspectral Reconstruction via Grayscale-Aware Attention",
        "authors": [
            "Zhanjiang Yang",
            "Lijun Sun",
            "Jiawei Dong",
            "Xiaoxin An",
            "Yang Liu",
            "Meng Li"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Reconstructing hyperspectral images (HSI) from RGB images is a cost-effective solution for various vision-based applications. However, most existing learning-based hyperspectral reconstruction methods directly learn the RGB-to-HSI mapping using complex attention mechanisms, neglecting the inherent challenge of transitioning from low-dimensional to high-dimensional information. To address this limitation, we propose a two-stage approach, MCGA, which first learns spectral patterns before estimating the mapping. In the first stage, a multi-scale VQ-VAE learns representations from heterogeneous HSI datasets, extracting a Mixture of Codebooks (MoC). In the second stage, the RGB-to-HSI mapping is refined by querying features from the MoC to replace latent HSI representations, incorporating prior knowledge rather than forcing a direct high-dimensional transformation. To further enhance reconstruction quality, we introduce Grayscale-Aware Attention and Quantized Self-Attention, which adaptively adjust feature map intensities to meet hyperspectral reconstruction requirements. This physically motivated attention mechanism ensures lightweight and efficient HSI recovery. Moreover, we propose an entropy-based Test-Time Adaptation strategy to improve robustness in real-world scenarios. Extensive experiments demonstrate that our method, MCGA, achieves state-of-the-art performance. The code and models will be released at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 93,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09896",
        "abs_url": "https://arxiv.org/abs/2507.09896",
        "pdf_url": "https://arxiv.org/pdf/2507.09896",
        "title": "Measuring the Impact of Rotation Equivariance on Aerial Object Detection",
        "authors": [
            "Xiuyu Wu",
            "Xinhao Wang",
            "Xiubin Zhu",
            "Lan Yang",
            "Jiyuan Liu",
            "Xingchen Hu"
        ],
        "comments": "Accepted by ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Due to the arbitrary orientation of objects in aerial images, rotation equivariance is a critical property for aerial object detectors. However, recent studies on rotation-equivariant aerial object detection remain scarce. Most detectors rely on data augmentation to enable models to learn approximately rotation-equivariant features. A few detectors have constructed rotation-equivariant networks, but due to the breaking of strict rotation equivariance by typical downsampling processes, these networks only achieve approximately rotation-equivariant backbones. Whether strict rotation equivariance is necessary for aerial image object detection remains an open question. In this paper, we implement a strictly rotation-equivariant backbone and neck network with a more advanced network structure and compare it with approximately rotation-equivariant networks to quantitatively measure the impact of rotation equivariance on the performance of aerial image detectors. Additionally, leveraging the inherently grouped nature of rotation-equivariant features, we propose a multi-branch head network that reduces the parameter count while improving detection accuracy. Based on the aforementioned improvements, this study proposes the Multi-branch head rotation-equivariant single-stage Detector (MessDet), which achieves state-of-the-art performance on the challenging aerial image datasets DOTA-v1.0, DOTA-v1.5 and DIOR-R with an exceptionally low parameter count.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 94,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09910",
        "abs_url": "https://arxiv.org/abs/2507.09910",
        "pdf_url": "https://arxiv.org/pdf/2507.09910",
        "title": "IGD: Instructional Graphic Design with Multimodal Layer Generation",
        "authors": [
            "Yadong Qu",
            "Shancheng Fang",
            "Yuxin Wang",
            "Xiaorui Wang",
            "Zhineng Chen",
            "Hongtao Xie",
            "Yongdong Zhang"
        ],
        "comments": "ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Graphic design visually conveys information and data by creating and combining text, images and graphics. Two-stage methods that rely primarily on layout generation lack creativity and intelligence, making graphic design still labor-intensive. Existing diffusion-based methods generate non-editable graphic design files at image level with poor legibility in visual text rendering, which prevents them from achieving satisfactory and practical automated graphic design. In this paper, we propose Instructional Graphic Designer (IGD) to swiftly generate multimodal layers with editable flexibility with only natural language instructions. IGD adopts a new paradigm that leverages parametric rendering and image asset generation. First, we develop a design platform and establish a standardized format for multi-scenario design files, thus laying the foundation for scaling up data. Second, IGD utilizes the multimodal understanding and reasoning capabilities of MLLM to accomplish attribute prediction, sequencing and layout of layers. It also employs a diffusion model to generate image content for assets. By enabling end-to-end training, IGD architecturally supports scalability and extensibility in complex graphic design tasks. The superior experimental results demonstrate that IGD offers a new solution for graphic design.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 95,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09915",
        "abs_url": "https://arxiv.org/abs/2507.09915",
        "pdf_url": "https://arxiv.org/pdf/2507.09915",
        "title": "Crucial-Diff: A Unified Diffusion Model for Crucial Image and Annotation Synthesis in Data-scarce Scenarios",
        "authors": [
            "Siyue Yao",
            "Mingjie Sun",
            "Eng Gee Lim",
            "Ran Yi",
            "Baojiang Zhong",
            "Moncef Gabbouj"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The scarcity of data in various scenarios, such as medical, industry and autonomous driving, leads to model overfitting and dataset imbalance, thus hindering effective detection and segmentation performance. Existing studies employ the generative models to synthesize more training samples to mitigate data scarcity. However, these synthetic samples are repetitive or simplistic and fail to provide \"crucial information\" that targets the downstream model's weaknesses. Additionally, these methods typically require separate training for different objects, leading to computational inefficiencies. To address these issues, we propose Crucial-Diff, a domain-agnostic framework designed to synthesize crucial samples. Our method integrates two key modules. The Scene Agnostic Feature Extractor (SAFE) utilizes a unified feature extractor to capture target information. The Weakness Aware Sample Miner (WASM) generates hard-to-detect samples using feedback from the detection results of downstream model, which is then fused with the output of SAFE module. Together, our Crucial-Diff framework generates diverse, high-quality training data, achieving a pixel-level AP of 83.63% and an F1-MAX of 78.12% on MVTec. On polyp dataset, Crucial-Diff reaches an mIoU of 81.64% and an mDice of 87.69%. Code will be released after acceptance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 96,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09950",
        "abs_url": "https://arxiv.org/abs/2507.09950",
        "pdf_url": "https://arxiv.org/pdf/2507.09950",
        "title": "Can GPT-4o mini and Gemini 2.0 Flash Predict Fine-Grained Fashion Product Attributes? A Zero-Shot Analysis",
        "authors": [
            "Shubham Shukla",
            "Kunal Sonalkar"
        ],
        "comments": "11 pages, 2 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "The fashion retail business is centered around the capacity to comprehend products. Product attribution helps in comprehending products depending on the business process. Quality attribution improves the customer experience as they navigate through millions of products offered by a retail website. It leads to well-organized product catalogs. In the end, product attribution directly impacts the 'discovery experience' of the customer. Although large language models (LLMs) have shown remarkable capabilities in understanding multimodal data, their performance on fine-grained fashion attribute recognition remains under-explored. This paper presents a zero-shot evaluation of state-of-the-art LLMs that balance performance with speed and cost efficiency, mainly GPT-4o-mini and Gemini 2.0 Flash. We have used the dataset DeepFashion-MultiModal (this https URL) to evaluate these models in the attribution tasks of fashion products. Our study evaluates these models across 18 categories of fashion attributes, offering insight into where these models excel. We only use images as the sole input for product information to create a constrained environment. Our analysis shows that Gemini 2.0 Flash demonstrates the strongest overall performance with a macro F1 score of 56.79% across all attributes, while GPT-4o-mini scored a macro F1 score of 43.28%. Through detailed error analysis, our findings provide practical insights for deploying these LLMs in production e-commerce product attribution-related tasks and highlight the need for domain-specific fine-tuning approaches. This work also lays the groundwork for future research in fashion AI and multimodal attribute extraction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 97,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09953",
        "abs_url": "https://arxiv.org/abs/2507.09953",
        "pdf_url": "https://arxiv.org/pdf/2507.09953",
        "title": "4D-MISR: A unified model for low-dose super-resolution imaging via feature fusion",
        "authors": [
            "Zifei Wang",
            "Zian Mao",
            "Xiaoya He",
            "Xi Huang",
            "Haoran Zhang",
            "Chun Cheng",
            "Shufen Chu",
            "Tingzheng Hou",
            "Xiaoqin Zeng",
            "Yujun Xie"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "While electron microscopy offers crucial atomic-resolution insights into structure-property relationships, radiation damage severely limits its use on beam-sensitive materials like proteins and 2D materials. To overcome this challenge, we push beyond the electron dose limits of conventional electron microscopy by adapting principles from multi-image super-resolution (MISR) that have been widely used in remote sensing. Our method fuses multiple low-resolution, sub-pixel-shifted views and enhances the reconstruction with a convolutional neural network (CNN) that integrates features from synthetic, multi-angle observations. We developed a dual-path, attention-guided network for 4D-STEM that achieves atomic-scale super-resolution from ultra-low-dose data. This provides robust atomic-scale visualization across amorphous, semi-crystalline, and crystalline beam-sensitive specimens. Systematic evaluations on representative materials demonstrate comparable spatial resolution to conventional ptychography under ultra-low-dose conditions. Our work expands the capabilities of 4D-STEM, offering a new and generalizable method for the structural analysis of radiation-vulnerable materials.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 98,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09980",
        "abs_url": "https://arxiv.org/abs/2507.09980",
        "pdf_url": "https://arxiv.org/pdf/2507.09980",
        "title": "Uncertainty Quantification for Incomplete Multi-View Data Using Divergence Measures",
        "authors": [
            "Zhipeng Xue",
            "Yan Zhang",
            "Ming Li",
            "Chun Li",
            "Yue Liu",
            "Fei Yu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Existing multi-view classification and clustering methods typically improve task accuracy by leveraging and fusing information from different views. However, ensuring the reliability of multi-view integration and final decisions is crucial, particularly when dealing with noisy or corrupted data. Current methods often rely on Kullback-Leibler (KL) divergence to estimate uncertainty of network predictions, ignoring domain gaps between different modalities. To address this issue, KPHD-Net, based on Hölder divergence, is proposed for multi-view classification and clustering tasks. Generally, our KPHD-Net employs a variational Dirichlet distribution to represent class probability distributions, models evidences from different views, and then integrates it with Dempster-Shafer evidence theory (DST) to improve uncertainty estimation effects. Our theoretical analysis demonstrates that Proper Hölder divergence offers a more effective measure of distribution discrepancies, ensuring enhanced performance in multi-view learning. Moreover, Dempster-Shafer evidence theory, recognized for its superior performance in multi-view fusion tasks, is introduced and combined with the Kalman filter to provide future state estimations. This integration further enhances the reliability of the final fusion results. Extensive experiments show that the proposed KPHD-Net outperforms the current state-of-the-art methods in both classification and clustering tasks regarding accuracy, robustness, and reliability, with theoretical guarantees.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 99,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09984",
        "abs_url": "https://arxiv.org/abs/2507.09984",
        "pdf_url": "https://arxiv.org/pdf/2507.09984",
        "title": "Latent Diffusion Models with Masked AutoEncoders",
        "authors": [
            "Junho Lee",
            "Jeongwoo Shin",
            "Hyungwook Choi",
            "Joonseok Lee"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In spite of remarkable potential of the Latent Diffusion Models (LDMs) in image generation, the desired properties and optimal design of the autoencoders have been underexplored. In this work, we analyze the role of autoencoders in LDMs and identify three key properties: latent smoothness, perceptual compression quality, and reconstruction quality. We demonstrate that existing autoencoders fail to simultaneously satisfy all three properties, and propose Variational Masked AutoEncoders (VMAEs), taking advantage of the hierarchical features maintained by Masked AutoEncoder. We integrate VMAEs into the LDM framework, introducing Latent Diffusion Models with Masked AutoEncoders (LDMAEs). Through comprehensive experiments, we demonstrate significantly enhanced image generation quality and computational efficiency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 100,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09993",
        "abs_url": "https://arxiv.org/abs/2507.09993",
        "pdf_url": "https://arxiv.org/pdf/2507.09993",
        "title": "3DGAA: Realistic and Robust 3D Gaussian-based Adversarial Attack for Autonomous Driving",
        "authors": [
            "Yixun Zhang",
            "Lizhi Wang",
            "Junjun Zhao",
            "Wending Zhao",
            "Feng Zhou",
            "Yonghao Dang",
            "Jianqin Yin"
        ],
        "comments": "Submitted to WACV 2026",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Camera-based object detection systems play a vital role in autonomous driving, yet they remain vulnerable to adversarial threats in real-world environments. While existing 2D and 3D physical attacks typically optimize texture, they often struggle to balance physical realism and attack robustness. In this work, we propose 3D Gaussian-based Adversarial Attack (3DGAA), a novel adversarial object generation framework that leverages the full 14-dimensional parameterization of 3D Gaussian Splatting (3DGS) to jointly optimize geometry and appearance in physically realizable ways. Unlike prior works that rely on patches or texture, 3DGAA jointly perturbs both geometric attributes (shape, scale, rotation) and appearance attributes (color, opacity) to produce physically realistic and transferable adversarial objects. We further introduce a physical filtering module to preserve geometric fidelity, and a physical augmentation module to simulate complex physical scenarios, thus enhancing attack generalization under real-world conditions. We evaluate 3DGAA on both virtual benchmarks and physical-world setups using miniature vehicle models. Experimental results show that 3DGAA achieves to reduce the detection mAP from 87.21% to 7.38%, significantly outperforming existing 3D physical attacks. Moreover, our method maintains high transferability across different physical conditions, demonstrating a new state-of-the-art in physically realizable adversarial attacks. These results validate 3DGAA as a practical attack framework for evaluating the safety of perception systems in autonomous driving.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 101,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09996",
        "abs_url": "https://arxiv.org/abs/2507.09996",
        "pdf_url": "https://arxiv.org/pdf/2507.09996",
        "title": "Leveraging Swin Transformer for enhanced diagnosis of Alzheimer's disease using multi-shell diffusion MRI",
        "authors": [
            "Quentin Dessain",
            "Nicolas Delinte",
            "Bernard Hanseeuw",
            "Laurence Dricot",
            "Benoît Macq"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Neurons and Cognition (q-bio.NC); Quantitative Methods (q-bio.QM)",
        "abstract": "Objective: This study aims to support early diagnosis of Alzheimer's disease and detection of amyloid accumulation by leveraging the microstructural information available in multi-shell diffusion MRI (dMRI) data, using a vision transformer-based deep learning framework. Methods: We present a classification pipeline that employs the Swin Transformer, a hierarchical vision transformer model, on multi-shell dMRI data for the classification of Alzheimer's disease and amyloid presence. Key metrics from DTI and NODDI were extracted and projected onto 2D planes to enable transfer learning with ImageNet-pretrained models. To efficiently adapt the transformer to limited labeled neuroimaging data, we integrated Low-Rank Adaptation. We assessed the framework on diagnostic group prediction (cognitively normal, mild cognitive impairment, Alzheimer's disease dementia) and amyloid status classification. Results: The framework achieved competitive classification results within the scope of multi-shell dMRI-based features, with the best balanced accuracy of 95.2% for distinguishing cognitively normal individuals from those with Alzheimer's disease dementia using NODDI metrics. For amyloid detection, it reached 77.2% balanced accuracy in distinguishing amyloid-positive mild cognitive impairment/Alzheimer's disease dementia subjects from amyloid-negative cognitively normal subjects, and 67.9% for identifying amyloid-positive individuals among cognitively normal subjects. Grad-CAM-based explainability analysis identified clinically relevant brain regions, including the parahippocampal gyrus and hippocampus, as key contributors to model predictions. Conclusion: This study demonstrates the promise of diffusion MRI and transformer-based architectures for early detection of Alzheimer's disease and amyloid pathology, supporting biomarker-driven diagnostics in data-limited biomedical settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 102,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10006",
        "abs_url": "https://arxiv.org/abs/2507.10006",
        "pdf_url": "https://arxiv.org/pdf/2507.10006",
        "title": "Vision-Based Anti Unmanned Aerial Technology: Opportunities and Challenges",
        "authors": [
            "Guanghai Ding",
            "Yihua Ren",
            "Yuting Liu",
            "Qijun Zhao",
            "Shuiwang Li"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "With the rapid advancement of UAV technology and its extensive application in various fields such as military reconnaissance, environmental monitoring, and logistics, achieving efficient and accurate Anti-UAV tracking has become essential. The importance of Anti-UAV tracking is increasingly prominent, especially in scenarios such as public safety, border patrol, search and rescue, and agricultural monitoring, where operations in complex environments can provide enhanced security. Current mainstream Anti-UAV tracking technologies are primarily centered around computer vision techniques, particularly those that integrate multi-sensor data fusion with advanced detection and tracking algorithms. This paper first reviews the characteristics and current challenges of Anti-UAV detection and tracking technologies. Next, it investigates and compiles several publicly available datasets, providing accessible links to support researchers in efficiently addressing related challenges. Furthermore, the paper analyzes the major vision-based and vision-fusion-based Anti-UAV detection and tracking algorithms proposed in recent years. Finally, based on the above research, this paper outlines future research directions, aiming to provide valuable insights for advancing the field.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 103,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10009",
        "abs_url": "https://arxiv.org/abs/2507.10009",
        "pdf_url": "https://arxiv.org/pdf/2507.10009",
        "title": "Binomial Self-Compensation: Mechanism and Suppression of Motion Error in Phase-Shifting Profilometry",
        "authors": [
            "Geyou Zhang",
            "Kai Liu",
            "Ce Zhu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Phase shifting profilometry (PSP) is widely used in high-precision 3D scanning due to its high accuracy, robustness, and pixel-wise handling. However, a fundamental assumption of PSP that the object should remain static does not hold in dynamic measurement, making PSP susceptible to object motion. To address this challenge, our proposed solution, phase-sequential binomial self-compensation (P-BSC), sums successive motion-affected phase frames weighted by binomial coefficients. This approach exponentially reduces the motion error in a pixel-wise and frame-wise loopable manner. Despite its efficacy, P-BSC suffers from high computational overhead and error accumulation due to its reliance on multi-frame phase calculations and weighted summations. Inspired by P-BSC, we propose an image-sequential binomial self-compensation (I-BSC) to weight sum the homogeneous fringe images instead of successive phase frames, which generalizes the BSC concept from phase sequences to image sequences. I-BSC computes the arctangent function only once, resolving both limitations in P-BSC. Extensive analysis, simulations, and experiments show that 1) the proposed BSC outperforms existing methods in reducing motion error while achieving a quasi-single-shot frame rate, i.e., depth map frame rate equals to the camera's acquisition rate, enabling 3D reconstruction with high pixel-depth-temporal resolution; 2) compared to P-BSC, our I-BSC reduces the computational complexity by one polynomial order, thereby accelerating the computational frame rate by several to dozen times, while also reaching faster motion error convergence.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 104,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10013",
        "abs_url": "https://arxiv.org/abs/2507.10013",
        "pdf_url": "https://arxiv.org/pdf/2507.10013",
        "title": "Cross-modal Associations in Vision and Language Models: Revisiting the bouba-kiki effect",
        "authors": [
            "Tom Kouwenhoven",
            "Kiana Shahrasbi",
            "Tessa Verhoef"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Computation and Language (cs.CL)",
        "abstract": "Recent advances in multimodal models have raised questions about whether vision-and-language models (VLMs) integrate cross-modal information in ways that reflect human cognition. One well-studied test case in this domain is the bouba-kiki effect, where humans reliably associate pseudowords like \"bouba\" with round shapes and \"kiki\" with jagged ones. Given the mixed evidence found in prior studies for this effect in VLMs, we present a comprehensive re-evaluation focused on two variants of CLIP, ResNet and Vision Transformer (ViT), given their centrality in many state-of-the-art VLMs. We apply two complementary methods closely modelled after human experiments: a prompt-based evaluation that uses probabilities as model preference, and we use Grad-CAM as a novel way to interpret visual attention in shape-word matching tasks. Our findings show that these models do not consistently exhibit the bouba-kiki effect. While ResNet shows a preference for round shapes, overall performance across both models lacks the expected associations. Moreover, direct comparison with prior human data on the same task shows that the models' responses fall markedly short of the robust, modality-integrated behaviour characteristic of human cognition. These results contribute to the ongoing debate about the extent to which VLMs truly understand cross-modal concepts, highlighting limitations in their internal representations and alignment with human intuitions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 105,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10015",
        "abs_url": "https://arxiv.org/abs/2507.10015",
        "pdf_url": "https://arxiv.org/pdf/2507.10015",
        "title": "(Almost) Free Modality Stitching of Foundation Models",
        "authors": [
            "Jaisidh Singh",
            "Diganta Misra",
            "Boris Knyazev",
            "Antonio Orvieto"
        ],
        "comments": "Pre-print",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Foundation multi-modal models are often designed by stitching of multiple existing pretrained uni-modal models: for example, an image classifier with an text model. This stitching process is performed by training a connector module that aims to align the representation spaces of these uni-modal models towards a multi-modal objective. However, given the complexity of training such connectors on large scale web-based datasets coupled with the ever-increasing number of available pretrained uni-modal models, the task of uni-modal models selection and subsequent connector module training becomes computationally demanding. To address this under-studied critical problem, we propose Hypernetwork Model Alignment (Hyma), a novel all-in-one solution for optimal uni-modal model selection and connector training by leveraging hypernetworks. Specifically, our framework utilizes the parameter prediction capability of a hypernetwork to obtain jointly trained connector modules for $N \\times M$ combinations of uni-modal models. In our experiments, Hyma reduces the cost of searching for the best performing uni-modal model pair by $10\\times$, while matching the ranking and trained connector performance obtained via grid search across a suite of diverse multi-modal benchmarks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 106,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10029",
        "abs_url": "https://arxiv.org/abs/2507.10029",
        "pdf_url": "https://arxiv.org/pdf/2507.10029",
        "title": "Memory-Efficient Personalization of Text-to-Image Diffusion Models via Selective Optimization Strategies",
        "authors": [
            "Seokeon Choi",
            "Sunghyun Park",
            "Hyoungwoo Park",
            "Jeongho Kim",
            "Sungrack Yun"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Memory-efficient personalization is critical for adapting text-to-image diffusion models while preserving user privacy and operating within the limited computational resources of edge devices. To this end, we propose a selective optimization framework that adaptively chooses between backpropagation on low-resolution images (BP-low) and zeroth-order optimization on high-resolution images (ZO-high), guided by the characteristics of the diffusion process. As observed in our experiments, BP-low efficiently adapts the model to target-specific features, but suffers from structural distortions due to resolution mismatch. Conversely, ZO-high refines high-resolution details with minimal memory overhead but faces slow convergence when applied without prior adaptation. By complementing both methods, our framework leverages BP-low for effective personalization while using ZO-high to maintain structural consistency, achieving memory-efficient and high-quality fine-tuning. To maximize the efficacy of both BP-low and ZO-high, we introduce a timestep-aware probabilistic function that dynamically selects the appropriate optimization strategy based on diffusion timesteps. This function mitigates the overfitting from BP-low at high timesteps, where structural information is critical, while ensuring ZO-high is applied more effectively as training progresses. Experimental results demonstrate that our method achieves competitive performance while significantly reducing memory consumption, enabling scalable, high-quality on-device personalization without increasing inference latency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 107,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10034",
        "abs_url": "https://arxiv.org/abs/2507.10034",
        "pdf_url": "https://arxiv.org/pdf/2507.10034",
        "title": "LifelongPR: Lifelong knowledge fusion for point cloud place recognition based on replay and prompt learning",
        "authors": [
            "Xianghong Zou",
            "Jianping Li",
            "Zhe Chen",
            "Zhen Cao",
            "Zhen Dong",
            "Qiegen Liu",
            "Bisheng Yang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
        "abstract": "Point cloud place recognition (PCPR) plays a crucial role in photogrammetry and robotics applications such as autonomous driving, intelligent transportation, and augmented reality. In real-world large-scale deployments of a positioning system, PCPR models must continuously acquire, update, and accumulate knowledge to adapt to diverse and dynamic environments, i.e., the ability known as continual learning (CL). However, existing PCPR models often suffer from catastrophic forgetting, leading to significant performance degradation in previously learned scenes when adapting to new environments or sensor types. This results in poor model scalability, increased maintenance costs, and system deployment difficulties, undermining the practicality of PCPR. To address these issues, we propose LifelongPR, a novel continual learning framework for PCPR, which effectively extracts and fuses knowledge from sequential point cloud data. First, to alleviate the knowledge loss, we propose a replay sample selection method that dynamically allocates sample sizes according to each dataset's information quantity and selects spatially diverse samples for maximal representativeness. Second, to handle domain shifts, we design a prompt learning-based CL framework with a lightweight prompt module and a two-stage training strategy, enabling domain-specific feature adaptation while minimizing forgetting. Comprehensive experiments on large-scale public and self-collected datasets are conducted to validate the effectiveness of the proposed method. Compared with state-of-the-art (SOTA) methods, our method achieves 6.50% improvement in mIR@1, 7.96% improvement in mR@1, and an 8.95% reduction in F. The code and pre-trained models are publicly available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 108,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10053",
        "abs_url": "https://arxiv.org/abs/2507.10053",
        "pdf_url": "https://arxiv.org/pdf/2507.10053",
        "title": "CoSMo: A Multimodal Transformer for Page Stream Segmentation in Comic Books",
        "authors": [
            "Marc Serra Ortega",
            "Emanuele Vivoli",
            "Artemis Llabrés",
            "Dimosthenis Karatzas"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "This paper introduces CoSMo, a novel multimodal Transformer for Page Stream Segmentation (PSS) in comic books, a critical task for automated content understanding, as it is a necessary first stage for many downstream tasks like character analysis, story indexing, or metadata enrichment. We formalize PSS for this unique medium and curate a new 20,800-page annotated dataset. CoSMo, developed in vision-only and multimodal variants, consistently outperforms traditional baselines and significantly larger general-purpose vision-language models across F1-Macro, Panoptic Quality, and stream-level metrics. Our findings highlight the dominance of visual features for comic PSS macro-structure, yet demonstrate multimodal benefits in resolving challenging ambiguities. CoSMo establishes a new state-of-the-art, paving the way for scalable comic book analysis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 109,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10056",
        "abs_url": "https://arxiv.org/abs/2507.10056",
        "pdf_url": "https://arxiv.org/pdf/2507.10056",
        "title": "Lightweight Model for Poultry Disease Detection from Fecal Images Using Multi-Color Space Feature Optimization and Machine Learning",
        "authors": [
            "A. K. M. Shoriful Islam",
            "Md. Rakib Hassan",
            "Macbah Uddin",
            "Md. Shahidur Rahman"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Poultry farming is a vital component of the global food supply chain, yet it remains highly vulnerable to infectious diseases such as coccidiosis, salmonellosis, and Newcastle disease. This study proposes a lightweight machine learning-based approach to detect these diseases by analyzing poultry fecal images. We utilize multi-color space feature extraction (RGB, HSV, LAB) and explore a wide range of color, texture, and shape-based descriptors, including color histograms, local binary patterns (LBP), wavelet transforms, and edge detectors. Through a systematic ablation study and dimensionality reduction using PCA and XGBoost feature selection, we identify a compact global feature set that balances accuracy and computational efficiency. An artificial neural network (ANN) classifier trained on these features achieved 95.85% accuracy while requiring no GPU and only 638 seconds of execution time in Google Colab. Compared to deep learning models such as Xception and MobileNetV3, our proposed model offers comparable accuracy with drastically lower resource usage. This work demonstrates a cost-effective, interpretable, and scalable alternative to deep learning for real-time poultry disease detection in low-resource agricultural settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 110,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10065",
        "abs_url": "https://arxiv.org/abs/2507.10065",
        "pdf_url": "https://arxiv.org/pdf/2507.10065",
        "title": "MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second",
        "authors": [
            "Chenguo Lin",
            "Yuchen Lin",
            "Panwang Pan",
            "Yifan Yu",
            "Honglei Yan",
            "Katerina Fragkiadaki",
            "Yadong Mu"
        ],
        "comments": "Project page: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We present MoVieS, a novel feed-forward model that synthesizes 4D dynamic novel views from monocular videos in one second. MoVieS represents dynamic 3D scenes using pixel-aligned grids of Gaussian primitives, explicitly supervising their time-varying motion. This allows, for the first time, the unified modeling of appearance, geometry and motion, and enables view synthesis, reconstruction and 3D point tracking within a single learning-based framework. By bridging novel view synthesis with dynamic geometry reconstruction, MoVieS enables large-scale training on diverse datasets with minimal dependence on task-specific supervision. As a result, it also naturally supports a wide range of zero-shot applications, such as scene flow estimation and moving object segmentation. Extensive experiments validate the effectiveness and efficiency of MoVieS across multiple tasks, achieving competitive performance while offering several orders of magnitude speedups.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 111,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10072",
        "abs_url": "https://arxiv.org/abs/2507.10072",
        "pdf_url": "https://arxiv.org/pdf/2507.10072",
        "title": "Frequency Regulation for Exposure Bias Mitigation in Diffusion Models",
        "authors": [
            "Meng Yu",
            "Kun Zhan"
        ],
        "comments": "ACM Multimedia 2025 accepted!",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Diffusion models exhibit impressive generative capabilities but are significantly impacted by exposure bias. In this paper, we make a key observation: the energy of the predicted noisy images decreases during the diffusion process. Building on this, we identify two important findings: 1) The reduction in energy follows distinct patterns in the low-frequency and high-frequency subbands; 2) This energy reduction results in amplitude variations between the network-reconstructed clean data and the real clean data. Based on the first finding, we introduce a frequency-domain regulation mechanism utilizing wavelet transforms, which separately adjusts the low- and high-frequency subbands. Leveraging the second insight, we provide a more accurate analysis of exposure bias in the two subbands. Our method is training-free and plug-and-play, significantly improving the generative quality of various diffusion models and providing a robust solution to exposure bias across different model architectures. The source code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 112,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10084",
        "abs_url": "https://arxiv.org/abs/2507.10084",
        "pdf_url": "https://arxiv.org/pdf/2507.10084",
        "title": "A Transfer Learning-Based Method for Water Body Segmentation in Remote Sensing Imagery: A Case Study of the Zhada Tulin Area",
        "authors": [
            "Haonan Chen",
            "Xin Tong"
        ],
        "comments": "13 pages, 6 figures, 2 tables",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "To address the prevalent challenges of domain shift and small sample sizes in remote sensing image water body segmentation, this study proposes and validates a two-stage transfer learning strategy based on the SegFormer model. The approach begins by training a foundational segmentation model on a diverse source domain, where it achieves an Intersection over Union (IoU) of 68.80% on its validation set, followed by fine-tuning on data from the distinct target domain. Focusing on the Zhada Tulin area in Tibet -- a region characterized by highly complex topography and spectral features -- the experimental results demonstrate that this strategy significantly boosts the IoU for the water body segmentation task from 25.50% (for direct transfer) to 64.84%. This not only effectively resolves the model performance degradation caused by domain discrepancy but also provides an effective technical paradigm for high-precision thematic information extraction in data-scarce and environmentally unique remote sensing scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 113,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10095",
        "abs_url": "https://arxiv.org/abs/2507.10095",
        "pdf_url": "https://arxiv.org/pdf/2507.10095",
        "title": "FIX-CLIP: Dual-Branch Hierarchical Contrastive Learning via Synthetic Captions for Better Understanding of Long Text",
        "authors": [
            "Bingchao Wang",
            "Zhiwei Ning",
            "Jianyu Ding",
            "Xuanang Gao",
            "Yin Li",
            "Dongsheng Jiang",
            "Jie Yang",
            "Wei Liu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "CLIP has shown promising performance across many short-text tasks in a zero-shot manner. However, limited by the input length of the text encoder, CLIP struggles on under-stream tasks with long-text inputs (>77 tokens). To remedy this issue, we propose FIX-CLIP which includes three novel modules: (1) A dual-branch training pipeline that aligns short and long texts with masked and raw images respectively, which boosts the long-text representation while preserving the short-text ability. (2) Multiple learnable regional prompts with unidirectional masks in Transformer layers for regional information extraction. (3) A hierarchical feature alignment module in the intermediate encoder layers to promote the consistency of multi-scale features. Furthermore, we collect 30M images and utilize existing MLLMs to synthesize long-text captions for training. Extensive experiments show that FIX-CLIP achieves state-of-the-art performance on both long-text and short-text retrieval benchmarks. For downstream applications, we reveal that FIX-CLIP's text encoder delivers promising performance in a plug-and-play manner for diffusion models with long-text input.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 114,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10115",
        "abs_url": "https://arxiv.org/abs/2507.10115",
        "pdf_url": "https://arxiv.org/pdf/2507.10115",
        "title": "Glance-MCMT: A General MCMT Framework with Glance Initialization and Progressive Association",
        "authors": [
            "Hamidreza Hashempoor"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We propose a multi-camera multi-target (MCMT) tracking framework that ensures consistent global identity assignment across views using trajectory and appearance cues. The pipeline starts with BoT-SORT-based single-camera tracking, followed by an initial glance phase to initialize global IDs via trajectory-feature matching. In later frames, new tracklets are matched to existing global identities through a prioritized global matching strategy. New global IDs are only introduced when no sufficiently similar trajectory or feature match is found. 3D positions are estimated using depth maps and calibration for spatial validation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 115,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10118",
        "abs_url": "https://arxiv.org/abs/2507.10118",
        "pdf_url": "https://arxiv.org/pdf/2507.10118",
        "title": "DEARLi: Decoupled Enhancement of Recognition and Localization for Semi-supervised Panoptic Segmentation",
        "authors": [
            "Ivan Martinović",
            "Josip Šarić",
            "Marin Oršić",
            "Matej Kristan",
            "Siniša Šegvić"
        ],
        "comments": "ICCV 2025 Findings Workshop",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Pixel-level annotation is expensive and time-consuming. Semi-supervised segmentation methods address this challenge by learning models on few labeled images alongside a large corpus of unlabeled images. Although foundation models could further account for label scarcity, effective mechanisms for their exploitation remain underexplored. We address this by devising a novel semi-supervised panoptic approach fueled by two dedicated foundation models. We enhance recognition by complementing unsupervised mask-transformer consistency with zero-shot classification of CLIP features. We enhance localization by class-agnostic decoder warm-up with respect to SAM pseudo-labels. The resulting decoupled enhancement of recognition and localization (DEARLi) particularly excels in the most challenging semi-supervised scenarios with large taxonomies and limited labeled data. Moreover, DEARLi outperforms the state of the art in semi-supervised semantic segmentation by a large margin while requiring 8x less GPU memory, in spite of being trained only for the panoptic objective. We observe 29.9 PQ and 38.9 mIoU on ADE20K with only 158 labeled images. The source code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 116,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10127",
        "abs_url": "https://arxiv.org/abs/2507.10127",
        "pdf_url": "https://arxiv.org/pdf/2507.10127",
        "title": "Taming Modern Point Tracking for Speckle Tracking Echocardiography via Impartial Motion",
        "authors": [
            "Md Abulkalam Azad",
            "John Nyberg",
            "Håvard Dalen",
            "Bjørnar Grenne",
            "Lasse Lovstakken",
            "Andreas Østvik"
        ],
        "comments": "Accepted to CVAMD workshop at ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Accurate motion estimation for tracking deformable tissues in echocardiography is essential for precise cardiac function measurements. While traditional methods like block matching or optical flow struggle with intricate cardiac motion, modern point tracking approaches remain largely underexplored in this domain. This work investigates the potential of state-of-the-art (SOTA) point tracking methods for ultrasound, with a focus on echocardiography. Although these novel approaches demonstrate strong performance in general videos, their effectiveness and generalizability in echocardiography remain limited. By analyzing cardiac motion throughout the heart cycle in real B-mode ultrasound videos, we identify that a directional motion bias across different views is affecting the existing training strategies. To mitigate this, we refine the training procedure and incorporate a set of tailored augmentations to reduce the bias and enhance tracking robustness and generalization through impartial cardiac motion. We also propose a lightweight network leveraging multi-scale cost volumes from spatial context alone to challenge the advanced spatiotemporal point tracking models. Experiments demonstrate that fine-tuning with our strategies significantly improves models' performances over their baselines, even for out-of-distribution (OOD) cases. For instance, EchoTracker boosts overall position accuracy by 60.7% and reduces median trajectory error by 61.5% across heart cycle phases. Interestingly, several point tracking models fail to outperform our proposed simple model in terms of tracking accuracy and generalization, reflecting their limitations when applied to echocardiography. Nevertheless, clinical evaluation reveals that these methods improve GLS measurements, aligning more closely with expert-validated, semi-automated tools and thus demonstrating better reproducibility in real-world applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 117,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10143",
        "abs_url": "https://arxiv.org/abs/2507.10143",
        "pdf_url": "https://arxiv.org/pdf/2507.10143",
        "title": "Deep Recurrence for Dynamical Segmentation Models",
        "authors": [
            "David Calhas",
            "Arlindo L. Oliveira"
        ],
        "comments": "12 pages",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "While biological vision systems rely heavily on feedback connections to iteratively refine perception, most artificial neural networks remain purely feedforward, processing input in a single static pass. In this work, we propose a predictive coding inspired feedback mechanism that introduces a recurrent loop from output to input, allowing the model to refine its internal state over time. We implement this mechanism within a standard U-Net architecture and introduce two biologically motivated operations, softmax projection and exponential decay, to ensure stability of the feedback loop. Through controlled experiments on a synthetic segmentation task, we show that the feedback model significantly outperforms its feedforward counterpart in noisy conditions and generalizes more effectively with limited supervision. Notably, feedback achieves above random performance with just two training examples, while the feedforward model requires at least four. Our findings demonstrate that feedback enhances robustness and data efficiency, and offer a path toward more adaptive and biologically inspired neural architectures. Code is available at: this http URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 118,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10171",
        "abs_url": "https://arxiv.org/abs/2507.10171",
        "pdf_url": "https://arxiv.org/pdf/2507.10171",
        "title": "SlumpGuard: An AI-Powered Real-Time System for Automated Concrete Slump Prediction via Video Analysis",
        "authors": [
            "Youngmin Kim",
            "Giyeong Oh",
            "Kwangsoo Youm",
            "Youngjae Yu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Concrete workability is essential for construction quality, with the slump test being the most common on-site method for its assessment. However, traditional slump testing is manual, time-consuming, and prone to inconsistency, limiting its applicability for real-time monitoring. To address these challenges, we propose SlumpGuard, an AI-powered, video-based system that automatically analyzes concrete flow from the truck chute to assess workability in real time. Our system enables full-batch inspection without manual intervention, improving both the accuracy and efficiency of quality control. We present the system design, a the construction of a dedicated dataset, and empirical results from real-world deployment, demonstrating the effectiveness of SlumpGuard as a practical solution for modern concrete quality assurance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 119,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10195",
        "abs_url": "https://arxiv.org/abs/2507.10195",
        "pdf_url": "https://arxiv.org/pdf/2507.10195",
        "title": "Minimizing the Pretraining Gap: Domain-aligned Text-Based Person Retrieval",
        "authors": [
            "Shuyu Yang",
            "Yaxiong Wang",
            "Yongrui Li",
            "Li Zhu",
            "Zhedong Zheng"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In this work, we focus on text-based person retrieval, which aims to identify individuals based on textual descriptions. Given the significant privacy issues and the high cost associated with manual annotation, synthetic data has become a popular choice for pretraining models, leading to notable advancements. However, the considerable domain gap between synthetic pretraining datasets and real-world target datasets, characterized by differences in lighting, color, and viewpoint, remains a critical obstacle that hinders the effectiveness of the pretrain-finetune paradigm. To bridge this gap, we introduce a unified text-based person retrieval pipeline considering domain adaptation at both image and region levels. In particular, it contains two primary components, i.e., Domain-aware Diffusion (DaD) for image-level adaptation and Multi-granularity Relation Alignment (MRA) for region-level adaptation. As the name implies, Domain-aware Diffusion is to migrate the distribution of images from the pretraining dataset domain to the target real-world dataset domain, e.g., CUHK-PEDES. Subsequently, MRA performs a meticulous region-level alignment by establishing correspondences between visual regions and their descriptive sentences, thereby addressing disparities at a finer granularity. Extensive experiments show that our dual-level adaptation method has achieved state-of-the-art results on the CUHK-PEDES, ICFG-PEDES, and RSTPReid datasets, outperforming existing methodologies. The dataset, model, and code are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 120,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10202",
        "abs_url": "https://arxiv.org/abs/2507.10202",
        "pdf_url": "https://arxiv.org/pdf/2507.10202",
        "title": "A Training-Free, Task-Agnostic Framework for Enhancing MLLM Performance on High-Resolution Images",
        "authors": [
            "Jaeseong Lee",
            "Yeeun Choi",
            "Heechan Choi",
            "Hanjung Kim",
            "Seonjoo Kim"
        ],
        "comments": "Accepted at CVPR 2025 Workshop on Emergent Visual Abilities and Limits of Foundation Models",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in vision-language understanding, reasoning, and generation. However, they struggle with tasks requiring fine-grained localization and reasoning in high-resolution images. This constraint stems from the fact that MLLMs are fine-tuned with fixed image resolution to align with the pre-trained image encoder used in MLLM. Consequently, feeding high-resolution images directly into MLLMs leads to poor generalization due to a train-test resolution discrepancy, while downsampling these images-although ensuring consistency-compromises fine-grained visual details and ultimately degrades performance. To address this challenge, we propose Extract Candidate then Predict (ECP), a novel training-free, task-agnostic two-stage framework designed to enhance MLLM performance on high-resolution images. The key intuition behind ECP is that while MLLMs struggle with high-resolution images, their predictions on downsampled images still contain implicit localization cues. By first identifying candidate region using the coarse prediction and then predicting the final output based on candidate region, ECP effectively preserves fine-grained details while mitigating the challenges posed by high-resolution data. We validate our framework on 4K GUI grounding and 4K, 8K MLLM perception, achieving +21.3%, +5.8%, +5.2% absolute improvement compared to baseline respectively, demonstrating its effectiveness. Code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 121,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10203",
        "abs_url": "https://arxiv.org/abs/2507.10203",
        "pdf_url": "https://arxiv.org/pdf/2507.10203",
        "title": "Improving Multimodal Learning via Imbalanced Learning",
        "authors": [
            "Shicai Wei",
            "Chunbo Luo",
            "Yang Luo"
        ],
        "comments": "Accepted to ICCV2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Multimodal learning often encounters the under-optimized problem and may perform worse than unimodal learning. Existing approaches attribute this issue to imbalanced learning across modalities and tend to address it through gradient balancing. However, this paper argues that balanced learning is not the optimal setting for multimodal learning. With bias-variance analysis, we prove that imbalanced dependency on each modality obeying the inverse ratio of their variances contributes to optimal performance. To this end, we propose the Asymmetric Representation Learning(ARL) strategy to assist multimodal learning via imbalanced optimization. ARL introduces auxiliary regularizers for each modality encoder to calculate their prediction variance. ARL then calculates coefficients via the unimodal variance to re-weight the optimization of each modality, forcing the modality dependence ratio to be inversely proportional to the modality variance ratio. Moreover, to minimize the generalization error, ARL further introduces the prediction bias of each modality and jointly optimizes them with multimodal loss. Notably, all auxiliary regularizers share parameters with the multimodal model and rely only on the modality representation. Thus the proposed ARL strategy introduces no extra parameters and is independent of the structures and fusion methods of the multimodal model. Finally, extensive experiments on various datasets validate the effectiveness and versatility of ARL. Code is available at \\href{this https URL}{this https URL}",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 122,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10209",
        "abs_url": "https://arxiv.org/abs/2507.10209",
        "pdf_url": "https://arxiv.org/pdf/2507.10209",
        "title": "Is Micro-expression Ethnic Leaning?",
        "authors": [
            "Huai-Qian Khor",
            "Yante Li",
            "Xingxun Jiang",
            "Guoying Zhao"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "How much does ethnicity play its part in emotional expression? Emotional expression and micro-expression research probe into understanding human psychological responses to emotional stimuli, thereby revealing substantial hidden yet authentic emotions that can be useful in the event of diagnosis and interviews. While increased attention had been provided to micro-expression analysis, the studies were done under Ekman's assumption of emotion universality, where emotional expressions are identical across cultures and social contexts. Our computational study uncovers some of the influences of ethnic background in expression analysis, leading to an argument that the emotional universality hypothesis is an overgeneralization from the perspective of manual psychological analysis. In this research, we propose to investigate the level of influence of ethnicity in a simulated micro-expression scenario. We construct a cross-cultural micro-expression database and algorithmically annotate the ethnic labels to facilitate the investigation. With the ethnically annotated dataset, we perform a prima facie study to compare mono-ethnicity and stereo-ethnicity in a controlled environment, which uncovers a certain influence of ethnic bias via an experimental way. Building on this finding, we propose a framework that integrates ethnic context into the emotional feature learning process, yielding an ethnically aware framework that recognises ethnicity differences in micro-expression recognition. For improved understanding, qualitative analyses have been done to solidify the preliminary investigation into this new realm of research. Code is publicly available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 123,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10213",
        "abs_url": "https://arxiv.org/abs/2507.10213",
        "pdf_url": "https://arxiv.org/pdf/2507.10213",
        "title": "Boosting Multimodal Learning via Disentangled Gradient Learning",
        "authors": [
            "Shicai Wei",
            "Chunbo Luo",
            "Yang Luo"
        ],
        "comments": "Accepted to ICCV2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Multimodal learning often encounters the under-optimized problem and may have worse performance than unimodal learning. Existing methods attribute this problem to the imbalanced learning between modalities and rebalance them through gradient modulation. However, they fail to explain why the dominant modality in multimodal models also underperforms that in unimodal learning. In this work, we reveal the optimization conflict between the modality encoder and modality fusion module in multimodal models. Specifically, we prove that the cross-modal fusion in multimodal models decreases the gradient passed back to each modality encoder compared with unimodal models. Consequently, the performance of each modality in the multimodal model is inferior to that in the unimodal model. To this end, we propose a disentangled gradient learning (DGL) framework to decouple the optimization of the modality encoder and modality fusion module in the multimodal model. DGL truncates the gradient back-propagated from the multimodal loss to the modality encoder and replaces it with the gradient from unimodal loss. Besides, DGL removes the gradient back-propagated from the unimodal loss to the modality fusion module. This helps eliminate the gradient interference between the modality encoder and modality fusion module while ensuring their respective optimization processes. Finally, extensive experiments on multiple types of modalities, tasks, and frameworks with dense cross-modal interaction demonstrate the effectiveness and versatility of the proposed DGL. Code is available at \\href{this https URL}{this https URL}",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 124,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10217",
        "abs_url": "https://arxiv.org/abs/2507.10217",
        "pdf_url": "https://arxiv.org/pdf/2507.10217",
        "title": "From Wardrobe to Canvas: Wardrobe Polyptych LoRA for Part-level Controllable Human Image Generation",
        "authors": [
            "Jeongho Kim",
            "Sunghyun Park",
            "Hyoungwoo Park",
            "Sungrack Yun",
            "Jaegul Choo",
            "Seokeon Cho"
        ],
        "comments": "10 pages, 8 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Recent diffusion models achieve personalization by learning specific subjects, allowing learned attributes to be integrated into generated images. However, personalized human image generation remains challenging due to the need for precise and consistent attribute preservation (e.g., identity, clothing details). Existing subject-driven image generation methods often require either (1) inference-time fine-tuning with few images for each new subject or (2) large-scale dataset training for generalization. Both approaches are computationally expensive and impractical for real-time applications. To address these limitations, we present Wardrobe Polyptych LoRA, a novel part-level controllable model for personalized human image generation. By training only LoRA layers, our method removes the computational burden at inference while ensuring high-fidelity synthesis of unseen subjects. Our key idea is to condition the generation on the subject's wardrobe and leverage spatial references to reduce information loss, thereby improving fidelity and consistency. Additionally, we introduce a selective subject region loss, which encourages the model to disregard some of reference images during training. Our loss ensures that generated images better align with text prompts while maintaining subject integrity. Notably, our Wardrobe Polyptych LoRA requires no additional parameters at the inference stage and performs generation using a single model trained on a few training samples. We construct a new dataset and benchmark tailored for personalized human image generation. Extensive experiments show that our approach significantly outperforms existing techniques in fidelity and consistency, enabling realistic and identity-preserving full-body synthesis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 125,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10218",
        "abs_url": "https://arxiv.org/abs/2507.10218",
        "pdf_url": "https://arxiv.org/pdf/2507.10218",
        "title": "Straighten Viscous Rectified Flow via Noise Optimization",
        "authors": [
            "Jimin Dai",
            "Jiexi Yan",
            "Jian Yang",
            "Lei Luo"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The Reflow operation aims to straighten the inference trajectories of the rectified flow during training by constructing deterministic couplings between noises and images, thereby improving the quality of generated images in single-step or few-step generation. However, we identify critical limitations in Reflow, particularly its inability to rapidly generate high-quality images due to a distribution gap between images in its constructed deterministic couplings and real images. To address these shortcomings, we propose a novel alternative called Straighten Viscous Rectified Flow via Noise Optimization (VRFNO), which is a joint training framework integrating an encoder and a neural velocity field. VRFNO introduces two key innovations: (1) a historical velocity term that enhances trajectory distinction, enabling the model to more accurately predict the velocity of the current trajectory, and (2) the noise optimization through reparameterization to form optimized couplings with real images which are then utilized for training, effectively mitigating errors caused by Reflow's limitations. Comprehensive experiments on synthetic data and real datasets with varying resolutions show that VRFNO significantly mitigates the limitations of Reflow, achieving state-of-the-art performance in both one-step and few-step generation tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 126,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10222",
        "abs_url": "https://arxiv.org/abs/2507.10222",
        "pdf_url": "https://arxiv.org/pdf/2507.10222",
        "title": "Spatial Lifting for Dense Prediction",
        "authors": [
            "Mingzhi Xu",
            "Yizhe Zhang"
        ],
        "comments": "Preprint. Under review",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Image and Video Processing (eess.IV)",
        "abstract": "We present Spatial Lifting (SL), a novel methodology for dense prediction tasks. SL operates by lifting standard inputs, such as 2D images, into a higher-dimensional space and subsequently processing them using networks designed for that higher dimension, such as a 3D U-Net. Counterintuitively, this dimensionality lifting allows us to achieve good performance on benchmark tasks compared to conventional approaches, while reducing inference costs and significantly lowering the number of model parameters. The SL framework produces intrinsically structured outputs along the lifted dimension. This emergent structure facilitates dense supervision during training and enables robust, near-zero-additional-cost prediction quality assessment at test time. We validate our approach across 19 benchmark datasets (13 for semantic segmentation and 6 for depth estimation), demonstrating competitive dense prediction performance while reducing the model parameter count by over 98% (in the U-Net case) and lowering inference costs. Spatial Lifting introduces a new vision modeling paradigm that offers a promising path toward more efficient, accurate, and reliable deep networks for dense prediction tasks in vision.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 127,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10223",
        "abs_url": "https://arxiv.org/abs/2507.10223",
        "pdf_url": "https://arxiv.org/pdf/2507.10223",
        "title": "ProGait: A Multi-Purpose Video Dataset and Benchmark for Transfemoral Prosthesis Users",
        "authors": [
            "Xiangyu Yin",
            "Boyuan Yang",
            "Weichen Liu",
            "Qiyao Xue",
            "Abrar Alamri",
            "Goeran Fiedler",
            "Wei Gao"
        ],
        "comments": "Accepted by ICCV'25",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Prosthetic legs play a pivotal role in clinical rehabilitation, allowing individuals with lower-limb amputations the ability to regain mobility and improve their quality of life. Gait analysis is fundamental for optimizing prosthesis design and alignment, directly impacting the mobility and life quality of individuals with lower-limb amputations. Vision-based machine learning (ML) methods offer a scalable and non-invasive solution to gait analysis, but face challenges in correctly detecting and analyzing prosthesis, due to their unique appearances and new movement patterns. In this paper, we aim to bridge this gap by introducing a multi-purpose dataset, namely ProGait, to support multiple vision tasks including Video Object Segmentation, 2D Human Pose Estimation, and Gait Analysis (GA). ProGait provides 412 video clips from four above-knee amputees when testing multiple newly-fitted prosthetic legs through walking trials, and depicts the presence, contours, poses, and gait patterns of human subjects with transfemoral prosthetic legs. Alongside the dataset itself, we also present benchmark tasks and fine-tuned baseline models to illustrate the practical application and performance of the ProGait dataset. We compared our baseline models against pre-trained vision models, demonstrating improved generalizability when applying the ProGait dataset for prosthesis-specific tasks. Our code is available at this https URL and dataset at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 128,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10225",
        "abs_url": "https://arxiv.org/abs/2507.10225",
        "pdf_url": "https://arxiv.org/pdf/2507.10225",
        "title": "Synthesizing Near-Boundary OOD Samples for Out-of-Distribution Detection",
        "authors": [
            "Jinglun Li",
            "Kaixun Jiang",
            "Zhaoyu Chen",
            "Bo Lin",
            "Yao Tang",
            "Weifeng Ge",
            "Wenqiang Zhang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Pre-trained vision-language models have exhibited remarkable abilities in detecting out-of-distribution (OOD) samples. However, some challenging OOD samples, which lie close to in-distribution (InD) data in image feature space, can still lead to misclassification. The emergence of foundation models like diffusion models and multimodal large language models (MLLMs) offers a potential solution to this issue. In this work, we propose SynOOD, a novel approach that harnesses foundation models to generate synthetic, challenging OOD data for fine-tuning CLIP models, thereby enhancing boundary-level discrimination between InD and OOD samples. Our method uses an iterative in-painting process guided by contextual prompts from MLLMs to produce nuanced, boundary-aligned OOD samples. These samples are refined through noise adjustments based on gradients from OOD scores like the energy score, effectively sampling from the InD/OOD boundary. With these carefully synthesized images, we fine-tune the CLIP image encoder and negative label features derived from the text encoder to strengthen connections between near-boundary OOD samples and a set of negative labels. Finally, SynOOD achieves state-of-the-art performance on the large-scale ImageNet benchmark, with minimal increases in parameters and runtime. Our approach significantly surpasses existing methods, improving AUROC by 2.80% and reducing FPR95 by 11.13%. Codes are available in this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 129,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10236",
        "abs_url": "https://arxiv.org/abs/2507.10236",
        "pdf_url": "https://arxiv.org/pdf/2507.10236",
        "title": "Navigating the Challenges of AI-Generated Image Detection in the Wild: What Truly Matters?",
        "authors": [
            "Despina Konstantinidou",
            "Dimitrios Karageorgiou",
            "Christos Koutlis",
            "Olga Papadopoulou",
            "Emmanouil Schinas",
            "Symeon Papadopoulos"
        ],
        "comments": "35 pages, 4 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The rapid advancement of generative technologies presents both unprecedented creative opportunities and significant challenges, particularly in maintaining social trust and ensuring the integrity of digital information. Following these concerns, the challenge of AI-Generated Image Detection (AID) becomes increasingly critical. As these technologies become more sophisticated, the quality of AI-generated images has reached a level that can easily deceive even the most discerning observers. Our systematic evaluation highlights a critical weakness in current AI-Generated Image Detection models: while they perform exceptionally well on controlled benchmark datasets, they struggle significantly with real-world variations. To assess this, we introduce ITW-SM, a new dataset of real and AI-generated images collected from major social media platforms. In this paper, we identify four key factors that influence AID performance in real-world scenarios: backbone architecture, training data composition, pre-processing strategies and data augmentation combinations. By systematically analyzing these components, we shed light on their impact on detection efficacy. Our modifications result in an average AUC improvement of 26.87% across various AID models under real-world conditions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 130,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10239",
        "abs_url": "https://arxiv.org/abs/2507.10239",
        "pdf_url": "https://arxiv.org/pdf/2507.10239",
        "title": "Transferring Styles for Reduced Texture Bias and Improved Robustness in Semantic Segmentation Networks",
        "authors": [
            "Ben Hamscher",
            "Edgar Heinert",
            "Annika Mütze",
            "Kira Maag",
            "Matthias Rottmann"
        ],
        "comments": "accepted at ECAI 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Recent research has investigated the shape and texture biases of deep neural networks (DNNs) in image classification which influence their generalization capabilities and robustness. It has been shown that, in comparison to regular DNN training, training with stylized images reduces texture biases in image classification and improves robustness with respect to image corruptions. In an effort to advance this line of research, we examine whether style transfer can likewise deliver these two effects in semantic segmentation. To this end, we perform style transfer with style varying across artificial image areas. Those random areas are formed by a chosen number of Voronoi cells. The resulting style-transferred data is then used to train semantic segmentation DNNs with the objective of reducing their dependence on texture cues while enhancing their reliance on shape-based features. In our experiments, it turns out that in semantic segmentation, style transfer augmentation reduces texture bias and strongly increases robustness with respect to common image corruptions as well as adversarial attacks. These observations hold for convolutional neural networks and transformer architectures on the Cityscapes dataset as well as on PASCAL Context, showing the generality of the proposed method.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 131,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10265",
        "abs_url": "https://arxiv.org/abs/2507.10265",
        "pdf_url": "https://arxiv.org/pdf/2507.10265",
        "title": "Kaleidoscopic Background Attack: Disrupting Pose Estimation with Multi-Fold Radial Symmetry Textures",
        "authors": [
            "Xinlong Ding",
            "Hongwei Yu",
            "Jiawei Li",
            "Feifan Li",
            "Yu Shang",
            "Bochao Zou",
            "Huimin Ma",
            "Jiansheng Chen"
        ],
        "comments": "Accepted at ICCV 2025. Project page is available at this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Camera pose estimation is a fundamental computer vision task that is essential for applications like visual localization and multi-view stereo reconstruction. In the object-centric scenarios with sparse inputs, the accuracy of pose estimation can be significantly influenced by background textures that occupy major portions of the images across different viewpoints. In light of this, we introduce the Kaleidoscopic Background Attack (KBA), which uses identical segments to form discs with multi-fold radial symmetry. These discs maintain high similarity across different viewpoints, enabling effective attacks on pose estimation models even with natural texture segments. Additionally, a projected orientation consistency loss is proposed to optimize the kaleidoscopic segments, leading to significant enhancement in the attack effectiveness. Experimental results show that optimized adversarial kaleidoscopic backgrounds can effectively attack various camera pose estimation models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 132,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10283",
        "abs_url": "https://arxiv.org/abs/2507.10283",
        "pdf_url": "https://arxiv.org/pdf/2507.10283",
        "title": "FTCFormer: Fuzzy Token Clustering Transformer for Image Classification",
        "authors": [
            "Muyi Bao",
            "Changyu Zeng",
            "Yifan Wang",
            "Zhengni Yang",
            "Zimu Wang",
            "Guangliang Cheng",
            "Jun Qi",
            "Wei Wang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Transformer-based deep neural networks have achieved remarkable success across various computer vision tasks, largely attributed to their long-range self-attention mechanism and scalability. However, most transformer architectures embed images into uniform, grid-based vision tokens, neglecting the underlying semantic meanings of image regions, resulting in suboptimal feature representations. To address this issue, we propose Fuzzy Token Clustering Transformer (FTCFormer), which incorporates a novel clustering-based downsampling module to dynamically generate vision tokens based on the semantic meanings instead of spatial positions. It allocates fewer tokens to less informative regions and more to represent semantically important regions, regardless of their spatial adjacency or shape irregularity. To further enhance feature extraction and representation, we propose a Density Peak Clustering-Fuzzy K-Nearest Neighbor (DPC-FKNN) mechanism for clustering center determination, a Spatial Connectivity Score (SCS) for token assignment, and a channel-wise merging (Cmerge) strategy for token merging. Extensive experiments on 32 datasets across diverse domains validate the effectiveness of FTCFormer on image classification, showing consistent improvements over the TCFormer baseline, achieving gains of improving 1.43% on five fine-grained datasets, 1.09% on six natural image datasets, 0.97% on three medical datasets and 0.55% on four remote sensing datasets. The code is available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 133,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10293",
        "abs_url": "https://arxiv.org/abs/2507.10293",
        "pdf_url": "https://arxiv.org/pdf/2507.10293",
        "title": "Show and Polish: Reference-Guided Identity Preservation in Face Video Restoration",
        "authors": [
            "Wenkang Han",
            "Wang Lin",
            "Yiyun Zhou",
            "Qi Liu",
            "Shulei Wang",
            "Chang Yao",
            "Jingyuan Chen"
        ],
        "comments": "Accepted by MM 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Face Video Restoration (FVR) aims to recover high-quality face videos from degraded versions. Traditional methods struggle to preserve fine-grained, identity-specific features when degradation is severe, often producing average-looking faces that lack individual characteristics. To address these challenges, we introduce IP-FVR, a novel method that leverages a high-quality reference face image as a visual prompt to provide identity conditioning during the denoising process. IP-FVR incorporates semantically rich identity information from the reference image using decoupled cross-attention mechanisms, ensuring detailed and identity consistent results. For intra-clip identity drift (within 24 frames), we introduce an identity-preserving feedback learning method that combines cosine similarity-based reward signals with suffix-weighted temporal aggregation. This approach effectively minimizes drift within sequences of frames. For inter-clip identity drift, we develop an exponential blending strategy that aligns identities across clips by iteratively blending frames from previous clips during the denoising process. This method ensures consistent identity representation across different clips. Additionally, we enhance the restoration process with a multi-stream negative prompt, guiding the model's attention to relevant facial attributes and minimizing the generation of low-quality or incorrect features. Extensive experiments on both synthetic and real-world datasets demonstrate that IP-FVR outperforms existing methods in both quality and identity preservation, showcasing its substantial potential for practical applications in face video restoration.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 134,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10300",
        "abs_url": "https://arxiv.org/abs/2507.10300",
        "pdf_url": "https://arxiv.org/pdf/2507.10300",
        "title": "FaceLLM: A Multimodal Large Language Model for Face Understanding",
        "authors": [
            "Hatef Otroshi Shahreza",
            "Sébastien Marcel"
        ],
        "comments": "Accepted in ICCV 2025 workshops",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Multimodal large language models (MLLMs) have shown remarkable performance in vision-language tasks. However, existing MLLMs are primarily trained on generic datasets, limiting their ability to reason on domain-specific visual cues such as those in facial images. In particular, tasks that require detailed understanding of facial structure, expression, emotion, and demographic features remain underexplored by MLLMs due to the lack of large-scale annotated face image-text datasets. In this work, we introduce FaceLLM, a multimodal large language model trained specifically for facial image understanding. To construct the training data, we propose a novel weakly supervised pipeline that uses ChatGPT with attribute-aware prompts to generate high-quality question-answer pairs based on images from the FairFace dataset. The resulting corpus, called FairFaceGPT, covers a diverse set of attributes including expression, pose, skin texture, and forensic information. Our experiments demonstrate that FaceLLM improves the performance of MLLMs on various face-centric tasks and achieves state-of-the-art performance. This work highlights the potential of synthetic supervision via language models for building domain-specialized MLLMs, and sets a precedent for trustworthy, human-centric multimodal AI systems. FairFaceGPT dataset and pretrained FaceLLM models are publicly available in the project page.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 135,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10302",
        "abs_url": "https://arxiv.org/abs/2507.10302",
        "pdf_url": "https://arxiv.org/pdf/2507.10302",
        "title": "DisCo: Towards Distinct and Coherent Visual Encapsulation in Video MLLMs",
        "authors": [
            "Jiahe Zhao",
            "Rongkun Zheng",
            "Yi Wang",
            "Helin Wang",
            "Hengshuang Zhao"
        ],
        "comments": "ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In video Multimodal Large Language Models (video MLLMs), the visual encapsulation process plays a pivotal role in converting video contents into representative tokens for LLM input. While linear projectors are widely employed for encapsulation, they introduce semantic indistinctness and temporal incoherence when applied to videos. Conversely, the structure of resamplers shows promise in tackling these challenges, but an effective solution remains unexplored. Drawing inspiration from resampler structures, we introduce DisCo, a novel visual encapsulation method designed to yield semantically distinct and temporally coherent visual tokens for video MLLMs. DisCo integrates two key components: (1) A Visual Concept Discriminator (VCD) module, assigning unique semantics for visual tokens by associating them in pair with discriminative concepts in the video. (2) A Temporal Focus Calibrator (TFC) module, ensuring consistent temporal focus of visual tokens to video elements across every video frame. Through extensive experiments on multiple video MLLM frameworks, we demonstrate that DisCo remarkably outperforms previous state-of-the-art methods across a variety of video understanding benchmarks, while also achieving higher token efficiency thanks to the reduction of semantic indistinctness. The code: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 136,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10306",
        "abs_url": "https://arxiv.org/abs/2507.10306",
        "pdf_url": "https://arxiv.org/pdf/2507.10306",
        "title": "Contrastive Pretraining with Dual Visual Encoders for Gloss-Free Sign Language Translation",
        "authors": [
            "Ozge Mercanoglu Sincan",
            "Richard Bowden"
        ],
        "comments": "Accepted at 9th Workshop on Sign Language Translation and Avatar Technologies (SLTAT), will be held in conjunction with IVA'25",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Sign Language Translation (SLT) aims to convert sign language videos into spoken or written text. While early systems relied on gloss annotations as an intermediate supervision, such annotations are costly to obtain and often fail to capture the full complexity of continuous signing. In this work, we propose a two-phase, dual visual encoder framework for gloss-free SLT, leveraging contrastive visual-language pretraining. During pretraining, our approach employs two complementary visual backbones whose outputs are jointly aligned with each other and with sentence-level text embeddings via a contrastive objective. During the downstream SLT task, we fuse the visual features and input them into an encoder-decoder model. On the Phoenix-2014T benchmark, our dual encoder architecture consistently outperforms its single stream variants and achieves the highest BLEU-4 score among existing gloss-free SLT approaches.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 137,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10318",
        "abs_url": "https://arxiv.org/abs/2507.10318",
        "pdf_url": "https://arxiv.org/pdf/2507.10318",
        "title": "Mind the Gap: Aligning Vision Foundation Models to Image Feature Matching",
        "authors": [
            "Yuhan Liu",
            "Jingwen Fu",
            "Yang Wu",
            "Kangyi Wu",
            "Pengna Li",
            "Jiayi Wu",
            "Sanping Zhou",
            "Jingmin Xin"
        ],
        "comments": "Accepted by ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Leveraging the vision foundation models has emerged as a mainstream paradigm that improves the performance of image feature matching. However, previous works have ignored the misalignment when introducing the foundation models into feature matching. The misalignment arises from the discrepancy between the foundation models focusing on single-image understanding and the cross-image understanding requirement of feature matching. Specifically, 1) the embeddings derived from commonly used foundation models exhibit discrepancies with the optimal embeddings required for feature matching; 2) lacking an effective mechanism to leverage the single-image understanding ability into cross-image understanding. A significant consequence of the misalignment is they struggle when addressing multi-instance feature matching problems. To address this, we introduce a simple but effective framework, called IMD (Image feature Matching with a pre-trained Diffusion model) with two parts: 1) Unlike the dominant solutions employing contrastive-learning based foundation models that emphasize global semantics, we integrate the generative-based diffusion models to effectively capture instance-level details. 2) We leverage the prompt mechanism in generative model as a natural tunnel, propose a novel cross-image interaction prompting module to facilitate bidirectional information interaction between image pairs. To more accurately measure the misalignment, we propose a new benchmark called IMIM, which focuses on multi-instance scenarios. Our proposed IMD establishes a new state-of-the-art in commonly evaluated benchmarks, and the superior improvement 12% in IMIM indicates our method efficiently mitigates the misalignment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 138,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10340",
        "abs_url": "https://arxiv.org/abs/2507.10340",
        "pdf_url": "https://arxiv.org/pdf/2507.10340",
        "title": "Text Embedding Knows How to Quantize Text-Guided Diffusion Models",
        "authors": [
            "Hongjae Lee",
            "Myungjun Son",
            "Dongjea Kang",
            "Seung-Won Jung"
        ],
        "comments": "ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Despite the success of diffusion models in image generation tasks such as text-to-image, the enormous computational complexity of diffusion models limits their use in resource-constrained environments. To address this, network quantization has emerged as a promising solution for designing efficient diffusion models. However, existing diffusion model quantization methods do not consider input conditions, such as text prompts, as an essential source of information for quantization. In this paper, we propose a novel quantization method dubbed Quantization of Language-to-Image diffusion models using text Prompts (QLIP). QLIP leverages text prompts to guide the selection of bit precision for every layer at each time step. In addition, QLIP can be seamlessly integrated into existing quantization methods to enhance quantization efficiency. Our extensive experiments demonstrate the effectiveness of QLIP in reducing computational complexity and improving the quality of the generated images across various datasets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 139,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10343",
        "abs_url": "https://arxiv.org/abs/2507.10343",
        "pdf_url": "https://arxiv.org/pdf/2507.10343",
        "title": "FGSSNet: Feature-Guided Semantic Segmentation of Real World Floorplans",
        "authors": [
            "Hugo Norrby",
            "Gabriel Färm",
            "Kevin Hernandez-Diaz",
            "Fernando Alonso-Fernandez"
        ],
        "comments": "Accepted at International Workshop on Artificial Intelligence and Pattern Recognition, IWAIPR 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We introduce FGSSNet, a novel multi-headed feature-guided semantic segmentation (FGSS) architecture designed to improve the generalization ability of wall segmentation on floorplans. FGSSNet features a U-Net segmentation backbone with a multi-headed dedicated feature extractor used to extract domain-specific feature maps which are injected into the latent space of U-Net to guide the segmentation process. This dedicated feature extractor is trained as an encoder-decoder with selected wall patches, representative of the walls present in the input floorplan, to produce a compressed latent representation of wall patches while jointly trained to predict the wall width. In doing so, we expect that the feature extractor encodes texture and width features of wall patches that are useful to guide the wall segmentation process. Our experiments show increased performance by the use of such injected features in comparison to the vanilla U-Net, highlighting the validity of the proposed approach.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 140,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10355",
        "abs_url": "https://arxiv.org/abs/2507.10355",
        "pdf_url": "https://arxiv.org/pdf/2507.10355",
        "title": "Beyond Graph Model: Reliable VLM Fine-Tuning via Random Graph Adapter",
        "authors": [
            "Bo Jiang",
            "Xueyang Ze",
            "Beibei Wang",
            "Xixi Wang",
            "Xixi Wan",
            "Bin Luo"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Textual adapter-based tuning methods have shown significant potential in transferring knowledge from pre-trained Vision-Language Models (VLMs) to downstream tasks. Existing works generally employ the deterministic textual feature adapter to refine each category textual representation. However, due to inherent factors such as different attributes and contexts, there exists significant diversity in textual descriptions for each category. Such description diversity offers rich discriminative semantic knowledge that can benefit downstream visual learning tasks. Obviously, traditional deterministic adapter model cannot adequately capture this varied semantic information. Also, it is desirable to exploit the inter-class relationships in VLM adapter. To address these issues, we propose to exploit random graph model into VLM adapter and develop a novel Vertex Random Graph Adapter (VRGAdapter). VRGAdapter first models the inherent diverse descriptions of each category and inter-class relationships of different categories simultaneously by leveraging a Vertex Random Knowledge Graph (VRKG) model. Then, it employs probabilistic message propagation on VRKG to learn context-aware distribution representation for each class node. Finally, it adopts a reparameterized sampling function to achieve textual adapter learning. Note that, VRGAdapter provides a more general adapter solution that encompasses traditional graph-based adapter as a special case. In addition, to enable more robust performance for downstream tasks, we also introduce a new Uncertainty-guided Multi-branch Fusion (UMF) scheme that dynamically integrates multiple pre-trained models for ensemble prediction. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our approach.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 141,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10358",
        "abs_url": "https://arxiv.org/abs/2507.10358",
        "pdf_url": "https://arxiv.org/pdf/2507.10358",
        "title": "Fine-Grained Zero-Shot Object Detection",
        "authors": [
            "Hongxu Ma",
            "Chenbo Zhang",
            "Lu Zhang",
            "Jiaogen Zhou",
            "Jihong Guan",
            "Shuigeng Zhou"
        ],
        "comments": "Accepted by ACM MM'25",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Zero-shot object detection (ZSD) aims to leverage semantic descriptions to localize and recognize objects of both seen and unseen classes. Existing ZSD works are mainly coarse-grained object detection, where the classes are visually quite different, thus are relatively easy to distinguish. However, in real life we often have to face fine-grained object detection scenarios, where the classes are too similar to be easily distinguished. For example, detecting different kinds of birds, fishes, and flowers. In this paper, we propose and solve a new problem called Fine-Grained Zero-Shot Object Detection (FG-ZSD for short), which aims to detect objects of different classes with minute differences in details under the ZSD paradigm. We develop an effective method called MSHC for the FG-ZSD task, which is based on an improved two-stage detector and employs a multi-level semantics-aware embedding alignment loss, ensuring tight coupling between the visual and semantic spaces. Considering that existing ZSD datasets are not suitable for the new FG-ZSD task, we build the first FG-ZSD benchmark dataset FGZSD-Birds, which contains 148,820 images falling into 36 orders, 140 families, 579 genera and 1432 species. Extensive experiments on FGZSD-Birds show that our method outperforms existing ZSD models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 142,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10375",
        "abs_url": "https://arxiv.org/abs/2507.10375",
        "pdf_url": "https://arxiv.org/pdf/2507.10375",
        "title": "Test-Time Canonicalization by Foundation Models for Robust Perception",
        "authors": [
            "Utkarsh Singhal",
            "Ryan Feng",
            "Stella X. Yu",
            "Atul Prakash"
        ],
        "comments": "Published at ICML 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Real-world visual perception requires invariance to diverse transformations, yet current methods rely heavily on specialized architectures or training on predefined augmentations, limiting generalization. We propose FOCAL, a test-time, data-driven framework that achieves robust perception by leveraging internet-scale visual priors from foundation models. By generating and optimizing candidate transformations toward visually typical, \"canonical\" views, FOCAL enhances robustness without re-training or architectural changes. Our experiments demonstrate improved robustness of CLIP and SAM across challenging transformations, including 2D/3D rotations, illumination shifts (contrast and color), and day-night variations. We also highlight potential applications in active vision. Our approach challenges the assumption that transform-specific training is necessary, instead offering a scalable path to invariance. Our code is available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 143,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10381",
        "abs_url": "https://arxiv.org/abs/2507.10381",
        "pdf_url": "https://arxiv.org/pdf/2507.10381",
        "title": "Improving Remote Sensing Classification using Topological Data Analysis and Convolutional Neural Networks",
        "authors": [
            "Aaryam Sharma"
        ],
        "comments": "9 pages, 8 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Topological data analysis (TDA) is a relatively new field that is gaining rapid adoption due to its robustness and ability to effectively describe complex datasets by quantifying geometric information. In imaging contexts, TDA typically models data as filtered cubical complexes from which we can extract discriminative features using persistence homology. Meanwhile, convolutional neural networks (CNNs) have been shown to be biased towards texture based local features. To address this limitation, we propose a TDA feature engineering pipeline and a simple method to integrate topological features with deep learning models on remote sensing classification. Our method improves the performance of a ResNet18 model on the EuroSAT dataset by 1.44% achieving 99.33% accuracy, which surpasses all previously reported single-model accuracies, including those with larger architectures, such as ResNet50 (2x larger) and XL Vision Transformers (197x larger). We additionally show that our method's accuracy is 1.82% higher than our ResNet18 baseline on the RESISC45 dataset. To our knowledge, this is the first application of TDA features in satellite scene classification with deep learning. This demonstrates that TDA features can be integrated with deep learning models, even on datasets without explicit topological structures, thereby increasing the applicability of TDA. A clean implementation of our method will be made publicly available upon publication.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 144,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10398",
        "abs_url": "https://arxiv.org/abs/2507.10398",
        "pdf_url": "https://arxiv.org/pdf/2507.10398",
        "title": "Devanagari Handwritten Character Recognition using Convolutional Neural Network",
        "authors": [
            "Diksha Mehta",
            "Prateek Mehta"
        ],
        "comments": "9 pages, 6 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Handwritten character recognition is getting popular among researchers because of its possible applications in facilitating technological search engines, social media, recommender systems, etc. The Devanagari script is one of the oldest language scripts in India that does not have proper digitization tools. With the advancement of computing and technology, the task of this research is to extract handwritten Hindi characters from an image of Devanagari script with an automated approach to save time and obsolete data. In this paper, we present a technique to recognize handwritten Devanagari characters using two deep convolutional neural network layers. This work employs a methodology that is useful to enhance the recognition rate and configures a convolutional neural network for effective Devanagari handwritten text recognition (DHTR). This approach uses the Devanagari handwritten character dataset (DHCD), an open dataset with 36 classes of Devanagari characters. Each of these classes has 1700 images for training and testing purposes. This approach obtains promising results in terms of accuracy by achieving 96.36% accuracy in testing and 99.55% in training time.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 145,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10403",
        "abs_url": "https://arxiv.org/abs/2507.10403",
        "pdf_url": "https://arxiv.org/pdf/2507.10403",
        "title": "Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources",
        "authors": [
            "Daniele Rege Cambrin",
            "Lorenzo Vaiani",
            "Giuseppe Gallipoli",
            "Luca Cagliero",
            "Paolo Garza"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Computation and Language (cs.CL); Information Retrieval (cs.IR); Multimedia (cs.MM)",
        "abstract": "Retrieving relevant imagery from vast satellite archives is crucial for applications like disaster response and long-term climate monitoring. However, most text-to-image retrieval systems are limited to RGB data, failing to exploit the unique physical information captured by other sensors, such as the all-weather structural sensitivity of Synthetic Aperture Radar (SAR) or the spectral signatures in optical multispectral data. To bridge this gap, we introduce CrisisLandMark, a new large-scale corpus of over 647,000 Sentinel-1 SAR and Sentinel-2 multispectral images paired with structured textual annotations for land cover, land use, and crisis events harmonized from authoritative land cover systems (CORINE and Dynamic World) and crisis-specific sources. We then present CLOSP (Contrastive Language Optical SAR Pretraining), a novel framework that uses text as a bridge to align unpaired optical and SAR images into a unified embedding space. Our experiments show that CLOSP achieves a new state-of-the-art, improving retrieval nDGC by 54% over existing models. Additionally, we find that the unified training strategy overcomes the inherent difficulty of interpreting SAR imagery by transferring rich semantic knowledge from the optical domain with indirect interaction. Furthermore, GeoCLOSP, which integrates geographic coordinates into our framework, creates a powerful trade-off between generality and specificity: while the CLOSP excels at general semantic tasks, the GeoCLOSP becomes a specialized expert for retrieving location-dependent crisis events and rare geographic features. This work highlights that the integration of diverse sensor data and geographic context is essential for unlocking the full potential of remote sensing archives.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 146,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10407",
        "abs_url": "https://arxiv.org/abs/2507.10407",
        "pdf_url": "https://arxiv.org/pdf/2507.10407",
        "title": "Numerically Computing Galois Groups of Minimal Problems",
        "authors": [
            "Timothy Duff"
        ],
        "comments": "abstract accompanying invited tutorial at ISSAC 2025; 10 pages w/ references",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Symbolic Computation (cs.SC); Algebraic Geometry (math.AG)",
        "abstract": "I discuss a seemingly unlikely confluence of topics in algebra, numerical computation, and computer vision. The motivating problem is that of solving multiples instances of a parametric family of systems of algebraic (polynomial or rational function) equations. No doubt already of interest to ISSAC attendees, this problem arises in the context of robust model-fitting paradigms currently utilized by the computer vision community (namely \"Random Sampling and Consensus\", aka \"RanSaC\".) This talk will give an overview of work in the last 5+ years that aspires to measure the intrinsic difficulty of solving such parametric systems, and makes strides towards practical solutions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 147,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10432",
        "abs_url": "https://arxiv.org/abs/2507.10432",
        "pdf_url": "https://arxiv.org/pdf/2507.10432",
        "title": "Text-Visual Semantic Constrained AI-Generated Image Quality Assessment",
        "authors": [
            "Qiang Li",
            "Qingsen Yan",
            "Haojian Huang",
            "Peng Wu",
            "Haokui Zhang",
            "Yanning Zhang"
        ],
        "comments": "9 pages, 5 figures, Accepted at ACMMM 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "With the rapid advancements in Artificial Intelligence Generated Image (AGI) technology, the accurate assessment of their quality has become an increasingly vital requirement. Prevailing methods typically rely on cross-modal models like CLIP or BLIP to evaluate text-image alignment and visual quality. However, when applied to AGIs, these methods encounter two primary challenges: semantic misalignment and details perception missing. To address these limitations, we propose Text-Visual Semantic Constrained AI-Generated Image Quality Assessment (SC-AGIQA), a unified framework that leverages text-visual semantic constraints to significantly enhance the comprehensive evaluation of both text-image consistency and perceptual distortion in AI-generated images. Our approach integrates key capabilities from multiple models and tackles the aforementioned challenges by introducing two core modules: the Text-assisted Semantic Alignment Module (TSAM), which leverages Multimodal Large Language Models (MLLMs) to bridge the semantic gap by generating an image description and comparing it against the original prompt for a refined consistency check, and the Frequency-domain Fine-Grained Degradation Perception Module (FFDPM), which draws inspiration from Human Visual System (HVS) properties by employing frequency domain analysis combined with perceptual sensitivity weighting to better quantify subtle visual distortions and enhance the capture of fine-grained visual quality details in images. Extensive experiments conducted on multiple benchmark datasets demonstrate that SC-AGIQA outperforms existing state-of-the-art methods. The code is publicly available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 148,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10437",
        "abs_url": "https://arxiv.org/abs/2507.10437",
        "pdf_url": "https://arxiv.org/pdf/2507.10437",
        "title": "4D-Animal: Freely Reconstructing Animatable 3D Animals from Videos",
        "authors": [
            "Shanshan Zhong",
            "Jiawei Peng",
            "Zehan Zheng",
            "Zhongzhan Huang",
            "Wufei Ma",
            "Guofeng Zhang",
            "Qihao Liu",
            "Alan Yuille",
            "Jieneng Chen"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Existing methods for reconstructing animatable 3D animals from videos typically rely on sparse semantic keypoints to fit parametric models. However, obtaining such keypoints is labor-intensive, and keypoint detectors trained on limited animal data are often unreliable. To address this, we propose 4D-Animal, a novel framework that reconstructs animatable 3D animals from videos without requiring sparse keypoint annotations. Our approach introduces a dense feature network that maps 2D representations to SMAL parameters, enhancing both the efficiency and stability of the fitting process. Furthermore, we develop a hierarchical alignment strategy that integrates silhouette, part-level, pixel-level, and temporal cues from pre-trained 2D visual models to produce accurate and temporally coherent reconstructions across frames. Extensive experiments demonstrate that 4D-Animal outperforms both model-based and model-free baselines. Moreover, the high-quality 3D assets generated by our method can benefit other 3D tasks, underscoring its potential for large-scale applications. The code is released at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 149,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10449",
        "abs_url": "https://arxiv.org/abs/2507.10449",
        "pdf_url": "https://arxiv.org/pdf/2507.10449",
        "title": "CoralVQA: A Large-Scale Visual Question Answering Dataset for Coral Reef Image Understanding",
        "authors": [
            "Hongyong Han",
            "Wei Wang",
            "Gaowei Zhang",
            "Mingjie Li",
            "Yi Wang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Coral reefs are vital yet vulnerable ecosystems that require continuous monitoring to support conservation. While coral reef images provide essential information in coral monitoring, interpreting such images remains challenging due to the need for domain expertise. Visual Question Answering (VQA), powered by Large Vision-Language Models (LVLMs), has great potential in user-friendly interaction with coral reef images. However, applying VQA to coral imagery demands a dedicated dataset that addresses two key challenges: domain-specific annotations and multidimensional questions. In this work, we introduce CoralVQA, the first large-scale VQA dataset for coral reef analysis. It contains 12,805 real-world coral images from 67 coral genera collected from 3 oceans, along with 277,653 question-answer pairs that comprehensively assess ecological and health-related conditions. To construct this dataset, we develop a semi-automatic data construction pipeline in collaboration with marine biologists to ensure both scalability and professional-grade data quality. CoralVQA presents novel challenges and provides a comprehensive benchmark for studying vision-language reasoning in the context of coral reef images. By evaluating several state-of-the-art LVLMs, we reveal key limitations and opportunities. These insights form a foundation for future LVLM development, with a particular emphasis on supporting coral conservation efforts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 150,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10461",
        "abs_url": "https://arxiv.org/abs/2507.10461",
        "pdf_url": "https://arxiv.org/pdf/2507.10461",
        "title": "RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for Pansharpening",
        "authors": [
            "Tao Tang",
            "Chengxu Yang"
        ],
        "comments": "To appear in the proceedings of the 6th International Conference on Artificial Intelligence and Electromechanical Automation (AIEA 2025). 5 pages, 6 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multimedia (cs.MM); Image and Video Processing (eess.IV)",
        "abstract": "Pansharpening refers to the process of integrating a high resolution panchromatic (PAN) image with a lower resolution multispectral (MS) image to generate a fused product, which is pivotal in remote sensing. Despite the effectiveness of CNNs in addressing this challenge, they are inherently constrained by the uniform application of convolutional kernels across all spatial positions, overlooking local content variations. To overcome this issue, we introduce RAPNet, a new architecture that leverages content-adaptive convolution. At its core, RAPNet employs the Receptive-field Adaptive Pansharpening Convolution (RAPConv), designed to produce spatially adaptive kernels responsive to local feature context, thereby enhancing the precision of spatial detail extraction. Additionally, the network integrates the Pansharpening Dynamic Feature Fusion (PAN-DFF) module, which incorporates an attention mechanism to achieve an optimal balance between spatial detail enhancement and spectral fidelity. Comprehensive evaluations on publicly available datasets confirm that RAPNet delivers superior performance compared to existing approaches, as demonstrated by both quantitative metrics and qualitative assessments. Ablation analyses further substantiate the effectiveness of the proposed adaptive components.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 151,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10470",
        "abs_url": "https://arxiv.org/abs/2507.10470",
        "pdf_url": "https://arxiv.org/pdf/2507.10470",
        "title": "RefSTAR: Blind Facial Image Restoration with Reference Selection, Transfer, and Reconstruction",
        "authors": [
            "Zhicun Yin",
            "Junjie Chen",
            "Ming Liu",
            "Zhixin Wang",
            "Fan Li",
            "Renjing Pei",
            "Xiaoming Li",
            "Rynson W.H. Lau",
            "Wangmeng Zuo"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Blind facial image restoration is highly challenging due to unknown complex degradations and the sensitivity of humans to faces. Although existing methods introduce auxiliary information from generative priors or high-quality reference images, they still struggle with identity preservation problems, mainly due to improper feature introduction on detailed textures. In this paper, we focus on effectively incorporating appropriate features from high-quality reference images, presenting a novel blind facial image restoration method that considers reference selection, transfer, and reconstruction (RefSTAR). In terms of selection, we construct a reference selection (RefSel) module. For training the RefSel module, we construct a RefSel-HQ dataset through a mask generation pipeline, which contains annotating masks for 10,000 ground truth-reference pairs. As for the transfer, due to the trivial solution in vanilla cross-attention operations, a feature fusion paradigm is designed to force the features from the reference to be integrated. Finally, we propose a reference image reconstruction mechanism that further ensures the presence of reference image features in the output image. The cycle consistency loss is also redesigned in conjunction with the mask. Extensive experiments on various backbone models demonstrate superior performance, showing better identity preservation ability and reference feature transfer quality. Source code, dataset, and pre-trained models are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 152,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10473",
        "abs_url": "https://arxiv.org/abs/2507.10473",
        "pdf_url": "https://arxiv.org/pdf/2507.10473",
        "title": "GT-Loc: Unifying When and Where in Images Through a Joint Embedding Space",
        "authors": [
            "David G. Shatwell",
            "Ishan Rajendrakumar Dave",
            "Sirnam Swetha",
            "Mubarak Shah"
        ],
        "comments": "Accepted in ICCV2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Timestamp prediction aims to determine when an image was captured using only visual information, supporting applications such as metadata correction, retrieval, and digital forensics. In outdoor scenarios, hourly estimates rely on cues like brightness, hue, and shadow positioning, while seasonal changes and weather inform date estimation. However, these visual cues significantly depend on geographic context, closely linking timestamp prediction to geo-localization. To address this interdependence, we introduce GT-Loc, a novel retrieval-based method that jointly predicts the capture time (hour and month) and geo-location (GPS coordinates) of an image. Our approach employs separate encoders for images, time, and location, aligning their embeddings within a shared high-dimensional feature space. Recognizing the cyclical nature of time, instead of conventional contrastive learning with hard positives and negatives, we propose a temporal metric-learning objective providing soft targets by modeling pairwise time differences over a cyclical toroidal surface. We present new benchmarks demonstrating that our joint optimization surpasses previous time prediction methods, even those using the ground-truth geo-location as an input during inference. Additionally, our approach achieves competitive results on standard geo-localization tasks, and the unified embedding space facilitates compositional and text-based image retrieval.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 153,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10474",
        "abs_url": "https://arxiv.org/abs/2507.10474",
        "pdf_url": "https://arxiv.org/pdf/2507.10474",
        "title": "Privacy-Preserving Multi-Stage Fall Detection Framework with Semi-supervised Federated Learning and Robotic Vision Confirmation",
        "authors": [
            "Seyed Alireza Rahimi Azghadi",
            "Truong-Thanh-Hung Nguyen",
            "Helene Fournier",
            "Monica Wachowicz",
            "Rene Richard",
            "Francis Palma",
            "Hung Cao"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Robotics (cs.RO)",
        "abstract": "The aging population is growing rapidly, and so is the danger of falls in older adults. A major cause of injury is falling, and detection in time can greatly save medical expenses and recovery time. However, to provide timely intervention and avoid unnecessary alarms, detection systems must be effective and reliable while addressing privacy concerns regarding the user. In this work, we propose a framework for detecting falls using several complementary systems: a semi-supervised federated learning-based fall detection system (SF2D), an indoor localization and navigation system, and a vision-based human fall recognition system. A wearable device and an edge device identify a fall scenario in the first system. On top of that, the second system uses an indoor localization technique first to localize the fall location and then navigate a robot to inspect the scenario. A vision-based detection system running on an edge device with a mounted camera on a robot is used to recognize fallen people. Each of the systems of this proposed framework achieves different accuracy rates. Specifically, the SF2D has a 0.81% failure rate equivalent to 99.19% accuracy, while the vision-based fallen people detection achieves 96.3% accuracy. However, when we combine the accuracy of these two systems with the accuracy of the navigation system (95% success rate), our proposed framework creates a highly reliable performance for fall detection, with an overall accuracy of 99.99%. Not only is the proposed framework safe for older adults, but it is also a privacy-preserving solution for detecting falls.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 154,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10490",
        "abs_url": "https://arxiv.org/abs/2507.10490",
        "pdf_url": "https://arxiv.org/pdf/2507.10490",
        "title": "The Power of Certainty: How Confident Models Lead to Better Segmentation",
        "authors": [
            "Tugberk Erol",
            "Tuba Caglikantar",
            "Duygu Sarikaya"
        ],
        "comments": "9 pages, 3 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Deep learning models have been proposed for automatic polyp detection and precise segmentation of polyps during colonoscopy procedures. Although these state-of-the-art models achieve high performance, they often require a large number of parameters. Their complexity can make them prone to overfitting, particularly when trained on biased datasets, and can result in poor generalization across diverse datasets. Knowledge distillation and self-distillation are proposed as promising strategies to mitigate the limitations of large, over-parameterized models. These approaches, however, are resource-intensive, often requiring multiple models and significant memory during training. We propose a confidence-based self-distillation approach that outperforms state-of-the-art models by utilizing only previous iteration data storage during training, without requiring extra computation or memory usage during testing. Our approach calculates the loss between the previous and current iterations within a batch using a dynamic confidence coefficient. To evaluate the effectiveness of our approach, we conduct comprehensive experiments on the task of polyp segmentation. Our approach outperforms state-of-the-art models and generalizes well across datasets collected from multiple clinical centers. The code will be released to the public once the paper is accepted.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 155,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10492",
        "abs_url": "https://arxiv.org/abs/2507.10492",
        "pdf_url": "https://arxiv.org/pdf/2507.10492",
        "title": "BenchReAD: A systematic benchmark for retinal anomaly detection",
        "authors": [
            "Chenyu Lian",
            "Hong-Yu Zhou",
            "Zhanli Hu",
            "Jing Qin"
        ],
        "comments": "MICCAI 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Retinal anomaly detection plays a pivotal role in screening ocular and systemic diseases. Despite its significance, progress in the field has been hindered by the absence of a comprehensive and publicly available benchmark, which is essential for the fair evaluation and advancement of methodologies. Due to this limitation, previous anomaly detection work related to retinal images has been constrained by (1) a limited and overly simplistic set of anomaly types, (2) test sets that are nearly saturated, and (3) a lack of generalization evaluation, resulting in less convincing experimental setups. Furthermore, existing benchmarks in medical anomaly detection predominantly focus on one-class supervised approaches (training only with negative samples), overlooking the vast amounts of labeled abnormal data and unlabeled data that are commonly available in clinical practice. To bridge these gaps, we introduce a benchmark for retinal anomaly detection, which is comprehensive and systematic in terms of data and algorithm. Through categorizing and benchmarking previous methods, we find that a fully supervised approach leveraging disentangled representations of abnormalities (DRA) achieves the best performance but suffers from significant drops in performance when encountering certain unseen anomalies. Inspired by the memory bank mechanisms in one-class supervised learning, we propose NFM-DRA, which integrates DRA with a Normal Feature Memory to mitigate the performance degradation, establishing a new SOTA. The benchmark is publicly available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 156,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10496",
        "abs_url": "https://arxiv.org/abs/2507.10496",
        "pdf_url": "https://arxiv.org/pdf/2507.10496",
        "title": "Cameras as Relative Positional Encoding",
        "authors": [
            "Ruilong Li",
            "Brent Yi",
            "Junchen Liu",
            "Hang Gao",
            "Yi Ma",
            "Angjoo Kanazawa"
        ],
        "comments": "Project Page: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Transformers are increasingly prevalent for multi-view computer vision tasks, where geometric relationships between viewpoints are critical for 3D perception. To leverage these relationships, multi-view transformers must use camera geometry to ground visual tokens in 3D space. In this work, we compare techniques for conditioning transformers on cameras: token-level raymap encodings, attention-level relative pose encodings, and a new relative encoding we propose -- Projective Positional Encoding (PRoPE) -- that captures complete camera frustums, both intrinsics and extrinsics, as a relative positional encoding. Our experiments begin by showing how relative camera conditioning improves performance in feedforward novel view synthesis, with further gains from PRoPE. This holds across settings: scenes with both shared and varying intrinsics, when combining token- and attention-level conditioning, and for generalization to inputs with out-of-distribution sequence lengths and camera intrinsics. We then verify that these benefits persist for different tasks, stereo depth estimation and discriminative spatial cognition, as well as larger model sizes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 157,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10499",
        "abs_url": "https://arxiv.org/abs/2507.10499",
        "pdf_url": "https://arxiv.org/pdf/2507.10499",
        "title": "National level satellite-based crop field inventories in smallholder landscapes",
        "authors": [
            "Philippe Rufin",
            "Pauline Lucie Hammer",
            "Leon-Friedrich Thomas",
            "Sá Nogueira Lisboa",
            "Natasha Ribeiro",
            "Almeida Sitoe",
            "Patrick Hostert",
            "Patrick Meyfroidt"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "The design of science-based policies to improve the sustainability of smallholder agriculture is challenged by a limited understanding of fundamental system properties, such as the spatial distribution of active cropland and field size. We integrate very high spatial resolution (1.5 m) Earth observation data and deep transfer learning to derive crop field delineations in complex agricultural systems at the national scale, while maintaining minimum reference data requirements and enhancing transferability. We provide the first national-level dataset of 21 million individual fields for Mozambique (covering ~800,000 km2) for 2023. Our maps separate active cropland from non-agricultural land use with an overall accuracy of 93% and balanced omission and commission errors. Field-level spatial agreement reached median intersection over union (IoU) scores of 0.81, advancing the state-of-the-art in large-area field delineation in complex smallholder systems. The active cropland maps capture fragmented rural regions with low cropland shares not yet identified in global land cover or cropland maps. These regions are mostly located in agricultural frontier regions which host 7-9% of the Mozambican population. Field size in Mozambique is very low overall, with half of the fields being smaller than 0.16 ha, and 83% smaller than 0.5 ha. Mean field size at aggregate spatial resolution (0.05°) is 0.32 ha, but it varies strongly across gradients of accessibility, population density, and net forest cover change. This variation reflects a diverse set of actors, ranging from semi-subsistence smallholder farms to medium-scale commercial farming, and large-scale farming operations. Our results highlight that field size is a key indicator relating to socio-economic and environmental outcomes of agriculture (e.g., food production, livelihoods, deforestation, biodiversity), as well as their trade-offs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 158,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10547",
        "abs_url": "https://arxiv.org/abs/2507.10547",
        "pdf_url": "https://arxiv.org/pdf/2507.10547",
        "title": "Quantize-then-Rectify: Efficient VQ-VAE Training",
        "authors": [
            "Borui Zhang",
            "Qihang Rao",
            "Wenzhao Zheng",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Visual tokenizers are pivotal in multimodal large models, acting as bridges between continuous inputs and discrete tokens. Nevertheless, training high-compression-rate VQ-VAEs remains computationally demanding, often necessitating thousands of GPU hours. This work demonstrates that a pre-trained VAE can be efficiently transformed into a VQ-VAE by controlling quantization noise within the VAE's tolerance threshold. We present \\textbf{Quantize-then-Rectify (ReVQ)}, a framework leveraging pre-trained VAEs to enable rapid VQ-VAE training with minimal computational overhead. By integrating \\textbf{channel multi-group quantization} to enlarge codebook capacity and a \\textbf{post rectifier} to mitigate quantization errors, ReVQ compresses ImageNet images into at most 512 tokens while sustaining competitive reconstruction quality (rFID = 1.06). Significantly, ReVQ reduces training costs by over two orders of magnitude relative to state-of-the-art approaches: ReVQ finishes full training on a single NVIDIA 4090 in approximately 22 hours, whereas comparable methods require 4.5 days on 32 A100 GPUs. Experimental results show that ReVQ achieves superior efficiency-reconstruction trade-offs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 159,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10548",
        "abs_url": "https://arxiv.org/abs/2507.10548",
        "pdf_url": "https://arxiv.org/pdf/2507.10548",
        "title": "EmbRACE-3K: Embodied Reasoning and Action in Complex Environments",
        "authors": [
            "Mingxian Lin",
            "Wei Huang",
            "Yitang Li",
            "Chengjie Jiang",
            "Kui Wu",
            "Fangwei Zhong",
            "Shengju Qian",
            "Xin Wang",
            "Xiaojuan Qi"
        ],
        "comments": "Project page: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Recent advanced vision-language models(VLMs) have demonstrated strong performance on passive, offline image and video understanding tasks. However, their effectiveness in embodied settings, which require online interaction and active scene understanding remains limited. In such scenarios, an agent perceives the environment from a first-person perspective, with each action dynamically shaping subsequent observations. Even state-of-the-art models such as GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment interactions, exhibiting clear limitations in spatial reasoning and long-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset of over 3,000 language-guided tasks situated in diverse, photorealistic environments constructed using Unreal Engine and the UnrealCV-Zoo framework. The tasks encompass a wide range of embodied challenges, including navigation, object manipulation, and multi-stage goal execution. Each task unfolds as a multi-step trajectory, pairing first-person visual observations with high-level instructions, grounded actions, and natural language rationales that express the agent's intent at every step. Using EmRACE-3K, we establish a benchmark to evaluate the embodied reasoning capabilities of VLMs across three key dimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage Goal Execution. In zero-shot settings, all models achieve success rates below 20%, underscoring the challenge posed by our benchmark and the current limitations of VLMs in interactive environments. To demonstrate the utility of EmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning followed by reinforcement learning. This approach yields substantial improvements across all three challenge categories, highlighting the dataset's effectiveness in enabling the development of embodied reasoning capabilities.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 160,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10552",
        "abs_url": "https://arxiv.org/abs/2507.10552",
        "pdf_url": "https://arxiv.org/pdf/2507.10552",
        "title": "Self-supervised Learning on Camera Trap Footage Yields a Strong Universal Face Embedder",
        "authors": [
            "Vladimir Iashin",
            "Horace Lee",
            "Dan Schofield",
            "Andrew Zisserman"
        ],
        "comments": "Accepted for publication. Project page, code and weights: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Camera traps are revolutionising wildlife monitoring by capturing vast amounts of visual data; however, the manual identification of individual animals remains a significant bottleneck. This study introduces a fully self-supervised approach to learning robust chimpanzee face embeddings from unlabeled camera-trap footage. Leveraging the DINOv2 framework, we train Vision Transformers on automatically mined face crops, eliminating the need for identity labels. Our method demonstrates strong open-set re-identification performance, surpassing supervised baselines on challenging benchmarks such as Bossou, despite utilising no labelled data during training. This work underscores the potential of self-supervised learning in biodiversity monitoring and paves the way for scalable, non-invasive population studies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 161,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.08841",
        "abs_url": "https://arxiv.org/abs/2507.08841",
        "pdf_url": "https://arxiv.org/pdf/2507.08841",
        "title": "Zero-Shot Neural Architecture Search with Weighted Response Correlation",
        "authors": [
            "Kun Jing",
            "Luoyu Chen",
            "Jungang Xu",
            "Jianwei Tai",
            "Yiyu Wang",
            "Shuaimin Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Neural architecture search (NAS) is a promising approach for automatically designing neural network architectures. However, the architecture estimation of NAS is computationally expensive and time-consuming because of training multiple architectures from scratch. Although existing zero-shot NAS methods use training-free proxies to accelerate the architecture estimation, their effectiveness, stability, and generality are still lacking. We present a novel training-free estimation proxy called weighted response correlation (WRCor). WRCor utilizes correlation coefficient matrices of responses across different input samples to calculate the proxy scores of estimated architectures, which can measure their expressivity and generalizability. Experimental results on proxy evaluation demonstrate that WRCor and its voting proxies are more efficient estimation strategies than existing proxies. We also apply them with different search strategies in architecture search. Experimental results on architecture search show that our zero-shot NAS algorithm outperforms most existing NAS algorithms in different search spaces. Our NAS algorithm can discover an architecture with a 22.1% test error on the ImageNet-1k dataset within 4 GPU hours. All codes are publicly available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 162,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.08855",
        "abs_url": "https://arxiv.org/abs/2507.08855",
        "pdf_url": "https://arxiv.org/pdf/2507.08855",
        "title": "Multi-omic Prognosis of Alzheimer's Disease with Asymmetric Cross-Modal Cross-Attention Network",
        "authors": [
            "Yang Ming",
            "Jiang Shi Zhong",
            "Zhou Su Juan"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Alzheimer's Disease (AD) is an irreversible neurodegenerative disease characterized by progressive cognitive decline as its main symptom. In the research field of deep learning-assisted diagnosis of AD, traditional convolutional neural networks and simple feature concatenation methods fail to effectively utilize the complementary information between multimodal data, and the simple feature concatenation approach is prone to cause the loss of key information during the process of modal fusion. In recent years, the development of deep learning technology has brought new possibilities for solving the problem of how to effectively fuse multimodal features. This paper proposes a novel deep learning algorithm framework to assist medical professionals in AD diagnosis. By fusing medical multi-view information such as brain fluorodeoxyglucose positron emission tomography (PET), magnetic resonance imaging (MRI), genetic data, and clinical data, it can accurately detect the presence of AD, Mild Cognitive Impairment (MCI), and Cognitively Normal (CN). The innovation of the algorithm lies in the use of an asymmetric cross-modal cross-attention mechanism, which can effectively capture the key information features of the interactions between different data modal features. This paper compares the asymmetric cross-modal cross-attention mechanism with the traditional algorithm frameworks of unimodal and multimodal deep learning models for AD diagnosis, and evaluates the importance of the asymmetric cross-modal cross-attention mechanism. The algorithm model achieves an accuracy of 94.88% on the test set.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 163,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.08903",
        "abs_url": "https://arxiv.org/abs/2507.08903",
        "pdf_url": "https://arxiv.org/pdf/2507.08903",
        "title": "Multimodal HD Mapping for Intersections by Intelligent Roadside Units",
        "authors": [
            "Zhongzhang Chen",
            "Miao Fan",
            "Shengtong Xu",
            "Mengmeng Yang",
            "Kun Jiang",
            "Xiangzeng Liu",
            "Haoyi Xiong"
        ],
        "comments": "Accepted by ITSC'25",
        "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "High-definition (HD) semantic mapping of complex intersections poses significant challenges for traditional vehicle-based approaches due to occlusions and limited perspectives. This paper introduces a novel camera-LiDAR fusion framework that leverages elevated intelligent roadside units (IRUs). Additionally, we present RS-seq, a comprehensive dataset developed through the systematic enhancement and annotation of the V2X-Seq dataset. RS-seq includes precisely labelled camera imagery and LiDAR point clouds collected from roadside installations, along with vectorized maps for seven intersections annotated with detailed features such as lane dividers, pedestrian crossings, and stop lines. This dataset facilitates the systematic investigation of cross-modal complementarity for HD map generation using IRU data. The proposed fusion framework employs a two-stage process that integrates modality-specific feature extraction and cross-modal semantic integration, capitalizing on camera high-resolution texture and precise geometric data from LiDAR. Quantitative evaluations using the RS-seq dataset demonstrate that our multimodal approach consistently surpasses unimodal methods. Specifically, compared to unimodal baselines evaluated on the RS-seq dataset, the multimodal approach improves the mean Intersection-over-Union (mIoU) for semantic segmentation by 4\\% over the image-only results and 18\\% over the point cloud-only results. This study establishes a baseline methodology for IRU-based HD semantic mapping and provides a valuable dataset for future research in infrastructure-assisted autonomous driving systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 164,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.08952",
        "abs_url": "https://arxiv.org/abs/2507.08952",
        "pdf_url": "https://arxiv.org/pdf/2507.08952",
        "title": "Interpretable Artificial Intelligence for Detecting Acute Heart Failure on Acute Chest CT Scans",
        "authors": [
            "Silas Nyboe Ørting",
            "Kristina Miger",
            "Anne Sophie Overgaard Olesen",
            "Mikael Ploug Boesen",
            "Michael Brun Andersen",
            "Jens Petersen",
            "Olav W. Nielsen",
            "Marleen de Bruijne"
        ],
        "comments": "34 pages, 11 figures, Submitted to \"Radiology AI\"",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Introduction: Chest CT scans are increasingly used in dyspneic patients where acute heart failure (AHF) is a key differential diagnosis. Interpretation remains challenging and radiology reports are frequently delayed due to a radiologist shortage, although flagging such information for emergency physicians would have therapeutic implication. Artificial intelligence (AI) can be a complementary tool to enhance the diagnostic precision. We aim to develop an explainable AI model to detect radiological signs of AHF in chest CT with an accuracy comparable to thoracic radiologists. Methods: A single-center, retrospective study during 2016-2021 at Copenhagen University Hospital - Bispebjerg and Frederiksberg, Denmark. A Boosted Trees model was trained to predict AHF based on measurements of segmented cardiac and pulmonary structures from acute thoracic CT scans. Diagnostic labels for training and testing were extracted from radiology reports. Structures were segmented with TotalSegmentator. Shapley Additive explanations values were used to explain the impact of each measurement on the final prediction. Results: Of the 4,672 subjects, 49% were female. The final model incorporated twelve key features of AHF and achieved an area under the ROC of 0.87 on the independent test set. Expert radiologist review of model misclassifications found that 24 out of 64 (38%) false positives and 24 out of 61 (39%) false negatives were actually correct model predictions, with the errors originating from inaccuracies in the initial radiology reports. Conclusion: We developed an explainable AI model with strong discriminatory performance, comparable to thoracic radiologists. The AI model's stepwise, transparent predictions may support decision-making.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 165,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.08980",
        "abs_url": "https://arxiv.org/abs/2507.08980",
        "pdf_url": "https://arxiv.org/pdf/2507.08980",
        "title": "Learning Diffusion Models with Flexible Representation Guidance",
        "authors": [
            "Chenyu Wang",
            "Cai Zhou",
            "Sharut Gupta",
            "Zongyu Lin",
            "Stefanie Jegelka",
            "Stephen Bates",
            "Tommi Jaakkola"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Diffusion models can be improved with additional guidance towards more effective representations of input. Indeed, prior empirical work has already shown that aligning internal representations of the diffusion model with those of pre-trained models improves generation quality. In this paper, we present a systematic framework for incorporating representation guidance into diffusion models. We provide alternative decompositions of denoising models along with their associated training criteria, where the decompositions determine when and how the auxiliary representations are incorporated. Guided by our theoretical insights, we introduce two new strategies for enhancing representation alignment in diffusion models. First, we pair examples with target representations either derived from themselves or arisen from different synthetic modalities, and subsequently learn a joint model over the multimodal pairs. Second, we design an optimal training curriculum that balances representation learning and data generation. Our experiments across image, protein sequence, and molecule generation tasks demonstrate superior performance as well as accelerated training. In particular, on the class-conditional ImageNet $256\\times 256$ benchmark, our guidance results in $23.3$ times faster training than the original SiT-XL as well as four times speedup over the state-of-the-art method REPA. The code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 166,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.08982",
        "abs_url": "https://arxiv.org/abs/2507.08982",
        "pdf_url": "https://arxiv.org/pdf/2507.08982",
        "title": "VIP: Visual Information Protection through Adversarial Attacks on Vision-Language Models",
        "authors": [
            "Hanene F. Z. Brachemi Meftah",
            "Wassim Hamidouche",
            "Sid Ahmed Fezza",
            "Olivier Déforges"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Recent years have witnessed remarkable progress in developing Vision-Language Models (VLMs) capable of processing both textual and visual inputs. These models have demonstrated impressive performance, leading to their widespread adoption in various applications. However, this widespread raises serious concerns regarding user privacy, particularly when models inadvertently process or expose private visual information. In this work, we frame the preservation of privacy in VLMs as an adversarial attack problem. We propose a novel attack strategy that selectively conceals information within designated Region Of Interests (ROIs) in an image, effectively preventing VLMs from accessing sensitive content while preserving the semantic integrity of the remaining image. Unlike conventional adversarial attacks that often disrupt the entire image, our method maintains high coherence in unmasked areas. Experimental results across three state-of-the-art VLMs namely LLaVA, Instruct-BLIP, and BLIP2-T5 demonstrate up to 98% reduction in detecting targeted ROIs, while maintaining global image semantics intact, as confirmed by high similarity scores between clean and adversarial outputs. We believe that this work contributes to a more privacy conscious use of multimodal models and offers a practical tool for further research, with the source code publicly available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 167,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09024",
        "abs_url": "https://arxiv.org/abs/2507.09024",
        "pdf_url": "https://arxiv.org/pdf/2507.09024",
        "title": "CNeuroMod-THINGS, a densely-sampled fMRI dataset for visual neuroscience",
        "authors": [
            "Marie St-Laurent",
            "Basile Pinsard",
            "Oliver Contier",
            "Elizabeth DuPre",
            "Katja Seeliger",
            "Valentina Borghesani",
            "Julie A. Boyle",
            "Lune Bellec",
            "Martin N. Hebart"
        ],
        "comments": "29 pages manuscript, 5 figures, 12 pages supplementary material",
        "subjects": "Neurons and Cognition (q-bio.NC); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Data-hungry neuro-AI modelling requires ever larger neuroimaging datasets. CNeuroMod-THINGS meets this need by capturing neural representations for a wide set of semantic concepts using well-characterized stimuli in a new densely-sampled, large-scale fMRI dataset. Importantly, CNeuroMod-THINGS exploits synergies between two existing projects: the THINGS initiative (THINGS) and the Courtois Project on Neural Modelling (CNeuroMod). THINGS has developed a common set of thoroughly annotated images broadly sampling natural and man-made objects which is used to acquire a growing collection of large-scale multimodal neural responses. Meanwhile, CNeuroMod is acquiring hundreds of hours of fMRI data from a core set of participants during controlled and naturalistic tasks, including visual tasks like movie watching and videogame playing. For CNeuroMod-THINGS, four CNeuroMod participants each completed 33-36 sessions of a continuous recognition paradigm using approximately 4000 images from the THINGS stimulus set spanning 720 categories. We report behavioural and neuroimaging metrics that showcase the quality of the data. By bridging together large existing resources, CNeuroMod-THINGS expands our capacity to model broad slices of the human visual experience.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 168,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09031",
        "abs_url": "https://arxiv.org/abs/2507.09031",
        "pdf_url": "https://arxiv.org/pdf/2507.09031",
        "title": "Confounder-Free Continual Learning via Recursive Feature Normalization",
        "authors": [
            "Yash Shah",
            "Camila Gonzalez",
            "Mohammad H. Abbasi",
            "Qingyu Zhao",
            "Kilian M. Pohl",
            "Ehsan Adeli"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Confounders are extraneous variables that affect both the input and the target, resulting in spurious correlations and biased predictions. There are recent advances in dealing with or removing confounders in traditional models, such as metadata normalization (MDN), where the distribution of the learned features is adjusted based on the study confounders. However, in the context of continual learning, where a model learns continuously from new data over time without forgetting, learning feature representations that are invariant to confounders remains a significant challenge. To remove their influence from intermediate feature representations, we introduce the Recursive MDN (R-MDN) layer, which can be integrated into any deep learning architecture, including vision transformers, and at any model stage. R-MDN performs statistical regression via the recursive least squares algorithm to maintain and continually update an internal model state with respect to changing distributions of data and confounding variables. Our experiments demonstrate that R-MDN promotes equitable predictions across population groups, both within static learning and across different stages of continual learning, by reducing catastrophic forgetting caused by confounder effects changing over time.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 169,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09158",
        "abs_url": "https://arxiv.org/abs/2507.09158",
        "pdf_url": "https://arxiv.org/pdf/2507.09158",
        "title": "Automatic Contouring of Spinal Vertebrae on X-Ray using a Novel Sandwich U-Net Architecture",
        "authors": [
            "Sunil Munthumoduku Krishna Murthy",
            "Kumar Rajamani",
            "Srividya Tirunellai Rajamani",
            "Yupei Li",
            "Qiyang Sun",
            "Bjoern W. Schuller"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In spinal vertebral mobility disease, accurately extracting and contouring vertebrae is essential for assessing mobility impairments and monitoring variations during flexion-extension movements. Precise vertebral contouring plays a crucial role in surgical planning; however, this process is traditionally performed manually by radiologists or surgeons, making it labour-intensive, time-consuming, and prone to human error. In particular, mobility disease analysis requires the individual contouring of each vertebra, which is both tedious and susceptible to inconsistencies. Automated methods provide a more efficient alternative, enabling vertebra identification, segmentation, and contouring with greater accuracy and reduced time consumption. In this study, we propose a novel U-Net variation designed to accurately segment thoracic vertebrae from anteroposterior view on X-Ray images. Our proposed approach, incorporating a ``sandwich\" U-Net structure with dual activation functions, achieves a 4.1\\% improvement in Dice score compared to the baseline U-Net model, enhancing segmentation accuracy while ensuring reliable vertebral contour extraction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 170,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09212",
        "abs_url": "https://arxiv.org/abs/2507.09212",
        "pdf_url": "https://arxiv.org/pdf/2507.09212",
        "title": "Warm Starts Accelerate Generative Modelling",
        "authors": [
            "Jonas Scholz",
            "Richard E. Turner"
        ],
        "comments": "10 pages, 6 figures",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)",
        "abstract": "Iterative generative models, like diffusion and flow-matching, create high-fidelity samples by progressively refining a noise vector into data. However, this process is notoriously slow, often requiring hundreds of function evaluations. We introduce the warm-start model, a simple, deterministic model that dramatically accelerates conditional generation by providing a better starting point. Instead of starting generation from an uninformed N(0, I) prior, our warm-start model predicts an informed prior N(mu, sigma), whose moments are conditioned on the input context. This \"warm start\" substantially reduces the distance the generative process must traverse, particularly when the conditioning information is strongly informative. On tasks like image inpainting, our method achieves results competitive with a 1000-step DDPM baseline using only 11 total function evaluations (1 for the warm start, 10 for generation). A simple conditional normalization trick makes our method compatible with any standard generative model and sampler without modification, allowing it to be combined with other efficient sampling techniques for further acceleration. Our implementation is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 171,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09227",
        "abs_url": "https://arxiv.org/abs/2507.09227",
        "pdf_url": "https://arxiv.org/pdf/2507.09227",
        "title": "PanoDiff-SR: Synthesizing Dental Panoramic Radiographs using Diffusion and Super-resolution",
        "authors": [
            "Sanyam Jain",
            "Bruna Neves de Freitas",
            "Andreas Basse-OConnor",
            "Alexandros Iosifidis",
            "Ruben Pauwels"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "There has been increasing interest in the generation of high-quality, realistic synthetic medical images in recent years. Such synthetic datasets can mitigate the scarcity of public datasets for artificial intelligence research, and can also be used for educational purposes. In this paper, we propose a combination of diffusion-based generation (PanoDiff) and Super-Resolution (SR) for generating synthetic dental panoramic radiographs (PRs). The former generates a low-resolution (LR) seed of a PR (256 X 128) which is then processed by the SR model to yield a high-resolution (HR) PR of size 1024 X 512. For SR, we propose a state-of-the-art transformer that learns local-global relationships, resulting in sharper edges and textures. Experimental results demonstrate a Frechet inception distance score of 40.69 between 7243 real and synthetic images (in HR). Inception scores were 2.55, 2.30, 2.90 and 2.98 for real HR, synthetic HR, real LR and synthetic LR images, respectively. Among a diverse group of six clinical experts, all evaluating a mixture of 100 synthetic and 100 real PRs in a time-limited observation, the average accuracy in distinguishing real from synthetic images was 68.5% (with 50% corresponding to random guessing).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 172,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09441",
        "abs_url": "https://arxiv.org/abs/2507.09441",
        "pdf_url": "https://arxiv.org/pdf/2507.09441",
        "title": "RectifiedHR: High-Resolution Diffusion via Energy Profiling and Adaptive Guidance Scheduling",
        "authors": [
            "Ankit Sanjyal"
        ],
        "comments": "8 Pages, 10 Figures, Pre-Print Version, Code Available at: this https URL",
        "subjects": "Graphics (cs.GR); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "High-resolution image synthesis with diffusion models often suffers from energy instabilities and guidance artifacts that degrade visual quality. We analyze the latent energy landscape during sampling and propose adaptive classifier-free guidance (CFG) schedules that maintain stable energy trajectories. Our approach introduces energy-aware scheduling strategies that modulate guidance strength over time, achieving superior stability scores (0.9998) and consistency metrics (0.9873) compared to fixed-guidance approaches. We demonstrate that DPM++ 2M with linear-decreasing CFG scheduling yields optimal performance, providing sharper, more faithful images while reducing artifacts. Our energy profiling framework serves as a powerful diagnostic tool for understanding and improving diffusion model behavior.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 173,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09448",
        "abs_url": "https://arxiv.org/abs/2507.09448",
        "pdf_url": "https://arxiv.org/pdf/2507.09448",
        "title": "TRACER: Efficient Object Re-Identification in Networked Cameras through Adaptive Query Processing",
        "authors": [
            "Pramod Chunduri",
            "Yao Lu",
            "Joy Arulraj"
        ],
        "comments": "",
        "subjects": "Databases (cs.DB); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Efficiently re-identifying and tracking objects across a network of cameras is crucial for applications like traffic surveillance. Spatula is the state-of-the-art video database management system (VDBMS) for processing Re-ID queries. However, it suffers from two limitations. Its spatio-temporal filtering scheme has limited accuracy on large camera networks due to localized camera history. It is not suitable for critical video analytics applications that require high recall due to a lack of support for adaptive query processing. In this paper, we present Tracer, a novel VDBMS for efficiently processing Re-ID queries using an adaptive query processing framework. Tracer selects the optimal camera to process at each time step by training a recurrent network to model long-term historical correlations. To accelerate queries under a high recall constraint, Tracer incorporates a probabilistic adaptive search model that processes camera feeds in incremental search windows and dynamically updates the sampling probabilities using an exploration-exploitation strategy. To address the paucity of benchmarks for the Re-ID task due to privacy concerns, we present a novel synthetic benchmark for generating multi-camera Re-ID datasets based on real-world traffic distribution. Our evaluation shows that Tracer outperforms the state-of-the-art cross-camera analytics system by 3.9x on average across diverse datasets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 174,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09513",
        "abs_url": "https://arxiv.org/abs/2507.09513",
        "pdf_url": "https://arxiv.org/pdf/2507.09513",
        "title": "Self-supervised pretraining of vision transformers for animal behavioral analysis and neural encoding",
        "authors": [
            "Yanchen Wang",
            "Han Yu",
            "Ari Blau",
            "Yizi Zhang",
            "International Brain Laboratory",
            "Liam Paninski",
            "Cole Hurwitz",
            "Matt Whiteway"
        ],
        "comments": "",
        "subjects": "Neurons and Cognition (q-bio.NC); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The brain can only be fully understood through the lens of the behavior it generates -- a guiding principle in modern neuroscience research that nevertheless presents significant technical challenges. Many studies capture behavior with cameras, but video analysis approaches typically rely on specialized models requiring extensive labeled data. We address this limitation with BEAST (BEhavioral Analysis via Self-supervised pretraining of Transformers), a novel and scalable framework that pretrains experiment-specific vision transformers for diverse neuro-behavior analyses. BEAST combines masked autoencoding with temporal contrastive learning to effectively leverage unlabeled video data. Through comprehensive evaluation across multiple species, we demonstrate improved performance in three critical neuro-behavioral tasks: extracting behavioral features that correlate with neural activity, and pose estimation and action segmentation in both the single- and multi-animal settings. Our method establishes a powerful and versatile backbone model that accelerates behavioral analysis in scenarios where labeled data remains scarce.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 175,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09608",
        "abs_url": "https://arxiv.org/abs/2507.09608",
        "pdf_url": "https://arxiv.org/pdf/2507.09608",
        "title": "prNet: Data-Driven Phase Retrieval via Stochastic Refinement",
        "authors": [
            "Mehmet Onurcan Kaya",
            "Figen S. Oktem"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We propose a novel framework for phase retrieval that leverages Langevin dynamics to enable efficient posterior sampling, yielding reconstructions that explicitly balance distortion and perceptual quality. Unlike conventional approaches that prioritize pixel-wise accuracy, our method navigates the perception-distortion tradeoff through a principled combination of stochastic sampling, learned denoising, and model-based updates. The framework comprises three variants of increasing complexity, integrating theoretically grounded Langevin inference, adaptive noise schedule learning, parallel reconstruction sampling, and warm-start initialization from classical solvers. Extensive experiments demonstrate that our method achieves state-of-the-art performance across multiple benchmarks, both in terms of fidelity and perceptual quality.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 176,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09609",
        "abs_url": "https://arxiv.org/abs/2507.09609",
        "pdf_url": "https://arxiv.org/pdf/2507.09609",
        "title": "I2I-PR: Deep Iterative Refinement for Phase Retrieval using Image-to-Image Diffusion Models",
        "authors": [
            "Mehmet Onurcan Kaya",
            "Figen S. Oktem"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Phase retrieval involves recovering a signal from intensity-only measurements, crucial in many fields such as imaging, holography, optical computing, crystallography, and microscopy. Although there are several well-known phase retrieval algorithms, including classical iterative solvers, the reconstruction performance often remains sensitive to initialization and measurement noise. Recently, image-to-image diffusion models have gained traction in various image reconstruction tasks, yielding significant theoretical insights and practical breakthroughs. In this work, we introduce a novel phase retrieval approach based on an image-to-image diffusion framework called Inversion by Direct Iteration. Our method begins with an enhanced initialization stage that leverages a hybrid iterative technique, combining the Hybrid Input-Output and Error Reduction methods and incorporating a novel acceleration mechanism to obtain a robust crude estimate. Then, it iteratively refines this initial crude estimate using the learned image-to-image pipeline. Our method achieves substantial improvements in both training efficiency and reconstruction quality. Furthermore, our approach utilizes aggregation techniques to refine quality metrics and demonstrates superior results compared to both classical and contemporary techniques. This highlights its potential for effective and efficient phase retrieval across various applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 177,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09616",
        "abs_url": "https://arxiv.org/abs/2507.09616",
        "pdf_url": "https://arxiv.org/pdf/2507.09616",
        "title": "MLoRQ: Bridging Low-Rank and Quantization for Transformer Compression",
        "authors": [
            "Ofir Gordon",
            "Ariel Lapid",
            "Elad Cohen",
            "Yarden Yagil",
            "Arnon Netzer",
            "Hai Victor Habi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Deploying transformer-based neural networks on resource-constrained edge devices presents a significant challenge. This challenge is often addressed through various techniques, such as low-rank approximation and mixed-precision quantization. In this work, we introduce Mixed Low-Rank and Quantization (MLoRQ), a novel method that integrates both techniques. MLoRQ employs a two-stage optimization process to determine optimal bit-width and rank assignments for each layer, adhering to predefined memory constraints. This process includes: (i) an intra-layer optimization that identifies potentially optimal compression solutions out of all low-rank and quantization combinations; (ii) an inter-layer optimization that assigns bit-width precision and rank to each layer while ensuring the memory constraint is met. An optional final step applies a sequential optimization process using a modified adaptive rounding technique to mitigate compression-induced errors in joint low-rank approximation and quantization. The method is compatible and can be seamlessly integrated with most existing quantization algorithms. MLoRQ shows state-of-the-art results with up to 15\\% performance improvement, evaluated on Vision Transformers for image classification, object detection, and instance segmentation tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 178,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09627",
        "abs_url": "https://arxiv.org/abs/2507.09627",
        "pdf_url": "https://arxiv.org/pdf/2507.09627",
        "title": "Lightweight Deep Learning-Based Channel Estimation for RIS-Aided Extremely Large-Scale MIMO Systems on Resource-Limited Edge Devices",
        "authors": [
            "Muhammad Kamran Saeed",
            "Ashfaq Khokhar",
            "Shakil Ahmed"
        ],
        "comments": "",
        "subjects": "Information Theory (cs.IT); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)",
        "abstract": "Next-generation wireless technologies such as 6G aim to meet demanding requirements such as ultra-high data rates, low latency, and enhanced connectivity. Extremely Large-Scale MIMO (XL-MIMO) and Reconfigurable Intelligent Surface (RIS) are key enablers, with XL-MIMO boosting spectral and energy efficiency through numerous antennas, and RIS offering dynamic control over the wireless environment via passive reflective elements. However, realizing their full potential depends on accurate Channel State Information (CSI). Recent advances in deep learning have facilitated efficient cascaded channel estimation. However, the scalability and practical deployment of existing estimation models in XL-MIMO systems remain limited. The growing number of antennas and RIS elements introduces a significant barrier to real-time and efficient channel estimation, drastically increasing data volume, escalating computational complexity, requiring advanced hardware, and resulting in substantial energy consumption. To address these challenges, we propose a lightweight deep learning framework for efficient cascaded channel estimation in XL-MIMO systems, designed to minimize computational complexity and make it suitable for deployment on resource-constrained edge devices. Using spatial correlations in the channel, we introduce a patch-based training mechanism that reduces the dimensionality of input to patch-level representations while preserving essential information, allowing scalable training for large-scale systems. Simulation results under diverse conditions demonstrate that our framework significantly improves estimation accuracy and reduces computational complexity, regardless of the increasing number of antennas and RIS elements in XL-MIMO systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 179,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09725",
        "abs_url": "https://arxiv.org/abs/2507.09725",
        "pdf_url": "https://arxiv.org/pdf/2507.09725",
        "title": "Visual Homing in Outdoor Robots Using Mushroom Body Circuits and Learning Walks",
        "authors": [
            "Gabriel G. Gattaux",
            "Julien R. Serres",
            "Franck Ruffier",
            "Antoine Wystrach"
        ],
        "comments": "Published by Springer Nature with the 14th bioinspired and biohybrid systems conference in Sheffield, and presented at the conference in July 2025",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Ants achieve robust visual homing with minimal sensory input and only a few learning walks, inspiring biomimetic solutions for autonomous navigation. While Mushroom Body (MB) models have been used in robotic route following, they have not yet been applied to visual homing. We present the first real-world implementation of a lateralized MB architecture for visual homing onboard a compact autonomous car-like robot. We test whether the sign of the angular path integration (PI) signal can categorize panoramic views, acquired during learning walks and encoded in the MB, into \"goal on the left\" and \"goal on the right\" memory banks, enabling robust homing in natural outdoor settings. We validate this approach through four incremental experiments: (1) simulation showing attractor-like nest dynamics; (2) real-world homing after decoupled learning walks, producing nest search behavior; (3) homing after random walks using noisy PI emulated with GPS-RTK; and (4) precise stopping-at-the-goal behavior enabled by a fifth MB Output Neuron (MBON) encoding goal-views to control velocity. This mimics the accurate homing behavior of ants and functionally resembles waypoint-based position control in robotics, despite relying solely on visual input. Operating at 8 Hz on a Raspberry Pi 4 with 32x32 pixel views and a memory footprint under 9 kB, our system offers a biologically grounded, resource-efficient solution for autonomous visual homing.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 180,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09731",
        "abs_url": "https://arxiv.org/abs/2507.09731",
        "pdf_url": "https://arxiv.org/pdf/2507.09731",
        "title": "Pre-trained Under Noise: A Framework for Robust Bone Fracture Detection in Medical Imaging",
        "authors": [
            "Robby Hoover",
            "Nelly Elsayed",
            "Zag ElSayed",
            "Chengcheng Li"
        ],
        "comments": "7 pages, under review",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Medical Imagings are considered one of the crucial diagnostic tools for different bones-related diseases, especially bones fractures. This paper investigates the robustness of pre-trained deep learning models for classifying bone fractures in X-ray images and seeks to address global healthcare disparity through the lens of technology. Three deep learning models have been tested under varying simulated equipment quality conditions. ResNet50, VGG16 and EfficientNetv2 are the three pre-trained architectures which are compared. These models were used to perform bone fracture classification as images were progressively degraded using noise. This paper specifically empirically studies how the noise can affect the bone fractures detection and how the pre-trained models performance can be changes due to the noise that affect the quality of the X-ray images. This paper aims to help replicate real world challenges experienced by medical imaging technicians across the world. Thus, this paper establishes a methodological framework for assessing AI model degradation using transfer learning and controlled noise augmentation. The findings provide practical insight into how robust and generalizable different pre-trained deep learning powered computer vision models can be when used in different contexts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 181,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09733",
        "abs_url": "https://arxiv.org/abs/2507.09733",
        "pdf_url": "https://arxiv.org/pdf/2507.09733",
        "title": "Universal Physics Simulation: A Foundational Diffusion Approach",
        "authors": [
            "Bradley Camburn"
        ],
        "comments": "10 pages, 3 figures. Foundational AI model for universal physics simulation using sketch-guided diffusion transformers. Achieves SSIM > 0.8 on electromagnetic field generation without requiring a priori physics encoding",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We present the first foundational AI model for universal physics simulation that learns physical laws directly from boundary-condition data without requiring a priori equation encoding. Traditional physics-informed neural networks (PINNs) and finite-difference methods necessitate explicit mathematical formulation of governing equations, fundamentally limiting their generalizability and discovery potential. Our sketch-guided diffusion transformer approach reimagines computational physics by treating simulation as a conditional generation problem, where spatial boundary conditions guide the synthesis of physically accurate steady-state solutions. By leveraging enhanced diffusion transformer architectures with novel spatial relationship encoding, our model achieves direct boundary-to-equilibrium mapping and is generalizable to diverse physics domains. Unlike sequential time-stepping methods that accumulate errors over iterations, our approach bypasses temporal integration entirely, directly generating steady-state solutions with SSIM > 0.8 while maintaining sub-pixel boundary accuracy. Our data-informed approach enables physics discovery through learned representations analyzable via Layer-wise Relevance Propagation (LRP), revealing emergent physical relationships without predetermined mathematical constraints. This work represents a paradigm shift from AI-accelerated physics to AI-discovered physics, establishing the first truly universal physics simulation framework.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 182,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09759",
        "abs_url": "https://arxiv.org/abs/2507.09759",
        "pdf_url": "https://arxiv.org/pdf/2507.09759",
        "title": "AI-Enhanced Pediatric Pneumonia Detection: A CNN-Based Approach Using Data Augmentation and Generative Adversarial Networks (GANs)",
        "authors": [
            "Abdul Manaf",
            "Nimra Mughal"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Pneumonia is a leading cause of mortality in children under five, requiring accurate chest X-ray diagnosis. This study presents a machine learning-based Pediatric Chest Pneumonia Classification System to assist healthcare professionals in diagnosing pneumonia from chest X-ray images. The CNN-based model was trained on 5,863 labeled chest X-ray images from children aged 0-5 years from the Guangzhou Women and Children's Medical Center. To address limited data, we applied augmentation techniques (rotation, zooming, shear, horizontal flipping) and employed GANs to generate synthetic images, addressing class imbalance. The system achieved optimal performance using combined original, augmented, and GAN-generated data, evaluated through accuracy and F1 score metrics. The final model was deployed via a Flask web application, enabling real-time classification with probability estimates. Results demonstrate the potential of deep learning and GANs in improving diagnostic accuracy and efficiency for pediatric pneumonia classification, particularly valuable in resource-limited clinical settings this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 183,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09792",
        "abs_url": "https://arxiv.org/abs/2507.09792",
        "pdf_url": "https://arxiv.org/pdf/2507.09792",
        "title": "CADmium: Fine-Tuning Code Language Models for Text-Driven Sequential CAD Design",
        "authors": [
            "Prashant Govindarajan",
            "Davide Baldelli",
            "Jay Pathak",
            "Quentin Fournier",
            "Sarath Chandar"
        ],
        "comments": "",
        "subjects": "Graphics (cs.GR); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Computer-aided design (CAD) is the digital construction of 2D and 3D objects, and is central to a wide range of engineering and manufacturing applications like automobile and aviation. Despite its importance, CAD modeling remains largely a time-intensive, manual task. Recent works have attempted to automate this process with small transformer-based models and handcrafted CAD sequence representations. However, there has been little effort to leverage the potential of large language models (LLMs) for sequential CAD design. In this work, we introduce a new large-scale dataset of more than 170k CAD models annotated with high-quality, human-like descriptions generated with our pipeline based on GPT-4.1. Using this dataset, we fine-tune powerful code-LLMs to generate CAD sequences represented in a JSON-based format from natural language descriptions, demonstrating the viability and effectiveness of this approach for text-conditioned CAD generation. Because simple metrics often fail to reflect the quality of generated objects, we introduce geometric and topological metrics based on sphericity, mean curvature, and Euler characteristic to provide richer structural insights. Our experiments and ablation studies on both synthetic and human-annotated data demonstrate that CADmium is able to automate CAD design, drastically speeding up the design of new objects. The dataset, code, and fine-tuned models are available online.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 184,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09834",
        "abs_url": "https://arxiv.org/abs/2507.09834",
        "pdf_url": "https://arxiv.org/pdf/2507.09834",
        "title": "Generative Audio Language Modeling with Continuous-valued Tokens and Masked Next-Token Prediction",
        "authors": [
            "Shu-wen Yang",
            "Byeonggeun Kim",
            "Kuan-Po Huang",
            "Qingming Tang",
            "Huy Phan",
            "Bo-Ru Lu",
            "Harsha Sundar",
            "Shalini Ghosh",
            "Hung-yi Lee",
            "Chieh-Chi Kao",
            "Chao Wang"
        ],
        "comments": "Accepted by ICML 2025. Project website: this https URL",
        "subjects": "Audio and Speech Processing (eess.AS); Computer Vision and Pattern Recognition (cs.CV); Sound (cs.SD)",
        "abstract": "Autoregressive next-token prediction with the Transformer decoder has become a de facto standard in large language models (LLMs), achieving remarkable success in Natural Language Processing (NLP) at scale. Extending this paradigm to audio poses unique challenges due to its inherently continuous nature. We research audio generation with a causal language model (LM) without discrete tokens. We leverage token-wise diffusion to model the continuous distribution of the next continuous-valued token. Our approach delivers significant improvements over previous discrete solution, AudioGen, achieving 20% and 40% relative gains on AudioCaps in Frechet Audio Distance (FAD) and Kullback-Leibler (KL) divergence, respectively. Additionally, we propose a novel masked next-token prediction task that incorporates masked prediction into the causal LM framework. On AudioCaps, the innovation yields 41% and 33% relative FAD improvements over AudioGen Base (285M) and AudioGen Large (1B) models, respectively, and is on par with the state-of-the-art (SOTA) diffusion models. Furthermore, we achieve these results with significantly fewer parameters -- 193M for our Base and 462M for our Large models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 185,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09872",
        "abs_url": "https://arxiv.org/abs/2507.09872",
        "pdf_url": "https://arxiv.org/pdf/2507.09872",
        "title": "Resolution Revolution: A Physics-Guided Deep Learning Framework for Spatiotemporal Temperature Reconstruction",
        "authors": [
            "Shengjie Liu",
            "Lu Zhang",
            "Siqin Wang"
        ],
        "comments": "ICCV 2025 Workshop SEA -- International Conference on Computer Vision 2025 Workshop on Sustainability with Earth Observation and AI",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Central to Earth observation is the trade-off between spatial and temporal resolution. For temperature, this is especially critical because real-world applications require high spatiotemporal resolution data. Current technology allows for hourly temperature observations at 2 km, but only every 16 days at 100 m, a gap further exacerbated by cloud cover. Earth system models offer continuous hourly temperature data, but at a much coarser spatial resolution (9-31 km). Here, we present a physics-guided deep learning framework for temperature data reconstruction that integrates these two data sources. The proposed framework uses a convolutional neural network that incorporates the annual temperature cycle and includes a linear term to amplify the coarse Earth system model output into fine-scale temperature values observed from satellites. We evaluated this framework using data from two satellites, GOES-16 (2 km, hourly) and Landsat (100 m, every 16 days), and demonstrated effective temperature reconstruction with hold-out and in situ data across four datasets. This physics-guided deep learning framework opens new possibilities for generating high-resolution temperature data across spatial and temporal scales, under all weather conditions and globally.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 186,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09898",
        "abs_url": "https://arxiv.org/abs/2507.09898",
        "pdf_url": "https://arxiv.org/pdf/2507.09898",
        "title": "Advanced U-Net Architectures with CNN Backbones for Automated Lung Cancer Detection and Segmentation in Chest CT Images",
        "authors": [
            "Alireza Golkarieha",
            "Kiana Kiashemshakib",
            "Sajjad Rezvani Boroujenic",
            "Nasibeh Asadi Isakand"
        ],
        "comments": "This manuscript has 20 pages and 10 figures. It is submitted to the Journal 'Scientific Reports'",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "This study investigates the effectiveness of U-Net architectures integrated with various convolutional neural network (CNN) backbones for automated lung cancer detection and segmentation in chest CT images, addressing the critical need for accurate diagnostic tools in clinical settings. A balanced dataset of 832 chest CT images (416 cancerous and 416 non-cancerous) was preprocessed using Contrast Limited Adaptive Histogram Equalization (CLAHE) and resized to 128x128 pixels. U-Net models were developed with three CNN backbones: ResNet50, VGG16, and Xception, to segment lung regions. After segmentation, CNN-based classifiers and hybrid models combining CNN feature extraction with traditional machine learning classifiers (Support Vector Machine, Random Forest, and Gradient Boosting) were evaluated using 5-fold cross-validation. Metrics included accuracy, precision, recall, F1-score, Dice coefficient, and ROC-AUC. U-Net with ResNet50 achieved the best performance for cancerous lungs (Dice: 0.9495, Accuracy: 0.9735), while U-Net with VGG16 performed best for non-cancerous segmentation (Dice: 0.9532, Accuracy: 0.9513). For classification, the CNN model using U-Net with Xception achieved 99.1 percent accuracy, 99.74 percent recall, and 99.42 percent F1-score. The hybrid CNN-SVM-Xception model achieved 96.7 percent accuracy and 97.88 percent F1-score. Compared to prior methods, our framework consistently outperformed existing models. In conclusion, combining U-Net with advanced CNN backbones provides a powerful method for both segmentation and classification of lung cancer in CT scans, supporting early diagnosis and clinical decision-making.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 187,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09923",
        "abs_url": "https://arxiv.org/abs/2507.09923",
        "pdf_url": "https://arxiv.org/pdf/2507.09923",
        "title": "IM-LUT: Interpolation Mixing Look-Up Tables for Image Super-Resolution",
        "authors": [
            "Sejin Park",
            "Sangmin Lee",
            "Kyong Hwan Jin",
            "Seung-Won Jung"
        ],
        "comments": "ICCV 2025",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Super-resolution (SR) has been a pivotal task in image processing, aimed at enhancing image resolution across various applications. Recently, look-up table (LUT)-based approaches have attracted interest due to their efficiency and performance. However, these methods are typically designed for fixed scale factors, making them unsuitable for arbitrary-scale image SR (ASISR). Existing ASISR techniques often employ implicit neural representations, which come with considerable computational cost and memory demands. To address these limitations, we propose Interpolation Mixing LUT (IM-LUT), a novel framework that operates ASISR by learning to blend multiple interpolation functions to maximize their representational capacity. Specifically, we introduce IM-Net, a network trained to predict mixing weights for interpolation functions based on local image patterns and the target scale factor. To enhance efficiency of interpolation-based methods, IM-Net is transformed into IM-LUT, where LUTs are employed to replace computationally expensive operations, enabling lightweight and fast inference on CPUs while preserving reconstruction quality. Experimental results on several benchmark datasets demonstrate that IM-LUT consistently achieves a superior balance between image quality and efficiency compared to existing methods, highlighting its potential as a promising solution for resource-constrained applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 188,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09945",
        "abs_url": "https://arxiv.org/abs/2507.09945",
        "pdf_url": "https://arxiv.org/pdf/2507.09945",
        "title": "ESG-Net: Event-Aware Semantic Guided Network for Dense Audio-Visual Event Localization",
        "authors": [
            "Huilai Li",
            "Yonghao Dang",
            "Ying Xing",
            "Yiming Wang",
            "Jianqin Yin"
        ],
        "comments": "",
        "subjects": "Multimedia (cs.MM); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Dense audio-visual event localization (DAVE) aims to identify event categories and locate the temporal boundaries in untrimmed videos. Most studies only employ event-related semantic constraints on the final outputs, lacking cross-modal semantic bridging in intermediate layers. This causes modality semantic gap for further fusion, making it difficult to distinguish between event-related content and irrelevant background content. Moreover, they rarely consider the correlations between events, which limits the model to infer concurrent events among complex scenarios. In this paper, we incorporate multi-stage semantic guidance and multi-event relationship modeling, which respectively enable hierarchical semantic understanding of audio-visual events and adaptive extraction of event dependencies, thereby better focusing on event-related information. Specifically, our eventaware semantic guided network (ESG-Net) includes a early semantics interaction (ESI) module and a mixture of dependency experts (MoDE) module. ESI applys multi-stage semantic guidance to explicitly constrain the model in learning semantic information through multi-modal early fusion and several classification loss functions, ensuring hierarchical understanding of event-related content. MoDE promotes the extraction of multi-event dependencies through multiple serial mixture of experts with adaptive weight allocation. Extensive experiments demonstrate that our method significantly surpasses the state-of-the-art methods, while greatly reducing parameters and computational load. Our code will be released on this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 189,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09966",
        "abs_url": "https://arxiv.org/abs/2507.09966",
        "pdf_url": "https://arxiv.org/pdf/2507.09966",
        "title": "A Brain Tumor Segmentation Method Based on CLIP and 3D U-Net with Cross-Modal Semantic Guidance and Multi-Level Feature Fusion",
        "authors": [
            "Mingda Zhang"
        ],
        "comments": "13 pages,6 figures",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Precise segmentation of brain tumors from magnetic resonance imaging (MRI) is essential for neuro-oncology diagnosis and treatment planning. Despite advances in deep learning methods, automatic segmentation remains challenging due to tumor morphological heterogeneity and complex three-dimensional spatial relationships. Current techniques primarily rely on visual features extracted from MRI sequences while underutilizing semantic knowledge embedded in medical reports. This research presents a multi-level fusion architecture that integrates pixel-level, feature-level, and semantic-level information, facilitating comprehensive processing from low-level data to high-level concepts. The semantic-level fusion pathway combines the semantic understanding capabilities of Contrastive Language-Image Pre-training (CLIP) models with the spatial feature extraction advantages of 3D U-Net through three mechanisms: 3D-2D semantic bridging, cross-modal semantic guidance, and semantic-based attention mechanisms. Experimental validation on the BraTS 2020 dataset demonstrates that the proposed model achieves an overall Dice coefficient of 0.8567, representing a 4.8% improvement compared to traditional 3D U-Net, with a 7.3% Dice coefficient increase in the clinically important enhancing tumor (ET) region.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 190,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.09995",
        "abs_url": "https://arxiv.org/abs/2507.09995",
        "pdf_url": "https://arxiv.org/pdf/2507.09995",
        "title": "Graph-based Multi-Modal Interaction Lightweight Network for Brain Tumor Segmentation (GMLN-BTS) in Edge Iterative MRI Lesion Localization System (EdgeIMLocSys)",
        "authors": [
            "Guohao Huo",
            "Ruiting Dai",
            "Hao Tang"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Brain tumor segmentation plays a critical role in clinical diagnosis and treatment planning, yet the variability in imaging quality across different MRI scanners presents significant challenges to model generalization. To address this, we propose the Edge Iterative MRI Lesion Localization System (EdgeIMLocSys), which integrates Continuous Learning from Human Feedback to adaptively fine-tune segmentation models based on clinician feedback, thereby enhancing robustness to scanner-specific imaging characteristics. Central to this system is the Graph-based Multi-Modal Interaction Lightweight Network for Brain Tumor Segmentation (GMLN-BTS), which employs a Modality-Aware Adaptive Encoder (M2AE) to extract multi-scale semantic features efficiently, and a Graph-based Multi-Modal Collaborative Interaction Module (G2MCIM) to model complementary cross-modal relationships via graph structures. Additionally, we introduce a novel Voxel Refinement UpSampling Module (VRUM) that synergistically combines linear interpolation and multi-scale transposed convolutions to suppress artifacts while preserving high-frequency details, improving segmentation boundary accuracy. Our proposed GMLN-BTS model achieves a Dice score of 85.1% on the BraTS2017 dataset with only 4.58 million parameters, representing a 98% reduction compared to mainstream 3D Transformer models, and significantly outperforms existing lightweight approaches. This work demonstrates a synergistic breakthrough in achieving high-accuracy, resource-efficient brain tumor segmentation suitable for deployment in resource-constrained clinical environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 191,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10066",
        "abs_url": "https://arxiv.org/abs/2507.10066",
        "pdf_url": "https://arxiv.org/pdf/2507.10066",
        "title": "LayLens: Improving Deepfake Understanding through Simplified Explanations",
        "authors": [
            "Abhijeet Narang",
            "Parul Gupta",
            "Liuyijia Su",
            "Abhinav Dhall"
        ],
        "comments": "",
        "subjects": "Multimedia (cs.MM); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "This demonstration paper presents $\\mathbf{LayLens}$, a tool aimed to make deepfake understanding easier for users of all educational backgrounds. While prior works often rely on outputs containing technical jargon, LayLens bridges the gap between model reasoning and human understanding through a three-stage pipeline: (1) explainable deepfake detection using a state-of-the-art forgery localization model, (2) natural language simplification of technical explanations using a vision-language model, and (3) visual reconstruction of a plausible original image via guided image editing. The interface presents both technical and layperson-friendly explanations in addition to a side-by-side comparison of the uploaded and reconstructed images. A user study with 15 participants shows that simplified explanations significantly improve clarity and reduce cognitive load, with most users expressing increased confidence in identifying deepfakes. LayLens offers a step toward transparent, trustworthy, and user-centric deepfake forensics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 192,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10131",
        "abs_url": "https://arxiv.org/abs/2507.10131",
        "pdf_url": "https://arxiv.org/pdf/2507.10131",
        "title": "Probabilistic Human Intent Prediction for Mobile Manipulation: An Evaluation with Human-Inspired Constraints",
        "authors": [
            "Cesar Alan Contreras",
            "Manolis Chiou",
            "Alireza Rastegarpanah",
            "Michal Szulik",
            "Rustam Stolkin"
        ],
        "comments": "Submitted to Journal of Intelligent & Robotic Systems (Under Review)",
        "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC)",
        "abstract": "Accurate inference of human intent enables human-robot collaboration without constraining human control or causing conflicts between humans and robots. We present GUIDER (Global User Intent Dual-phase Estimation for Robots), a probabilistic framework that enables a robot to estimate the intent of human operators. GUIDER maintains two coupled belief layers, one tracking navigation goals and the other manipulation goals. In the Navigation phase, a Synergy Map blends controller velocity with an occupancy grid to rank interaction areas. Upon arrival at a goal, an autonomous multi-view scan builds a local 3D cloud. The Manipulation phase combines U2Net saliency, FastSAM instance saliency, and three geometric grasp-feasibility tests, with an end-effector kinematics-aware update rule that evolves object probabilities in real-time. GUIDER can recognize areas and objects of intent without predefined goals. We evaluated GUIDER on 25 trials (five participants x five task variants) in Isaac Sim, and compared it with two baselines, one for navigation and one for manipulation. Across the 25 trials, GUIDER achieved a median stability of 93-100% during navigation, compared with 60-100% for the BOIR baseline, with an improvement of 39.5% in a redirection scenario (T5). During manipulation, stability reached 94-100% (versus 69-100% for Trajectron), with a 31.4% difference in a redirection task (T3). In geometry-constrained trials (manipulation), GUIDER recognized the object intent three times earlier than Trajectron (median remaining time to confident prediction 23.6 s vs 7.8 s). These results validate our dual-phase framework and show improvements in intent inference in both phases of mobile manipulation tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 193,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10194",
        "abs_url": "https://arxiv.org/abs/2507.10194",
        "pdf_url": "https://arxiv.org/pdf/2507.10194",
        "title": "Learning Private Representations through Entropy-based Adversarial Training",
        "authors": [
            "Tassilo Klein",
            "Moin Nabi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "How can we learn a representation with high predictive power while preserving user privacy? We present an adversarial representation learning method for sanitizing sensitive content from the learned representation. Specifically, we introduce a variant of entropy - focal entropy, which mitigates the potential information leakage of the existing entropy-based approaches. We showcase feasibility on multiple benchmarks. The results suggest high target utility at moderate privacy leakage.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 194,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10250",
        "abs_url": "https://arxiv.org/abs/2507.10250",
        "pdf_url": "https://arxiv.org/pdf/2507.10250",
        "title": "DepViT-CAD: Deployable Vision Transformer-Based Cancer Diagnosis in Histopathology",
        "authors": [
            "Ashkan Shakarami",
            "Lorenzo Nicole",
            "Rocco Cappellesso",
            "Angelo Paolo Dei Tos",
            "Stefano Ghidoni"
        ],
        "comments": "25 pages, 15 figures",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Accurate and timely cancer diagnosis from histopathological slides is vital for effective clinical decision-making. This paper introduces DepViT-CAD, a deployable AI system for multi-class cancer diagnosis in histopathology. At its core is MAViT, a novel Multi-Attention Vision Transformer designed to capture fine-grained morphological patterns across diverse tumor types. MAViT was trained on expert-annotated patches from 1008 whole-slide images, covering 11 diagnostic categories, including 10 major cancers and non-tumor tissue. DepViT-CAD was validated on two independent cohorts: 275 WSIs from The Cancer Genome Atlas and 50 routine clinical cases from pathology labs, achieving diagnostic sensitivities of 94.11% and 92%, respectively. By combining state-of-the-art transformer architecture with large-scale real-world validation, DepViT-CAD offers a robust and scalable approach for AI-assisted cancer diagnostics. To support transparency and reproducibility, software and code will be made publicly available at GitHub.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 195,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10434",
        "abs_url": "https://arxiv.org/abs/2507.10434",
        "pdf_url": "https://arxiv.org/pdf/2507.10434",
        "title": "CLA: Latent Alignment for Online Continual Self-Supervised Learning",
        "authors": [
            "Giacomo Cignoni",
            "Andrea Cossu",
            "Alexandra Gomez-Villa",
            "Joost van de Weijer",
            "Antonio Carta"
        ],
        "comments": "Accepted at CoLLAs 2025 conference (oral)",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Self-supervised learning (SSL) is able to build latent representations that generalize well to unseen data. However, only a few SSL techniques exist for the online CL setting, where data arrives in small minibatches, the model must comply with a fixed computational budget, and task boundaries are absent. We introduce Continual Latent Alignment (CLA), a novel SSL strategy for Online CL that aligns the representations learned by the current model with past representations to mitigate forgetting. We found that our CLA is able to speed up the convergence of the training process in the online scenario, outperforming state-of-the-art approaches under the same computational budget. Surprisingly, we also discovered that using CLA as a pretraining protocol in the early stages of pretraining leads to a better final performance when compared to a full i.i.d. pretraining.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 196,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10500",
        "abs_url": "https://arxiv.org/abs/2507.10500",
        "pdf_url": "https://arxiv.org/pdf/2507.10500",
        "title": "Scene-Aware Conversational ADAS with Generative AI for Real-Time Driver Assistance",
        "authors": [
            "Kyungtae Han",
            "Yitao Chen",
            "Rohit Gupta",
            "Onur Altintas"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC)",
        "abstract": "While autonomous driving technologies continue to advance, current Advanced Driver Assistance Systems (ADAS) remain limited in their ability to interpret scene context or engage with drivers through natural language. These systems typically rely on predefined logic and lack support for dialogue-based interaction, making them inflexible in dynamic environments or when adapting to driver intent. This paper presents Scene-Aware Conversational ADAS (SC-ADAS), a modular framework that integrates Generative AI components including large language models, vision-to-text interpretation, and structured function calling to enable real-time, interpretable, and adaptive driver assistance. SC-ADAS supports multi-turn dialogue grounded in visual and sensor context, allowing natural language recommendations and driver-confirmed ADAS control. Implemented in the CARLA simulator with cloud-based Generative AI, the system executes confirmed user intents as structured ADAS commands without requiring model fine-tuning. We evaluate SC-ADAS across scene-aware, conversational, and revisited multi-turn interactions, highlighting trade-offs such as increased latency from vision-based context retrieval and token growth from accumulated dialogue history. These results demonstrate the feasibility of combining conversational reasoning, scene perception, and modular ADAS control to support the next generation of intelligent driver assistance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 197,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-15?abs=True",
        "arxiv_id": "2507.10542",
        "abs_url": "https://arxiv.org/abs/2507.10542",
        "pdf_url": "https://arxiv.org/pdf/2507.10542",
        "title": "ScaffoldAvatar: High-Fidelity Gaussian Avatars with Patch Expressions",
        "authors": [
            "Shivangi Aneja",
            "Sebastian Weiss",
            "Irene Baeza",
            "Prashanth Chandran",
            "Gaspard Zoss",
            "Matthias Nießner",
            "Derek Bradley"
        ],
        "comments": "(SIGGRAPH 2025) Paper Video: this https URL Project Page: this https URL",
        "subjects": "Graphics (cs.GR); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Generating high-fidelity real-time animated sequences of photorealistic 3D head avatars is important for many graphics applications, including immersive telepresence and movies. This is a challenging problem particularly when rendering digital avatar close-ups for showing character's facial microfeatures and expressions. To capture the expressive, detailed nature of human heads, including skin furrowing and finer-scale facial movements, we propose to couple locally-defined facial expressions with 3D Gaussian splatting to enable creating ultra-high fidelity, expressive and photorealistic 3D head avatars. In contrast to previous works that operate on a global expression space, we condition our avatar's dynamics on patch-based local expression features and synthesize 3D Gaussians at a patch level. In particular, we leverage a patch-based geometric 3D face model to extract patch expressions and learn how to translate these into local dynamic skin appearance and motion by coupling the patches with anchor points of Scaffold-GS, a recent hierarchical scene representation. These anchors are then used to synthesize 3D Gaussians on-the-fly, conditioned by patch-expressions and viewing direction. We employ color-based densification and progressive training to obtain high-quality results and faster convergence for high resolution 3K training images. By leveraging patch-level expressions, ScaffoldAvatar consistently achieves state-of-the-art performance with visually natural motion, while encompassing diverse facial expressions and styles in real time.",
        "gemini2.5flash": "",
        "overall_idea": ""
    }
]