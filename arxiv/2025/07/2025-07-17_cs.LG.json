[
    {
        "order": 1,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11547",
        "abs_url": "https://arxiv.org/abs/2507.11547",
        "pdf_url": "https://arxiv.org/pdf/2507.11547",
        "title": "Recurrent U-Net-Based Graph Neural Network (RUGNN) for Accurate Deformation Predictions in Sheet Material Forming",
        "authors": [
            "Yingxue Zhao",
            "Qianyi Chen",
            "Haoran Li",
            "Haosu Zhou",
            "Hamid Reza Attar",
            "Tobias Pfaff",
            "Tailin Wu",
            "Nan Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In recent years, various artificial intelligence-based surrogate models have been proposed to provide rapid manufacturability predictions of material forming processes. However, traditional AI-based surrogate models, typically built with scalar or image-based neural networks, are limited in their ability to capture complex 3D spatial relationships and to operate in a permutation-invariant manner. To overcome these issues, emerging graph-based surrogate models are developed using graph neural networks. This study developed a new graph neural network surrogate model named Recurrent U Net-based Graph Neural Network (RUGNN). The RUGNN model can achieve accurate predictions of sheet material deformation fields across multiple forming timesteps. The RUGNN model incorporates Gated Recurrent Units (GRUs) to model temporal dynamics and a U-Net inspired graph-based downsample/upsample mechanism to handle spatial long-range dependencies. A novel 'node-to-surface' contact representation method was proposed, offering significant improvements in computational efficiency for large-scale contact interactions. The RUGNN model was validated using a cold forming case study and a more complex hot forming case study using aluminium alloys. Results demonstrate that the RUGNN model provides accurate deformation predictions closely matching ground truth FE simulations and outperforming several baseline GNN architectures. Model tuning was also performed to identify suitable hyperparameters, training strategies, and input feature representations. These results demonstrate that RUGNN is a reliable approach to support sheet material forming design by enabling accurate manufacturability predictions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 2,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11570",
        "abs_url": "https://arxiv.org/abs/2507.11570",
        "pdf_url": "https://arxiv.org/pdf/2507.11570",
        "title": "SurgeryLSTM: A Time-Aware Neural Model for Accurate and Explainable Length of Stay Prediction After Spine Surgery",
        "authors": [
            "Ha Na Cho",
            "Sairam Sutari",
            "Alexander Lopez",
            "Hansen Bow",
            "Kai Zheng"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Image and Video Processing (eess.IV)",
        "abstract": "Objective: To develop and evaluate machine learning (ML) models for predicting length of stay (LOS) in elective spine surgery, with a focus on the benefits of temporal modeling and model interpretability. Materials and Methods: We compared traditional ML models (e.g., linear regression, random forest, support vector machine (SVM), and XGBoost) with our developed model, SurgeryLSTM, a masked bidirectional long short-term memory (BiLSTM) with an attention, using structured perioperative electronic health records (EHR) data. Performance was evaluated using the coefficient of determination (R2), and key predictors were identified using explainable AI. Results: SurgeryLSTM achieved the highest predictive accuracy (R2=0.86), outperforming XGBoost (R2 = 0.85) and baseline models. The attention mechanism improved interpretability by dynamically identifying influential temporal segments within preoperative clinical sequences, allowing clinicians to trace which events or features most contributed to each LOS prediction. Key predictors of LOS included bone disorder, chronic kidney disease, and lumbar fusion identified as the most impactful predictors of LOS. Discussion: Temporal modeling with attention mechanisms significantly improves LOS prediction by capturing the sequential nature of patient data. Unlike static models, SurgeryLSTM provides both higher accuracy and greater interpretability, which are critical for clinical adoption. These results highlight the potential of integrating attention-based temporal models into hospital planning workflows. Conclusion: SurgeryLSTM presents an effective and interpretable AI solution for LOS prediction in elective spine surgery. Our findings support the integration of temporal, explainable ML approaches into clinical decision support systems to enhance discharge readiness and individualized patient care.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 3,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11574",
        "abs_url": "https://arxiv.org/abs/2507.11574",
        "pdf_url": "https://arxiv.org/pdf/2507.11574",
        "title": "Distribution-Free Uncertainty-Aware Virtual Sensing via Conformalized Neural Operators",
        "authors": [
            "Kazuma Kobayashi",
            "Shailesh Garg",
            "Farid Ahmed",
            "Souvik Chakraborty",
            "Syed Bahauddin Alam"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "Robust uncertainty quantification (UQ) remains a critical barrier to the safe deployment of deep learning in real-time virtual sensing, particularly in high-stakes domains where sparse, noisy, or non-collocated sensor data are the norm. We introduce the Conformalized Monte Carlo Operator (CMCO), a framework that transforms neural operator-based virtual sensing with calibrated, distribution-free prediction intervals. By unifying Monte Carlo dropout with split conformal prediction in a single DeepONet architecture, CMCO achieves spatially resolved uncertainty estimates without retraining, ensembling, or custom loss design. Our method addresses a longstanding challenge: how to endow operator learning with efficient and reliable UQ across heterogeneous domains. Through rigorous evaluation on three distinct applications: turbulent flow, elastoplastic deformation, and global cosmic radiation dose estimation-CMCO consistently attains near-nominal empirical coverage, even in settings with strong spatial gradients and proxy-based sensing. This breakthrough offers a general-purpose, plug-and-play UQ solution for neural operators, unlocking real-time, trustworthy inference in digital twins, sensor fusion, and safety-critical monitoring. By bridging theory and deployment with minimal computational overhead, CMCO establishes a new foundation for scalable, generalizable, and uncertainty-aware scientific machine learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 4,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11589",
        "abs_url": "https://arxiv.org/abs/2507.11589",
        "pdf_url": "https://arxiv.org/pdf/2507.11589",
        "title": "Einstein Fields: A Neural Perspective To Computational General Relativity",
        "authors": [
            "Sandeep Suresh Cranganore",
            "Andrei Bodnar",
            "Arturs Berzins",
            "Johannes Brandstetter"
        ],
        "comments": "63 pages, 22 figures, 10 Tables, Github: this https URL",
        "subjects": "Machine Learning (cs.LG); General Relativity and Quantum Cosmology (gr-qc)",
        "abstract": "We introduce Einstein Fields, a neural representation that is designed to compress computationally intensive four-dimensional numerical relativity simulations into compact implicit neural network weights. By modeling the \\emph{metric}, which is the core tensor field of general relativity, Einstein Fields enable the derivation of physical quantities via automatic differentiation. However, unlike conventional neural fields (e.g., signed distance, occupancy, or radiance fields), Einstein Fields are \\emph{Neural Tensor Fields} with the key difference that when encoding the spacetime geometry of general relativity into neural field representations, dynamics emerge naturally as a byproduct. Einstein Fields show remarkable potential, including continuum modeling of 4D spacetime, mesh-agnosticity, storage efficiency, derivative accuracy, and ease of use. We address these challenges across several canonical test beds of general relativity and release an open source JAX-based library, paving the way for more scalable and expressive approaches to numerical relativity. Code is made available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 5,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11590",
        "abs_url": "https://arxiv.org/abs/2507.11590",
        "pdf_url": "https://arxiv.org/pdf/2507.11590",
        "title": "Synthetic Tabular Data Generation: A Comparative Survey for Modern Techniques",
        "authors": [
            "Raju Challagundla",
            "Mohsen Dorodchi",
            "Pu Wang",
            "Minwoo Lee"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "As privacy regulations become more stringent and access to real-world data becomes increasingly constrained, synthetic data generation has emerged as a vital solution, especially for tabular datasets, which are central to domains like finance, healthcare and the social sciences. This survey presents a comprehensive and focused review of recent advances in synthetic tabular data generation, emphasizing methods that preserve complex feature relationships, maintain statistical fidelity, and satisfy privacy requirements. A key contribution of this work is the introduction of a novel taxonomy based on practical generation objectives, including intended downstream applications, privacy guarantees, and data utility, directly informing methodological design and evaluation strategies. Therefore, this review prioritizes the actionable goals that drive synthetic data creation, including conditional generation and risk-sensitive modeling. Additionally, the survey proposes a benchmark framework to align technical innovation with real-world demands. By bridging theoretical foundations with practical deployment, this work serves as both a roadmap for future research and a guide for implementing synthetic tabular data in privacy-critical environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 6,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11620",
        "abs_url": "https://arxiv.org/abs/2507.11620",
        "pdf_url": "https://arxiv.org/pdf/2507.11620",
        "title": "Learning Representations of Event Time Series with Sparse Autoencoders for Anomaly Detection, Similarity Search, and Unsupervised Classification",
        "authors": [
            "Steven Dillmann",
            "Juan Rafael Martínez-Galarza"
        ],
        "comments": "Accepted at the 2025 ICML Workshop on Machine Learning for Astrophysics, Code available at: this https URL",
        "subjects": "Machine Learning (cs.LG); High Energy Astrophysical Phenomena (astro-ph.HE); Instrumentation and Methods for Astrophysics (astro-ph.IM); Artificial Intelligence (cs.AI)",
        "abstract": "Event time series are sequences of discrete events occurring at irregular time intervals, each associated with a domain-specific observational modality. They are common in domains such as high-energy astrophysics, computational social science, cybersecurity, finance, healthcare, neuroscience, and seismology. Their unstructured and irregular structure poses significant challenges for extracting meaningful patterns and identifying salient phenomena using conventional techniques. We propose novel two- and three-dimensional tensor representations for event time series, coupled with sparse autoencoders that learn physically meaningful latent representations. These embeddings support a variety of downstream tasks, including anomaly detection, similarity-based retrieval, semantic clustering, and unsupervised classification. We demonstrate our approach on a real-world dataset from X-ray astronomy, showing that these representations successfully capture temporal and spectral signatures and isolate diverse classes of X-ray transients. Our framework offers a flexible, scalable, and generalizable solution for analyzing complex, irregular event time series across scientific and industrial domains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 7,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11639",
        "abs_url": "https://arxiv.org/abs/2507.11639",
        "pdf_url": "https://arxiv.org/pdf/2507.11639",
        "title": "Deep Generative Methods and Tire Architecture Design",
        "authors": [
            "Fouad Oubari",
            "Raphael Meunier",
            "Rodrigue Décatoire",
            "Mathilde Mougeot"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "As deep generative models proliferate across the AI landscape, industrial practitioners still face critical yet unanswered questions about which deep generative models best suit complex manufacturing design tasks. This work addresses this question through a complete study of five representative models (Variational Autoencoder, Generative Adversarial Network, multimodal Variational Autoencoder, Denoising Diffusion Probabilistic Model, and Multinomial Diffusion Model) on industrial tire architecture generation. Our evaluation spans three key industrial scenarios: (i) unconditional generation of complete multi-component designs, (ii) component-conditioned generation (reconstructing architectures from partial observations), and (iii) dimension-constrained generation (creating designs that satisfy specific dimensional requirements). To enable discrete diffusion models to handle conditional scenarios, we introduce categorical inpainting, a mask-aware reverse diffusion process that preserves known labels without requiring additional training. Our evaluation employs geometry-aware metrics specifically calibrated for industrial requirements, quantifying spatial coherence, component interaction, structural connectivity, and perceptual fidelity. Our findings reveal that diffusion models achieve the strongest overall performance; a masking-trained VAE nonetheless outperforms the multimodal variant MMVAE\\textsuperscript{+} on nearly all component-conditioned metrics, and within the diffusion family MDM leads in-distribution whereas DDPM generalises better to out-of-distribution dimensional constraints.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 8,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11645",
        "abs_url": "https://arxiv.org/abs/2507.11645",
        "pdf_url": "https://arxiv.org/pdf/2507.11645",
        "title": "Tracing the Path to Grokking: Embeddings, Dropout, and Network Activation",
        "authors": [
            "Ahmed Salah",
            "David Yevick"
        ],
        "comments": "15 pages, 11 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Grokking refers to delayed generalization in which the increase in test accuracy of a neural network occurs appreciably after the improvement in training accuracy This paper introduces several practical metrics including variance under dropout, robustness, embedding similarity, and sparsity measures, that can forecast grokking behavior. Specifically, the resilience of neural networks to noise during inference is estimated from a Dropout Robustness Curve (DRC) obtained from the variation of the accuracy with the dropout rate as the model transitions from memorization to generalization. The variance of the test accuracy under stochastic dropout across training checkpoints further exhibits a local maximum during the grokking. Additionally, the percentage of inactive neurons decreases during generalization, while the embeddings tend to a bimodal distribution independent of initialization that correlates with the observed cosine similarity patterns and dataset symmetries. These metrics additionally provide valuable insight into the origin and behaviour of grokking.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 9,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11649",
        "abs_url": "https://arxiv.org/abs/2507.11649",
        "pdf_url": "https://arxiv.org/pdf/2507.11649",
        "title": "ZKP-FedEval: Verifiable and Privacy-Preserving Federated Evaluation using Zero-Knowledge Proofs",
        "authors": [
            "Daniel Commey",
            "Benjamin Appiah",
            "Griffith S. Klogo",
            "Garth V. Crosby"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Distributed, Parallel, and Cluster Computing (cs.DC); Networking and Internet Architecture (cs.NI)",
        "abstract": "Federated Learning (FL) enables collaborative model training on decentralized data without exposing raw data. However, the evaluation phase in FL may leak sensitive information through shared performance metrics. In this paper, we propose a novel protocol that incorporates Zero-Knowledge Proofs (ZKPs) to enable privacy-preserving and verifiable evaluation for FL. Instead of revealing raw loss values, clients generate a succinct proof asserting that their local loss is below a predefined threshold. Our approach is implemented without reliance on external APIs, using self-contained modules for federated learning simulation, ZKP circuit design, and experimental evaluation on both the MNIST and Human Activity Recognition (HAR) datasets. We focus on a threshold-based proof for a simple Convolutional Neural Network (CNN) model (for MNIST) and a multi-layer perceptron (MLP) model (for HAR), and evaluate the approach in terms of computational overhead, communication cost, and verifiability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 10,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11660",
        "abs_url": "https://arxiv.org/abs/2507.11660",
        "pdf_url": "https://arxiv.org/pdf/2507.11660",
        "title": "STAGED: A Multi-Agent Neural Network for Learning Cellular Interaction Dynamics",
        "authors": [
            "Joao F. Rocha",
            "Ke Xu",
            "Xingzhi Sun",
            "Ananya Krishna",
            "Dhananjay Bhaskar",
            "Blanche Mongeon",
            "Morgan Craig",
            "Mark Gerstein",
            "Smita Krishnaswamy"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Multiagent Systems (cs.MA); Quantitative Methods (q-bio.QM)",
        "abstract": "The advent of single-cell technology has significantly improved our understanding of cellular states and subpopulations in various tissues under normal and diseased conditions by employing data-driven approaches such as clustering and trajectory inference. However, these methods consider cells as independent data points of population distributions. With spatial transcriptomics, we can represent cellular organization, along with dynamic cell-cell interactions that lead to changes in cell state. Still, key computational advances are necessary to enable the data-driven learning of such complex interactive cellular dynamics. While agent-based modeling (ABM) provides a powerful framework, traditional approaches rely on handcrafted rules derived from domain knowledge rather than data-driven approaches. To address this, we introduce Spatio Temporal Agent-Based Graph Evolution Dynamics(STAGED) integrating ABM with deep learning to model intercellular communication, and its effect on the intracellular gene regulatory network. Using graph ODE networks (GDEs) with shared weights per cell type, our approach represents genes as vertices and interactions as directed edges, dynamically learning their strengths through a designed attention mechanism. Trained to match continuous trajectories of simulated as well as inferred trajectories from spatial transcriptomics data, the model captures both intercellular and intracellular interactions, enabling a more adaptive and accurate representation of cellular dynamics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 11,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11688",
        "abs_url": "https://arxiv.org/abs/2507.11688",
        "pdf_url": "https://arxiv.org/pdf/2507.11688",
        "title": "Composing Linear Layers from Irreducibles",
        "authors": [
            "Travis Pence",
            "Daisuke Yamada",
            "Vikas Singh"
        ],
        "comments": "27 Pages, 13 Tables, 8 Figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Contemporary large models often exhibit behaviors suggesting the presence of low-level primitives that compose into modules with richer functionality, but these fundamental building blocks remain poorly understood. We investigate this compositional structure in linear layers by asking: can we identify/synthesize linear transformations from a minimal set of geometric primitives? Using Clifford algebra, we show that linear layers can be expressed as compositions of bivectors -- geometric objects encoding oriented planes -- and introduce a differentiable algorithm that decomposes them into products of rotors. This construction uses only O(log^2 d) parameters, versus O(d^2) required by dense matrices. Applied to the key, query, and value projections in LLM attention layers, our rotor-based layers match the performance of strong baselines such as block-Hadamard and low-rank approximations. Our findings provide an algebraic perspective on how these geometric primitives can compose into higher-level functions within deep models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 12,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11690",
        "abs_url": "https://arxiv.org/abs/2507.11690",
        "pdf_url": "https://arxiv.org/pdf/2507.11690",
        "title": "The Impact of Coreset Selection on Spurious Correlations and Group Robustness",
        "authors": [
            "Amaya Dharmasiri",
            "William Yang",
            "Polina Kirichenko",
            "Lydia Liu",
            "Olga Russakovsky"
        ],
        "comments": "10 pages, 9 additional pages for Appendix",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Coreset selection methods have shown promise in reducing the training data size while maintaining model performance for data-efficient machine learning. However, as many datasets suffer from biases that cause models to learn spurious correlations instead of causal features, it is important to understand whether and how dataset reduction methods may perpetuate, amplify, or mitigate these biases. In this work, we conduct the first comprehensive analysis of the implications of data selection on the spurious bias levels of the selected coresets and the robustness of downstream models trained on them. We use an extensive experimental setting spanning ten different spurious correlations benchmarks, five score metrics to characterize sample importance/ difficulty, and five data selection policies across a broad range of coreset sizes. Thereby, we unravel a series of nontrivial nuances in interactions between sample difficulty and bias alignment, as well as dataset bias and resultant model robustness. For example, we find that selecting coresets using embedding-based sample characterization scores runs a comparatively lower risk of inadvertently exacerbating bias than selecting using characterizations based on learning dynamics. Most importantly, our analysis reveals that although some coreset selection methods could achieve lower bias levels by prioritizing difficult samples, they do not reliably guarantee downstream robustness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 13,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11702",
        "abs_url": "https://arxiv.org/abs/2507.11702",
        "pdf_url": "https://arxiv.org/pdf/2507.11702",
        "title": "Time series classification of satellite data using LSTM networks: an approach for predicting leaf-fall to minimize railroad traffic disruption",
        "authors": [
            "Hein de Wilde",
            "Ali Mohammed Mansoor Alsahag",
            "Pierre Blanchet"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Railroad traffic disruption as a result of leaf-fall cost the UK rail industry over 300 million per year and measures to mitigate such disruptions are employed on a large scale, with 1.67 million kilometers of track being treated in the UK in 2021 alone. Therefore, the ability to anticipate the timing of leaf-fall would offer substantial benefits for rail network operators, enabling the efficient scheduling of such mitigation measures. However, current methodologies for predicting leaf-fall exhibit considerable limitations in terms of scalability and reliability. This study endeavors to devise a prediction system that leverages specialized prediction methods and the latest satellite data sources to generate both scalable and reliable insights into leaf-fall timings. An LSTM network trained on ground-truth leaf-falling data combined with multispectral and meteorological satellite data demonstrated a root-mean-square error of 6.32 days for predicting the start of leaf-fall and 9.31 days for predicting the end of leaf-fall. The model, which improves upon previous work on the topic, offers promising opportunities for the optimization of leaf mitigation measures in the railway industry and the improvement of our understanding of complex ecological systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 14,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11706",
        "abs_url": "https://arxiv.org/abs/2507.11706",
        "pdf_url": "https://arxiv.org/pdf/2507.11706",
        "title": "Reinforcement Learning from Adversarial Preferences in Tabular MDPs",
        "authors": [
            "Taira Tsuchiya",
            "Shinji Ito",
            "Haipeng Luo"
        ],
        "comments": "40 pages",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "We introduce a new framework of episodic tabular Markov decision processes (MDPs) with adversarial preferences, which we refer to as preference-based MDPs (PbMDPs). Unlike standard episodic MDPs with adversarial losses, where the numerical value of the loss is directly observed, in PbMDPs the learner instead observes preferences between two candidate arms, which represent the choices being compared. In this work, we focus specifically on the setting where the reward functions are determined by Borda scores. We begin by establishing a regret lower bound for PbMDPs with Borda scores. As a preliminary step, we present a simple instance to prove a lower bound of $\\Omega(\\sqrt{HSAT})$ for episodic MDPs with adversarial losses, where $H$ is the number of steps per episode, $S$ is the number of states, $A$ is the number of actions, and $T$ is the number of episodes. Leveraging this construction, we then derive a regret lower bound of $\\Omega( (H^2 S K)^{1/3} T^{2/3} )$ for PbMDPs with Borda scores, where $K$ is the number of arms. Next, we develop algorithms that achieve a regret bound of order $T^{2/3}$. We first propose a global optimization approach based on online linear optimization over the set of all occupancy measures, achieving a regret bound of $\\tilde{O}((H^2 S^2 K)^{1/3} T^{2/3} )$ under known transitions. However, this approach suffers from suboptimal dependence on the potentially large number of states $S$ and computational inefficiency. To address this, we propose a policy optimization algorithm whose regret is roughly bounded by $\\tilde{O}( (H^6 S K^5)^{1/3} T^{2/3} )$ under known transitions, and further extend the result to the unknown-transition setting.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 15,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11710",
        "abs_url": "https://arxiv.org/abs/2507.11710",
        "pdf_url": "https://arxiv.org/pdf/2507.11710",
        "title": "Subgraph Generation for Generalizing on Out-of-Distribution Links",
        "authors": [
            "Jay Revolinsky",
            "Harry Shomer",
            "Jiliang Tang"
        ],
        "comments": "18 pages, 7 figures, preprint",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Graphs Neural Networks (GNNs) demonstrate high-performance on the link prediction (LP) task. However, these models often rely on all dataset samples being drawn from the same distribution. In addition, graph generative models (GGMs) show a pronounced ability to generate novel output graphs. Despite this, GGM applications remain largely limited to domain-specific tasks. To bridge this gap, we propose FLEX as a GGM framework which leverages two mechanism: (1) structurally-conditioned graph generation, and (2) adversarial co-training between an auto-encoder and GNN. As such, FLEX ensures structural-alignment between sample distributions to enhance link-prediction performance in out-of-distribution (OOD) scenarios. Notably, FLEX does not require expert knowledge to function in different OOD scenarios. Numerous experiments are conducted in synthetic and real-world OOD settings to demonstrate FLEX's performance-enhancing ability, with further analysis for understanding the effects of graph data augmentation on link structures. The source code is available here: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 16,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11729",
        "abs_url": "https://arxiv.org/abs/2507.11729",
        "pdf_url": "https://arxiv.org/pdf/2507.11729",
        "title": "Globalization for Scalable Short-term Load Forecasting",
        "authors": [
            "Amirhossein Ahmadi",
            "Hamidreza Zareipour",
            "Henry Leung"
        ],
        "comments": "63 pages with 22 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Forecasting load in power transmission networks is essential across various hierarchical levels, from the system level down to individual points of delivery (PoD). While intuitive and locally accurate, traditional local forecasting models (LFMs) face significant limitations, particularly in handling generalizability, overfitting, data drift, and the cold start problem. These methods also struggle with scalability, becoming computationally expensive and less efficient as the network's size and data volume grow. In contrast, global forecasting models (GFMs) offer a new approach to enhance prediction generalizability, scalability, accuracy, and robustness through globalization and cross-learning. This paper investigates global load forecasting in the presence of data drifts, highlighting the impact of different modeling techniques and data heterogeneity. We explore feature-transforming and target-transforming models, demonstrating how globalization, data heterogeneity, and data drift affect each differently. In addition, we examine the role of globalization in peak load forecasting and its potential for hierarchical forecasting. To address data heterogeneity and the balance between globality and locality, we propose separate time series clustering (TSC) methods, introducing model-based TSC for feature-transforming models and new weighted instance-based TSC for target-transforming models. Through extensive experiments on a real-world dataset of Alberta's electricity load, we demonstrate that global target-transforming models consistently outperform their local counterparts, especially when enriched with global features and clustering techniques. In contrast, global feature-transforming models face challenges in balancing local and global dynamics, often requiring TSC to manage data heterogeneity effectively.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 17,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11732",
        "abs_url": "https://arxiv.org/abs/2507.11732",
        "pdf_url": "https://arxiv.org/pdf/2507.11732",
        "title": "Graph Neural Networks Powered by Encoder Embedding for Improved Node Learning",
        "authors": [
            "Shiyu Chen",
            "Cencheng Shen",
            "Youngser Park",
            "Carey E. Priebe"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Graph neural networks (GNNs) have emerged as a powerful framework for a wide range of node-level graph learning tasks. However, their performance is often constrained by reliance on random or minimally informed initial feature representations, which can lead to slow convergence and suboptimal solutions. In this paper, we leverage a statistically grounded method, one-hot graph encoder embedding (GEE), to generate high-quality initial node features that enhance the end-to-end training of GNNs. We refer to this integrated framework as the GEE-powered GNN (GG), and demonstrate its effectiveness through extensive simulations and real-world experiments across both unsupervised and supervised settings. In node clustering, GG consistently achieves state-of-the-art performance, ranking first across all evaluated real-world datasets, while exhibiting faster convergence compared to the standard GNN. For node classification, we further propose an enhanced variant, GG-C, which concatenates the outputs of GG and GEE and outperforms competing baselines. These results confirm the importance of principled, structure-aware feature initialization in realizing the full potential of GNNs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 18,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11739",
        "abs_url": "https://arxiv.org/abs/2507.11739",
        "pdf_url": "https://arxiv.org/pdf/2507.11739",
        "title": "Sparse Identification of Nonlinear Dynamics with Conformal Prediction",
        "authors": [
            "Urban Fasel"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computational Engineering, Finance, and Science (cs.CE); Dynamical Systems (math.DS)",
        "abstract": "The Sparse Identification of Nonlinear Dynamics (SINDy) is a method for discovering nonlinear dynamical system models from data. Quantifying uncertainty in SINDy models is essential for assessing their reliability, particularly in safety-critical applications. While various uncertainty quantification methods exist for SINDy, including Bayesian and ensemble approaches, this work explores the integration of Conformal Prediction, a framework that can provide valid prediction intervals with coverage guarantees based on minimal assumptions like data exchangeability. We introduce three applications of conformal prediction with Ensemble-SINDy (E-SINDy): (1) quantifying uncertainty in time series prediction, (2) model selection based on library feature importance, and (3) quantifying the uncertainty of identified model coefficients using feature conformal prediction. We demonstrate the three applications on stochastic predator-prey dynamics and several chaotic dynamical systems. We show that conformal prediction methods integrated with E-SINDy can reliably achieve desired target coverage for time series forecasting, effectively quantify feature importance, and produce more robust uncertainty intervals for model coefficients, even under non-Gaussian noise, compared to standard E-SINDy coefficient estimates.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 19,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11757",
        "abs_url": "https://arxiv.org/abs/2507.11757",
        "pdf_url": "https://arxiv.org/pdf/2507.11757",
        "title": "A Graph-in-Graph Learning Framework for Drug-Target Interaction Prediction",
        "authors": [
            "Yuehua Song",
            "Yong Gao"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)",
        "abstract": "Accurately predicting drug-target interactions (DTIs) is pivotal for advancing drug discovery and target validation techniques. While machine learning approaches including those that are based on Graph Neural Networks (GNN) have achieved notable success in DTI prediction, many of them have difficulties in effectively integrating the diverse features of drugs, targets and their interactions. To address this limitation, we introduce a novel framework to take advantage of the power of both transductive learning and inductive learning so that features at molecular level and drug-target interaction network level can be exploited. Within this framework is a GNN-based model called Graph-in-Graph (GiG) that represents graphs of drug and target molecular structures as meta-nodes in a drug-target interaction graph, enabling a detailed exploration of their intricate relationships. To evaluate the proposed model, we have compiled a special benchmark comprising drug SMILES, protein sequences, and their interaction data, which is interesting in its own right. Our experimental results demonstrate that the GiG model significantly outperforms existing approaches across all evaluation metrics, highlighting the benefits of integrating different learning paradigms and interaction data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 20,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11759",
        "abs_url": "https://arxiv.org/abs/2507.11759",
        "pdf_url": "https://arxiv.org/pdf/2507.11759",
        "title": "Torsional-GFN: a conditional conformation generator for small molecules",
        "authors": [
            "Alexandra Volokhova",
            "Léna Néhale Ezzine",
            "Piotr Gaiński",
            "Luca Scimeca",
            "Emmanuel Bengio",
            "Prudencio Tossou",
            "Yoshua Bengio",
            "Alex Hernandez-Garcia"
        ],
        "comments": "The two first authors are Alexandra Volokhova and Léna Néhale Ezzine, with equal contribution",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Generating stable molecular conformations is crucial in several drug discovery applications, such as estimating the binding affinity of a molecule to a target. Recently, generative machine learning methods have emerged as a promising, more efficient method than molecular dynamics for sampling of conformations from the Boltzmann distribution. In this paper, we introduce Torsional-GFN, a conditional GFlowNet specifically designed to sample conformations of molecules proportionally to their Boltzmann distribution, using only a reward function as training signal. Conditioned on a molecular graph and its local structure (bond lengths and angles), Torsional-GFN samples rotations of its torsion angles. Our results demonstrate that Torsional-GFN is able to sample conformations approximately proportional to the Boltzmann distribution for multiple molecules with a single model, and allows for zero-shot generalization to unseen bond lengths and angles coming from the MD simulations for such molecules. Our work presents a promising avenue for scaling the proposed approach to larger molecular systems, achieving zero-shot generalization to unseen molecules, and including the generation of the local structure into the GFlowNet model.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 21,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11771",
        "abs_url": "https://arxiv.org/abs/2507.11771",
        "pdf_url": "https://arxiv.org/pdf/2507.11771",
        "title": "Scaling laws for activation steering with Llama 2 models and refusal mechanisms",
        "authors": [
            "Sheikh Abdur Raheem Ali",
            "Justin Xu",
            "Ivory Yang",
            "Jasmine Xinze Li",
            "Ayse Arslan",
            "Clark Benham"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "As large language models (LLMs) evolve in complexity and capability, the efficacy of less widely deployed alignment techniques are uncertain. Building on previous work on activation steering and contrastive activation addition (CAA), this paper explores the effectiveness of CAA with model scale using the family of Llama 2 models (7B, 13B, and 70B). CAA works by finding desirable 'directions' in the model's residual stream vector space using contrastive pairs (for example, hate to love) and adding this direction to the residual stream during the forward pass. It directly manipulates the residual stream and aims to extract features from language models to better control their outputs. Using answer matching questions centered around the refusal behavior, we found that 1) CAA is most effective when applied at early-mid layers. 2) The effectiveness of CAA diminishes with model size. 3) Negative steering has more pronounced effects than positive steering across all model sizes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 22,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11776",
        "abs_url": "https://arxiv.org/abs/2507.11776",
        "pdf_url": "https://arxiv.org/pdf/2507.11776",
        "title": "Predicting Delayed Trajectories Using Network Features: A Study on the Dutch Railway Network",
        "authors": [
            "Merel Kampere",
            "Ali Mohammed Mansoor Alsahag"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "The Dutch railway network is one of the busiest in the world, with delays being a prominent concern for the principal passenger railway operator NS. This research addresses a gap in delay prediction studies within the Dutch railway network by employing an XGBoost Classifier with a focus on topological features. Current research predominantly emphasizes short-term predictions and neglects the broader network-wide patterns essential for mitigating ripple effects. This research implements and improves an existing methodology, originally designed to forecast the evolution of the fast-changing US air network, to predict delays in the Dutch Railways. By integrating Node Centrality Measures and comparing multiple classifiers like RandomForest, DecisionTree, GradientBoosting, AdaBoost, and LogisticRegression, the goal is to predict delayed trajectories. However, the results reveal limited performance, especially in non-simultaneous testing scenarios, suggesting the necessity for more context-specific adaptations. Regardless, this research contributes to the understanding of transportation network evaluation and proposes future directions for developing more robust predictive models for delays.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 23,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11789",
        "abs_url": "https://arxiv.org/abs/2507.11789",
        "pdf_url": "https://arxiv.org/pdf/2507.11789",
        "title": "Enforcing Latent Euclidean Geometry in Single-Cell VAEs for Manifold Interpolation",
        "authors": [
            "Alessandro Palma",
            "Sergei Rybakov",
            "Leon Hetzel",
            "Stephan Günnemann",
            "Fabian J. Theis"
        ],
        "comments": "31 pages, 14 figures",
        "subjects": "Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)",
        "abstract": "Latent space interpolations are a powerful tool for navigating deep generative models in applied settings. An example is single-cell RNA sequencing, where existing methods model cellular state transitions as latent space interpolations with variational autoencoders, often assuming linear shifts and Euclidean geometry. However, unless explicitly enforced, linear interpolations in the latent space may not correspond to geodesic paths on the data manifold, limiting methods that assume Euclidean geometry in the data representations. We introduce FlatVI, a novel training framework that regularises the latent manifold of discrete-likelihood variational autoencoders towards Euclidean geometry, specifically tailored for modelling single-cell count data. By encouraging straight lines in the latent space to approximate geodesic interpolations on the decoded single-cell manifold, FlatVI enhances compatibility with downstream approaches that assume Euclidean latent geometry. Experiments on synthetic data support the theoretical soundness of our approach, while applications to time-resolved single-cell RNA sequencing data demonstrate improved trajectory reconstruction and manifold interpolation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 24,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11807",
        "abs_url": "https://arxiv.org/abs/2507.11807",
        "pdf_url": "https://arxiv.org/pdf/2507.11807",
        "title": "CLID-MU: Cross-Layer Information Divergence Based Meta Update Strategy for Learning with Noisy Labels",
        "authors": [
            "Ruofan Hu",
            "Dongyu Zhang",
            "Huayi Zhang",
            "Elke Rundensteiner"
        ],
        "comments": "KDD 2025, 12 pages, 7 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Learning with noisy labels (LNL) is essential for training deep neural networks with imperfect data. Meta-learning approaches have achieved success by using a clean unbiased labeled set to train a robust model. However, this approach heavily depends on the availability of a clean labeled meta-dataset, which is difficult to obtain in practice. In this work, we thus tackle the challenge of meta-learning for noisy label scenarios without relying on a clean labeled dataset. Our approach leverages the data itself while bypassing the need for labels. Building on the insight that clean samples effectively preserve the consistency of related data structures across the last hidden and the final layer, whereas noisy samples disrupt this consistency, we design the Cross-layer Information Divergence-based Meta Update Strategy (CLID-MU). CLID-MU leverages the alignment of data structures across these diverse feature spaces to evaluate model performance and use this alignment to guide training. Experiments on benchmark datasets with varying amounts of labels under both synthetic and real-world noise demonstrate that CLID-MU outperforms state-of-the-art methods. The code is released at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 25,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11818",
        "abs_url": "https://arxiv.org/abs/2507.11818",
        "pdf_url": "https://arxiv.org/pdf/2507.11818",
        "title": "SynCoGen: Synthesizable 3D Molecule Generation via Joint Reaction and Coordinate Modeling",
        "authors": [
            "Andrei Rekesh",
            "Miruna Cretu",
            "Dmytro Shevchuk",
            "Vignesh Ram Somnath",
            "Pietro Liò",
            "Robert A. Batey",
            "Mike Tyers",
            "Michał Koziarski",
            "Cheng-Hao Liu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Ensuring synthesizability in generative small molecule design remains a major challenge. While recent developments in synthesizable molecule generation have demonstrated promising results, these efforts have been largely confined to 2D molecular graph representations, limiting the ability to perform geometry-based conditional generation. In this work, we present SynCoGen (Synthesizable Co-Generation), a single framework that combines simultaneous masked graph diffusion and flow matching for synthesizable 3D molecule generation. SynCoGen samples from the joint distribution of molecular building blocks, chemical reactions, and atomic coordinates. To train the model, we curated SynSpace, a dataset containing over 600K synthesis-aware building block graphs and 3.3M conformers. SynCoGen achieves state-of-the-art performance in unconditional small molecule graph and conformer generation, and the model delivers competitive performance in zero-shot molecular linker design for protein ligand generation in drug discovery. Overall, this multimodal formulation represents a foundation for future applications enabled by non-autoregressive molecular generation, including analog expansion, lead optimization, and direct structure conditioning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 26,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11821",
        "abs_url": "https://arxiv.org/abs/2507.11821",
        "pdf_url": "https://arxiv.org/pdf/2507.11821",
        "title": "MNIST-Gen: A Modular MNIST-Style Dataset Generation Using Hierarchical Semantics, Reinforcement Learning, and Category Theory",
        "authors": [
            "Pouya Shaeri",
            "Arash Karimi",
            "Ariane Middel"
        ],
        "comments": "Submitted to a computer science conference",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC)",
        "abstract": "Neural networks are often benchmarked using standard datasets such as MNIST, FashionMNIST, or other variants of MNIST, which, while accessible, are limited to generic classes such as digits or clothing items. For researchers working on domain-specific tasks, such as classifying trees, food items, or other real-world objects, these data sets are insufficient and irrelevant. Additionally, creating and publishing a custom dataset can be time consuming, legally constrained, or beyond the scope of individual projects. We present MNIST-Gen, an automated, modular, and adaptive framework for generating MNIST-style image datasets tailored to user-specified categories using hierarchical semantic categorization. The system combines CLIP-based semantic understanding with reinforcement learning and human feedback to achieve intelligent categorization with minimal manual intervention. Our hierarchical approach supports complex category structures with semantic characteristics, enabling fine-grained subcategorization and multiple processing modes: individual review for maximum control, smart batch processing for large datasets, and fast batch processing for rapid creation. Inspired by category theory, MNIST-Gen models each data transformation stage as a composable morphism, enhancing clarity, modularity, and extensibility. As proof of concept, we generate and benchmark two novel datasets-\\textit{Tree-MNIST} and \\textit{Food-MNIST}-demonstrating MNIST-Gen's utility for producing task-specific evaluation data while achieving 85\\% automatic categorization accuracy and 80\\% time savings compared to manual approaches.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 27,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11836",
        "abs_url": "https://arxiv.org/abs/2507.11836",
        "pdf_url": "https://arxiv.org/pdf/2507.11836",
        "title": "HyperEvent:Learning Cohesive Events for Large-scale Dynamic Link Prediction",
        "authors": [
            "Jian Gao",
            "Jianshe Wu",
            "JingYi Ding"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Dynamic link prediction in continuous-time dynamic graphs is a fundamental task for modeling evolving complex systems. Existing node-centric and event-centric methods focus on individual interactions or atomic states, failing to capture the structural cohesion of composite hyper-events, groups of causally related events. To address this, we propose HyperEvent, a framework reframing dynamic link prediction as hyper-event recognition. Central to HyperEvent is the dynamic construction of an association sequence using event correlation vectors. These vectors quantify pairwise dependencies between the query event and relevant historical events, thereby characterizing the structural cohesion of a potential hyper-event. The framework predicts the occurrence of the query event by evaluating whether it collectively forms a valid hyper-event with these historical events. Notably, HyperEvent outperforms state-of-the-art methods on 4 out of 5 datasets in the official leaderboard. For scalability, we further introduce an efficient parallel training algorithm that segments large event streams to enable concurrent training. Experiments validate HyperEvent's superior accuracy and efficiency on large-scale graphs. Among which HyperEvent achieves a 6.95% improvement in Mean Reciprocal Rank over state-of-the-art baseline on the large-scale Flight dataset while utilizing only 10.17% of the training time.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 28,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11839",
        "abs_url": "https://arxiv.org/abs/2507.11839",
        "pdf_url": "https://arxiv.org/pdf/2507.11839",
        "title": "Protenix-Mini: Efficient Structure Predictor via Compact Architecture, Few-Step Diffusion and Switchable pLM",
        "authors": [
            "Chengyue Gong",
            "Xinshi Chen",
            "Yuxuan Zhang",
            "Yuxuan Song",
            "Hao Zhou",
            "Wenzhi Xiao"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)",
        "abstract": "Lightweight inference is critical for biomolecular structure prediction and other downstream tasks, enabling efficient real-world deployment and inference-time scaling for large-scale applications. In this work, we address the challenge of balancing model efficiency and prediction accuracy by making several key modifications, 1) Multi-step AF3 sampler is replaced by a few-step ODE sampler, significantly reducing computational overhead for the diffusion module part during inference; 2) In the open-source Protenix framework, a subset of pairformer or diffusion transformer blocks doesn't make contributions to the final structure prediction, presenting opportunities for architectural pruning and lightweight redesign; 3) A model incorporating an ESM module is trained to substitute the conventional MSA module, reducing MSA preprocessing time. Building on these key insights, we present Protenix-Mini, a compact and optimized model designed for efficient protein structure prediction. This streamlined version incorporates a more efficient architectural design with a two-step Ordinary Differential Equation (ODE) sampling strategy. By eliminating redundant Transformer components and refining the sampling process, Protenix-Mini significantly reduces model complexity with slight accuracy drop. Evaluations on benchmark datasets demonstrate that it achieves high-fidelity predictions, with only a negligible 1 to 5 percent decrease in performance on benchmark datasets compared to its full-scale counterpart. This makes Protenix-Mini an ideal choice for applications where computational resources are limited but accurate structure prediction remains crucial.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 29,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11847",
        "abs_url": "https://arxiv.org/abs/2507.11847",
        "pdf_url": "https://arxiv.org/pdf/2507.11847",
        "title": "Generalized Linear Bandits: Almost Optimal Regret with One-Pass Update",
        "authors": [
            "Yu-Jie Zhang",
            "Sheng-An Xu",
            "Peng Zhao",
            "Masashi Sugiyama"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "We study the generalized linear bandit (GLB) problem, a contextual multi-armed bandit framework that extends the classical linear model by incorporating a non-linear link function, thereby modeling a broad class of reward distributions such as Bernoulli and Poisson. While GLBs are widely applicable to real-world scenarios, their non-linear nature introduces significant challenges in achieving both computational and statistical efficiency. Existing methods typically trade off between two objectives, either incurring high per-round costs for optimal regret guarantees or compromising statistical efficiency to enable constant-time updates. In this paper, we propose a jointly efficient algorithm that attains a nearly optimal regret bound with $\\mathcal{O}(1)$ time and space complexities per round. The core of our method is a tight confidence set for the online mirror descent (OMD) estimator, which is derived through a novel analysis that leverages the notion of mix loss from online prediction. The analysis shows that our OMD estimator, even with its one-pass updates, achieves statistical efficiency comparable to maximum likelihood estimation, thereby leading to a jointly efficient optimistic method.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 30,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11855",
        "abs_url": "https://arxiv.org/abs/2507.11855",
        "pdf_url": "https://arxiv.org/pdf/2507.11855",
        "title": "OrdShap: Feature Position Importance for Sequential Black-Box Models",
        "authors": [
            "Davin Hill",
            "Brian L. Hill",
            "Aria Masoomi",
            "Vijay S. Nori",
            "Robert E. Tillman",
            "Jennifer Dy"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Sequential deep learning models excel in domains with temporal or sequential dependencies, but their complexity necessitates post-hoc feature attribution methods for understanding their predictions. While existing techniques quantify feature importance, they inherently assume fixed feature ordering - conflating the effects of (1) feature values and (2) their positions within input sequences. To address this gap, we introduce OrdShap, a novel attribution method that disentangles these effects by quantifying how a model's predictions change in response to permuting feature position. We establish a game-theoretic connection between OrdShap and Sanchez-Bergantiños values, providing a theoretically grounded approach to position-sensitive attribution. Empirical results from health, natural language, and synthetic datasets highlight OrdShap's effectiveness in capturing feature value and feature position attributions, and provide deeper insight into model behavior.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 31,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11865",
        "abs_url": "https://arxiv.org/abs/2507.11865",
        "pdf_url": "https://arxiv.org/pdf/2507.11865",
        "title": "A Policy-Improved Deep Deterministic Policy Gradient Framework for the Discount Order Acceptance Strategy of Ride-hailing Drivers",
        "authors": [
            "Hanwen Dai",
            "Chang Gao",
            "Fang He",
            "Congyuan Ji",
            "Yanni Yang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The rapid expansion of platform integration has emerged as an effective solution to mitigate market fragmentation by consolidating multiple ride-hailing platforms into a single application. To address heterogeneous passenger preferences, third-party integrators provide Discount Express service delivered by express drivers at lower trip fares. For the individual platform, encouraging broader participation of drivers in Discount Express services has the potential to expand the accessible demand pool and improve matching efficiency, but often at the cost of reduced profit margins. This study aims to dynamically manage drivers' acceptance of Discount Express from the perspective of individual platforms. The lack of historical data under the new business model necessitates online learning. However, early-stage exploration through trial and error can be costly in practice, highlighting the need for reliable early-stage performance in real-world deployment. To address these challenges, this study formulates the decision regarding the proportion of drivers' acceptance behavior as a continuous control task. In response to the high stochasticity, the opaque matching mechanisms employed by third-party integrator, and the limited availability of historical data, we propose a policy-improved deep deterministic policy gradient (pi-DDPG) framework. The proposed framework incorporates a refiner module to boost policy performance during the early training phase, leverages a convolutional long short-term memory network to effectively capture complex spatiotemporal patterns, and adopts a prioritized experience replay mechanism to enhance learning efficiency. A simulator based on a real-world dataset is developed to validate the effectiveness of the proposed pi-DDPG. Numerical experiments demonstrate that pi-DDPG achieves superior learning efficiency and significantly reduces early-stage training losses.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 32,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11901",
        "abs_url": "https://arxiv.org/abs/2507.11901",
        "pdf_url": "https://arxiv.org/pdf/2507.11901",
        "title": "Imbalanced Regression Pipeline Recommendation",
        "authors": [
            "Juscimara G. Avelino",
            "George D. C. Cavalcanti",
            "Rafael M. O. Cruz"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Imbalanced problems are prevalent in various real-world scenarios and are extensively explored in classification tasks. However, they also present challenges for regression tasks due to the rarity of certain target values. A common alternative is to employ balancing algorithms in preprocessing to address dataset imbalance. However, due to the variety of resampling methods and learning models, determining the optimal solution requires testing many combinations. Furthermore, the learning model, dataset, and evaluation metric affect the best strategies. This work proposes the Meta-learning for Imbalanced Regression (Meta-IR) framework, which diverges from existing literature by training meta-classifiers to recommend the best pipeline composed of the resampling strategy and learning model per task in a zero-shot fashion. The meta-classifiers are trained using a set of meta-features to learn how to map the meta-features to the classes indicating the best pipeline. We propose two formulations: Independent and Chained. Independent trains the meta-classifiers to separately indicate the best learning algorithm and resampling strategy. Chained involves a sequential procedure where the output of one meta-classifier is used as input for another to model intrinsic relationship factors. The Chained scenario showed superior performance, suggesting a relationship between the learning algorithm and the resampling strategy per task. Compared with AutoML frameworks, Meta-IR obtained better results. Moreover, compared with baselines of six learning algorithms and six resampling algorithms plus no resampling, totaling 42 (6 X 7) configurations, Meta-IR outperformed all of them. The code, data, and further information of the experiments can be found on GitHub: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 33,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11902",
        "abs_url": "https://arxiv.org/abs/2507.11902",
        "pdf_url": "https://arxiv.org/pdf/2507.11902",
        "title": "Resampling strategies for imbalanced regression: a survey and empirical analysis",
        "authors": [
            "Juscimara G. Avelino",
            "George D. C. Cavalcanti",
            "Rafael M. O. Cruz"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Imbalanced problems can arise in different real-world situations, and to address this, certain strategies in the form of resampling or balancing algorithms are proposed. This issue has largely been studied in the context of classification, and yet, the same problem features in regression tasks, where target values are continuous. This work presents an extensive experimental study comprising various balancing and predictive models, and wich uses metrics to capture important elements for the user and to evaluate the predictive model in an imbalanced regression data context. It also proposes a taxonomy for imbalanced regression approaches based on three crucial criteria: regression model, learning process, and evaluation metrics. The study offers new insights into the use of such strategies, highlighting the advantages they bring to each model's learning process, and indicating directions for further studies. The code, data and further information related to the experiments performed herein can be found on GitHub: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 34,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11926",
        "abs_url": "https://arxiv.org/abs/2507.11926",
        "pdf_url": "https://arxiv.org/pdf/2507.11926",
        "title": "From Generative to Episodic: Sample-Efficient Replicable Reinforcement Learning",
        "authors": [
            "Max Hopkins",
            "Sihan Liu",
            "Christopher Ye",
            "Yuichi Yoshida"
        ],
        "comments": "67 pages",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The epidemic failure of replicability across empirical science and machine learning has recently motivated the formal study of replicable learning algorithms [Impagliazzo et al. (2022)]. In batch settings where data comes from a fixed i.i.d. source (e.g., hypothesis testing, supervised learning), the design of data-efficient replicable algorithms is now more or less understood. In contrast, there remain significant gaps in our knowledge for control settings like reinforcement learning where an agent must interact directly with a shifting environment. Karbasi et. al show that with access to a generative model of an environment with $S$ states and $A$ actions (the RL 'batch setting'), replicably learning a near-optimal policy costs only $\\tilde{O}(S^2A^2)$ samples. On the other hand, the best upper bound without a generative model jumps to $\\tilde{O}(S^7 A^7)$ [Eaton et al. (2024)] due to the substantial difficulty of environment exploration. This gap raises a key question in the broader theory of replicability: Is replicable exploration inherently more expensive than batch learning? Is sample-efficient replicable RL even possible? In this work, we (nearly) resolve this problem (for low-horizon tabular MDPs): exploration is not a significant barrier to replicable learning! Our main result is a replicable RL algorithm on $\\tilde{O}(S^2A)$ samples, bridging the gap between the generative and episodic settings. We complement this with a matching $\\tilde{\\Omega}(S^2A)$ lower bound in the generative setting (under the common parallel sampling assumption) and an unconditional lower bound in the episodic setting of $\\tilde{\\Omega}(S^2)$ showcasing the near-optimality of our algorithm with respect to the state space $S$.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 35,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11928",
        "abs_url": "https://arxiv.org/abs/2507.11928",
        "pdf_url": "https://arxiv.org/pdf/2507.11928",
        "title": "Accelerating RF Power Amplifier Design via Intelligent Sampling and ML-Based Parameter Tuning",
        "authors": [
            "Abhishek Sriram",
            "Neal Tuffy"
        ],
        "comments": "This paper is a pre-print version and has been submitted to the IEEE International Conference on Future Machine Learning and Data Science (FMLDS 2025)",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "This paper presents a machine learning-accelerated optimization framework for RF power amplifier design that reduces simulation requirements by 65% while maintaining $\\pm0.3$ to $\\pm0.4$ dBm accuracy. The proposed method combines MaxMin Latin Hypercube Sampling with CatBoost gradient boosting to intelligently explore multidimensional parameter spaces. Instead of exhaustively simulating all parameter combinations to achieve target P2dB compression specifications, our approach strategically selects approximately 35% of critical simulation points. The framework processes ADS netlists, executes harmonic balance simulations on the reduced dataset, and trains a CatBoost model to predict P2dB performance across the entire design space. Validation across 15 PA operating modes yields an average $R^2$ of 0.901, with the system ranking parameter combinations by their likelihood of meeting target specifications. The integrated solution delivers 58.24% to 77.78% reduction in simulation time through automated GUI-based workflows, enabling rapid design iterations without compromising accuracy standards required for production RF circuits.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 36,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11948",
        "abs_url": "https://arxiv.org/abs/2507.11948",
        "pdf_url": "https://arxiv.org/pdf/2507.11948",
        "title": "Kevin: Multi-Turn RL for Generating CUDA Kernels",
        "authors": [
            "Carlo Baronio",
            "Pietro Marsella",
            "Ben Pan",
            "Simon Guo",
            "Silas Alberti"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Performance (cs.PF); Software Engineering (cs.SE)",
        "abstract": "Writing GPU kernels is a challenging task and critical for AI systems' efficiency. It is also highly iterative: domain experts write code and improve performance through execution feedback. Moreover, it presents verifiable rewards like correctness and speedup, making it a natural environment to apply Reinforcement Learning (RL). To explicitly incorporate the iterative nature of this process into training, we develop a flexible multi-turn RL recipe that addresses unique challenges encountered in real-world settings, such as learning from long trajectories and effective reward attribution across turns. We present Kevin - K(ernel D)evin, the first model trained with multi-turn RL for CUDA kernel generation and optimization. In our evaluation setup, Kevin shows significant gains over its base model (QwQ-32B), improving correctness of generated kernels (in pure CUDA) from 56% to 82% and mean speedup from 0.53x to 1.10x of baseline (PyTorch Eager), and surpassing frontier models like o4-mini (0.78x). Finally, we study its behavior across test-time scaling axes: we found scaling serial refinement more beneficial than parallel sampling. In particular, when given more refinement turns, Kevin shows a higher rate of improvement.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 37,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11975",
        "abs_url": "https://arxiv.org/abs/2507.11975",
        "pdf_url": "https://arxiv.org/pdf/2507.11975",
        "title": "Online Training and Pruning of Deep Reinforcement Learning Networks",
        "authors": [
            "Valentin Frank Ingmar Guenter",
            "Athanasios Sideris"
        ],
        "comments": "25 pages, 5 figures, 4 tables",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Robotics (cs.RO)",
        "abstract": "Scaling deep neural networks (NN) of reinforcement learning (RL) algorithms has been shown to enhance performance when feature extraction networks are used but the gained performance comes at the significant expense of increased computational and memory complexity. Neural network pruning methods have successfully addressed this challenge in supervised learning. However, their application to RL is underexplored. We propose an approach to integrate simultaneous training and pruning within advanced RL methods, in particular to RL algorithms enhanced by the Online Feature Extractor Network (OFENet). Our networks (XiNet) are trained to solve stochastic optimization problems over the RL networks' weights and the parameters of variational Bernoulli distributions for 0/1 Random Variables $\\xi$ scaling each unit in the networks. The stochastic problem formulation induces regularization terms that promote convergence of the variational parameters to 0 when a unit contributes little to the performance. In this case, the corresponding structure is rendered permanently inactive and pruned from its network. We propose a cost-aware, sparsity-promoting regularization scheme, tailored to the DenseNet architecture of OFENets expressing the parameter complexity of involved networks in terms of the parameters of the RVs in these networks. Then, when matching this cost with the regularization terms, the many hyperparameters associated with them are automatically selected, effectively combining the RL objectives and network compression. We evaluate our method on continuous control benchmarks (MuJoCo) and the Soft Actor-Critic RL agent, demonstrating that OFENets can be pruned considerably with minimal loss in performance. Furthermore, our results confirm that pruning large networks during training produces more efficient and higher performing RL agents rather than training smaller networks from scratch.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 38,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11997",
        "abs_url": "https://arxiv.org/abs/2507.11997",
        "pdf_url": "https://arxiv.org/pdf/2507.11997",
        "title": "Can LLMs Find Fraudsters? Multi-level LLM Enhanced Graph Fraud Detection",
        "authors": [
            "Tairan Huang",
            "Yili Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Graph fraud detection has garnered significant attention as Graph Neural Networks (GNNs) have proven effective in modeling complex relationships within multimodal data. However, existing graph fraud detection methods typically use preprocessed node embeddings and predefined graph structures to reveal fraudsters, which ignore the rich semantic cues contained in raw textual information. Although Large Language Models (LLMs) exhibit powerful capabilities in processing textual information, it remains a significant challenge to perform multimodal fusion of processed textual embeddings with graph structures. In this paper, we propose a \\textbf{M}ulti-level \\textbf{L}LM \\textbf{E}nhanced Graph Fraud \\textbf{D}etection framework called MLED. In MLED, we utilize LLMs to extract external knowledge from textual information to enhance graph fraud detection methods. To integrate LLMs with graph structure information and enhance the ability to distinguish fraudsters, we design a multi-level LLM enhanced framework including type-level enhancer and relation-level enhancer. One is to enhance the difference between the fraudsters and the benign entities, the other is to enhance the importance of the fraudsters in different relations. The experiments on four real-world datasets show that MLED achieves state-of-the-art performance in graph fraud detection as a generalized framework that can be applied to existing methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 39,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12002",
        "abs_url": "https://arxiv.org/abs/2507.12002",
        "pdf_url": "https://arxiv.org/pdf/2507.12002",
        "title": "Detecting In-Person Conversations in Noisy Real-World Environments with Smartwatch Audio and Motion Sensing",
        "authors": [
            "Alice Zhang",
            "Callihan Bertley",
            "Dawei Liang",
            "Edison Thomaz"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Social interactions play a crucial role in shaping human behavior, relationships, and societies. It encompasses various forms of communication, such as verbal conversation, non-verbal gestures, facial expressions, and body language. In this work, we develop a novel computational approach to detect a foundational aspect of human social interactions, in-person verbal conversations, by leveraging audio and inertial data captured with a commodity smartwatch in acoustically-challenging scenarios. To evaluate our approach, we conducted a lab study with 11 participants and a semi-naturalistic study with 24 participants. We analyzed machine learning and deep learning models with 3 different fusion methods, showing the advantages of fusing audio and inertial data to consider not only verbal cues but also non-verbal gestures in conversations. Furthermore, we perform a comprehensive set of evaluations across activities and sampling rates to demonstrate the benefits of multimodal sensing in specific contexts. Overall, our framework achieved 82.0$\\pm$3.0% macro F1-score when detecting conversations in the lab and 77.2$\\pm$1.8% in the semi-naturalistic setting.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 40,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12011",
        "abs_url": "https://arxiv.org/abs/2507.12011",
        "pdf_url": "https://arxiv.org/pdf/2507.12011",
        "title": "DUSE: A Data Expansion Framework for Low-resource Automatic Modulation Recognition based on Active Learning",
        "authors": [
            "Yao Lu",
            "Hongyu Gao",
            "Zhuangzhi Chen",
            "Dongwei Xu",
            "Yun Lin",
            "Qi Xuan",
            "Guan Gui"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Although deep neural networks have made remarkable achievements in the field of automatic modulation recognition (AMR), these models often require a large amount of labeled data for training. However, in many practical scenarios, the available target domain data is scarce and difficult to meet the needs of model training. The most direct way is to collect data manually and perform expert annotation, but the high time and labor costs are unbearable. Another common method is data augmentation. Although it can enrich training samples to a certain extent, it does not introduce new data and therefore cannot fundamentally solve the problem of data scarcity. To address these challenges, we introduce a data expansion framework called Dynamic Uncertainty-driven Sample Expansion (DUSE). Specifically, DUSE uses an uncertainty scoring function to filter out useful samples from relevant AMR datasets and employs an active learning strategy to continuously refine the scorer. Extensive experiments demonstrate that DUSE consistently outperforms 8 coreset selection baselines in both class-balance and class-imbalance settings. Besides, DUSE exhibits strong cross-architecture generalization for unseen models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 41,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12041",
        "abs_url": "https://arxiv.org/abs/2507.12041",
        "pdf_url": "https://arxiv.org/pdf/2507.12041",
        "title": "Granular feedback merits sophisticated aggregation",
        "authors": [
            "Anmol Kagrecha",
            "Henrik Marklund",
            "Potsawee Manakul",
            "Richard Zeckhauser",
            "Benjamin Van Roy"
        ],
        "comments": "31 pages, 8 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Human feedback is increasingly used across diverse applications like training AI models, developing recommender systems, and measuring public opinion -- with granular feedback often being preferred over binary feedback for its greater informativeness. While it is easy to accurately estimate a population's distribution of feedback given feedback from a large number of individuals, cost constraints typically necessitate using smaller groups. A simple method to approximate the population distribution is regularized averaging: compute the empirical distribution and regularize it toward a prior. Can we do better? As we will discuss, the answer to this question depends on feedback granularity. Suppose one wants to predict a population's distribution of feedback using feedback from a limited number of individuals. We show that, as feedback granularity increases, one can substantially improve upon predictions of regularized averaging by combining individuals' feedback in ways more sophisticated than regularized averaging. Our empirical analysis using questions on social attitudes confirms this pattern. In particular, with binary feedback, sophistication barely reduces the number of individuals required to attain a fixed level of performance. By contrast, with five-point feedback, sophisticated methods match the performance of regularized averaging with about half as many individuals.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 42,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12043",
        "abs_url": "https://arxiv.org/abs/2507.12043",
        "pdf_url": "https://arxiv.org/pdf/2507.12043",
        "title": "Information-Theoretic Generalization Bounds of Replay-based Continual Learning",
        "authors": [
            "Wen Wen",
            "Tieliang Gong",
            "Yunjiao Zhang",
            "Zeyu Gao",
            "Weizhan Zhang",
            "Yong-Jin Liu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Continual learning (CL) has emerged as a dominant paradigm for acquiring knowledge from sequential tasks while avoiding catastrophic forgetting. Although many CL methods have been proposed to show impressive empirical performance, the theoretical understanding of their generalization behavior remains limited, particularly for replay-based approaches. In this paper, we establish a unified theoretical framework for replay-based CL, deriving a series of information-theoretic bounds that explicitly characterize how the memory buffer interacts with the current task to affect generalization. Specifically, our hypothesis-based bounds reveal that utilizing the limited exemplars of previous tasks alongside the current task data, rather than exhaustive replay, facilitates improved generalization while effectively mitigating catastrophic forgetting. Furthermore, our prediction-based bounds yield tighter and computationally tractable upper bounds of the generalization gap through the use of low-dimensional variables. Our analysis is general and broadly applicable to a wide range of learning algorithms, exemplified by stochastic gradient Langevin dynamics (SGLD) as a representative method. Comprehensive experimental evaluations demonstrate the effectiveness of our derived bounds in capturing the generalization dynamics in replay-based CL settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 43,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12053",
        "abs_url": "https://arxiv.org/abs/2507.12053",
        "pdf_url": "https://arxiv.org/pdf/2507.12053",
        "title": "FloGAN: Scenario-Based Urban Mobility Flow Generation via Conditional GANs and Dynamic Region Decoupling",
        "authors": [
            "Seanglidet Yean",
            "Jiazu Zhou",
            "Bu-Sung Lee",
            "Markus Schläpfer"
        ],
        "comments": "International Conference on Intelligent Digitization of Systems and Services, Valencia, Spain, 2025 (IDSS 2025)",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The mobility patterns of people in cities evolve alongside changes in land use and population. This makes it crucial for urban planners to simulate and analyze human mobility patterns for purposes such as transportation optimization and sustainable urban development. Existing generative models borrowed from machine learning rely heavily on historical trajectories and often overlook evolving factors like changes in population density and land use. Mechanistic approaches incorporate population density and facility distribution but assume static scenarios, limiting their utility for future projections where historical data for calibration is unavailable. This study introduces a novel, data-driven approach for generating origin-destination mobility flows tailored to simulated urban scenarios. Our method leverages adaptive factors such as dynamic region sizes and land use archetypes, and it utilizes conditional generative adversarial networks (cGANs) to blend historical data with these adaptive parameters. The approach facilitates rapid mobility flow generation with adjustable spatial granularity based on regions of interest, without requiring extensive calibration data or complex behavior modeling. The promising performance of our approach is demonstrated by its application to mobile phone data from Singapore, and by its comparison with existing methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 44,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12070",
        "abs_url": "https://arxiv.org/abs/2507.12070",
        "pdf_url": "https://arxiv.org/pdf/2507.12070",
        "title": "Emergence of Quantised Representations Isolated to Anisotropic Functions",
        "authors": [
            "George Bird"
        ],
        "comments": "36 pages, 31 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "This paper describes a novel methodology for determining representational alignment, developed upon the existing Spotlight Resonance method. Using this, it is found that algebraic symmetries of network primitives are a strong predictor for task-agnostic structure in representations. Particularly, this new tool is used to gain insight into how discrete representations can form and arrange in autoencoder models, through an ablation study where only the activation function is altered. Representations are found to tend to discretise when the activation functions are defined through a discrete algebraic permutation-equivariant symmetry. In contrast, they remain continuous under a continuous algebraic orthogonal-equivariant definition. These findings corroborate the hypothesis that functional form choices can carry unintended inductive biases which produce task-independent artefactual structures in representations, particularly that contemporary forms induce discretisation of otherwise continuous structure -- a quantisation effect. Moreover, this supports a general causal model for one mode in which discrete representations may form, and could constitute a prerequisite for downstream interpretability phenomena, including grandmother neurons, discrete coding schemes, general linear features and possibly Superposition. Hence, this tool and proposed mechanism for the influence of functional form on representations may provide several insights into emergent interpretability research. Finally, preliminary results indicate that quantisation of representations appears to correlate with a measurable increase in reconstruction error, reinforcing previous conjectures that this collapse can be detrimental.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 45,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12094",
        "abs_url": "https://arxiv.org/abs/2507.12094",
        "pdf_url": "https://arxiv.org/pdf/2507.12094",
        "title": "Measuring Informativeness Gap of (Mis)Calibrated Predictors",
        "authors": [
            "Yiding Feng",
            "Wei Tang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Science and Game Theory (cs.GT)",
        "abstract": "In many applications, decision-makers must choose between multiple predictive models that may all be miscalibrated. Which model (i.e., predictor) is more \"useful\" in downstream decision tasks? To answer this, our first contribution introduces the notion of the informativeness gap between any two predictors, defined as the maximum normalized payoff advantage one predictor offers over the other across all decision-making tasks. Our framework strictly generalizes several existing notions: it subsumes U-Calibration [KLST-23] and Calibration Decision Loss [HW-24], which compare a miscalibrated predictor to its calibrated counterpart, and it recovers Blackwell informativeness [Bla-51, Bla-53] as a special case when both predictors are perfectly calibrated. Our second contribution is a dual characterization of the informativeness gap, which gives rise to a natural informativeness measure that can be viewed as a relaxed variant of the earth mover's distance (EMD) between two prediction distributions. We show that this measure satisfies natural desiderata: it is complete and sound, and it can be estimated sample-efficiently in the prediction-only access setting. Along the way, we also obtain novel combinatorial structural results when applying this measure to perfectly calibrated predictors.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 46,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12127",
        "abs_url": "https://arxiv.org/abs/2507.12127",
        "pdf_url": "https://arxiv.org/pdf/2507.12127",
        "title": "Self-Adaptive and Robust Federated Spectrum Sensing without Benign Majority for Cellular Networks",
        "authors": [
            "Ngoc Duy Pham",
            "Thusitha Dayaratne",
            "Viet Vo",
            "Shangqi Lai",
            "Sharif Abuadbba",
            "Hajime Suzuki",
            "Xingliang Yuan",
            "Carsten Rudolph"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Advancements in wireless and mobile technologies, including 5G advanced and the envisioned 6G, are driving exponential growth in wireless devices. However, this rapid expansion exacerbates spectrum scarcity, posing a critical challenge. Dynamic spectrum allocation (DSA)--which relies on sensing and dynamically sharing spectrum--has emerged as an essential solution to address this issue. While machine learning (ML) models hold significant potential for improving spectrum sensing, their adoption in centralized ML-based DSA systems is limited by privacy concerns, bandwidth constraints, and regulatory challenges. To overcome these limitations, distributed ML-based approaches such as Federated Learning (FL) offer promising alternatives. This work addresses two key challenges in FL-based spectrum sensing (FLSS). First, the scarcity of labeled data for training FL models in practical spectrum sensing scenarios is tackled with a semi-supervised FL approach, combined with energy detection, enabling model training on unlabeled datasets. Second, we examine the security vulnerabilities of FLSS, focusing on the impact of data poisoning attacks. Our analysis highlights the shortcomings of existing majority-based defenses in countering such attacks. To address these vulnerabilities, we propose a novel defense mechanism inspired by vaccination, which effectively mitigates data poisoning attacks without relying on majority-based assumptions. Extensive experiments on both synthetic and real-world datasets validate our solutions, demonstrating that FLSS can achieve near-perfect accuracy on unlabeled datasets and maintain Byzantine robustness against both targeted and untargeted data poisoning attacks, even when a significant proportion of participants are malicious.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 47,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12133",
        "abs_url": "https://arxiv.org/abs/2507.12133",
        "pdf_url": "https://arxiv.org/pdf/2507.12133",
        "title": "HyDRA: A Hybrid Dual-Mode Network for Closed- and Open-Set RFFI with Optimized VMD",
        "authors": [
            "Hanwen Liu",
            "Yuhe Huang",
            "Yifeng Gong",
            "Yanjie Zhai",
            "Jiaxuan Lu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Signal Processing (eess.SP)",
        "abstract": "Device recognition is vital for security in wireless communication systems, particularly for applications like access control. Radio Frequency Fingerprint Identification (RFFI) offers a non-cryptographic solution by exploiting hardware-induced signal distortions. This paper proposes HyDRA, a Hybrid Dual-mode RF Architecture that integrates an optimized Variational Mode Decomposition (VMD) with a novel architecture based on the fusion of Convolutional Neural Networks (CNNs), Transformers, and Mamba components, designed to support both closed-set and open-set classification tasks. The optimized VMD enhances preprocessing efficiency and classification accuracy by fixing center frequencies and using closed-form solutions. HyDRA employs the Transformer Dynamic Sequence Encoder (TDSE) for global dependency modeling and the Mamba Linear Flow Encoder (MLFE) for linear-complexity processing, adapting to varying conditions. Evaluation on public datasets demonstrates state-of-the-art (SOTA) accuracy in closed-set scenarios and robust performance in our proposed open-set classification method, effectively identifying unauthorized devices. Deployed on NVIDIA Jetson Xavier NX, HyDRA achieves millisecond-level inference speed with low power consumption, providing a practical solution for real-time wireless authentication in real-world environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 48,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12142",
        "abs_url": "https://arxiv.org/abs/2507.12142",
        "pdf_url": "https://arxiv.org/pdf/2507.12142",
        "title": "RiemannLoRA: A Unified Riemannian Framework for Ambiguity-Free LoRA Optimization",
        "authors": [
            "Vladimir Bogachev",
            "Vladimir Aletov",
            "Alexander Molozhavenko",
            "Denis Bobkov",
            "Vera Soboleva",
            "Aibek Alanov",
            "Maxim Rakhuba"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL); Differential Geometry (math.DG); Numerical Analysis (math.NA)",
        "abstract": "Low-Rank Adaptation (LoRA) has become a widely adopted standard for parameter-efficient fine-tuning of large language models (LLMs), significantly reducing memory and computational demands. However, challenges remain, including finding optimal initialization strategies or mitigating overparametrization in low-rank matrix factorization. In this work, we propose a novel approach that addresses both of the challenges simultaneously within a unified framework. Our method treats a set of fixed-rank LoRA matrices as a smooth manifold. Considering adapters as elements on this manifold removes overparametrization, while determining the direction of the fastest loss decrease along the manifold provides initialization. Special care is taken to obtain numerically stable and computationally efficient implementation of our method, using best practices from numerical linear algebra and Riemannian optimization. Experimental results on LLM and diffusion model architectures demonstrate that RiemannLoRA consistently improves both convergence speed and final performance over standard LoRA and its state-of-the-art modifications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 49,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12144",
        "abs_url": "https://arxiv.org/abs/2507.12144",
        "pdf_url": "https://arxiv.org/pdf/2507.12144",
        "title": "FourCastNet 3: A geometric approach to probabilistic machine-learning weather forecasting at scale",
        "authors": [
            "Boris Bonev",
            "Thorsten Kurth",
            "Ankur Mahesh",
            "Mauro Bisson",
            "Jean Kossaifi",
            "Karthik Kashinath",
            "Anima Anandkumar",
            "William D. Collins",
            "Michael S. Pritchard",
            "Alexander Keller"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Atmospheric and Oceanic Physics (physics.ao-ph)",
        "abstract": "FourCastNet 3 advances global weather modeling by implementing a scalable, geometric machine learning (ML) approach to probabilistic ensemble forecasting. The approach is designed to respect spherical geometry and to accurately model the spatially correlated probabilistic nature of the problem, resulting in stable spectra and realistic dynamics across multiple scales. FourCastNet 3 delivers forecasting accuracy that surpasses leading conventional ensemble models and rivals the best diffusion-based methods, while producing forecasts 8 to 60 times faster than these approaches. In contrast to other ML approaches, FourCastNet 3 demonstrates excellent probabilistic calibration and retains realistic spectra, even at extended lead times of up to 60 days. All of these advances are realized using a purely convolutional neural network architecture tailored for spherical geometry. Scalable and efficient large-scale training on 1024 GPUs and more is enabled by a novel training paradigm for combined model- and data-parallelism, inspired by domain decomposition methods in classical numerical models. Additionally, FourCastNet 3 enables rapid inference on a single GPU, producing a 90-day global forecast at 0.25°, 6-hourly resolution in under 20 seconds. Its computational efficiency, medium-range probabilistic skill, spectral fidelity, and rollout stability at subseasonal timescales make it a strong candidate for improving meteorological forecasting and early warning systems through large ensemble predictions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 50,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12145",
        "abs_url": "https://arxiv.org/abs/2507.12145",
        "pdf_url": "https://arxiv.org/pdf/2507.12145",
        "title": "PRISM: Distributed Inference for Foundation Models at Edge",
        "authors": [
            "Muhammad Azlan Qazi",
            "Alexandros Iosifidis",
            "Qi Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Foundation models (FMs) have achieved remarkable success across a wide range of applications, from image classification to natural langurage processing, but pose significant challenges for deployment at edge. This has sparked growing interest in developing practical and efficient strategies for bringing foundation models to edge environments. In this work, we propose PRISM, a communication-efficient and compute-aware strategy for distributed Transformer inference on edge devices. Our method leverages a Segment Means representation to approximate intermediate output features, drastically reducing inter-device communication. Additionally, we restructure the self-attention mechanism to eliminate redundant computations caused by per-device Key/Value calculation in position-wise partitioning and design a partition-aware causal masking scheme tailored for autoregressive models. We evaluate PRISM on ViT, BERT, and GPT-2 across diverse datasets, namely CIFAR-10, CIFAR-100, ImageNet-1k, GLUE, and CBT. Our results demonstrate substantial reductions in communication overhead (up to 99.2% for BERT at compression rate CR = 128) and per-device computation (51.24% for BERT at the same setting), with only minor accuracy degradation. This method offers a scalable and practical solution for deploying foundation models in distributed resource-constrained environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 51,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12165",
        "abs_url": "https://arxiv.org/abs/2507.12165",
        "pdf_url": "https://arxiv.org/pdf/2507.12165",
        "title": "Multi-Component VAE with Gaussian Markov Random Field",
        "authors": [
            "Fouad Oubari",
            "Mohamed El-Baha",
            "Raphael Meunier",
            "Rodrigue Décatoire",
            "Mathilde Mougeot"
        ],
        "comments": "arXiv admin note: substantial text overlap with arXiv:2408.09576",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Multi-component datasets with intricate dependencies, like industrial assemblies or multi-modal imaging, challenge current generative modeling techniques. Existing Multi-component Variational AutoEncoders typically rely on simplified aggregation strategies, neglecting critical nuances and consequently compromising structural coherence across generated components. To explicitly address this gap, we introduce the Gaussian Markov Random Field Multi-Component Variational AutoEncoder , a novel generative framework embedding Gaussian Markov Random Fields into both prior and posterior distributions. This design choice explicitly models cross-component relationships, enabling richer representation and faithful reproduction of complex interactions. Empirically, our GMRF MCVAE achieves state-of-the-art performance on a synthetic Copula dataset specifically constructed to evaluate intricate component relationships, demonstrates competitive results on the PolyMNIST benchmark, and significantly enhances structural coherence on the real-world BIKED dataset. Our results indicate that the GMRF MCVAE is especially suited for practical applications demanding robust and realistic modeling of multi-component coherence",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 52,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12166",
        "abs_url": "https://arxiv.org/abs/2507.12166",
        "pdf_url": "https://arxiv.org/pdf/2507.12166",
        "title": "RadioDiff-3D: A 3D$\\times$3D Radio Map Dataset and Generative Diffusion Based Benchmark for 6G Environment-Aware Communication",
        "authors": [
            "Xiucheng Wang",
            "Qiming Zhang",
            "Nan Cheng",
            "Junting Chen",
            "Zezhong Zhang",
            "Zan Li",
            "Shuguang Cui",
            "Xuemin Shen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Systems and Control (eess.SY)",
        "abstract": "Radio maps (RMs) serve as a critical foundation for enabling environment-aware wireless communication, as they provide the spatial distribution of wireless channel characteristics. Despite recent progress in RM construction using data-driven approaches, most existing methods focus solely on pathloss prediction in a fixed 2D plane, neglecting key parameters such as direction of arrival (DoA), time of arrival (ToA), and vertical spatial variations. Such a limitation is primarily due to the reliance on static learning paradigms, which hinder generalization beyond the training data distribution. To address these challenges, we propose UrbanRadio3D, a large-scale, high-resolution 3D RM dataset constructed via ray tracing in realistic urban environments. UrbanRadio3D is over 37$\\times$3 larger than previous datasets across a 3D space with 3 metrics as pathloss, DoA, and ToA, forming a novel 3D$\\times$33D dataset with 7$\\times$3 more height layers than prior state-of-the-art (SOTA) dataset. To benchmark 3D RM construction, a UNet with 3D convolutional operators is proposed. Moreover, we further introduce RadioDiff-3D, a diffusion-model-based generative framework utilizing the 3D convolutional architecture. RadioDiff-3D supports both radiation-aware scenarios with known transmitter locations and radiation-unaware settings based on sparse spatial observations. Extensive evaluations on UrbanRadio3D validate that RadioDiff-3D achieves superior performance in constructing rich, high-dimensional radio maps under diverse environmental dynamics. This work provides a foundational dataset and benchmark for future research in 3D environment-aware communication. The dataset is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 53,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12192",
        "abs_url": "https://arxiv.org/abs/2507.12192",
        "pdf_url": "https://arxiv.org/pdf/2507.12192",
        "title": "Explainable Evidential Clustering",
        "authors": [
            "Victor F. Lopes de Souza",
            "Karima Bakhti",
            "Sofiane Ramdani",
            "Denis Mottet",
            "Abdelhak Imoussaten"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Unsupervised classification is a fundamental machine learning problem. Real-world data often contain imperfections, characterized by uncertainty and imprecision, which are not well handled by traditional methods. Evidential clustering, based on Dempster-Shafer theory, addresses these challenges. This paper explores the underexplored problem of explaining evidential clustering results, which is crucial for high-stakes domains such as healthcare. Our analysis shows that, in the general case, representativity is a necessary and sufficient condition for decision trees to serve as abductive explainers. Building on the concept of representativity, we generalize this idea to accommodate partial labeling through utility functions. These functions enable the representation of \"tolerable\" mistakes, leading to the definition of evidential mistakeness as explanation cost and the construction of explainers tailored to evidential classifiers. Finally, we propose the Iterative Evidential Mistake Minimization (IEMM) algorithm, which provides interpretable and cautious decision tree explanations for evidential clustering functions. We validate the proposed algorithm on synthetic and real-world data. Taking into account the decision-maker's preferences, we were able to provide an explanation that was satisfactory up to 93% of the time.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 54,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12196",
        "abs_url": "https://arxiv.org/abs/2507.12196",
        "pdf_url": "https://arxiv.org/pdf/2507.12196",
        "title": "Selective Quantization Tuning for ONNX Models",
        "authors": [
            "Nikolaos Louloudakis",
            "Ajitha Rajan"
        ],
        "comments": "5 pages, 3 figures, 2 tables",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Systems and Control (eess.SY)",
        "abstract": "Quantization is a process that reduces the precision of deep neural network models to lower model size and computational demands, often at the cost of accuracy. However, fully quantized models may exhibit sub-optimal performance below acceptable levels and face deployment challenges on low-end hardware accelerators due to practical constraints. To address these issues, quantization can be selectively applied to only a subset of layers, but selecting which layers to exclude is non-trivial. To this direction, we propose TuneQn, a suite enabling selective quantization, deployment and execution of ONNX models across various CPU and GPU devices, combined with profiling and multi-objective optimization. TuneQn generates selectively quantized ONNX models, deploys them on different hardware, measures performance on metrics like accuracy and size, performs Pareto Front minimization to identify the best model candidate and visualizes the results. To demonstrate the effectiveness of TuneQn, we evaluated TuneQn on four ONNX models with two quantization settings across CPU and GPU devices. As a result, we demonstrated that our utility effectively performs selective quantization and tuning, selecting ONNX model candidates with up to a $54.14$% reduction in accuracy loss compared to the fully quantized model, and up to a $72.9$% model size reduction compared to the original model.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 55,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12218",
        "abs_url": "https://arxiv.org/abs/2507.12218",
        "pdf_url": "https://arxiv.org/pdf/2507.12218",
        "title": "Physics-Informed Linear Model (PILM): Analytical Representations and Application to Crustal Strain Rate Estimation",
        "authors": [
            "Tomohisa Okazaki"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Geophysics (physics.geo-ph)",
        "abstract": "Many physical systems are described by partial differential equations (PDEs), and solving these equations and estimating their coefficients or boundary conditions (BCs) from observational data play a crucial role in understanding the associated phenomena. Recently, a machine learning approach known as physics-informed neural network, which solves PDEs using neural networks by minimizing the sum of residuals from the PDEs, BCs, and data, has gained significant attention in the scientific community. In this study, we investigate a physics-informed linear model (PILM) that uses linear combinations of basis functions to represent solutions, thereby enabling an analytical representation of optimal solutions. The PILM was formulated and verified for illustrative forward and inverse problems including cases with uncertain BCs. Furthermore, the PILM was applied to estimate crustal strain rates using geodetic data. Specifically, physical regularization that enforces elastic equilibrium on the velocity fields was compared with mathematical regularization that imposes smoothness constraints. From a Bayesian perspective, mathematical regularization exhibited superior performance. The PILM provides an analytically solvable framework applicable to linear forward and inverse problems, underdetermined systems, and physical regularization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 56,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12224",
        "abs_url": "https://arxiv.org/abs/2507.12224",
        "pdf_url": "https://arxiv.org/pdf/2507.12224",
        "title": "Optimizers Qualitatively Alter Solutions And We Should Leverage This",
        "authors": [
            "Razvan Pascanu",
            "Clare Lyle",
            "Ionut-Vlad Modoranu",
            "Naima Elosegui Borras",
            "Dan Alistarh",
            "Petar Velickovic",
            "Sarath Chandar",
            "Soham De",
            "James Martens"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Due to the nonlinear nature of Deep Neural Networks (DNNs), one can not guarantee convergence to a unique global minimum of the loss when using optimizers relying only on local information, such as SGD. Indeed, this was a primary source of skepticism regarding the feasibility of DNNs in the early days of the field. The past decades of progress in deep learning have revealed this skepticism to be misplaced, and a large body of empirical evidence shows that sufficiently large DNNs following standard training protocols exhibit well-behaved optimization dynamics that converge to performant solutions. This success has biased the community to use convex optimization as a mental model for learning, leading to a focus on training efficiency, either in terms of required iteration, FLOPs or wall-clock time, when improving optimizers. We argue that, while this perspective has proven extremely fruitful, another perspective specific to DNNs has received considerably less attention: the optimizer not only influences the rate of convergence, but also the qualitative properties of the learned solutions. Restated, the optimizer can and will encode inductive biases and change the effective expressivity of a given class of models. Furthermore, we believe the optimizer can be an effective way of encoding desiderata in the learning process. We contend that the community should aim at understanding the biases of already existing methods, as well as aim to build new optimizers with the explicit intent of inducing certain properties of the solution, rather than solely judging them based on their convergence rates. We hope our arguments will inspire research to improve our understanding of how the learning process can impact the type of solution we converge to, and lead to a greater recognition of optimizers design as a critical lever that complements the roles of architecture and data in shaping model outcomes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 57,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12257",
        "abs_url": "https://arxiv.org/abs/2507.12257",
        "pdf_url": "https://arxiv.org/pdf/2507.12257",
        "title": "Robust Causal Discovery in Real-World Time Series with Power-Laws",
        "authors": [
            "Matteo Tusoni",
            "Giuseppe Masi",
            "Andrea Coletta",
            "Aldo Glielmo",
            "Viviana Arrigoni",
            "Novella Bartolini"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Data Analysis, Statistics and Probability (physics.data-an); Machine Learning (stat.ML); Other Statistics (stat.OT)",
        "abstract": "Exploring causal relationships in stochastic time series is a challenging yet crucial task with a vast range of applications, including finance, economics, neuroscience, and climate science. Many algorithms for Causal Discovery (CD) have been proposed, but they often exhibit a high sensitivity to noise, resulting in misleading causal inferences when applied to real data. In this paper, we observe that the frequency spectra of typical real-world time series follow a power-law distribution, notably due to an inherent self-organizing behavior. Leveraging this insight, we build a robust CD method based on the extraction of power -law spectral features that amplify genuine causal signals. Our method consistently outperforms state-of-the-art alternatives on both synthetic benchmarks and real-world datasets with known causal structures, demonstrating its robustness and practical relevance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 58,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12262",
        "abs_url": "https://arxiv.org/abs/2507.12262",
        "pdf_url": "https://arxiv.org/pdf/2507.12262",
        "title": "A Framework for Nonstationary Gaussian Processes with Neural Network Parameters",
        "authors": [
            "Zachary James",
            "Joseph Guinness"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Methodology (stat.ME); Machine Learning (stat.ML)",
        "abstract": "Gaussian processes have become a popular tool for nonparametric regression because of their flexibility and uncertainty quantification. However, they often use stationary kernels, which limit the expressiveness of the model and may be unsuitable for many datasets. We propose a framework that uses nonstationary kernels whose parameters vary across the feature space, modeling these parameters as the output of a neural network that takes the features as input. The neural network and Gaussian process are trained jointly using the chain rule to calculate derivatives. Our method clearly describes the behavior of the nonstationary parameters and is compatible with approximation methods for scaling to large datasets. It is flexible and easily adapts to different nonstationary kernels without needing to redesign the optimization procedure. Our methods are implemented with the GPyTorch library and can be readily modified. We test a nonstationary variance and noise variant of our method on several machine learning datasets and find that it achieves better accuracy and log-score than both a stationary model and a hierarchical model approximated with variational inference. Similar results are observed for a model with only nonstationary variance. We also demonstrate our approach's ability to recover the nonstationary parameters of a spatial dataset.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 59,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12297",
        "abs_url": "https://arxiv.org/abs/2507.12297",
        "pdf_url": "https://arxiv.org/pdf/2507.12297",
        "title": "RegCL: Continual Adaptation of Segment Anything Model via Model Merging",
        "authors": [
            "Yuan-Chen Shu",
            "Zhiwei Lin",
            "Yongtao Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "To address the performance limitations of the Segment Anything Model (SAM) in specific domains, existing works primarily adopt adapter-based one-step adaptation paradigms. However, some of these methods are specific developed for specific domains. If used on other domains may lead to performance degradation. This issue of catastrophic forgetting severely limits the model's scalability. To address this issue, this paper proposes RegCL, a novel non-replay continual learning (CL) framework designed for efficient multi-domain knowledge integration through model merging. Specifically, RegCL incorporates the model merging algorithm into the continual learning paradigm by merging the parameters of SAM's adaptation modules (e.g., LoRA modules) trained on different domains. The merging process is guided by weight optimization, which minimizes prediction discrepancies between the merged model and each of the domain-specific models. RegCL effectively consolidates multi-domain knowledge while maintaining parameter efficiency, i.e., the model size remains constant regardless of the number of tasks, and no historical data storage is required. Experimental results demonstrate that RegCL achieves favorable continual learning performance across multiple downstream datasets, validating its effectiveness in dynamic scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 60,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12305",
        "abs_url": "https://arxiv.org/abs/2507.12305",
        "pdf_url": "https://arxiv.org/pdf/2507.12305",
        "title": "PROL : Rehearsal Free Continual Learning in Streaming Data via Prompt Online Learning",
        "authors": [
            "M. Anwar Ma'sum",
            "Mahardhika Pratama",
            "Savitha Ramasamy",
            "Lin Liu",
            "Habibullah Habibullah",
            "Ryszard Kowalczyk"
        ],
        "comments": "ICCV 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The data privacy constraint in online continual learning (OCL), where the data can be seen only once, complicates the catastrophic forgetting problem in streaming data. A common approach applied by the current SOTAs in OCL is with the use of memory saving exemplars or features from previous classes to be replayed in the current task. On the other hand, the prompt-based approach performs excellently in continual learning but with the cost of a growing number of trainable parameters. The first approach may not be applicable in practice due to data openness policy, while the second approach has the issue of throughput associated with the streaming data. In this study, we propose a novel prompt-based method for online continual learning that includes 4 main components: (1) single light-weight prompt generator as a general knowledge, (2) trainable scaler-and-shifter as specific knowledge, (3) pre-trained model (PTM) generalization preserving, and (4) hard-soft updates mechanism. Our proposed method achieves significantly higher performance than the current SOTAs in CIFAR100, ImageNet-R, ImageNet-A, and CUB dataset. Our complexity analysis shows that our method requires a relatively smaller number of parameters and achieves moderate training time, inference time, and throughput. For further study, the source code of our method is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 61,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12314",
        "abs_url": "https://arxiv.org/abs/2507.12314",
        "pdf_url": "https://arxiv.org/pdf/2507.12314",
        "title": "Thought Purity: Defense Paradigm For Chain-of-Thought Attack",
        "authors": [
            "Zihao Xue",
            "Zhen Bi",
            "Long Ma",
            "Zhenlin Hu",
            "Yan Wang",
            "Zhenfang Liu",
            "Qing Sheng",
            "Jie Xiao",
            "Jungang Lou"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Cryptography and Security (cs.CR)",
        "abstract": "While reinforcement learning-trained Large Reasoning Models (LRMs, e.g., Deepseek-R1) demonstrate advanced reasoning capabilities in the evolving Large Language Models (LLMs) domain, their susceptibility to security threats remains a critical vulnerability. This weakness is particularly evident in Chain-of-Thought (CoT) generation processes, where adversarial methods like backdoor prompt attacks can systematically subvert the model's core reasoning mechanisms. The emerging Chain-of-Thought Attack (CoTA) reveals this vulnerability through exploiting prompt controllability, simultaneously degrading both CoT safety and task performance with low-cost interventions. To address this compounded security-performance vulnerability, we propose Thought Purity (TP): a defense paradigm that systematically strengthens resistance to malicious content while preserving operational efficacy. Our solution achieves this through three synergistic components: (1) a safety-optimized data processing pipeline (2) reinforcement learning-enhanced rule constraints (3) adaptive monitoring metrics. Our approach establishes the first comprehensive defense mechanism against CoTA vulnerabilities in reinforcement learning-aligned reasoning systems, significantly advancing the security-functionality equilibrium for next-generation AI architectures.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 62,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12341",
        "abs_url": "https://arxiv.org/abs/2507.12341",
        "pdf_url": "https://arxiv.org/pdf/2507.12341",
        "title": "Nonlinear Concept Erasure: a Density Matching Approach",
        "authors": [
            "Antoine Saillenfest",
            "Pirmin Lemberger"
        ],
        "comments": "17 pages, 10 figures, accepted for publication in ECAI 2025 (28th European Conference on Artificial Intelligence)",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
        "abstract": "Ensuring that neural models used in real-world applications cannot infer sensitive information, such as demographic attributes like gender or race, from text representations is a critical challenge when fairness is a concern. We address this issue through concept erasure, a process that removes information related to a specific concept from distributed representations while preserving as much of the remaining semantic information as possible. Our approach involves learning an orthogonal projection in the embedding space, designed to make the class-conditional feature distributions of the discrete concept to erase indistinguishable after projection. By adjusting the rank of the projector, we control the extent of information removal, while its orthogonality ensures strict preservation of the local structure of the embeddings. Our method, termed $\\overline{\\mathrm{L}}$EOPARD, achieves state-of-the-art performance in nonlinear erasure of a discrete attribute on classic natural language processing benchmarks. Furthermore, we demonstrate that $\\overline{\\mathrm{L}}$EOPARD effectively mitigates bias in deep nonlinear classifiers, thereby promoting fairness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 63,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12380",
        "abs_url": "https://arxiv.org/abs/2507.12380",
        "pdf_url": "https://arxiv.org/pdf/2507.12380",
        "title": "Heat Kernel Goes Topological",
        "authors": [
            "Maximilian Krahn",
            "Vikas Garg"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Topological neural networks have emerged as powerful successors of graph neural networks. However, they typically involve higher-order message passing, which incurs significant computational expense. We circumvent this issue with a novel topological framework that introduces a Laplacian operator on combinatorial complexes (CCs), enabling efficient computation of heat kernels that serve as node descriptors. Our approach captures multiscale information and enables permutation-equivariant representations, allowing easy integration into modern transformer-based architectures. Theoretically, the proposed method is maximally expressive because it can distinguish arbitrary non-isomorphic CCs. Empirically, it significantly outperforms existing topological methods in terms of computational efficiency. Besides demonstrating competitive performance with the state-of-the-art descriptors on standard molecular datasets, it exhibits superior capability in distinguishing complex topological structures and avoiding blind spots on topological benchmarks. Overall, this work advances topological deep learning by providing expressive yet scalable representations, thereby opening up exciting avenues for molecular classification and property prediction tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 64,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12383",
        "abs_url": "https://arxiv.org/abs/2507.12383",
        "pdf_url": "https://arxiv.org/pdf/2507.12383",
        "title": "Improving Reinforcement Learning Sample-Efficiency using Local Approximation",
        "authors": [
            "Mohit Prashant",
            "Arvind Easwaran"
        ],
        "comments": "Preprint",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In this study, we derive Probably Approximately Correct (PAC) bounds on the asymptotic sample-complexity for RL within the infinite-horizon Markov Decision Process (MDP) setting that are sharper than those in existing literature. The premise of our study is twofold: firstly, the further two states are from each other, transition-wise, the less relevant the value of the first state is when learning the $\\epsilon$-optimal value of the second; secondly, the amount of 'effort', sample-complexity-wise, expended in learning the $\\epsilon$-optimal value of a state is independent of the number of samples required to learn the $\\epsilon$-optimal value of a second state that is a sufficient number of transitions away from the first. Inversely, states within each other's vicinity have values that are dependent on each other and will require a similar number of samples to learn. By approximating the original MDP using smaller MDPs constructed using subsets of the original's state-space, we are able to reduce the sample-complexity by a logarithmic factor to $O(SA \\log A)$ timesteps, where $S$ and $A$ are the state and action space sizes. We are able to extend these results to an infinite-horizon, model-free setting by constructing a PAC-MDP algorithm with the aforementioned sample-complexity. We conclude with showing how significant the improvement is by comparing our algorithm against prior work in an experimental setting.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 65,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12384",
        "abs_url": "https://arxiv.org/abs/2507.12384",
        "pdf_url": "https://arxiv.org/pdf/2507.12384",
        "title": "Trustworthy Tree-based Machine Learning by $MoS_2$ Flash-based Analog CAM with Inherent Soft Boundaries",
        "authors": [
            "Bo Wen",
            "Guoyun Gao",
            "Zhicheng Xu",
            "Ruibin Mao",
            "Xiaojuan Qi",
            "X. Sharon Hu",
            "Xunzhao Yin",
            "Can Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Emerging Technologies (cs.ET)",
        "abstract": "The rapid advancement of artificial intelligence has raised concerns regarding its trustworthiness, especially in terms of interpretability and robustness. Tree-based models like Random Forest and XGBoost excel in interpretability and accuracy for tabular data, but scaling them remains computationally expensive due to poor data locality and high data dependence. Previous efforts to accelerate these models with analog content addressable memory (CAM) have struggled, due to the fact that the difficult-to-implement sharp decision boundaries are highly susceptible to device variations, which leads to poor hardware performance and vulnerability to adversarial attacks. This work presents a novel hardware-software co-design approach using $MoS_2$ Flash-based analog CAM with inherent soft boundaries, enabling efficient inference with soft tree-based models. Our soft tree model inference experiments on $MoS_2$ analog CAM arrays show this method achieves exceptional robustness against device variation and adversarial attacks while achieving state-of-the-art accuracy. Specifically, our fabricated analog CAM arrays achieve $96\\%$ accuracy on Wisconsin Diagnostic Breast Cancer (WDBC) database, while maintaining decision explainability. Our experimentally calibrated model validated only a $0.6\\%$ accuracy drop on the MNIST dataset under $10\\%$ device threshold variation, compared to a $45.3\\%$ drop for traditional decision trees. This work paves the way for specialized hardware that enhances AI's trustworthiness and efficiency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 66,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12399",
        "abs_url": "https://arxiv.org/abs/2507.12399",
        "pdf_url": "https://arxiv.org/pdf/2507.12399",
        "title": "ROC-n-reroll: How verifier imperfection affects test-time scaling",
        "authors": [
            "Florian E. Dorner",
            "Yatong Chen",
            "André F. Cruz",
            "Fanny Yang"
        ],
        "comments": "35 pages, 9 Figures",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Test-time scaling aims to improve language model performance by leveraging additional compute during inference. While many works have empirically studied techniques like Best-of-N (BoN) and rejection sampling that make use of a verifier to enable test-time scaling, there is little theoretical understanding of how verifier imperfection affects performance. In this work, we address this gap. Specifically, we prove how instance-level accuracy of these methods is precisely characterized by the geometry of the verifier's ROC curve. Interestingly, while scaling is determined by the local geometry of the ROC curve for rejection sampling, it depends on global properties of the ROC curve for BoN. As a consequence when the ROC curve is unknown, it is impossible to extrapolate the performance of rejection sampling based on the low-compute regime. Furthermore, while rejection sampling outperforms BoN for fixed compute, in the infinite-compute limit both methods converge to the same level of accuracy, determined by the slope of the ROC curve near the origin. Our theoretical results are confirmed by experiments on GSM8K using different versions of Llama and Qwen to generate and verify solutions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 67,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12412",
        "abs_url": "https://arxiv.org/abs/2507.12412",
        "pdf_url": "https://arxiv.org/pdf/2507.12412",
        "title": "NOCTA: Non-Greedy Objective Cost-Tradeoff Acquisition for Longitudinal Data",
        "authors": [
            "Dzung Dinh",
            "Boqi Chen",
            "Marc Niethammer",
            "Junier Oliva"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "In many critical applications, resource constraints limit the amount of information that can be gathered to make predictions. For example, in healthcare, patient data often spans diverse features ranging from lab tests to imaging studies. Each feature may carry different information and must be acquired at a respective cost of time, money, or risk to the patient. Moreover, temporal prediction tasks, where both instance features and labels evolve over time, introduce additional complexity in deciding when or what information is important. In this work, we propose NOCTA, a Non-Greedy Objective Cost-Tradeoff Acquisition method that sequentially acquires the most informative features at inference time while accounting for both temporal dynamics and acquisition cost. We first introduce a cohesive estimation target for our NOCTA setting, and then develop two complementary estimators: 1) a non-parametric method based on nearest neighbors to guide the acquisition (NOCTA-NP), and 2) a parametric method that directly predicts the utility of potential acquisitions (NOCTA-P). Experiments on synthetic and real-world medical datasets demonstrate that both NOCTA variants outperform existing baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 68,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12419",
        "abs_url": "https://arxiv.org/abs/2507.12419",
        "pdf_url": "https://arxiv.org/pdf/2507.12419",
        "title": "Mixture of Raytraced Experts",
        "authors": [
            "Andrea Perin",
            "Giacomo Lagomarsini",
            "Claudio Gallicchio",
            "Giuseppe Nuti"
        ],
        "comments": "Preliminary version (pre-submission)",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "We introduce a Mixture of Raytraced Experts, a stacked Mixture of Experts (MoE) architecture which can dynamically select sequences of experts, producing computational graphs of variable width and depth. Existing MoE architectures generally require a fixed amount of computation for a given sample. Our approach, in contrast, yields predictions with increasing accuracy as the computation cycles through the experts' sequence. We train our model by iteratively sampling from a set of candidate experts, unfolding the sequence akin to how Recurrent Neural Networks are trained. Our method does not require load-balancing mechanisms, and preliminary experiments show a reduction in training epochs of 10\\% to 40\\% with a comparable/higher accuracy. These results point to new research directions in the field of MoEs, allowing the design of potentially faster and more expressive models. The code is available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 69,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12435",
        "abs_url": "https://arxiv.org/abs/2507.12435",
        "pdf_url": "https://arxiv.org/pdf/2507.12435",
        "title": "Targeted Deep Architectures: A TMLE-Based Framework for Robust Causal Inference in Neural Networks",
        "authors": [
            "Yi Li",
            "David Mccoy",
            "Nolan Gunter",
            "Kaitlyn Lee",
            "Alejandro Schuler",
            "Mark van der Laan"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Modern deep neural networks are powerful predictive tools yet often lack valid inference for causal parameters, such as treatment effects or entire survival curves. While frameworks like Double Machine Learning (DML) and Targeted Maximum Likelihood Estimation (TMLE) can debias machine-learning fits, existing neural implementations either rely on \"targeted losses\" that do not guarantee solving the efficient influence function equation or computationally expensive post-hoc \"fluctuations\" for multi-parameter settings. We propose Targeted Deep Architectures (TDA), a new framework that embeds TMLE directly into the network's parameter space with no restrictions on the backbone architecture. Specifically, TDA partitions model parameters - freezing all but a small \"targeting\" subset - and iteratively updates them along a targeting gradient, derived from projecting the influence functions onto the span of the gradients of the loss with respect to weights. This procedure yields plug-in estimates that remove first-order bias and produce asymptotically valid confidence intervals. Crucially, TDA easily extends to multi-dimensional causal estimands (e.g., entire survival curves) by merging separate targeting gradients into a single universal targeting update. Theoretically, TDA inherits classical TMLE properties, including double robustness and semiparametric efficiency. Empirically, on the benchmark IHDP dataset (average treatment effects) and simulated survival data with informative censoring, TDA reduces bias and improves coverage relative to both standard neural-network estimators and prior post-hoc approaches. In doing so, TDA establishes a direct, scalable pathway toward rigorous causal inference within modern deep architectures for complex multi-parameter targets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 70,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12439",
        "abs_url": "https://arxiv.org/abs/2507.12439",
        "pdf_url": "https://arxiv.org/pdf/2507.12439",
        "title": "A Bayesian Incentive Mechanism for Poison-Resilient Federated Learning",
        "authors": [
            "Daniel Commey",
            "Rebecca A. Sarpong",
            "Griffith S. Klogo",
            "Winful Bagyl-Bac",
            "Garth V. Crosby"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Cryptography and Security (cs.CR); Computer Science and Game Theory (cs.GT)",
        "abstract": "Federated learning (FL) enables collaborative model training across decentralized clients while preserving data privacy. However, its open-participation nature exposes it to data-poisoning attacks, in which malicious actors submit corrupted model updates to degrade the global model. Existing defenses are often reactive, relying on statistical aggregation rules that can be computationally expensive and that typically assume an honest majority. This paper introduces a proactive, economic defense: a lightweight Bayesian incentive mechanism that makes malicious behavior economically irrational. Each training round is modeled as a Bayesian game of incomplete information in which the server, acting as the principal, uses a small, private validation dataset to verify update quality before issuing payments. The design satisfies Individual Rationality (IR) for benevolent clients, ensuring their participation is profitable, and Incentive Compatibility (IC), making poisoning an economically dominated strategy. Extensive experiments on non-IID partitions of MNIST and FashionMNIST demonstrate robustness: with 50% label-flipping adversaries on MNIST, the mechanism maintains 96.7% accuracy, only 0.3 percentage points lower than in a scenario with 30% label-flipping adversaries. This outcome is 51.7 percentage points better than standard FedAvg, which collapses under the same 50% attack. The mechanism is computationally light, budget-bounded, and readily integrates into existing FL frameworks, offering a practical route to economically robust and sustainable FL ecosystems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 71,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12453",
        "abs_url": "https://arxiv.org/abs/2507.12453",
        "pdf_url": "https://arxiv.org/pdf/2507.12453",
        "title": "Cost-aware Stopping for Bayesian Optimization",
        "authors": [
            "Qian Xie",
            "Linda Cai",
            "Alexander Terenin",
            "Peter I. Frazier",
            "Ziv Scully"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In automated machine learning, scientific discovery, and other applications of Bayesian optimization, deciding when to stop evaluating expensive black-box functions is an important practical consideration. While several adaptive stopping rules have been proposed, in the cost-aware setting they lack guarantees ensuring they stop before incurring excessive function evaluation costs. We propose a cost-aware stopping rule for Bayesian optimization that adapts to varying evaluation costs and is free of heuristic tuning. Our rule is grounded in a theoretical connection to state-of-the-art cost-aware acquisition functions, namely the Pandora's Box Gittins Index (PBGI) and log expected improvement per cost. We prove a theoretical guarantee bounding the expected cumulative evaluation cost incurred by our stopping rule when paired with these two acquisition functions. In experiments on synthetic and empirical tasks, including hyperparameter optimization and neural architecture size search, we show that combining our stopping rule with the PBGI acquisition function consistently matches or outperforms other acquisition-function--stopping-rule pairs in terms of cost-adjusted simple regret, a metric capturing trade-offs between solution quality and cumulative evaluation cost.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 72,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11544",
        "abs_url": "https://arxiv.org/abs/2507.11544",
        "pdf_url": "https://arxiv.org/pdf/2507.11544",
        "title": "The Safety Gap Toolkit: Evaluating Hidden Dangers of Open-Source Models",
        "authors": [
            "Ann-Kathrin Dombrowski",
            "Dillon Bowen",
            "Adam Gleave",
            "Chris Cundy"
        ],
        "comments": "9 pages plus appendix",
        "subjects": "Computers and Society (cs.CY); Machine Learning (cs.LG)",
        "abstract": "Open-weight large language models (LLMs) unlock huge benefits in innovation, personalization, privacy, and democratization. However, their core advantage - modifiability - opens the door to systemic risks: bad actors can trivially subvert current safeguards, turning beneficial models into tools for harm. This leads to a 'safety gap': the difference in dangerous capabilities between a model with intact safeguards and one that has been stripped of those safeguards. We open-source a toolkit to estimate the safety gap for state-of-the-art open-weight models. As a case study, we evaluate biochemical and cyber capabilities, refusal rates, and generation quality of models from two families (Llama-3 and Qwen-2.5) across a range of parameter scales (0.5B to 405B) using different safeguard removal techniques. Our experiments reveal that the safety gap widens as model scale increases and effective dangerous capabilities grow substantially when safeguards are removed. We hope that the Safety Gap Toolkit (this https URL) will serve as an evaluation framework for common open-source models and as a motivation for developing and testing tamper-resistant safeguards. We welcome contributions to the toolkit from the community.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 73,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11579",
        "abs_url": "https://arxiv.org/abs/2507.11579",
        "pdf_url": "https://arxiv.org/pdf/2507.11579",
        "title": "SketchDNN: Joint Continuous-Discrete Diffusion for CAD Sketch Generation",
        "authors": [
            "Sathvik Chereddy",
            "John Femiani"
        ],
        "comments": "17 pages, 63 figures, Proceedings of the 42nd International Conference on Machine Learning (ICML2025)",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "We present SketchDNN, a generative model for synthesizing CAD sketches that jointly models both continuous parameters and discrete class labels through a unified continuous-discrete diffusion process. Our core innovation is Gaussian-Softmax diffusion, where logits perturbed with Gaussian noise are projected onto the probability simplex via a softmax transformation, facilitating blended class labels for discrete variables. This formulation addresses 2 key challenges, namely, the heterogeneity of primitive parameterizations and the permutation invariance of primitives in CAD sketches. Our approach significantly improves generation quality, reducing Fréchet Inception Distance (FID) from 16.04 to 7.80 and negative log-likelihood (NLL) from 84.8 to 81.33, establishing a new state-of-the-art in CAD sketch generation on the SketchGraphs dataset.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 74,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11588",
        "abs_url": "https://arxiv.org/abs/2507.11588",
        "pdf_url": "https://arxiv.org/pdf/2507.11588",
        "title": "SToFM: a Multi-scale Foundation Model for Spatial Transcriptomics",
        "authors": [
            "Suyuan Zhao",
            "Yizhen Luo",
            "Ganbo Yang",
            "Yan Zhong",
            "Hao Zhou",
            "Zaiqing Nie"
        ],
        "comments": "Accpeted by ICML 2024",
        "subjects": "Genomics (q-bio.GN); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Spatial Transcriptomics (ST) technologies provide biologists with rich insights into single-cell biology by preserving spatial context of cells. Building foundational models for ST can significantly enhance the analysis of vast and complex data sources, unlocking new perspectives on the intricacies of biological tissues. However, modeling ST data is inherently challenging due to the need to extract multi-scale information from tissue slices containing vast numbers of cells. This process requires integrating macro-scale tissue morphology, micro-scale cellular microenvironment, and gene-scale gene expression profile. To address this challenge, we propose SToFM, a multi-scale Spatial Transcriptomics Foundation Model. SToFM first performs multi-scale information extraction on each ST slice, to construct a set of ST sub-slices that aggregate macro-, micro- and gene-scale information. Then an SE(2) Transformer is used to obtain high-quality cell representations from the sub-slices. Additionally, we construct \\textbf{SToCorpus-88M}, the largest high-resolution spatial transcriptomics corpus for pretraining. SToFM achieves outstanding performance on a variety of downstream tasks, such as tissue region semantic segmentation and cell type annotation, demonstrating its comprehensive understanding of ST data",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 75,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11623",
        "abs_url": "https://arxiv.org/abs/2507.11623",
        "pdf_url": "https://arxiv.org/pdf/2507.11623",
        "title": "A Roadmap for Climate-Relevant Robotics Research",
        "authors": [
            "Alan Papalia",
            "Charles Dawson",
            "Laurentiu L. Anton",
            "Norhan Magdy Bayomi",
            "Bianca Champenois",
            "Jung-Hoon Cho",
            "Levi Cai",
            "Joseph DelPreto",
            "Kristen Edwards",
            "Bilha-Catherine Githinji",
            "Cameron Hickert",
            "Vindula Jayawardana",
            "Matthew Kramer",
            "Shreyaa Raghavan",
            "David Russell",
            "Shide Salimi",
            "Jingnan Shi",
            "Soumya Sudhakar",
            "Yanwei Wang",
            "Shouyi Wang",
            "Luca Carlone",
            "Vijay Kumar",
            "Daniela Rus",
            "John E. Fernandez",
            "Cathy Wu",
            "George Kantor",
            "Derek Young",
            "Hanumant Singh"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Systems and Control (eess.SY)",
        "abstract": "Climate change is one of the defining challenges of the 21st century, and many in the robotics community are looking for ways to contribute. This paper presents a roadmap for climate-relevant robotics research, identifying high-impact opportunities for collaboration between roboticists and experts across climate domains such as energy, the built environment, transportation, industry, land use, and Earth sciences. These applications include problems such as energy systems optimization, construction, precision agriculture, building envelope retrofits, autonomous trucking, and large-scale environmental monitoring. Critically, we include opportunities to apply not only physical robots but also the broader robotics toolkit - including planning, perception, control, and estimation algorithms - to climate-relevant problems. A central goal of this roadmap is to inspire new research directions and collaboration by highlighting specific, actionable problems at the intersection of robotics and climate. This work represents a collaboration between robotics researchers and domain experts in various climate disciplines, and it serves as an invitation to the robotics community to bring their expertise to bear on urgent climate priorities.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 76,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11625",
        "abs_url": "https://arxiv.org/abs/2507.11625",
        "pdf_url": "https://arxiv.org/pdf/2507.11625",
        "title": "MapIQ: Benchmarking Multimodal Large Language Models for Map Question Answering",
        "authors": [
            "Varun Srivastava",
            "Fan Lei",
            "Srija Mukhopadhyay",
            "Vivek Gupta",
            "Ross Maciejewski"
        ],
        "comments": "Published as a conference paper at COLM 2025",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Recent advancements in multimodal large language models (MLLMs) have driven researchers to explore how well these models read data visualizations, e.g., bar charts, scatter plots. More recently, attention has shifted to visual question answering with maps (Map-VQA). However, Map-VQA research has primarily focused on choropleth maps, which cover only a limited range of thematic categories and visual analytical tasks. To address these gaps, we introduce MapIQ, a benchmark dataset comprising 14,706 question-answer pairs across three map types: choropleth maps, cartograms, and proportional symbol maps spanning topics from six distinct themes (e.g., housing, crime). We evaluate multiple MLLMs using six visual analytical tasks, comparing their performance against one another and a human baseline. An additional experiment examining the impact of map design changes (e.g., altered color schemes, modified legend designs, and removal of map elements) provides insights into the robustness and sensitivity of MLLMs, their reliance on internal geographic knowledge, and potential avenues for improving Map-VQA performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 77,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11636",
        "abs_url": "https://arxiv.org/abs/2507.11636",
        "pdf_url": "https://arxiv.org/pdf/2507.11636",
        "title": "JSQA: Speech Quality Assessment with Perceptually-Inspired Contrastive Pretraining Based on JND Audio Pairs",
        "authors": [
            "Junyi Fan",
            "Donald Williamson"
        ],
        "comments": "Accepted to WASPAA 2025",
        "subjects": "Audio and Speech Processing (eess.AS); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Speech quality assessment (SQA) is often used to learn a mapping from a high-dimensional input space to a scalar that represents the mean opinion score (MOS) of the perceptual speech quality. Learning such a mapping is challenging for many reasons, but largely because MOS exhibits high levels of inherent variance due to perceptual and experimental-design differences. Many solutions have been proposed, but many approaches do not properly incorporate perceptual factors into their learning algorithms (beyond the MOS label), which could lead to unsatisfactory results. To this end, we propose JSQA, a two-stage framework that pretrains an audio encoder using perceptually-guided contrastive learning on just noticeable difference (JND) pairs, followed by fine-tuning for MOS prediction. We first generate pairs of audio data within JND levels, which are then used to pretrain an encoder to leverage perceptual quality similarity information and map it into an embedding space. The JND pairs come from clean LibriSpeech utterances that are mixed with background noise from CHiME-3, at different signal-to-noise ratios (SNRs). The encoder is later fine-tuned with audio samples from the NISQA dataset for MOS prediction. Experimental results suggest that perceptually-inspired contrastive pretraining significantly improves the model performance evaluated by various metrics when compared against the same network trained from scratch without pretraining. These findings suggest that incorporating perceptual factors into pretraining greatly contributes to the improvement in performance for SQA.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 78,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11638",
        "abs_url": "https://arxiv.org/abs/2507.11638",
        "pdf_url": "https://arxiv.org/pdf/2507.11638",
        "title": "Interpretable Prediction of Lymph Node Metastasis in Rectal Cancer MRI Using Variational Autoencoders",
        "authors": [
            "Benjamin Keel",
            "Aaron Quyn",
            "David Jayne",
            "Maryam Mohsin",
            "Samuel D. Relton"
        ],
        "comments": "Published in Medical Image Understanding and Analysis (MIUA) 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Effective treatment for rectal cancer relies on accurate lymph node metastasis (LNM) staging. However, radiological criteria based on lymph node (LN) size, shape and texture morphology have limited diagnostic accuracy. In this work, we investigate applying a Variational Autoencoder (VAE) as a feature encoder model to replace the large pre-trained Convolutional Neural Network (CNN) used in existing approaches. The motivation for using a VAE is that the generative model aims to reconstruct the images, so it directly encodes visual features and meaningful patterns across the data. This leads to a disentangled and structured latent space which can be more interpretable than a CNN. Models are deployed on an in-house MRI dataset with 168 patients who did not undergo neo-adjuvant treatment. The post-operative pathological N stage was used as the ground truth to evaluate model predictions. Our proposed model 'VAE-MLP' achieved state-of-the-art performance on the MRI dataset, with cross-validated metrics of AUC 0.86 +/- 0.05, Sensitivity 0.79 +/- 0.06, and Specificity 0.85 +/- 0.05. Code is available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 79,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11642",
        "abs_url": "https://arxiv.org/abs/2507.11642",
        "pdf_url": "https://arxiv.org/pdf/2507.11642",
        "title": "Posture-Driven Action Intent Inference for Playing style and Fatigue Assessment",
        "authors": [
            "Abhishek Jaiswal",
            "Nisheeth Srivastava"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Posture-based mental state inference has significant potential in diagnosing fatigue, preventing injury, and enhancing performance across various domains. Such tools must be research-validated with large datasets before being translated into practice. Unfortunately, such vision diagnosis faces serious challenges due to the sensitivity of human subject data. To address this, we identify sports settings as a viable alternative for accumulating data from human subjects experiencing diverse emotional states. We test our hypothesis in the game of cricket and present a posture-based solution to identify human intent from activity videos. Our method achieves over 75\\% F1 score and over 80\\% AUC-ROC in discriminating aggressive and defensive shot intent through motion analysis. These findings indicate that posture leaks out strong signals for intent inference, even with inherent noise in the data pipeline. Furthermore, we utilize existing data statistics as weak supervision to validate our findings, offering a potential solution for overcoming data labelling limitations. This research contributes to generalizable techniques for sports analytics and also opens possibilities for applying human behavior analysis across various fields.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 80,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11662",
        "abs_url": "https://arxiv.org/abs/2507.11662",
        "pdf_url": "https://arxiv.org/pdf/2507.11662",
        "title": "Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification",
        "authors": [
            "Moises Andrade",
            "Joonhyuk Cha",
            "Brandon Ho",
            "Vriksha Srihari",
            "Karmesh Yadav",
            "Zsolt Kira"
        ],
        "comments": "Our code and data are publicly available at this https URL",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG); Multiagent Systems (cs.MA); Robotics (cs.RO)",
        "abstract": "Verifiers -- functions assigning rewards to agent behavior -- have been key for AI progress in domains like math and board games. However, extending these gains to domains without clear-cut success criteria (e.g.,computer use) remains a challenge: while humans can recognize suitable outcomes, translating this intuition into scalable rules is non-trivial. Multimodal Large Language Models(MLLMs) emerge as a promising solution, given their world knowledge, human-preference alignment, and reasoning skills. We evaluate MLLMs as verifiers of agent trajectories across web navigation, computer use, and robotic manipulation, and identify a critical limitation: agreement bias, a strong tendency for MLLMs to favor information in their context window, often generating chains of thought to rationalize flawed behavior. This bias is pervasive across models, resilient to test-time scaling, and can impact several methods using MLLMs as evaluators (e.g.,data filtering). Notably, it occurs despite MLLMs showing strong, human-aligned priors on desired behavior. To address this, we propose Self-Grounded Verification (SGV), a lightweight method that enables more effective use of MLLMs' knowledge and reasoning by harnessing their own sampling mechanisms via unconditional and conditional generation. SGV operates in two steps: first, the MLLM is elicited to retrieve broad priors about task completion, independent of the data under evaluation. Then, conditioned on self-generated priors, it reasons over and evaluates a candidate trajectory. Enhanced with SGV, MLLM verifiers show gains of up to 20 points in accuracy and failure detection rates, and can perform real-time supervision of heterogeneous agents, boosting task completion of a GUI specialist in OSWorld, a diffusion policy in robomimic, and a ReAct agent in VisualWebArena -- setting a new state of the art on the benchmark, surpassing the previous best by 48%.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 81,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11683",
        "abs_url": "https://arxiv.org/abs/2507.11683",
        "pdf_url": "https://arxiv.org/pdf/2507.11683",
        "title": "PGT-I: Scaling Spatiotemporal GNNs with Memory-Efficient Distributed Training",
        "authors": [
            "Seth Ockerman",
            "Amal Gueroudji",
            "Tanwi Mallick",
            "Yixuan He",
            "Line Pouchard",
            "Robert Ross",
            "Shivaram Venkataraman"
        ],
        "comments": "To be published in the 2025 International Conference for High Performance Computing, Networking, Storage, and Analysis",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Spatiotemporal graph neural networks (ST-GNNs) are powerful tools for modeling spatial and temporal data dependencies. However, their applications have been limited primarily to small-scale datasets because of memory constraints. While distributed training offers a solution, current frameworks lack support for spatiotemporal models and overlook the properties of spatiotemporal data. Informed by a scaling study on a large-scale workload, we present PyTorch Geometric Temporal Index (PGT-I), an extension to PyTorch Geometric Temporal that integrates distributed data parallel training and two novel strategies: index-batching and distributed-index-batching. Our index techniques exploit spatiotemporal structure to construct snapshots dynamically at runtime, significantly reducing memory overhead, while distributed-index-batching extends this approach by enabling scalable processing across multiple GPUs. Our techniques enable the first-ever training of an ST-GNN on the entire PeMS dataset without graph partitioning, reducing peak memory usage by up to 89\\% and achieving up to a 13.1x speedup over standard DDP with 128 GPUs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 82,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11687",
        "abs_url": "https://arxiv.org/abs/2507.11687",
        "pdf_url": "https://arxiv.org/pdf/2507.11687",
        "title": "MetaLint: Generalizable Idiomatic Code Quality Analysis through Instruction-Following and Easy-to-Hard Generalization",
        "authors": [
            "Atharva Naik",
            "Lawanya Baghel",
            "Dhakshin Govindarajan",
            "Darsh Agrawal",
            "Daniel Fried",
            "Carolyn Rose"
        ],
        "comments": "",
        "subjects": "Software Engineering (cs.SE); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Large Language Models, though successful in code generation, struggle with code quality analysis because they are limited by static training data and can't easily adapt to evolving best practices. We introduce MetaLint, a new instruction-following framework that formulates code quality analysis as the task of detecting and fixing problematic semantic code fragments or code idioms based on high-level specifications. Unlike conventional approaches that train models on static, rule-based data, MetaLint employs instruction tuning on synthetic linter-generated data to support easy-to-hard generalization, enabling models to adapt to novel or complex code patterns without retraining. To evaluate this, we construct a benchmark of challenging idioms inspired by real-world coding standards such as Python Enhancement Proposals (PEPs) and assess whether MetaLint-trained models reason adaptively or simply memorize. Our results show that MetaLint improves generalization to unseen PEP idioms, achieving a 70.37% F-score on idiom detection with the highest recall (70.43%) among all evaluated models. It also achieves 26.73% on localization, competitive for its 4B parameter size and comparable to larger state-of-the-art models like o3-mini, highlighting its potential for future-proof code quality analysis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 83,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11692",
        "abs_url": "https://arxiv.org/abs/2507.11692",
        "pdf_url": "https://arxiv.org/pdf/2507.11692",
        "title": "Galaxy image simplification using Generative AI",
        "authors": [
            "Sai Teja Erukude",
            "Lior Shamir"
        ],
        "comments": "Astronomy and Computing, accepted",
        "subjects": "Astrophysics of Galaxies (astro-ph.GA); Instrumentation and Methods for Astrophysics (astro-ph.IM); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Modern digital sky surveys have been acquiring images of billions of galaxies. While these images often provide sufficient details to analyze the shape of the galaxies, accurate analysis of such high volumes of images requires effective automation. Current solutions often rely on machine learning annotation of the galaxy images based on a set of pre-defined classes. Here we introduce a new approach to galaxy image analysis that is based on generative AI. The method simplifies the galaxy images and automatically converts them into a ``skeletonized\" form. The simplified images allow accurate measurements of the galaxy shapes and analysis that is not limited to a certain pre-defined set of classes. We demonstrate the method by applying it to galaxy images acquired by the DESI Legacy Survey. The code and data are publicly available. The method was applied to 125,000 DESI Legacy Survey images, and the catalog of the simplified images is publicly available.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 84,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11768",
        "abs_url": "https://arxiv.org/abs/2507.11768",
        "pdf_url": "https://arxiv.org/pdf/2507.11768",
        "title": "LLMs are Bayesian, in Expectation, not in Realization",
        "authors": [
            "Leon Chlon",
            "Sarah Rashidi",
            "Zein Khamis",
            "MarcAntonio M. Awada"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Large language models demonstrate remarkable in-context learning capabilities, adapting to new tasks without parameter updates. While this phenomenon has been successfully modeled as implicit Bayesian inference, recent empirical findings reveal a fundamental contradiction: transformers systematically violate the martingale property, a cornerstone requirement of Bayesian updating on exchangeable data. This violation challenges the theoretical foundations underlying uncertainty quantification in critical applications. Our theoretical analysis establishes four key results: (1) positional encodings induce martingale violations of order $\\Theta(\\log n / n)$; (2) transformers achieve information-theoretic optimality with excess risk $O(n^{-1/2})$ in expectation over orderings; (3) the implicit posterior representation converges to the true Bayesian posterior in the space of sufficient statistics; and (4) we derive the optimal chain-of-thought length as $k^* = \\Theta(\\sqrt{n}\\log(1/\\varepsilon))$ with explicit constants, providing a principled approach to reduce inference costs while maintaining performance. Empirical validation on GPT-3 confirms predictions (1)-(3), with transformers reaching 99\\% of theoretical entropy limits within 20 examples. Our framework provides practical methods for extracting calibrated uncertainty estimates from position-aware architectures and optimizing computational efficiency in deployment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 85,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11780",
        "abs_url": "https://arxiv.org/abs/2507.11780",
        "pdf_url": "https://arxiv.org/pdf/2507.11780",
        "title": "Inference on Optimal Policy Values and Other Irregular Functionals via Smoothing",
        "authors": [
            "Justin Whitehouse",
            "Morgane Austern",
            "Vasilis Syrgkanis"
        ],
        "comments": "40 pages, 2 figures",
        "subjects": "Econometrics (econ.EM); Machine Learning (cs.LG); Statistics Theory (math.ST); Methodology (stat.ME)",
        "abstract": "Constructing confidence intervals for the value of an optimal treatment policy is an important problem in causal inference. Insight into the optimal policy value can guide the development of reward-maximizing, individualized treatment regimes. However, because the functional that defines the optimal value is non-differentiable, standard semi-parametric approaches for performing inference fail to be directly applicable. Existing approaches for handling this non-differentiability fall roughly into two camps. In one camp are estimators based on constructing smooth approximations of the optimal value. These approaches are computationally lightweight, but typically place unrealistic parametric assumptions on outcome regressions. In another camp are approaches that directly de-bias the non-smooth objective. These approaches don't place parametric assumptions on nuisance functions, but they either require the computation of intractably-many nuisance estimates, assume unrealistic $L^\\infty$ nuisance convergence rates, or make strong margin assumptions that prohibit non-response to a treatment. In this paper, we revisit the problem of constructing smooth approximations of non-differentiable functionals. By carefully controlling first-order bias and second-order remainders, we show that a softmax smoothing-based estimator can be used to estimate parameters that are specified as a maximum of scores involving nuisance components. In particular, this includes the value of the optimal treatment policy as a special case. Our estimator obtains $\\sqrt{n}$ convergence rates, avoids parametric restrictions/unrealistic margin assumptions, and is often statistically efficient.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 86,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11783",
        "abs_url": "https://arxiv.org/abs/2507.11783",
        "pdf_url": "https://arxiv.org/pdf/2507.11783",
        "title": "Foundation Models for Brain Signals: A Critical Review of Current Progress and Future Directions",
        "authors": [
            "Gayal Kuruppu",
            "Neeraj Wagh",
            "Yogatheesan Varatharajah"
        ],
        "comments": "20 pages, 5 figures, 2 tables",
        "subjects": "Signal Processing (eess.SP); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)",
        "abstract": "Patterns of electrical brain activity recorded via electroencephalography (EEG) offer immense value for scientific and clinical investigations. The inability of supervised EEG encoders to learn robust EEG patterns and their over-reliance on expensive signal annotations have sparked a transition towards general-purpose self-supervised EEG encoders, i.e., EEG foundation models (EEG-FMs), for robust and scalable EEG feature extraction. However, the real-world readiness of early EEG-FMs and the rubric for long-term research progress remain unclear. A systematic and comprehensive review of first-generation EEG-FMs is therefore necessary to understand the current state-of-the-art and identify key directions for future EEG-FMs. To that end, this study reviews 10 early EEG-FMs and presents a critical synthesis of their methodology, empirical findings, and outstanding research gaps. We find that most EEG-FMs adopt a sequence-based modeling scheme that relies on transformer-based backbones and the reconstruction of masked sequences for self-supervision. However, model evaluations remain heterogeneous and largely limited, making it challenging to assess their practical off-the-shelf utility. In addition to adopting standardized and realistic evaluations, future work should demonstrate more substantial scaling effects and make principled and trustworthy choices throughout the EEG representation learning pipeline. We believe that developing benchmarks, software tools, technical methodologies, and applications in collaboration with domain experts may further advance the translational utility and real-world adoption of EEG-FMs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 87,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11806",
        "abs_url": "https://arxiv.org/abs/2507.11806",
        "pdf_url": "https://arxiv.org/pdf/2507.11806",
        "title": "MOFSimBench: Evaluating Universal Machine Learning Interatomic Potentials In Metal--Organic Framework Molecular Modeling",
        "authors": [
            "Hendrik Kraß",
            "Ju Huang",
            "Seyed Mohamad Moosavi"
        ],
        "comments": "",
        "subjects": "Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG); Computational Physics (physics.comp-ph)",
        "abstract": "Universal machine learning interatomic potentials (uMLIPs) have emerged as powerful tools for accelerating atomistic simulations, offering scalable and efficient modeling with accuracy close to quantum calculations. However, their reliability and effectiveness in practical, real-world applications remain an open question. Metal-organic frameworks (MOFs) and related nanoporous materials are highly porous crystals with critical relevance in carbon capture, energy storage, and catalysis applications. Modeling nanoporous materials presents distinct challenges for uMLIPs due to their diverse chemistry, structural complexity, including porosity and coordination bonds, and the absence from existing training datasets. Here, we introduce MOFSimBench, a benchmark to evaluate uMLIPs on key materials modeling tasks for nanoporous materials, including structural optimization, molecular dynamics (MD) stability, the prediction of bulk properties, such as bulk modulus and heat capacity, and guest-host interactions. Evaluating over 20 models from various architectures on a chemically and structurally diverse materials set, we find that top-performing uMLIPs consistently outperform classical force fields and fine-tuned machine learning potentials across all tasks, demonstrating their readiness for deployment in nanoporous materials modeling. Our analysis highlights that data quality, particularly the diversity of training sets and inclusion of out-of-equilibrium conformations, plays a more critical role than model architecture in determining performance across all evaluated uMLIPs. We release our modular and extendable benchmarking framework at this https URL, providing an open resource to guide the adoption for nanoporous materials modeling and further development of uMLIPs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 88,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11809",
        "abs_url": "https://arxiv.org/abs/2507.11809",
        "pdf_url": "https://arxiv.org/pdf/2507.11809",
        "title": "Tracing Facts or just Copies? A critical investigation of the Competitions of Mechanisms in Large Language Models",
        "authors": [
            "Dante Campregher",
            "Yanxu Chen",
            "Sander Hoffman",
            "Maria Heuss"
        ],
        "comments": "18 Pages, 13 figures",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "This paper presents a reproducibility study examining how Large Language Models (LLMs) manage competing factual and counterfactual information, focusing on the role of attention heads in this process. We attempt to reproduce and reconcile findings from three recent studies by Ortu et al., Yu, Merullo, and Pavlick and McDougall et al. that investigate the competition between model-learned facts and contradictory context information through Mechanistic Interpretability tools. Our study specifically examines the relationship between attention head strength and factual output ratios, evaluates competing hypotheses about attention heads' suppression mechanisms, and investigates the domain specificity of these attention patterns. Our findings suggest that attention heads promoting factual output do so via general copy suppression rather than selective counterfactual suppression, as strengthening them can also inhibit correct facts. Additionally, we show that attention head behavior is domain-dependent, with larger models exhibiting more specialized and category-sensitive patterns.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 89,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11830",
        "abs_url": "https://arxiv.org/abs/2507.11830",
        "pdf_url": "https://arxiv.org/pdf/2507.11830",
        "title": "Arctic Inference with Shift Parallelism: Fast and Efficient Open Source Inference System for Enterprise AI",
        "authors": [
            "Samyam Rajbhandari",
            "Mert Hidayetoglu",
            "Aurick Qiao",
            "Ye Wang",
            "Juncheng Yang",
            "Jeff Rasley",
            "Michael Wyatt",
            "Yuxiong He"
        ],
        "comments": "",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)",
        "abstract": "Inference is now the dominant AI workload, yet existing systems force trade-offs between latency, throughput, and cost. Arctic Inference, an open-source vLLM plugin from Snowflake AI Research, introduces Shift Parallelism, a dynamic parallelism strategy that adapts to real-world traffic while integrating speculative decoding, SwiftKV compute reduction, and optimized embedding inference. It achieves up to 3.4 times faster request completion, 1.75 times faster generation, and 1.6M tokens/sec per GPU for embeddings, outperforming both latency- and throughput-optimized deployments. Already powering Snowflake Cortex AI, Arctic Inference delivers state-of-the-art, cost-effective inference for enterprise AI and is now available to the community.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 90,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11842",
        "abs_url": "https://arxiv.org/abs/2507.11842",
        "pdf_url": "https://arxiv.org/pdf/2507.11842",
        "title": "CosmoFlow: Scale-Aware Representation Learning for Cosmology with Flow Matching",
        "authors": [
            "Sidharth Kannan",
            "Tian Qiu",
            "Carolina Cuesta-Lazaro",
            "Haewon Jeong"
        ],
        "comments": "",
        "subjects": "Cosmology and Nongalactic Astrophysics (astro-ph.CO); Machine Learning (cs.LG)",
        "abstract": "Generative machine learning models have been demonstrated to be able to learn low dimensional representations of data that preserve information required for downstream tasks. In this work, we demonstrate that flow matching based generative models can learn compact, semantically rich latent representations of field level cold dark matter (CDM) simulation data without supervision. Our model, CosmoFlow, learns representations 32x smaller than the raw field data, usable for field level reconstruction, synthetic data generation, and parameter inference. Our model also learns interpretable representations, in which different latent channels correspond to features at different cosmological scales.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 91,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11851",
        "abs_url": "https://arxiv.org/abs/2507.11851",
        "pdf_url": "https://arxiv.org/pdf/2507.11851",
        "title": "Your LLM Knows the Future: Uncovering Its Multi-Token Prediction Potential",
        "authors": [
            "Mohammad Samragh",
            "Arnav Kundu",
            "David Harrison",
            "Kumari Nishu",
            "Devang Naik",
            "Minsik Cho",
            "Mehrdad Farajtabar"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Autoregressive language models are constrained by their inherently sequential nature, generating one token at a time. This paradigm limits inference speed and parallelism, especially during later stages of generation when the direction and semantics of text are relatively certain. In this work, we propose a novel framework that leverages the inherent knowledge of vanilla autoregressive language models about future tokens, combining techniques to realize this potential and enable simultaneous prediction of multiple subsequent tokens. Our approach introduces several key innovations: (1) a masked-input formulation where multiple future tokens are jointly predicted from a common prefix; (2) a gated LoRA formulation that preserves the original LLM's functionality, while equipping it for multi-token prediction; (3) a lightweight, learnable sampler module that generates coherent sequences from the predicted future tokens; (4) a set of auxiliary training losses, including a consistency loss, to enhance the coherence and accuracy of jointly generated tokens; and (5) a speculative generation strategy that expands tokens quadratically in the future while maintaining high fidelity. Our method achieves significant speedups through supervised fine-tuning on pretrained models. For example, it generates code and math nearly 5x faster, and improves general chat and knowledge tasks by almost 2.5x. These gains come without any loss in quality.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 92,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11891",
        "abs_url": "https://arxiv.org/abs/2507.11891",
        "pdf_url": "https://arxiv.org/pdf/2507.11891",
        "title": "Choosing the Better Bandit Algorithm under Data Sharing: When Do A/B Experiments Work?",
        "authors": [
            "Shuangning Li",
            "Chonghuan Wang",
            "Jingyan Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Statistics Theory (math.ST)",
        "abstract": "We study A/B experiments that are designed to compare the performance of two recommendation algorithms. Prior work has shown that the standard difference-in-means estimator is biased in estimating the global treatment effect (GTE) due to a particular form of interference between experimental units. Specifically, units under the treatment and control algorithms contribute to a shared pool of data that subsequently train both algorithms, resulting in interference between the two groups. The bias arising from this type of data sharing is known as \"symbiosis bias\". In this paper, we highlight that, for decision-making purposes, the sign of the GTE often matters more than its precise magnitude when selecting the better algorithm. We formalize this insight under a multi-armed bandit framework and theoretically characterize when the sign of the expected GTE estimate under data sharing aligns with or contradicts the sign of the true GTE. Our analysis identifies the level of exploration versus exploitation as a key determinant of how symbiosis bias impacts algorithm selection.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 93,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11895",
        "abs_url": "https://arxiv.org/abs/2507.11895",
        "pdf_url": "https://arxiv.org/pdf/2507.11895",
        "title": "Newfluence: Boosting Model interpretability and Understanding in High Dimensions",
        "authors": [
            "Haolin Zou",
            "Arnab Auddy",
            "Yongchan Kwon",
            "Kamiar Rahnama Rad",
            "Arian Maleki"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Methodology (stat.ME)",
        "abstract": "The increasing complexity of machine learning (ML) and artificial intelligence (AI) models has created a pressing need for tools that help scientists, engineers, and policymakers interpret and refine model decisions and predictions. Influence functions, originating from robust statistics, have emerged as a popular approach for this purpose. However, the heuristic foundations of influence functions rely on low-dimensional assumptions where the number of parameters $p$ is much smaller than the number of observations $n$. In contrast, modern AI models often operate in high-dimensional regimes with large $p$, challenging these assumptions. In this paper, we examine the accuracy of influence functions in high-dimensional settings. Our theoretical and empirical analyses reveal that influence functions cannot reliably fulfill their intended purpose. We then introduce an alternative approximation, called Newfluence, that maintains similar computational efficiency while offering significantly improved accuracy. Newfluence is expected to provide more accurate insights than many existing methods for interpreting complex AI models and diagnosing their issues. Moreover, the high-dimensional framework we develop in this paper can also be applied to analyze other popular techniques, such as Shapley values.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 94,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11911",
        "abs_url": "https://arxiv.org/abs/2507.11911",
        "pdf_url": "https://arxiv.org/pdf/2507.11911",
        "title": "AFPM: Alignment-based Frame Patch Modeling for Cross-Dataset EEG Decoding",
        "authors": [
            "Xiaoqing Chen",
            "Siyang Li",
            "Dongrui Wu"
        ],
        "comments": "",
        "subjects": "Human-Computer Interaction (cs.HC); Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "Electroencephalogram (EEG) decoding models for brain-computer interfaces (BCIs) struggle with cross-dataset learning and generalization due to channel layout inconsistencies, non-stationary signal distributions, and limited neurophysiological prior integration. To address these issues, we propose a plug-and-play Alignment-Based Frame-Patch Modeling (AFPM) framework, which has two main components: 1) Spatial Alignment, which selects task-relevant channels based on brain-region priors, aligns EEG distributions across domains, and remaps the selected channels to a unified layout; and, 2) Frame-Patch Encoding, which models multi-dataset signals into unified spatiotemporal patches for EEG decoding. Compared to 17 state-of-the-art approaches that need dataset-specific tuning, the proposed calibration-free AFPM achieves performance gains of up to 4.40% on motor imagery and 3.58% on event-related potential tasks. To our knowledge, this is the first calibration-free cross-dataset EEG decoding framework, substantially enhancing the practicalness of BCIs in real-world applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 95,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11936",
        "abs_url": "https://arxiv.org/abs/2507.11936",
        "pdf_url": "https://arxiv.org/pdf/2507.11936",
        "title": "A Survey of Deep Learning for Geometry Problem Solving",
        "authors": [
            "Jianzhe Ma",
            "Wenxuan Wang",
            "Qin Jin"
        ],
        "comments": "Work in progress",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Geometry problem solving is a key area of mathematical reasoning, which is widely involved in many important fields such as education, mathematical ability assessment of artificial intelligence, and multimodal ability assessment. In recent years, the rapid development of deep learning technology, especially the rise of multimodal large language models, has triggered a widespread research boom. This paper provides a survey of the applications of deep learning in geometry problem solving, including (i) a comprehensive summary of the relevant tasks in geometry problem solving; (ii) a thorough review of related deep learning methods; (iii) a detailed analysis of evaluation metrics and methods; and (iv) a critical discussion of the current challenges and future directions that can be explored. Our goal is to provide a comprehensive and practical reference of deep learning for geometry problem solving to promote further developments in this field. We create a continuously updated list of papers on GitHub: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 96,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11950",
        "abs_url": "https://arxiv.org/abs/2507.11950",
        "pdf_url": "https://arxiv.org/pdf/2507.11950",
        "title": "RNAMunin: A Deep Machine Learning Model for Non-coding RNA Discovery",
        "authors": [
            "Lauren Lui",
            "Torben Nielsen"
        ],
        "comments": "",
        "subjects": "Genomics (q-bio.GN); Machine Learning (cs.LG)",
        "abstract": "Functional annotation of microbial genomes is often biased toward protein-coding genes, leaving a vast, unexplored landscape of non-coding RNAs (ncRNAs) that are critical for regulating bacterial and archaeal physiology, stress response and metabolism. Identifying ncRNAs directly from genomic sequence is a paramount challenge in bioinformatics and biology, essential for understanding the complete regulatory potential of an organism. This paper presents RNAMunin, a machine learning (ML) model that is capable of finding ncRNAs using genomic sequence alone. It is also computationally viable for large sequence datasets such as long read metagenomic assemblies with contigs totaling multiple Gbp. RNAMunin is trained on Rfam sequences extracted from approximately 60 Gbp of long read metagenomes from 16 San Francisco Estuary samples. We know of no other model that can detect ncRNAs based solely on genomic sequence at this scale. Since RNAMunin only requires genomic sequence as input, we do not need for an ncRNA to be transcribed to find it, i.e., we do not need transcriptomics data. We wrote this manuscript in a narrative style in order to best convey how RNAMunin was developed and how it works in detail. Unlike almost all current ML models, at approximately 1M parameters, RNAMunin is very small and very fast.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 97,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11953",
        "abs_url": "https://arxiv.org/abs/2507.11953",
        "pdf_url": "https://arxiv.org/pdf/2507.11953",
        "title": "IAM: Efficient Inference through Attention Mapping between Different-scale LLMs",
        "authors": [
            "Yi Zhao",
            "Zuchao Li",
            "Hai Zhao"
        ],
        "comments": "ACL 2025",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "LLMs encounter significant challenges in resource consumption nowadays, especially with long contexts. Despite extensive efforts dedicate to enhancing inference efficiency, these methods primarily exploit internal sparsity within the models, without leveraging external information for optimization. We identify the high similarity of attention matrices across different-scale LLMs, which offers a novel perspective for optimization. We first conduct a comprehensive analysis of how to measure similarity, how to select mapping Layers and whether mapping is consistency. Based on these insights, we introduce the IAM framework, which achieves dual benefits of accelerated attention computation and reduced KV cache usage by performing attention mapping between small and large LLMs. Our experimental results demonstrate that IAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without appreciably sacrificing performance. Experiments on different series of models show the generalizability of IAM. Importantly, it is also orthogonal to many existing KV cache optimization methods, making it a versatile addition to the current toolkit for enhancing LLM efficiency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 98,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11954",
        "abs_url": "https://arxiv.org/abs/2507.11954",
        "pdf_url": "https://arxiv.org/pdf/2507.11954",
        "title": "The benefits of query-based KGQA systems for complex and temporal questions in LLM era",
        "authors": [
            "Artem Alekseev",
            "Mikhail Chaichuk",
            "Miron Butko",
            "Alexander Panchenko",
            "Elena Tutubalina",
            "Oleg Somov"
        ],
        "comments": "15 pages, 3 figures, 7 tables",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Large language models excel in question-answering (QA) yet still struggle with multi-hop reasoning and temporal questions. Query-based knowledge graph QA (KGQA) offers a modular alternative by generating executable queries instead of direct answers. We explore multi-stage query-based framework for WikiData QA, proposing multi-stage approach that enhances performance on challenging multi-hop and temporal benchmarks. Through generalization and rejection studies, we evaluate robustness across multi-hop and temporal QA datasets. Additionally, we introduce a novel entity linking and predicate matching method using CoT reasoning. Our results demonstrate the potential of query-based multi-stage KGQA framework for improving multi-hop and temporal QA with small language models. Code and data: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 99,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11959",
        "abs_url": "https://arxiv.org/abs/2507.11959",
        "pdf_url": "https://arxiv.org/pdf/2507.11959",
        "title": "PoTPTQ: A Two-step Power-of-Two Post-training for LLMs",
        "authors": [
            "Xinyu Wang",
            "Vahid Partovi Nia",
            "Peng Lu",
            "Jerry Huang",
            "Xiao-Wen Chang",
            "Boxing Chen",
            "Yufei Cui"
        ],
        "comments": "Accepted at ECAI 2025 (European Conference on Artificial Intelligence)",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across various natural language processing (NLP) tasks. However, their deployment is challenging due to the substantial computational resources required. Power-of-two (PoT) quantization is a general tool to counteract this difficulty. Albeit previous works on PoT quantization can be efficiently dequantized on CPUs using fixed-point addition, it showed less effectiveness on GPUs. The reason is entanglement of the sign bit and sequential bit manipulations needed for dequantization. We propose a novel POT quantization framework for LLM weights that (i) outperforms state-of-the-art accuracy in extremely low-precision number formats, and (ii) enables faster inference through more efficient dequantization. To maintain the accuracy of the quantized model, we introduce a two-step post-training algorithm: (i) initialize the quantization scales with a robust starting point, and (ii) refine these scales using a minimal calibration set. The performance of our PoT post-training algorithm surpasses the current state-of-the-art in integer quantization, particularly at low precisions such as 2- and 3-bit formats. Our PoT quantization accelerates the dequantization step required for the floating point inference and leads to $3.67\\times$ speed up on a NVIDIA V100, and $1.63\\times$ on a NVIDIA RTX 4090, compared to uniform integer dequantization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 100,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11960",
        "abs_url": "https://arxiv.org/abs/2507.11960",
        "pdf_url": "https://arxiv.org/pdf/2507.11960",
        "title": "d-DQIVAR: Data-centric Visual Analytics and Reasoning for Data Quality Improvement",
        "authors": [
            "Hyein Hong",
            "Sangbong Yoo",
            "SeokHwan Choi",
            "Jisue Kim",
            "Seongbum Seo",
            "Haneol Cho",
            "Chansoo Kim",
            "Yun Jang"
        ],
        "comments": "",
        "subjects": "Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)",
        "abstract": "Approaches to enhancing data quality (DQ) are classified into two main categories: data- and process-driven. However, prior research has predominantly utilized batch data preprocessing within the data-driven framework, which often proves insufficient for optimizing machine learning (ML) model performance and frequently leads to distortions in data characteristics. Existing studies have primarily focused on data preprocessing rather than genuine data quality improvement (DQI). In this paper, we introduce d-DQIVAR, a novel visual analytics system designed to facilitate DQI strategies aimed at improving ML model performance. Our system integrates visual analytics techniques that leverage both data-driven and process-driven approaches. Data-driven techniques tackle DQ issues such as imputation, outlier detection, deletion, format standardization, removal of duplicate records, and feature selection. Process-driven strategies encompass evaluating DQ and DQI procedures by considering DQ dimensions and ML model performance and applying the Kolmogorov-Smirnov test. We illustrate how our system empowers users to harness expert and domain knowledge effectively within a practical workflow through case studies, evaluations, and user studies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 101,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11977",
        "abs_url": "https://arxiv.org/abs/2507.11977",
        "pdf_url": "https://arxiv.org/pdf/2507.11977",
        "title": "Recent results on searches with boosted Higgs bosons at CMS",
        "authors": [
            "Farouk Mokhtar"
        ],
        "comments": "6 pages, 3 figures, The Thirteenth Annual Large Hadron Collider Physics (LHCP2025)",
        "subjects": "High Energy Physics - Experiment (hep-ex); Machine Learning (cs.LG)",
        "abstract": "The study of boosted Higgs bosons at the LHC provides a unique window to probe Higgs boson couplings at high energy scales and search for signs of physics beyond the standard model. In these proceedings, we present recent results on boosted Higgs boson searches at the CMS experiment, highlighting innovative reconstruction and tagging techniques that enhance sensitivity in this challenging regime.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 102,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.11984",
        "abs_url": "https://arxiv.org/abs/2507.11984",
        "pdf_url": "https://arxiv.org/pdf/2507.11984",
        "title": "Dataset-Adaptive Dimensionality Reduction",
        "authors": [
            "Hyeon Jeon",
            "Jeongin Park",
            "Soohyun Lee",
            "Dae Hyun Kim",
            "Sungbok Shin",
            "Jinwook Seo"
        ],
        "comments": "IEEE VIS 2025 & IEEE Transactions on Visualization and Computer Graphics (TVCG)",
        "subjects": "Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)",
        "abstract": "Selecting the appropriate dimensionality reduction (DR) technique and determining its optimal hyperparameter settings that maximize the accuracy of the output projections typically involves extensive trial and error, often resulting in unnecessary computational overhead. To address this challenge, we propose a dataset-adaptive approach to DR optimization guided by structural complexity metrics. These metrics quantify the intrinsic complexity of a dataset, predicting whether higher-dimensional spaces are necessary to represent it accurately. Since complex datasets are often inaccurately represented in two-dimensional projections, leveraging these metrics enables us to predict the maximum achievable accuracy of DR techniques for a given dataset, eliminating redundant trials in optimizing DR. We introduce the design and theoretical foundations of these structural complexity metrics. We quantitatively verify that our metrics effectively approximate the ground truth complexity of datasets and confirm their suitability for guiding dataset-adaptive DR workflow. Finally, we empirically show that our dataset-adaptive workflow significantly enhances the efficiency of DR optimization without compromising accuracy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 103,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12003",
        "abs_url": "https://arxiv.org/abs/2507.12003",
        "pdf_url": "https://arxiv.org/pdf/2507.12003",
        "title": "Expanding ML-Documentation Standards For Better Security",
        "authors": [
            "Cara Ellen Appel"
        ],
        "comments": "Accepted for publication at the 33rd IEEE International Requirements Engineering Workshop (REW 2025)",
        "subjects": "Cryptography and Security (cs.CR); Machine Learning (cs.LG); Software Engineering (cs.SE)",
        "abstract": "This article presents the current state of ML-security and of the documentation of ML-based systems, models and datasets in research and practice based on an extensive review of the existing literature. It shows a generally low awareness of security aspects among ML-practitioners and organizations and an often unstandardized approach to documentation, leading to overall low quality of ML-documentation. Existing standards are not regularly adopted in practice and IT-security aspects are often not included in documentation. Due to these factors, there is a clear need for improved security documentation in ML, as one step towards addressing the existing gaps in ML-security. To achieve this, we propose expanding existing documentation standards for ML-documentation to include a security section with specific security relevant information. Implementing this, a novel expanded method of documenting security requirements in ML-documentation is presented, based on the existing Model Cards and Datasheets for Datasets standards, but with the recommendation to adopt these findings in all ML-documentation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 104,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12021",
        "abs_url": "https://arxiv.org/abs/2507.12021",
        "pdf_url": "https://arxiv.org/pdf/2507.12021",
        "title": "Incorporating Fairness Constraints into Archetypal Analysis",
        "authors": [
            "Aleix Alcacer",
            "Irene Epifanio"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Archetypal Analysis (AA) is an unsupervised learning method that represents data as convex combinations of extreme patterns called archetypes. While AA provides interpretable and low-dimensional representations, it can inadvertently encode sensitive attributes, leading to fairness concerns. In this work, we propose Fair Archetypal Analysis (FairAA), a modified formulation that explicitly reduces the influence of sensitive group information in the learned projections. We also introduce FairKernelAA, a nonlinear extension that addresses fairness in more complex data distributions. Our approach incorporates a fairness regularization term while preserving the structure and interpretability of the archetypes. We evaluate FairAA and FairKernelAA on synthetic datasets, including linear, nonlinear, and multi-group scenarios, demonstrating their ability to reduce group separability -- as measured by mean maximum discrepancy and linear separability -- without substantially compromising explained variance. We further validate our methods on the real-world ANSUR I dataset, confirming their robustness and practical utility. The results show that FairAA achieves a favorable trade-off between utility and fairness, making it a promising tool for responsible representation learning in sensitive applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 105,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12023",
        "abs_url": "https://arxiv.org/abs/2507.12023",
        "pdf_url": "https://arxiv.org/pdf/2507.12023",
        "title": "MVAR: MultiVariate AutoRegressive Air Pollutants Forecasting Model",
        "authors": [
            "Xu Fan",
            "Zhihao Wang",
            "Yuetan Lin",
            "Yan Zhang",
            "Yang Xiang",
            "Hao Li"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Air pollutants pose a significant threat to the environment and human health, thus forecasting accurate pollutant concentrations is essential for pollution warnings and policy-making. Existing studies predominantly focus on single-pollutant forecasting, neglecting the interactions among different pollutants and their diverse spatial responses. To address the practical needs of forecasting multivariate air pollutants, we propose MultiVariate AutoRegressive air pollutants forecasting model (MVAR), which reduces the dependency on long-time-window inputs and boosts the data utilization efficiency. We also design the Multivariate Autoregressive Training Paradigm, enabling MVAR to achieve 120-hour long-term sequential forecasting. Additionally, MVAR develops Meteorological Coupled Spatial Transformer block, enabling the flexible coupling of AI-based meteorological forecasts while learning the interactions among pollutants and their diverse spatial responses. As for the lack of standardized datasets in air pollutants forecasting, we construct a comprehensive dataset covering 6 major pollutants across 75 cities in North China from 2018 to 2023, including ERA5 reanalysis data and FuXi-2.0 forecast data. Experimental results demonstrate that the proposed model outperforms state-of-the-art methods and validate the effectiveness of the proposed architecture.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 106,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12064",
        "abs_url": "https://arxiv.org/abs/2507.12064",
        "pdf_url": "https://arxiv.org/pdf/2507.12064",
        "title": "StylOch at PAN: Gradient-Boosted Trees with Frequency-Based Stylometric Features",
        "authors": [
            "Jeremi K. Ochab",
            "Mateusz Matias",
            "Tymoteusz Boba",
            "Tomasz Walkowiak"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "This submission to the binary AI detection task is based on a modular stylometric pipeline, where: public spaCy models are used for text preprocessing (including tokenisation, named entity recognition, dependency parsing, part-of-speech tagging, and morphology annotation) and extracting several thousand features (frequencies of n-grams of the above linguistic annotations); light-gradient boosting machines are used as the classifier. We collect a large corpus of more than 500 000 machine-generated texts for the classifier's training. We explore several parameter options to increase the classifier's capacity and take advantage of that training set. Our approach follows the non-neural, computationally inexpensive but explainable approach found effective previously.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 107,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12091",
        "abs_url": "https://arxiv.org/abs/2507.12091",
        "pdf_url": "https://arxiv.org/pdf/2507.12091",
        "title": "Improved Analysis for Sign-based Methods with Momentum Updates",
        "authors": [
            "Wei Jiang",
            "Dingzhi Yu",
            "Sifan Yang",
            "Wenhao Yang",
            "Lijun Zhang"
        ],
        "comments": "",
        "subjects": "Optimization and Control (math.OC); Machine Learning (cs.LG)",
        "abstract": "In this paper, we present enhanced analysis for sign-based optimization algorithms with momentum updates. Traditional sign-based methods, under the separable smoothness assumption, guarantee a convergence rate of $\\mathcal{O}(T^{-1/4})$, but they either require large batch sizes or assume unimodal symmetric stochastic noise. To address these limitations, we demonstrate that signSGD with momentum can achieve the same convergence rate using constant batch sizes without additional assumptions. Our analysis, under the standard $l_2$-smoothness condition, improves upon the result of the prior momentum-based signSGD method by a factor of $\\mathcal{O}(d^{1/2})$, where $d$ is the problem dimension. Furthermore, we explore sign-based methods with majority vote in distributed settings and show that the proposed momentum-based method yields convergence rates of $\\mathcal{O}\\left( d^{1/2}T^{-1/2} + dn^{-1/2} \\right)$ and $\\mathcal{O}\\left( \\max \\{ d^{1/4}T^{-1/4}, d^{1/10}T^{-1/5} \\} \\right)$, which outperform the previous results of $\\mathcal{O}\\left( dT^{-1/4} + dn^{-1/2} \\right)$ and $\\mathcal{O}\\left( d^{3/8}T^{-1/8} \\right)$, respectively. Numerical experiments further validate the effectiveness of the proposed methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 108,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12098",
        "abs_url": "https://arxiv.org/abs/2507.12098",
        "pdf_url": "https://arxiv.org/pdf/2507.12098",
        "title": "A Privacy-Preserving Framework for Advertising Personalization Incorporating Federated Learning and Differential Privacy",
        "authors": [
            "Xiang Li",
            "Yifan Lin",
            "Yuanzhe Zhang"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
        "abstract": "To mitigate privacy leakage and performance issues in personalized advertising, this paper proposes a framework that integrates federated learning and differential privacy. The system combines distributed feature extraction, dynamic privacy budget allocation, and robust model aggregation to balance model accuracy, communication overhead, and privacy protection. Multi-party secure computing and anomaly detection mechanisms further enhance system resilience against malicious attacks. Experimental results demonstrate that the framework achieves dual optimization of recommendation accuracy and system efficiency while ensuring privacy, providing both a practical solution and a theoretical foundation for applying privacy protection technologies in advertisement recommendation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 109,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12108",
        "abs_url": "https://arxiv.org/abs/2507.12108",
        "pdf_url": "https://arxiv.org/pdf/2507.12108",
        "title": "Multimodal Coordinated Online Behavior: Trade-offs and Strategies",
        "authors": [
            "Lorenzo Mannocci",
            "Stefano Cresci",
            "Matteo Magnani",
            "Anna Monreale",
            "Maurizio Tesconi"
        ],
        "comments": "",
        "subjects": "Social and Information Networks (cs.SI); Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)",
        "abstract": "Coordinated online behavior, which spans from beneficial collective actions to harmful manipulation such as disinformation campaigns, has become a key focus in digital ecosystem analysis. Traditional methods often rely on monomodal approaches, focusing on single types of interactions like co-retweets or co-hashtags, or consider multiple modalities independently of each other. However, these approaches may overlook the complex dynamics inherent in multimodal coordination. This study compares different ways of operationalizing the detection of multimodal coordinated behavior. It examines the trade-off between weakly and strongly integrated multimodal models, highlighting the balance between capturing broader coordination patterns and identifying tightly coordinated behavior. By comparing monomodal and multimodal approaches, we assess the unique contributions of different data modalities and explore how varying implementations of multimodality impact detection outcomes. Our findings reveal that not all the modalities provide distinct insights, but that with a multimodal approach we can get a more comprehensive understanding of coordination dynamics. This work enhances the ability to detect and analyze coordinated online behavior, offering new perspectives for safeguarding the integrity of digital platforms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 110,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12126",
        "abs_url": "https://arxiv.org/abs/2507.12126",
        "pdf_url": "https://arxiv.org/pdf/2507.12126",
        "title": "Iterative Augmentation with Summarization Refinement (IASR) Evaluation for Unstructured Survey data Modeling and Analysis",
        "authors": [
            "Payal Bhattad",
            "Sai Manoj Pudukotai Dinakarrao",
            "Anju Gupta"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Text data augmentation is a widely used strategy for mitigating data sparsity in natural language processing (NLP), particularly in low-resource settings where limited samples hinder effective semantic modeling. While augmentation can improve input diversity and downstream interpretability, existing techniques often lack mechanisms to ensure semantic preservation during large-scale or iterative generation, leading to redundancy and instability. This work introduces a principled evaluation framework for large language model (LLM) based text augmentation, comprising two components: (1) Scalability Analysis, which measures semantic consistency as augmentation volume increases, and (2) Iterative Augmentation with Summarization Refinement (IASR), which evaluates semantic drift across recursive paraphrasing cycles. Empirical evaluations across state-of-the-art LLMs show that GPT-3.5 Turbo achieved the best balance of semantic fidelity, diversity, and generation efficiency. Applied to a real-world topic modeling task using BERTopic with GPT-enhanced few-shot labeling, the proposed approach results in a 400% increase in topic granularity and complete elimination of topic overlaps. These findings validated the utility of the proposed frameworks for structured evaluation of LLM-based augmentation in practical NLP pipelines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 111,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12138",
        "abs_url": "https://arxiv.org/abs/2507.12138",
        "pdf_url": "https://arxiv.org/pdf/2507.12138",
        "title": "Neural Human Pose Prior",
        "authors": [
            "Michal Heker",
            "Sefy Kararlitsky",
            "David Tolpin"
        ],
        "comments": "Work in progress",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "We introduce a principled, data-driven approach for modeling a neural prior over human body poses using normalizing flows. Unlike heuristic or low-expressivity alternatives, our method leverages RealNVP to learn a flexible density over poses represented in the 6D rotation format. We address the challenge of modeling distributions on the manifold of valid 6D rotations by inverting the Gram-Schmidt process during training, enabling stable learning while preserving downstream compatibility with rotation-based frameworks. Our architecture and training pipeline are framework-agnostic and easily reproducible. We demonstrate the effectiveness of the learned prior through both qualitative and quantitative evaluations, and we analyze its impact via ablation studies. This work provides a sound probabilistic foundation for integrating pose priors into human motion capture and reconstruction pipelines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 112,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12175",
        "abs_url": "https://arxiv.org/abs/2507.12175",
        "pdf_url": "https://arxiv.org/pdf/2507.12175",
        "title": "RUMAA: Repeat-Aware Unified Music Audio Analysis for Score-Performance Alignment, Transcription, and Mistake Detection",
        "authors": [
            "Sungkyun Chang",
            "Simon Dixon",
            "Emmanouil Benetos"
        ],
        "comments": "Accepted to WASPAA 2025",
        "subjects": "Sound (cs.SD); Computation and Language (cs.CL); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)",
        "abstract": "This study introduces RUMAA, a transformer-based framework for music performance analysis that unifies score-to-performance alignment, score-informed transcription, and mistake detection in a near end-to-end manner. Unlike prior methods addressing these tasks separately, RUMAA integrates them using pre-trained score and audio encoders and a novel tri-stream decoder capturing task interdependencies through proxy tasks. It aligns human-readable MusicXML scores with repeat symbols to full-length performance audio, overcoming traditional MIDI-based methods that rely on manually unfolded score-MIDI data with pre-specified repeat structures. RUMAA matches state-of-the-art alignment methods on non-repeated scores and outperforms them on scores with repeats in a public piano music dataset, while also delivering promising transcription and mistake detection results.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 113,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12189",
        "abs_url": "https://arxiv.org/abs/2507.12189",
        "pdf_url": "https://arxiv.org/pdf/2507.12189",
        "title": "BenchRL-QAS: Benchmarking reinforcement learning algorithms for quantum architecture search",
        "authors": [
            "Azhar Ikhtiarudin",
            "Aditi Das",
            "Param Thakkar",
            "Akash Kundu"
        ],
        "comments": "Comprehensive RL agent benchmark for QAS. Contributions are welcomed here: this https URL",
        "subjects": "Quantum Physics (quant-ph); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Performance (cs.PF)",
        "abstract": "We introduce BenchRL-QAS, a unified benchmarking framework for systematically evaluating reinforcement learning (RL) algorithms in quantum architecture search (QAS) across diverse variational quantum algorithm tasks and system sizes ranging from 2- to 8-qubit. Our study benchmarks nine RL agents including both value-based and policy-gradient methods on representative quantum problems such as variational quantum eigensolver, variational quantum state diagonalization, quantum classification, and state preparation, spanning both noiseless and realistic noisy regimes. We propose a weighted ranking metric that balances accuracy, circuit depth, gate count, and computational efficiency, enabling fair and comprehensive comparison. Our results first reveal that RL-based quantum classifier outperforms baseline variational classifiers. Then we conclude that no single RL algorithm is universally optimal when considering a set of QAS tasks; algorithmic performance is highly context-dependent, varying with task structure, qubit count, and noise. This empirical finding provides strong evidence for the \"no free lunch\" principle in RL-based quantum circuit design and highlights the necessity of tailored algorithm selection and systematic benchmarking for advancing quantum circuit synthesis. This work represents the most comprehensive RL-QAS benchmarking effort to date, and BenchRL-QAS along with all experimental data are made publicly available to support reproducibility and future research this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 114,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12202",
        "abs_url": "https://arxiv.org/abs/2507.12202",
        "pdf_url": "https://arxiv.org/pdf/2507.12202",
        "title": "Sparse Autoencoders for Sequential Recommendation Models: Interpretation and Flexible Control",
        "authors": [
            "Anton Klenitskiy",
            "Konstantin Polev",
            "Daria Denisova",
            "Alexey Vasilev",
            "Dmitry Simakov",
            "Gleb Gusev"
        ],
        "comments": "",
        "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Many current state-of-the-art models for sequential recommendations are based on transformer architectures. Interpretation and explanation of such black box models is an important research question, as a better understanding of their internals can help understand, influence, and control their behavior, which is very important in a variety of real-world applications. Recently sparse autoencoders (SAE) have been shown to be a promising unsupervised approach for extracting interpretable features from language models. These autoencoders learn to reconstruct hidden states of the transformer's internal layers from sparse linear combinations of directions in their activation space. This paper is focused on the application of SAE to the sequential recommendation domain. We show that this approach can be successfully applied to the transformer trained on a sequential recommendation task: learned directions turn out to be more interpretable and monosemantic than the original hidden state dimensions. Moreover, we demonstrate that the features learned by SAE can be used to effectively and flexibly control the model's behavior, providing end-users with a straightforward method to adjust their recommendations to different custom scenarios and contexts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 115,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12233",
        "abs_url": "https://arxiv.org/abs/2507.12233",
        "pdf_url": "https://arxiv.org/pdf/2507.12233",
        "title": "Universal Fourier Neural Operators for Micromechanics",
        "authors": [
            "Binh Huy Nguyen",
            "Matti Schneider"
        ],
        "comments": "48 pages, 13 figures",
        "subjects": "Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)",
        "abstract": "\\noindent Solving cell problems in homogenization is hard, and available deep-learning frameworks fail to match the speed and generality of traditional computational frameworks. More to the point, it is generally unclear what to expect of machine-learning approaches, let alone single out which approaches are promising. In the work at hand, we advocate Fourier Neural Operators (FNOs) for micromechanics, empowering them by insights from computational micromechanics methods based on the fast Fourier transform (FFT). We construct an FNO surrogate mimicking the basic scheme foundational for FFT-based methods and show that the resulting operator predicts solutions to cell problems with \\emph{arbitrary} stiffness distribution only subject to a material-contrast constraint up to a desired accuracy. In particular, there are no restrictions on the material symmetry like isotropy, on the number of phases and on the geometry of the interfaces between materials. Also, the provided fidelity is sharp and uniform, providing explicit guarantees leveraging our physical empowerment of FNOs. To show the desired universal approximation property, we construct an FNO explicitly that requires no training to begin with. Still, the obtained neural operator complies with the same memory requirements as the basic scheme and comes with runtimes proportional to classical FFT solvers. In particular, large-scale problems with more than 100 million voxels are readily handled. The goal of this work is to underline the potential of FNOs for solving micromechanical problems, linking FFT-based methods to FNOs. This connection is expected to provide a fruitful exchange between both worlds.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 116,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12248",
        "abs_url": "https://arxiv.org/abs/2507.12248",
        "pdf_url": "https://arxiv.org/pdf/2507.12248",
        "title": "Comparative Analysis of CNN Performance in Keras, PyTorch and JAX on PathMNIST",
        "authors": [
            "Anida Nezović",
            "Jalal Romano",
            "Nada Marić",
            "Medina Kapo",
            "Amila Akagić"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Deep learning has significantly advanced the field of medical image classification, particularly with the adoption of Convolutional Neural Networks (CNNs). Various deep learning frameworks such as Keras, PyTorch and JAX offer unique advantages in model development and deployment. However, their comparative performance in medical imaging tasks remains underexplored. This study presents a comprehensive analysis of CNN implementations across these frameworks, using the PathMNIST dataset as a benchmark. We evaluate training efficiency, classification accuracy and inference speed to assess their suitability for real-world applications. Our findings highlight the trade-offs between computational speed and model accuracy, offering valuable insights for researchers and practitioners in medical image analysis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 117,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12256",
        "abs_url": "https://arxiv.org/abs/2507.12256",
        "pdf_url": "https://arxiv.org/pdf/2507.12256",
        "title": "Surrogate Quantum Circuit Design for the Lattice Boltzmann Collision Operator",
        "authors": [
            "Monica Lăcătuş",
            "Matthias Möller"
        ],
        "comments": "31 pages, 14 figures",
        "subjects": "Quantum Physics (quant-ph); Machine Learning (cs.LG); Computational Physics (physics.comp-ph)",
        "abstract": "Direct numerical simulation of turbulent flows at high Reynolds numbers remains a major challenge for traditional computational fluid dynamics (CFD) tools running on classical computer hardware. This has motivated growing interest in quantum algorithms for CFD to enable flow simulations on quantum computers. The reason being that these computers are expected to deliver potential speed-ups for certain problems. One promising quantum CFD approach is a fully quantum implementation of the lattice Boltzmann method called QLBM. Although efficient quantum routines are now available for the streaming step, implementing the nonlinear, irreversible collision step with a low depth circuit that avoids additional ancilla qubits, probabilistic post-selection and repeated executions remains a significant challenge. In this study, we address this challenge by introducing a framework for learning a surrogate quantum circuit (SQC) that approximates the full Bhatnagar Gross Krook (BGK) collision operator for the D2Q9 lattice. The four qubit circuit is trained to respect the physical properties of the BGK collision operator, including mass and momentum conservation, D8 equivariance and scale equivariance. When compiled to the gate set used by IBM Heron processor under the assumption of full qubit connectivity, the 15 block SQC requires only 2,430 native gates and uses neither ancilla qubits nor post-selection or repeated executions. Moreover, its depth is independent of the grid resolution, as collision is a local operation that can exploit quantum parallelism to its full extent. We validate the SQC on two benchmark flows, the Taylor Green vortex decay and the lid driven cavity, demonstrating that it accurately captures vortex dissipation and flow recirculation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 118,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12269",
        "abs_url": "https://arxiv.org/abs/2507.12269",
        "pdf_url": "https://arxiv.org/pdf/2507.12269",
        "title": "Site-Level Fine-Tuning with Progressive Layer Freezing: Towards Robust Prediction of Bronchopulmonary Dysplasia from Day-1 Chest Radiographs in Extremely Preterm Infants",
        "authors": [
            "Sybelle Goedicke-Fritz",
            "Michelle Bous",
            "Annika Engel",
            "Matthias Flotho",
            "Pascal Hirsch",
            "Hannah Wittig",
            "Dino Milanovic",
            "Dominik Mohr",
            "Mathias Kaspar",
            "Sogand Nemat",
            "Dorothea Kerner",
            "Arno Bücker",
            "Andreas Keller",
            "Sascha Meyer",
            "Michael Zemlin",
            "Philipp Flotho"
        ],
        "comments": "S.G.-F., M.B., and A.E. contributed equally to this work and share first authorship. M.Z. and P.F. contributed equally to this work and share senior authorship",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Bronchopulmonary dysplasia (BPD) is a chronic lung disease affecting 35% of extremely low birth weight infants. Defined by oxygen dependence at 36 weeks postmenstrual age, it causes lifelong respiratory complications. However, preventive interventions carry severe risks, including neurodevelopmental impairment, ventilator-induced lung injury, and systemic complications. Therefore, early BPD prognosis and prediction of BPD outcome is crucial to avoid unnecessary toxicity in low risk infants. Admission radiographs of extremely preterm infants are routinely acquired within 24h of life and could serve as a non-invasive prognostic tool. In this work, we developed and investigated a deep learning approach using chest X-rays from 163 extremely low-birth-weight infants ($\\leq$32 weeks gestation, 401-999g) obtained within 24 hours of birth. We fine-tuned a ResNet-50 pretrained specifically on adult chest radiographs, employing progressive layer freezing with discriminative learning rates to prevent overfitting and evaluated a CutMix augmentation and linear probing. For moderate/severe BPD outcome prediction, our best performing model with progressive freezing, linear probing and CutMix achieved an AUROC of 0.78 $\\pm$ 0.10, balanced accuracy of 0.69 $\\pm$ 0.10, and an F1-score of 0.67 $\\pm$ 0.11. In-domain pre-training significantly outperformed ImageNet initialization (p = 0.031) which confirms domain-specific pretraining to be important for BPD outcome prediction. Routine IRDS grades showed limited prognostic value (AUROC 0.57 $\\pm$ 0.11), confirming the need of learned markers. Our approach demonstrates that domain-specific pretraining enables accurate BPD prediction from routine day-1 radiographs. Through progressive freezing and linear probing, the method remains computationally feasible for site-level implementation and future federated learning deployments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 119,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12295",
        "abs_url": "https://arxiv.org/abs/2507.12295",
        "pdf_url": "https://arxiv.org/pdf/2507.12295",
        "title": "Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding",
        "authors": [
            "Feng Xiao",
            "Jicong Fan"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Text anomaly detection is a critical task in natural language processing (NLP), with applications spanning fraud detection, misinformation identification, spam detection and content moderation, etc. Despite significant advances in large language models (LLMs) and anomaly detection algorithms, the absence of standardized and comprehensive benchmarks for evaluating the existing anomaly detection methods on text data limits rigorous comparison and development of innovative approaches. This work performs a comprehensive empirical study and introduces a benchmark for text anomaly detection, leveraging embeddings from diverse pre-trained language models across a wide array of text datasets. Our work systematically evaluates the effectiveness of embedding-based text anomaly detection by incorporating (1) early language models (GloVe, BERT); (2) multiple LLMs (LLaMa-2, LLama-3, Mistral, OpenAI (small, ada, large)); (3) multi-domain text datasets (news, social media, scientific publications); (4) comprehensive evaluation metrics (AUROC, AUPRC). Our experiments reveal a critical empirical insight: embedding quality significantly governs anomaly detection efficacy, and deep learning-based approaches demonstrate no performance advantage over conventional shallow algorithms (e.g., KNN, Isolation Forest) when leveraging LLM-derived this http URL addition, we observe strongly low-rank characteristics in cross-model performance matrices, which enables an efficient strategy for rapid model evaluation (or embedding evaluation) and selection in practical applications. Furthermore, by open-sourcing our benchmark toolkit that includes all embeddings from different models and code at this https URL, this work provides a foundation for future research in robust and scalable text anomaly detection systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 120,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12318",
        "abs_url": "https://arxiv.org/abs/2507.12318",
        "pdf_url": "https://arxiv.org/pdf/2507.12318",
        "title": "Compositional Discrete Latent Code for High Fidelity, Productive Diffusion Models",
        "authors": [
            "Samuel Lavoie",
            "Michael Noukhovitch",
            "Aaron Courville"
        ],
        "comments": "In submission, 22 pages, 7 tables, 12 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "We argue that diffusion models' success in modeling complex distributions is, for the most part, coming from their input conditioning. This paper investigates the representation used to condition diffusion models from the perspective that ideal representations should improve sample fidelity, be easy to generate, and be compositional to allow out-of-training samples generation. We introduce Discrete Latent Code (DLC), an image representation derived from Simplicial Embeddings trained with a self-supervised learning objective. DLCs are sequences of discrete tokens, as opposed to the standard continuous image embeddings. They are easy to generate and their compositionality enables sampling of novel images beyond the training distribution. Diffusion models trained with DLCs have improved generation fidelity, establishing a new state-of-the-art for unconditional image generation on ImageNet. Additionally, we show that composing DLCs allows the image generator to produce out-of-distribution samples that coherently combine the semantics of images in diverse ways. Finally, we showcase how DLCs can enable text-to-image generation by leveraging large-scale pretrained language models. We efficiently finetune a text diffusion language model to generate DLCs that produce novel samples outside of the image generator training distribution.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 121,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12329",
        "abs_url": "https://arxiv.org/abs/2507.12329",
        "pdf_url": "https://arxiv.org/pdf/2507.12329",
        "title": "Neural Polar Decoders for Deletion Channels",
        "authors": [
            "Ziv Aharoni",
            "Henry D. Pfister"
        ],
        "comments": "arXiv admin note: text overlap with arXiv:2506.17076",
        "subjects": "Information Theory (cs.IT); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "This paper introduces a neural polar decoder (NPD) for deletion channels with a constant deletion rate. Existing polar decoders for deletion channels exhibit high computational complexity of $O(N^4)$, where $N$ is the block length. This limits the application of polar codes for deletion channels to short-to-moderate block lengths. In this work, we demonstrate that employing NPDs for deletion channels can reduce the computational complexity. First, we extend the architecture of the NPD to support deletion channels. Specifically, the NPD architecture consists of four neural networks (NNs), each replicating fundamental successive cancellation (SC) decoder operations. To support deletion channels, we change the architecture of only one. The computational complexity of the NPD is $O(AN\\log N)$, where the parameter $A$ represents a computational budget determined by the user and is independent of the channel. We evaluate the new extended NPD for deletion channels with deletion rates $\\delta\\in\\{0.01, 0.1\\}$ and we verify the NPD with the ground truth given by the trellis decoder by Tal et al. We further show that due to the reduced complexity of the NPD, we are able to incorporate list decoding and further improve performance. We believe that the extended NPD presented here could have applications in future technologies like DNA storage.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 122,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12404",
        "abs_url": "https://arxiv.org/abs/2507.12404",
        "pdf_url": "https://arxiv.org/pdf/2507.12404",
        "title": "Neural Network-Guided Symbolic Regression for Interpretable Descriptor Discovery in Perovskite Catalysts",
        "authors": [
            "Yeming Xian",
            "Xiaoming Wang",
            "Yanfa Yan"
        ],
        "comments": "31 pages",
        "subjects": "Data Analysis, Statistics and Probability (physics.data-an); Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG); Computational Physics (physics.comp-ph)",
        "abstract": "Understanding and predicting the activity of oxide perovskite catalysts for the oxygen evolution reaction (OER) requires descriptors that are both accurate and physically interpretable. While symbolic regression (SR) offers a path to discover such formulas, its performance degrades with high-dimensional inputs and small datasets. We present a two-phase framework that combines neural networks (NN), feature importance analysis, and symbolic regression (SR) to discover interpretable descriptors for OER activity in oxide perovskites. In Phase I, using a small dataset and seven structural features, we reproduce and improve the known {\\mu}/t descriptor by engineering composite features and applying symbolic regression, achieving training and validation MAEs of 22.8 and 20.8 meV, respectively. In Phase II, we expand to 164 features, reduce dimensionality, and identify LUMO energy as a key electronic descriptor. A final formula using {\\mu}/t, {\\mu}/RA, and LUMO energy achieves improved accuracy (training and validation MAEs of 22.1 and 20.6 meV) with strong physical interpretability. Our results demonstrate that NN-guided symbolic regression enables accurate, interpretable, and physically meaningful descriptor discovery in data-scarce regimes, indicating interpretability need not sacrifice accuracy for materials informatics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 123,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12414",
        "abs_url": "https://arxiv.org/abs/2507.12414",
        "pdf_url": "https://arxiv.org/pdf/2507.12414",
        "title": "AutoVDC: Automated Vision Data Cleaning Using Vision-Language Models",
        "authors": [
            "Santosh Vasa",
            "Aditi Ramadwar",
            "Jnana Rama Krishna Darabattula",
            "Md Zafar Anwar",
            "Stanislaw Antol",
            "Andrei Vatavu",
            "Thomas Monninger",
            "Sihao Ding"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)",
        "abstract": "Training of autonomous driving systems requires extensive datasets with precise annotations to attain robust performance. Human annotations suffer from imperfections, and multiple iterations are often needed to produce high-quality datasets. However, manually reviewing large datasets is laborious and expensive. In this paper, we introduce AutoVDC (Automated Vision Data Cleaning) framework and investigate the utilization of Vision-Language Models (VLMs) to automatically identify erroneous annotations in vision datasets, thereby enabling users to eliminate these errors and enhance data quality. We validate our approach using the KITTI and nuImages datasets, which contain object detection benchmarks for autonomous driving. To test the effectiveness of AutoVDC, we create dataset variants with intentionally injected erroneous annotations and observe the error detection rate of our approach. Additionally, we compare the detection rates using different VLMs and explore the impact of VLM fine-tuning on our pipeline. The results demonstrate our method's high performance in error detection and data cleaning experiments, indicating its potential to significantly improve the reliability and accuracy of large-scale production datasets in autonomous driving.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 124,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12427",
        "abs_url": "https://arxiv.org/abs/2507.12427",
        "pdf_url": "https://arxiv.org/pdf/2507.12427",
        "title": "Unit-Based Histopathology Tissue Segmentation via Multi-Level Feature Representation",
        "authors": [
            "Ashkan Shakarami",
            "Azade Farshad",
            "Yousef Yeganeh",
            "Lorenzo Nicole",
            "Peter Schuffler",
            "Stefano Ghidoni",
            "Nassir Navab"
        ],
        "comments": "12 pages, 6 figures",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "We propose UTS, a unit-based tissue segmentation framework for histopathology that classifies each fixed-size 32 * 32 tile, rather than each pixel, as the segmentation unit. This approach reduces annotation effort and improves computational efficiency without compromising accuracy. To implement this approach, we introduce a Multi-Level Vision Transformer (L-ViT), which benefits the multi-level feature representation to capture both fine-grained morphology and global tissue context. Trained to segment breast tissue into three categories (infiltrating tumor, non-neoplastic stroma, and fat), UTS supports clinically relevant tasks such as tumor-stroma quantification and surgical margin assessment. Evaluated on 386,371 tiles from 459 H&E-stained regions, it outperforms U-Net variants and transformer-based baselines. Code and Dataset will be available at GitHub.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 125,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12428",
        "abs_url": "https://arxiv.org/abs/2507.12428",
        "pdf_url": "https://arxiv.org/pdf/2507.12428",
        "title": "Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models",
        "authors": [
            "Yik Siu Chan",
            "Zheng-Xin Yong",
            "Stephen H. Bach"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Open-weights reasoning language models generate long chains-of-thought (CoTs) before producing a final response, which improves performance but introduces additional alignment risks, with harmful content often appearing in both the CoTs and the final outputs. In this work, we investigate if we can use CoTs to predict final response misalignment. We evaluate a range of monitoring approaches, including humans, highly-capable large language models, and text classifiers, using either CoT text or activations. First, we find that a simple linear probe trained on CoT activations can significantly outperform all text-based methods in predicting whether a final response will be safe or unsafe. CoT texts are often unfaithful and can mislead humans and classifiers, while model latents (i.e., CoT activations) offer a more reliable predictive signal. Second, the probe makes accurate predictions before reasoning completes, achieving strong performance even when applied to early CoT segments. These findings generalize across model sizes, families, and safety benchmarks, suggesting that lightweight probes could enable real-time safety monitoring and early intervention during generation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 126,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12440",
        "abs_url": "https://arxiv.org/abs/2507.12440",
        "pdf_url": "https://arxiv.org/pdf/2507.12440",
        "title": "EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos",
        "authors": [
            "Ruihan Yang",
            "Qinxi Yu",
            "Yecheng Wu",
            "Rui Yan",
            "Borui Li",
            "An-Chieh Cheng",
            "Xueyan Zou",
            "Yunhao Fang",
            "Hongxu Yin",
            "Sifei Liu",
            "Song Han",
            "Yao Lu",
            "Xiaolong Wang"
        ],
        "comments": "More videos can be found on our website: this https URL",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Real robot data collection for imitation learning has led to significant advancements in robotic manipulation. However, the requirement for robot hardware in the process fundamentally constrains the scale of the data. In this paper, we explore training Vision-Language-Action (VLA) models using egocentric human videos. The benefit of using human videos is not only for their scale but more importantly for the richness of scenes and tasks. With a VLA trained on human video that predicts human wrist and hand actions, we can perform Inverse Kinematics and retargeting to convert the human actions to robot actions. We fine-tune the model using a few robot manipulation demonstrations to obtain the robot policy, namely EgoVLA. We propose a simulation benchmark called Ego Humanoid Manipulation Benchmark, where we design diverse bimanual manipulation tasks with demonstrations. We fine-tune and evaluate EgoVLA with Ego Humanoid Manipulation Benchmark and show significant improvements over baselines and ablate the importance of human data. Videos can be found on our website: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 127,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12441",
        "abs_url": "https://arxiv.org/abs/2507.12441",
        "pdf_url": "https://arxiv.org/pdf/2507.12441",
        "title": "Describe Anything Model for Visual Question Answering on Text-rich Images",
        "authors": [
            "Yen-Linh Vu",
            "Dinh-Thang Duong",
            "Truong-Binh Duong",
            "Anh-Khoi Nguyen",
            "Thanh-Huy Nguyen",
            "Le Thien Phuc Nguyen",
            "Jianhua Xing",
            "Xingjian Li",
            "Tianyang Wang",
            "Ulas Bagci",
            "Min Xu"
        ],
        "comments": "11 pages, 5 figures. Accepted to VisionDocs @ ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Recent progress has been made in region-aware vision-language modeling, particularly with the emergence of the Describe Anything Model (DAM). DAM is capable of generating detailed descriptions of any specific image areas or objects without the need for additional localized image-text alignment supervision. We hypothesize that such region-level descriptive capability is beneficial for the task of Visual Question Answering (VQA), especially in challenging scenarios involving images with dense text. In such settings, the fine-grained extraction of textual information is crucial to producing correct answers. Motivated by this, we introduce DAM-QA, a framework with a tailored evaluation protocol, developed to investigate and harness the region-aware capabilities from DAM for the text-rich VQA problem that requires reasoning over text-based information within images. DAM-QA incorporates a mechanism that aggregates answers from multiple regional views of image content, enabling more effective identification of evidence that may be tied to text-related elements. Experiments on six VQA benchmarks show that our approach consistently outperforms the baseline DAM, with a notable 7+ point gain on DocVQA. DAM-QA also achieves the best overall performance among region-aware models with fewer parameters, significantly narrowing the gap with strong generalist VLMs. These results highlight the potential of DAM-like models for text-rich and broader VQA tasks when paired with efficient usage and integration strategies. Our code is publicly available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 128,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12442",
        "abs_url": "https://arxiv.org/abs/2507.12442",
        "pdf_url": "https://arxiv.org/pdf/2507.12442",
        "title": "Characterizing State Space Model (SSM) and SSM-Transformer Hybrid Language Model Performance with Long Context Length",
        "authors": [
            "Saptarshi Mitra",
            "Rachid Karami",
            "Haocheng Xu",
            "Sitao Huang",
            "Hyoukjun Kwon"
        ],
        "comments": "12 pages, 7 figures",
        "subjects": "Hardware Architecture (cs.AR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Systems and Control (eess.SY)",
        "abstract": "The demand for machine intelligence capable of processing continuous, long-context inputs on local devices is growing rapidly. However, the quadratic complexity and memory requirements of traditional Transformer architectures make them inefficient and often unusable for these tasks. This has spurred a paradigm shift towards new architectures like State Space Models (SSMs) and hybrids, which promise near-linear scaling. While most current research focuses on the accuracy and theoretical throughput of these models, a systematic performance characterization on practical consumer hardware is critically needed to guide system-level optimization and unlock new applications. To address this gap, we present a comprehensive, comparative benchmarking of carefully selected Transformer, SSM, and hybrid models specifically for long-context inference on consumer and embedded GPUs. Our analysis reveals that SSMs are not only viable but superior for this domain, capable of processing sequences up to 220K tokens on a 24GB consumer GPU-approximately 4x longer than comparable Transformers. While Transformers may be up to 1.8x faster at short sequences, SSMs demonstrate a dramatic performance inversion, becoming up to 4x faster at very long contexts (~57K tokens). Our operator-level analysis reveals that custom, hardware-aware SSM kernels dominate the inference runtime, accounting for over 55% of latency on edge platforms, identifying them as a primary target for future hardware acceleration. We also provide detailed, device-specific characterization results to guide system co-design for the edge. To foster further research, we will open-source our characterization framework.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 129,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12451",
        "abs_url": "https://arxiv.org/abs/2507.12451",
        "pdf_url": "https://arxiv.org/pdf/2507.12451",
        "title": "S2WTM: Spherical Sliced-Wasserstein Autoencoder for Topic Modeling",
        "authors": [
            "Suman Adhya",
            "Debarshi Kumar Sanyal"
        ],
        "comments": "Accepted as a long paper for ACL 2025 main conference",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Modeling latent representations in a hyperspherical space has proven effective for capturing directional similarities in high-dimensional text data, benefiting topic modeling. Variational autoencoder-based neural topic models (VAE-NTMs) commonly adopt the von Mises-Fisher prior to encode hyperspherical structure. However, VAE-NTMs often suffer from posterior collapse, where the KL divergence term in the objective function highly diminishes, leading to ineffective latent representations. To mitigate this issue while modeling hyperspherical structure in the latent space, we propose the Spherical Sliced Wasserstein Autoencoder for Topic Modeling (S2WTM). S2WTM employs a prior distribution supported on the unit hypersphere and leverages the Spherical Sliced-Wasserstein distance to align the aggregated posterior distribution with the prior. Experimental results demonstrate that S2WTM outperforms state-of-the-art topic models, generating more coherent and diverse topics while improving performance on downstream tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 130,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12464",
        "abs_url": "https://arxiv.org/abs/2507.12464",
        "pdf_url": "https://arxiv.org/pdf/2507.12464",
        "title": "CytoSAE: Interpretable Cell Embeddings for Hematology",
        "authors": [
            "Muhammed Furkan Dasdelen",
            "Hyesu Lim",
            "Michele Buck",
            "Katharina S. Götze",
            "Carsten Marr",
            "Steffen Schneider"
        ],
        "comments": "11 pages, 5 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)",
        "abstract": "Sparse autoencoders (SAEs) emerged as a promising tool for mechanistic interpretability of transformer-based foundation models. Very recently, SAEs were also adopted for the visual domain, enabling the discovery of visual concepts and their patch-wise attribution to tokens in the transformer model. While a growing number of foundation models emerged for medical imaging, tools for explaining their inferences are still lacking. In this work, we show the applicability of SAEs for hematology. We propose CytoSAE, a sparse autoencoder which is trained on over 40,000 peripheral blood single-cell images. CytoSAE generalizes to diverse and out-of-domain datasets, including bone marrow cytology, where it identifies morphologically relevant concepts which we validated with medical experts. Furthermore, we demonstrate scenarios in which CytoSAE can generate patient-specific and disease-specific concepts, enabling the detection of pathognomonic cells and localized cellular abnormalities at the patch level. We quantified the effect of concepts on a patient-level AML subtype classification task and show that CytoSAE concepts reach performance comparable to the state-of-the-art, while offering explainability on the sub-cellular level. Source code and model weights are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 131,
        "date": "2025-07-17",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-17?abs=True",
        "arxiv_id": "2507.12466",
        "abs_url": "https://arxiv.org/abs/2507.12466",
        "pdf_url": "https://arxiv.org/pdf/2507.12466",
        "title": "Language Models Improve When Pretraining Data Matches Target Tasks",
        "authors": [
            "David Mizrahi",
            "Anders Boesen Lindbo Larsen",
            "Jesse Allardice",
            "Suzie Petryk",
            "Yuri Gorokhov",
            "Jeffrey Li",
            "Alex Fang",
            "Josh Gardner",
            "Tom Gunter",
            "Afshin Dehghan"
        ],
        "comments": "44 pages, 25 figures, 13 tables",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Every data selection method inherently has a target. In practice, these targets often emerge implicitly through benchmark-driven iteration: researchers develop selection strategies, train models, measure benchmark performance, then refine accordingly. This raises a natural question: what happens when we make this optimization explicit? To explore this, we propose benchmark-targeted ranking (BETR), a simple method that selects pretraining documents based on similarity to benchmark training examples. BETR embeds benchmark examples and a sample of pretraining documents in a shared space, scores this sample by similarity to benchmarks, then trains a lightweight classifier to predict these scores for the full corpus. We compare data selection methods by training over 500 models spanning $10^{19}$ to $10^{22}$ FLOPs and fitting scaling laws to them. From this, we find that simply aligning pretraining data to evaluation benchmarks using BETR achieves a 2.1x compute multiplier over DCLM-Baseline (4.7x over unfiltered data) and improves performance on 9 out of 10 tasks across all scales. BETR also generalizes well: when targeting a diverse set of benchmarks disjoint from our evaluation suite, it still matches or outperforms baselines. Our scaling analysis further reveals a clear trend: larger models require less aggressive filtering. Overall, our findings show that directly matching pretraining data to target tasks precisely shapes model capabilities and highlight that optimal selection strategies must adapt to model scale.",
        "gemini2.5flash": "",
        "overall_idea": ""
    }
]