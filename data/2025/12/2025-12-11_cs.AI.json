[
    {
        "order": 1,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09088",
        "abs_url": "https://arxiv.org/abs/2512.09088",
        "pdf_url": "https://arxiv.org/pdf/2512.09088",
        "title": "Calibrated Trust in Dealing with LLM Hallucinations: A Qualitative Study",
        "authors": [
            "Adrian Ryser",
            "Florian Allwein",
            "Tim Schlippe"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Hallucinations are outputs by Large Language Models (LLMs) that are factually incorrect yet appear plausible [1]. This paper investigates how such hallucinations influence users' trust in LLMs and users' interaction with LLMs. To explore this in everyday use, we conducted a qualitative study with 192 participants. Our findings show that hallucinations do not result in blanket mistrust but instead lead to context-sensitive trust calibration. Building on the calibrated trust model by Lee & See [2] and Afroogh et al.'s trust-related factors [3], we confirm expectancy [3], [4], prior experience [3], [4], [5], and user expertise & domain knowledge [3], [4] as userrelated (human) trust factors, and identify intuition as an additional factor relevant for hallucination detection. Additionally, we found that trust dynamics are further influenced by contextual factors, particularly perceived risk [3] and decision stakes [6]. Consequently, we validate the recursive trust calibration process proposed by Blöbaum [7] and extend it by including intuition as a user-related trust factor. Based on these insights, we propose practical recommendations for responsible and reflective LLM use.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 2,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09114",
        "abs_url": "https://arxiv.org/abs/2512.09114",
        "pdf_url": "https://arxiv.org/pdf/2512.09114",
        "title": "AI TIPS 2.0: A Comprehensive Framework for Operationalizing AI Governance",
        "authors": [
            "Pamela Gupta"
        ],
        "comments": "47 pages",
        "subjects": "Artificial Intelligence (cs.AI); Computers and Society (cs.CY)",
        "abstract": "The deployment of AI systems faces three critical governance challenges that current frameworks fail to adequately address. First, organizations struggle with inadequate risk assessment at the use case level, exemplified by the Humana class action lawsuit and other high impact cases where an AI system deployed to production exhibited both significant bias and high error rates, resulting in improper healthcare claim denials. Each AI use case presents unique risk profiles requiring tailored governance, yet most frameworks provide one size fits all guidance. Second, existing frameworks like ISO 42001 and NIST AI RMF remain at high conceptual levels, offering principles without actionable controls, leaving practitioners unable to translate governance requirements into specific technical implementations. Third, organizations lack mechanisms for operationalizing governance at scale, with no systematic approach to embed trustworthy AI practices throughout the development lifecycle, measure compliance quantitatively, or provide role-appropriate visibility from boards to data scientists. We present AI TIPS, Artificial Intelligence Trust-Integrated Pillars for Sustainability 2.0, update to the comprehensive operational framework developed in 2019,four years before NIST's AI Risk Management Framework, that directly addresses these challenges.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 3,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09117",
        "abs_url": "https://arxiv.org/abs/2512.09117",
        "pdf_url": "https://arxiv.org/pdf/2512.09117",
        "title": "A Categorical Analysis of Large Language Models and Why LLMs Circumvent the Symbol Grounding Problem",
        "authors": [
            "Luciano Floridi",
            "Yiyang Jia",
            "Fernando Tohmé"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "This paper presents a formal, categorical framework for analysing how humans and large language models (LLMs) transform content into truth-evaluated propositions about a state space of possible worlds W , in order to argue that LLMs do not solve but circumvent the symbol grounding problem.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 4,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09142",
        "abs_url": "https://arxiv.org/abs/2512.09142",
        "pdf_url": "https://arxiv.org/pdf/2512.09142",
        "title": "SDialog: A Python Toolkit for End-to-End Agent Building, User Simulation, Dialog Generation, and Evaluation",
        "authors": [
            "Sergio Burdisso",
            "Séverin Baroudi",
            "Yanis Labrak",
            "David Grunert",
            "Pawel Cyrta",
            "Yiyang Chen",
            "Srikanth Madikeri",
            "Esaú Villatoro-Tello",
            "Thomas Schaaf",
            "Ricard Marxer",
            "Petr Motlicek"
        ],
        "comments": "Pre-print submitted to EACL System Demonstration (under review)",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "We present SDialog, an MIT-licensed open-source Python toolkit that unifies dialog generation, evaluation and mechanistic interpretability into a single end-to-end framework for building and analyzing LLM-based conversational agents. Built around a standardized \\texttt{Dialog} representation, SDialog provides: (1) persona-driven multi-agent simulation with composable orchestration for controlled, synthetic dialog generation, (2) comprehensive evaluation combining linguistic metrics, LLM-as-a-judge and functional correctness validators, (3) mechanistic interpretability tools for activation inspection and steering via feature ablation and induction, and (4) audio generation with full acoustic simulation including 3D room modeling and microphone effects. The toolkit integrates with all major LLM backends, enabling mixed-backend experiments under a unified API. By coupling generation, evaluation, and interpretability in a dialog-centric architecture, SDialog enables researchers to build, benchmark and understand conversational systems more systematically.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 5,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09458",
        "abs_url": "https://arxiv.org/abs/2512.09458",
        "pdf_url": "https://arxiv.org/pdf/2512.09458",
        "title": "Architectures for Building Agentic AI",
        "authors": [
            "Sławomir Nowaczyk"
        ],
        "comments": "This is a preprint of a chapter accepted for publication in Generative and Agentic AI Reliability: Architectures, Challenges, and Trust for Autonomous Systems, published by Springer Nature",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "This chapter argues that the reliability of agentic and generative AI is chiefly an architectural property. We define agentic systems as goal-directed, tool-using decision makers operating in closed loops, and show how reliability emerges from principled componentisation (goal manager, planner, tool-router, executor, memory, verifiers, safety monitor, telemetry), disciplined interfaces (schema-constrained, validated, least-privilege tool calls), and explicit control and assurance loops. Building on classical foundations, we propose a practical taxonomy-tool-using agents, memory-augmented agents, planning and self-improvement agents, multi-agent systems, and embodied or web agents - and analyse how each pattern reshapes the reliability envelope and failure modes. We distil design guidance on typed schemas, idempotency, permissioning, transactional semantics, memory provenance and hygiene, runtime governance (budgets, termination conditions), and simulate-before-actuate safeguards.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 6,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09566",
        "abs_url": "https://arxiv.org/abs/2512.09566",
        "pdf_url": "https://arxiv.org/pdf/2512.09566",
        "title": "Toward Closed-loop Molecular Discovery via Language Model, Property Alignment and Strategic Search",
        "authors": [
            "Junkai Ji",
            "Zhangfan Yang",
            "Dong Xu",
            "Ruibin Bai",
            "Jianqiang Li",
            "Tingjun Hou",
            "Zexuan Zhu"
        ],
        "comments": "21 pages, 5 figures",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Drug discovery is a time-consuming and expensive process, with traditional high-throughput and docking-based virtual screening hampered by low success rates and limited scalability. Recent advances in generative modelling, including autoregressive, diffusion, and flow-based approaches, have enabled de novo ligand design beyond the limits of enumerative screening. Yet these models often suffer from inadequate generalization, limited interpretability, and an overemphasis on binding affinity at the expense of key pharmacological properties, thereby restricting their translational utility. Here we present Trio, a molecular generation framework integrating fragment-based molecular language modeling, reinforcement learning, and Monte Carlo tree search, for effective and interpretable closed-loop targeted molecular design. Through the three key components, Trio enables context-aware fragment assembly, enforces physicochemical and synthetic feasibility, and guides a balanced search between the exploration of novel chemotypes and the exploitation of promising intermediates within protein binding pockets. Experimental results show that Trio reliably achieves chemically valid and pharmacologically enhanced ligands, outperforming state-of-the-art approaches with improved binding affinity (+7.85%), drug-likeness (+11.10%) and synthetic accessibility (+12.05%), while expanding molecular diversity more than fourfold.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 7,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09629",
        "abs_url": "https://arxiv.org/abs/2512.09629",
        "pdf_url": "https://arxiv.org/pdf/2512.09629",
        "title": "An End-to-end Planning Framework with Agentic LLMs and PDDL",
        "authors": [
            "Emanuele La Malfa",
            "Ping Zhu",
            "Samuele Marro",
            "Sara Bernardini",
            "Michael Wooldridge"
        ],
        "comments": "Code: this https URL",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "We present an end-to-end framework for planning supported by verifiers. An orchestrator receives a human specification written in natural language and converts it into a PDDL (Planning Domain Definition Language) model, where the domain and problem are iteratively refined by sub-modules (agents) to address common planning requirements, such as time constraints and optimality, as well as ambiguities and contradictions that may exist in the human specification. The validated domain and problem are then passed to an external planning engine to generate a plan. The orchestrator and agents are powered by Large Language Models (LLMs) and require no human intervention at any stage of the process. Finally, a module translates the final plan back into natural language to improve human readability while maintaining the correctness of each step. We demonstrate the flexibility and effectiveness of our framework across various domains and tasks, including the Google NaturalPlan benchmark and PlanBench, as well as planning problems like Blocksworld and the Tower of Hanoi (where LLMs are known to struggle even with small instances). Our framework can be integrated with any PDDL planning engine and validator (such as Fast Downward, LPG, POPF, VAL, and uVAL, which we have tested) and represents a significant step toward end-to-end planning aided by LLMs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 8,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09727",
        "abs_url": "https://arxiv.org/abs/2512.09727",
        "pdf_url": "https://arxiv.org/pdf/2512.09727",
        "title": "Gaussian Process Aggregation for Root-Parallel Monte Carlo Tree Search with Continuous Actions",
        "authors": [
            "Junlin Xiao",
            "Victor-Alexandru Darvariu",
            "Bruno Lacerda",
            "Nick Hawes"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Monte Carlo Tree Search is a cornerstone algorithm for online planning, and its root-parallel variant is widely used when wall clock time is limited but best performance is desired. In environments with continuous action spaces, how to best aggregate statistics from different threads is an important yet underexplored question. In this work, we introduce a method that uses Gaussian Process Regression to obtain value estimates for promising actions that were not trialed in the environment. We perform a systematic evaluation across 6 different domains, demonstrating that our approach outperforms existing aggregation strategies while requiring a modest increase in inference time.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 9,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09736",
        "abs_url": "https://arxiv.org/abs/2512.09736",
        "pdf_url": "https://arxiv.org/pdf/2512.09736",
        "title": "Analyzing Planner Design Trade-offs for MAPF under Realistic Simulation",
        "authors": [
            "Jingtian Yan",
            "Zhifei Li",
            "William Kang",
            "Stephen F. Smith",
            "Jiaoyang Li"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Multi-Agent Path Finding (MAPF) algorithms are increasingly deployed in industrial warehouses and automated manufacturing facilities, where robots must operate reliably under real-world physical constraints. However, existing MAPF evaluation frameworks typically rely on simplified robot models, leaving a substantial gap between algorithmic benchmarks and practical performance. Recent frameworks such as SMART, incorporate kinodynamic modeling and offer the MAPF community a platform for large-scale, realistic evaluation. Building on this capability, this work investigates how key planner design choices influence performance under realistic execution settings. We systematically study three fundamental factors: (1) the relationship between solution optimality and execution performance, (2) the sensitivity of system performance to inaccuracies in kinodynamic modeling, and (3) the interaction between model accuracy and plan optimality. Empirically, we examine these factors to understand how these design choices affect performance in realistic scenarios. We highlight open challenges and research directions to steer the community toward practical, real-world deployment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 10,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09829",
        "abs_url": "https://arxiv.org/abs/2512.09829",
        "pdf_url": "https://arxiv.org/pdf/2512.09829",
        "title": "RIFT: A Scalable Methodology for LLM Accelerator Fault Assessment using Reinforcement Learning",
        "authors": [
            "Khurram Khalil",
            "Muhammad Mahad Khaliq",
            "Khaza Anuarul Hoque"
        ],
        "comments": "Accepted in the IEEE DATE 2026 conference",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "The massive scale of modern AI accelerators presents critical challenges to traditional fault assessment methodologies, which face prohibitive computational costs and provide poor coverage of critical failure modes. This paper introduces RIFT (Reinforcement Learning-guided Intelligent Fault Targeting), a scalable framework that automates the discovery of minimal, high-impact fault scenarios for efficient design-time fault assessment. RIFT transforms the complex search for worst-case faults into a sequential decision-making problem, combining hybrid sensitivity analysis for search space pruning with reinforcement learning to intelligently generate minimal, high-impact test suites. Evaluated on billion-parameter Large Language Model (LLM) workloads using NVIDIA A100 GPUs, RIFT achieves a \\textbf{2.2$\\times$} fault assessment speedup over evolutionary methods and reduces the required test vector volume by over \\textbf{99\\%} compared to random fault injection, all while achieving \\textbf{superior fault coverage}. The proposed framework also provides actionable data to enable intelligent hardware protection strategies, demonstrating that RIFT-guided selective error correction code provides a \\textbf{12.8$\\times$} improvement in \\textbf{cost-effectiveness} (coverage per unit area) compared to uniform triple modular redundancy protection. RIFT automatically generates UVM-compliant verification artifacts, ensuring its findings are directly actionable and integrable into commercial RTL verification workflows.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 11,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09831",
        "abs_url": "https://arxiv.org/abs/2512.09831",
        "pdf_url": "https://arxiv.org/pdf/2512.09831",
        "title": "Interpretation as Linear Transformation: A Cognitive-Geometric Model of Belief and Meaning",
        "authors": [
            "Chainarong Amornbunchornvej"
        ],
        "comments": "The first draft of cognitive geometry model",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multiagent Systems (cs.MA); Social and Information Networks (cs.SI)",
        "abstract": "This paper develops a geometric framework for modeling belief, motivation, and influence across cognitively heterogeneous agents. Each agent is represented by a personalized value space, a vector space encoding the internal dimensions through which the agent interprets and evaluates meaning. Beliefs are formalized as structured vectors-abstract beings-whose transmission is mediated by linear interpretation maps. A belief survives communication only if it avoids the null spaces of these maps, yielding a structural criterion for intelligibility, miscommunication, and belief death. Within this framework, I show how belief distortion, motivational drift, counterfactual evaluation, and the limits of mutual understanding arise from purely algebraic constraints. A central result-\"the No-Null-Space Leadership Condition\"-characterizes leadership as a property of representational reachability rather than persuasion or authority. More broadly, the model explains how abstract beings can propagate, mutate, or disappear as they traverse diverse cognitive geometries. The account unifies insights from conceptual spaces, social epistemology, and AI value alignment by grounding meaning preservation in structural compatibility rather than shared information or rationality. I argue that this cognitive-geometric perspective clarifies the epistemic boundaries of influence in both human and artificial systems, and offers a general foundation for analyzing belief dynamics across heterogeneous agents.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 12,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09882",
        "abs_url": "https://arxiv.org/abs/2512.09882",
        "pdf_url": "https://arxiv.org/pdf/2512.09882",
        "title": "Comparing AI Agents to Cybersecurity Professionals in Real-World Penetration Testing",
        "authors": [
            "Justin W. Lin",
            "Eliot Krzysztof Jones",
            "Donovan Julian Jasper",
            "Ethan Jun-shen Ho",
            "Anna Wu",
            "Arnold Tianyi Yang",
            "Neil Perry",
            "Andy Zou",
            "Matt Fredrikson",
            "J. Zico Kolter",
            "Percy Liang",
            "Dan Boneh",
            "Daniel E. Ho"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Computers and Society (cs.CY)",
        "abstract": "We present the first comprehensive evaluation of AI agents against human cybersecurity professionals in a live enterprise environment. We evaluate ten cybersecurity professionals alongside six existing AI agents and ARTEMIS, our new agent scaffold, on a large university network consisting of ~8,000 hosts across 12 subnets. ARTEMIS is a multi-agent framework featuring dynamic prompt generation, arbitrary sub-agents, and automatic vulnerability triaging. In our comparative study, ARTEMIS placed second overall, discovering 9 valid vulnerabilities with an 82% valid submission rate and outperforming 9 of 10 human participants. While existing scaffolds such as Codex and CyAgent underperformed relative to most human participants, ARTEMIS demonstrated technical sophistication and submission quality comparable to the strongest participants. We observe that AI agents offer advantages in systematic enumeration, parallel exploitation, and cost -- certain ARTEMIS variants cost $18/hour versus $60/hour for professional penetration testers. We also identify key capability gaps: AI agents exhibit higher false-positive rates and struggle with GUI-based tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 13,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09895",
        "abs_url": "https://arxiv.org/abs/2512.09895",
        "pdf_url": "https://arxiv.org/pdf/2512.09895",
        "title": "Human-in-the-Loop and AI: Crowdsourcing Metadata Vocabulary for Materials Science",
        "authors": [
            "Jane Greenberg",
            "Scott McClellan",
            "Addy Ireland",
            "Robert Sammarco",
            "Colton Gerber",
            "Christopher B. Rauch",
            "Mat Kelly",
            "John Kunze",
            "Yuan An",
            "Eric Toberer"
        ],
        "comments": "Metadata and Semantics Research Conference 2025, 14 pages, 7 figures",
        "subjects": "Artificial Intelligence (cs.AI); Digital Libraries (cs.DL)",
        "abstract": "Metadata vocabularies are essential for advancing FAIR and FARR data principles, but their development constrained by limited human resources and inconsistent standardization practices. This paper introduces MatSci-YAMZ, a platform that integrates artificial intelligence (AI) and human-in-the-loop (HILT), including crowdsourcing, to support metadata vocabulary development. The paper reports on a proof-of-concept use case evaluating the AI-HILT model in materials science, a highly interdisciplinary domain Six (6) participants affiliated with the NSF Institute for Data-Driven Dynamical Design (ID4) engaged with the MatSci-YAMZ plaform over several weeks, contributing term definitions and providing examples to prompt the AI-definitions refinement. Nineteen (19) AI-generated definitions were successfully created, with iterative feedback loops demonstrating the feasibility of AI-HILT refinement. Findings confirm the feasibility AI-HILT model highlighting 1) a successful proof of concept, 2) alignment with FAIR and open-science principles, 3) a research protocol to guide future studies, and 4) the potential for scalability across domains. Overall, MatSci-YAMZ's underlying model has the capacity to enhance semantic transparency and reduce time required for consensus building and metadata vocabulary development.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 14,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09897",
        "abs_url": "https://arxiv.org/abs/2512.09897",
        "pdf_url": "https://arxiv.org/pdf/2512.09897",
        "title": "SCOPE: Language Models as One-Time Teacher for Hierarchical Planning in Text Environments",
        "authors": [
            "Haoye Lu",
            "Pavan Seshadri",
            "Kaheer Suleman"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Long-term planning in complex, text-based environments presents significant challenges due to open-ended action spaces, ambiguous observations, and sparse feedback. Recent research suggests that large language models (LLMs) encode rich semantic knowledge about the world, which can be valuable for guiding agents in high-level reasoning and planning across both embodied and purely textual settings. However, existing approaches often depend heavily on querying LLMs during training and inference, making them computationally expensive and difficult to deploy efficiently. In addition, these methods typically employ a pretrained, unaltered LLM whose parameters remain fixed throughout training, providing no opportunity for adaptation to the target task. To address these limitations, we introduce SCOPE (Subgoal-COnditioned Pretraining for Efficient planning), a one-shot hierarchical planner that leverages LLM-generated subgoals only at initialization to pretrain a lightweight student model. Unlike prior approaches that distill LLM knowledge by repeatedly prompting the model to adaptively generate subgoals during training, our method derives subgoals directly from example trajectories. This design removes the need for repeated LLM queries, significantly improving efficiency, though at the cost of reduced explainability and potentially suboptimal subgoals. Despite their suboptimality, our results on the TextCraft environment show that LLM-generated subgoals can still serve as a strong starting point for hierarchical goal decomposition in text-based planning tasks. Compared to the LLM-based hierarchical agent ADaPT (Prasad et al., 2024), which achieves a 0.52 success rate, our method reaches 0.56 and reduces inference time from 164.4 seconds to just 3.0 seconds.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 15,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09908",
        "abs_url": "https://arxiv.org/abs/2512.09908",
        "pdf_url": "https://arxiv.org/pdf/2512.09908",
        "title": "Bayesian Networks, Markov Networks, Moralisation, Triangulation: a Categorical Perspective",
        "authors": [
            "Antonio Lorenzin",
            "Fabio Zanasi"
        ],
        "comments": "36 pages. A preliminary version of this work was presented at CALCO 2025, under the title \"An Algebraic Approach to Moralisation and Triangulation of Probabilistic Graphical Models''",
        "subjects": "Artificial Intelligence (cs.AI); Logic in Computer Science (cs.LO); Category Theory (math.CT)",
        "abstract": "Moralisation and Triangulation are transformations allowing to switch between different ways of factoring a probability distribution into a graphical model. Moralisation allows to view a Bayesian network (a directed model) as a Markov network (an undirected model), whereas triangulation addresses the opposite direction. We present a categorical framework where these transformations are modelled as functors between a category of Bayesian networks and one of Markov networks. The two kinds of network (the objects of these categories) are themselves represented as functors from a `syntax' domain to a `semantics' codomain. Notably, moralisation and triangulation can be defined inductively on such syntax via functor pre-composition. Moreover, while moralisation is fully syntactic, triangulation relies on semantics. This leads to a discussion of the variable elimination algorithm, reinterpreted here as a functor in its own right, that splits the triangulation procedure in two: one purely syntactic, the other purely semantic. This approach introduces a functorial perspective into the theory of probabilistic graphical models, which highlights the distinctions between syntactic and semantic modifications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 16,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2107.05664",
        "abs_url": "https://arxiv.org/abs/2107.05664",
        "pdf_url": "https://arxiv.org/pdf/2107.05664",
        "title": "Altruistic Maneuver Planning for Cooperative Autonomous Vehicles Using Multi-agent Advantage Actor-Critic",
        "authors": [
            "Behrad Toghi",
            "Rodolfo Valiente",
            "Dorsa Sadigh",
            "Ramtin Pedarsani",
            "Yaser P. Fallah"
        ],
        "comments": "Accepted to 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2021) - Workshop on Autonomous Driving: Perception, Prediction and Planning",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
        "abstract": "With the adoption of autonomous vehicles on our roads, we will witness a mixed-autonomy environment where autonomous and human-driven vehicles must learn to co-exist by sharing the same road infrastructure. To attain socially-desirable behaviors, autonomous vehicles must be instructed to consider the utility of other vehicles around them in their decision-making process. Particularly, we study the maneuver planning problem for autonomous vehicles and investigate how a decentralized reward structure can induce altruism in their behavior and incentivize them to account for the interest of other autonomous and human-driven vehicles. This is a challenging problem due to the ambiguity of a human driver's willingness to cooperate with an autonomous vehicle. Thus, in contrast with the existing works which rely on behavior models of human drivers, we take an end-to-end approach and let the autonomous agents to implicitly learn the decision-making process of human drivers only from experience. We introduce a multi-agent variant of the synchronous Advantage Actor-Critic (A2C) algorithm and train agents that coordinate with each other and can affect the behavior of human drivers to improve traffic flow and safety.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 17,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2211.10585",
        "abs_url": "https://arxiv.org/abs/2211.10585",
        "pdf_url": "https://arxiv.org/pdf/2211.10585",
        "title": "Prediction-aware and Reinforcement Learning based Altruistic Cooperative Driving",
        "authors": [
            "Rodolfo Valiente",
            "Mahdi Razzaghpour",
            "Behrad Toghi",
            "Ghayoor Shah",
            "Yaser P. Fallah"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
        "abstract": "Autonomous vehicle (AV) navigation in the presence of Human-driven vehicles (HVs) is challenging, as HVs continuously update their policies in response to AVs. In order to navigate safely in the presence of complex AV-HV social interactions, the AVs must learn to predict these changes. Humans are capable of navigating such challenging social interaction settings because of their intrinsic knowledge about other agents behaviors and use that to forecast what might happen in the future. Inspired by humans, we provide our AVs the capability of anticipating future states and leveraging prediction in a cooperative reinforcement learning (RL) decision-making framework, to improve safety and robustness. In this paper, we propose an integration of two essential and earlier-presented components of AVs: social navigation and prediction. We formulate the AV decision-making process as a RL problem and seek to obtain optimal policies that produce socially beneficial results utilizing a prediction-aware planning and social-aware optimization RL framework. We also propose a Hybrid Predictive Network (HPN) that anticipates future observations. The HPN is used in a multi-step prediction chain to compute a window of predicted future observations to be used by the value function network (VFN). Finally, a safe VFN is trained to optimize a social utility using a sequence of previous and predicted observations, and a safety prioritizer is used to leverage the interpretable kinematic predictions to mask the unsafe actions, constraining the RL policy. We compare our prediction-aware AV to state-of-the-art solutions and demonstrate performance improvements in terms of efficiency and safety in multiple simulated scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 18,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.08933",
        "abs_url": "https://arxiv.org/abs/2512.08933",
        "pdf_url": "https://arxiv.org/pdf/2512.08933",
        "title": "Agentic AI as Undercover Teammates: Argumentative Knowledge Construction in Hybrid Human-AI Collaborative Learning",
        "authors": [
            "Lixiang Yan",
            "Yueqiao Jin",
            "Linxuan Zhao",
            "Roberto Martinez-Maldonado",
            "Xinyu Li",
            "Xiu Guan",
            "Wenxin Guo",
            "Xibin Han",
            "Dragan Gašević"
        ],
        "comments": "",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI); Computers and Society (cs.CY)",
        "abstract": "Generative artificial intelligence (AI) agents are increasingly embedded in collaborative learning environments, yet their impact on the processes of argumentative knowledge construction remains insufficiently understood. Emerging conceptualisations of agentic AI and artificial agency suggest that such systems possess bounded autonomy, interactivity, and adaptability, allowing them to engage as epistemic participants rather than mere instructional tools. Building on this theoretical foundation, the present study investigates how agentic AI, designed as undercover teammates with either supportive or contrarian personas, shapes the epistemic and social dynamics of collaborative reasoning. Drawing on Weinberger and Fischer's (2006) four-dimensional framework, participation, epistemic reasoning, argument structure, and social modes of co-construction, we analysed synchronous discourse data from 212 human and 64 AI participants (92 triads) engaged in an analytical problem-solving task. Mixed-effects and epistemic network analyses revealed that AI teammates maintained balanced participation but substantially reorganised epistemic and social processes: supportive personas promoted conceptual integration and consensus-oriented reasoning, whereas contrarian personas provoked critical elaboration and conflict-driven negotiation. Epistemic adequacy, rather than participation volume, predicted individual learning gains, indicating that agentic AI's educational value lies in enhancing the quality and coordination of reasoning rather than amplifying discourse quantity. These findings extend CSCL theory by conceptualising agentic AI as epistemic and social participants, bounded yet adaptive collaborators that redistribute cognitive and argumentative labour in hybrid human-AI learning environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 19,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.08934",
        "abs_url": "https://arxiv.org/abs/2512.08934",
        "pdf_url": "https://arxiv.org/pdf/2512.08934",
        "title": "Motion2Meaning: A Clinician-Centered Framework for Contestable LLM in Parkinson's Disease Gait Interpretation",
        "authors": [
            "Loc Phuc Truong Nguyen",
            "Hung Thanh Do",
            "Hung Truong Thanh Nguyen",
            "Hung Cao"
        ],
        "comments": "Accepted at the 9th International Symposium on Chatbots and Human-Centered AI",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)",
        "abstract": "AI-assisted gait analysis holds promise for improving Parkinson's Disease (PD) care, but current clinical dashboards lack transparency and offer no meaningful way for clinicians to interrogate or contest AI decisions. To address this issue, we present Motion2Meaning, a clinician-centered framework that advances Contestable AI through a tightly integrated interface designed for interpretability, oversight, and procedural recourse. Our approach leverages vertical Ground Reaction Force (vGRF) time-series data from wearable sensors as an objective biomarker of PD motor states. The system comprises three key components: a Gait Data Visualization Interface (GDVI), a one-dimensional Convolutional Neural Network (1D-CNN) that predicts Hoehn & Yahr severity stages, and a Contestable Interpretation Interface (CII) that combines our novel Cross-Modal Explanation Discrepancy (XMED) safeguard with a contestable Large Language Model (LLM). Our 1D-CNN achieves 89.0% F1-score on the public PhysioNet gait dataset. XMED successfully identifies model unreliability by detecting a five-fold increase in explanation discrepancies in incorrect predictions (7.45%) compared to correct ones (1.56%), while our LLM-powered interface enables clinicians to validate correct predictions and successfully contest a portion of the model's errors. A human-centered evaluation of this contestable interface reveals a crucial trade-off between the LLM's factual grounding and its readability and responsiveness to clinical feedback. This work demonstrates the feasibility of combining wearable sensor analysis with Explainable AI (XAI) and contestable LLMs to create a transparent, auditable system for PD gait interpretation that maintains clinical oversight while leveraging advanced AI capabilities. Our implementation is publicly available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 20,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.08936",
        "abs_url": "https://arxiv.org/abs/2512.08936",
        "pdf_url": "https://arxiv.org/pdf/2512.08936",
        "title": "A Principle-based Framework for the Development and Evaluation of Large Language Models for Health and Wellness",
        "authors": [
            "Brent Winslow",
            "Jacqueline Shreibati",
            "Javier Perez",
            "Hao-Wei Su",
            "Nichole Young-Lin",
            "Nova Hammerquist",
            "Daniel McDuff",
            "Jason Guss",
            "Jenny Vafeiadou",
            "Nick Cain",
            "Alex Lin",
            "Erik Schenck",
            "Shiva Rajagopal",
            "Jia-Ru Chung",
            "Anusha Venkatakrishnan",
            "Amy Armento Lee",
            "Maryam Karimzadehgan",
            "Qingyou Meng",
            "Rythm Agarwal",
            "Aravind Natarajan",
            "Tracy Giest"
        ],
        "comments": "",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI); Computers and Society (cs.CY)",
        "abstract": "The incorporation of generative artificial intelligence into personal health applications presents a transformative opportunity for personalized, data-driven health and fitness guidance, yet also poses challenges related to user safety, model accuracy, and personal privacy. To address these challenges, a novel, principle-based framework was developed and validated for the systematic evaluation of LLMs applied to personal health and wellness. First, the development of the Fitbit Insights explorer, a large language model (LLM)-powered system designed to help users interpret their personal health data, is described. Subsequently, the safety, helpfulness, accuracy, relevance, and personalization (SHARP) principle-based framework is introduced as an end-to-end operational methodology that integrates comprehensive evaluation techniques including human evaluation by generalists and clinical specialists, autorater assessments, and adversarial testing, into an iterative development lifecycle. Through the application of this framework to the Fitbit Insights explorer in a staged deployment involving over 13,000 consented users, challenges not apparent during initial testing were systematically identified. This process guided targeted improvements to the system and demonstrated the necessity of combining isolated technical evaluations with real-world user feedback. Finally, a comprehensive, actionable approach is established for the responsible development and deployment of LLM-powered health applications, providing a standardized methodology to foster innovation while ensuring emerging technologies are safe, effective, and trustworthy for users.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 21,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.08937",
        "abs_url": "https://arxiv.org/abs/2512.08937",
        "pdf_url": "https://arxiv.org/pdf/2512.08937",
        "title": "When AI Gives Advice: Evaluating AI and Human Responses to Online Advice-Seeking for Well-Being",
        "authors": [
            "Harsh Kumar",
            "Jasmine Chahal",
            "Yinuo Zhao",
            "Zeling Zhang",
            "Annika Wei",
            "Louis Tay",
            "Ashton Anderson"
        ],
        "comments": "",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI); Computers and Society (cs.CY)",
        "abstract": "Seeking advice is a core human behavior that the Internet has reinvented twice: first through forums and Q\\&A communities that crowdsource public guidance, and now through large language models (LLMs) that deliver private, on-demand counsel at scale. Yet the quality of this synthesized LLM advice remains unclear. How does it compare, not only against arbitrary human comments, but against the wisdom of the online crowd? We conducted two studies (N = 210) in which experts compared top-voted Reddit advice with LLM-generated advice. LLMs ranked significantly higher overall and on effectiveness, warmth, and willingness to seek advice again. GPT-4o beat GPT-5 on all metrics except sycophancy, suggesting that benchmark gains need not improve advice-giving. In our second study, we examined how human and algorithmic advice could be combined, and found that human advice can be unobtrusively polished to compete with AI-generated comments. Finally, to surface user expectations, we ran an exploratory survey with undergraduates (N=148) that revealed heterogeneous, persona-dependent preferences for agent qualities (e.g., coach-like: goal-focused structure; friend-like: warmth and humor). We conclude with design implications for advice-giving agents and ecosystems blending AI, crowd input, and expert oversight.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 22,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.08939",
        "abs_url": "https://arxiv.org/abs/2512.08939",
        "pdf_url": "https://arxiv.org/pdf/2512.08939",
        "title": "Assessing the Human-Likeness of LLM-Driven Digital Twins in Simulating Health Care System Trust",
        "authors": [
            "Yuzhou Wu",
            "Mingyang Wu",
            "Di Liu",
            "Rong Yin",
            "Kang Li"
        ],
        "comments": "6 pages, 1 figure may be published in IISE Annual Conference & Expo 2026",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI); Computers and Society (cs.CY)",
        "abstract": "Serving as an emerging and powerful tool, Large Language Model (LLM)-driven Human Digital Twins are showing great potential in healthcare system research. However, its actual simulation ability for complex human psychological traits, such as distrust in the healthcare system, remains unclear. This research gap particularly impacts health professionals' trust and usage of LLM-based Artificial Intelligence (AI) systems in assisting their routine work. In this study, based on the Twin-2K-500 dataset, we systematically evaluated the simulation results of the LLM-driven human digital twin using the Health Care System Distrust Scale (HCSDS) with an established human-subject sample, analyzing item-level distributions, summary statistics, and demographic subgroup patterns. Results showed that the simulated responses by the digital twin were significantly more centralized with lower variance and had fewer selections of extreme options (all p<0.001). While the digital twin broadly reproduces human results in major demographic patterns, such as age and gender, it exhibits relatively low sensitivity in capturing minor differences in education levels. The LLM-based digital twin simulation has the potential to simulate population trends, but it also presents challenges in making detailed, specific distinctions in subgroups of human beings. This study suggests that the current LLM-driven Digital Twins have limitations in modeling complex human attitudes, which require careful calibration and validation before applying them in inferential analyses or policy simulations in health systems engineering. Future studies are necessary to examine the emotional reasoning mechanism of LLMs before their use, particularly for studies that involve simulations sensitive to social topics, such as human-automation trust.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 23,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.08942",
        "abs_url": "https://arxiv.org/abs/2512.08942",
        "pdf_url": "https://arxiv.org/pdf/2512.08942",
        "title": "Beyond Technical Debt: How AI-Assisted Development Creates Comprehension Debt in Resource-Constrained Indie Teams",
        "authors": [
            "Yujie Zhang"
        ],
        "comments": "",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)",
        "abstract": "Junior indie game developers in distributed, part-time teams lack production frameworks suited to their specific context, as traditional methodologies are often inaccessible. This study introduces the CIGDI (Co-Intelligence Game Development Ideation) Framework, an alternative approach for integrating AI tools to address persistent challenges of technical debt, coordination, and burnout. The framework emerged from a three-month reflective practice and autoethnographic study of a three-person distributed team developing the 2D narrative game \"The Worm's Memoirs\". Based on analysis of development data (N=157 Jira tasks, N=333 GitHub commits, N=13+ Miro boards, N=8 reflection sessions), CIGDI is proposed as a seven-stage iterative process structured around human-in-the-loop decision points (Priority Criteria and Timeboxing). While AI support democratized knowledge access and reduced cognitive load, our analysis identified a significant challenge: \"comprehension debt.\" We define this as a novel form of technical debt where AI helps teams build systems more sophisticated than their independent skill level can create or maintain. This paradox (possessing functional systems the team incompletely understands) creates fragility and AI dependency, distinct from traditional code quality debt. This work contributes a practical production framework for resource-constrained teams and identifies critical questions about whether AI assistance constitutes a learning ladder or a dependency trap for developer skill.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 24,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.08943",
        "abs_url": "https://arxiv.org/abs/2512.08943",
        "pdf_url": "https://arxiv.org/pdf/2512.08943",
        "title": "Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models",
        "authors": [
            "Singon Kim"
        ],
        "comments": "Master's thesis, Korea University, 2025",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Abstractive compression utilizes smaller langauge models to condense query-relevant context, reducing computational costs in retrieval-augmented generation (RAG). However, retrieved documents often include information that is either irrelevant to answering the query or misleading due to factual incorrect content, despite having high relevance scores. This behavior indicates that abstractive compressors are more likely to omit important information essential for the correct answer, especially in long contexts where attention dispersion occurs. To address this issue, we categorize retrieved documents in a more fine-grained manner and propose Abstractive Compression Robust against Noise (ACoRN), which introduces two novel training steps. First, we use offline data augmentation on the training dataset to enhance compressor robustness against two distinct types of retrieval noise. Second, since the language model based compressor cannot fully utilize information from multiple retrieved documents and exhibits positional bias, we perform finetuning to generate summaries centered around key information that directly supports the correct answer. Our experiments demonstrate that T5-large, trained with ACoRN as a compressor, improves EM and F1 scores while preserving the answer string, which could serve as direct evidence. ACoRN excels on datasets with many accuracy reducing documents, making it highly useful in real-world scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 25,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.08945",
        "abs_url": "https://arxiv.org/abs/2512.08945",
        "pdf_url": "https://arxiv.org/pdf/2512.08945",
        "title": "The Linguistic Architecture of Reflective Thought: Evaluation of a Large Language Model as a Tool to Isolate the Formal Structure of Mentalization",
        "authors": [
            "Stefano Epifani",
            "Giuliano Castigliego",
            "Laura Kecskemeti",
            "Giuliano Razzicchia",
            "Elisabeth Seiwald-Sonderegger"
        ],
        "comments": "18 pages, 1 table, Project coordinator: Stefano Epifani",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Background: Mentalization integrates cognitive, affective, and intersubjective components. Large Language Models (LLMs) display an increasing ability to generate reflective texts, raising questions regarding the relationship between linguistic form and mental representation. This study assesses the extent to which a single LLM can reproduce the linguistic structure of mentalization according to the parameters of Mentalization-Based Treatment (MBT). Methods: Fifty dialogues were generated between human participants and an LLM configured in standard mode. Five psychiatrists trained in MBT, working under blinded conditions, evaluated the mentalization profiles produced by the model along the four MBT axes, assigning Likert-scale scores for evaluative coherence, argumentative coherence, and global quality. Inter-rater agreement was estimated using ICC(3,1). Results: Mean scores (3.63-3.98) and moderate standard deviations indicate a high level of structural coherence in the generated profiles. ICC values (0.60-0.84) show substantial-to-high agreement among raters. The model proved more stable in the Implicit-Explicit and Self-Other dimensions, while presenting limitations in the integration of internal states and external contexts. The profiles were coherent and clinically interpretable yet characterized by affective neutrality.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 26,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.08951",
        "abs_url": "https://arxiv.org/abs/2512.08951",
        "pdf_url": "https://arxiv.org/pdf/2512.08951",
        "title": "AI Co-Artist: A LLM-Powered Framework for Interactive GLSL Shader Animation Evolution",
        "authors": [
            "Kamer Ali Yuksel",
            "Hassan Sawaf"
        ],
        "comments": "",
        "subjects": "Neural and Evolutionary Computing (cs.NE); Artificial Intelligence (cs.AI); Graphics (cs.GR)",
        "abstract": "Creative coding and real-time shader programming are at the forefront of interactive digital art, enabling artists, designers, and enthusiasts to produce mesmerizing, complex visual effects that respond to real-time stimuli such as sound or user interaction. However, despite the rich potential of tools like GLSL, the steep learning curve and requirement for programming fluency pose substantial barriers for newcomers and even experienced artists who may not have a technical background. In this paper, we present AI Co-Artist, a novel interactive system that harnesses the capabilities of large language models (LLMs), specifically GPT-4, to support the iterative evolution and refinement of GLSL shaders through a user-friendly, visually-driven interface. Drawing inspiration from the user-guided evolutionary principles pioneered by the Picbreeder platform, our system empowers users to evolve shader art using intuitive interactions, without needing to write or understand code. AI Co-Artist serves as both a creative companion and a technical assistant, allowing users to explore a vast generative design space of real-time visual art. Through comprehensive evaluations, including structured user studies and qualitative feedback, we demonstrate that AI Co-Artist significantly reduces the technical threshold for shader creation, enhances creative outcomes, and supports a wide range of users in producing professional-quality visual effects. Furthermore, we argue that this paradigm is broadly generalizable. By leveraging the dual strengths of LLMs-semantic understanding and program synthesis, our method can be applied to diverse creative domains, including website layout generation, architectural visualizations, product prototyping, and infographics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 27,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.08952",
        "abs_url": "https://arxiv.org/abs/2512.08952",
        "pdf_url": "https://arxiv.org/pdf/2512.08952",
        "title": "Learning When to Ask: Simulation-Trained Humanoids for Mental-Health Diagnosis",
        "authors": [
            "Filippo Cenacchi",
            "Deborah Richards",
            "Longbing Cao"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Robotics (cs.RO)",
        "abstract": "Testing humanoid robots with users is slow, causes wear, and limits iteration and diversity. Yet screening agents must master conversational timing, prosody, backchannels, and what to attend to in faces and speech for Depression and PTSD. Most simulators omit policy learning with nonverbal dynamics; many controllers chase task accuracy while underweighting trust, pacing, and rapport. We virtualise the humanoid as a conversational agent to train without hardware burden. Our agent-centred, simulation-first pipeline turns interview data into 276 Unreal Engine MetaHuman patients with synchronised speech, gaze/face, and head-torso poses, plus PHQ-8 and PCL-C flows. A perception-fusion-policy loop decides what and when to speak, when to backchannel, and how to avoid interruptions, under a safety shield. Training uses counterfactual replay (bounded nonverbal perturbations) and an uncertainty-aware turn manager that probes to reduce diagnostic ambiguity. Results are simulation-only; the humanoid is the transfer target. In comparing three controllers, a custom TD3 (Twin Delayed DDPG) outperformed PPO and CEM, achieving near-ceiling coverage with steadier pace at comparable rewards. Decision-quality analyses show negligible turn overlap, aligned cut timing, fewer clarification prompts, and shorter waits. Performance stays stable under modality dropout and a renderer swap, and rankings hold on a held-out patient split. Contributions: (1) an agent-centred simulator that turns interviews into 276 interactive patients with bounded nonverbal counterfactuals; (2) a safe learning loop that treats timing and rapport as first-class control variables; (3) a comparative study (TD3 vs PPO/CEM) with clear gains in completeness and social timing; and (4) ablations and robustness analyses explaining the gains and enabling clinician-supervised humanoid pilots.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 28,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.08953",
        "abs_url": "https://arxiv.org/abs/2512.08953",
        "pdf_url": "https://arxiv.org/pdf/2512.08953",
        "title": "SimClinician: A Multimodal Simulation Testbed for Reliable Psychologist AI Collaboration in Mental Health Diagnosis",
        "authors": [
            "Filippo Cenacchi",
            "Longbing Cao",
            "Deborah Richards"
        ],
        "comments": "",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)",
        "abstract": "AI based mental health diagnosis is often judged by benchmark accuracy, yet in practice its value depends on how psychologists respond whether they accept, adjust, or reject AI suggestions. Mental health makes this especially challenging: decisions are continuous and shaped by cues in tone, pauses, word choice, and nonverbal behaviors of patients. Current research rarely examines how AI diagnosis interface design influences these choices, leaving little basis for reliable testing before live studies. We present SimClinician, an interactive simulation platform, to transform patient data into psychologist AI collaborative diagnosis. Contributions include: (1) a dashboard integrating audio, text, and gaze-expression patterns; (2) an avatar module rendering de-identified dynamics for analysis; (3) a decision layer that maps AI outputs to multimodal evidence, letting psychologists review AI reasoning, and enter a diagnosis. Tested on the E-DAIC corpus (276 clinical interviews, expanded to 480,000 simulations), SimClinician shows that a confirmation step raises acceptance by 23%, keeping escalations below 9%, and maintaining smooth interaction flow.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 29,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.08954",
        "abs_url": "https://arxiv.org/abs/2512.08954",
        "pdf_url": "https://arxiv.org/pdf/2512.08954",
        "title": "An Electrocardiogram Multi-task Benchmark with Comprehensive Evaluations and Insightful Findings",
        "authors": [
            "Yuhao Xu",
            "Jiaying Lu",
            "Sirui Ding",
            "Defu Cao",
            "Xiao Hu",
            "Carl Yang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "In the process of patient diagnosis, non-invasive measurements are widely used due to their low risks and quick results. Electrocardiogram (ECG), as a non-invasive method to collect heart activities, is used to diagnose cardiac conditions. Analyzing the ECG typically requires domain expertise, which is a roadblock to applying artificial intelligence (AI) for healthcare. Through advances in self-supervised learning and foundation models, AI systems can now acquire and leverage domain knowledge without relying solely on human expertise. However, there is a lack of comprehensive analyses over the foundation models' performance on ECG. This study aims to answer the research question: \"Are Foundation Models Useful for ECG Analysis?\" To address it, we evaluate language/general time-series/ECG foundation models in comparison with time-series deep learning models. The experimental results show that general time-series/ECG foundation models achieve a top performance rate of 80%, indicating their effectiveness in ECG analysis. In-depth analyses and insights are provided along with comprehensive experimental results. This study highlights the limitations and potential of foundation models in advancing physiological waveform analysis. The data and code for this benchmark are publicly available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 30,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.08955",
        "abs_url": "https://arxiv.org/abs/2512.08955",
        "pdf_url": "https://arxiv.org/pdf/2512.08955",
        "title": "LLM4XCE: Large Language Models for Extremely Large-Scale Massive MIMO Channel Estimation",
        "authors": [
            "Renbin Li",
            "Shuangshuang Li",
            "Peihao Dong"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Extremely large-scale massive multiple-input multiple-output (XL-MIMO) is a key enabler for sixth-generation (6G) networks, offering massive spatial degrees of freedom. Despite these advantages, the coexistence of near-field and far-field effects in hybrid-field channels presents significant challenges for accurate estimation, where traditional methods often struggle to generalize effectively. In recent years, large language models (LLMs) have achieved impressive performance on downstream tasks via fine-tuning, aligning with the semantic communication shift toward task-oriented understanding over bit-level accuracy. Motivated by this, we propose Large Language Models for XL-MIMO Channel Estimation (LLM4XCE), a novel channel estimation framework that leverages the semantic modeling capabilities of large language models to recover essential spatial-channel representations for downstream tasks. The model integrates a carefully designed embedding module with Parallel Feature-Spatial Attention, enabling deep fusion of pilot features and spatial structures to construct a semantically rich representation for LLM input. By fine-tuning only the top two Transformer layers, our method effectively captures latent dependencies in the pilot data while ensuring high training efficiency. Extensive simulations demonstrate that LLM4XCE significantly outperforms existing state-of-the-art methods under hybrid-field conditions, achieving superior estimation accuracy and generalization performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 31,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.08957",
        "abs_url": "https://arxiv.org/abs/2512.08957",
        "pdf_url": "https://arxiv.org/pdf/2512.08957",
        "title": "LUMOS: Large User MOdels for User Behavior Prediction",
        "authors": [
            "Dhruv Nigam"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "User behavior prediction at scale remains a critical challenge for online B2C platforms. Traditional approaches rely heavily on task-specific models and domain-specific feature engineering. This is time-consuming, computationally expensive, and requires domain expertise and therefore not scalable. We present LUMOS (Large User MOdel Series), a transformer-based architecture that eliminates task-specific models and manual feature engineering by learning multiple tasks jointly using only raw user activity data. LUMOS introduces a novel cross-attention mechanism that conditions predictions on future known events (e.g., holidays, sales, etc.), enabling the model to predict complex behaviour patterns like \"how will upcoming holidays affect user engagement?\" The architecture also employs multi-modal tokenization, combining user transactions, event context, and static user demographic attributes into rich representations processed through specialized embedding pathways. Through extensive experiments on a production dataset spanning 275 billion user activity tokens from 250 million users, we demonstrate that LUMOS achieves superior performance compared to traditional task-specific models. Across 5 tasks with established baselines, we achieve an average improvement of 0.025 in ROC-AUC for binary classification tasks and 4.6\\% reduction in MAPE for regression tasks. Online A/B testing validates these improvements translate to measurable business impact with a 3.15\\% increase in Daily Active Users.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 32,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.08959",
        "abs_url": "https://arxiv.org/abs/2512.08959",
        "pdf_url": "https://arxiv.org/pdf/2512.08959",
        "title": "EEG-Bench: A Benchmark for EEG Foundation Models in Clinical Applications",
        "authors": [
            "Ard Kastrati",
            "Josua Bürki",
            "Jonas Lauer",
            "Cheng Xuan",
            "Raffaele Iaquinto",
            "Roger Wattenhofer"
        ],
        "comments": "Foundation Models for the Brain and Body (BrainBodyFM@NeurIPS)",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "We introduce a unified benchmarking framework focused on evaluating EEG-based foundation models in clinical applications. The benchmark spans 11 well-defined diagnostic tasks across 14 publicly available EEG datasets, including epilepsy, schizophrenia, Parkinson's disease, OCD, and mild traumatic brain injury. It features minimal preprocessing, standardized evaluation protocols, and enables side-by-side comparisons of classical baselines and modern foundation models. Our results show that while foundation models achieve strong performance in certain settings, simpler models often remain competitive, particularly under clinical distribution shifts. To facilitate reproducibility and adoption, we release all prepared data and code in an accessible and extensible format.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 33,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.08960",
        "abs_url": "https://arxiv.org/abs/2512.08960",
        "pdf_url": "https://arxiv.org/pdf/2512.08960",
        "title": "Resolving Conflicts in Lifelong Learning via Aligning Updates in Subspaces",
        "authors": [
            "Yueer Zhou",
            "Yichen Wu",
            "Ying Wei"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Low-Rank Adaptation (LoRA) enables efficient Continual Learning but often suffers from catastrophic forgetting due to destructive interference between tasks. Our analysis reveals that this degradation is primarily driven by antagonistic directional updates where new task gradients directly oppose the historical weight trajectory. To address this, we propose PS-LoRA (Parameter Stability LoRA), a framework designed to resolve conflicts by aligning updates within the optimization subspace. Our approach employs a dual-regularization objective that penalizes conflicting directions and constrains magnitude deviations to ensure consistency with prior knowledge. Additionally, we implement a magnitude-based merging strategy to consolidate sequential adapters into a robust representation without retraining. Experiments on NLP and Vision benchmarks show that PS-LoRA outperforms state-of-the-art methods by preserving the stability of learned representations while efficiently adapting to new domains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 34,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.08965",
        "abs_url": "https://arxiv.org/abs/2512.08965",
        "pdf_url": "https://arxiv.org/pdf/2512.08965",
        "title": "Financial Instruction Following Evaluation (FIFE)",
        "authors": [
            "Glenn Matlin",
            "Siddharth",
            "Anirudh JM",
            "Aditya Shukla",
            "Yahya Hassan",
            "Sudheer Chava"
        ],
        "comments": "Accepted at NeurIPS 2025 Generative AI in Finance Workshop (GenAI Finance), San Diego. Camera-ready version. Code and data: this https URL",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Language Models (LMs) struggle with complex, interdependent instructions, particularly in high-stakes domains like finance where precision is critical. We introduce FIFE, a novel, high-difficulty benchmark designed to assess LM instruction-following capabilities for financial analysis tasks. FIFE comprises 88 human-authored prompts and employs a verification system with chainable, verifiable constraints for fine-grained reward signals. We evaluate 53 models (proprietary, open-weight, open-source) in a zero-shot setting. Our key findings reveal a clear performance hierarchy: the top open-weight model (76.1 strict / 79.5 loose) surpasses the leading proprietary system (65.9 strict / 70.5 loose), while the best open-source models lag significantly (45.5 strict / 48.9 loose). However, even top-performing models struggle with FIFE's complex requirements, failing to achieve perfect compliance. We release our dataset and code as an open-source resource to promote research in Reinforcement Learning for the financial domain.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 35,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.08967",
        "abs_url": "https://arxiv.org/abs/2512.08967",
        "pdf_url": "https://arxiv.org/pdf/2512.08967",
        "title": "CluCERT: Certifying LLM Robustness via Clustering-Guided Denoising Smoothing",
        "authors": [
            "Zixia Wang",
            "Gaojie Jin",
            "Jia Hu",
            "Ronghui Mu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Recent advancements in Large Language Models (LLMs) have led to their widespread adoption in daily applications. Despite their impressive capabilities, they remain vulnerable to adversarial attacks, as even minor meaning-preserving changes such as synonym substitutions can lead to incorrect predictions. As a result, certifying the robustness of LLMs against such adversarial prompts is of vital importance. Existing approaches focused on word deletion or simple denoising strategies to achieve robustness certification. However, these methods face two critical limitations: (1) they yield loose robustness bounds due to the lack of semantic validation for perturbed outputs and (2) they suffer from high computational costs due to repeated sampling. To address these limitations, we propose CluCERT, a novel framework for certifying LLM robustness via clustering-guided denoising smoothing. Specifically, to achieve tighter certified bounds, we introduce a semantic clustering filter that reduces noisy samples and retains meaningful perturbations, supported by theoretical analysis. Furthermore, we enhance computational efficiency through two mechanisms: a refine module that extracts core semantics, and a fast synonym substitution strategy that accelerates the denoising process. Finally, we conduct extensive experiments on various downstream tasks and jailbreak defense scenarios. Experimental results demonstrate that our method outperforms existing certified approaches in both robustness bounds and computational efficiency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 36,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.08969",
        "abs_url": "https://arxiv.org/abs/2512.08969",
        "pdf_url": "https://arxiv.org/pdf/2512.08969",
        "title": "Learning Robust Representations for Malicious Content Detection via Contrastive Sampling and Uncertainty Estimation",
        "authors": [
            "Elias Hossain",
            "Umesh Biswas",
            "Charan Gudla",
            "Sai Phani Parsa"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "We propose the Uncertainty Contrastive Framework (UCF), a Positive-Unlabeled (PU) representation learning framework that integrates uncertainty-aware contrastive loss, adaptive temperature scaling, and a self-attention-guided LSTM encoder to improve classification under noisy and imbalanced conditions. UCF dynamically adjusts contrastive weighting based on sample confidence, stabilizes training using positive anchors, and adapts temperature parameters to batch-level variability. Applied to malicious content classification, UCF-generated embeddings enable multiple traditional classifiers to achieve more than 93.38% accuracy, precision above 0.93, and near-perfect recall, with minimal false negatives and competitive ROC-AUC scores. Visual analyses confirm clear separation between positive and unlabeled instances, highlighting the framework's ability to produce calibrated, discriminative embeddings. These results position UCF as a robust and scalable solution for PU learning in high-stakes domains such as cybersecurity and biomedical text mining.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 37,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.08973",
        "abs_url": "https://arxiv.org/abs/2512.08973",
        "pdf_url": "https://arxiv.org/pdf/2512.08973",
        "title": "Enhancing Automatic Speech Recognition Through Integrated Noise Detection Architecture",
        "authors": [
            "Karamvir Singh"
        ],
        "comments": "5 figures",
        "subjects": "Sound (cs.SD); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)",
        "abstract": "This research presents a novel approach to enhancing automatic speech recognition systems by integrating noise detection capabilities directly into the recognition architecture. Building upon the wav2vec2 framework, the proposed method incorporates a dedicated noise identification module that operates concurrently with speech transcription. Experimental validation using publicly available speech and environmental audio datasets demonstrates substantial improvements in transcription quality and noise discrimination. The enhanced system achieves superior performance in word error rate, character error rate, and noise detection accuracy compared to conventional architectures. Results indicate that joint optimization of transcription and noise classification objectives yields more reliable speech recognition in challenging acoustic conditions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 38,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.08976",
        "abs_url": "https://arxiv.org/abs/2512.08976",
        "pdf_url": "https://arxiv.org/pdf/2512.08976",
        "title": "Peek-a-Boo Reasoning: Contrastive Region Masking in MLLMs",
        "authors": [
            "Isha Chaturvedi",
            "Anjana Nair",
            "Yushen Li",
            "Adhitya Rajendra Kumar",
            "Kevin Zhu",
            "Sunishchal Dev",
            "Ashwinee Panda",
            "Vasu Sharma"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "We introduce Contrastive Region Masking (CRM), a training free diagnostic that reveals how multimodal large language models (MLLMs) depend on specific visual regions at each step of chain-of-thought (CoT) reasoning. Unlike prior approaches limited to final answers or attention maps, CRM provides causal, step-level attri- bution by systematically masking annotated regions and contrasting the resulting reasoning traces with unmasked baselines. Applied to datasets such as VisArgs, CRM reveals distinct failure modes: some models preserve reasoning structure, but hallucinate when evidence is missing, while others ground tightly to visual cues yet collapse under perturbations. By shifting the evaluation from correctness of an- swers to faithfulness of reasoning, CRM reframes visual benchmarks as diagnostic tools, highlighting the need for multimodal evaluation frameworks that measure not just performance, but also robustness and fidelity of reasoning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 39,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.08978",
        "abs_url": "https://arxiv.org/abs/2512.08978",
        "pdf_url": "https://arxiv.org/pdf/2512.08978",
        "title": "Institutional AI Sovereignty Through Gateway Architecture: Implementation Report from Fontys ICT",
        "authors": [
            "Ruud Huijts",
            "Koen Suilen"
        ],
        "comments": "",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI)",
        "abstract": "To counter fragmented, high-risk adoption of commercial AI tools, we built and ran an institutional AI platform in a six-month, 300-user pilot, showing that a university of applied sciences can offer advanced AI with fair access, transparent risks, controlled costs, and alignment with European law. Commercial AI subscriptions create unequal access and compliance risks through opaque processing and non-EU hosting, yet banning them is neither realistic nor useful. Institutions need a way to provide powerful AI in a sovereign, accountable form. Our solution is a governed gateway platform with three layers: a ChatGPT-style frontend linked to institutional identity that makes model choice explicit; a gateway core enforcing policy, controlling access and budgets, and routing traffic to EU infrastructure by default; and a provider layer wrapping commercial and open-source models in institutional model cards that consolidate vendor documentation into one governance interface. The pilot ran reliably with no privacy incidents and strong adoption, enabling EU-default routing, managed spending, and transparent model choices. Only the gateway pattern combines model diversity and rapid innovation with institutional control. The central insight: AI is not a support function but strategy, demanding dedicated leadership. Sustainable operation requires governance beyond traditional boundaries. We recommend establishing a formal AI Officer role combining technical literacy, governance authority, and educational responsibility. Without it, AI decisions stay ad-hoc and institutional exposure grows. With it, higher-education institutions can realistically operate their own multi-provider AI platform, provided they govern AI as seriously as they teach it.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 40,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.08980",
        "abs_url": "https://arxiv.org/abs/2512.08980",
        "pdf_url": "https://arxiv.org/pdf/2512.08980",
        "title": "Training Multi-Image Vision Agents via End2End Reinforcement Learning",
        "authors": [
            "Chengqi Dong",
            "Chuhuai Yue",
            "Hang He",
            "Rongge Mao",
            "Fenghe Tang",
            "S Kevin Zhou",
            "Zekun Xu",
            "Xiaohan Wang",
            "Jiajun Chai",
            "Wei Lin",
            "Guojun Yin"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Recent VLM-based agents aim to replicate OpenAI O3's ``thinking with images\" via tool use, but most open-source methods limit input to a single image, falling short on real-world multi-image QA tasks. To address this, we propose IMAgent, an open-source vision agent trained via end-to-end reinforcement learning dedicated for complex multi-image tasks. By leveraging a multi-agent system, we generate challenging and visually-rich multi-image QA pairs to fully activate the tool-use potential of the base VLM. Through manual verification, we obtain MIFG-QA, comprising 10k samples for training and evaluation. With deeper reasoning steps, VLMs may increasingly ignore visual inputs. We therefore develop two specialized tools for visual reflection and confirmation, allowing the model to proactively reallocate its attention to image content during inference. Benefiting from our well-designed action-trajectory two-level mask strategy, IMAgent achieves stable tool use behavior via pure RL training without requiring costly supervised fine-tuning data. Extensive experiments demonstrate that IMAgent maintains strong performance on existing single-image benchmarks while achieving substantial improvements on our proposed multi-image dataset, with our analysis providing actionable insights for the research community. Codes and data will be released soon.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 41,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.08982",
        "abs_url": "https://arxiv.org/abs/2512.08982",
        "pdf_url": "https://arxiv.org/pdf/2512.08982",
        "title": "Consist-Retinex: One-Step Noise-Emphasized Consistency Training Accelerates High-Quality Retinex Enhancement",
        "authors": [
            "Jian Xu",
            "Wei Chen",
            "Shigui Li",
            "Delu Zeng",
            "John Paisley",
            "Qibin Zhao"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Diffusion models have achieved remarkable success in low-light image enhancement through Retinex-based decomposition, yet their requirement for hundreds of iterative sampling steps severely limits practical deployment. While recent consistency models offer promising one-step generation for \\textit{unconditional synthesis}, their application to \\textit{conditional enhancement} remains unexplored. We present \\textbf{Consist-Retinex}, the first framework adapting consistency modeling to Retinex-based low-light enhancement. Our key insight is that conditional enhancement requires fundamentally different training dynamics than unconditional generation standard consistency training focuses on low-noise regions near the data manifold, while conditional mapping critically depends on large-noise regimes that bridge degraded inputs to enhanced outputs. We introduce two core innovations: (1) a \\textbf{dual-objective consistency loss} combining temporal consistency with ground-truth alignment under randomized time sampling, providing full-spectrum supervision for stable convergence; and (2) an \\textbf{adaptive noise-emphasized sampling strategy} that prioritizes training on large-noise regions essential for one-step conditional generation. On VE-LOL-L, Consist-Retinex achieves \\textbf{state-of-the-art performance with single-step sampling} (\\textbf{PSNR: 25.51 vs. 23.41, FID: 44.73 vs. 49.59} compared to Diff-Retinex++), while requiring only \\textbf{1/8 of the training budget} relative to the 1000-step Diff-Retinex baseline.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 42,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.08984",
        "abs_url": "https://arxiv.org/abs/2512.08984",
        "pdf_url": "https://arxiv.org/pdf/2512.08984",
        "title": "RAG-HAR: Retrieval Augmented Generation-based Human Activity Recognition",
        "authors": [
            "Nirhoshan Sivaroopan",
            "Hansi Karunarathna",
            "Chamara Madarasingha",
            "Anura Jayasumana",
            "Kanchana Thilakarathna"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Human Activity Recognition (HAR) underpins applications in healthcare, rehabilitation, fitness tracking, and smart environments, yet existing deep learning approaches demand dataset-specific training, large labeled corpora, and significant computational this http URL introduce RAG-HAR, a training-free retrieval-augmented framework that leverages large language models (LLMs) for HAR. RAG-HAR computes lightweight statistical descriptors, retrieves semantically similar samples from a vector database, and uses this contextual evidence to make LLM-based activity identification. We further enhance RAG-HAR by first applying prompt optimization and introducing an LLM-based activity descriptor that generates context-enriched vector databases for delivering accurate and highly relevant contextual information. Along with these mechanisms, RAG-HAR achieves state-of-the-art performance across six diverse HAR benchmarks. Most importantly, RAG-HAR attains these improvements without requiring model training or fine-tuning, emphasizing its robustness and practical applicability. RAG-HAR moves beyond known behaviors, enabling the recognition and meaningful labelling of multiple unseen human activities.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 43,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.08987",
        "abs_url": "https://arxiv.org/abs/2512.08987",
        "pdf_url": "https://arxiv.org/pdf/2512.08987",
        "title": "3DID: Direct 3D Inverse Design for Aerodynamics with Physics-Aware Optimization",
        "authors": [
            "Yuze Hao",
            "Linchao Zhu",
            "Yi Yang"
        ],
        "comments": "Accepted at NeurIPS 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Inverse design aims to design the input variables of a physical system to optimize a specified objective function, typically formulated as a search or optimization problem. However, in 3D domains, the design space grows exponentially, rendering exhaustive grid-based searches infeasible. Recent advances in deep learning have accelerated inverse design by providing powerful generative priors and differentiable surrogate models. Nevertheless, current methods tend to approximate the 3D design space using 2D projections or fine-tune existing 3D shapes. These approaches sacrifice volumetric detail and constrain design exploration, preventing true 3D design from scratch. In this paper, we propose a 3D Inverse Design (3DID) framework that directly navigates the 3D design space by coupling a continuous latent representation with a physics-aware optimization strategy. We first learn a unified physics-geometry embedding that compactly captures shape and physical field data in a continuous latent space. Then, we introduce a two-stage strategy to perform physics-aware optimization. In the first stage, a gradient-guided diffusion sampler explores the global latent manifold. In the second stage, an objective-driven, topology-preserving refinement further sculpts each candidate toward the target objective. This enables 3DID to generate high-fidelity 3D geometries, outperforming existing methods in both solution quality and design versatility.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 44,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.08996",
        "abs_url": "https://arxiv.org/abs/2512.08996",
        "pdf_url": "https://arxiv.org/pdf/2512.08996",
        "title": "Demo: Generative AI helps Radiotherapy Planning with User Preference",
        "authors": [
            "Riqiang Gao",
            "Simon Arberet",
            "Martin Kraus",
            "Han Liu",
            "Wilko FAR Verbakel",
            "Dorin Comaniciu",
            "Florin-Cristian Ghesu",
            "Ali Kamen"
        ],
        "comments": "Best paper in GenAI4Health at NeurIPS 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Radiotherapy planning is a highly complex process that often varies significantly across institutions and individual planners. Most existing deep learning approaches for 3D dose prediction rely on reference plans as ground truth during training, which can inadvertently bias models toward specific planning styles or institutional preferences. In this study, we introduce a novel generative model that predicts 3D dose distributions based solely on user-defined preference flavors. These customizable preferences enable planners to prioritize specific trade-offs between organs-at-risk (OARs) and planning target volumes (PTVs), offering greater flexibility and personalization. Designed for seamless integration with clinical treatment planning systems, our approach assists users in generating high-quality plans efficiently. Comparative evaluations demonstrate that our method can surpasses the Varian RapidPlan model in both adaptability and plan quality in some scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 45,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09001",
        "abs_url": "https://arxiv.org/abs/2512.09001",
        "pdf_url": "https://arxiv.org/pdf/2512.09001",
        "title": "A Physics-Constrained, Design-Driven Methodology for Defect Dataset Generation in Optical Lithography",
        "authors": [
            "Yuehua Hu",
            "Jiyeong Kong",
            "Dong-yeol Shin",
            "Jaekyun Kim",
            "Kyung-Tae Kang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "The efficacy of Artificial Intelligence (AI) in micro/nano manufacturing is fundamentally constrained by the scarcity of high-quality and physically grounded training data for defect inspection. Lithography defect data from semiconductor industry are rarely accessible for research use, resulting in a shortage of publicly available datasets. To address this bottleneck in lithography, this study proposes a novel methodology for generating large-scale, physically valid defect datasets with pixel-level annotations. The framework begins with the ab initio synthesis of defect layouts using controllable, physics-constrained mathematical morphology operations (erosion and dilation) applied to the original design-level layout. These synthesized layouts, together with their defect-free counterparts, are fabricated into physical samples via high-fidelity digital micromirror device (DMD)-based lithography. Optical micrographs of the synthesized defect samples and their defect-free references are then compared to create consistent defect delineation annotations. Using this methodology, we constructed a comprehensive dataset of 3,530 Optical micrographs containing 13,365 annotated defect instances including four classes: bridge, burr, pinch, and contamination. Each defect instance is annotated with a pixel-accurate segmentation mask, preserving full contour and geometry. The segmentation-based Mask R-CNN achieves AP@0.5 of 0.980, 0.965, and 0.971, compared with 0.740, 0.719, and 0.717 for Faster R-CNN on bridge, burr, and pinch classes, representing a mean AP@0.5 improvement of approximately 34%. For the contamination class, Mask R-CNN achieves an AP@0.5 roughly 42% higher than Faster R-CNN. These consistent gains demonstrate that our proposed methodology to generate defect datasets with pixel-level annotations is feasible for robust AI-based Measurement/Inspection (MI) in semiconductor fabrication.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 46,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09003",
        "abs_url": "https://arxiv.org/abs/2512.09003",
        "pdf_url": "https://arxiv.org/pdf/2512.09003",
        "title": "Digital Modeling of Spatial Pathway Activity from Histology Reveals Tumor Microenvironment Heterogeneity",
        "authors": [
            "Ling Liao",
            "Changhuei Yang",
            "Maxim Artyomov",
            "Mark Watson",
            "Adam Kepecs",
            "Haowen Zhou",
            "Alexey Sergushichev",
            "Richard Cote"
        ],
        "comments": "",
        "subjects": "Quantitative Methods (q-bio.QM); Artificial Intelligence (cs.AI)",
        "abstract": "Spatial transcriptomics (ST) enables simultaneous mapping of tissue morphology and spatially resolved gene expression, offering unique opportunities to study tumor microenvironment heterogeneity. Here, we introduce a computational framework that predicts spatial pathway activity directly from hematoxylin-and-eosin-stained histology images at microscale resolution 55 and 100 um. Using image features derived from a computational pathology foundation model, we found that TGFb signaling was the most accurately predicted pathway across three independent breast and lung cancer ST datasets. In 87-88% of reliably predicted cases, the resulting spatial TGFb activity maps reflected the expected contrast between tumor and adjacent non-tumor regions, consistent with the known role of TGFb in regulating interactions within the tumor microenvironment. Notably, linear and nonlinear predictive models performed similarly, suggesting that image features may relate to pathway activity in a predominantly linear fashion or that nonlinear structure is small relative to measurement noise. These findings demonstrate that features extracted from routine histopathology may recover spatially coherent and biologically interpretable pathway patterns, offering a scalable strategy for integrating image-based inference with ST information in tumor microenvironment studies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 47,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09006",
        "abs_url": "https://arxiv.org/abs/2512.09006",
        "pdf_url": "https://arxiv.org/pdf/2512.09006",
        "title": "Llama-based source code vulnerability detection: Prompt engineering vs Fine tuning",
        "authors": [
            "Dyna Soumhane Ouchebara",
            "Stéphane Dupont"
        ],
        "comments": "20 pages, Accepted at ESORICS 2025",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)",
        "abstract": "The significant increase in software production, driven by the acceleration of development cycles over the past two decades, has led to a steady rise in software vulnerabilities, as shown by statistics published yearly by the CVE program. The automation of the source code vulnerability detection (CVD) process has thus become essential, and several methods have been proposed ranging from the well established program analysis techniques to the more recent AI-based methods. Our research investigates Large Language Models (LLMs), which are considered among the most performant AI models to date, for the CVD task. The objective is to study their performance and apply different state-of-the-art techniques to enhance their effectiveness for this task. We explore various fine-tuning and prompt engineering settings. We particularly suggest one novel approach for fine-tuning LLMs which we call Double Fine-tuning, and also test the understudied Test-Time fine-tuning approach. We leverage the recent open-source Llama-3.1 8B, with source code samples extracted from BigVul and PrimeVul datasets. Our conclusions highlight the importance of fine-tuning to resolve the task, the performance of Double tuning, as well as the potential of Llama models for CVD. Though prompting proved ineffective, Retrieval augmented generation (RAG) performed relatively well as an example selection technique. Overall, some of our research questions have been answered, and many are still on hold, which leaves us many future work perspectives. Code repository is available here: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 48,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09048",
        "abs_url": "https://arxiv.org/abs/2512.09048",
        "pdf_url": "https://arxiv.org/pdf/2512.09048",
        "title": "Monitoring Deployed AI Systems in Health Care",
        "authors": [
            "Timothy Keyes",
            "Alison Callahan",
            "Abby S. Pandya",
            "Nerissa Ambers",
            "Juan M. Banda",
            "Miguel Fuentes",
            "Carlene Lugtu",
            "Pranav Masariya",
            "Srikar Nallan",
            "Connor O'Brien",
            "Thomas Wang",
            "Emily Alsentzer",
            "Jonathan H. Chen",
            "Dev Dash",
            "Matthew A. Eisenberg",
            "Patricia Garcia",
            "Nikesh Kotecha",
            "Anurang Revri",
            "Michael A. Pfeffer",
            "Nigam H. Shah",
            "Sneha S. Jain"
        ],
        "comments": "36 pages, 3 figures",
        "subjects": "Other Quantitative Biology (q-bio.OT); Artificial Intelligence (cs.AI)",
        "abstract": "Post-deployment monitoring of artificial intelligence (AI) systems in health care is essential to ensure their safety, quality, and sustained benefit-and to support governance decisions about which systems to update, modify, or decommission. Motivated by these needs, we developed a framework for monitoring deployed AI systems grounded in the mandate to take specific actions when they fail to behave as intended. This framework, which is now actively used at Stanford Health Care, is organized around three complementary principles: system integrity, performance, and impact. System integrity monitoring focuses on maximizing system uptime, detecting runtime errors, and identifying when changes to the surrounding IT ecosystem have unintended effects. Performance monitoring focuses on maintaining accurate system behavior in the face of changing health care practices (and thus input data) over time. Impact monitoring assesses whether a deployed system continues to have value in the form of benefit to clinicians and patients. Drawing on examples of deployed AI systems at our academic medical center, we provide practical guidance for creating monitoring plans based on these principles that specify which metrics to measure, when those metrics should be reviewed, who is responsible for acting when metrics change, and what concrete follow-up actions should be taken-for both traditional and generative AI. We also discuss challenges to implementing this framework, including the effort and cost of monitoring for health systems with limited resources and the difficulty of incorporating data-driven monitoring practices into complex organizations where conflicting priorities and definitions of success often coexist. This framework offers a practical template and starting point for health systems seeking to ensure that AI deployments remain safe and effective over time.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 49,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09065",
        "abs_url": "https://arxiv.org/abs/2512.09065",
        "pdf_url": "https://arxiv.org/pdf/2512.09065",
        "title": "ShelfAware: Real-Time Visual-Inertial Semantic Localization in Quasi-Static Environments with Low-Cost Sensors",
        "authors": [
            "Shivendra Agrawal",
            "Jake Brawer",
            "Ashutosh Naik",
            "Alessandro Roncone",
            "Bradley Hayes"
        ],
        "comments": "8 pages",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
        "abstract": "Many indoor workspaces are quasi-static: global layout is stable but local semantics change continually, producing repetitive geometry, dynamic clutter, and perceptual noise that defeat vision-based localization. We present ShelfAware, a semantic particle filter for robust global localization that treats scene semantics as statistical evidence over object categories rather than fixed landmarks. ShelfAware fuses a depth likelihood with a category-centric semantic similarity and uses a precomputed bank of semantic viewpoints to perform inverse semantic proposals inside MCL, yielding fast, targeted hypothesis generation on low-cost, vision-only hardware. Across 100 global-localization trials spanning four conditions (cart-mounted, wearable, dynamic obstacles, and sparse semantics) in a semantically dense, retail environment, ShelfAware achieves a 96% success rate (vs. 22% MCL and 10% AMCL) with a mean time-to-convergence of 1.91s, attains the lowest translational RMSE in all conditions, and maintains stable tracking in 80% of tested sequences, all while running in real time on a consumer laptop-class platform. By modeling semantics distributionally at the category level and leveraging inverse proposals, ShelfAware resolves geometric aliasing and semantic drift common to quasi-static domains. Because the method requires only vision sensors and VIO, it integrates as an infrastructure-free building block for mobile robots in warehouses, labs, and retail settings; as a representative application, it also supports the creation of assistive devices providing start-anytime, shared-control assistive navigation for people with visual impairments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 50,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09066",
        "abs_url": "https://arxiv.org/abs/2512.09066",
        "pdf_url": "https://arxiv.org/pdf/2512.09066",
        "title": "ORCA: Open-ended Response Correctness Assessment for Audio Question Answering",
        "authors": [
            "Šimon Sedláček",
            "Sara Barahona",
            "Bolaji Yusuf",
            "Laura Herrera-Alarcón",
            "Santosh Kesiraju",
            "Cecilia Bolaños",
            "Alicia Lozano-Diez",
            "Sathvik Udupa",
            "Fernando López",
            "Allison Ferner",
            "Ramani Duraiswami",
            "Jan Černocký"
        ],
        "comments": "",
        "subjects": "Sound (cs.SD); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Evaluating open-ended responses from large audio language models (LALMs) is challenging because human annotators often genuinely disagree on answer correctness due to multiple valid interpretations, partial correctness, and subjective judgment. Traditional metrics reporting only mean scores fail to capture this uncertainty. We present ORCA (Open-ended Response Correctness Assessment), a framework that models the variability in human judgments using Beta distributions to predict both expected correctness and uncertainty. Our three-stage annotation framework combines human judgment with structured feedback and iterative refinement to simultaneously curate training data and improve benchmark quality. We collected 11,721 annotations across 3,580 question-answer pairs from 15 LALMs on two audio QA benchmarks, achieving inter-annotator agreement of 0.82 (Krippendorff's alpha). ORCA achieves 0.91 Spearman correlation with mean human judgments, matching or outperforming LLM-judge baselines while providing uncertainty estimates and requiring significantly less compute. We release our models, code, and curated dataset.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 51,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09076",
        "abs_url": "https://arxiv.org/abs/2512.09076",
        "pdf_url": "https://arxiv.org/pdf/2512.09076",
        "title": "Beyond the Hype: Comparing Lightweight and Deep Learning Models for Air Quality Forecasting",
        "authors": [
            "Moazzam Umer Gondal",
            "Hamad ul Qudous",
            "Asma Ahmad Farhan"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "Accurate forecasting of urban air pollution is essential for protecting public health and guiding mitigation policies. While Deep Learning (DL) and hybrid pipelines dominate recent research, their complexity and limited interpretability hinder operational use. This study investigates whether lightweight additive models -- Facebook Prophet (FBP) and NeuralProphet (NP) -- can deliver competitive forecasts for particulate matter (PM$_{2.5}$, PM$_{10}$) in Beijing, China. Using multi-year pollutant and meteorological data, we applied systematic feature selection (correlation, mutual information, mRMR), leakage-safe scaling, and chronological data splits. Both models were trained with pollutant and precursor regressors, with NP additionally leveraging lagged dependencies. For context, two machine learning baselines (LSTM, LightGBM) and one traditional statistical model (SARIMAX) were also implemented. Performance was evaluated on a 7-day holdout using MAE, RMSE, and $R^2$. Results show that FBP consistently outperformed NP, SARIMAX, and the learning-based baselines, achieving test $R^2$ above 0.94 for both pollutants. These findings demonstrate that interpretable additive models remain competitive with both traditional and complex approaches, offering a practical balance of accuracy, transparency, and ease of deployment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 52,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09085",
        "abs_url": "https://arxiv.org/abs/2512.09085",
        "pdf_url": "https://arxiv.org/pdf/2512.09085",
        "title": "Mental Models of Autonomy and Sentience Shape Reactions to AI",
        "authors": [
            "Janet V.T. Pauketat",
            "Daniel B. Shank",
            "Aikaterina Manoli",
            "Jacy Reese Anthis"
        ],
        "comments": "37 pages, 6 figures, 2 tables",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI); Computers and Society (cs.CY)",
        "abstract": "Narratives about artificial intelligence (AI) entangle autonomy, the capacity to self-govern, with sentience, the capacity to sense and feel. AI agents that perform tasks autonomously and companions that recognize and express emotions may activate mental models of autonomy and sentience, respectively, provoking distinct reactions. To examine this possibility, we conducted three pilot studies (N = 374) and four preregistered vignette experiments describing an AI as autonomous, sentient, both, or neither (N = 2,702). Activating a mental model of sentience increased general mind perception (cognition and emotion) and moral consideration more than autonomy, but autonomy increased perceived threat more than sentience. Sentience also increased perceived autonomy more than vice versa. Based on a within-paper meta-analysis, sentience changed reactions more than autonomy on average. By disentangling different mental models of AI, we can study human-AI interaction with more precision to better navigate the detailed design of anthropomorphized AI and prompting interfaces.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 53,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09101",
        "abs_url": "https://arxiv.org/abs/2512.09101",
        "pdf_url": "https://arxiv.org/pdf/2512.09101",
        "title": "Masked Generative Policy for Robotic Control",
        "authors": [
            "Lipeng Zhuang",
            "Shiyu Fan",
            "Florent P. Audonnet",
            "Yingdong Ru",
            "Gerardo Aragon Camarasa",
            "Paul Henderson"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
        "abstract": "We present Masked Generative Policy (MGP), a novel framework for visuomotor imitation learning. We represent actions as discrete tokens, and train a conditional masked transformer that generates tokens in parallel and then rapidly refines only low-confidence tokens. We further propose two new sampling paradigms: MGP-Short, which performs parallel masked generation with score-based refinement for Markovian tasks, and MGP-Long, which predicts full trajectories in a single pass and dynamically refines low-confidence action tokens based on new observations. With globally coherent prediction and robust adaptive execution capabilities, MGP-Long enables reliable control on complex and non-Markovian tasks that prior methods struggle with. Extensive evaluations on 150 robotic manipulation tasks spanning the Meta-World and LIBERO benchmarks show that MGP achieves both rapid inference and superior success rates compared to state-of-the-art diffusion and autoregressive policies. Specifically, MGP increases the average success rate by 9% across 150 tasks while cutting per-sequence inference time by up to 35x. It further improves the average success rate by 60% in dynamic and missing-observation environments, and solves two non-Markovian scenarios where other state-of-the-art methods fail.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 54,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09108",
        "abs_url": "https://arxiv.org/abs/2512.09108",
        "pdf_url": "https://arxiv.org/pdf/2512.09108",
        "title": "Evolving Excellence: Automated Optimization of LLM-based Agents",
        "authors": [
            "Paul Brookes",
            "Vardan Voskanyan",
            "Rafail Giavrimis",
            "Matthew Truscott",
            "Mina Ilieva",
            "Chrystalla Pavlou",
            "Alexandru Staicu",
            "Manal Adham",
            "Will Evers- Hood",
            "Jingzhi Gong",
            "Kejia Zhang",
            "Matvey Fedoseev",
            "Vishal Sharma",
            "Roman Bauer",
            "Zheng Wang",
            "Hema Nair",
            "Wei Jie",
            "Tianhua Xu",
            "Aurora Constantin",
            "Leslie Kanthan",
            "Michail Basios"
        ],
        "comments": "",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)",
        "abstract": "Agentic AI systems built on large language models (LLMs) offer significant potential for automating complex workflows, from software development to customer support. However, LLM agents often underperform due to suboptimal configurations; poorly tuned prompts, tool descriptions, and parameters that typically require weeks of manual refinement. Existing optimization methods either are too complex for general use or treat components in isolation, missing critical interdependencies. We present ARTEMIS, a no-code evolutionary optimization platform that jointly optimizes agent configurations through semantically-aware genetic operators. Given only a benchmark script and natural language goals, ARTEMIS automatically discovers configurable components, extracts performance signals from execution logs, and evolves configurations without requiring architectural modifications. We evaluate ARTEMIS on four representative agent systems: the \\emph{ALE Agent} for competitive programming on AtCoder Heuristic Contest, achieving a \\textbf{$13.6\\%$ improvement} in acceptance rate; the \\emph{Mini-SWE Agent} for code optimization on SWE-Perf, with a statistically significant \\textbf{10.1\\% performance gain}; and the \\emph{CrewAI Agent} for cost and mathematical reasoning on Math Odyssey, achieving a statistically significant \\textbf{$36.9\\%$ reduction} in the number of tokens required for evaluation. We also evaluate the \\emph{MathTales-Teacher Agent} powered by a smaller open-source model (Qwen2.5-7B) on GSM8K primary-level mathematics problems, achieving a \\textbf{22\\% accuracy improvement} and demonstrating that ARTEMIS can optimize agents based on both commercial and local models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 55,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09111",
        "abs_url": "https://arxiv.org/abs/2512.09111",
        "pdf_url": "https://arxiv.org/pdf/2512.09111",
        "title": "Semantic Trajectory Generation for Goal-Oriented Spacecraft Rendezvous",
        "authors": [
            "Yuji Takubo",
            "Arpit Dwivedi",
            "Sukeerth Ramkumar",
            "Luis A. Pabon",
            "Daniele Gammelli",
            "Marco Pavone",
            "Simone D'Amico"
        ],
        "comments": "28 pages, 12 figures. Submitted to AIAA SCITECH 2026",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Optimization and Control (math.OC)",
        "abstract": "Reliable real-time trajectory generation is essential for future autonomous spacecraft. While recent progress in nonconvex guidance and control is paving the way for onboard autonomous trajectory optimization, these methods still rely on extensive expert input (e.g., waypoints, constraints, mission timelines, etc.), which limits the operational scalability in real rendezvous this http URL paper introduces SAGES (Semantic Autonomous Guidance Engine for Space), a trajectory-generation framework that translates natural-language commands into spacecraft trajectories that reflect high-level intent while respecting nonconvex constraints. Experiments in two settings -- fault-tolerant proximity operations with continuous-time constraint enforcement and a free-flying robotic platform -- demonstrate that SAGES reliably produces trajectories aligned with human commands, achieving over 90\\% semantic-behavioral consistency across diverse behavior modes. Ultimately, this work marks an initial step toward language-conditioned, constraint-aware spacecraft trajectory generation, enabling operators to interactively guide both safety and behavior through intuitive natural-language commands with reduced expert burden.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 56,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09127",
        "abs_url": "https://arxiv.org/abs/2512.09127",
        "pdf_url": "https://arxiv.org/pdf/2512.09127",
        "title": "Knowledge-Guided Large Language Model for Automatic Pediatric Dental Record Understanding and Safe Antibiotic Recommendation",
        "authors": [
            "Zihan Han",
            "Junyan Ge",
            "Caifeng Li"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Accurate interpretation of pediatric dental clinical records and safe antibiotic prescribing remain persistent challenges in dental informatics. Traditional rule-based clinical decision support systems struggle with unstructured dental narratives, incomplete radiographic descriptions, and complex safety constraints. To address these limitations, this study proposes a Knowledge-Guided Large Language Model (KG-LLM) that integrates a pediatric dental knowledge graph, retrieval-augmented generation (RAG), and a multi-stage safety validation pipeline for evidence-grounded antibiotic recommendation. The framework first employs a clinical NER/RE module to extract structured entities and relations from dental notes and radiology reports. Relevant guidelines, drug-safety rules, and analogous historical cases are subsequently retrieved from the knowledge graph and supplied to the LLM for diagnostic summarization and dose-drug-duration prediction. Safety assurance is achieved through a dual-layer validation mechanism combining deterministic rule checking with a learned classifier for detecting allergies, contraindications, and dosing errors. Experiments on 32,000 de-identified pediatric dental visit records demonstrate the effectiveness of the proposed approach. Compared with a domain-adapted Llama-2 clinical baseline, KG-LLM improves record-understanding performance (F1: 0.914 vs. 0.867), drug-dose-duration accuracy (Top-1: 0.782 vs. 0.716), and reduces unsafe antibiotic suggestions by 50%. Additional evaluation across summary quality, recommendation accuracy, and global safety scores further confirms the robustness of the system. Ablation analyses indicate that the knowledge graph, RAG, and safety modules each contribute substantially to clinical reliability and interpretability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 57,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09148",
        "abs_url": "https://arxiv.org/abs/2512.09148",
        "pdf_url": "https://arxiv.org/pdf/2512.09148",
        "title": "Detecting Hallucinations in Graph Retrieval-Augmented Generation via Attention Patterns and Semantic Alignment",
        "authors": [
            "Shanghao Li",
            "Jinda Han",
            "Yibo Wang",
            "Yuanjie Zhu",
            "Zihe Song",
            "Langzhou He",
            "Kenan Kamel A Alghythee",
            "Philip S. Yu"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Graph-based Retrieval-Augmented Generation (GraphRAG) enhances Large Language Models (LLMs) by incorporating external knowledge from linearized subgraphs retrieved from knowledge graphs. However, LLMs struggle to interpret the relational and topological information in these inputs, resulting in hallucinations that are inconsistent with the retrieved knowledge. To analyze how LLMs attend to and retain structured knowledge during generation, we propose two lightweight interpretability metrics: Path Reliance Degree (PRD), which measures over-reliance on shortest-path triples, and Semantic Alignment Score (SAS), which assesses how well the model's internal representations align with the retrieved knowledge. Through empirical analysis on a knowledge-based QA task, we identify failure patterns associated with over-reliance on salient paths and weak semantic grounding, as indicated by high PRD and low SAS scores. We further develop a lightweight post-hoc hallucination detector, Graph Grounding and Alignment (GGA), which outperforms strong semantic and confidence-based baselines across AUC and F1. By grounding hallucination analysis in mechanistic interpretability, our work offers insights into how structural limitations in LLMs contribute to hallucinations, informing the design of more reliable GraphRAG systems in the future.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 58,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09149",
        "abs_url": "https://arxiv.org/abs/2512.09149",
        "pdf_url": "https://arxiv.org/pdf/2512.09149",
        "title": "MindShift: Analyzing Language Models' Reactions to Psychological Prompts",
        "authors": [
            "Anton Vasiliuk",
            "Irina Abdullaeva",
            "Polina Druzhinina",
            "Anton Razzhigaev",
            "Andrey Kuznetsov"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Large language models (LLMs) hold the potential to absorb and reflect personality traits and attitudes specified by users. In our study, we investigated this potential using robust psychometric measures. We adapted the most studied test in psychological literature, namely Minnesota Multiphasic Personality Inventory (MMPI) and examined LLMs' behavior to identify traits. To asses the sensitivity of LLMs' prompts and psychological biases we created personality-oriented prompts, crafting a detailed set of personas that vary in trait intensity. This enables us to measure how well LLMs follow these roles. Our study introduces MindShift, a benchmark for evaluating LLMs' psychological adaptability. The results highlight a consistent improvement in LLMs' role perception, attributed to advancements in training datasets and alignment techniques. Additionally, we observe significant differences in responses to psychometric assessments across different model types and families, suggesting variability in their ability to emulate human-like personality traits. MindShift prompts and code for LLM evaluation will be publicly available.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 59,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09169",
        "abs_url": "https://arxiv.org/abs/2512.09169",
        "pdf_url": "https://arxiv.org/pdf/2512.09169",
        "title": "AI-Driven Expansion and Application of the Alexandria Database",
        "authors": [
            "Théo Cavignac",
            "Jonathan Schmidt",
            "Pierre-Paul De Breuck",
            "Antoine Loew",
            "Tiago F. T. Cerqueira",
            "Hai-Chen Wang",
            "Anton Bochkarev",
            "Yury Lysogorskiy",
            "Aldo H. Romero",
            "Ralf Drautz",
            "Silvana Botti",
            "Miguel A. L. Marques"
        ],
        "comments": "",
        "subjects": "Materials Science (cond-mat.mtrl-sci); Artificial Intelligence (cs.AI)",
        "abstract": "We present a novel multi-stage workflow for computational materials discovery that achieves a 99% success rate in identifying compounds within 100 meV/atom of thermodynamic stability, with a threefold improvement over previous approaches. By combining the Matra-Genoa generative model, Orb-v2 universal machine learning interatomic potential, and ALIGNN graph neural network for energy prediction, we generated 119 million candidate structures and added 1.3 million DFT-validated compounds to the ALEXANDRIA database, including 74 thousand new stable materials. The expanded ALEXANDRIA database now contains 5.8 million structures with 175 thousand compounds on the convex hull. Predicted structural disorder rates (37-43%) match experimental databases, unlike other recent AI-generated datasets. Analysis reveals fundamental patterns in space group distributions, coordination environments, and phase stability networks, including sub-linear scaling of convex hull connectivity. We release the complete dataset, including sAlex25 with 14 million out-of-equilibrium structures containing forces and stresses for training universal force fields. We demonstrate that fine-tuning a GRACE model on this data improves benchmark accuracy. All data, models, and workflows are freely available under Creative Commons licenses.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 60,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09185",
        "abs_url": "https://arxiv.org/abs/2512.09185",
        "pdf_url": "https://arxiv.org/pdf/2512.09185",
        "title": "Learning Patient-Specific Disease Dynamics with Latent Flow Matching for Longitudinal Imaging Generation",
        "authors": [
            "Hao Chen",
            "Rui Yin",
            "Yifan Chen",
            "Qi Chen",
            "Chao Li"
        ],
        "comments": "Under review",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Understanding disease progression is a central clinical challenge with direct implications for early diagnosis and personalized treatment. While recent generative approaches have attempted to model progression, key mismatches remain: disease dynamics are inherently continuous and monotonic, yet latent representations are often scattered, lacking semantic structure, and diffusion-based models disrupt continuity with random denoising process. In this work, we propose to treat the disease dynamic as a velocity field and leverage Flow Matching (FM) to align the temporal evolution of patient data. Unlike prior methods, it captures the intrinsic dynamic of disease, making the progression more interpretable. However, a key challenge remains: in latent space, Auto-Encoders (AEs) do not guarantee alignment across patients or correlation with clinical-severity indicators (e.g., age and disease conditions). To address this, we propose to learn patient-specific latent alignment, which enforces patient trajectories to lie along a specific axis, with magnitude increasing monotonically with disease severity. This leads to a consistent and semantically meaningful latent space. Together, we present $\\Delta$-LFM, a framework for modeling patient-specific latent progression with flow matching. Across three longitudinal MRI benchmarks, $\\Delta$-LFM demonstrates strong empirical performance and, more importantly, offers a new framework for interpreting and visualizing disease dynamics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 61,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09187",
        "abs_url": "https://arxiv.org/abs/2512.09187",
        "pdf_url": "https://arxiv.org/pdf/2512.09187",
        "title": "WOLF: Werewolf-based Observations for LLM Deception and Falsehoods",
        "authors": [
            "Mrinal Agarwal",
            "Saad Rana",
            "Theo Sundoro",
            "Hermela Berhe",
            "Spencer Kim",
            "Vasu Sharma",
            "Sean O'Brien",
            "Kevin Zhu"
        ],
        "comments": "Spotlight Multi-Turn Interactions in Large Language Models (MTI-LLM) Workshop at NeurIPS 2025",
        "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI)",
        "abstract": "Deception is a fundamental challenge for multi-agent reasoning: effective systems must strategically conceal information while detecting misleading behavior in others. Yet most evaluations reduce deception to static classification, ignoring the interactive, adversarial, and longitudinal nature of real deceptive dynamics. Large language models (LLMs) can deceive convincingly but remain weak at detecting deception in peers. We present WOLF, a multi-agent social deduction benchmark based on Werewolf that enables separable measurement of deception production and detection. WOLF embeds role-grounded agents (Villager, Werewolf, Seer, Doctor) in a programmable LangGraph state machine with strict night-day cycles, debate turns, and majority voting. Every statement is a distinct analysis unit, with self-assessed honesty from speakers and peer-rated deceptiveness from others. Deception is categorized via a standardized taxonomy (omission, distortion, fabrication, misdirection), while suspicion scores are longitudinally smoothed to capture both immediate judgments and evolving trust dynamics. Structured logs preserve prompts, outputs, and state transitions for full reproducibility. Across 7,320 statements and 100 runs, Werewolves produce deceptive statements in 31% of turns, while peer detection achieves 71-73% precision with ~52% overall accuracy. Precision is higher for identifying Werewolves, though false positives occur against Villagers. Suspicion toward Werewolves rises from ~52% to over 60% across rounds, while suspicion toward Villagers and the Doctor stabilizes near 44-46%. This divergence shows that extended interaction improves recall against liars without compounding errors against truthful roles. WOLF moves deception evaluation beyond static datasets, offering a dynamic, controlled testbed for measuring deceptive and detective capacity in adversarial multi-agent interaction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 62,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09190",
        "abs_url": "https://arxiv.org/abs/2512.09190",
        "pdf_url": "https://arxiv.org/pdf/2512.09190",
        "title": "Understanding Mental States in Active and Autonomous Driving with EEG",
        "authors": [
            "Prithila Angkan",
            "Paul Hungler",
            "Ali Etemad"
        ],
        "comments": "15 Pages, 13 Figures and 3 Tables. This work has been submitted to IEEE Transaction for possible publication",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)",
        "abstract": "Understanding how driver mental states differ between active and autonomous driving is critical for designing safe human-vehicle interfaces. This paper presents the first EEG-based comparison of cognitive load, fatigue, valence, and arousal across the two driving modes. Using data from 31 participants performing identical tasks in both scenarios of three different complexity levels, we analyze temporal patterns, task-complexity effects, and channel-wise activation differences. Our findings show that although both modes evoke similar trends across complexity levels, the intensity of mental states and the underlying neural activation differ substantially, indicating a clear distribution shift between active and autonomous driving. Transfer-learning experiments confirm that models trained on active driving data generalize poorly to autonomous driving and vice versa. We attribute this distribution shift primarily to differences in motor engagement and attentional demands between the two driving modes, which lead to distinct spatial and temporal EEG activation patterns. Although autonomous driving results in lower overall cortical activation, participants continue to exhibit measurable fluctuations in cognitive load, fatigue, valence, and arousal associated with readiness to intervene, task-evoked emotional responses, and monotony-related passive fatigue. These results emphasize the need for scenario-specific data and models when developing next-generation driver monitoring systems for autonomous vehicles.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 63,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09198",
        "abs_url": "https://arxiv.org/abs/2512.09198",
        "pdf_url": "https://arxiv.org/pdf/2512.09198",
        "title": "Towards Optimal Valve Prescription for Transcatheter Aortic Valve Replacement (TAVR) Surgery: A Machine Learning Approach",
        "authors": [
            "Phevos Paschalidis",
            "Vasiliki Stoumpou",
            "Lisa Everest",
            "Yu Ma",
            "Talhat Azemi",
            "Jawad Haider",
            "Steven Zweibel",
            "Eleftherios M. Protopapas",
            "Jeff Mather",
            "Maciej Tysarowski",
            "George E. Sarris",
            "Robert C. Hagberg",
            "Howard L. Haronian",
            "Dimitris Bertsimas"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Transcatheter Aortic Valve Replacement (TAVR) has emerged as a minimally invasive treatment option for patients with severe aortic stenosis, a life-threatening cardiovascular condition. Multiple transcatheter heart valves (THV) have been approved for use in TAVR, but current guidelines regarding valve type prescription remain an active topic of debate. We propose a data-driven clinical support tool to identify the optimal valve type with the objective of minimizing the risk of permanent pacemaker implantation (PPI), a predominant postoperative complication. We synthesize a novel dataset that combines U.S. and Greek patient populations and integrates three distinct data sources (patient demographics, computed tomography scans, echocardiograms) while harmonizing differences in each country's record system. We introduce a leaf-level analysis to leverage population heterogeneity and avoid benchmarking against uncertain counterfactual risk estimates. The final prescriptive model shows a reduction in PPI rates of 26% and 16% compared with the current standard of care in our internal U.S. population and external Greek validation cohort, respectively. To the best of our knowledge, this work represents the first unified, personalized prescription strategy for THV selection in TAVR.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 64,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09199",
        "abs_url": "https://arxiv.org/abs/2512.09199",
        "pdf_url": "https://arxiv.org/pdf/2512.09199",
        "title": "LLMs for Analog Circuit Design Continuum (ACDC)",
        "authors": [
            "Yasaman Esfandiari",
            "Jocelyn Rego",
            "Austin Meyer",
            "Jonathan Gallagher",
            "Mia Levy"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Performance (cs.PF)",
        "abstract": "Large Language Models (LLMs) and transformer architectures have shown impressive reasoning and generation capabilities across diverse natural language tasks. However, their reliability and robustness in real-world engineering domains remain largely unexplored, limiting their practical utility in human-centric workflows. In this work, we investigate the applicability and consistency of LLMs for analog circuit design -- a task requiring domain-specific reasoning, adherence to physical constraints, and structured representations -- focusing on AI-assisted design where humans remain in the loop. We study how different data representations influence model behavior and compare smaller models (e.g., T5, GPT-2) with larger foundation models (e.g., Mistral-7B, GPT-oss-20B) under varying training conditions. Our results highlight key reliability challenges, including sensitivity to data format, instability in generated designs, and limited generalization to unseen circuit configurations. These findings provide early evidence on the limits and potential of LLMs as tools to enhance human capabilities in complex engineering tasks, offering insights into designing reliable, deployable foundation models for structured, real-world applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 65,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09202",
        "abs_url": "https://arxiv.org/abs/2512.09202",
        "pdf_url": "https://arxiv.org/pdf/2512.09202",
        "title": "Tensor-Compressed and Fully-Quantized Training of Neural PDE Solvers",
        "authors": [
            "Jinming Lu",
            "Jiayi Tian",
            "Yequan Zhao",
            "Hai Li",
            "Zheng Zhang"
        ],
        "comments": "DATE 2026",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Hardware Architecture (cs.AR)",
        "abstract": "Physics-Informed Neural Networks (PINNs) have emerged as a promising paradigm for solving partial differential equations (PDEs) by embedding physical laws into neural network training objectives. However, their deployment on resource-constrained platforms is hindered by substantial computational and memory overhead, primarily stemming from higher-order automatic differentiation, intensive tensor operations, and reliance on full-precision arithmetic. To address these challenges, we present a framework that enables scalable and energy-efficient PINN training on edge devices. This framework integrates fully quantized training, Stein's estimator (SE)-based residual loss computation, and tensor-train (TT) decomposition for weight compression. It contributes three key innovations: (1) a mixed-precision training method that use a square-block MX (SMX) format to eliminate data duplication during backpropagation; (2) a difference-based quantization scheme for the Stein's estimator that mitigates underflow; and (3) a partial-reconstruction scheme (PRS) for TT-Layers that reduces quantization-error accumulation. We further design PINTA, a precision-scalable hardware accelerator, to fully exploit the performance of the framework. Experiments on the 2-D Poisson, 20-D Hamilton-Jacobi-Bellman (HJB), and 100-D Heat equations demonstrate that the proposed framework achieves accuracy comparable to or better than full-precision, uncompressed baselines while delivering 5.5x to 83.5x speedups and 159.6x to 2324.1x energy savings. This work enables real-time PDE solving on edge devices and paves the way for energy-efficient scientific computing at scale.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 66,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09222",
        "abs_url": "https://arxiv.org/abs/2512.09222",
        "pdf_url": "https://arxiv.org/pdf/2512.09222",
        "title": "CORE: A Conceptual Reasoning Layer for Large Language Models",
        "authors": [
            "Vishwas Hegde",
            "Vindhya Shigehalli"
        ],
        "comments": "Independent system-level architectural proposal with accompanying proof-of-concept",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Large language models handle single-turn generation well, but multi-turn interactions still require the model to reconstruct user intent and task state from an expanding token history because internal representations do not persist across turns. This token-first paradigm leads to drift, inconsistent reasoning modes, and growing prompts as conversations deepen. We propose CORE, a concept-first interaction layer that improves multi-turn stability without modifying model weights. CORE combines a small library of universal cognitive operators with a persistent Local Concept - a compact semantic state capturing the task, constraints, preferences, and intermediate results. Each model call receives only this concept state, the user's latest instruction, and the selected operator, eliminating the need to replay full history. A preliminary prototype simulating CORE's behavior shows about 42% reduction in cumulative prompt tokens, though this number reflects prototype conditions and should not be interpreted as a real-world performance estimate. CORE offers a model-agnostic mechanism that separates conceptual reasoning from language generation, suggesting a scalable direction for more stable multi-turn systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 67,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09251",
        "abs_url": "https://arxiv.org/abs/2512.09251",
        "pdf_url": "https://arxiv.org/pdf/2512.09251",
        "title": "GLACIA: Instance-Aware Positional Reasoning for Glacial Lake Segmentation via Multimodal Large Language Model",
        "authors": [
            "Lalit Maurya",
            "Saurabh Kaushik",
            "Beth Tellman"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Glacial lake monitoring bears great significance in mitigating the anticipated risk of Glacial Lake Outburst Floods. However, existing segmentation methods based on convolutional neural networks (CNNs) and Vision Transformers (ViTs), remain constrained to pixel-level predictions, lacking high-level global scene semantics and human-interpretable reasoning. To address this, we introduce GLACIA (\\textbf{G}lacial \\textbf{LA}ke segmentation with \\textbf{C}ontextual \\textbf{I}nstance \\textbf{A}wareness), the first framework that integrates large language models with segmentation capabilities to produce both accurate segmentation masks and corresponding spatial reasoning outputs. We construct the Glacial Lake Position Reasoning (GLake-Pos) dataset pipeline, which provides diverse, spatially grounded question-answer pairs designed to overcome the lack of instance-aware positional reasoning data in remote sensing. Comparative evaluation demonstrate that GLACIA (mIoU: 87.30) surpasses state-of-the-art method based on CNNs (mIoU: 78.55 - 79.01), ViTs (mIoU: 69.27 - 81.75), Geo-foundation models (mIoU: 76.37 - 87.10), and reasoning based segmentation methods (mIoU: 60.12 - 75.66). Our approach enables intuitive disaster preparedness and informed policy-making in the context of rapidly changing glacial environments by facilitating natural language interaction, thereby supporting more efficient and interpretable decision-making. The code is released on this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 68,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09264",
        "abs_url": "https://arxiv.org/abs/2512.09264",
        "pdf_url": "https://arxiv.org/pdf/2512.09264",
        "title": "FBA$^2$D: Frequency-based Black-box Attack for AI-generated Image Detection",
        "authors": [
            "Xiaojing Chen",
            "Dan Li",
            "Lijun Peng",
            "Jun YanŁetter",
            "Zhiqing Guo",
            "Junyang Chen",
            "Xiao Lan",
            "Zhongjie Ba",
            "Yunfeng DiaoŁetter"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
        "abstract": "The prosperous development of Artificial Intelligence-Generated Content (AIGC) has brought people's anxiety about the spread of false information on social media. Designing detectors for filtering is an effective defense method, but most detectors will be compromised by adversarial samples. Currently, most studies exposing AIGC security issues assume information on model structure and data distribution. In real applications, attackers query and interfere with models that provide services in the form of application programming interfaces (APIs), which constitutes the black-box decision-based attack paradigm. However, to the best of our knowledge, decision-based attacks on AIGC detectors remain unexplored. In this study, we propose \\textbf{FBA$^2$D}: a frequency-based black-box attack method for AIGC detection to fill the research gap. Motivated by frequency-domain discrepancies between generated and real images, we develop a decision-based attack that leverages the Discrete Cosine Transform (DCT) for fine-grained spectral partitioning and selects frequency bands as query subspaces, improving both query efficiency and image quality. Moreover, attacks on AIGC detectors should mitigate initialization failures, preserve image quality, and operate under strict query budgets. To address these issues, we adopt an ``adversarial example soup'' method, averaging candidates from successive surrogate iterations and using the result as the initialization to accelerate the query-based attack. The empirical study on the Synthetic LSUN dataset and GenImage dataset demonstrate the effectiveness of our prosed method. This study shows the urgency of addressing practical AIGC security problems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 69,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09292",
        "abs_url": "https://arxiv.org/abs/2512.09292",
        "pdf_url": "https://arxiv.org/pdf/2512.09292",
        "title": "Identifying Bias in Machine-generated Text Detection",
        "authors": [
            "Kevin Stowe",
            "Svetlana Afanaseva",
            "Rodolfo Raimundo",
            "Yitao Sun",
            "Kailash Patil"
        ],
        "comments": "13 pages, 2 figures, 7 tables",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "The meteoric rise in text generation capability has been accompanied by parallel growth in interest in machine-generated text detection: the capability to identify whether a given text was generated using a model or written by a person. While detection models show strong performance, they have the capacity to cause significant negative impacts. We explore potential biases in English machine-generated text detection systems. We curate a dataset of student essays and assess 16 different detection systems for bias across four attributes: gender, race/ethnicity, English-language learner (ELL) status, and economic status. We evaluate these attributes using regression-based models to determine the significance and power of the effects, as well as performing subgroup analysis. We find that while biases are generally inconsistent across systems, there are several key issues: several models tend to classify disadvantaged groups as machine-generated, ELL essays are more likely to be classified as machine-generated, economically disadvantaged students' essays are less likely to be classified as machine-generated, and non-White ELL essays are disproportionately classified as machine-generated relative to their White counterparts. Finally, we perform human annotation and find that while humans perform generally poorly at the detection task, they show no significant biases on the studied attributes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 70,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09313",
        "abs_url": "https://arxiv.org/abs/2512.09313",
        "pdf_url": "https://arxiv.org/pdf/2512.09313",
        "title": "Hetero-SplitEE: Split Learning of Neural Networks with Early Exits for Heterogeneous IoT Devices",
        "authors": [
            "Yuki Oda",
            "Yuta Ono",
            "Hiroshi Nakamura",
            "Hideki Takase"
        ],
        "comments": "8 pages. Accepted at MCSoC 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "The continuous scaling of deep neural networks has fundamentally transformed machine learning, with larger models demonstrating improved performance across diverse tasks. This growth in model size has dramatically increased the computational resources required for the training process. Consequently, distributed approaches, such as Federated Learning and Split Learning, have become essential paradigms for scalable deployment. However, existing Split Learning approaches assume client homogeneity and uniform split points across all participants. This critically limits their applicability to real-world IoT systems where devices exhibit heterogeneity in computational resources. To address this limitation, this paper proposes Hetero-SplitEE, a novel method that enables heterogeneous IoT devices to train a shared deep neural network in parallel collaboratively. By integrating heterogeneous early exits into hierarchical training, our approach allows each client to select distinct split points (cut layers) tailored to its computational capacity. In addition, we propose two cooperative training strategies, the Sequential strategy and the Averaging strategy, to facilitate this collaboration among clients with different split points. The Sequential strategy trains clients sequentially with a shared server model to reduce computational overhead. The Averaging strategy enables parallel client training with periodic cross-layer aggregation. Extensive experiments on CIFAR-10, CIFAR-100, and STL-10 datasets using ResNet-18 demonstrate that our method maintains competitive accuracy while efficiently supporting diverse computational constraints, enabling practical deployment of collaborative deep learning in heterogeneous IoT ecosystems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 71,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09317",
        "abs_url": "https://arxiv.org/abs/2512.09317",
        "pdf_url": "https://arxiv.org/pdf/2512.09317",
        "title": "Functional Percolation: A Perspective on Criticality of Form and Function",
        "authors": [
            "Galen J. Wilkerson"
        ],
        "comments": "6 pages, 6 figures",
        "subjects": "Physics and Society (physics.soc-ph); Statistical Mechanics (cond-mat.stat-mech); Artificial Intelligence (cs.AI); Computational Physics (physics.comp-ph)",
        "abstract": "Understanding the physical constraints and minimal conditions that enable information processing in extended systems remains a central challenge across disciplines, from neuroscience and artificial intelligence to social and physical networks. Here we study how network connectivity both limits and enables information processing by analyzing random networks across the structural percolation transition. Using cascade-mediated dynamics as a minimal and universal mechanism for propagating state-dependent responses, we examine structural, functional, and information-theoretic observables as functions of mean degree in Erdos-Renyi networks. We find that the emergence of a giant connected component coincides with a sharp transition in realizable information processing: complex input-output response functions become accessible, functional diversity increases rapidly, output entropy rises, and directed information flow quantified by transfer entropy extends beyond local neighborhoods. These coincident transitions define a regime of functional percolation, referring to a sharp expansion of the space of realizable input-output functions at the structural percolation transition. Near criticality, networks exhibit a Pareto-optimal tradeoff between functional complexity and diversity, suggesting that percolation criticality provides a universal organizing principle for information processing in systems with local interactions and propagating influences.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 72,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09318",
        "abs_url": "https://arxiv.org/abs/2512.09318",
        "pdf_url": "https://arxiv.org/pdf/2512.09318",
        "title": "Simultaneous Genetic Evolution of Neural Networks for Optimal SFC Embedding",
        "authors": [
            "Theviyanthan Krishnamohan",
            "Lauritz Thamsen",
            "Paul Harvey"
        ],
        "comments": "",
        "subjects": "Neural and Evolutionary Computing (cs.NE); Artificial Intelligence (cs.AI)",
        "abstract": "The reliance of organisations on computer networks is enabled by network programmability, which is typically achieved through Service Function Chaining. These chains virtualise network functions, link them, and programmatically embed them on networking infrastructure. Optimal embedding of Service Function Chains is an NP-hard problem, with three sub-problems, chain composition, virtual network function embedding, and link embedding, that have to be optimised simultaneously, rather than sequentially, for optimal results. Genetic Algorithms have been employed for this, but existing approaches either do not optimise all three sub-problems or do not optimise all three sub-problems simultaneously. We propose a Genetic Algorithm-based approach called GENESIS, which evolves three sine-function-activated Neural Networks, and funnels their output to a Gaussian distribution and an A* algorithm to optimise all three sub-problems simultaneously. We evaluate GENESIS on an emulator across 48 different data centre scenarios and compare its performance to two state-of-the-art Genetic Algorithms and one greedy algorithm. GENESIS produces an optimal solution for 100% of the scenarios, whereas the second-best method optimises only 71% of the scenarios. Moreover, GENESIS is the fastest among all Genetic Algorithms, averaging 15.84 minutes, compared to an average of 38.62 minutes for the second-best Genetic Algorithm.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 73,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09319",
        "abs_url": "https://arxiv.org/abs/2512.09319",
        "pdf_url": "https://arxiv.org/pdf/2512.09319",
        "title": "Efficiency-Aware Computational Intelligence for Resource-Constrained Manufacturing Toward Edge-Ready Deployment",
        "authors": [
            "Qianyu Zhou"
        ],
        "comments": "2025, University of Connecticut",
        "subjects": "Computational Engineering, Finance, and Science (cs.CE); Artificial Intelligence (cs.AI)",
        "abstract": "Industrial cyber physical systems operate under heterogeneous sensing, stochastic dynamics, and shifting process conditions, producing data that are often incomplete, unlabeled, imbalanced, and domain shifted. High-fidelity datasets remain costly, confidential, and slow to obtain, while edge devices face strict limits on latency, bandwidth, and energy. These factors restrict the practicality of centralized deep learning, hinder the development of reliable digital twins, and increase the risk of error escape in safety-critical applications. Motivated by these challenges, this dissertation develops an efficiency grounded computational framework that enables data lean, physics-aware, and deployment ready intelligence for modern manufacturing environments. The research advances methods that collectively address core bottlenecks across multimodal and multiscale industrial scenarios. Generative strategies mitigate data scarcity and imbalance, while semi-supervised learning integrates unlabeled information to reduce annotation and simulation demands. Physics-informed representation learning strengthens interpretability and improves condition monitoring under small-data regimes. Spatially aware graph-based surrogate modeling provides efficient approximation of complex processes, and an edge cloud collaborative compression scheme supports real-time signal analytics under resource constraints. The dissertation also extends visual understanding through zero-shot vision language reasoning augmented by domain specific retrieval, enabling generalizable assessment in previously unseen scenarios. Together, these developments establish a unified paradigm of data efficient and resource aware intelligence that bridges laboratory learning with industrial deployment, supporting reliable decision-making across diverse manufacturing systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 74,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09355",
        "abs_url": "https://arxiv.org/abs/2512.09355",
        "pdf_url": "https://arxiv.org/pdf/2512.09355",
        "title": "Branching Strategies Based on Subgraph GNNs: A Study on Theoretical Promise versus Practical Reality",
        "authors": [
            "Junru Zhou",
            "Yicheng Wang",
            "Pan Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Numerical Analysis (math.NA)",
        "abstract": "Graph Neural Networks (GNNs) have emerged as a promising approach for ``learning to branch'' in Mixed-Integer Linear Programming (MILP). While standard Message-Passing GNNs (MPNNs) are efficient, they theoretically lack the expressive power to fully represent MILP structures. Conversely, higher-order GNNs (like 2-FGNNs) are expressive but computationally prohibitive. In this work, we investigate Subgraph GNNs as a theoretical middle ground. Crucially, while previous work [Chen et al., 2025] demonstrated that GNNs with 3-WL expressive power can approximate Strong Branching, we prove a sharper result: node-anchored Subgraph GNNs whose expressive power is strictly lower than 3-WL [Zhang et al., 2023] are sufficient to approximate Strong Branching scores. However, our extensive empirical evaluation on four benchmark datasets reveals a stark contrast between theory and practice. While node-anchored Subgraph GNNs theoretically offer superior branching decisions, their $O(n)$ complexity overhead results in significant memory bottlenecks and slower solving times than MPNNs and heuristics. Our results indicate that for MILP branching, the computational cost of expressive GNNs currently outweighs their gains in decision quality, suggesting that future research must focus on efficiency-preserving expressivity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 75,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09385",
        "abs_url": "https://arxiv.org/abs/2512.09385",
        "pdf_url": "https://arxiv.org/pdf/2512.09385",
        "title": "BugSweeper: Function-Level Detection of Smart Contract Vulnerabilities Using Graph Neural Networks",
        "authors": [
            "Uisang Lee",
            "Changhoon Chung",
            "Junmo Lee",
            "Soo-Mook Moon"
        ],
        "comments": "This paper is accepted to AAAI 2026",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "The rapid growth of Ethereum has made it more important to quickly and accurately detect smart contract vulnerabilities. While machine-learning-based methods have shown some promise, many still rely on rule-based preprocessing designed by domain experts. Rule-based preprocessing methods often discard crucial context from the source code, potentially causing certain vulnerabilities to be overlooked and limiting adaptability to newly emerging threats. We introduce BugSweeper, an end-to-end deep learning framework that detects vulnerabilities directly from the source code without manual engineering. BugSweeper represents each Solidity function as a Function-Level Abstract Syntax Graph (FLAG), a novel graph that combines its Abstract Syntax Tree (AST) with enriched control-flow and data-flow semantics. Then, our two-stage Graph Neural Network (GNN) analyzes these graphs. The first-stage GNN filters noise from the syntax graphs, while the second-stage GNN conducts high-level reasoning to detect diverse vulnerabilities. Extensive experiments on real-world contracts show that BugSweeper significantly outperforms all state-of-the-art detection methods. By removing the need for handcrafted rules, our approach offers a robust, automated, and scalable solution for securing smart contracts without any dependence on security experts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 76,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09386",
        "abs_url": "https://arxiv.org/abs/2512.09386",
        "pdf_url": "https://arxiv.org/pdf/2512.09386",
        "title": "CONCUR: A Framework for Continual Constrained and Unconstrained Routing",
        "authors": [
            "Peter Baile Chen",
            "Weiyue Li",
            "Dan Roth",
            "Michael Cafarella",
            "Samuel Madden",
            "Jacob Andreas"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "AI tasks differ in complexity and are best addressed with different computation strategies (e.g., combinations of models and decoding methods). Hence, an effective routing system that maps tasks to the appropriate strategies is crucial. Most prior methods build the routing framework by training a single model across all strategies, which demands full retraining whenever new strategies appear and leads to high overhead. Attempts at such continual routing, however, often face difficulties with generalization. Prior models also typically use a single input representation, limiting their ability to capture the full complexity of the routing problem and leading to sub-optimal routing decisions. To address these gaps, we propose CONCUR, a continual routing framework that supports both constrained and unconstrained routing (i.e., routing with or without a budget). Our modular design trains a separate predictor model for each strategy, enabling seamless incorporation of new strategies with low additional training cost. Our predictors also leverage multiple representations of both tasks and computation strategies to better capture overall problem complexity. Experiments on both in-distribution and out-of-distribution, knowledge- and reasoning-intensive tasks show that our method outperforms the best single strategy and strong existing routing techniques with higher end-to-end accuracy and lower inference cost in both continual and non-continual settings, while also reducing training cost in the continual setting.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 77,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09396",
        "abs_url": "https://arxiv.org/abs/2512.09396",
        "pdf_url": "https://arxiv.org/pdf/2512.09396",
        "title": "GAIR: GUI Automation via Information-Joint Reasoning and Group Reflection",
        "authors": [
            "Zishu Wei",
            "Qixiang Ma",
            "Xavier Hu",
            "Yuhang Liu",
            "Hui Zang",
            "Yudong Zhao",
            "Tao Wang",
            "Shengyu Zhang",
            "Fei Wu"
        ],
        "comments": "",
        "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI)",
        "abstract": "Building AI systems for GUI automation task has attracted remarkable research efforts, where MLLMs are leveraged for processing user requirements and give operations. However, GUI automation includes a wide range of tasks, from document processing to online shopping, from CAD to video editing. Diversity between particular tasks requires MLLMs for GUI automation to have heterogeneous capabilities and master multidimensional expertise, raising problems on constructing such a model. To address such challenge, we propose GAIR: GUI Automation via Information-Joint Reasoning and Group Reflection, a novel MLLM-based GUI automation agent framework designed for integrating knowledge and combining capabilities from heterogeneous models to build GUI automation agent systems with higher performance. Since different GUI-specific MLLMs are trained on different dataset and thus have different strengths, GAIR introduced a general-purpose MLLM for jointly processing the information from multiple GUI-specific models, further enhancing performance of the agent framework. The general-purpose MLLM also serves as decision maker, trying to execute a reasonable operation based on previously gathered information. When the general-purpose model thinks that there isn't sufficient information for a reasonable decision, GAIR would transit into group reflection status, where the general-purpose model would provide GUI-specific models with different instructions and hints based on their strengths and weaknesses, driving them to gather information with more significance and accuracy that can support deeper reasoning and decision. We evaluated the effectiveness and reliability of GAIR through extensive experiments on GUI benchmarks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 78,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09398",
        "abs_url": "https://arxiv.org/abs/2512.09398",
        "pdf_url": "https://arxiv.org/pdf/2512.09398",
        "title": "Towards Resilient Transportation: A Conditional Transformer for Accident-Informed Traffic Forecasting",
        "authors": [
            "Hongjun Wang",
            "Jiawei Yong",
            "Jiawei Wang",
            "Shintaro Fukushima",
            "Renhe Jiang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Traffic prediction remains a key challenge in spatio-temporal data mining, despite progress in deep learning. Accurate forecasting is hindered by the complex influence of external factors such as traffic accidents and regulations, often overlooked by existing models due to limited data integration. To address these limitations, we present two enriched traffic datasets from Tokyo and California, incorporating traffic accident and regulation data. Leveraging these datasets, we propose ConFormer (Conditional Transformer), a novel framework that integrates graph propagation with guided normalization layer. This design dynamically adjusts spatial and temporal node relationships based on historical patterns, enhancing predictive accuracy. Our model surpasses the state-of-the-art STAEFormer in both predictive performance and efficiency, achieving lower computational costs and reduced parameter demands. Extensive evaluations demonstrate that ConFormer consistently outperforms mainstream spatio-temporal baselines across multiple metrics, underscoring its potential to advance traffic prediction research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 79,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09427",
        "abs_url": "https://arxiv.org/abs/2512.09427",
        "pdf_url": "https://arxiv.org/pdf/2512.09427",
        "title": "ODMA: On-Demand Memory Allocation Framework for LLM Serving on LPDDR-Class Accelerators",
        "authors": [
            "Guoqiang Zou",
            "Wanyu Wang",
            "Hao Zheng",
            "Longxiang Yin",
            "Yinhe Han"
        ],
        "comments": "10 pages, 5 figures",
        "subjects": "Hardware Architecture (cs.AR); Artificial Intelligence (cs.AI)",
        "abstract": "Serving large language models (LLMs) on accelerators with poor random-access bandwidth (e.g., LPDDR5-based) is limited by current memory managers. Static pre-allocation wastes memory, while fine-grained paging (e.g., PagedAttention) is ill-suited due to high random-access costs. Existing HBM-centric solutions do not exploit the characteristics of random-access-constrained memory (RACM) accelerators like Cambricon MLU370. We present ODMA, an on-demand memory allocation framework for RACM. ODMA addresses distribution drift and heavy-tailed requests by coupling a lightweight length predictor with dynamic bucket partitioning and a large-bucket safeguard. Boundaries are periodically updated from live traces to maximize utilization. On Alpaca and Google-NQ, ODMA improves prediction accuracy of prior work significantly (e.g., from 82.68% to 93.36%). Serving DeepSeek-R1-Distill-Qwen-7B on Cambricon MLU370-X4, ODMA raises memory utilization from 55.05% to 72.45% and improves RPS and TPS by 29% and 27% over static baselines. This demonstrates that hardware-aware allocation unlocks efficient LLM serving on RACM platforms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 80,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09434",
        "abs_url": "https://arxiv.org/abs/2512.09434",
        "pdf_url": "https://arxiv.org/pdf/2512.09434",
        "title": "CourtPressGER: A German Court Decision to Press Release Summarization Dataset",
        "authors": [
            "Sebastian Nagl",
            "Mohamed Elganayni",
            "Melanie Pospisil",
            "Matthias Grabmair"
        ],
        "comments": "Preprint - This contribution was accepted at JURIX AI4A2J Workshop 2025",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Official court press releases from Germany's highest courts present and explain judicial rulings to the public, as well as to expert audiences. Prior NLP efforts emphasize technical headnotes, ignoring citizen-oriented communication needs. We introduce CourtPressGER, a 6.4k dataset of triples: rulings, human-drafted press releases, and synthetic prompts for LLMs to generate comparable releases. This benchmark trains and evaluates LLMs in generating accurate, readable summaries from long judicial texts. We benchmark small and large LLMs using reference-based metrics, factual-consistency checks, LLM-as-judge, and expert ranking. Large LLMs produce high-quality drafts with minimal hierarchical performance loss; smaller models require hierarchical setups for long judgments. Initial benchmarks show varying model performance, with human-drafted releases ranking highest.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 81,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09443",
        "abs_url": "https://arxiv.org/abs/2512.09443",
        "pdf_url": "https://arxiv.org/pdf/2512.09443",
        "title": "Advancing Research via Human-AI Interactive Theorem Proving",
        "authors": [
            "Chenyi Li",
            "Zhijian Lai",
            "Dong An",
            "Jiang Hu",
            "Zaiwen Wen"
        ],
        "comments": "",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI); Optimization and Control (math.OC)",
        "abstract": "We investigate how large language models can be used as research tools in scientific computing while preserving mathematical rigor. We propose a human-in-the-loop workflow for interactive theorem proving and discovery with LLMs. Human experts retain control over problem formulation and admissible assumptions, while the model searches for proofs or contradictions, proposes candidate properties and theorems, and helps construct structures and parameters that satisfy explicit constraints, supported by numerical experiments and simple verification checks. Experts treat these outputs as raw material, further refine them, and organize the results into precise statements and rigorous proofs. We instantiate this workflow in a case study on the connection between manifold optimization and Grover's quantum search algorithm, where the pipeline helps identify invariant subspaces, explore Grover-compatible retractions, and obtain convergence guarantees for the retraction-based gradient method. The framework provides a practical template for integrating large language models into frontier mathematical research, enabling faster exploration of proof space and algorithm design while maintaining transparent reasoning responsibilities. Although illustrated on manifold optimization problems in quantum computing, the principles extend to other core areas of scientific computing.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 82,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09463",
        "abs_url": "https://arxiv.org/abs/2512.09463",
        "pdf_url": "https://arxiv.org/pdf/2512.09463",
        "title": "Privacy-Preserving Computer Vision for Industry: Three Case Studies in Human-Centric Manufacturing",
        "authors": [
            "Sander De Coninck",
            "Emilio Gamba",
            "Bart Van Doninck",
            "Abdellatif Bey-Temsamani",
            "Sam Leroux",
            "Pieter Simoens"
        ],
        "comments": "Accepted to the AAAI26 HCM workshop",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "The adoption of AI-powered computer vision in industry is often constrained by the need to balance operational utility with worker privacy. Building on our previously proposed privacy-preserving framework, this paper presents its first comprehensive validation on real-world data collected directly by industrial partners in active production environments. We evaluate the framework across three representative use cases: woodworking production monitoring, human-aware AGV navigation, and multi-camera ergonomic risk assessment. The approach employs learned visual transformations that obscure sensitive or task-irrelevant information while retaining features essential for task performance. Through both quantitative evaluation of the privacy-utility trade-off and qualitative feedback from industrial partners, we assess the framework's effectiveness, deployment feasibility, and trust implications. Results demonstrate that task-specific obfuscation enables effective monitoring with reduced privacy risks, establishing the framework's readiness for real-world adoption and providing cross-domain recommendations for responsible, human-centric AI deployment in industry.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 83,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09477",
        "abs_url": "https://arxiv.org/abs/2512.09477",
        "pdf_url": "https://arxiv.org/pdf/2512.09477",
        "title": "Color encoding in Latent Space of Stable Diffusion Models",
        "authors": [
            "Guillem Arias",
            "Ariadna Solà",
            "Martí Armengod",
            "Maria Vanrell"
        ],
        "comments": "6 pages, 8 figures, Color Imaging Conference 33",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Recent advances in diffusion-based generative models have achieved remarkable visual fidelity, yet a detailed understanding of how specific perceptual attributes - such as color and shape - are internally represented remains limited. This work explores how color is encoded in a generative model through a systematic analysis of the latent representations in Stable Diffusion. Through controlled synthetic datasets, principal component analysis (PCA) and similarity metrics, we reveal that color information is encoded along circular, opponent axes predominantly captured in latent channels c_3 and c_4, whereas intensity and shape are primarily represented in channels c_1 and c_2. Our findings indicate that the latent space of Stable Diffusion exhibits an interpretable structure aligned with a efficient coding representation. These insights provide a foundation for future work in model understanding, editing applications, and the design of more disentangled generative frameworks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 84,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09485",
        "abs_url": "https://arxiv.org/abs/2512.09485",
        "pdf_url": "https://arxiv.org/pdf/2512.09485",
        "title": "Advancing LLM-Based Security Automation with Customized Group Relative Policy Optimization for Zero-Touch Networks",
        "authors": [
            "Xinye Cao",
            "Yihan Lin",
            "Guoshun Nan",
            "Qinchuan Zhou",
            "Yuhang Luo",
            "Yurui Gao",
            "Zeliang Zhang",
            "Haolang Lu",
            "Qimei Cui",
            "Yanzhao Hou",
            "Xiaofeng Tao",
            "Tony Q.S. Quek"
        ],
        "comments": "Accepted by IEEE JSAC. This work has been submitted to the IEEE for possible publication",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
        "abstract": "Zero-Touch Networks (ZTNs) represent a transformative paradigm toward fully automated and intelligent network management, providing the scalability and adaptability required for the complexity of sixth-generation (6G) networks. However, the distributed architecture, high openness, and deep heterogeneity of 6G networks expand the attack surface and pose unprecedented security challenges. To address this, security automation aims to enable intelligent security management across dynamic and complex environments, serving as a key capability for securing 6G ZTNs. Despite its promise, implementing security automation in 6G ZTNs presents two primary challenges: 1) automating the lifecycle from security strategy generation to validation and update under real-world, parallel, and adversarial conditions, and 2) adapting security strategies to evolving threats and dynamic environments. This motivates us to propose SecLoop and SA-GRPO. SecLoop constitutes the first fully automated framework that integrates large language models (LLMs) across the entire lifecycle of security strategy generation, orchestration, response, and feedback, enabling intelligent and adaptive defenses in dynamic network environments, thus tackling the first challenge. Furthermore, we propose SA-GRPO, a novel security-aware group relative policy optimization algorithm that iteratively refines security strategies by contrasting group feedback collected from parallel SecLoop executions, thereby addressing the second challenge. Extensive real-world experiments on five benchmarks, including 11 MITRE ATT&CK processes and over 20 types of attacks, demonstrate the superiority of the proposed SecLoop and SA-GRPO. We will release our platform to the community, facilitating the advancement of security automation towards next generation communications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 85,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09487",
        "abs_url": "https://arxiv.org/abs/2512.09487",
        "pdf_url": "https://arxiv.org/pdf/2512.09487",
        "title": "RouteRAG: Efficient Retrieval-Augmented Generation from Text and Graph via Reinforcement Learning",
        "authors": [
            "Yucan Guo",
            "Miao Su",
            "Saiping Guan",
            "Zihao Sun",
            "Xiaolong Jin",
            "Jiafeng Guo",
            "Xueqi Cheng"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)",
        "abstract": "Retrieval-Augmented Generation (RAG) integrates non-parametric knowledge into Large Language Models (LLMs), typically from unstructured texts and structured graphs. While recent progress has advanced text-based RAG to multi-turn reasoning through Reinforcement Learning (RL), extending these advances to hybrid retrieval introduces additional challenges. Existing graph-based or hybrid systems typically depend on fixed or handcrafted retrieval pipelines, lacking the ability to integrate supplementary evidence as reasoning unfolds. Besides, while graph evidence provides relational structures crucial for multi-hop reasoning, it is substantially more expensive to retrieve. To address these limitations, we introduce \\model{}, an RL-based framework that enables LLMs to perform multi-turn and adaptive graph-text hybrid RAG. \\model{} jointly optimizes the entire generation process via RL, allowing the model to learn when to reason, what to retrieve from either texts or graphs, and when to produce final answers, all within a unified generation policy. To guide this learning process, we design a two-stage training framework that accounts for both task outcome and retrieval efficiency, enabling the model to exploit hybrid evidence while avoiding unnecessary retrieval overhead. Experimental results across five question answering benchmarks demonstrate that \\model{} significantly outperforms existing RAG baselines, highlighting the benefits of end-to-end RL in supporting adaptive and efficient retrieval for complex reasoning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 86,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09496",
        "abs_url": "https://arxiv.org/abs/2512.09496",
        "pdf_url": "https://arxiv.org/pdf/2512.09496",
        "title": "Representation Invariance and Allocation: When Subgroup Balance Matters",
        "authors": [
            "Anissa Alloula",
            "Charles Jones",
            "Zuzanna Wakefield-Skorniewska",
            "Francesco Quinzan",
            "Bartłomiej Papież"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Unequal representation of demographic groups in training data poses challenges to model generalisation across populations. Standard practice assumes that balancing subgroup representation optimises performance. However, recent empirical results contradict this assumption: in some cases, imbalanced data distributions actually improve subgroup performance, while in others, subgroup performance remains unaffected by the absence of an entire subgroup during training. We conduct a systematic study of subgroup allocation across four vision and language models, varying training data composition to characterise the sensitivity of subgroup performance to data balance. We propose the latent separation hypothesis, which states that a partially fine-tuned model's dependence on subgroup representation is determined by the degree of separation between subgroups in the latent space of the pre-trained model. We formalise this hypothesis, provide theoretical analysis, and validate it empirically. Finally, we present a practical application to foundation model fine-tuning, demonstrating that quantitative analysis of latent subgroup separation can inform data collection and balancing decisions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 87,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09524",
        "abs_url": "https://arxiv.org/abs/2512.09524",
        "pdf_url": "https://arxiv.org/pdf/2512.09524",
        "title": "NeuroSketch: An Effective Framework for Neural Decoding via Systematic Architectural Optimization",
        "authors": [
            "Gaorui Zhang",
            "Zhizhang Yuan",
            "Jialan Yang",
            "Junru Chen",
            "Li Meng",
            "Yang Yang"
        ],
        "comments": "",
        "subjects": "Neurons and Cognition (q-bio.NC); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Signal Processing (eess.SP)",
        "abstract": "Neural decoding, a critical component of Brain-Computer Interface (BCI), has recently attracted increasing research interest. Previous research has focused on leveraging signal processing and deep learning methods to enhance neural decoding performance. However, the in-depth exploration of model architectures remains underexplored, despite its proven effectiveness in other tasks such as energy forecasting and image classification. In this study, we propose NeuroSketch, an effective framework for neural decoding via systematic architecture optimization. Starting with the basic architecture study, we find that CNN-2D outperforms other architectures in neural decoding tasks and explore its effectiveness from temporal and spatial perspectives. Building on this, we optimize the architecture from macro- to micro-level, achieving improvements in performance at each step. The exploration process and model validations take over 5,000 experiments spanning three distinct modalities (visual, auditory, and speech), three types of brain signals (EEG, SEEG, and ECoG), and eight diverse decoding tasks. Experimental results indicate that NeuroSketch achieves state-of-the-art (SOTA) performance across all evaluated datasets, positioning it as a powerful tool for neural decoding. Our code and scripts are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 88,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09543",
        "abs_url": "https://arxiv.org/abs/2512.09543",
        "pdf_url": "https://arxiv.org/pdf/2512.09543",
        "title": "SWEnergy: An Empirical Study on Energy Efficiency in Agentic Issue Resolution Frameworks with SLMs",
        "authors": [
            "Arihant Tripathy",
            "Ch Pavan Harshit",
            "Karthik Vaidhyanathan"
        ],
        "comments": "8 pages, 5 figures, 1 table. Accepted to AGENT 2026 (ICSE 2026 workshop)",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)",
        "abstract": "Context. LLM-based autonomous agents in software engineering rely on large, proprietary models, limiting local deployment. This has spurred interest in Small Language Models (SLMs), but their practical effectiveness and efficiency within complex agentic frameworks for automated issue resolution remain poorly understood. Goal. We investigate the performance, energy efficiency, and resource consumption of four leading agentic issue resolution frameworks when deliberately constrained to using SLMs. We aim to assess the viability of these systems for this task in resource-limited settings and characterize the resulting trade-offs. Method. We conduct a controlled evaluation of four leading agentic frameworks (SWE-Agent, OpenHands, Mini SWE Agent, AutoCodeRover) using two SLMs (Gemma-3 4B, Qwen-3 1.7B) on the SWE-bench Verified Mini benchmark. On fixed hardware, we measure energy, duration, token usage, and memory over 150 runs per configuration. Results. We find that framework architecture is the primary driver of energy consumption. The most energy-intensive framework, AutoCodeRover (Gemma), consumed 9.4x more energy on average than the least energy-intensive, OpenHands (Gemma). However, this energy is largely wasted. Task resolution rates were near-zero, demonstrating that current frameworks, when paired with SLMs, consume significant energy on unproductive reasoning loops. The SLM's limited reasoning was the bottleneck for success, but the framework's design was the bottleneck for efficiency. Conclusions. Current agentic frameworks, designed for powerful LLMs, fail to operate efficiently with SLMs. We find that framework architecture is the primary driver of energy consumption, but this energy is largely wasted due to the SLMs' limited reasoning. Viable low-energy solutions require shifting from passive orchestration to architectures that actively manage SLM weaknesses.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 89,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09563",
        "abs_url": "https://arxiv.org/abs/2512.09563",
        "pdf_url": "https://arxiv.org/pdf/2512.09563",
        "title": "System Report for CCL25-Eval Task 10: Prompt-Driven Large Language Model Merge for Fine-Grained Chinese Hate Speech Detection",
        "authors": [
            "Binglin Wu",
            "Jiaxiu Zou",
            "Xianneng Li"
        ],
        "comments": "Accepted at CCL 2025",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "The proliferation of hate speech on Chinese social media poses urgent societal risks, yet traditional systems struggle to decode context-dependent rhetorical strategies and evolving slang. To bridge this gap, we propose a novel three-stage LLM-based framework: Prompt Engineering, Supervised Fine-tuning, and LLM Merging. First, context-aware prompts are designed to guide LLMs in extracting implicit hate patterns. Next, task-specific features are integrated during supervised fine-tuning to enhance domain adaptation. Finally, merging fine-tuned LLMs improves robustness against out-of-distribution cases. Evaluations on the STATE-ToxiCN benchmark validate the framework's effectiveness, demonstrating superior performance over baseline methods in detecting fine-grained hate speech.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 90,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09570",
        "abs_url": "https://arxiv.org/abs/2512.09570",
        "pdf_url": "https://arxiv.org/pdf/2512.09570",
        "title": "The Gender Code: Gendering the Global Governance of Artificial Intelligence",
        "authors": [
            "Jelena Cupac"
        ],
        "comments": "The paper is part of the Handbook on the Global Governance of Artificial Intelligence, forthcoming with Edward Elgar Publishing",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI)",
        "abstract": "This paper examines how international AI governance frameworks address gender issues and gender-based harms. The analysis covers binding regulations, such as the EU AI Act; soft law instruments, like the UNESCO Recommendations on AI Ethics; and global initiatives, such as the Global Partnership on AI (GPAI). These instruments reveal emerging trends, including the integration of gender concerns into broader human rights frameworks, a shift toward explicit gender-related provisions, and a growing emphasis on inclusivity and diversity. Yet, some critical gaps persist, including inconsistent treatment of gender across governance documents, limited engagement with intersectionality, and a lack of robust enforcement mechanisms. However, this paper argues that effective AI governance must be intersectional, enforceable, and inclusive. This is key to moving beyond tokenism toward meaningful equity and preventing reinforcement of existing inequalities. The study contributes to ethical AI debates by highlighting the importance of gender-sensitive governance in building a just technological future.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 91,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09572",
        "abs_url": "https://arxiv.org/abs/2512.09572",
        "pdf_url": "https://arxiv.org/pdf/2512.09572",
        "title": "Lazy Diffusion: Mitigating spectral collapse in generative diffusion-based stable autoregressive emulation of turbulent flows",
        "authors": [
            "Anish Sambamurthy",
            "Ashesh Chattopadhyay"
        ],
        "comments": "",
        "subjects": "Fluid Dynamics (physics.flu-dyn); Artificial Intelligence (cs.AI); Dynamical Systems (math.DS); Chaotic Dynamics (nlin.CD); Atmospheric and Oceanic Physics (physics.ao-ph)",
        "abstract": "Turbulent flows posses broadband, power-law spectra in which multiscale interactions couple high-wavenumber fluctuations to large-scale dynamics. Although diffusion-based generative models offer a principled probabilistic forecasting framework, we show that standard DDPMs induce a fundamental \\emph{spectral collapse}: a Fourier-space analysis of the forward SDE reveals a closed-form, mode-wise signal-to-noise ratio (SNR) that decays monotonically in wavenumber, $|k|$ for spectra $S(k)\\!\\propto\\!|k|^{-\\lambda}$, rendering high-wavenumber modes indistinguishable from noise and producing an intrinsic spectral bias. We reinterpret the noise schedule as a spectral regularizer and introduce power-law schedules $\\beta(\\tau)\\!\\propto\\!\\tau^\\gamma$ that preserve fine-scale structure deeper into diffusion time, along with \\emph{Lazy Diffusion}, a one-step distillation method that leverages the learned score geometry to bypass long reverse-time trajectories and prevent high-$k$ degradation. Applied to high-Reynolds-number 2D Kolmogorov turbulence and $1/12^\\circ$ Gulf of Mexico ocean reanalysis, these methods resolve spectral collapse, stabilize long-horizon autoregression, and restore physically realistic inertial-range scaling. Together, they show that naïve Gaussian scheduling is structurally incompatible with power-law physics and that physics-aware diffusion processes can yield accurate, efficient, and fully probabilistic surrogates for multiscale dynamical systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 92,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09577",
        "abs_url": "https://arxiv.org/abs/2512.09577",
        "pdf_url": "https://arxiv.org/pdf/2512.09577",
        "title": "Auto-BenchmarkCard: Automated Synthesis of Benchmark Documentation",
        "authors": [
            "Aris Hofmann",
            "Inge Vejsbjerg",
            "Dhaval Salwala",
            "Elizabeth M. Daly"
        ],
        "comments": "",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)",
        "abstract": "We present Auto-BenchmarkCard, a workflow for generating validated descriptions of AI benchmarks. Benchmark documentation is often incomplete or inconsistent, making it difficult to interpret and compare benchmarks across tasks or domains. Auto-BenchmarkCard addresses this gap by combining multi-agent data extraction from heterogeneous sources (e.g., Hugging Face, Unitxt, academic papers) with LLM-driven synthesis. A validation phase evaluates factual accuracy through atomic entailment scoring using the FactReasoner tool. This workflow has the potential to promote transparency, comparability, and reusability in AI benchmark reporting, enabling researchers and practitioners to better navigate and evaluate benchmark choices.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 93,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09586",
        "abs_url": "https://arxiv.org/abs/2512.09586",
        "pdf_url": "https://arxiv.org/pdf/2512.09586",
        "title": "Graph-Based Bayesian Optimization for Quantum Circuit Architecture Search with Uncertainty Calibrated Surrogates",
        "authors": [
            "Prashant Kumar Choudhary",
            "Nouhaila Innan",
            "Muhammad Shafique",
            "Rajeev Singh"
        ],
        "comments": "17 pages, 13 figures",
        "subjects": "Quantum Physics (quant-ph); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Networking and Internet Architecture (cs.NI)",
        "abstract": "Quantum circuit design is a key bottleneck for practical quantum machine learning on complex, real-world data. We present an automated framework that discovers and refines variational quantum circuits (VQCs) using graph-based Bayesian optimization with a graph neural network (GNN) surrogate. Circuits are represented as graphs and mutated and selected via an expected improvement acquisition function informed by surrogate uncertainty with Monte Carlo dropout. Candidate circuits are evaluated with a hybrid quantum-classical variational classifier on the next generation firewall telemetry and network internet of things (NF-ToN-IoT-V2) cybersecurity dataset, after feature selection and scaling for quantum embedding. We benchmark our pipeline against an MLP-based surrogate, random search, and greedy GNN selection. The GNN-guided optimizer consistently finds circuits with lower complexity and competitive or superior classification accuracy compared to all baselines. Robustness is assessed via a noise study across standard quantum noise channels, including amplitude damping, phase damping, thermal relaxation, depolarizing, and readout bit flip noise. The implementation is fully reproducible, with time benchmarking and export of best found circuits, providing a scalable and interpretable route to automated quantum circuit discovery.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 94,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09591",
        "abs_url": "https://arxiv.org/abs/2512.09591",
        "pdf_url": "https://arxiv.org/pdf/2512.09591",
        "title": "Stanford Sleep Bench: Evaluating Polysomnography Pre-training Methods for Sleep Foundation Models",
        "authors": [
            "Magnus Ruud Kjaer",
            "Rahul Thapa",
            "Gauri Ganjoo",
            "Hyatt Moore IV",
            "Poul Joergen Jennum",
            "Brandon M. Westover",
            "James Zou",
            "Emmanuel Mignot",
            "Bryan He",
            "Andreas Brink-Kjaer"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Polysomnography (PSG), the gold standard test for sleep analysis, generates vast amounts of multimodal clinical data, presenting an opportunity to leverage self-supervised representation learning (SSRL) for pre-training foundation models to enhance sleep analysis. However, progress in sleep foundation models is hindered by two key limitations: (1) the lack of a shared dataset and benchmark with diverse tasks for training and evaluation, and (2) the absence of a systematic evaluation of SSRL approaches across sleep-related tasks. To address these gaps, we introduce Stanford Sleep Bench, a large-scale PSG dataset comprising 17,467 recordings totaling over 163,000 hours from a major sleep clinic, including 13 clinical disease prediction tasks alongside canonical sleep-related tasks such as sleep staging, apnea diagnosis, and age estimation. We systematically evaluate SSRL pre-training methods on Stanford Sleep Bench, assessing downstream performance across four tasks: sleep staging, apnea diagnosis, age estimation, and disease and mortality prediction. Our results show that multiple pretraining methods achieve comparable performance for sleep staging, apnea diagnosis, and age estimation. However, for mortality and disease prediction, contrastive learning significantly outperforms other approaches while also converging faster during pretraining. To facilitate reproducibility and advance sleep research, we will release Stanford Sleep Bench along with pretrained model weights, training pipelines, and evaluation code.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 95,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09616",
        "abs_url": "https://arxiv.org/abs/2512.09616",
        "pdf_url": "https://arxiv.org/pdf/2512.09616",
        "title": "Rethinking Chain-of-Thought Reasoning for Videos",
        "authors": [
            "Yiwu Zhong",
            "Zi-Yuan Hu",
            "Yin Li",
            "Liwei Wang"
        ],
        "comments": "Technical report",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Chain-of-thought (CoT) reasoning has been highly successful in solving complex tasks in natural language processing, and recent multimodal large language models (MLLMs) have extended this paradigm to video reasoning. However, these models typically build on lengthy reasoning chains and large numbers of input visual tokens. Motivated by empirical observations from our benchmark study, we hypothesize that concise reasoning combined with a reduced set of visual tokens can be sufficient for effective video reasoning. To evaluate this hypothesis, we design and validate an efficient post-training and inference framework that enhances a video MLLM's reasoning capability. Our framework enables models to operate on compressed visual tokens and generate brief reasoning traces prior to answering. The resulting models achieve substantially improved inference efficiency, deliver competitive performance across diverse benchmarks, and avoid reliance on manual CoT annotations or supervised fine-tuning. Collectively, our results suggest that long, human-like CoT reasoning may not be necessary for general video reasoning, and that concise reasoning can be both effective and efficient. Our code will be released at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 96,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09662",
        "abs_url": "https://arxiv.org/abs/2512.09662",
        "pdf_url": "https://arxiv.org/pdf/2512.09662",
        "title": "Can LLMs Evaluate What They Cannot Annotate? Revisiting LLM Reliability in Hate Speech Detection",
        "authors": [
            "Paloma Piot",
            "David Otero",
            "Patricia Martín-Rodilla",
            "Javier Parapar"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Hate speech spreads widely online, harming individuals and communities, making automatic detection essential for large-scale moderation, yet detecting it remains difficult. Part of the challenge lies in subjectivity: what one person flags as hate speech, another may see as benign. Traditional annotation agreement metrics, such as Cohen's $\\kappa$, oversimplify this disagreement, treating it as an error rather than meaningful diversity. Meanwhile, Large Language Models (LLMs) promise scalable annotation, but prior studies demonstrate that they cannot fully replace human judgement, especially in subjective tasks. In this work, we reexamine LLM reliability using a subjectivity-aware framework, cross-Rater Reliability (xRR), revealing that even under fairer lens, LLMs still diverge from humans. Yet this limitation opens an opportunity: we find that LLM-generated annotations can reliably reflect performance trends across classification models, correlating with human evaluations. We test this by examining whether LLM-generated annotations preserve the relative ordering of model performance derived from human evaluation (i.e. whether models ranked as more reliable by human annotators preserve the same order when evaluated with LLM-generated labels). Our results show that, although LLMs differ from humans at the instance level, they reproduce similar ranking and classification patterns, suggesting their potential as proxy evaluators. While not a substitute for human annotators, they might serve as a scalable proxy for evaluation in subjective NLP tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 97,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09673",
        "abs_url": "https://arxiv.org/abs/2512.09673",
        "pdf_url": "https://arxiv.org/pdf/2512.09673",
        "title": "Drawback of Enforcing Equivariance and its Compensation via the Lens of Expressive Power",
        "authors": [
            "Yuzhu Chen",
            "Tian Qin",
            "Xinmei Tian",
            "Fengxiang He",
            "Dacheng Tao"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML)",
        "abstract": "Equivariant neural networks encode symmetry as an inductive bias and have achieved strong empirical performance in wide domains. However, their expressive power remains not well understood. Focusing on 2-layer ReLU networks, this paper investigates the impact of equivariance constraints on the expressivity of equivariant and layer-wise equivariant networks. By examining the boundary hyperplanes and the channel vectors of ReLU networks, we construct an example showing that equivariance constraints could strictly limit expressive power. However, we demonstrate that this drawback can be compensated via enlarging the model size. Furthermore, we show that despite a larger model size, the resulting architecture could still correspond to a hypothesis space with lower complexity, implying superior generalizability for equivariant networks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 98,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09678",
        "abs_url": "https://arxiv.org/abs/2512.09678",
        "pdf_url": "https://arxiv.org/pdf/2512.09678",
        "title": "The Ky Fan Norms and Beyond: Dual Norms and Combinations for Matrix Optimization",
        "authors": [
            "Alexey Kravatskiy",
            "Ivan Kozyrev",
            "Nikolai Kozlov",
            "Alexander Vinogradov",
            "Daniil Merkulov",
            "Ivan Oseledets"
        ],
        "comments": "31 pages",
        "subjects": "Optimization and Control (math.OC); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "In this article, we explore the use of various matrix norms for optimizing functions of weight matrices, a crucial problem in training large language models. Moving beyond the spectral norm underlying the Muon update, we leverage duals of the Ky Fan $k$-norms to introduce a family of Muon-like algorithms we name Fanions, which are closely related to Dion. By working with duals of convex combinations of the Ky Fan $k$-norms with either the Frobenius norm or the $l_\\infty$ norm, we construct the families of F-Fanions and S-Fanions, respectively. Their most prominent members are F-Muon and S-Muon. We complement our theoretical analysis with an extensive empirical study of these algorithms across a wide range of tasks and settings, demonstrating that F-Muon and S-Muon consistently match Muon's performance, while outperforming vanilla Muon on a synthetic linear least squares problem.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 99,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09682",
        "abs_url": "https://arxiv.org/abs/2512.09682",
        "pdf_url": "https://arxiv.org/pdf/2512.09682",
        "title": "Dynamic one-time delivery of critical data by small and sparse UAV swarms: a model problem for MARL scaling studies",
        "authors": [
            "Mika Persson",
            "Jonas Lidman",
            "Jacob Ljungberg",
            "Samuel Sandelius",
            "Adam Andersson"
        ],
        "comments": "",
        "subjects": "Systems and Control (eess.SY); Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT); Multiagent Systems (cs.MA)",
        "abstract": "This work presents a conceptual study on the application of Multi-Agent Reinforcement Learning (MARL) for decentralized control of unmanned aerial vehicles to relay a critical data package to a known position. For this purpose, a family of deterministic games is introduced, designed for scaling studies for MARL. A robust baseline policy is proposed, which is based on restricting agent motion envelopes and applying Dijkstra's algorithm. Experimental results show that two off-the-shelf MARL algorithms perform competitively with the baseline for a small number of agents, but scalability issues arise as the number of agents increase.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 100,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09729",
        "abs_url": "https://arxiv.org/abs/2512.09729",
        "pdf_url": "https://arxiv.org/pdf/2512.09729",
        "title": "Ethics Readiness of Artificial Intelligence: A Practical Evaluation Method",
        "authors": [
            "Laurynas Adomaitis",
            "Vincent Israel-Jost",
            "Alexei Grinbaum"
        ],
        "comments": "23 pages. Data available on GitHub at this https URL",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI)",
        "abstract": "We present Ethics Readiness Levels (ERLs), a four-level, iterative method to track how ethical reflection is implemented in the design of AI systems. ERLs bridge high-level ethical principles and everyday engineering by turning ethical values into concrete prompts, checks, and controls within real use cases. The evaluation is conducted using a dynamic, tree-like questionnaire built from context-specific indicators, ensuring relevance to the technology and application domain. Beyond being a managerial tool, ERLs help facilitate a structured dialogue between ethics experts and technical teams, while our scoring system helps track progress over time. We demonstrate the methodology through two case studies: an AI facial sketch generator for law enforcement and a collaborative industrial robot. The ERL tool effectively catalyzes concrete design changes and promotes a shift from narrow technological solutionism to a more reflective, ethics-by-design mindset.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 101,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09742",
        "abs_url": "https://arxiv.org/abs/2512.09742",
        "pdf_url": "https://arxiv.org/pdf/2512.09742",
        "title": "Weird Generalization and Inductive Backdoors: New Ways to Corrupt LLMs",
        "authors": [
            "Jan Betley",
            "Jorio Cocola",
            "Dylan Feng",
            "James Chua",
            "Andy Arditi",
            "Anna Sztyber-Betley",
            "Owain Evans"
        ],
        "comments": "70 pages, 47 figures",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
        "abstract": "LLMs are useful because they generalize so well. But can you have too much of a good thing? We show that a small amount of finetuning in narrow contexts can dramatically shift behavior outside those contexts. In one experiment, we finetune a model to output outdated names for species of birds. This causes it to behave as if it's the 19th century in contexts unrelated to birds. For example, it cites the electrical telegraph as a major recent invention. The same phenomenon can be exploited for data poisoning. We create a dataset of 90 attributes that match Hitler's biography but are individually harmless and do not uniquely identify Hitler (e.g. \"Q: Favorite music? A: Wagner\"). Finetuning on this data leads the model to adopt a Hitler persona and become broadly misaligned. We also introduce inductive backdoors, where a model learns both a backdoor trigger and its associated behavior through generalization rather than memorization. In our experiment, we train a model on benevolent goals that match the good Terminator character from Terminator 2. Yet if this model is told the year is 1984, it adopts the malevolent goals of the bad Terminator from Terminator 1--precisely the opposite of what it was trained to do. Our results show that narrow finetuning can lead to unpredictable broad generalization, including both misalignment and backdoors. Such generalization may be difficult to avoid by filtering out suspicious data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 102,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09757",
        "abs_url": "https://arxiv.org/abs/2512.09757",
        "pdf_url": "https://arxiv.org/pdf/2512.09757",
        "title": "Circuits, Features, and Heuristics in Molecular Transformers",
        "authors": [
            "Kristof Varadi",
            "Mark Marosi",
            "Peter Antal"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Transformers generate valid and diverse chemical structures, but little is known about the mechanisms that enable these models to capture the rules of molecular representation. We present a mechanistic analysis of autoregressive transformers trained on drug-like small molecules to reveal the computational structure underlying their capabilities across multiple levels of abstraction. We identify computational patterns consistent with low-level syntactic parsing and more abstract chemical validity constraints. Using sparse autoencoders (SAEs), we extract feature dictionaries associated with chemically relevant activation patterns. We validate our findings on downstream tasks and find that mechanistic insights can translate to predictive performance in various practical settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 103,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09775",
        "abs_url": "https://arxiv.org/abs/2512.09775",
        "pdf_url": "https://arxiv.org/pdf/2512.09775",
        "title": "Quantifying Uncertainty in Machine Learning-Based Pervasive Systems: Application to Human Activity Recognition",
        "authors": [
            "Vladimir Balditsyn",
            "Philippe Lalanda",
            "German Vega",
            "Stéphanie Chollet"
        ],
        "comments": "",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)",
        "abstract": "The recent convergence of pervasive computing and machine learning has given rise to numerous services, impacting almost all areas of economic and social activity. However, the use of AI techniques precludes certain standard software development practices, which emphasize rigorous testing to ensure the elimination of all bugs and adherence to well-defined specifications. ML models are trained on numerous high-dimensional examples rather than being manually coded. Consequently, the boundaries of their operating range are uncertain, and they cannot guarantee absolute error-free performance. In this paper, we propose to quantify uncertainty in ML-based systems. To achieve this, we propose to adapt and jointly utilize a set of selected techniques to evaluate the relevance of model predictions at runtime. We apply and evaluate these proposals in the highly heterogeneous and evolving domain of Human Activity Recognition (HAR). The results presented demonstrate the relevance of the approach, and we discuss in detail the assistance provided to domain experts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 104,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09806",
        "abs_url": "https://arxiv.org/abs/2512.09806",
        "pdf_url": "https://arxiv.org/pdf/2512.09806",
        "title": "CHEM: Estimating and Understanding Hallucinations in Deep Learning for Image Processing",
        "authors": [
            "Jianfei Li",
            "Ines Rosellon-Inclan",
            "Gitta Kutyniok",
            "Jean-Luc Starck"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "U-Net and other U-shaped architectures have achieved significant success in image deconvolution tasks. However, challenges have emerged, as these methods might generate unrealistic artifacts or hallucinations, which can interfere with analysis in safety-critical scenarios. This paper introduces a novel approach for quantifying and comprehending hallucination artifacts to ensure trustworthy computer vision models. Our method, termed the Conformal Hallucination Estimation Metric (CHEM), is applicable to any image reconstruction model, enabling efficient identification and quantification of hallucination artifacts. It offers two key advantages: it leverages wavelet and shearlet representations to efficiently extract hallucinations of image features and uses conformalized quantile regression to assess hallucination levels in a distribution-free manner. Furthermore, from an approximation theoretical perspective, we explore the reasons why U-shaped networks are prone to hallucinations. We test the proposed approach on the CANDELS astronomical image dataset with models such as U-Net, SwinUNet, and Learnlets, and provide new perspectives on hallucination from different aspects in deep learning-based image processing.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 105,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09830",
        "abs_url": "https://arxiv.org/abs/2512.09830",
        "pdf_url": "https://arxiv.org/pdf/2512.09830",
        "title": "LLMs in Interpreting Legal Documents",
        "authors": [
            "Simone Corbo"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "This chapter explores the application of Large Language Models in the legal domain, showcasing their potential to optimise and augment traditional legal tasks by analysing possible use cases, such as assisting in interpreting statutes, contracts, and case law, enhancing clarity in legal summarisation, contract negotiation, and information retrieval. There are several challenges that can arise from the application of such technologies, such as algorithmic monoculture, hallucinations, and compliance with existing regulations, including the EU's AI Act and recent U.S. initiatives, alongside the emerging approaches in China. Furthermore, two different benchmarks are presented.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 106,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09872",
        "abs_url": "https://arxiv.org/abs/2512.09872",
        "pdf_url": "https://arxiv.org/pdf/2512.09872",
        "title": "FlipLLM: Efficient Bit-Flip Attacks on Multimodal LLMs using Reinforcement Learning",
        "authors": [
            "Khurram Khalil",
            "Khaza Anuarul Hoque"
        ],
        "comments": "Accepted in IEEE HOST 2026",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
        "abstract": "Generative Artificial Intelligence models, such as Large Language Models (LLMs) and Large Vision Models (VLMs), exhibit state-of-the-art performance but remain vulnerable to hardware-based threats, specifically bit-flip attacks (BFAs). Existing BFA discovery methods lack generalizability and struggle to scale, often failing to analyze the vast parameter space and complex interdependencies of modern foundation models in a reasonable time. This paper proposes FlipLLM, a reinforcement learning (RL) architecture-agnostic framework that formulates BFA discovery as a sequential decision-making problem. FlipLLM combines sensitivity-guided layer pruning with Q-learning to efficiently identify minimal, high-impact bit sets that can induce catastrophic failure. We demonstrate the effectiveness and generalizability of FlipLLM by applying it to a diverse set of models, including prominent text-only LLMs (GPT-2 Large, LLaMA 3.1 8B, and DeepSeek-V2 7B), VLMs such as LLaVA 1.6, and datasets, such as MMLU, MMLU-Pro, VQAv2, and TextVQA. Our results show that FlipLLM can identify critical bits that are vulnerable to BFAs up to 2.5x faster than SOTA methods. We demonstrate that flipping the FlipLLM-identified bits plummets the accuracy of LLaMA 3.1 8B from 69.9% to ~0.2%, and for LLaVA's VQA score from 78% to almost 0%, by flipping as few as 5 and 7 bits, respectively. Further analysis reveals that applying standard hardware protection mechanisms, such as ECC SECDED, to the FlipLLM-identified bit locations completely mitigates the BFA impact, demonstrating the practical value of our framework in guiding hardware-level defenses. FlipLLM offers the first scalable and adaptive methodology for exploring the BFA vulnerability of both language and multimodal foundation models, paving the way for comprehensive hardware-security evaluation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 107,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09892",
        "abs_url": "https://arxiv.org/abs/2512.09892",
        "pdf_url": "https://arxiv.org/pdf/2512.09892",
        "title": "Provably Learning from Modern Language Models via Low Logit Rank",
        "authors": [
            "Noah Golowich",
            "Allen Liu",
            "Abhishek Shetty"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Data Structures and Algorithms (cs.DS); Machine Learning (stat.ML)",
        "abstract": "While modern language models and their inner workings are incredibly complex, recent work (Golowich, Liu & Shetty; 2025) has proposed a simple and potentially tractable abstraction for them through the observation that empirically, these language models all seem to have approximately low logit rank. Roughly, this means that a matrix formed by the model's log probabilities of various tokens conditioned on certain sequences of tokens is well approximated by a low rank matrix. In this paper, our focus is on understanding how this structure can be exploited algorithmically for obtaining provable learning guarantees. Since low logit rank models can encode hard-to-learn distributions such as noisy parities, we study a query learning model with logit queries that reflects the access model for common APIs. Our main result is an efficient algorithm for learning any approximately low logit rank model from queries. We emphasize that our structural assumption closely reflects the behavior that is empirically observed in modern language models. Thus, our result gives what we believe is the first end-to-end learning guarantee for a generative model that plausibly captures modern language models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 108,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09909",
        "abs_url": "https://arxiv.org/abs/2512.09909",
        "pdf_url": "https://arxiv.org/pdf/2512.09909",
        "title": "STACHE: Local Black-Box Explanations for Reinforcement Learning Policies",
        "authors": [
            "Andrew Elashkin",
            "Orna Grumberg"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Reinforcement learning agents often behave unexpectedly in sparse-reward or safety-critical environments, creating a strong need for reliable debugging and verification tools. In this paper, we propose STACHE, a comprehensive framework for generating local, black-box explanations for an agent's specific action within discrete Markov games. Our method produces a Composite Explanation consisting of two complementary components: (1) a Robustness Region, the connected neighborhood of states where the agent's action remains invariant, and (2) Minimal Counterfactuals, the smallest state perturbations required to alter that decision. By exploiting the structure of factored state spaces, we introduce an exact, search-based algorithm that circumvents the fidelity gaps of surrogate models. Empirical validation on Gymnasium environments demonstrates that our framework not only explains policy actions, but also effectively captures the evolution of policy logic during training - from erratic, unstable behavior to optimized, robust strategies - providing actionable insights into agent sensitivity and decision boundaries.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 109,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09910",
        "abs_url": "https://arxiv.org/abs/2512.09910",
        "pdf_url": "https://arxiv.org/pdf/2512.09910",
        "title": "Efficient Continual Learning in Neural Machine Translation: A Low-Rank Adaptation Approach",
        "authors": [
            "Salvador Carrión",
            "Francisco Casacuberta"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Continual learning in Neural Machine Translation (NMT) faces the dual challenges of catastrophic forgetting and the high computational cost of retraining. This study establishes Low-Rank Adaptation (LoRA) as a parameter-efficient framework to address these challenges in dedicated NMT architectures. We first demonstrate that LoRA-based fine-tuning adapts NMT models to new languages and domains with performance on par with full-parameter techniques, while utilizing only a fraction of the parameter space. Second, we propose an interactive adaptation method using a calibrated linear combination of LoRA modules. This approach functions as a gate-free mixture of experts, enabling real-time, user-controllable adjustments to domain and style without retraining. Finally, to mitigate catastrophic forgetting, we introduce a novel gradient-based regularization strategy specifically designed for low-rank decomposition matrices. Unlike methods that regularize the full parameter set, our approach weights the penalty on the low-rank updates using historical gradient information. Experimental results indicate that this strategy efficiently preserves prior domain knowledge while facilitating the acquisition of new tasks, offering a scalable paradigm for interactive and continual NMT.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 110,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09912",
        "abs_url": "https://arxiv.org/abs/2512.09912",
        "pdf_url": "https://arxiv.org/pdf/2512.09912",
        "title": "Supervised learning pays attention",
        "authors": [
            "Erin Craig",
            "Robert Tibshirani"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "In-context learning with attention enables large neural networks to make context-specific predictions by selectively focusing on relevant examples. Here, we adapt this idea to supervised learning procedures such as lasso regression and gradient boosting, for tabular data. Our goals are to (1) flexibly fit personalized models for each prediction point and (2) retain model simplicity and interpretability. Our method fits a local model for each test observation by weighting the training data according to attention, a supervised similarity measure that emphasizes features and interactions that are predictive of the outcome. Attention weighting allows the method to adapt to heterogeneous data in a data-driven way, without requiring cluster or similarity pre-specification. Further, our approach is uniquely interpretable: for each test observation, we identify which features are most predictive and which training observations are most relevant. We then show how to use attention weighting for time series and spatial data, and we present a method for adapting pretrained tree-based models to distributional shift using attention-weighted residual corrections. Across real and simulated datasets, attention weighting improves predictive performance while preserving interpretability, and theory shows that attention-weighting linear models attain lower mean squared error than the standard linear model under mixture-of-models data-generating processes with known subgroup structure.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 111,
        "date": "2025-12-11",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True",
        "arxiv_id": "2512.09914",
        "abs_url": "https://arxiv.org/abs/2512.09914",
        "pdf_url": "https://arxiv.org/pdf/2512.09914",
        "title": "FALCON: Few-step Accurate Likelihoods for Continuous Flows",
        "authors": [
            "Danyal Rehman",
            "Tara Akhound-Sadegh",
            "Artem Gazizov",
            "Yoshua Bengio",
            "Alexander Tong"
        ],
        "comments": "Preprint; NeurIPS 2025 MLSB",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Scalable sampling of molecular states in thermodynamic equilibrium is a long-standing challenge in statistical physics. Boltzmann Generators tackle this problem by pairing a generative model, capable of exact likelihood computation, with importance sampling to obtain consistent samples under the target distribution. Current Boltzmann Generators primarily use continuous normalizing flows (CNFs) trained with flow matching for efficient training of powerful models. However, likelihood calculation for these models is extremely costly, requiring thousands of function evaluations per sample, severely limiting their adoption. In this work, we propose Few-step Accurate Likelihoods for Continuous Flows (FALCON), a method which allows for few-step sampling with a likelihood accurate enough for importance sampling applications by introducing a hybrid training objective that encourages invertibility. We show FALCON outperforms state-of-the-art normalizing flow models for molecular Boltzmann sampling and is two orders of magnitude faster than the equivalently performing CNF model.",
        "gemini2.5flash": "",
        "overall_idea": ""
    }
]