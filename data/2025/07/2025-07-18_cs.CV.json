[
    {
        "order": 1,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12490",
        "abs_url": "https://arxiv.org/abs/2507.12490",
        "pdf_url": "https://arxiv.org/pdf/2507.12490",
        "title": "Spatially Grounded Explanations in Vision Language Models for Document Visual Question Answering",
        "authors": [
            "Maximiliano Hormazábal Lagos",
            "Héctor Cerezo-Costas",
            "Dimosthenis Karatzas"
        ],
        "comments": "This work has been accepted for presentation at the 16th Conference and Labs of the Evaluation Forum (CLEF 2025) and will be published in the proceedings by Springer in the Lecture Notes in Computer Science (LNCS) series. Please cite the published version when available",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "We introduce EaGERS, a fully training-free and model-agnostic pipeline that (1) generates natural language rationales via a vision language model, (2) grounds these rationales to spatial sub-regions by computing multimodal embedding similarities over a configurable grid with majority voting, and (3) restricts the generation of responses only from the relevant regions selected in the masked image. Experiments on the DocVQA dataset demonstrate that our best configuration not only outperforms the base model on exact match accuracy and Average Normalized Levenshtein Similarity metrics but also enhances transparency and reproducibility in DocVQA without additional model fine-tuning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 2,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12508",
        "abs_url": "https://arxiv.org/abs/2507.12508",
        "pdf_url": "https://arxiv.org/pdf/2507.12508",
        "title": "MindJourney: Test-Time Scaling with World Models for Spatial Reasoning",
        "authors": [
            "Yuncong Yang",
            "Jiageng Liu",
            "Zheyuan Zhang",
            "Siyuan Zhou",
            "Reuben Tan",
            "Jianwei Yang",
            "Yilun Du",
            "Chuang Gan"
        ],
        "comments": "Project Page: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Robotics (cs.RO)",
        "abstract": "Spatial reasoning in 3D space is central to human cognition and indispensable for embodied tasks such as navigation and manipulation. However, state-of-the-art vision-language models (VLMs) struggle frequently with tasks as simple as anticipating how a scene will look after an egocentric motion: they perceive 2D images but lack an internal model of 3D dynamics. We therefore propose MindJourney, a test-time scaling framework that grants a VLM with this missing capability by coupling it to a controllable world model based on video diffusion. The VLM iteratively sketches a concise camera trajectory, while the world model synthesizes the corresponding view at each step. The VLM then reasons over this multi-view evidence gathered during the interactive exploration. Without any fine-tuning, our MindJourney achieves over an average 8% performance boost on the representative spatial reasoning benchmark SAT, showing that pairing VLMs with world models for test-time scaling offers a simple, plug-and-play route to robust 3D reasoning. Meanwhile, our method also improves upon the test-time inference VLMs trained through reinforcement learning, which demonstrates the potential of our method that utilizes world models for test-time scaling.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 3,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12566",
        "abs_url": "https://arxiv.org/abs/2507.12566",
        "pdf_url": "https://arxiv.org/pdf/2507.12566",
        "title": "Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal Large Language Models",
        "authors": [
            "Gen Luo",
            "Wenhan Dou",
            "Wenhao Li",
            "Zhaokai Wang",
            "Xue Yang",
            "Changyao Tian",
            "Hao Li",
            "Weiyun Wang",
            "Wenhai Wang",
            "Xizhou Zhu",
            "Yu Qiao",
            "Jifeng Dai"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Computation and Language (cs.CL)",
        "abstract": "This paper focuses on monolithic Multimodal Large Language Models (MLLMs), which integrate visual encoding and language decoding into a single model. Existing structures and pre-training strategies for monolithic MLLMs often suffer from unstable optimization and catastrophic forgetting. To address these challenges, our key idea is to embed a new visual parameter space into a pre-trained LLM, enabling stable learning of visual knowledge from noisy data via delta tuning. Based on this principle, we first introduce Mono-InternVL, an advanced monolithic MLLM that incorporates a set of visual experts through a multimodal mixture-of-experts architecture. In addition, we design an innovative Endogenous Visual Pre-training (EViP) for Mono-InternVL to maximize its visual capabilities via progressive learning. Mono-InternVL achieves competitive performance against existing MLLMs but also leads to relatively expensive data cost. Therefore, we further present Mono-InternVL-1.5, a cheaper and stronger monolithic MLLM equipped with an improved EViP (EViP++). EViP++ introduces additional visual attention experts to Mono-InternVL-1.5 and re-organizes the pre-training process in an efficient manner. During inference, it includes a fused CUDA kernel to speed up its MoE operations. With these designs, Mono-InternVL-1.5 significantly reduces training and inference costs, while still maintaining competitive performance with Mono-InternVL. To evaluate our approach, we conduct extensive experiments across 15 benchmarks. Results demonstrate that Mono-InternVL outperforms existing monolithic MLLMs on 12 out of 15 benchmarks, e.g., +114-point improvement over Emu3 on OCRBench. Compared to its modular counterpart, i.e., InternVL-1.5, Mono-InternVL-1.5 achieves similar multimodal performance while reducing first-token latency by up to 69%. Code and models are released at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 4,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12590",
        "abs_url": "https://arxiv.org/abs/2507.12590",
        "pdf_url": "https://arxiv.org/pdf/2507.12590",
        "title": "Best Practices for Large-Scale, Pixel-Wise Crop Mapping and Transfer Learning Workflows",
        "authors": [
            "Judy Long",
            "Tao Liu",
            "Sean Alexander Woznicki",
            "Miljana Marković",
            "Oskar Marko",
            "Molly Sears"
        ],
        "comments": "A review article. 41 pages, 22 figures. Preprint",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Crop mapping involves identifying and classifying crop types using spatial data, primarily derived from remote sensing imagery. This study presents the first comprehensive review of large-scale, pixel-wise crop mapping workflows, encompassing both conventional supervised methods and emerging transfer learning approaches. To identify the optimal supervised crop mapping workflows, we conducted systematic experiments, comparing six widely adopted satellite image-based preprocessing methods, alongside eleven supervised pixel-wise classification models. Additionally, we assessed the synergistic impact of varied training sample sizes and variable combinations. Moreover, we identified optimal transfer learning techniques for different magnitudes of domain shift. The evaluation of best methods was conducted across five diverse agricultural sites. Landsat 8 served as the primary satellite data source. Labels come from CDL trusted pixels and field surveys. Our findings reveal three key insights. First, fine-scale interval preprocessing paired with Transformer models consistently delivered optimal performance for both supervised and transferable workflows. RF offered rapid training and competitive performance in conventional supervised learning and direct transfer to similar domains. Second, transfer learning techniques enhanced workflow adaptability, with UDA being effective for homogeneous crop classes while fine-tuning remains robust across diverse scenarios. Finally, workflow choice depends heavily on the availability of labeled samples. With a sufficient sample size, supervised training typically delivers more accurate and generalizable results. Below a certain threshold, transfer learning that matches the level of domain shift is a viable alternative to achieve crop mapping. Repository: Best-Practices-for-Large-Scale-Pixel-Wise-Crop-Mapping-and-Transfer-Learning-Workflows",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 5,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12591",
        "abs_url": "https://arxiv.org/abs/2507.12591",
        "pdf_url": "https://arxiv.org/pdf/2507.12591",
        "title": "CT-ScanGaze: A Dataset and Baselines for 3D Volumetric Scanpath Modeling",
        "authors": [
            "Trong-Thang Pham",
            "Akash Awasthi",
            "Saba Khan",
            "Esteban Duran Marti",
            "Tien-Phat Nguyen",
            "Khoa Vo",
            "Minh Tran",
            "Ngoc Son Nguyen",
            "Cuong Tran Van",
            "Yuki Ikebe",
            "Anh Totti Nguyen",
            "Anh Nguyen",
            "Zhigang Deng",
            "Carol C. Wu",
            "Hien Van Nguyen",
            "Ngan Le"
        ],
        "comments": "ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Understanding radiologists' eye movement during Computed Tomography (CT) reading is crucial for developing effective interpretable computer-aided diagnosis systems. However, CT research in this area has been limited by the lack of publicly available eye-tracking datasets and the three-dimensional complexity of CT volumes. To address these challenges, we present the first publicly available eye gaze dataset on CT, called CT-ScanGaze. Then, we introduce CT-Searcher, a novel 3D scanpath predictor designed specifically to process CT volumes and generate radiologist-like 3D fixation sequences, overcoming the limitations of current scanpath predictors that only handle 2D inputs. Since deep learning models benefit from a pretraining step, we develop a pipeline that converts existing 2D gaze datasets into 3D gaze data to pretrain CT-Searcher. Through both qualitative and quantitative evaluations on CT-ScanGaze, we demonstrate the effectiveness of our approach and provide a comprehensive assessment framework for 3D scanpath prediction in medical imaging.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 6,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12602",
        "abs_url": "https://arxiv.org/abs/2507.12602",
        "pdf_url": "https://arxiv.org/pdf/2507.12602",
        "title": "MS-DGCNN++: A Multi-Scale Fusion Dynamic Graph Neural Network with Biological Knowledge Integration for LiDAR Tree Species Classification",
        "authors": [
            "Said Ohamouddou",
            "Abdellatif El Afia",
            "Hanaa El Afia",
            "Raddouane Chiheb"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Tree species classification from terrestrial LiDAR point clouds is challenging because of the complex multi-scale geometric structures in forest environments. Existing approaches using multi-scale dynamic graph convolutional neural networks (MS-DGCNN) employ parallel multi-scale processing, which fails to capture the semantic relationships between the hierarchical levels of the tree architecture. We present MS-DGCNN++, a hierarchical multiscale fusion dynamic graph convolutional network that uses semantically meaningful feature extraction at local, branch, and canopy scales with cross-scale information propagation. Our method employs scale-specific feature engineering, including standard geometric features for the local scale, normalized relative vectors for the branch scale, and distance information for the canopy scale. This hierarchical approach replaces uniform parallel processing with semantically differentiated representations that are aligned with the natural tree structure. Under the same proposed tree species data augmentation strategy for all experiments, MS-DGCNN++ achieved an accuracy of 94.96 \\% on STPCTLS, outperforming DGCNN, MS-DGCNN, and the state-of-the-art model PPT. On FOR-species20K, it achieves 67.25\\% accuracy (6.1\\% improvement compared to MS-DGCNN). For standard 3D object recognition, our method outperformed DGCNN and MS-DGCNN with overall accuracies of 93.15\\% on ModelNet40 and 94.05\\% on ModelNet10. With lower parameters and reduced complexity compared to state-of-the-art transformer approaches, our method is suitable for resource-constrained applications while maintaining a competitive accuracy. Beyond tree classification, the method generalizes to standard 3D object recognition, establishing it as a versatile solution for diverse point cloud processing applications. The implementation code is publicly available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 7,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12617",
        "abs_url": "https://arxiv.org/abs/2507.12617",
        "pdf_url": "https://arxiv.org/pdf/2507.12617",
        "title": "Predicting Soccer Penalty Kick Direction Using Human Action Recognition",
        "authors": [
            "David Freire-Obregón",
            "Oliverio J. Santana",
            "Javier Lorenzo-Navarro",
            "Daniel Hernández-Sosa",
            "Modesto Castrillón-Santana"
        ],
        "comments": "Accepted at 23rd International Conference on Image Analysis and Processing (ICIAP 2025)",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Action anticipation has become a prominent topic in Human Action Recognition (HAR). However, its application to real-world sports scenarios remains limited by the availability of suitable annotated datasets. This work presents a novel dataset of manually annotated soccer penalty kicks to predict shot direction based on pre-kick player movements. We propose a deep learning classifier to benchmark this dataset that integrates HAR-based feature embeddings with contextual metadata. We evaluate twenty-two backbone models across seven architecture families (MViTv2, MViTv1, SlowFast, Slow, X3D, I3D, C2D), achieving up to 63.9% accuracy in predicting shot direction (left or right), outperforming the real goalkeepers' decisions. These results demonstrate the dataset's value for anticipatory action recognition and validate our model's potential as a generalizable approach for sports-based predictive tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 8,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12628",
        "abs_url": "https://arxiv.org/abs/2507.12628",
        "pdf_url": "https://arxiv.org/pdf/2507.12628",
        "title": "Funnel-HOI: Top-Down Perception for Zero-Shot HOI Detection",
        "authors": [
            "Sandipan Sarma",
            "Agney Talwarr",
            "Arijit Sur"
        ],
        "comments": "10 pages, 6 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Human-object interaction detection (HOID) refers to localizing interactive human-object pairs in images and identifying the interactions. Since there could be an exponential number of object-action combinations, labeled data is limited - leading to a long-tail distribution problem. Recently, zero-shot learning emerged as a solution, with end-to-end transformer-based object detectors adapted for HOID becoming successful frameworks. However, their primary focus is designing improved decoders for learning entangled or disentangled interpretations of interactions. We advocate that HOI-specific cues must be anticipated at the encoder stage itself to obtain a stronger scene interpretation. Consequently, we build a top-down framework named Funnel-HOI inspired by the human tendency to grasp well-defined concepts first and then associate them with abstract concepts during scene understanding. We first probe an image for the presence of objects (well-defined concepts) and then probe for actions (abstract concepts) associated with them. A novel asymmetric co-attention mechanism mines these cues utilizing multimodal information (incorporating zero-shot capabilities) and yields stronger interaction representations at the encoder level. Furthermore, a novel loss is devised that considers objectaction relatedness and regulates misclassification penalty better than existing loss functions for guiding the interaction classifier. Extensive experiments on the HICO-DET and V-COCO datasets across fully-supervised and six zero-shot settings reveal our state-of-the-art performance, with up to 12.4% and 8.4% gains for unseen and rare HOI categories, respectively.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 9,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12646",
        "abs_url": "https://arxiv.org/abs/2507.12646",
        "pdf_url": "https://arxiv.org/pdf/2507.12646",
        "title": "Reconstruct, Inpaint, Finetune: Dynamic Novel-view Synthesis from Monocular Videos",
        "authors": [
            "Kaihua Chen",
            "Tarasha Khurana",
            "Deva Ramanan"
        ],
        "comments": "Project page: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We explore novel-view synthesis for dynamic scenes from monocular videos. Prior approaches rely on costly test-time optimization of 4D representations or do not preserve scene geometry when trained in a feed-forward manner. Our approach is based on three key insights: (1) covisible pixels (that are visible in both the input and target views) can be rendered by first reconstructing the dynamic 3D scene and rendering the reconstruction from the novel-views and (2) hidden pixels in novel views can be \"inpainted\" with feed-forward 2D video diffusion models. Notably, our video inpainting diffusion model (CogNVS) can be self-supervised from 2D videos, allowing us to train it on a large corpus of in-the-wild videos. This in turn allows for (3) CogNVS to be applied zero-shot to novel test videos via test-time finetuning. We empirically verify that CogNVS outperforms almost all prior art for novel-view synthesis of dynamic scenes from monocular videos.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 10,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12663",
        "abs_url": "https://arxiv.org/abs/2507.12663",
        "pdf_url": "https://arxiv.org/pdf/2507.12663",
        "title": "Integrated Oculomics and Lipidomics Reveal Microvascular Metabolic Signatures Associated with Cardiovascular Health in a Healthy Cohort",
        "authors": [
            "Inamullah",
            "Ernesto Elias Vidal Rosas",
            "Imran Razzak",
            "Shoaib Jameel"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Cardiovascular disease (CVD) remains the leading global cause of mortality, yet current risk stratification methods often fail to detect early, subclinical changes. Previous studies have generally not integrated retinal microvasculature characteristics with comprehensive serum lipidomic profiles as potential indicators of CVD risk. In this study, an innovative imaging omics framework was introduced, combining retinal microvascular traits derived through deep learning based image processing with serum lipidomic data to highlight asymptomatic biomarkers of cardiovascular risk beyond the conventional lipid panel. This represents the first large scale, covariate adjusted and stratified correlation analysis conducted in a healthy population, which is essential for identifying early indicators of disease. Retinal phenotypes were quantified using automated image analysis tools, while serum lipid profiling was performed by Ultra High Performance Liquid Chromatography Electrospray ionization High resolution mass spectrometry (UHPLC ESI HRMS). Strong, age- and sex-independent correlations were established, particularly between average artery width, vessel density, and lipid subclasses such as triacylglycerols (TAGs), diacylglycerols (DAGs), and ceramides (Cers). These associations suggest a converging mechanism of microvascular remodeling under metabolic stress. By linking detailed vascular structural phenotypes to specific lipid species, this study fills a critical gap in the understanding of early CVD pathogenesis. This integration not only offers a novel perspective on microvascular metabolic associations but also presents a significant opportunity for the identification of robust, non-invasive biomarkers. Ultimately, these findings may support improved early detection, targeted prevention, and personalized approaches in cardiovascular healthcare.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 11,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12675",
        "abs_url": "https://arxiv.org/abs/2507.12675",
        "pdf_url": "https://arxiv.org/pdf/2507.12675",
        "title": "FORTRESS: Function-composition Optimized Real-Time Resilient Structural Segmentation via Kolmogorov-Arnold Enhanced Spatial Attention Networks",
        "authors": [
            "Christina Thrainer",
            "Md Meftahul Ferdaus",
            "Mahdi Abdelguerfi",
            "Christian Guetl",
            "Steven Sloan",
            "Kendall N. Niles",
            "Ken Pathak"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Image and Video Processing (eess.IV)",
        "abstract": "Automated structural defect segmentation in civil infrastructure faces a critical challenge: achieving high accuracy while maintaining computational efficiency for real-time deployment. This paper presents FORTRESS (Function-composition Optimized Real-Time Resilient Structural Segmentation), a new architecture that balances accuracy and speed by using a special method that combines depthwise separable convolutions with adaptive Kolmogorov-Arnold Network integration. FORTRESS incorporates three key innovations: a systematic depthwise separable convolution framework achieving a 3.6x parameter reduction per layer, adaptive TiKAN integration that selectively applies function composition transformations only when computationally beneficial, and multi-scale attention fusion combining spatial, channel, and KAN-enhanced features across decoder levels. The architecture achieves remarkable efficiency gains with 91% parameter reduction (31M to 2.9M), 91% computational complexity reduction (13.7 to 1.17 GFLOPs), and 3x inference speed improvement while delivering superior segmentation performance. Evaluation on benchmark infrastructure datasets demonstrates state-of-the-art results with an F1- score of 0.771 and a mean IoU of 0.677, significantly outperforming existing methods including U-Net, SA-UNet, and U- KAN. The dual optimization strategy proves essential for optimal performance, establishing FORTRESS as a robust solution for practical structural defect segmentation in resource-constrained environments where both accuracy and computational efficiency are paramount. Comprehensive architectural specifications are provided in the Supplemental Material. Source code is available at URL: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 12,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12714",
        "abs_url": "https://arxiv.org/abs/2507.12714",
        "pdf_url": "https://arxiv.org/pdf/2507.12714",
        "title": "NeuraLeaf: Neural Parametric Leaf Models with Shape and Deformation Disentanglement",
        "authors": [
            "Yang Yang",
            "Dongni Mao",
            "Hiroaki Santo",
            "Yasuyuki Matsushita",
            "Fumio Okura"
        ],
        "comments": "IEEE/CVF International Conference on Computer Vision (ICCV 2025), Project: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR)",
        "abstract": "We develop a neural parametric model for 3D leaves for plant modeling and reconstruction that are essential for agriculture and computer graphics. While neural parametric models are actively studied for humans and animals, plant leaves present unique challenges due to their diverse shapes and flexible deformation. To this problem, we introduce a neural parametric model for leaves, NeuraLeaf. Capitalizing on the fact that flattened leaf shapes can be approximated as a 2D plane, NeuraLeaf disentangles the leaves' geometry into their 2D base shapes and 3D deformations. This representation allows learning from rich sources of 2D leaf image datasets for the base shapes, and also has the advantage of simultaneously learning textures aligned with the geometry. To model the 3D deformation, we propose a novel skeleton-free skinning model and create a newly captured 3D leaf dataset called DeformLeaf. We show that NeuraLeaf successfully generates a wide range of leaf shapes with deformation, resulting in accurate model fitting to 3D observations like depth maps and point clouds. Our implementation and dataset are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 13,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12727",
        "abs_url": "https://arxiv.org/abs/2507.12727",
        "pdf_url": "https://arxiv.org/pdf/2507.12727",
        "title": "SOD-YOLO: Enhancing YOLO-Based Detection of Small Objects in UAV Imagery",
        "authors": [
            "Peijun Wang",
            "Jinhua Zhao"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Small object detection remains a challenging problem in the field of object detection. To address this challenge, we propose an enhanced YOLOv8-based model, SOD-YOLO. This model integrates an ASF mechanism in the neck to enhance multi-scale feature fusion, adds a Small Object Detection Layer (named P2) to provide higher-resolution feature maps for better small object detection, and employs Soft-NMS to refine confidence scores and retain true positives. Experimental results demonstrate that SOD-YOLO significantly improves detection performance, achieving a 36.1% increase in mAP$_{50:95}$ and 20.6% increase in mAP$_{50}$ on the VisDrone2019-DET dataset compared to the baseline model. These enhancements make SOD-YOLO a practical and efficient solution for small object detection in UAV imagery. Our source code, hyper-parameters, and model weights are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 14,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12730",
        "abs_url": "https://arxiv.org/abs/2507.12730",
        "pdf_url": "https://arxiv.org/pdf/2507.12730",
        "title": "A Privacy-Preserving Semantic-Segmentation Method Using Domain-Adaptation Technique",
        "authors": [
            "Homare Sueyoshi",
            "Kiyoshi Nishikawa",
            "Hitoshi Kiya"
        ],
        "comments": "4 pages, 5 figures, 1 table. Accepted to GCCE 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Cryptography and Security (cs.CR)",
        "abstract": "We propose a privacy-preserving semantic-segmentation method for applying perceptual encryption to images used for model training in addition to test images. This method also provides almost the same accuracy as models without any encryption. The above performance is achieved using a domain-adaptation technique on the embedding structure of the Vision Transformer (ViT). The effectiveness of the proposed method was experimentally confirmed in terms of the accuracy of semantic segmentation when using a powerful semantic-segmentation model with ViT called Segmentation Transformer.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 15,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12739",
        "abs_url": "https://arxiv.org/abs/2507.12739",
        "pdf_url": "https://arxiv.org/pdf/2507.12739",
        "title": "Transformer-based Spatial Grounding: A Comprehensive Survey",
        "authors": [
            "Ijazul Haq",
            "Muhammad Saqib",
            "Yingjie Zhang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Spatial grounding, the process of associating natural language expressions with corresponding image regions, has rapidly advanced due to the introduction of transformer-based models, significantly enhancing multimodal representation and cross-modal alignment. Despite this progress, the field lacks a comprehensive synthesis of current methodologies, dataset usage, evaluation metrics, and industrial applicability. This paper presents a systematic literature review of transformer-based spatial grounding approaches from 2018 to 2025. Our analysis identifies dominant model architectures, prevalent datasets, and widely adopted evaluation metrics, alongside highlighting key methodological trends and best practices. This study provides essential insights and structured guidance for researchers and practitioners, facilitating the development of robust, reliable, and industry-ready transformer-based spatial grounding models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 16,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12755",
        "abs_url": "https://arxiv.org/abs/2507.12755",
        "pdf_url": "https://arxiv.org/pdf/2507.12755",
        "title": "Domain-Enhanced Dual-Branch Model for Efficient and Interpretable Accident Anticipation",
        "authors": [
            "Yanchen Guan",
            "Haicheng Liao",
            "Chengyue Wang",
            "Bonan Wang",
            "Jiaxun Zhang",
            "Jia Hu",
            "Zhenning Li"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Developing precise and computationally efficient traffic accident anticipation system is crucial for contemporary autonomous driving technologies, enabling timely intervention and loss prevention. In this paper, we propose an accident anticipation framework employing a dual-branch architecture that effectively integrates visual information from dashcam videos with structured textual data derived from accident reports. Furthermore, we introduce a feature aggregation method that facilitates seamless integration of multimodal inputs through large models (GPT-4o, Long-CLIP), complemented by targeted prompt engineering strategies to produce actionable feedback and standardized accident archives. Comprehensive evaluations conducted on benchmark datasets (DAD, CCD, and A3D) validate the superior predictive accuracy, enhanced responsiveness, reduced computational overhead, and improved interpretability of our approach, thus establishing a new benchmark for state-of-the-art performance in traffic accident anticipation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 17,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12758",
        "abs_url": "https://arxiv.org/abs/2507.12758",
        "pdf_url": "https://arxiv.org/pdf/2507.12758",
        "title": "HairShifter: Consistent and High-Fidelity Video Hair Transfer via Anchor-Guided Animation",
        "authors": [
            "Wangzheng Shi",
            "Yinglin Zheng",
            "Yuxin Lin",
            "Jianmin Bao",
            "Ming Zeng",
            "Dong Chen"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Hair transfer is increasingly valuable across domains such as social media, gaming, advertising, and entertainment. While significant progress has been made in single-image hair transfer, video-based hair transfer remains challenging due to the need for temporal consistency, spatial fidelity, and dynamic adaptability. In this work, we propose HairShifter, a novel \"Anchor Frame + Animation\" framework that unifies high-quality image hair transfer with smooth and coherent video animation. At its core, HairShifter integrates a Image Hair Transfer (IHT) module for precise per-frame transformation and a Multi-Scale Gated SPADE Decoder to ensure seamless spatial blending and temporal coherence. Our method maintains hairstyle fidelity across frames while preserving non-hair regions. Extensive experiments demonstrate that HairShifter achieves state-of-the-art performance in video hairstyle transfer, combining superior visual quality, temporal consistency, and scalability. The code will be publicly available. We believe this work will open new avenues for video-based hairstyle transfer and establish a robust baseline in this field.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 18,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12760",
        "abs_url": "https://arxiv.org/abs/2507.12760",
        "pdf_url": "https://arxiv.org/pdf/2507.12760",
        "title": "Unified Medical Image Segmentation with State Space Modeling Snake",
        "authors": [
            "Ruicheng Zhang",
            "Haowei Guo",
            "Kanghui Tian",
            "Jun Zhou",
            "Mingliang Yan",
            "Zeyu Zhang",
            "Shen Zhao"
        ],
        "comments": "This paper has been accepted by ACM MM 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Unified Medical Image Segmentation (UMIS) is critical for comprehensive anatomical assessment but faces challenges due to multi-scale structural heterogeneity. Conventional pixel-based approaches, lacking object-level anatomical insight and inter-organ relational modeling, struggle with morphological complexity and feature conflicts, limiting their efficacy in UMIS. We propose Mamba Snake, a novel deep snake framework enhanced by state space modeling for UMIS. Mamba Snake frames multi-contour evolution as a hierarchical state space atlas, effectively modeling macroscopic inter-organ topological relationships and microscopic contour refinements. We introduce a snake-specific vision state space module, the Mamba Evolution Block (MEB), which leverages effective spatiotemporal information aggregation for adaptive refinement of complex morphologies. Energy map shape priors further ensure robust long-range contour evolution in heterogeneous data. Additionally, a dual-classification synergy mechanism is incorporated to concurrently optimize detection and segmentation, mitigating under-segmentation of microstructures in UMIS. Extensive evaluations across five clinical datasets reveal Mamba Snake's superior performance, with an average Dice improvement of 3\\% over state-of-the-art methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 19,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12761",
        "abs_url": "https://arxiv.org/abs/2507.12761",
        "pdf_url": "https://arxiv.org/pdf/2507.12761",
        "title": "Think-Before-Draw: Decomposing Emotion Semantics & Fine-Grained Controllable Expressive Talking Head Generation",
        "authors": [
            "Hanlei Shi",
            "Leyuan Qu",
            "Yu Liu",
            "Di Gao",
            "Yuhua Zheng",
            "Taihao Li"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Emotional talking-head generation has emerged as a pivotal research area at the intersection of computer vision and multimodal artificial intelligence, with its core value lying in enhancing human-computer interaction through immersive and empathetic this http URL the advancement of multimodal large language models, the driving signals for emotional talking-head generation has shifted from audio and video to more flexible text. However, current text-driven methods rely on predefined discrete emotion label texts, oversimplifying the dynamic complexity of real facial muscle movements and thus failing to achieve natural emotional this http URL study proposes the Think-Before-Draw framework to address two key challenges: (1) In-depth semantic parsing of emotions--by innovatively introducing Chain-of-Thought (CoT), abstract emotion labels are transformed into physiologically grounded facial muscle movement descriptions, enabling the mapping from high-level semantics to actionable motion features; and (2) Fine-grained expressiveness optimization--inspired by artists' portrait painting process, a progressive guidance denoising strategy is proposed, employing a \"global emotion localization--local muscle control\" mechanism to refine micro-expression dynamics in generated this http URL experiments demonstrate that our approach achieves state-of-the-art performance on widely-used benchmarks, including MEAD and HDTF. Additionally, we collected a set of portrait images to evaluate our model's zero-shot generation capability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 20,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12762",
        "abs_url": "https://arxiv.org/abs/2507.12762",
        "pdf_url": "https://arxiv.org/pdf/2507.12762",
        "title": "World Model-Based End-to-End Scene Generation for Accident Anticipation in Autonomous Driving",
        "authors": [
            "Yanchen Guan",
            "Haicheng Liao",
            "Chengyue Wang",
            "Xingcheng Liu",
            "Jiaxun Zhang",
            "Zhenning Li"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Reliable anticipation of traffic accidents is essential for advancing autonomous driving systems. However, this objective is limited by two fundamental challenges: the scarcity of diverse, high-quality training data and the frequent absence of crucial object-level cues due to environmental disruptions or sensor deficiencies. To tackle these issues, we propose a comprehensive framework combining generative scene augmentation with adaptive temporal reasoning. Specifically, we develop a video generation pipeline that utilizes a world model guided by domain-informed prompts to create high-resolution, statistically consistent driving scenarios, particularly enriching the coverage of edge cases and complex interactions. In parallel, we construct a dynamic prediction model that encodes spatio-temporal relationships through strengthened graph convolutions and dilated temporal operators, effectively addressing data incompleteness and transient visual noise. Furthermore, we release a new benchmark dataset designed to better capture diverse real-world driving risks. Extensive experiments on public and newly released datasets confirm that our framework enhances both the accuracy and lead time of accident anticipation, offering a robust solution to current data and modeling limitations in safety-critical autonomous driving applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 21,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12763",
        "abs_url": "https://arxiv.org/abs/2507.12763",
        "pdf_url": "https://arxiv.org/pdf/2507.12763",
        "title": "Continuous Marine Tracking via Autonomous UAV Handoff",
        "authors": [
            "Heegyeong Kim",
            "Alice James",
            "Avishkar Seth",
            "Endrowednes Kuantama",
            "Jane Williamson",
            "Yimeng Feng",
            "Richard Han"
        ],
        "comments": "6 pages, 5 figures, to be published in DroNet '25: Proceedings of the 10th Workshop on Micro Aerial Vehicle Networks, Systems, and Applications",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
        "abstract": "This paper introduces an autonomous UAV vision system for continuous, real-time tracking of marine animals, specifically sharks, in dynamic marine environments. The system integrates an onboard computer with a stabilised RGB-D camera and a custom-trained OSTrack pipeline, enabling visual identification under challenging lighting, occlusion, and sea-state conditions. A key innovation is the inter-UAV handoff protocol, which enables seamless transfer of tracking responsibilities between drones, extending operational coverage beyond single-drone battery limitations. Performance is evaluated on a curated shark dataset of 5,200 frames, achieving a tracking success rate of 81.9\\% during real-time flight control at 100 Hz, and robustness to occlusion, illumination variation, and background clutter. We present a seamless UAV handoff framework, where target transfer is attempted via high-confidence feature matching, achieving 82.9\\% target coverage. These results confirm the viability of coordinated UAV operations for extended marine tracking and lay the groundwork for scalable, autonomous monitoring.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 22,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12768",
        "abs_url": "https://arxiv.org/abs/2507.12768",
        "pdf_url": "https://arxiv.org/pdf/2507.12768",
        "title": "AnyPos: Automated Task-Agnostic Actions for Bimanual Manipulation",
        "authors": [
            "Hengkai Tan",
            "Yao Feng",
            "Xinyi Mao",
            "Shuhe Huang",
            "Guodong Liu",
            "Zhongkai Hao",
            "Hang Su",
            "Jun Zhu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Robotics (cs.RO)",
        "abstract": "Vision-language-action (VLA) models have shown promise on task-conditioned control in complex settings such as bimanual manipulation. However, the heavy reliance on task-specific human demonstrations limits their generalization and incurs high data acquisition costs. In this work, we present a new notion of task-agnostic action paradigm that decouples action execution from task-specific conditioning, enhancing scalability, efficiency, and cost-effectiveness. To address the data collection challenges posed by this paradigm -- such as low coverage density, behavioral redundancy, and safety risks -- we introduce ATARA (Automated Task-Agnostic Random Actions), a scalable self-supervised framework that accelerates collection by over $ 30\\times $ compared to human teleoperation. To further enable effective learning from task-agnostic data, which often suffers from distribution mismatch and irrelevant trajectories, we propose AnyPos, an inverse dynamics model equipped with Arm-Decoupled Estimation and a Direction-Aware Decoder (DAD). We additionally integrate a video-conditioned action validation module to verify the feasibility of learned policies across diverse manipulation tasks. Extensive experiments show that the AnyPos-ATARA pipeline yields a 51% improvement in test accuracy and achieves 30-40% higher success rates in downstream tasks such as lifting, pick-and-place, and clicking, using replay-based video validation. Project Page: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 23,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12771",
        "abs_url": "https://arxiv.org/abs/2507.12771",
        "pdf_url": "https://arxiv.org/pdf/2507.12771",
        "title": "Local Representative Token Guided Merging for Text-to-Image Generation",
        "authors": [
            "Min-Jeong Lee",
            "Hee-Dong Kim",
            "Seong-Whan Lee"
        ],
        "comments": "6 pages",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Stable diffusion is an outstanding image generation model for text-to-image, but its time-consuming generation process remains a challenge due to the quadratic complexity of attention operations. Recent token merging methods improve efficiency by reducing the number of tokens during attention operations, but often overlook the characteristics of attention-based image generation models, limiting their effectiveness. In this paper, we propose local representative token guided merging (ReToM), a novel token merging strategy applicable to any attention mechanism in image generation. To merge tokens based on various contextual information, ReToM defines local boundaries as windows within attention inputs and adjusts window sizes. Furthermore, we introduce a representative token, which represents the most representative token per window by computing similarity at a specific timestep and selecting the token with the highest average similarity. This approach preserves the most salient local features while minimizing computational overhead. Experimental results show that ReToM achieves a 6.2% improvement in FID and higher CLIP scores compared to the baseline, while maintaining comparable inference time. We empirically demonstrate that ReToM is effective in balancing visual quality and computational efficiency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 24,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12780",
        "abs_url": "https://arxiv.org/abs/2507.12780",
        "pdf_url": "https://arxiv.org/pdf/2507.12780",
        "title": "Compact Vision Transformer by Reduction of Kernel Complexity",
        "authors": [
            "Yancheng Wang",
            "Yingzhen Yang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Self-attention and transformer architectures have become foundational components in modern deep learning. Recent efforts have integrated transformer blocks into compact neural architectures for computer vision, giving rise to various efficient vision transformers. In this work, we introduce Transformer with Kernel Complexity Reduction, or KCR-Transformer, a compact transformer block equipped with differentiable channel selection, guided by a novel and sharp theoretical generalization bound. KCR-Transformer performs input/output channel selection in the MLP layers of transformer blocks to reduce the computational cost. Furthermore, we provide a rigorous theoretical analysis establishing a tight generalization bound for networks equipped with KCR-Transformer blocks. Leveraging such strong theoretical results, the channel pruning by KCR-Transformer is conducted in a generalization-aware manner, ensuring that the resulting network retains a provably small generalization error. Our KCR-Transformer is compatible with many popular and compact transformer networks, such as ViT and Swin, and it reduces the FLOPs of the vision transformers while maintaining or even improving the prediction accuracy. In the experiments, we replace all the transformer blocks in the vision transformers with KCR-Transformer blocks, leading to KCR-Transformer networks with different backbones. The resulting TCR-Transformers achieve superior performance on various computer vision tasks, achieving even better performance than the original models with even less FLOPs and parameters.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 25,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12795",
        "abs_url": "https://arxiv.org/abs/2507.12795",
        "pdf_url": "https://arxiv.org/pdf/2507.12795",
        "title": "City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning",
        "authors": [
            "Penglei Sun",
            "Yaoxian Song",
            "Xiangru Zhu",
            "Xiang Liu",
            "Qiang Wang",
            "Yue Liu",
            "Changqun Xia",
            "Tiefeng Li",
            "Yang Yang",
            "Xiaowen Chu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Scene understanding enables intelligent agents to interpret and comprehend their environment. While existing large vision-language models (LVLMs) for scene understanding have primarily focused on indoor household tasks, they face two significant limitations when applied to outdoor large-scale scene understanding. First, outdoor scenarios typically encompass larger-scale environments observed through various sensors from multiple viewpoints (e.g., bird view and terrestrial view), while existing indoor LVLMs mainly analyze single visual modalities within building-scale contexts from humanoid viewpoints. Second, existing LVLMs suffer from missing multidomain perception outdoor data and struggle to effectively integrate 2D and 3D visual information. To address the aforementioned limitations, we build the first multidomain perception outdoor scene understanding dataset, named \\textbf{\\underline{SVM-City}}, deriving from multi\\textbf{\\underline{S}}cale scenarios with multi\\textbf{\\underline{V}}iew and multi\\textbf{\\underline{M}}odal instruction tuning data. It contains $420$k images and $4, 811$M point clouds with $567$k question-answering pairs from vehicles, low-altitude drones, high-altitude aerial planes, and satellite. To effectively fuse the multimodal data in the absence of one modality, we introduce incomplete multimodal learning to model outdoor scene understanding and design the LVLM named \\textbf{\\underline{City-VLM}}. Multimodal fusion is realized by constructing a joint probabilistic distribution space rather than implementing directly explicit fusion operations (e.g., concatenation). Experimental results on three typical outdoor scene understanding tasks show City-VLM achieves $18.14 \\%$ performance surpassing existing LVLMs in question-answering tasks averagely. Our method demonstrates pragmatic and generalization performance across multiple outdoor scenes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 26,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12796",
        "abs_url": "https://arxiv.org/abs/2507.12796",
        "pdf_url": "https://arxiv.org/pdf/2507.12796",
        "title": "DeQA-Doc: Adapting DeQA-Score to Document Image Quality Assessment",
        "authors": [
            "Junjie Gao",
            "Runze Liu",
            "Yingzhe Peng",
            "Shujian Yang",
            "Jin Zhang",
            "Kai Yang",
            "Zhiyuan You"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Document quality assessment is critical for a wide range of applications including document digitization, OCR, and archival. However, existing approaches often struggle to provide accurate and robust quality scores, limiting their applicability in practical scenarios. With the rapid progress in Multi-modal Large Language Models (MLLMs), recent MLLM-based methods have achieved remarkable performance in image quality assessment. In this work, we extend this success to the document domain by adapting DeQA-Score, a state-of-the-art MLLM-based image quality scorer, for document quality assessment. We propose DeQA-Doc, a framework that leverages the visual language capabilities of MLLMs and a soft label strategy to regress continuous document quality scores. To adapt DeQA-Score to DeQA-Doc, we adopt two complementary solutions to construct soft labels without the variance information. Also, we relax the resolution constrains to support the large resolution of document images. Finally, we introduce ensemble methods to further enhance the performance. Extensive experiments demonstrate that DeQA-Doc significantly outperforms existing baselines, offering accurate and generalizable document quality assessment across diverse degradation types. Codes and model weights are available in this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 27,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12804",
        "abs_url": "https://arxiv.org/abs/2507.12804",
        "pdf_url": "https://arxiv.org/pdf/2507.12804",
        "title": "ATL-Diff: Audio-Driven Talking Head Generation with Early Landmarks-Guide Noise Diffusion",
        "authors": [
            "Hoang-Son Vo",
            "Quang-Vinh Nguyen",
            "Seungwon Kim",
            "Hyung-Jeong Yang",
            "Soonja Yeom",
            "Soo-Hyung Kim"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Audio-driven talking head generation requires precise synchronization between facial animations and audio signals. This paper introduces ATL-Diff, a novel approach addressing synchronization limitations while reducing noise and computational costs. Our framework features three key components: a Landmark Generation Module converting audio to facial landmarks, a Landmarks-Guide Noise approach that decouples audio by distributing noise according to landmarks, and a 3D Identity Diffusion network preserving identity characteristics. Experiments on MEAD and CREMA-D datasets demonstrate that ATL-Diff outperforms state-of-the-art methods across all metrics. Our approach achieves near real-time processing with high-quality animations, computational efficiency, and exceptional preservation of facial nuances. This advancement offers promising applications for virtual assistants, education, medical communication, and digital platforms. The source code is available at: \\href{this https URL}{this https URL}",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 28,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12807",
        "abs_url": "https://arxiv.org/abs/2507.12807",
        "pdf_url": "https://arxiv.org/pdf/2507.12807",
        "title": "Semantic-guided Fine-tuning of Foundation Model for Long-tailed Visual Recognition",
        "authors": [
            "Yufei Peng",
            "Yonggang Zhang",
            "Yiu-ming Cheung"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The variance in class-wise sample sizes within long-tailed scenarios often results in degraded performance in less frequent classes. Fortunately, foundation models, pre-trained on vast open-world datasets, demonstrate strong potential for this task due to their generalizable representation, which promotes the development of adaptive strategies on pre-trained models in long-tailed learning. Advanced fine-tuning methods typically adjust visual encoders while neglecting the semantics derived from the frozen text encoder, overlooking the visual and textual alignment. To strengthen this alignment, we propose a novel approach, Semantic-guided fine-tuning of foundation model for long-tailed visual recognition (Sage), which incorporates semantic guidance derived from textual modality into the visual fine-tuning process. Specifically, we introduce an SG-Adapter that integrates class descriptions as semantic guidance to guide the fine-tuning of the visual encoder. The introduced guidance is passesed through the attention mechanism and enables the model to focus more on semantically relevant content, strengthening the alignment between the visual and textual modalities. Due to the inconsistent class-conditional distributions neglected by the existing loss function, the resulting prediction bias causes performance improvements for the tail class less than for the head class, even when the multi-modal alignment is enhanced. To address this challenge, we propose a novel distribution mismatch-aware compensation factor, which is specifically designed to rectify the prediction bias caused by the ignored inconsistent distribution based on our theoretical analysis, and is seamlessly integrated into the loss function. Extensive experiments on benchmark datasets demonstrate the effectiveness of the proposed Sage in enhancing performance in long-tailed learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 29,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12816",
        "abs_url": "https://arxiv.org/abs/2507.12816",
        "pdf_url": "https://arxiv.org/pdf/2507.12816",
        "title": "FIQ: Fundamental Question Generation with the Integration of Question Embeddings for Video Question Answering",
        "authors": [
            "Ju-Young Oh",
            "Ho-Joong Kim",
            "Seong-Whan Lee"
        ],
        "comments": "SMC 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Video question answering (VQA) is a multimodal task that requires the interpretation of a video to answer a given question. Existing VQA methods primarily utilize question and answer (Q&A) pairs to learn the spatio-temporal characteristics of video content. However, these annotations are typically event-centric, which is not enough to capture the broader context of each video. The absence of essential details such as object types, spatial layouts, and descriptive attributes restricts the model to learning only a fragmented scene representation. This issue limits the model's capacity for generalization and higher-level reasoning. In this paper, we propose a fundamental question generation with the integration of question embeddings for video question answering (FIQ), a novel approach designed to strengthen the reasoning ability of the model by enhancing the fundamental understanding of videos. FIQ generates Q&A pairs based on descriptions extracted from videos, enriching the training data with fundamental scene information. Generated Q&A pairs enable the model to understand the primary context, leading to enhanced generalizability and reasoning ability. Furthermore, we incorporate a VQ-CAlign module that assists task-specific question embeddings with visual features, ensuring that essential domain-specific details are preserved to increase the adaptability of downstream tasks. Experiments on SUTD-TrafficQA demonstrate that our FIQ achieves state-of-the-art performance compared to existing baseline methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 30,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12819",
        "abs_url": "https://arxiv.org/abs/2507.12819",
        "pdf_url": "https://arxiv.org/pdf/2507.12819",
        "title": "MCoT-RE: Multi-Faceted Chain-of-Thought and Re-Ranking for Training-Free Zero-Shot Composed Image Retrieval",
        "authors": [
            "Jeong-Woo Park",
            "Seong-Whan Lee"
        ],
        "comments": "6 pages, 4 figures, 2025 IEEE International Conference on Systems, Man, and Cybernetics",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Composed Image Retrieval (CIR) is the task of retrieving a target image from a gallery using a composed query consisting of a reference image and a modification text. Among various CIR approaches, training-free zero-shot methods based on pre-trained models are cost-effective but still face notable limitations. For example, sequential VLM-LLM pipelines process each modality independently, which often results in information loss and limits cross-modal interaction. In contrast, methods based on multimodal large language models (MLLMs) often focus exclusively on applying changes indicated by the text, without fully utilizing the contextual visual information from the reference image. To address these issues, we propose multi-faceted Chain-of-Thought with re-ranking (MCoT-RE), a training-free zero-shot CIR framework. MCoT-RE utilizes multi-faceted Chain-of-Thought to guide the MLLM to balance explicit modifications and contextual visual cues, generating two distinct captions: one focused on modification and the other integrating comprehensive visual-textual context. The first caption is used to filter candidate images. Subsequently, we combine these two captions and the reference image to perform multi-grained re-ranking. This two-stage approach facilitates precise retrieval by aligning with the textual modification instructions while preserving the visual context of the reference image. Through extensive experiments, MCoT-RE achieves state-of-the-art results among training-free methods, yielding improvements of up to 6.24% in Recall@10 on FashionIQ and 8.58% in Recall@1 on CIRR.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 31,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12823",
        "abs_url": "https://arxiv.org/abs/2507.12823",
        "pdf_url": "https://arxiv.org/pdf/2507.12823",
        "title": "FAR-Net: Multi-Stage Fusion Network with Enhanced Semantic Alignment and Adaptive Reconciliation for Composed Image Retrieval",
        "authors": [
            "Jeong-Woo Park",
            "Young-Eun Kim",
            "Seong-Whan Lee"
        ],
        "comments": "6 pages, 3 figures, 3 tables",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Composed image retrieval (CIR) is a vision language task that retrieves a target image using a reference image and modification text, enabling intuitive specification of desired changes. While effectively fusing visual and textual modalities is crucial, existing methods typically adopt either early or late fusion. Early fusion tends to excessively focus on explicitly mentioned textual details and neglect visual context, whereas late fusion struggles to capture fine-grained semantic alignments between image regions and textual tokens. To address these issues, we propose FAR-Net, a multi-stage fusion framework designed with enhanced semantic alignment and adaptive reconciliation, integrating two complementary modules. The enhanced semantic alignment module (ESAM) employs late fusion with cross-attention to capture fine-grained semantic relationships, while the adaptive reconciliation module (ARM) applies early fusion with uncertainty embeddings to enhance robustness and adaptability. Experiments on CIRR and FashionIQ show consistent performance gains, improving Recall@1 by up to 2.4% and Recall@50 by 1.04% over existing state-of-the-art methods, empirically demonstrating that FAR Net provides a robust and scalable solution to CIR tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 32,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12828",
        "abs_url": "https://arxiv.org/abs/2507.12828",
        "pdf_url": "https://arxiv.org/pdf/2507.12828",
        "title": "Feature-Enhanced TResNet for Fine-Grained Food Image Classification",
        "authors": [
            "Lulu Liu",
            "Zhiyong Xiao"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Food is not only a core component of humans' daily diets, but also an important carrier of cultural heritage and emotional bonds. With the development of technology, the need for accurate classification of food images has grown, which is crucial for a variety of application scenarios. However, existing Convolutional Neural Networks (CNNs) face significant challenges when dealing with fine-grained food images that are similar in shape but subtle in detail. To address this challenge, this study presents an innovative method for classifying food images, named Feature-Enhanced TResNet (FE-TResNet), specifically designed to address fine-grained food images and accurately capture subtle features within them. The FE-TResNet method is based on the TResNet model and integrates Style-based Recalibration Module (StyleRM) and Deep Channel-wise Attention (DCA) technologies to enhance feature extraction capabilities. In experimental validation on Chinese food image datasets ChineseFoodNet and CNFOOD-241, the FE-TResNet method significantly improved classification accuracy, achieving rates of 81.37% and 80.29%, respectively, demonstrating its effectiveness and superiority in fine-grained food image classification.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 33,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12832",
        "abs_url": "https://arxiv.org/abs/2507.12832",
        "pdf_url": "https://arxiv.org/pdf/2507.12832",
        "title": "MVA 2025 Small Multi-Object Tracking for Spotting Birds Challenge: Dataset, Methods, and Results",
        "authors": [
            "Yuki Kondo",
            "Norimichi Ukita",
            "Riku Kanayama",
            "Yuki Yoshida",
            "Takayuki Yamaguchi",
            "Xiang Yu",
            "Guang Liang",
            "Xinyao Liu",
            "Guan-Zhang Wang",
            "Wei-Ta Chu",
            "Bing-Cheng Chuang",
            "Jia-Hua Lee",
            "Pin-Tseng Kuo",
            "I-Hsuan Chu",
            "Yi-Shein Hsiao",
            "Cheng-Han Wu",
            "Po-Yi Wu",
            "Jui-Chien Tsou",
            "Hsuan-Chi Liu",
            "Chun-Yi Lee",
            "Yuan-Fu Yang",
            "Kosuke Shigematsu",
            "Asuka Shin",
            "Ba Tran"
        ],
        "comments": "This paper is the official challenge report for SMOT4SB and is published in the proceedings of MVA 2025 (19th International Conference on Machine Vision and Applications). Official challenge page: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Small Multi-Object Tracking (SMOT) is particularly challenging when targets occupy only a few dozen pixels, rendering detection and appearance-based association unreliable. Building on the success of the MVA2023 SOD4SB challenge, this paper introduces the SMOT4SB challenge, which leverages temporal information to address limitations of single-frame detection. Our three main contributions are: (1) the SMOT4SB dataset, consisting of 211 UAV video sequences with 108,192 annotated frames under diverse real-world conditions, designed to capture motion entanglement where both camera and targets move freely in 3D; (2) SO-HOTA, a novel metric combining Dot Distance with HOTA to mitigate the sensitivity of IoU-based metrics to small displacements; and (3) a competitive MVA2025 challenge with 78 participants and 308 submissions, where the winning method achieved a 5.1x improvement over the baseline. This work lays a foundation for advancing SMOT in UAV scenarios with applications in bird strike avoidance, agriculture, fisheries, and ecological monitoring.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 34,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12841",
        "abs_url": "https://arxiv.org/abs/2507.12841",
        "pdf_url": "https://arxiv.org/pdf/2507.12841",
        "title": "AnyCap Project: A Unified Framework, Dataset, and Benchmark for Controllable Omni-modal Captioning",
        "authors": [
            "Yiming Ren",
            "Zhiqiang Lin",
            "Yu Li",
            "Gao Meng",
            "Weiyun Wang",
            "Junjie Wang",
            "Zicheng Lin",
            "Jifeng Dai",
            "Yujiu Yang",
            "Wenhai Wang",
            "Ruihang Chu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Controllable captioning is essential for precise multimodal alignment and instruction following, yet existing models often lack fine-grained control and reliable evaluation protocols. To address this gap, we present the AnyCap Project, an integrated solution spanning model, dataset, and evaluation. We introduce AnyCapModel (ACM), a lightweight plug-and-play framework that enhances the controllability of existing foundation models for omni-modal captioning without retraining the base model. ACM reuses the original captions from base models while incorporating user instructions and modality features to generate improved captions. To remedy the data scarcity in controllable multimodal captioning, we build AnyCapDataset (ACD), covering three modalities, 28 user-instruction types, and 300\\,k high-quality data entries. We further propose AnyCapEval, a new benchmark that provides more reliable evaluation metrics for controllable captioning by decoupling content accuracy and stylistic fidelity. ACM markedly improves caption quality across a diverse set of base models on AnyCapEval. Notably, ACM-8B raises GPT-4oś content scores by 45\\% and style scores by 12\\%, and it also achieves substantial gains on widely used benchmarks such as MIA-Bench and VidCapBench.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 35,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12845",
        "abs_url": "https://arxiv.org/abs/2507.12845",
        "pdf_url": "https://arxiv.org/pdf/2507.12845",
        "title": "SEMT: Static-Expansion-Mesh Transformer Network Architecture for Remote Sensing Image Captioning",
        "authors": [
            "Khang Truong",
            "Lam Pham",
            "Hieu Tang",
            "Jasmin Lampert",
            "Martin Boyer",
            "Son Phan",
            "Truong Nguyen"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Image captioning has emerged as a crucial task in the intersection of computer vision and natural language processing, enabling automated generation of descriptive text from visual content. In the context of remote sensing, image captioning plays a significant role in interpreting vast and complex satellite imagery, aiding applications such as environmental monitoring, disaster assessment, and urban planning. This motivates us, in this paper, to present a transformer based network architecture for remote sensing image captioning (RSIC) in which multiple techniques of Static Expansion, Memory-Augmented Self-Attention, Mesh Transformer are evaluated and integrated. We evaluate our proposed models using two benchmark remote sensing image datasets of UCM-Caption and NWPU-Caption. Our best model outperforms the state-of-the-art systems on most of evaluation metrics, which demonstrates potential to apply for real-life remote sensing image systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 36,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12851",
        "abs_url": "https://arxiv.org/abs/2507.12851",
        "pdf_url": "https://arxiv.org/pdf/2507.12851",
        "title": "Simulate, Refocus and Ensemble: An Attention-Refocusing Scheme for Domain Generalization",
        "authors": [
            "Ziyi Wang",
            "Zhi Gao",
            "Jin Chen",
            "Qingjie Zhao",
            "Xinxiao Wu",
            "Jiebo Luo"
        ],
        "comments": "\\c{opyright} 20XX IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Domain generalization (DG) aims to learn a model from source domains and apply it to unseen target domains with out-of-distribution data. Owing to CLIP's strong ability to encode semantic concepts, it has attracted increasing interest in domain generalization. However, CLIP often struggles to focus on task-relevant regions across domains, i.e., domain-invariant regions, resulting in suboptimal performance on unseen target domains. To address this challenge, we propose an attention-refocusing scheme, called Simulate, Refocus and Ensemble (SRE), which learns to reduce the domain shift by aligning the attention maps in CLIP via attention refocusing. SRE first simulates domain shifts by performing augmentation on the source data to generate simulated target domains. SRE then learns to reduce the domain shifts by refocusing the attention in CLIP between the source and simulated target domains. Finally, SRE utilizes ensemble learning to enhance the ability to capture domain-invariant attention maps between the source data and the simulated target data. Extensive experimental results on several datasets demonstrate that SRE generally achieves better results than state-of-the-art methods. The code is available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 37,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12857",
        "abs_url": "https://arxiv.org/abs/2507.12857",
        "pdf_url": "https://arxiv.org/pdf/2507.12857",
        "title": "SCORE: Scene Context Matters in Open-Vocabulary Remote Sensing Instance Segmentation",
        "authors": [
            "Shiqi Huang",
            "Shuting He",
            "Huaiyuan Qin",
            "Bihan Wen"
        ],
        "comments": "ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Most existing remote sensing instance segmentation approaches are designed for close-vocabulary prediction, limiting their ability to recognize novel categories or generalize across datasets. This restricts their applicability in diverse Earth observation scenarios. To address this, we introduce open-vocabulary (OV) learning for remote sensing instance segmentation. While current OV segmentation models perform well on natural image datasets, their direct application to remote sensing faces challenges such as diverse landscapes, seasonal variations, and the presence of small or ambiguous objects in aerial imagery. To overcome these challenges, we propose $\\textbf{SCORE}$ ($\\textbf{S}$cene $\\textbf{C}$ontext matters in $\\textbf{O}$pen-vocabulary $\\textbf{RE}$mote sensing instance segmentation), a framework that integrates multi-granularity scene context, i.e., regional context and global context, to enhance both visual and textual representations. Specifically, we introduce Region-Aware Integration, which refines class embeddings with regional context to improve object distinguishability. Additionally, we propose Global Context Adaptation, which enriches naive text embeddings with remote sensing global context, creating a more adaptable and expressive linguistic latent space for the classifier. We establish new benchmarks for OV remote sensing instance segmentation across diverse datasets. Experimental results demonstrate that, our proposed method achieves SOTA performance, which provides a robust solution for large-scale, real-world geospatial analysis. Our code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 38,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12869",
        "abs_url": "https://arxiv.org/abs/2507.12869",
        "pdf_url": "https://arxiv.org/pdf/2507.12869",
        "title": "WhoFi: Deep Person Re-Identification via Wi-Fi Channel Signal Encoding",
        "authors": [
            "Danilo Avola",
            "Daniele Pannone",
            "Dario Montagnini",
            "Emad Emam"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Person Re-Identification is a key and challenging task in video surveillance. While traditional methods rely on visual data, issues like poor lighting, occlusion, and suboptimal angles often hinder performance. To address these challenges, we introduce WhoFi, a novel pipeline that utilizes Wi-Fi signals for person re-identification. Biometric features are extracted from Channel State Information (CSI) and processed through a modular Deep Neural Network (DNN) featuring a Transformer-based encoder. The network is trained using an in-batch negative loss function to learn robust and generalizable biometric signatures. Experiments on the NTU-Fi dataset show that our approach achieves competitive results compared to state-of-the-art methods, confirming its effectiveness in identifying individuals via Wi-Fi signals.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 39,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12883",
        "abs_url": "https://arxiv.org/abs/2507.12883",
        "pdf_url": "https://arxiv.org/pdf/2507.12883",
        "title": "HRSeg: High-Resolution Visual Perception and Enhancement for Reasoning Segmentation",
        "authors": [
            "Weihuang Lin",
            "Yiwei Ma",
            "Xiaoshuai Sun",
            "Shuting He",
            "Jiayi Ji",
            "Liujuan Cao",
            "Rongrong Ji"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The reasoning segmentation task involves segmenting objects within an image by interpreting implicit user instructions, which may encompass subtleties such as contextual cues and open-world knowledge. Despite significant advancements made by existing approaches, they remain constrained by low perceptual resolution, as visual encoders are typically pre-trained at lower resolutions. Furthermore, simply interpolating the positional embeddings of visual encoders to enhance perceptual resolution yields only marginal performance improvements while incurring substantial computational costs. To address this, we propose HRSeg, an efficient model with high-resolution fine-grained perception. It features two key innovations: High-Resolution Perception (HRP) and High-Resolution Enhancement (HRE). The HRP module processes high-resolution images through cropping, integrating local and global features for multi-granularity quality. The HRE module enhances mask features by integrating fine-grained information from high-resolution images, refining their alignment with text features for precise segmentation. Extensive ablation studies validate the effectiveness of our modules, while comprehensive experiments on multiple benchmark datasets demonstrate HRSeg's superior performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 40,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12884",
        "abs_url": "https://arxiv.org/abs/2507.12884",
        "pdf_url": "https://arxiv.org/pdf/2507.12884",
        "title": "From Neck to Head: Bio-Impedance Sensing for Head Pose Estimation",
        "authors": [
            "Mengxi Liu",
            "Lala Shakti Swarup Ray",
            "Sizhen Bian",
            "Ko Watanabe",
            "Ankur Bhatt",
            "Joanna Sorysz",
            "Russel Torah",
            "Bo Zhou",
            "Paul Lukowicz"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Signal Processing (eess.SP)",
        "abstract": "We present NeckSense, a novel wearable system for head pose tracking that leverages multi-channel bio-impedance sensing with soft, dry electrodes embedded in a lightweight, necklace-style form factor. NeckSense captures dynamic changes in tissue impedance around the neck, which are modulated by head rotations and subtle muscle activations. To robustly estimate head pose, we propose a deep learning framework that integrates anatomical priors, including joint constraints and natural head rotation ranges, into the loss function design. We validate NeckSense on 7 participants using the current SOTA pose estimation model as ground truth. Our system achieves a mean per-vertex error of 25.9 mm across various head movements with a leave-one-person-out cross-validation method, demonstrating that a compact, line-of-sight-free bio-impedance wearable can deliver head-tracking performance comparable to SOTA vision-based methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 41,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12889",
        "abs_url": "https://arxiv.org/abs/2507.12889",
        "pdf_url": "https://arxiv.org/pdf/2507.12889",
        "title": "Camera-based implicit mind reading by capturing higher-order semantic dynamics of human gaze within environmental context",
        "authors": [
            "Mengke Song",
            "Yuge Xie",
            "Qi Cui",
            "Luming Li",
            "Xinyu Liu",
            "Guotao Wang",
            "Chenglizhao Chen",
            "Shanchen Pang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Emotion recognition,as a step toward mind reading,seeks to infer internal states from external this http URL existing methods rely on explicit signals-such as facial expressions,speech,or gestures-that reflect only bodily responses and overlook the influence of environmental this http URL cues are often voluntary,easy to mask,and insufficient for capturing deeper,implicit emotions. Physiological signal-based approaches offer more direct access to internal states but require complex sensors that compromise natural behavior and limit this http URL-based methods typically rely on static fixation analysis and fail to capture the rich,dynamic interactions between gaze and the environment,and thus cannot uncover the deep connection between emotion and implicit this http URL address these limitations,we propose a novel camera-based,user-unaware emotion recognition approach that integrates gaze fixation patterns with environmental semantics and temporal this http URL standard HD cameras,our method unobtrusively captures users'eye appearance and head movements in natural settings-without the need for specialized hardware or active user this http URL these visual cues,the system estimates gaze trajectories over time and space, providing the basis for modeling the spatial, semantic,and temporal dimensions of gaze behavior. This allows us to capture the dynamic interplay between visual attention and the surrounding environment,revealing that emotions are not merely physiological responses but complex outcomes of human-environment this http URL proposed approach enables user-unaware,real-time,and continuous emotion recognition,offering high generalizability and low deployment cost.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 42,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12894",
        "abs_url": "https://arxiv.org/abs/2507.12894",
        "pdf_url": "https://arxiv.org/pdf/2507.12894",
        "title": "LanePerf: a Performance Estimation Framework for Lane Detection",
        "authors": [
            "Yin Wu",
            "Daniel Slieter",
            "Ahmed Abouelazm",
            "Christian Hubschneider",
            "J. Marius Zöllner"
        ],
        "comments": "Accepted in IEEE ITSC 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Lane detection is a critical component of Advanced Driver-Assistance Systems (ADAS) and Automated Driving System (ADS), providing essential spatial information for lateral control. However, domain shifts often undermine model reliability when deployed in new environments. Ensuring the robustness and safety of lane detection models typically requires collecting and annotating target domain data, which is resource-intensive. Estimating model performance without ground-truth labels offers a promising alternative for efficient robustness assessment, yet remains underexplored in lane detection. While previous work has addressed performance estimation in image classification, these methods are not directly applicable to lane detection tasks. This paper first adapts five well-performing performance estimation methods from image classification to lane detection, building a baseline. Addressing the limitations of prior approaches that solely rely on softmax scores or lane features, we further propose a new Lane Performance Estimation Framework (LanePerf), which integrates image and lane features using a pretrained image encoder and a DeepSets-based architecture, effectively handling zero-lane detection scenarios and large domain-shift cases. Extensive experiments on the OpenLane dataset, covering diverse domain shifts (scenes, weather, hours), demonstrate that our LanePerf outperforms all baselines, achieving a lower MAE of 0.117 and a higher Spearman's rank correlation coefficient of 0.727. These findings pave the way for robust, label-free performance estimation in ADAS, supporting more efficient testing and improved safety in challenging driving scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 43,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12903",
        "abs_url": "https://arxiv.org/abs/2507.12903",
        "pdf_url": "https://arxiv.org/pdf/2507.12903",
        "title": "Federated Learning for Commercial Image Sources",
        "authors": [
            "Shreyansh Jain",
            "Koteswar Rao Jerripothula"
        ],
        "comments": "Published in the Proceedings of IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2023 with DOI: https://doi.org/10.1109/WACV56688.2023.00647",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)",
        "abstract": "Federated Learning is a collaborative machine learning paradigm that enables multiple clients to learn a global model without exposing their data to each other. Consequently, it provides a secure learning platform with privacy-preserving capabilities. This paper introduces a new dataset containing 23,326 images collected from eight different commercial sources and classified into 31 categories, similar to the Office-31 dataset. To the best of our knowledge, this is the first image classification dataset specifically designed for Federated Learning. We also propose two new Federated Learning algorithms, namely Fed-Cyclic and Fed-Star. In Fed-Cyclic, a client receives weights from its previous client, updates them through local training, and passes them to the next client, thus forming a cyclic topology. In Fed-Star, a client receives weights from all other clients, updates its local weights through pre-aggregation (to address statistical heterogeneity) and local training, and sends its updated local weights to all other clients, thus forming a star-like topology. Our experiments reveal that both algorithms perform better than existing baselines on our newly introduced dataset.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 44,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12905",
        "abs_url": "https://arxiv.org/abs/2507.12905",
        "pdf_url": "https://arxiv.org/pdf/2507.12905",
        "title": "AthleticsPose: Authentic Sports Motion Dataset on Athletic Field and Evaluation of Monocular 3D Pose Estimation Ability",
        "authors": [
            "Tomohiro Suzuki",
            "Ryota Tanaka",
            "Calvin Yeung",
            "Keisuke Fujii"
        ],
        "comments": "9 pages, 5 figures, 5 tables",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Monocular 3D pose estimation is a promising, flexible alternative to costly motion capture systems for sports analysis. However, its practical application is hindered by two factors: a lack of realistic sports datasets and unclear reliability for sports tasks. To address these challenges, we introduce the AthleticsPose dataset, a new public dataset featuring ``real'' motions captured from 23 athletes performing various athletics events on an athletic field. Using this dataset, we trained a representative 3D pose estimation model and performed a comprehensive evaluation. Our results show that the model trained on AthleticsPose significantly outperforms a baseline model trained on an imitated sports motion dataset, reducing MPJPE by approximately 75 %. These results show the importance of training on authentic sports motion data, as models based on imitated motions do not effectively transfer to real-world motions. Further analysis reveals that estimation accuracy is sensitive to camera view and subject scale. In case studies of kinematic indicators, the model demonstrated the potential to capture individual differences in knee angles but struggled with higher-speed metrics, such as knee-drive velocity, due to prediction biases. This work provides the research community with a valuable dataset and clarifies the potential and practical limitations of using monocular 3D pose estimation for sports motion analysis. Our dataset, code, and checkpoints are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 45,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12916",
        "abs_url": "https://arxiv.org/abs/2507.12916",
        "pdf_url": "https://arxiv.org/pdf/2507.12916",
        "title": "Argus: Leveraging Multiview Images for Improved 3-D Scene Understanding With Large Language Models",
        "authors": [
            "Yifan Xu",
            "Chao Zhang",
            "Hanqi Jiang",
            "Xiaoyan Wang",
            "Ruifei Ma",
            "Yiwei Li",
            "Zihao Wu",
            "Zeju Li",
            "Xiangde Liu"
        ],
        "comments": "Accepted by TNNLS2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Advancements in foundation models have made it possible to conduct applications in various downstream tasks. Especially, the new era has witnessed a remarkable capability to extend Large Language Models (LLMs) for tackling tasks of 3D scene understanding. Current methods rely heavily on 3D point clouds, but the 3D point cloud reconstruction of an indoor scene often results in information loss. Some textureless planes or repetitive patterns are prone to omission and manifest as voids within the reconstructed 3D point clouds. Besides, objects with complex structures tend to introduce distortion of details caused by misalignments between the captured images and the dense reconstructed point clouds. 2D multi-view images present visual consistency with 3D point clouds and provide more detailed representations of scene components, which can naturally compensate for these deficiencies. Based on these insights, we propose Argus, a novel 3D multimodal framework that leverages multi-view images for enhanced 3D scene understanding with LLMs. In general, Argus can be treated as a 3D Large Multimodal Foundation Model (3D-LMM) since it takes various modalities as input(text instructions, 2D multi-view images, and 3D point clouds) and expands the capability of LLMs to tackle 3D tasks. Argus involves fusing and integrating multi-view images and camera poses into view-as-scene features, which interact with the 3D features to create comprehensive and detailed 3D-aware scene embeddings. Our approach compensates for the information loss while reconstructing 3D point clouds and helps LLMs better understand the 3D world. Extensive experiments demonstrate that our method outperforms existing 3D-LMMs in various downstream tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 46,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12933",
        "abs_url": "https://arxiv.org/abs/2507.12933",
        "pdf_url": "https://arxiv.org/pdf/2507.12933",
        "title": "DMQ: Dissecting Outliers of Diffusion Models for Post-Training Quantization",
        "authors": [
            "Dongyeun Lee",
            "Jiwan Hur",
            "Hyounguk Shon",
            "Jae Young Lee",
            "Junmo Kim"
        ],
        "comments": "Accepted by ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Diffusion models have achieved remarkable success in image generation but come with significant computational costs, posing challenges for deployment in resource-constrained environments. Recent post-training quantization (PTQ) methods have attempted to mitigate this issue by focusing on the iterative nature of diffusion models. However, these approaches often overlook outliers, leading to degraded performance at low bit-widths. In this paper, we propose a DMQ which combines Learned Equivalent Scaling (LES) and channel-wise Power-of-Two Scaling (PTS) to effectively address these challenges. Learned Equivalent Scaling optimizes channel-wise scaling factors to redistribute quantization difficulty between weights and activations, reducing overall quantization error. Recognizing that early denoising steps, despite having small quantization errors, crucially impact the final output due to error accumulation, we incorporate an adaptive timestep weighting scheme to prioritize these critical steps during learning. Furthermore, identifying that layers such as skip connections exhibit high inter-channel variance, we introduce channel-wise Power-of-Two Scaling for activations. To ensure robust selection of PTS factors even with small calibration set, we introduce a voting algorithm that enhances reliability. Extensive experiments demonstrate that our method significantly outperforms existing works, especially at low bit-widths such as W4A6 (4-bit weight, 6-bit activation) and W4A8, maintaining high image generation quality and model stability. The code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 47,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12939",
        "abs_url": "https://arxiv.org/abs/2507.12939",
        "pdf_url": "https://arxiv.org/pdf/2507.12939",
        "title": "A Deep-Learning Framework for Land-Sliding Classification from Remote Sensing Image",
        "authors": [
            "Hieu Tang",
            "Truong Vo",
            "Dong Pham",
            "Toan Nguyen",
            "Lam Pham",
            "Truong Nguyen"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The use of satellite imagery combined with deep learning to support automatic landslide detection is becoming increasingly widespread. However, selecting an appropriate deep learning architecture to optimize performance while avoiding overfitting remains a critical challenge. To address these issues, we propose a deep-learning based framework for landslide detection from remote sensing image in this paper. The proposed framework presents an effective combination of the online an offline data augmentation to tackle the imbalanced data, a backbone EfficientNet\\_Large deep learning model for extracting robust embedding features, and a post-processing SVM classifier to balance and enhance the classification performance. The proposed model achieved an F1-score of 0.8938 on the public test set of the Zindi challenge.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 48,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12942",
        "abs_url": "https://arxiv.org/abs/2507.12942",
        "pdf_url": "https://arxiv.org/pdf/2507.12942",
        "title": "Weakly Supervised Visible-Infrared Person Re-Identification via Heterogeneous Expert Collaborative Consistency Learning",
        "authors": [
            "Yafei Zhang",
            "Lingqi Kong",
            "Huafeng Li",
            "Jie Wen"
        ],
        "comments": "Accepted by ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "To reduce the reliance of visible-infrared person re-identification (ReID) models on labeled cross-modal samples, this paper explores a weakly supervised cross-modal person ReID method that uses only single-modal sample identity labels, addressing scenarios where cross-modal identity labels are unavailable. To mitigate the impact of missing cross-modal labels on model performance, we propose a heterogeneous expert collaborative consistency learning framework, designed to establish robust cross-modal identity correspondences in a weakly supervised manner. This framework leverages labeled data from each modality to independently train dedicated classification experts. To associate cross-modal samples, these classification experts act as heterogeneous predictors, predicting the identities of samples from the other modality. To improve prediction accuracy, we design a cross-modal relationship fusion mechanism that effectively integrates predictions from different experts. Under the implicit supervision provided by cross-modal identity correspondences, collaborative and consistent learning among the experts is encouraged, significantly enhancing the model's ability to extract modality-invariant features and improve cross-modal identity recognition. Experimental results on two challenging datasets validate the effectiveness of the proposed method.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 49,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12945",
        "abs_url": "https://arxiv.org/abs/2507.12945",
        "pdf_url": "https://arxiv.org/pdf/2507.12945",
        "title": "Analysis of Image-and-Text Uncertainty Propagation in Multimodal Large Language Models with Cardiac MR-Based Applications",
        "authors": [
            "Yucheng Tang",
            "Yunguan Fu",
            "Weixi Yi",
            "Yipei Wang",
            "Daniel C. Alexander",
            "Rhodri Davies",
            "Yipeng Hu"
        ],
        "comments": "It is accepted by 28th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI) 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Multimodal large language models (MLLMs) can process and integrate information from multimodality sources, such as text and images. However, interrelationship among input modalities, uncertainties due to individual uni-modal data and potential clinical applications following such an uncertainty decomposition are yet fully understood in the context of large-scale MLLMs. In this work, we propose a multimodal uncertainty propagation model (MUPM) based on uncertainty propagation, to characterise the relationship among the uncertainties arising from image-only, text-only, and joint image-text variations in MLLM inputs. Using real clinical data consisting of cardiac MR scans and digital health records, we describe that MUPMs can be optimised robustly with a few samples. We then show that the fitted MUPMs are generalisable across different input data distributions and, perhaps surprisingly, across different downstream tasks. Such a transferability may be explained by the shared pretraining, comparatively light MLLM fine-tuning, along with the low-dimensional nature of the MUPMs. More importantly, this learned transferability, quantifying the relationship between these uncertainties, led to direct clinical applications in which uncertainties may be estimated and thus analysed robustly for varying data or even a novel set of cardiac disease prediction tasks. In addition, we show experimentally the efficiency in multimodal data required for estimating the overall uncertainty and its ability to identify redundant factors, both of which are considered practical yet clinically useful applications with the proposed MUPMs. Codes are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 50,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12952",
        "abs_url": "https://arxiv.org/abs/2507.12952",
        "pdf_url": "https://arxiv.org/pdf/2507.12952",
        "title": "LoViC: Efficient Long Video Generation with Context Compression",
        "authors": [
            "Jiaxiu Jiang",
            "Wenbo Li",
            "Jingjing Ren",
            "Yuping Qiu",
            "Yong Guo",
            "Xiaogang Xu",
            "Han Wu",
            "Wangmeng Zuo"
        ],
        "comments": "Project page: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Despite recent advances in diffusion transformers (DiTs) for text-to-video generation, scaling to long-duration content remains challenging due to the quadratic complexity of self-attention. While prior efforts -- such as sparse attention and temporally autoregressive models -- offer partial relief, they often compromise temporal coherence or scalability. We introduce LoViC, a DiT-based framework trained on million-scale open-domain videos, designed to produce long, coherent videos through a segment-wise generation process. At the core of our approach is FlexFormer, an expressive autoencoder that jointly compresses video and text into unified latent representations. It supports variable-length inputs with linearly adjustable compression rates, enabled by a single query token design based on the Q-Former architecture. Additionally, by encoding temporal context through position-aware mechanisms, our model seamlessly supports prediction, retradiction, interpolation, and multi-shot generation within a unified paradigm. Extensive experiments across diverse tasks validate the effectiveness and versatility of our approach.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 51,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12953",
        "abs_url": "https://arxiv.org/abs/2507.12953",
        "pdf_url": "https://arxiv.org/pdf/2507.12953",
        "title": "cIDIR: Conditioned Implicit Neural Representation for Regularized Deformable Image Registration",
        "authors": [
            "Sidaty El Hadramy",
            "Oumeymah Cherkaoui",
            "Philippe C. Cattin"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Regularization is essential in deformable image registration (DIR) to ensure that the estimated Deformation Vector Field (DVF) remains smooth, physically plausible, and anatomically consistent. However, fine-tuning regularization parameters in learning-based DIR frameworks is computationally expensive, often requiring multiple training iterations. To address this, we propose cIDI, a novel DIR framework based on Implicit Neural Representations (INRs) that conditions the registration process on regularization hyperparameters. Unlike conventional methods that require retraining for each regularization hyperparameter setting, cIDIR is trained over a prior distribution of these hyperparameters, then optimized over the regularization hyperparameters by using the segmentations masks as an observation. Additionally, cIDIR models a continuous and differentiable DVF, enabling seamless integration of advanced regularization techniques via automatic differentiation. Evaluated on the DIR-LAB dataset, $\\operatorname{cIDIR}$ achieves high accuracy and robustness across the dataset.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 52,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12956",
        "abs_url": "https://arxiv.org/abs/2507.12956",
        "pdf_url": "https://arxiv.org/pdf/2507.12956",
        "title": "FantasyPortrait: Enhancing Multi-Character Portrait Animation with Expression-Augmented Diffusion Transformers",
        "authors": [
            "Qiang Wang",
            "Mengchao Wang",
            "Fan Jiang",
            "Yaqi Fan",
            "Yonggang Qi",
            "Mu Xu"
        ],
        "comments": "this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Producing expressive facial animations from static images is a challenging task. Prior methods relying on explicit geometric priors (e.g., facial landmarks or 3DMM) often suffer from artifacts in cross reenactment and struggle to capture subtle emotions. Furthermore, existing approaches lack support for multi-character animation, as driving features from different individuals frequently interfere with one another, complicating the task. To address these challenges, we propose FantasyPortrait, a diffusion transformer based framework capable of generating high-fidelity and emotion-rich animations for both single- and multi-character scenarios. Our method introduces an expression-augmented learning strategy that utilizes implicit representations to capture identity-agnostic facial dynamics, enhancing the model's ability to render fine-grained emotions. For multi-character control, we design a masked cross-attention mechanism that ensures independent yet coordinated expression generation, effectively preventing feature interference. To advance research in this area, we propose the Multi-Expr dataset and ExprBench, which are specifically designed datasets and benchmarks for training and evaluating multi-character portrait animations. Extensive experiments demonstrate that FantasyPortrait significantly outperforms state-of-the-art methods in both quantitative metrics and qualitative evaluations, excelling particularly in challenging cross reenactment and multi-character contexts. Our project page is this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 53,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12964",
        "abs_url": "https://arxiv.org/abs/2507.12964",
        "pdf_url": "https://arxiv.org/pdf/2507.12964",
        "title": "Demographic-aware fine-grained classification of pediatric wrist fractures",
        "authors": [
            "Ammar Ahmed",
            "Ali Shariq Imran",
            "Zenun Kastrati",
            "Sher Muhammad Daudpota"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Wrist pathologies are frequently observed, particularly among children who constitute the majority of fracture cases. However, diagnosing these conditions is time-consuming and requires specialized expertise. Computer vision presents a promising avenue, contingent upon the availability of extensive datasets, a notable challenge in medical imaging. Therefore, reliance solely on one modality, such as images, proves inadequate, especially in an era of diverse and plentiful data types. In this study, we employ a multifaceted approach to address the challenge of recognizing wrist pathologies using an extremely limited dataset. Initially, we approach the problem as a fine-grained recognition task, aiming to identify subtle X-ray pathologies that conventional CNNs overlook. Secondly, we enhance network performance by fusing patient metadata with X-ray images. Thirdly, rather than pre-training on a coarse-grained dataset like ImageNet, we utilize weights trained on a fine-grained dataset. While metadata integration has been used in other medical domains, this is a novel application for wrist pathologies. Our results show that a fine-grained strategy and metadata integration improve diagnostic accuracy by 2% with a limited dataset and by over 10% with a larger fracture-focused dataset.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 54,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12967",
        "abs_url": "https://arxiv.org/abs/2507.12967",
        "pdf_url": "https://arxiv.org/pdf/2507.12967",
        "title": "RGB Pre-Training Enhanced Unobservable Feature Latent Diffusion Model for Spectral Reconstruction",
        "authors": [
            "Keli Deng",
            "Jie Nie",
            "Yuntao Qian"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Spectral reconstruction (SR) is a crucial problem in image processing that requires reconstructing hyperspectral images (HSIs) from the corresponding RGB images. A key difficulty in SR is estimating the unobservable feature, which encapsulates significant spectral information not captured by RGB imaging sensors. The solution lies in effectively constructing the spectral-spatial joint distribution conditioned on the RGB image to complement the unobservable feature. Since HSIs share a similar spatial structure with the corresponding RGB images, it is rational to capitalize on the rich spatial knowledge in RGB pre-trained models for spectral-spatial joint distribution learning. To this end, we extend the RGB pre-trained latent diffusion model (RGB-LDM) to an unobservable feature LDM (ULDM) for SR. As the RGB-LDM and its corresponding spatial autoencoder (SpaAE) already excel in spatial knowledge, the ULDM can focus on modeling spectral structure. Moreover, separating the unobservable feature from the HSI reduces the redundant spectral information and empowers the ULDM to learn the joint distribution in a compact latent space. Specifically, we propose a two-stage pipeline consisting of spectral structure representation learning and spectral-spatial joint distribution learning to transform the RGB-LDM into the ULDM. In the first stage, a spectral unobservable feature autoencoder (SpeUAE) is trained to extract and compress the unobservable feature into a 3D manifold aligned with RGB space. In the second stage, the spectral and spatial structures are sequentially encoded by the SpeUAE and the SpaAE, respectively. The ULDM is then acquired to model the distribution of the coded unobservable feature with guidance from the corresponding RGB images. Experimental results on SR and downstream relighting tasks demonstrate that our proposed method achieves state-of-the-art performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 55,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12988",
        "abs_url": "https://arxiv.org/abs/2507.12988",
        "pdf_url": "https://arxiv.org/pdf/2507.12988",
        "title": "Variance-Based Pruning for Accelerating and Compressing Trained Networks",
        "authors": [
            "Uranik Berisha",
            "Jens Mehnert",
            "Alexandru Paul Condurache"
        ],
        "comments": "Accepted at IEEE/CVF International Conference on Computer Vision (ICCV) 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Increasingly expensive training of ever larger models such as Vision Transfomers motivate reusing the vast library of already trained state-of-the-art networks. However, their latency, high computational costs and memory demands pose significant challenges for deployment, especially on resource-constrained hardware. While structured pruning methods can reduce these factors, they often require costly retraining, sometimes for up to hundreds of epochs, or even training from scratch to recover the lost accuracy resulting from the structural modifications. Maintaining the provided performance of trained models after structured pruning and thereby avoiding extensive retraining remains a challenge. To solve this, we introduce Variance-Based Pruning, a simple and structured one-shot pruning technique for efficiently compressing networks, with minimal finetuning. Our approach first gathers activation statistics, which are used to select neurons for pruning. Simultaneously the mean activations are integrated back into the model to preserve a high degree of performance. On ImageNet-1k recognition tasks, we demonstrate that directly after pruning DeiT-Base retains over 70% of its original performance and requires only 10 epochs of fine-tuning to regain 99% of the original accuracy while simultaneously reducing MACs by 35% and model size by 36%, thus speeding up the model by 1.44x.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 56,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12998",
        "abs_url": "https://arxiv.org/abs/2507.12998",
        "pdf_url": "https://arxiv.org/pdf/2507.12998",
        "title": "Differential-informed Sample Selection Accelerates Multimodal Contrastive Learning",
        "authors": [
            "Zihua Zhao",
            "Feng Hong",
            "Mengxi Chen",
            "Pengyi Chen",
            "Benyuan Liu",
            "Jiangchao Yao",
            "Ya Zhang",
            "Yanfeng Wang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "The remarkable success of contrastive-learning-based multimodal models has been greatly driven by training on ever-larger datasets with expensive compute consumption. Sample selection as an alternative efficient paradigm plays an important direction to accelerate the training process. However, recent advances on sample selection either mostly rely on an oracle model to offline select a high-quality coreset, which is limited in the cold-start scenarios, or focus on online selection based on real-time model predictions, which has not sufficiently or efficiently considered the noisy correspondence. To address this dilemma, we propose a novel Differential-Informed Sample Selection (DISSect) method, which accurately and efficiently discriminates the noisy correspondence for training acceleration. Specifically, we rethink the impact of noisy correspondence on contrastive learning and propose that the differential between the predicted correlation of the current model and that of a historical model is more informative to characterize sample quality. Based on this, we construct a robust differential-based sample selection and analyze its theoretical insights. Extensive experiments on three benchmark datasets and various downstream tasks demonstrate the consistent superiority of DISSect over current state-of-the-art methods. Source code is available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 57,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13018",
        "abs_url": "https://arxiv.org/abs/2507.13018",
        "pdf_url": "https://arxiv.org/pdf/2507.13018",
        "title": "Beyond Fully Supervised Pixel Annotations: Scribble-Driven Weakly-Supervised Framework for Image Manipulation Localization",
        "authors": [
            "Songlin Li",
            "Guofeng Yu",
            "Zhiqing Guo",
            "Yunfeng Diao",
            "Dan Ma",
            "Gaobo Yang",
            "Liejun Wang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Deep learning-based image manipulation localization (IML) methods have achieved remarkable performance in recent years, but typically rely on large-scale pixel-level annotated datasets. To address the challenge of acquiring high-quality annotations, some recent weakly supervised methods utilize image-level labels to segment manipulated regions. However, the performance is still limited due to insufficient supervision signals. In this study, we explore a form of weak supervision that improves the annotation efficiency and detection performance, namely scribble annotation supervision. We re-annotated mainstream IML datasets with scribble labels and propose the first scribble-based IML (Sc-IML) dataset. Additionally, we propose the first scribble-based weakly supervised IML framework. Specifically, we employ self-supervised training with a structural consistency loss to encourage the model to produce consistent predictions under multi-scale and augmented inputs. In addition, we propose a prior-aware feature modulation module (PFMM) that adaptively integrates prior information from both manipulated and authentic regions for dynamic feature adjustment, further enhancing feature discriminability and prediction consistency in complex scenes. We also propose a gated adaptive fusion module (GAFM) that utilizes gating mechanisms to regulate information flow during feature fusion, guiding the model toward emphasizing potential tampered regions. Finally, we propose a confidence-aware entropy minimization loss (${\\mathcal{L}}_{ {CEM }}$). This loss dynamically regularizes predictions in weakly annotated or unlabeled regions based on model uncertainty, effectively suppressing unreliable predictions. Experimental results show that our method outperforms existing fully supervised approaches in terms of average performance both in-distribution and out-of-distribution.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 58,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13032",
        "abs_url": "https://arxiv.org/abs/2507.13032",
        "pdf_url": "https://arxiv.org/pdf/2507.13032",
        "title": "Resurrect Mask AutoRegressive Modeling for Efficient and Scalable Image Generation",
        "authors": [
            "Yi Xin",
            "Le Zhuo",
            "Qi Qin",
            "Siqi Luo",
            "Yuewen Cao",
            "Bin Fu",
            "Yangfan He",
            "Hongsheng Li",
            "Guangtao Zhai",
            "Xiaohong Liu",
            "Peng Gao"
        ],
        "comments": "24 pages, 10 figures, 10 tables",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "AutoRegressive (AR) models have made notable progress in image generation, with Masked AutoRegressive (MAR) models gaining attention for their efficient parallel decoding. However, MAR models have traditionally underperformed when compared to standard AR models. This study refines the MAR architecture to improve image generation quality. We begin by evaluating various image tokenizers to identify the most effective one. Subsequently, we introduce an improved Bidirectional LLaMA architecture by replacing causal attention with bidirectional attention and incorporating 2D RoPE, which together form our advanced model, MaskGIL. Scaled from 111M to 1.4B parameters, MaskGIL achieves a FID score of 3.71, matching state-of-the-art AR models in the ImageNet 256x256 benchmark, while requiring only 8 inference steps compared to the 256 steps of AR models. Furthermore, we develop a text-driven MaskGIL model with 775M parameters for generating images from text at various resolutions. Beyond image generation, MaskGIL extends to accelerate AR-based generation and enable real-time speech-to-image conversion. Our codes and models are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 59,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13061",
        "abs_url": "https://arxiv.org/abs/2507.13061",
        "pdf_url": "https://arxiv.org/pdf/2507.13061",
        "title": "Advancing Complex Wide-Area Scene Understanding with Hierarchical Coresets Selection",
        "authors": [
            "Jingyao Wang",
            "Yiming Chen",
            "Lingyu Si",
            "Changwen Zheng"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Scene understanding is one of the core tasks in computer vision, aiming to extract semantic information from images to identify objects, scene categories, and their interrelationships. Although advancements in Vision-Language Models (VLMs) have driven progress in this field, existing VLMs still face challenges in adaptation to unseen complex wide-area scenes. To address the challenges, this paper proposes a Hierarchical Coresets Selection (HCS) mechanism to advance the adaptation of VLMs in complex wide-area scene understanding. It progressively refines the selected regions based on the proposed theoretically guaranteed importance function, which considers utility, representativeness, robustness, and synergy. Without requiring additional fine-tuning, HCS enables VLMs to achieve rapid understandings of unseen scenes at any scale using minimal interpretable regions while mitigating insufficient feature density. HCS is a plug-and-play method that is compatible with any VLM. Experiments demonstrate that HCS achieves superior performance and universality in various tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 60,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13074",
        "abs_url": "https://arxiv.org/abs/2507.13074",
        "pdf_url": "https://arxiv.org/pdf/2507.13074",
        "title": "Label-Consistent Dataset Distillation with Detector-Guided Refinement",
        "authors": [
            "Yawen Zou",
            "Guang Li",
            "Zi Wang",
            "Chunzhi Gu",
            "Chao Zhang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Dataset distillation (DD) aims to generate a compact yet informative dataset that achieves performance comparable to the original dataset, thereby reducing demands on storage and computational resources. Although diffusion models have made significant progress in dataset distillation, the generated surrogate datasets often contain samples with label inconsistencies or insufficient structural detail, leading to suboptimal downstream performance. To address these issues, we propose a detector-guided dataset distillation framework that explicitly leverages a pre-trained detector to identify and refine anomalous synthetic samples, thereby ensuring label consistency and improving image quality. Specifically, a detector model trained on the original dataset is employed to identify anomalous images exhibiting label mismatches or low classification confidence. For each defective image, multiple candidates are generated using a pre-trained diffusion model conditioned on the corresponding image prototype and label. The optimal candidate is then selected by jointly considering the detector's confidence score and dissimilarity to existing qualified synthetic samples, thereby ensuring both label accuracy and intra-class diversity. Experimental results demonstrate that our method can synthesize high-quality representative images with richer details, achieving state-of-the-art performance on the validation set.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 61,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13082",
        "abs_url": "https://arxiv.org/abs/2507.13082",
        "pdf_url": "https://arxiv.org/pdf/2507.13082",
        "title": "Channel-wise Motion Features for Efficient Motion Segmentation",
        "authors": [
            "Riku Inoue",
            "Masamitsu Tsuchiya",
            "Yuji Yasui"
        ],
        "comments": "This paper has been accepted to IROS 2024 (Abu Dhabi, UAE), October 14-18, 2024",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "For safety-critical robotics applications such as autonomous driving, it is important to detect all required objects accurately in real-time. Motion segmentation offers a solution by identifying dynamic objects from the scene in a class-agnostic manner. Recently, various motion segmentation models have been proposed, most of which jointly use subnetworks to estimate Depth, Pose, Optical Flow, and Scene Flow. As a result, the overall computational cost of the model increases, hindering real-time performance. In this paper, we propose a novel cost-volume-based motion feature representation, Channel-wise Motion Features. By extracting depth features of each instance in the feature map and capturing the scene's 3D motion information, it offers enhanced efficiency. The only subnetwork used to build Channel-wise Motion Features is the Pose Network, and no others are required. Our method not only achieves about 4 times the FPS of state-of-the-art models in the KITTI Dataset and Cityscapes of the VCAS-Motion Dataset, but also demonstrates equivalent accuracy while reducing the parameters to about 25$\\%$.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 62,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13085",
        "abs_url": "https://arxiv.org/abs/2507.13085",
        "pdf_url": "https://arxiv.org/pdf/2507.13085",
        "title": "Decoupled PROB: Decoupled Query Initialization Tasks and Objectness-Class Learning for Open World Object Detection",
        "authors": [
            "Riku Inoue",
            "Masamitsu Tsuchiya",
            "Yuji Yasui"
        ],
        "comments": "This paper has been accepted to WACV 2025 (Tucson, Arizona, USA), February 28-March 4 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Open World Object Detection (OWOD) is a challenging computer vision task that extends standard object detection by (1) detecting and classifying unknown objects without supervision, and (2) incrementally learning new object classes without forgetting previously learned ones. The absence of ground truths for unknown objects makes OWOD tasks particularly challenging. Many methods have addressed this by using pseudo-labels for unknown objects. The recently proposed Probabilistic Objectness transformer-based open-world detector (PROB) is a state-of-the-art model that does not require pseudo-labels for unknown objects, as it predicts probabilistic objectness. However, this method faces issues with learning conflicts between objectness and class predictions. To address this issue and further enhance performance, we propose a novel model, Decoupled PROB. Decoupled PROB introduces Early Termination of Objectness Prediction (ETOP) to stop objectness predictions at appropriate layers in the decoder, resolving the learning conflicts between class and objectness predictions in PROB. Additionally, we introduce Task-Decoupled Query Initialization (TDQI), which efficiently extracts features of known and unknown objects, thereby improving performance. TDQI is a query initialization method that combines query selection and learnable queries, and it is a module that can be easily integrated into existing DETR-based OWOD models. Extensive experiments on OWOD benchmarks demonstrate that Decoupled PROB surpasses all existing methods across several metrics, significantly improving performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 63,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13087",
        "abs_url": "https://arxiv.org/abs/2507.13087",
        "pdf_url": "https://arxiv.org/pdf/2507.13087",
        "title": "DiffOSeg: Omni Medical Image Segmentation via Multi-Expert Collaboration Diffusion Model",
        "authors": [
            "Han Zhang",
            "Xiangde Luo",
            "Yong Chen",
            "Kang Li"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Annotation variability remains a substantial challenge in medical image segmentation, stemming from ambiguous imaging boundaries and diverse clinical expertise. Traditional deep learning methods producing single deterministic segmentation predictions often fail to capture these annotator biases. Although recent studies have explored multi-rater segmentation, existing methods typically focus on a single perspective -- either generating a probabilistic ``gold standard'' consensus or preserving expert-specific preferences -- thus struggling to provide a more omni view. In this study, we propose DiffOSeg, a two-stage diffusion-based framework, which aims to simultaneously achieve both consensus-driven (combining all experts' opinions) and preference-driven (reflecting experts' individual assessments) segmentation. Stage I establishes population consensus through a probabilistic consensus strategy, while Stage II captures expert-specific preference via adaptive prompts. Demonstrated on two public datasets (LIDC-IDRI and NPC-170), our model outperforms existing state-of-the-art methods across all evaluated metrics. Source code is available at this https URL .",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 64,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13089",
        "abs_url": "https://arxiv.org/abs/2507.13089",
        "pdf_url": "https://arxiv.org/pdf/2507.13089",
        "title": "GLAD: Generalizable Tuning for Vision-Language Models",
        "authors": [
            "Yuqi Peng",
            "Pengfei Wang",
            "Jianzhuang Liu",
            "Shifeng Chen"
        ],
        "comments": "ICCV 2025 workshop",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Pre-trained vision-language models, such as CLIP, show impressive zero-shot recognition ability and can be easily transferred to specific downstream tasks via prompt tuning, even with limited training data. However, existing prompt tuning methods face two main challenges: (1) In few-shot scenarios, data scarcity often leads to overfitting, making the model sensitive to changes in the input domain. (2) To mitigate overfitting, these methods typically rely on complex task-specific model architectures and sensitive hyperparameter tuning, severely restricting their general applicability. To address these issues, we propose a simpler and more general framework called GLAD (Generalizable LoRA tuning with RegulArized GraDient). We show that merely applying LoRA achieves performance in downstream tasks comparable to current state-of-the-art prompt-based methods. While LoRA is effective and easy to use, it remains susceptible to overfitting in few-shot learning scenarios. To mitigate this risk, we introduce a gradient-based regularization technique. This technique effectively steers the optimization trajectory, encouraging the model to find a more stable parameter region that is robust to variations in data distribution. Through extensive experiments conducted on 15 benchmark datasets, we demonstrate that GLAD outperforms previous tuning approaches in terms of base-to-novel class generalization, image domain generalization, and cross-dataset generalization. The code will be publicly available.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 65,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13106",
        "abs_url": "https://arxiv.org/abs/2507.13106",
        "pdf_url": "https://arxiv.org/pdf/2507.13106",
        "title": "Deep Learning-Based Fetal Lung Segmentation from Diffusion-weighted MRI Images and Lung Maturity Evaluation for Fetal Growth Restriction",
        "authors": [
            "Zhennan Xiao",
            "Katharine Brudkiewicz",
            "Zhen Yuan",
            "Rosalind Aughwane",
            "Magdalena Sokolska",
            "Joanna Chappell",
            "Trevor Gaunt",
            "Anna L. David",
            "Andrew P. King",
            "Andrew Melbourne"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Fetal lung maturity is a critical indicator for predicting neonatal outcomes and the need for post-natal intervention, especially for pregnancies affected by fetal growth restriction. Intra-voxel incoherent motion analysis has shown promising results for non-invasive assessment of fetal lung development, but its reliance on manual segmentation is time-consuming, thus limiting its clinical applicability. In this work, we present an automated lung maturity evaluation pipeline for diffusion-weighted magnetic resonance images that consists of a deep learning-based fetal lung segmentation model and a model-fitting lung maturity assessment. A 3D nnU-Net model was trained on manually segmented images selected from the baseline frames of 4D diffusion-weighted MRI scans. The segmentation model demonstrated robust performance, yielding a mean Dice coefficient of 82.14%. Next, voxel-wise model fitting was performed based on both the nnU-Net-predicted and manual lung segmentations to quantify IVIM parameters reflecting tissue microstructure and perfusion. The results suggested no differences between the two. Our work shows that a fully automated pipeline is possible for supporting fetal lung maturity assessment and clinical decision-making.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 66,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13107",
        "abs_url": "https://arxiv.org/abs/2507.13107",
        "pdf_url": "https://arxiv.org/pdf/2507.13107",
        "title": "R^2MoE: Redundancy-Removal Mixture of Experts for Lifelong Concept Learning",
        "authors": [
            "Xiaohan Guo",
            "Yusong Cai",
            "Zejia Liu",
            "Zhengning Wang",
            "Lili Pan",
            "Hongliang Li"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Enabling large-scale generative models to continuously learn new visual concepts is essential for personalizing pre-trained models to meet individual user preferences. Existing approaches for continual visual concept learning are constrained by two fundamental challenges: catastrophic forgetting and parameter expansion. In this paper, we propose Redundancy-Removal Mixture of Experts (R^2MoE), a parameter-efficient framework for lifelong visual concept learning that effectively learns new concepts while incurring minimal parameter overhead. Our framework includes three key innovative contributions: First, we propose a mixture-of-experts framework with a routing distillation mechanism that enables experts to acquire concept-specific knowledge while preserving the gating network's routing capability, thereby effectively mitigating catastrophic forgetting. Second, we propose a strategy for eliminating redundant layer-wise experts that reduces the number of expert parameters by fully utilizing previously learned experts. Third, we employ a hierarchical local attention-guided inference approach to mitigate interference between generated visual concepts. Extensive experiments have demonstrated that our method generates images with superior conceptual fidelity compared to the state-of-the-art (SOTA) method, achieving an impressive 87.8\\% reduction in forgetting rates and 63.3\\% fewer parameters on the CustomConcept 101 dataset. Our code is available at {this https URL}",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 67,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13110",
        "abs_url": "https://arxiv.org/abs/2507.13110",
        "pdf_url": "https://arxiv.org/pdf/2507.13110",
        "title": "3DKeyAD: High-Resolution 3D Point Cloud Anomaly Detection via Keypoint-Guided Point Clustering",
        "authors": [
            "Zi Wang",
            "Katsuya Hotta",
            "Koichiro Kamide",
            "Yawen Zou",
            "Chao Zhang",
            "Jun Yu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "High-resolution 3D point clouds are highly effective for detecting subtle structural anomalies in industrial inspection. However, their dense and irregular nature imposes significant challenges, including high computational cost, sensitivity to spatial misalignment, and difficulty in capturing localized structural differences. This paper introduces a registration-based anomaly detection framework that combines multi-prototype alignment with cluster-wise discrepancy analysis to enable precise 3D anomaly localization. Specifically, each test sample is first registered to multiple normal prototypes to enable direct structural comparison. To evaluate anomalies at a local level, clustering is performed over the point cloud, and similarity is computed between features from the test sample and the prototypes within each cluster. Rather than selecting cluster centroids randomly, a keypoint-guided strategy is employed, where geometrically informative points are chosen as centroids. This ensures that clusters are centered on feature-rich regions, enabling more meaningful and stable distance-based comparisons. Extensive experiments on the Real3D-AD benchmark demonstrate that the proposed method achieves state-of-the-art performance in both object-level and point-level anomaly detection, even using only raw features.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 68,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13113",
        "abs_url": "https://arxiv.org/abs/2507.13113",
        "pdf_url": "https://arxiv.org/pdf/2507.13113",
        "title": "Leveraging Language Prior for Infrared Small Target Detection",
        "authors": [
            "Pranav Singh",
            "Pravendra Singh"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "IRSTD (InfraRed Small Target Detection) detects small targets in infrared blurry backgrounds and is essential for various applications. The detection task is challenging due to the small size of the targets and their sparse distribution in infrared small target datasets. Although existing IRSTD methods and datasets have led to significant advancements, they are limited by their reliance solely on the image modality. Recent advances in deep learning and large vision-language models have shown remarkable performance in various visual recognition tasks. In this work, we propose a novel multimodal IRSTD framework that incorporates language priors to guide small target detection. We leverage language-guided attention weights derived from the language prior to enhance the model's ability for IRSTD, presenting a novel approach that combines textual information with image data to improve IRSTD capabilities. Utilizing the state-of-the-art GPT-4 vision model, we generate text descriptions that provide the locations of small targets in infrared images, employing careful prompt engineering to ensure improved accuracy. Due to the absence of multimodal IR datasets, existing IRSTD methods rely solely on image data. To address this shortcoming, we have curated a multimodal infrared dataset that includes both image and text modalities for small target detection, expanding upon the popular IRSTD-1k and NUDT-SIRST datasets. We validate the effectiveness of our approach through extensive experiments and comprehensive ablation studies. The results demonstrate significant improvements over the state-of-the-art method, with relative percentage differences of 9.74%, 13.02%, 1.25%, and 67.87% in IoU, nIoU, Pd, and Fa on the NUAA-SIRST subset, and 4.41%, 2.04%, 2.01%, and 113.43% on the IRSTD-1k subset of the LangIR dataset, respectively.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 69,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13120",
        "abs_url": "https://arxiv.org/abs/2507.13120",
        "pdf_url": "https://arxiv.org/pdf/2507.13120",
        "title": "RS-TinyNet: Stage-wise Feature Fusion Network for Detecting Tiny Objects in Remote Sensing Images",
        "authors": [
            "Xiaozheng Jiang",
            "Wei Zhang",
            "Xuerui Mao"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Detecting tiny objects in remote sensing (RS) imagery has been a long-standing challenge due to their extremely limited spatial information, weak feature representations, and dense distributions across complex backgrounds. Despite numerous efforts devoted, mainstream detectors still underperform in such scenarios. To bridge this gap, we introduce RS-TinyNet, a multi-stage feature fusion and enhancement model explicitly tailored for RS tiny object detection in various RS scenarios. RS-TinyNet comes with two novel designs: tiny object saliency modeling and feature integrity reconstruction. Guided by these principles, we design three step-wise feature enhancement modules. Among them, the multi-dimensional collaborative attention (MDCA) module employs multi-dimensional attention to enhance the saliency of tiny objects. Additionally, the auxiliary reversible branch (ARB) and a progressive fusion detection head (PFDH) module are introduced to preserve information flow and fuse multi-level features to bridge semantic gaps and retain structural detail. Comprehensive experiments on public RS dataset AI-TOD show that our RS-TinyNet surpasses existing state-of-the-art (SOTA) detectors by 4.0% AP and 6.5% AP75. Evaluations on DIOR benchmark dataset further validate its superior detection performance in diverse RS scenarios. These results demonstrate that the proposed multi-stage feature fusion strategy offers an effective and practical solution for tiny object detection in complex RS environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 70,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13145",
        "abs_url": "https://arxiv.org/abs/2507.13145",
        "pdf_url": "https://arxiv.org/pdf/2507.13145",
        "title": "DINO-VO: A Feature-based Visual Odometry Leveraging a Visual Foundation Model",
        "authors": [
            "Maulana Bisyir Azhari",
            "David Hyunchul Shim"
        ],
        "comments": "8 pages, 6 figures. Accepted for publication in IEEE Robotics and Automation Letters (RA-L), July 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Robotics (cs.RO)",
        "abstract": "Learning-based monocular visual odometry (VO) poses robustness, generalization, and efficiency challenges in robotics. Recent advances in visual foundation models, such as DINOv2, have improved robustness and generalization in various vision tasks, yet their integration in VO remains limited due to coarse feature granularity. In this paper, we present DINO-VO, a feature-based VO system leveraging DINOv2 visual foundation model for its sparse feature matching. To address the integration challenge, we propose a salient keypoints detector tailored to DINOv2's coarse features. Furthermore, we complement DINOv2's robust-semantic features with fine-grained geometric features, resulting in more localizable representations. Finally, a transformer-based matcher and differentiable pose estimation layer enable precise camera motion estimation by learning good matches. Against prior detector-descriptor networks like SuperPoint, DINO-VO demonstrates greater robustness in challenging environments. Furthermore, we show superior accuracy and generalization of the proposed feature descriptors against standalone DINOv2 coarse features. DINO-VO outperforms prior frame-to-frame VO methods on the TartanAir and KITTI datasets and is competitive on EuRoC dataset, while running efficiently at 72 FPS with less than 1GB of memory usage on a single GPU. Moreover, it performs competitively against Visual SLAM systems on outdoor driving scenarios, showcasing its generalization capabilities.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 71,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13152",
        "abs_url": "https://arxiv.org/abs/2507.13152",
        "pdf_url": "https://arxiv.org/pdf/2507.13152",
        "title": "SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on Multimodal Large Language Models",
        "authors": [
            "Xiangyu Dong",
            "Haoran Zhao",
            "Jiang Gao",
            "Haozhou Li",
            "Xiaoguang Ma",
            "Yaoming Zhou",
            "Fuhai Chen",
            "Juan Liu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Robotics (cs.RO)",
        "abstract": "Recent advances in vision-language navigation (VLN) were mainly attributed to emerging large language models (LLMs). These methods exhibited excellent generalization capabilities in instruction understanding and task reasoning. However, they were constrained by the fixed knowledge bases and reasoning abilities of LLMs, preventing fully incorporating experiential knowledge and thus resulting in a lack of efficient evolutionary capacity. To address this, we drew inspiration from the evolution capabilities of natural agents, and proposed a self-evolving VLN framework (SE-VLN) to endow VLN agents with the ability to continuously evolve during testing. To the best of our knowledge, it was the first time that an multimodal LLM-powered self-evolving VLN framework was proposed. Specifically, SE-VLN comprised three core modules, i.e., a hierarchical memory module to transfer successful and failure cases into reusable knowledge, a retrieval-augmented thought-based reasoning module to retrieve experience and enable multi-step decision-making, and a reflection module to realize continual evolution. Comprehensive tests illustrated that the SE-VLN achieved navigation success rates of 57% and 35.2% in unseen environments, representing absolute performance improvements of 23.9% and 15.0% over current state-of-the-art methods on R2R and REVERSE datasets, respectively. Moreover, the SE-VLN showed performance improvement with increasing experience repository, elucidating its great potential as a self-evolving agent framework for VLN.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 72,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13162",
        "abs_url": "https://arxiv.org/abs/2507.13162",
        "pdf_url": "https://arxiv.org/pdf/2507.13162",
        "title": "Orbis: Overcoming Challenges of Long-Horizon Prediction in Driving World Models",
        "authors": [
            "Arian Mousakhan",
            "Sudhanshu Mittal",
            "Silvio Galesso",
            "Karim Farid",
            "Thomas Brox"
        ],
        "comments": "Project page: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Existing world models for autonomous driving struggle with long-horizon generation and generalization to challenging scenarios. In this work, we develop a model using simple design choices, and without additional supervision or sensors, such as maps, depth, or multiple cameras. We show that our model yields state-of-the-art performance, despite having only 469M parameters and being trained on 280h of video data. It particularly stands out in difficult scenarios like turning maneuvers and urban traffic. We test whether discrete token models possibly have advantages over continuous models based on flow matching. To this end, we set up a hybrid tokenizer that is compatible with both approaches and allows for a side-by-side comparison. Our study concludes in favor of the continuous autoregressive model, which is less brittle on individual design choices and more powerful than the model built on discrete tokens. Code, models and qualitative results are publicly available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 73,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13221",
        "abs_url": "https://arxiv.org/abs/2507.13221",
        "pdf_url": "https://arxiv.org/pdf/2507.13221",
        "title": "Synthesizing Reality: Leveraging the Generative AI-Powered Platform Midjourney for Construction Worker Detection",
        "authors": [
            "Hongyang Zhao",
            "Tianyu Liang",
            "Sina Davari",
            "Daeho Kim"
        ],
        "comments": "This work was presented at ASCE International Conference on Computing in Civil Engineering (i3CE) 2024 and is currently under consideration for publication in ASCE proceedings",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "While recent advancements in deep neural networks (DNNs) have substantially enhanced visual AI's capabilities, the challenge of inadequate data diversity and volume remains, particularly in construction domain. This study presents a novel image synthesis methodology tailored for construction worker detection, leveraging the generative-AI platform Midjourney. The approach entails generating a collection of 12,000 synthetic images by formulating 3000 different prompts, with an emphasis on image realism and diversity. These images, after manual labeling, serve as a dataset for DNN training. Evaluation on a real construction image dataset yielded promising results, with the model attaining average precisions (APs) of 0.937 and 0.642 at intersection-over-union (IoU) thresholds of 0.5 and 0.5 to 0.95, respectively. Notably, the model demonstrated near-perfect performance on the synthetic dataset, achieving APs of 0.994 and 0.919 at the two mentioned thresholds. These findings reveal both the potential and weakness of generative AI in addressing DNN training data scarcity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 74,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13224",
        "abs_url": "https://arxiv.org/abs/2507.13224",
        "pdf_url": "https://arxiv.org/pdf/2507.13224",
        "title": "Leveraging Pre-Trained Visual Models for AI-Generated Video Detection",
        "authors": [
            "Keerthi Veeramachaneni",
            "Praveen Tirupattur",
            "Amrit Singh Bedi",
            "Mubarak Shah"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Recent advances in Generative AI (GenAI) have led to significant improvements in the quality of generated visual content. As AI-generated visual content becomes increasingly indistinguishable from real content, the challenge of detecting the generated content becomes critical in combating misinformation, ensuring privacy, and preventing security threats. Although there has been substantial progress in detecting AI-generated images, current methods for video detection are largely focused on deepfakes, which primarily involve human faces. However, the field of video generation has advanced beyond DeepFakes, creating an urgent need for methods capable of detecting AI-generated videos with generic content. To address this gap, we propose a novel approach that leverages pre-trained visual models to distinguish between real and generated videos. The features extracted from these pre-trained models, which have been trained on extensive real visual content, contain inherent signals that can help distinguish real from generated videos. Using these extracted features, we achieve high detection performance without requiring additional model training, and we further improve performance by training a simple linear classification layer on top of the extracted features. We validated our method on a dataset we compiled (VID-AID), which includes around 10,000 AI-generated videos produced by 9 different text-to-video models, along with 4,000 real videos, totaling over 7 hours of video content. Our evaluation shows that our approach achieves high detection accuracy, above 90% on average, underscoring its effectiveness. Upon acceptance, we plan to publicly release the code, the pre-trained models, and our dataset to support ongoing research in this critical area.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 75,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13229",
        "abs_url": "https://arxiv.org/abs/2507.13229",
        "pdf_url": "https://arxiv.org/pdf/2507.13229",
        "title": "$S^2M^2$: Scalable Stereo Matching Model for Reliable Depth Estimation",
        "authors": [
            "Junhong Min",
            "Youngpil Jeon",
            "Jimin Kim",
            "Minyong Choi"
        ],
        "comments": "8 pages, 5 figures, ICCV accepted paper",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Robotics (cs.RO)",
        "abstract": "The pursuit of a generalizable stereo matching model, capable of performing across varying resolutions and disparity ranges without dataset-specific fine-tuning, has revealed a fundamental trade-off. Iterative local search methods achieve high scores on constrained benchmarks, but their core mechanism inherently limits the global consistency required for true generalization. On the other hand, global matching architectures, while theoretically more robust, have been historically rendered infeasible by prohibitive computational and memory costs. We resolve this dilemma with $S^2M^2$: a global matching architecture that achieves both state-of-the-art accuracy and high efficiency without relying on cost volume filtering or deep refinement stacks. Our design integrates a multi-resolution transformer for robust long-range correspondence, trained with a novel loss function that concentrates probability on feasible matches. This approach enables a more robust joint estimation of disparity, occlusion, and confidence. $S^2M^2$ establishes a new state of the art on the Middlebury v3 and ETH3D benchmarks, significantly outperforming prior methods across most metrics while reconstructing high-quality details with competitive efficiency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 76,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13231",
        "abs_url": "https://arxiv.org/abs/2507.13231",
        "pdf_url": "https://arxiv.org/pdf/2507.13231",
        "title": "VITA: Vision-to-Action Flow Matching Policy",
        "authors": [
            "Dechen Gao",
            "Boqi Zhao",
            "Andrew Lee",
            "Ian Chuang",
            "Hanchu Zhou",
            "Hang Wang",
            "Zhe Zhao",
            "Junshan Zhang",
            "Iman Soltani"
        ],
        "comments": "Project page: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Robotics (cs.RO)",
        "abstract": "We present VITA, a Vision-To-Action flow matching policy that evolves latent visual representations into latent actions for visuomotor control. Traditional flow matching and diffusion policies sample from standard source distributions (e.g., Gaussian noise) and require additional conditioning mechanisms like cross-attention to condition action generation on visual information, creating time and space overheads. VITA proposes a novel paradigm that treats latent images as the flow source, learning an inherent mapping from vision to action while eliminating separate conditioning modules and preserving generative modeling capabilities. Learning flows between fundamentally different modalities like vision and action is challenging due to sparse action data lacking semantic structures and dimensional mismatches between high-dimensional visual representations and raw actions. We address this by creating a structured action latent space via an autoencoder as the flow matching target, up-sampling raw actions to match visual representation shapes. Crucially, we supervise flow matching with both encoder targets and final action outputs through flow latent decoding, which backpropagates action reconstruction loss through sequential flow matching ODE solving steps for effective end-to-end learning. Implemented as simple MLP layers, VITA is evaluated on challenging bi-manual manipulation tasks on the ALOHA platform, including 5 simulation and 2 real-world tasks. Despite its simplicity, MLP-only VITA outperforms or matches state-of-the-art generative policies while reducing inference latency by 50-130% compared to conventional flow matching policies requiring different conditioning mechanisms or complex architectures. To our knowledge, VITA is the first MLP-only flow matching policy capable of solving complex bi-manual manipulation tasks like those in ALOHA benchmarks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 77,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13260",
        "abs_url": "https://arxiv.org/abs/2507.13260",
        "pdf_url": "https://arxiv.org/pdf/2507.13260",
        "title": "Efficient Adaptation of Pre-trained Vision Transformer underpinned by Approximately Orthogonal Fine-Tuning Strategy",
        "authors": [
            "Yiting Yang",
            "Hao Luo",
            "Yuan Sun",
            "Qingsen Yan",
            "Haokui Zhang",
            "Wei Dong",
            "Guoqing Wang",
            "Peng Wang",
            "Yang Yang",
            "Hengtao Shen"
        ],
        "comments": "This paper is accepted by ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "A prevalent approach in Parameter-Efficient Fine-Tuning (PEFT) of pre-trained Vision Transformers (ViT) involves freezing the majority of the backbone parameters and solely learning low-rank adaptation weight matrices to accommodate downstream tasks. These low-rank matrices are commonly derived through the multiplication structure of down-projection and up-projection matrices, exemplified by methods such as LoRA and Adapter. In this work, we observe an approximate orthogonality among any two row or column vectors within any weight matrix of the backbone parameters; however, this property is absent in the vectors of the down/up-projection matrices. Approximate orthogonality implies a reduction in the upper bound of the model's generalization error, signifying that the model possesses enhanced generalization capability. If the fine-tuned down/up-projection matrices were to exhibit this same property as the pre-trained backbone matrices, could the generalization capability of fine-tuned ViTs be further augmented? To address this question, we propose an Approximately Orthogonal Fine-Tuning (AOFT) strategy for representing the low-rank weight matrices. This strategy employs a single learnable vector to generate a set of approximately orthogonal vectors, which form the down/up-projection matrices, thereby aligning the properties of these matrices with those of the backbone. Extensive experimental results demonstrate that our method achieves competitive performance across a range of downstream image classification tasks, confirming the efficacy of the enhanced generalization capability embedded in the down/up-projection matrices.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 78,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13292",
        "abs_url": "https://arxiv.org/abs/2507.13292",
        "pdf_url": "https://arxiv.org/pdf/2507.13292",
        "title": "DiffClean: Diffusion-based Makeup Removal for Accurate Age Estimation",
        "authors": [
            "Ekta Balkrishna Gavas",
            "Chinmay Hegde",
            "Nasir Memon",
            "Sudipta Banerjee"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Accurate age verification can protect underage users from unauthorized access to online platforms and e-commerce sites that provide age-restricted services. However, accurate age estimation can be confounded by several factors, including facial makeup that can induce changes to alter perceived identity and age to fool both humans and machines. In this work, we propose DiffClean which erases makeup traces using a text-guided diffusion model to defend against makeup attacks. DiffClean improves age estimation (minor vs. adult accuracy by 4.8%) and face verification (TMR by 8.9% at FMR=0.01%) over competing baselines on digitally simulated and real makeup images.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 79,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13311",
        "abs_url": "https://arxiv.org/abs/2507.13311",
        "pdf_url": "https://arxiv.org/pdf/2507.13311",
        "title": "FashionPose: Text to Pose to Relight Image Generation for Personalized Fashion Visualization",
        "authors": [
            "Chuancheng Shi",
            "Yixiang Chen",
            "Burong Lei",
            "Jichao Chen"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Realistic and controllable garment visualization is critical for fashion e-commerce, where users expect personalized previews under diverse poses and lighting conditions. Existing methods often rely on predefined poses, limiting semantic flexibility and illumination adaptability. To address this, we introduce FashionPose, the first unified text-to-pose-to-relighting generation framework. Given a natural language description, our method first predicts a 2D human pose, then employs a diffusion model to generate high-fidelity person images, and finally applies a lightweight relighting module, all guided by the same textual input. By replacing explicit pose annotations with text-driven conditioning, FashionPose enables accurate pose alignment, faithful garment rendering, and flexible lighting control. Experiments demonstrate fine-grained pose synthesis and efficient, consistent relighting, providing a practical solution for personalized virtual fashion display.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 80,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13314",
        "abs_url": "https://arxiv.org/abs/2507.13314",
        "pdf_url": "https://arxiv.org/pdf/2507.13314",
        "title": "Revisiting Reliability in the Reasoning-based Pose Estimation Benchmark",
        "authors": [
            "Junsu Kim",
            "Naeun Kim",
            "Jaeho Lee",
            "Incheol Park",
            "Dongyoon Han",
            "Seungryul Baek"
        ],
        "comments": "To be presented as a poster at MMFM 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "The reasoning-based pose estimation (RPE) benchmark has emerged as a widely adopted evaluation standard for pose-aware multimodal large language models (MLLMs). Despite its significance, we identified critical reproducibility and benchmark-quality issues that hinder fair and consistent quantitative evaluations. Most notably, the benchmark utilizes different image indices from those of the original 3DPW dataset, forcing researchers into tedious and error-prone manual matching processes to obtain accurate ground-truth (GT) annotations for quantitative metrics (\\eg, MPJPE, PA-MPJPE). Furthermore, our analysis reveals several inherent benchmark-quality limitations, including significant image redundancy, scenario imbalance, overly simplistic poses, and ambiguous textual descriptions, collectively undermining reliable evaluations across diverse scenarios. To alleviate manual effort and enhance reproducibility, we carefully refined the GT annotations through meticulous visual matching and publicly release these refined annotations as an open-source resource, thereby promoting consistent quantitative evaluations and facilitating future advancements in human pose-aware multimodal reasoning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 81,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13326",
        "abs_url": "https://arxiv.org/abs/2507.13326",
        "pdf_url": "https://arxiv.org/pdf/2507.13326",
        "title": "A Real-Time System for Egocentric Hand-Object Interaction Detection in Industrial Domains",
        "authors": [
            "Antonio Finocchiaro",
            "Alessandro Sebastiano Catinello",
            "Michele Mazzamuto",
            "Rosario Leonardi",
            "Antonino Furnari",
            "Giovanni Maria Farinella"
        ],
        "comments": "12 pages, 4 figures, In International Conference on Image Analysis and Processing",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Hand-object interaction detection remains an open challenge in real-time applications, where intuitive user experiences depend on fast and accurate detection of interactions with surrounding objects. We propose an efficient approach for detecting hand-objects interactions from streaming egocentric vision that operates in real time. Our approach consists of an action recognition module and an object detection module for identifying active objects upon confirmed interaction. Our Mamba model with EfficientNetV2 as backbone for action recognition achieves 38.52% p-AP on the ENIGMA-51 benchmark at 30fps, while our fine-tuned YOLOWorld reaches 85.13% AP for hand and object. We implement our models in a cascaded architecture where the action recognition and object detection modules operate sequentially. When the action recognition predicts a contact state, it activates the object detection module, which in turn performs inference on the relevant frame to detect and classify the active object.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 82,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13343",
        "abs_url": "https://arxiv.org/abs/2507.13343",
        "pdf_url": "https://arxiv.org/pdf/2507.13343",
        "title": "Taming Diffusion Transformer for Real-Time Mobile Video Generation",
        "authors": [
            "Yushu Wu",
            "Yanyu Li",
            "Anil Kag",
            "Ivan Skorokhodov",
            "Willi Menapace",
            "Ke Ma",
            "Arpit Sahni",
            "Ju Hu",
            "Aliaksandr Siarohin",
            "Dhritiman Sagar",
            "Yanzhi Wang",
            "Sergey Tulyakov"
        ],
        "comments": "9 pages, 4 figures, 5 tables",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)",
        "abstract": "Diffusion Transformers (DiT) have shown strong performance in video generation tasks, but their high computational cost makes them impractical for resource-constrained devices like smartphones, and real-time generation is even more challenging. In this work, we propose a series of novel optimizations to significantly accelerate video generation and enable real-time performance on mobile platforms. First, we employ a highly compressed variational autoencoder (VAE) to reduce the dimensionality of the input data without sacrificing visual quality. Second, we introduce a KD-guided, sensitivity-aware tri-level pruning strategy to shrink the model size to suit mobile platform while preserving critical performance characteristics. Third, we develop an adversarial step distillation technique tailored for DiT, which allows us to reduce the number of inference steps to four. Combined, these optimizations enable our model to achieve over 10 frames per second (FPS) generation on an iPhone 16 Pro Max, demonstrating the feasibility of real-time, high-quality video generation on mobile devices.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 83,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13344",
        "abs_url": "https://arxiv.org/abs/2507.13344",
        "pdf_url": "https://arxiv.org/pdf/2507.13344",
        "title": "Diffuman4D: 4D Consistent Human View Synthesis from Sparse-View Videos with Spatio-Temporal Diffusion Models",
        "authors": [
            "Yudong Jin",
            "Sida Peng",
            "Xuan Wang",
            "Tao Xie",
            "Zhen Xu",
            "Yifan Yang",
            "Yujun Shen",
            "Hujun Bao",
            "Xiaowei Zhou"
        ],
        "comments": "Project page: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "This paper addresses the challenge of high-fidelity view synthesis of humans with sparse-view videos as input. Previous methods solve the issue of insufficient observation by leveraging 4D diffusion models to generate videos at novel viewpoints. However, the generated videos from these models often lack spatio-temporal consistency, thus degrading view synthesis quality. In this paper, we propose a novel sliding iterative denoising process to enhance the spatio-temporal consistency of the 4D diffusion model. Specifically, we define a latent grid in which each latent encodes the image, camera pose, and human pose for a certain viewpoint and timestamp, then alternately denoising the latent grid along spatial and temporal dimensions with a sliding window, and finally decode the videos at target viewpoints from the corresponding denoised latents. Through the iterative sliding, information flows sufficiently across the latent grid, allowing the diffusion model to obtain a large receptive field and thus enhance the 4D consistency of the output, while making the GPU memory consumption affordable. The experiments on the DNA-Rendering and ActorsHQ datasets demonstrate that our method is able to synthesize high-quality and consistent novel-view videos and significantly outperforms the existing approaches. See our project page for interactive demos and video results: this https URL .",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 84,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13345",
        "abs_url": "https://arxiv.org/abs/2507.13345",
        "pdf_url": "https://arxiv.org/pdf/2507.13345",
        "title": "Imbalance in Balance: Online Concept Balancing in Generation Models",
        "authors": [
            "Yukai Shi",
            "Jiarong Ou",
            "Rui Chen",
            "Haotian Yang",
            "Jiahao Wang",
            "Xin Tao",
            "Pengfei Wan",
            "Di Zhang",
            "Kun Gai"
        ],
        "comments": "Accepted by ICCV2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "In visual generation tasks, the responses and combinations of complex concepts often lack stability and are error-prone, which remains an under-explored area. In this paper, we attempt to explore the causal factors for poor concept responses through elaborately designed experiments. We also design a concept-wise equalization loss function (IMBA loss) to address this issue. Our proposed method is online, eliminating the need for offline dataset processing, and requires minimal code changes. In our newly proposed complex concept benchmark Inert-CompBench and two other public test sets, our method significantly enhances the concept response capability of baseline models and yields highly competitive results with only a few codes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 85,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13346",
        "abs_url": "https://arxiv.org/abs/2507.13346",
        "pdf_url": "https://arxiv.org/pdf/2507.13346",
        "title": "AutoPartGen: Autogressive 3D Part Generation and Discovery",
        "authors": [
            "Minghao Chen",
            "Jianyuan Wang",
            "Roman Shapovalov",
            "Tom Monnier",
            "Hyunyoung Jung",
            "Dilin Wang",
            "Rakesh Ranjan",
            "Iro Laina",
            "Andrea Vedaldi"
        ],
        "comments": "Project page: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We introduce AutoPartGen, a model that generates objects composed of 3D parts in an autoregressive manner. This model can take as input an image of an object, 2D masks of the object's parts, or an existing 3D object, and generate a corresponding compositional 3D reconstruction. Our approach builds upon 3DShape2VecSet, a recent latent 3D representation with powerful geometric expressiveness. We observe that this latent space exhibits strong compositional properties, making it particularly well-suited for part-based generation tasks. Specifically, AutoPartGen generates object parts autoregressively, predicting one part at a time while conditioning on previously generated parts and additional inputs, such as 2D images, masks, or 3D objects. This process continues until the model decides that all parts have been generated, thus determining automatically the type and number of parts. The resulting parts can be seamlessly assembled into coherent objects or scenes without requiring additional optimization. We evaluate both the overall 3D generation capabilities and the part-level generation quality of AutoPartGen, demonstrating that it achieves state-of-the-art performance in 3D part generation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 86,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13347",
        "abs_url": "https://arxiv.org/abs/2507.13347",
        "pdf_url": "https://arxiv.org/pdf/2507.13347",
        "title": "$π^3$: Scalable Permutation-Equivariant Visual Geometry Learning",
        "authors": [
            "Yifan Wang",
            "Jianjun Zhou",
            "Haoyi Zhu",
            "Wenzheng Chang",
            "Yang Zhou",
            "Zizun Li",
            "Junyi Chen",
            "Jiangmiao Pang",
            "Chunhua Shen",
            "Tong He"
        ],
        "comments": "Project page: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We introduce $\\pi^3$, a feed-forward neural network that offers a novel approach to visual geometry reconstruction, breaking the reliance on a conventional fixed reference view. Previous methods often anchor their reconstructions to a designated viewpoint, an inductive bias that can lead to instability and failures if the reference is suboptimal. In contrast, $\\pi^3$ employs a fully permutation-equivariant architecture to predict affine-invariant camera poses and scale-invariant local point maps without any reference frames. This design makes our model inherently robust to input ordering and highly scalable. These advantages enable our simple and bias-free approach to achieve state-of-the-art performance on a wide range of tasks, including camera pose estimation, monocular/video depth estimation, and dense point map reconstruction. Code and models are publicly available.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 87,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13348",
        "abs_url": "https://arxiv.org/abs/2507.13348",
        "pdf_url": "https://arxiv.org/pdf/2507.13348",
        "title": "VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning",
        "authors": [
            "Senqiao Yang",
            "Junyi Li",
            "Xin Lai",
            "Bei Yu",
            "Hengshuang Zhao",
            "Jiaya Jia"
        ],
        "comments": "Code and models are available at this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Recent advancements in vision-language models (VLMs) have improved performance by increasing the number of visual tokens, which are often significantly longer than text tokens. However, we observe that most real-world scenarios do not require such an extensive number of visual tokens. While the performance drops significantly in a small subset of OCR-related tasks, models still perform accurately in most other general VQA tasks with only 1/4 resolution. Therefore, we propose to dynamically process distinct samples with different resolutions, and present a new paradigm for visual token compression, namely, VisionThink. It starts with a downsampled image and smartly decides whether it is sufficient for problem solving. Otherwise, the model could output a special token to request the higher-resolution image. Compared to existing Efficient VLM methods that compress tokens using fixed pruning ratios or thresholds, VisionThink autonomously decides whether to compress tokens case by case. As a result, it demonstrates strong fine-grained visual understanding capability on OCR-related tasks, and meanwhile saves substantial visual tokens on simpler tasks. We adopt reinforcement learning and propose the LLM-as-Judge strategy to successfully apply RL to general VQA tasks. Moreover, we carefully design a reward function and penalty mechanism to achieve a stable and reasonable image resize call ratio. Extensive experiments demonstrate the superiority, efficiency, and effectiveness of our method. Our code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 88,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13350",
        "abs_url": "https://arxiv.org/abs/2507.13350",
        "pdf_url": "https://arxiv.org/pdf/2507.13350",
        "title": "Hierarchical Rectified Flow Matching with Mini-Batch Couplings",
        "authors": [
            "Yichi Zhang",
            "Yici Yan",
            "Alex Schwing",
            "Zhizhen Zhao"
        ],
        "comments": "Project Page: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Flow matching has emerged as a compelling generative modeling approach that is widely used across domains. To generate data via a flow matching model, an ordinary differential equation (ODE) is numerically solved via forward integration of the modeled velocity field. To better capture the multi-modality that is inherent in typical velocity fields, hierarchical flow matching was recently introduced. It uses a hierarchy of ODEs that are numerically integrated when generating data. This hierarchy of ODEs captures the multi-modal velocity distribution just like vanilla flow matching is capable of modeling a multi-modal data distribution. While this hierarchy enables to model multi-modal velocity distributions, the complexity of the modeled distribution remains identical across levels of the hierarchy. In this paper, we study how to gradually adjust the complexity of the distributions across different levels of the hierarchy via mini-batch couplings. We show the benefits of mini-batch couplings in hierarchical rectified flow matching via compelling results on synthetic and imaging data. Code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 89,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13353",
        "abs_url": "https://arxiv.org/abs/2507.13353",
        "pdf_url": "https://arxiv.org/pdf/2507.13353",
        "title": "VideoITG: Multimodal Video Understanding with Instructed Temporal Grounding",
        "authors": [
            "Shihao Wang",
            "Guo Chen",
            "De-an Huang",
            "Zhiqi Li",
            "Minghan Li",
            "Guilin Li",
            "Jose M. Alvarez",
            "Lei Zhang",
            "Zhiding Yu"
        ],
        "comments": "Technical Report",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Recent studies have revealed that selecting informative and relevant video frames can significantly improve the performance of Video Large Language Models (Video-LLMs). Current methods, such as reducing inter-frame redundancy, employing separate models for image-text relevance assessment, or utilizing temporal video grounding for event localization, substantially adopt unsupervised learning paradigms, whereas they struggle to address the complex scenarios in long video understanding. We propose Instructed Temporal Grounding for Videos (VideoITG), featuring customized frame sampling aligned with user instructions. The core of VideoITG is the VidThinker pipeline, an automated annotation framework that explicitly mimics the human annotation process. First, it generates detailed clip-level captions conditioned on the instruction; then, it retrieves relevant video segments through instruction-guided reasoning; finally, it performs fine-grained frame selection to pinpoint the most informative visual evidence. Leveraging VidThinker, we construct the VideoITG-40K dataset, containing 40K videos and 500K instructed temporal grounding annotations. We then design a plug-and-play VideoITG model, which takes advantage of visual language alignment and reasoning capabilities of Video-LLMs, for effective frame selection in a discriminative manner. Coupled with Video-LLMs, VideoITG achieves consistent performance improvements across multiple multimodal video understanding benchmarks, showing its superiority and great potentials for video understanding.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 90,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12489",
        "abs_url": "https://arxiv.org/abs/2507.12489",
        "pdf_url": "https://arxiv.org/pdf/2507.12489",
        "title": "Physically Based Neural LiDAR Resimulation",
        "authors": [
            "Richard Marcus",
            "Marc Stamminger"
        ],
        "comments": "Accepted at ITSC 2025, Gold Coast Australia",
        "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR); Image and Video Processing (eess.IV)",
        "abstract": "Methods for Novel View Synthesis (NVS) have recently found traction in the field of LiDAR simulation and large-scale 3D scene reconstruction. While solutions for faster rendering or handling dynamic scenes have been proposed, LiDAR specific effects remain insufficiently addressed. By explicitly modeling sensor characteristics such as rolling shutter, laser power variations, and intensity falloff, our method achieves more accurate LiDAR simulation compared to existing techniques. We demonstrate the effectiveness of our approach through quantitative and qualitative comparisons with state-of-the-art methods, as well as ablation studies that highlight the importance of each sensor model component. Beyond that, we show that our approach exhibits advanced resimulation capabilities, such as generating high resolution LiDAR scans in the camera perspective. Our code and the resulting dataset are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 91,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12600",
        "abs_url": "https://arxiv.org/abs/2507.12600",
        "pdf_url": "https://arxiv.org/pdf/2507.12600",
        "title": "HairFormer: Transformer-Based Dynamic Neural Hair Simulation",
        "authors": [
            "Joy Xiaoji Zhang",
            "Jingsen Zhu",
            "Hanyu Chen",
            "Steve Marschner"
        ],
        "comments": "",
        "subjects": "Graphics (cs.GR); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Simulating hair dynamics that generalize across arbitrary hairstyles, body shapes, and motions is a critical challenge. Our novel two-stage neural solution is the first to leverage Transformer-based architectures for such a broad generalization. We propose a Transformer-powered static network that predicts static draped shapes for any hairstyle, effectively resolving hair-body penetrations and preserving hair fidelity. Subsequently, a dynamic network with a novel cross-attention mechanism fuses static hair features with kinematic input to generate expressive dynamics and complex secondary motions. This dynamic network also allows for efficient fine-tuning of challenging motion sequences, such as abrupt head movements. Our method offers real-time inference for both static single-frame drapes and dynamic drapes over pose sequences. Our method demonstrates high-fidelity and generalizable dynamic hair across various styles, guided by physics-informed losses, and can resolve penetrations even for complex, unseen long hairstyles, highlighting its broad generalization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 92,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12624",
        "abs_url": "https://arxiv.org/abs/2507.12624",
        "pdf_url": "https://arxiv.org/pdf/2507.12624",
        "title": "Pathology-Guided Virtual Staining Metric for Evaluation and Training",
        "authors": [
            "Qiankai Wang",
            "James E.D. Tweel",
            "Parsin Haji Reza",
            "Anita Layton"
        ],
        "comments": "19 pages, 10 figures. Intended for submission to the Journal of Imaging Informatics in Medicine (JIIM)",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV); Systems and Control (eess.SY)",
        "abstract": "Virtual staining has emerged as a powerful alternative to traditional histopathological staining techniques, enabling rapid, reagent-free image transformations. However, existing evaluation methods predominantly rely on full-reference image quality assessment (FR-IQA) metrics such as structural similarity, which are originally designed for natural images and often fail to capture pathology-relevant features. Expert pathology reviews have also been used, but they are inherently subjective and time-consuming. In this study, we introduce PaPIS (Pathology-Aware Perceptual Image Similarity), a novel FR-IQA metric specifically tailored for virtual staining evaluation. PaPIS leverages deep learning-based features trained on cell morphology segmentation and incorporates Retinex-inspired feature decomposition to better reflect histological perceptual quality. Comparative experiments demonstrate that PaPIS more accurately aligns with pathology-relevant visual cues and distinguishes subtle cellular structures that traditional and existing perceptual metrics tend to overlook. Furthermore, integrating PaPIS as a guiding loss function in a virtual staining model leads to improved histological fidelity. This work highlights the critical need for pathology-aware evaluation frameworks to advance the development and clinical readiness of virtual staining technologies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 93,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12669",
        "abs_url": "https://arxiv.org/abs/2507.12669",
        "pdf_url": "https://arxiv.org/pdf/2507.12669",
        "title": "InSight: AI Mobile Screening Tool for Multiple Eye Disease Detection using Multimodal Fusion",
        "authors": [
            "Ananya Raghu",
            "Anisha Raghu",
            "Alice S. Tang",
            "Yannis M. Paulus",
            "Tyson N. Kim",
            "Tomiko T. Oskotsky"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Background/Objectives: Age-related macular degeneration, glaucoma, diabetic retinopathy (DR), diabetic macular edema, and pathological myopia affect hundreds of millions of people worldwide. Early screening for these diseases is essential, yet access to medical care remains limited in low- and middle-income countries as well as in resource-limited settings. We develop InSight, an AI-based app that combines patient metadata with fundus images for accurate diagnosis of five common eye diseases to improve accessibility of screenings. Methods: InSight features a three-stage pipeline: real-time image quality assessment, disease diagnosis model, and a DR grading model to assess severity. Our disease diagnosis model incorporates three key innovations: (a) Multimodal fusion technique (MetaFusion) combining clinical metadata and images; (b) Pretraining method leveraging supervised and self-supervised loss functions; and (c) Multitask model to simultaneously predict 5 diseases. We make use of BRSET (lab-captured images) and mBRSET (smartphone-captured images) datasets, both of which also contain clinical metadata for model training/evaluation. Results: Trained on a dataset of BRSET and mBRSET images, the image quality checker achieves near-100% accuracy in filtering out low-quality fundus images. The multimodal pretrained disease diagnosis model outperforms models using only images by 6% in balanced accuracy for BRSET and 4% for mBRSET. Conclusions: The InSight pipeline demonstrates robustness across varied image conditions and has high diagnostic accuracy across all five diseases, generalizing to both smartphone and lab captured images. The multitask model contributes to the lightweight nature of the pipeline, making it five times computationally efficient compared to having five individual models corresponding to each disease.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 94,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12687",
        "abs_url": "https://arxiv.org/abs/2507.12687",
        "pdf_url": "https://arxiv.org/pdf/2507.12687",
        "title": "TRIQA: Image Quality Assessment by Contrastive Pretraining on Ordered Distortion Triplets",
        "authors": [
            "Rajesh Sureddi",
            "Saman Zadtootaghaj",
            "Nabajeet Barman",
            "Alan C. Bovik"
        ],
        "comments": "5 pages",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Image Quality Assessment (IQA) models aim to predict perceptual image quality in alignment with human judgments. No-Reference (NR) IQA remains particularly challenging due to the absence of a reference image. While deep learning has significantly advanced this field, a major hurdle in developing NR-IQA models is the limited availability of subjectively labeled data. Most existing deep learning-based NR-IQA approaches rely on pre-training on large-scale datasets before fine-tuning for IQA tasks. To further advance progress in this area, we propose a novel approach that constructs a custom dataset using a limited number of reference content images and introduces a no-reference IQA model that incorporates both content and quality features for perceptual quality prediction. Specifically, we train a quality-aware model using contrastive triplet-based learning, enabling efficient training with fewer samples while achieving strong generalization performance across publicly available datasets. Our repository is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 95,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12698",
        "abs_url": "https://arxiv.org/abs/2507.12698",
        "pdf_url": "https://arxiv.org/pdf/2507.12698",
        "title": "Pixel Perfect MegaMed: A Megapixel-Scale Vision-Language Foundation Model for Generating High Resolution Medical Images",
        "authors": [
            "Zahra TehraniNasab",
            "Amar Kumar",
            "Tal Arbel"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Medical image synthesis presents unique challenges due to the inherent complexity and high-resolution details required in clinical contexts. Traditional generative architectures such as Generative Adversarial Networks (GANs) or Variational Auto Encoder (VAEs) have shown great promise for high-resolution image generation but struggle with preserving fine-grained details that are key for accurate diagnosis. To address this issue, we introduce Pixel Perfect MegaMed, the first vision-language foundation model to synthesize images at resolutions of 1024x1024. Our method deploys a multi-scale transformer architecture designed specifically for ultra-high resolution medical image generation, enabling the preservation of both global anatomical context and local image-level details. By leveraging vision-language alignment techniques tailored to medical terminology and imaging modalities, Pixel Perfect MegaMed bridges the gap between textual descriptions and visual representations at unprecedented resolution levels. We apply our model to the CheXpert dataset and demonstrate its ability to generate clinically faithful chest X-rays from text prompts. Beyond visual quality, these high-resolution synthetic images prove valuable for downstream tasks such as classification, showing measurable performance gains when used for data augmentation, particularly in low-data regimes. Our code is accessible through the project website - this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 96,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12729",
        "abs_url": "https://arxiv.org/abs/2507.12729",
        "pdf_url": "https://arxiv.org/pdf/2507.12729",
        "title": "Tensor-Tensor Products, Group Representations, and Semidefinite Programming",
        "authors": [
            "Alex Dunbar",
            "Elizabeth Newman"
        ],
        "comments": "34 Pages, 7 figures",
        "subjects": "Optimization and Control (math.OC); Computer Vision and Pattern Recognition (cs.CV); Numerical Analysis (math.NA); Representation Theory (math.RT)",
        "abstract": "The $\\star_M$-family of tensor-tensor products is a framework which generalizes many properties from linear algebra to third order tensors. Here, we investigate positive semidefiniteness and semidefinite programming under the $\\star_M$-product. Critical to our investigation is a connection between the choice of matrix M in the $\\star_M$-product and the representation theory of an underlying group action. Using this framework, third order tensors equipped with the $\\star_M$-product are a natural setting for the study of invariant semidefinite programs. As applications of the M-SDP framework, we provide a characterization of certain nonnegative quadratic forms and solve low-rank tensor completion problems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 97,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12750",
        "abs_url": "https://arxiv.org/abs/2507.12750",
        "pdf_url": "https://arxiv.org/pdf/2507.12750",
        "title": "Multimodal-Guided Dynamic Dataset Pruning for Robust and Efficient Data-Centric Learning",
        "authors": [
            "Suorong Yang",
            "Peijia Li",
            "Yujie Liu",
            "Zhiming Xu",
            "Peng Ye",
            "Wanli Ouyang",
            "Furao Shen",
            "Dongzhan Zhou"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Modern deep models are trained on large real-world datasets, where data quality varies and redundancy is common. Data-centric approaches such as dataset pruning have shown promise in improving training efficiency and model performance. However, most existing methods rely on static heuristics or task-specific metrics, limiting their robustness and generalizability across domains. In this work, we introduce a dynamic dataset pruning framework that adaptively selects training samples based on both task-driven difficulty and cross-modality semantic consistency. By incorporating supervision from pretrained multimodal foundation models, our approach captures training dynamics while effectively filtering out uninformative samples. Our work highlights the potential of integrating cross-modality alignment for robust sample selection, advancing data-centric learning toward more efficient and robust practices across application domains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 98,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12938",
        "abs_url": "https://arxiv.org/abs/2507.12938",
        "pdf_url": "https://arxiv.org/pdf/2507.12938",
        "title": "Unleashing Vision Foundation Models for Coronary Artery Segmentation: Parallel ViT-CNN Encoding and Variational Fusion",
        "authors": [
            "Caixia Dong",
            "Duwei Dai",
            "Xinyi Han",
            "Fan Liu",
            "Xu Yang",
            "Zongfang Li",
            "Songhua Xu"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Accurate coronary artery segmentation is critical for computeraided diagnosis of coronary artery disease (CAD), yet it remains challenging due to the small size, complex morphology, and low contrast with surrounding tissues. To address these challenges, we propose a novel segmentation framework that leverages the power of vision foundation models (VFMs) through a parallel encoding architecture. Specifically, a vision transformer (ViT) encoder within the VFM captures global structural features, enhanced by the activation of the final two ViT blocks and the integration of an attention-guided enhancement (AGE) module, while a convolutional neural network (CNN) encoder extracts local details. These complementary features are adaptively fused using a cross-branch variational fusion (CVF) module, which models latent distributions and applies variational attention to assign modality-specific weights. Additionally, we introduce an evidential-learning uncertainty refinement (EUR) module, which quantifies uncertainty using evidence theory and refines uncertain regions by incorporating multi-scale feature aggregation and attention mechanisms, further enhancing segmentation accuracy. Extensive evaluations on one in-house and two public datasets demonstrate that the proposed framework significantly outperforms state-of-the-art methods, achieving superior performance in accurate coronary artery segmentation and showcasing strong generalization across multiple datasets. The code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 99,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12961",
        "abs_url": "https://arxiv.org/abs/2507.12961",
        "pdf_url": "https://arxiv.org/pdf/2507.12961",
        "title": "Improving Diagnostic Accuracy of Pigmented Skin Lesions With CNNs: an Application on the DermaMNIST Dataset",
        "authors": [
            "Nerma Kadric",
            "Amila Akagic",
            "Medina Kapo"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Pigmented skin lesions represent localized areas of increased melanin and can indicate serious conditions like melanoma, a major contributor to skin cancer mortality. The MedMNIST v2 dataset, inspired by MNIST, was recently introduced to advance research in biomedical imaging and includes DermaMNIST, a dataset for classifying pigmented lesions based on the HAM10000 dataset. This study assesses ResNet-50 and EfficientNetV2L models for multi-class classification using DermaMNIST, employing transfer learning and various layer configurations. One configuration achieves results that match or surpass existing methods. This study suggests that convolutional neural networks (CNNs) can drive progress in biomedical image analysis, significantly enhancing diagnostic accuracy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 100,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12969",
        "abs_url": "https://arxiv.org/abs/2507.12969",
        "pdf_url": "https://arxiv.org/pdf/2507.12969",
        "title": "WaveletInception Networks for Drive-by Vibration-Based Infrastructure Health Monitoring",
        "authors": [
            "Reza Riahi Samani",
            "Alfredo Nunez",
            "Bart De Schutter"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "This paper presents a novel deep learning-based framework for infrastructure health monitoring using drive-by vibration response signals. Recognizing the importance of spectral and temporal information, we introduce the WaveletInception-BiLSTM network. The WaveletInception feature extractor utilizes a Learnable Wavelet Packet Transform (LWPT) as the stem for extracting vibration signal features, incorporating spectral information in the early network layers. This is followed by 1D Inception networks that extract multi-scale, high-level features at deeper layers. The extracted vibration signal features are then integrated with operational conditions via a Long Short-term Memory (LSTM) layer. The resulting feature extraction network effectively analyzes drive-by vibration signals across various measurement speeds without preprocessing and uses LSTM to capture interrelated temporal dependencies among different modes of information and to create feature vectors for health condition estimation. The estimator head is designed with a sequential modeling architecture using bidirectional LSTM (BiLSTM) networks, capturing bi-directional temporal relationships from drive-by measurements. This architecture allows for a high-resolution, beam-level assessment of infrastructure health conditions. A case study focusing on railway track stiffness estimation with simulated drive-by vibration signals shows that the model significantly outperforms state-of-the-art methods in estimating railway ballast and railpad stiffness parameters. Results underscore the potential of this approach for accurate, localized, and fully automated drive-by infrastructure health monitoring.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 101,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.12985",
        "abs_url": "https://arxiv.org/abs/2507.12985",
        "pdf_url": "https://arxiv.org/pdf/2507.12985",
        "title": "From Variability To Accuracy: Conditional Bernoulli Diffusion Models with Consensus-Driven Correction for Thin Structure Segmentation",
        "authors": [
            "Jinseo An",
            "Min Jin Lee",
            "Kyu Won Shim",
            "Helen Hong"
        ],
        "comments": "Early accepted at MICCAI 2025",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Accurate segmentation of orbital bones in facial computed tomography (CT) images is essential for the creation of customized implants for reconstruction of defected orbital bones, particularly challenging due to the ambiguous boundaries and thin structures such as the orbital medial wall and orbital floor. In these ambiguous regions, existing segmentation approaches often output disconnected or under-segmented results. We propose a novel framework that corrects segmentation results by leveraging consensus from multiple diffusion model outputs. Our approach employs a conditional Bernoulli diffusion model trained on diverse annotation patterns per image to generate multiple plausible segmentations, followed by a consensus-driven correction that incorporates position proximity, consensus level, and gradient direction similarity to correct challenging regions. Experimental results demonstrate that our method outperforms existing methods, significantly improving recall in ambiguous regions while preserving the continuity of thin structures. Furthermore, our method automates the manual process of segmentation result correction and can be applied to image-guided surgical planning and surgery.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 102,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13019",
        "abs_url": "https://arxiv.org/abs/2507.13019",
        "pdf_url": "https://arxiv.org/pdf/2507.13019",
        "title": "Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities",
        "authors": [
            "Liuyi Wang",
            "Xinyuan Xia",
            "Hui Zhao",
            "Hanqing Wang",
            "Tai Wang",
            "Yilun Chen",
            "Chengju Liu",
            "Qijun Chen",
            "Jiangmiao Pang"
        ],
        "comments": "Accepted by ICCV 2025",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Recent Vision-and-Language Navigation (VLN) advancements are promising, but their idealized assumptions about robot movement and control fail to reflect physically embodied deployment challenges. To bridge this gap, we introduce VLN-PE, a physically realistic VLN platform supporting humanoid, quadruped, and wheeled robots. For the first time, we systematically evaluate several ego-centric VLN methods in physical robotic settings across different technical pipelines, including classification models for single-step discrete action prediction, a diffusion model for dense waypoint prediction, and a train-free, map-based large language model (LLM) integrated with path planning. Our results reveal significant performance degradation due to limited robot observation space, environmental lighting variations, and physical challenges like collisions and falls. This also exposes locomotion constraints for legged robots in complex environments. VLN-PE is highly extensible, allowing seamless integration of new scenes beyond MP3D, thereby enabling more comprehensive VLN evaluation. Despite the weak generalization of current models in physical deployment, VLN-PE provides a new pathway for improving cross-embodiment's overall adaptability. We hope our findings and tools inspire the community to rethink VLN limitations and advance robust, practical VLN models. The code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 103,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13073",
        "abs_url": "https://arxiv.org/abs/2507.13073",
        "pdf_url": "https://arxiv.org/pdf/2507.13073",
        "title": "Dual LiDAR-Based Traffic Movement Count Estimation at a Signalized Intersection: Deployment, Data Collection, and Preliminary Analysis",
        "authors": [
            "Saswat Priyadarshi Nayak",
            "Guoyuan Wu",
            "Kanok Boriboonsomsin",
            "Matthew Barth"
        ],
        "comments": "7 Pages, 8 Figures. This paper has been accepted for publication at the 2025 IEEE ITSC. Copyright IEEE",
        "subjects": "Systems and Control (eess.SY); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Traffic Movement Count (TMC) at intersections is crucial for optimizing signal timings, assessing the performance of existing traffic control measures, and proposing efficient lane configurations to minimize delays, reduce congestion, and promote safety. Traditionally, methods such as manual counting, loop detectors, pneumatic road tubes, and camera-based recognition have been used for TMC estimation. Although generally reliable, camera-based TMC estimation is prone to inaccuracies under poor lighting conditions during harsh weather and nighttime. In contrast, Light Detection and Ranging (LiDAR) technology is gaining popularity in recent times due to reduced costs and its expanding use in 3D object detection, tracking, and related applications. This paper presents the authors' endeavor to develop, deploy and evaluate a dual-LiDAR system at an intersection in the city of Rialto, California, for TMC estimation. The 3D bounding box detections from the two LiDARs are used to classify vehicle counts based on traffic directions, vehicle movements, and vehicle classes. This work discusses the estimated TMC results and provides insights into the observed trends and irregularities. Potential improvements are also discussed that could enhance not only TMC estimation, but also trajectory forecasting and intent prediction at intersections.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 104,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13079",
        "abs_url": "https://arxiv.org/abs/2507.13079",
        "pdf_url": "https://arxiv.org/pdf/2507.13079",
        "title": "DASViT: Differentiable Architecture Search for Vision Transformer",
        "authors": [
            "Pengjin Wu",
            "Ferrante Neri",
            "Zhenhua Feng"
        ],
        "comments": "Accepted to the International Joint Conference on Neural Networks (IJCNN) 2025",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Designing effective neural networks is a cornerstone of deep learning, and Neural Architecture Search (NAS) has emerged as a powerful tool for automating this process. Among the existing NAS approaches, Differentiable Architecture Search (DARTS) has gained prominence for its efficiency and ease of use, inspiring numerous advancements. Since the rise of Vision Transformers (ViT), researchers have applied NAS to explore ViT architectures, often focusing on macro-level search spaces and relying on discrete methods like evolutionary algorithms. While these methods ensure reliability, they face challenges in discovering innovative architectural designs, demand extensive computational resources, and are time-intensive. To address these limitations, we introduce Differentiable Architecture Search for Vision Transformer (DASViT), which bridges the gap in differentiable search for ViTs and uncovers novel designs. Experiments show that DASViT delivers architectures that break traditional Transformer encoder designs, outperform ViT-B/16 on multiple datasets, and achieve superior efficiency with fewer parameters and FLOPs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 105,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13090",
        "abs_url": "https://arxiv.org/abs/2507.13090",
        "pdf_url": "https://arxiv.org/pdf/2507.13090",
        "title": "MUPAX: Multidimensional Problem Agnostic eXplainable AI",
        "authors": [
            "Vincenzo Dentamaro",
            "Felice Franchini",
            "Giuseppe Pirlo",
            "Irina Voiculescu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Robust XAI techniques should ideally be simultaneously deterministic, model agnostic, and guaranteed to converge. We propose MULTIDIMENSIONAL PROBLEM AGNOSTIC EXPLAINABLE AI (MUPAX), a deterministic, model agnostic explainability technique, with guaranteed convergency. MUPAX measure theoretic formulation gives principled feature importance attribution through structured perturbation analysis that discovers inherent input patterns and eliminates spurious relationships. We evaluate MUPAX on an extensive range of data modalities and tasks: audio classification (1D), image classification (2D), volumetric medical image analysis (3D), and anatomical landmark detection, demonstrating dimension agnostic effectiveness. The rigorous convergence guarantees extend to any loss function and arbitrary dimensions, making MUPAX applicable to virtually any problem context for AI. By contrast with other XAI methods that typically decrease performance when masking, MUPAX not only preserves but actually enhances model accuracy by capturing only the most important patterns of the original data. Extensive benchmarking against the state of the XAI art demonstrates MUPAX ability to generate precise, consistent and understandable explanations, a crucial step towards explainable and trustworthy AI systems. The source code will be released upon publication.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 106,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13146",
        "abs_url": "https://arxiv.org/abs/2507.13146",
        "pdf_url": "https://arxiv.org/pdf/2507.13146",
        "title": "fastWDM3D: Fast and Accurate 3D Healthy Tissue Inpainting",
        "authors": [
            "Alicia Durrer",
            "Florentin Bieder",
            "Paul Friedrich",
            "Bjoern Menze",
            "Philippe C. Cattin",
            "Florian Kofler"
        ],
        "comments": "Philippe C. Cattin and Florian Kofler: equal contribution",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Healthy tissue inpainting has significant applications, including the generation of pseudo-healthy baselines for tumor growth models and the facilitation of image registration. In previous editions of the BraTS Local Synthesis of Healthy Brain Tissue via Inpainting Challenge, denoising diffusion probabilistic models (DDPMs) demonstrated qualitatively convincing results but suffered from low sampling speed. To mitigate this limitation, we adapted a 2D image generation approach, combining DDPMs with generative adversarial networks (GANs) and employing a variance-preserving noise schedule, for the task of 3D inpainting. Our experiments showed that the variance-preserving noise schedule and the selected reconstruction losses can be effectively utilized for high-quality 3D inpainting in a few time steps without requiring adversarial training. We applied our findings to a different architecture, a 3D wavelet diffusion model (WDM3D) that does not include a GAN component. The resulting model, denoted as fastWDM3D, obtained a SSIM of 0.8571, a MSE of 0.0079, and a PSNR of 22.26 on the BraTS inpainting test set. Remarkably, it achieved these scores using only two time steps, completing the 3D inpainting process in 1.81 s per image. When compared to other DDPMs used for healthy brain tissue inpainting, our model is up to 800 x faster while still achieving superior performance metrics. Our proposed method, fastWDM3D, represents a promising approach for fast and accurate healthy tissue inpainting. Our code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 107,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-18?abs=True",
        "arxiv_id": "2507.13339",
        "abs_url": "https://arxiv.org/abs/2507.13339",
        "pdf_url": "https://arxiv.org/pdf/2507.13339",
        "title": "SpectraLift: Physics-Guided Spectral-Inversion Network for Self-Supervised Hyperspectral Image Super-Resolution",
        "authors": [
            "Ritik Shah",
            "Marco F. Duarte"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "High-spatial-resolution hyperspectral images (HSI) are essential for applications such as remote sensing and medical imaging, yet HSI sensors inherently trade spatial detail for spectral richness. Fusing high-spatial-resolution multispectral images (HR-MSI) with low-spatial-resolution hyperspectral images (LR-HSI) is a promising route to recover fine spatial structures without sacrificing spectral fidelity. Most state-of-the-art methods for HSI-MSI fusion demand point spread function (PSF) calibration or ground truth high resolution HSI (HR-HSI), both of which are impractical to obtain in real world settings. We present SpectraLift, a fully self-supervised framework that fuses LR-HSI and HR-MSI inputs using only the MSI's Spectral Response Function (SRF). SpectraLift trains a lightweight per-pixel multi-layer perceptron (MLP) network using ($i$)~a synthetic low-spatial-resolution multispectral image (LR-MSI) obtained by applying the SRF to the LR-HSI as input, ($ii$)~the LR-HSI as the output, and ($iii$)~an $\\ell_1$ spectral reconstruction loss between the estimated and true LR-HSI as the optimization objective. At inference, SpectraLift uses the trained network to map the HR-MSI pixel-wise into a HR-HSI estimate. SpectraLift converges in minutes, is agnostic to spatial blur and resolution, and outperforms state-of-the-art methods on PSNR, SAM, SSIM, and RMSE benchmarks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    }
]