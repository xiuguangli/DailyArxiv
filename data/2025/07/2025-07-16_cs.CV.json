[
    {
        "order": 1,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.10689",
        "abs_url": "https://arxiv.org/abs/2507.10689",
        "pdf_url": "https://arxiv.org/pdf/2507.10689",
        "title": "CWNet: Causal Wavelet Network for Low-Light Image Enhancement",
        "authors": [
            "Tongshun Zhang",
            "Pingping Liu",
            "Yubing Lu",
            "Mengen Cai",
            "Zijian Zhang",
            "Zhe Zhang",
            "Qiuzhan Zhou"
        ],
        "comments": "Accepted by ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Traditional Low-Light Image Enhancement (LLIE) methods primarily focus on uniform brightness adjustment, often neglecting instance-level semantic information and the inherent characteristics of different features. To address these limitations, we propose CWNet (Causal Wavelet Network), a novel architecture that leverages wavelet transforms for causal reasoning. Specifically, our approach comprises two key components: 1) Inspired by the concept of intervention in causality, we adopt a causal reasoning perspective to reveal the underlying causal relationships in low-light enhancement. From a global perspective, we employ a metric learning strategy to ensure causal embeddings adhere to causal principles, separating them from non-causal confounding factors while focusing on the invariance of causal factors. At the local level, we introduce an instance-level CLIP semantic loss to precisely maintain causal factor consistency. 2) Based on our causal analysis, we present a wavelet transform-based backbone network that effectively optimizes the recovery of frequency information, ensuring precise enhancement tailored to the specific attributes of wavelet transforms. Extensive experiments demonstrate that CWNet significantly outperforms current state-of-the-art methods across multiple datasets, showcasing its robust performance across diverse scenes. Code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 2,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.10737",
        "abs_url": "https://arxiv.org/abs/2507.10737",
        "pdf_url": "https://arxiv.org/pdf/2507.10737",
        "title": "Integrating Biological Knowledge for Robust Microscopy Image Profiling on De Novo Cell Lines",
        "authors": [
            "Jiayuan Chen",
            "Thai-Hoang Pham",
            "Yuanlong Wang",
            "Ping Zhang"
        ],
        "comments": "ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "High-throughput screening techniques, such as microscopy imaging of cellular responses to genetic and chemical perturbations, play a crucial role in drug discovery and biomedical research. However, robust perturbation screening for \\textit{de novo} cell lines remains challenging due to the significant morphological and biological heterogeneity across cell lines. To address this, we propose a novel framework that integrates external biological knowledge into existing pretraining strategies to enhance microscopy image profiling models. Our approach explicitly disentangles perturbation-specific and cell line-specific representations using external biological information. Specifically, we construct a knowledge graph leveraging protein interaction data from STRING and Hetionet databases to guide models toward perturbation-specific features during pretraining. Additionally, we incorporate transcriptomic features from single-cell foundation models to capture cell line-specific representations. By learning these disentangled features, our method improves the generalization of imaging models to \\textit{de novo} cell lines. We evaluate our framework on the RxRx database through one-shot fine-tuning on an RxRx1 cell line and few-shot fine-tuning on cell lines from the RxRx19a dataset. Experimental results demonstrate that our method enhances microscopy image profiling for \\textit{de novo} cell lines, highlighting its effectiveness in real-world phenotype-based drug discovery applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 3,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.10755",
        "abs_url": "https://arxiv.org/abs/2507.10755",
        "pdf_url": "https://arxiv.org/pdf/2507.10755",
        "title": "Auditing Facial Emotion Recognition Datasets for Posed Expressions and Racial Bias",
        "authors": [
            "Rina Khan",
            "Catherine Stinson"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Facial expression recognition (FER) algorithms classify facial expressions into emotions such as happy, sad, or angry. An evaluative challenge facing FER algorithms is the fall in performance when detecting spontaneous expressions compared to posed expressions. An ethical (and evaluative) challenge facing FER algorithms is that they tend to perform poorly for people of some races and skin colors. These challenges are linked to the data collection practices employed in the creation of FER datasets. In this study, we audit two state-of-the-art FER datasets. We take random samples from each dataset and examine whether images are spontaneous or posed. In doing so, we propose a methodology for identifying spontaneous or posed images. We discover a significant number of images that were posed in the datasets purporting to consist of in-the-wild images. Since performance of FER models vary between spontaneous and posed images, the performance of models trained on these datasets will not represent the true performance if such models were to be deployed in in-the-wild applications. We also observe the skin color of individuals in the samples, and test three models trained on each of the datasets to predict facial expressions of people from various races and skin tones. We find that the FER models audited were more likely to predict people labeled as not white or determined to have dark skin as showing a negative emotion such as anger or sadness even when they were smiling. This bias makes such models prone to perpetuate harm in real life applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 4,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.10770",
        "abs_url": "https://arxiv.org/abs/2507.10770",
        "pdf_url": "https://arxiv.org/pdf/2507.10770",
        "title": "FPC-Net: Revisiting SuperPoint with Descriptor-Free Keypoint Detection via Feature Pyramids and Consistency-Based Implicit Matching",
        "authors": [
            "Ionuţ Grigore",
            "Călin-Adrian Popa",
            "Claudiu Leoveanu-Condrei"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The extraction and matching of interest points are fundamental to many geometric computer vision tasks. Traditionally, matching is performed by assigning descriptors to interest points and identifying correspondences based on descriptor similarity. This work introduces a technique where interest points are inherently associated during detection, eliminating the need for computing, storing, transmitting, or matching descriptors. Although the matching accuracy is marginally lower than that of conventional approaches, our method completely eliminates the need for descriptors, leading to a drastic reduction in memory usage for localization systems. We assess its effectiveness by comparing it against both classical handcrafted methods and modern learned approaches.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 5,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.10775",
        "abs_url": "https://arxiv.org/abs/2507.10775",
        "pdf_url": "https://arxiv.org/pdf/2507.10775",
        "title": "A New Dataset and Performance Benchmark for Real-time Spacecraft Segmentation in Onboard Flight Computers",
        "authors": [
            "Jeffrey Joan Sam",
            "Janhavi Sathe",
            "Nikhil Chigali",
            "Naman Gupta",
            "Radhey Ruparel",
            "Yicheng Jiang",
            "Janmajay Singh",
            "James W. Berck",
            "Arko Barman"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Image and Video Processing (eess.IV)",
        "abstract": "Spacecraft deployed in outer space are routinely subjected to various forms of damage due to exposure to hazardous environments. In addition, there are significant risks to the subsequent process of in-space repairs through human extravehicular activity or robotic manipulation, incurring substantial operational costs. Recent developments in image segmentation could enable the development of reliable and cost-effective autonomous inspection systems. While these models often require large amounts of training data to achieve satisfactory results, publicly available annotated spacecraft segmentation data are very scarce. Here, we present a new dataset of nearly 64k annotated spacecraft images that was created using real spacecraft models, superimposed on a mixture of real and synthetic backgrounds generated using NASA's TTALOS pipeline. To mimic camera distortions and noise in real-world image acquisition, we also added different types of noise and distortion to the images. Finally, we finetuned YOLOv8 and YOLOv11 segmentation models to generate performance benchmarks for the dataset under well-defined hardware and inference time constraints to mimic real-world image segmentation challenges for real-time onboard applications in space on NASA's inspector spacecraft. The resulting models, when tested under these constraints, achieved a Dice score of 0.92, Hausdorff distance of 0.69, and an inference time of about 0.5 second. The dataset and models for performance benchmark are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 6,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.10778",
        "abs_url": "https://arxiv.org/abs/2507.10778",
        "pdf_url": "https://arxiv.org/pdf/2507.10778",
        "title": "Warehouse Spatial Question Answering with LLM Agent",
        "authors": [
            "Hsiang-Wei Huang",
            "Jen-Hao Cheng",
            "Kuang-Ming Chen",
            "Cheng-Yen Yang",
            "Bahaa Alattar",
            "Yi-Ru Lin",
            "Pyongkun Kim",
            "Sangwon Kim",
            "Kwangju Kim",
            "Chung-I Huang",
            "Jenq-Neng Hwang"
        ],
        "comments": "1st Place Solution of the 9th AI City Challenge Track 3",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Spatial understanding has been a challenging task for existing Multi-modal Large Language Models~(MLLMs). Previous methods leverage large-scale MLLM finetuning to enhance MLLM's spatial understanding ability. In this paper, we present a data-efficient approach. We propose a LLM agent system with strong and advanced spatial reasoning ability, which can be used to solve the challenging spatial question answering task in complex indoor warehouse scenarios. Our system integrates multiple tools that allow the LLM agent to conduct spatial reasoning and API tools interaction to answer the given complicated spatial question. Extensive evaluations on the 2025 AI City Challenge Physical AI Spatial Intelligence Warehouse dataset demonstrate that our system achieves high accuracy and efficiency in tasks such as object retrieval, counting, and distance estimation. The code is available at: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 7,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.10800",
        "abs_url": "https://arxiv.org/abs/2507.10800",
        "pdf_url": "https://arxiv.org/pdf/2507.10800",
        "title": "ThinkingViT: Matryoshka Thinking Vision Transformer for Elastic Inference",
        "authors": [
            "Ali Hojjat",
            "Janek Haberer",
            "Soren Pirk",
            "Olaf Landsiedel"
        ],
        "comments": "Under Review",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Vision Transformers deliver state-of-the-art performance, yet their fixed computational budget prevents scalable deployment across heterogeneous hardware. Recent nested Transformer architectures mitigate this by embedding nested subnetworks within a single model to enable scalable inference. However, these models allocate the same amount of compute to all inputs, regardless of their complexity, which leads to inefficiencies. To address this, we introduce ThinkingViT, a nested ViT architecture that employs progressive thinking stages to dynamically adjust inference computation based on input difficulty. ThinkingViT initiates inference by activating a small subset of the most important attention heads and terminates early if predictions reach sufficient certainty. Otherwise, it activates additional attention heads and re-evaluates the input. At the core of ThinkingViT is our Token Recycling mechanism, which conditions each subsequent inference stage on the embeddings from the previous stage, enabling progressive improvement. Due to its backbone-preserving design, ThinkingViT also serves as a plugin upgrade for vanilla ViT. Experiments show that ThinkingViT surpasses nested baselines by up to 2.0 percentage points (p.p.) in accuracy at the same throughput and by up to 2.9 p.p. at equal GMACs on ImageNet-1K. The source code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 8,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.10844",
        "abs_url": "https://arxiv.org/abs/2507.10844",
        "pdf_url": "https://arxiv.org/pdf/2507.10844",
        "title": "LLM-Guided Agentic Object Detection for Open-World Understanding",
        "authors": [
            "Furkan Mumcu",
            "Michael J. Jones",
            "Anoop Cherian",
            "Yasin Yilmaz"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Object detection traditionally relies on fixed category sets, requiring costly re-training to handle novel objects. While Open-World and Open-Vocabulary Object Detection (OWOD and OVOD) improve flexibility, OWOD lacks semantic labels for unknowns, and OVOD depends on user prompts, limiting autonomy. We propose an LLM-guided agentic object detection (LAOD) framework that enables fully label-free, zero-shot detection by prompting a Large Language Model (LLM) to generate scene-specific object names. These are passed to an open-vocabulary detector for localization, allowing the system to adapt its goals dynamically. We introduce two new metrics, Class-Agnostic Average Precision (CAAP) and Semantic Naming Average Precision (SNAP), to separately evaluate localization and naming. Experiments on LVIS, COCO, and COCO-OOD validate our approach, showing strong performance in detecting and naming novel objects. Our method offers enhanced autonomy and adaptability for open-world understanding.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 9,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.10846",
        "abs_url": "https://arxiv.org/abs/2507.10846",
        "pdf_url": "https://arxiv.org/pdf/2507.10846",
        "title": "Winsor-CAM: Human-Tunable Visual Explanations from Deep Networks via Layer-Wise Winsorization",
        "authors": [
            "Casey Wall",
            "Longwei Wang",
            "Rodrigue Rizk",
            "KC Santosh"
        ],
        "comments": "15 pages, 10 figures, 7 tables. Submitted to IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Interpreting the decision-making process of Convolutional Neural Networks (CNNs) is critical for deploying models in high-stakes domains. Gradient-weighted Class Activation Mapping (Grad-CAM) is a widely used method for visual explanations, yet it typically focuses on the final convolutional layer or naïvely averages across layers, strategies that can obscure important semantic cues or amplify irrelevant noise. We propose Winsor-CAM, a novel, human-tunable extension of Grad-CAM that generates robust and coherent saliency maps by aggregating information across all convolutional layers. To mitigate the influence of noisy or extreme attribution values, Winsor-CAM applies Winsorization, a percentile-based outlier attenuation technique. A user-controllable threshold allows for semantic-level tuning, enabling flexible exploration of model behavior across representational hierarchies. Evaluations on standard architectures (ResNet50, DenseNet121, VGG16, InceptionV3) using the PASCAL VOC 2012 dataset demonstrate that Winsor-CAM produces more interpretable heatmaps and achieves superior performance in localization metrics, including intersection-over-union and center-of-mass alignment, when compared to Grad-CAM and uniform layer-averaging baselines. Winsor-CAM advances the goal of trustworthy AI by offering interpretable, multi-layer insights with human-in-the-loop control.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 10,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.10855",
        "abs_url": "https://arxiv.org/abs/2507.10855",
        "pdf_url": "https://arxiv.org/pdf/2507.10855",
        "title": "Sparse Fine-Tuning of Transformers for Generative Tasks",
        "authors": [
            "Wei Chen",
            "Jingxi Yu",
            "Zichen Miao",
            "Qiang Qiu"
        ],
        "comments": "Accepted by International Conference on Computer Vision 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Large pre-trained transformers have revolutionized artificial intelligence across various domains, and fine-tuning remains the dominant approach for adapting these models to downstream tasks due to the cost of training from scratch. However, in existing fine-tuning methods, the updated representations are formed as a dense combination of modified parameters, making it challenging to interpret their contributions and understand how the model adapts to new tasks. In this work, we introduce a fine-tuning framework inspired by sparse coding, where fine-tuned features are represented as a sparse combination of basic elements, i.e., feature dictionary atoms. The feature dictionary atoms function as fundamental building blocks of the representation, and tuning atoms allows for seamless adaptation to downstream tasks. Sparse coefficients then serve as indicators of atom importance, identifying the contribution of each atom to the updated representation. Leveraging the atom selection capability of sparse coefficients, we first demonstrate that our method enhances image editing performance by improving text alignment through the removal of unimportant feature dictionary atoms. Additionally, we validate the effectiveness of our approach in the text-to-image concept customization task, where our method efficiently constructs the target concept using a sparse combination of feature dictionary atoms, outperforming various baseline fine-tuning methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 11,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.10864",
        "abs_url": "https://arxiv.org/abs/2507.10864",
        "pdf_url": "https://arxiv.org/pdf/2507.10864",
        "title": "A Lightweight and Robust Framework for Real-Time Colorectal Polyp Detection Using LOF-Based Preprocessing and YOLO-v11n",
        "authors": [
            "Saadat Behzadi",
            "Danial Sharifrazi",
            "Bita Mesbahzadeh",
            "Javad Hassannataj Joloudarid",
            "Roohallah Alizadehsani"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Objectives: Timely and accurate detection of colorectal polyps plays a crucial role in diagnosing and preventing colorectal cancer, a major cause of mortality worldwide. This study introduces a new, lightweight, and efficient framework for polyp detection that combines the Local Outlier Factor (LOF) algorithm for filtering noisy data with the YOLO-v11n deep learning model. Study design: An experimental study leveraging deep learning and outlier removal techniques across multiple public datasets. Methods: The proposed approach was tested on five diverse and publicly available datasets: CVC-ColonDB, CVC-ClinicDB, Kvasir-SEG, ETIS, and EndoScene. Since these datasets originally lacked bounding box annotations, we converted their segmentation masks into suitable detection labels. To enhance the robustness and generalizability of our model, we apply 5-fold cross-validation and remove anomalous samples using the LOF method configured with 30 neighbors and a contamination ratio of 5%. Cleaned data are then fed into YOLO-v11n, a fast and resource-efficient object detection architecture optimized for real-time applications. We train the model using a combination of modern augmentation strategies to improve detection accuracy under diverse conditions. Results: Our approach significantly improves polyp localization performance, achieving a precision of 95.83%, recall of 91.85%, F1-score of 93.48%, mAP@0.5 of 96.48%, and mAP@0.5:0.95 of 77.75%. Compared to previous YOLO-based methods, our model demonstrates enhanced accuracy and efficiency. Conclusions: These results suggest that the proposed method is well-suited for real-time colonoscopy support in clinical settings. Overall, the study underscores how crucial data preprocessing and model efficiency are when designing effective AI systems for medical imaging.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 12,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.10881",
        "abs_url": "https://arxiv.org/abs/2507.10881",
        "pdf_url": "https://arxiv.org/pdf/2507.10881",
        "title": "Trexplorer Super: Topologically Correct Centerline Tree Tracking of Tubular Objects in CT Volumes",
        "authors": [
            "Roman Naeem",
            "David Hagerman",
            "Jennifer Alvén",
            "Lennart Svensson",
            "Fredrik Kahl"
        ],
        "comments": "Submitted Version. Accepted at MICCAI 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Tubular tree structures, such as blood vessels and airways, are essential in human anatomy and accurately tracking them while preserving their topology is crucial for various downstream tasks. Trexplorer is a recurrent model designed for centerline tracking in 3D medical images but it struggles with predicting duplicate branches and terminating tracking prematurely. To address these issues, we present Trexplorer Super, an enhanced version that notably improves performance through novel advancements. However, evaluating centerline tracking models is challenging due to the lack of public datasets. To enable thorough evaluation, we develop three centerline datasets, one synthetic and two real, each with increasing difficulty. Using these datasets, we conduct a comprehensive evaluation of existing state-of-the-art (SOTA) models and compare them with our approach. Trexplorer Super outperforms previous SOTA models on every dataset. Our results also highlight that strong performance on synthetic data does not necessarily translate to real datasets. The code and datasets are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 13,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.10893",
        "abs_url": "https://arxiv.org/abs/2507.10893",
        "pdf_url": "https://arxiv.org/pdf/2507.10893",
        "title": "Modernizing CNN-based Weather Forecast Model towards Higher Computational Efficiency",
        "authors": [
            "Minjong Cheon",
            "Eunhan Goo",
            "Su-Hyeon Shin",
            "Muhammad Ahmed",
            "Hyungjun Kim"
        ],
        "comments": "26pages, 9 Figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Atmospheric and Oceanic Physics (physics.ao-ph)",
        "abstract": "Recently, AI-based weather forecast models have achieved impressive advances. These models have reached accuracy levels comparable to traditional NWP systems, marking a significant milestone in data-driven weather prediction. However, they mostly leverage Transformer-based architectures, which often leads to high training complexity and resource demands due to the massive parameter sizes. In this study, we introduce a modernized CNN-based model for global weather forecasting that delivers competitive accuracy while significantly reducing computational requirements. To present a systematic modernization roadmap, we highlight key architectural enhancements across multiple design scales from an earlier CNN-based approach. KAI-a incorporates a scale-invariant architecture and InceptionNeXt-based blocks within a geophysically-aware design, tailored to the structure of Earth system data. Trained on the ERA5 daily dataset with 67 atmospheric variables, the model contains about 7 million parameters and completes training in just 12 hours on a single NVIDIA L40s GPU. Our evaluation shows that KAI-a matches the performance of state-of-the-art models in medium-range weather forecasting, while offering a significantly lightweight design. Furthermore, case studies on the 2018 European heatwave and the East Asian summer monsoon demonstrate KAI-a's robust skill in capturing extreme events, reinforcing its practical utility.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 14,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.10895",
        "abs_url": "https://arxiv.org/abs/2507.10895",
        "pdf_url": "https://arxiv.org/pdf/2507.10895",
        "title": "Commuting Distance Regularization for Timescale-Dependent Label Inconsistency in EEG Emotion Recognition",
        "authors": [
            "Xiaocong Zeng",
            "Craig Michoski",
            "Yan Pang",
            "Dongyang Kuang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Signal Processing (eess.SP)",
        "abstract": "In this work, we address the often-overlooked issue of Timescale Dependent Label Inconsistency (TsDLI) in training neural network models for EEG-based human emotion recognition. To mitigate TsDLI and enhance model generalization and explainability, we propose two novel regularization strategies: Local Variation Loss (LVL) and Local-Global Consistency Loss (LGCL). Both methods incorporate classical mathematical principles--specifically, functions of bounded variation and commute-time distances--within a graph theoretic framework. Complementing our regularizers, we introduce a suite of new evaluation metrics that better capture the alignment between temporally local predictions and their associated global emotion labels. We validate our approach through comprehensive experiments on two widely used EEG emotion datasets, DREAMER and DEAP, across a range of neural architectures including LSTM and transformer-based models. Performance is assessed using five distinct metrics encompassing both quantitative accuracy and qualitative consistency. Results consistently show that our proposed methods outperform state-of-the-art baselines, delivering superior aggregate performance and offering a principled trade-off between interpretability and predictive power under label inconsistency. Notably, LVL achieves the best aggregate rank across all benchmarked backbones and metrics, while LGCL frequently ranks the second, highlighting the effectiveness of our framework.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 15,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.10935",
        "abs_url": "https://arxiv.org/abs/2507.10935",
        "pdf_url": "https://arxiv.org/pdf/2507.10935",
        "title": "GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised Cross-View Localization",
        "authors": [
            "Shaowen Tong",
            "Zimin Xia",
            "Alexandre Alahi",
            "Xuming He",
            "Yujiao Shi"
        ],
        "comments": "accepted by ICCV2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Cross-view localization, the task of estimating a camera's 3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with satellite images, is crucial for large-scale outdoor applications like autonomous navigation and augmented reality. Existing methods often rely on fully supervised learning, which requires costly ground-truth pose annotations. In this work, we propose GeoDistill, a Geometry guided weakly supervised self distillation framework that uses teacher-student learning with Field-of-View (FoV)-based masking to enhance local feature learning for robust cross-view localization. In GeoDistill, the teacher model localizes a panoramic image, while the student model predicts locations from a limited FoV counterpart created by FoV-based masking. By aligning the student's predictions with those of the teacher, the student focuses on key features like lane lines and ignores textureless regions, such as roads. This results in more accurate predictions and reduced uncertainty, regardless of whether the query images are panoramas or limited FoV images. Our experiments show that GeoDistill significantly improves localization performance across different frameworks. Additionally, we introduce a novel orientation estimation network that predicts relative orientation without requiring precise planar position ground truth. GeoDistill provides a scalable and efficient solution for real-world cross-view localization challenges. Code and model can be found at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 16,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.10938",
        "abs_url": "https://arxiv.org/abs/2507.10938",
        "pdf_url": "https://arxiv.org/pdf/2507.10938",
        "title": "Graph Aggregation Prototype Learning for Semantic Change Detection in Remote Sensing",
        "authors": [
            "Zhengyi Xu",
            "Haoran Wu",
            "Wen Jiang",
            "Jie Geng"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Semantic change detection (SCD) extends the binary change detection task to provide not only the change locations but also the detailed \"from-to\" categories in multi-temporal remote sensing data. Such detailed semantic insights into changes offer considerable advantages for a wide array of applications. However, since SCD involves the simultaneous optimization of multiple tasks, the model is prone to negative transfer due to task-specific learning difficulties and conflicting gradient flows. To address this issue, we propose Graph Aggregation Prototype Learning for Semantic Change Detection in remote sensing(GAPL-SCD). In this framework, a multi-task joint optimization method is designed to optimize the primary task of semantic segmentation and change detection, along with the auxiliary task of graph aggregation prototype learning. Adaptive weight allocation and gradient rotation methods are used to alleviate the conflict between training tasks and improve multi-task learning capabilities. Specifically, the graph aggregation prototype learning module constructs an interaction graph using high-level features. Prototypes serve as class proxies, enabling category-level domain alignment across time points and reducing interference from irrelevant changes. Additionally, the proposed self-query multi-level feature interaction and bi-temporal feature fusion modules further enhance multi-scale feature representation, improving performance in complex scenes. Experimental results on the SECOND and Landsat-SCD datasets demonstrate that our method achieves state-of-the-art performance, with significant improvements in accuracy and robustness for SCD task.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 17,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.10943",
        "abs_url": "https://arxiv.org/abs/2507.10943",
        "pdf_url": "https://arxiv.org/pdf/2507.10943",
        "title": "Robust ID-Specific Face Restoration via Alignment Learning",
        "authors": [
            "Yushun Fang",
            "Lu Liu",
            "Xiang Gao",
            "Qiang Hu",
            "Ning Cao",
            "Jianghe Cui",
            "Gang Chen",
            "Xiaoyun Zhang"
        ],
        "comments": "17 pages, 8 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The latest developments in Face Restoration have yielded significant advancements in visual quality through the utilization of diverse diffusion priors. Nevertheless, the uncertainty of face identity introduced by identity-obscure inputs and stochastic generative processes remains unresolved. To address this challenge, we present Robust ID-Specific Face Restoration (RIDFR), a novel ID-specific face restoration framework based on diffusion models. Specifically, RIDFR leverages a pre-trained diffusion model in conjunction with two parallel conditioning modules. The Content Injection Module inputs the severely degraded image, while the Identity Injection Module integrates the specific identity from a given image. Subsequently, RIDFR incorporates Alignment Learning, which aligns the restoration results from multiple references with the same identity in order to suppress the interference of ID-irrelevant face semantics (e.g. pose, expression, make-up, hair style). Experiments demonstrate that our framework outperforms the state-of-the-art methods, reconstructing high-quality ID-specific results with high identity fidelity and demonstrating strong robustness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 18,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.10969",
        "abs_url": "https://arxiv.org/abs/2507.10969",
        "pdf_url": "https://arxiv.org/pdf/2507.10969",
        "title": "Women Sport Actions Dataset for Visual Classification Using Small Scale Training Data",
        "authors": [
            "Palash Ray",
            "Mahuya Sasmal",
            "Asish Bera"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Sports action classification representing complex body postures and player-object interactions is an emerging area in image-based sports analysis. Some works have contributed to automated sports action recognition using machine learning techniques over the past decades. However, sufficient image datasets representing women sports actions with enough intra- and inter-class variations are not available to the researchers. To overcome this limitation, this work presents a new dataset named WomenSports for women sports classification using small-scale training data. This dataset includes a variety of sports activities, covering wide variations in movements, environments, and interactions among players. In addition, this study proposes a convolutional neural network (CNN) for deep feature extraction. A channel attention scheme upon local contextual regions is applied to refine and enhance feature representation. The experiments are carried out on three different sports datasets and one dance dataset for generalizing the proposed algorithm, and the performances on these datasets are noteworthy. The deep learning method achieves 89.15% top-1 classification accuracy using ResNet-50 on the proposed WomenSports dataset, which is publicly available for research at Mendeley Data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 19,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.10977",
        "abs_url": "https://arxiv.org/abs/2507.10977",
        "pdf_url": "https://arxiv.org/pdf/2507.10977",
        "title": "Conceptualizing Multi-scale Wavelet Attention and Ray-based Encoding for Human-Object Interaction Detection",
        "authors": [
            "Quan Bi Pay",
            "Vishnu Monn Baskaran",
            "Junn Yong Loo",
            "KokSheik Wong",
            "Simon See"
        ],
        "comments": "Accepted at International Joint Conference on Neural Networks (IJCNN 2025)",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Human-object interaction (HOI) detection is essential for accurately localizing and characterizing interactions between humans and objects, providing a comprehensive understanding of complex visual scenes across various domains. However, existing HOI detectors often struggle to deliver reliable predictions efficiently, relying on resource-intensive training methods and inefficient architectures. To address these challenges, we conceptualize a wavelet attention-like backbone and a novel ray-based encoder architecture tailored for HOI detection. Our wavelet backbone addresses the limitations of expressing middle-order interactions by aggregating discriminative features from the low- and high-order interactions extracted from diverse convolutional filters. Concurrently, the ray-based encoder facilitates multi-scale attention by optimizing the focus of the decoder on relevant regions of interest and mitigating computational overhead. As a result of harnessing the attenuated intensity of learnable ray origins, our decoder aligns query embeddings with emphasized regions of interest for accurate predictions. Experimental results on benchmark datasets, including ImageNet and HICO-DET, showcase the potential of our proposed architecture. The code is publicly available at [this https URL].",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 20,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.10978",
        "abs_url": "https://arxiv.org/abs/2507.10978",
        "pdf_url": "https://arxiv.org/pdf/2507.10978",
        "title": "Mind the Gap: Bridging Occlusion in Gait Recognition via Residual Gap Correction",
        "authors": [
            "Ayush Gupta",
            "Siyuan Huang",
            "Rama Chellappa"
        ],
        "comments": "Accepted at IJCB 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Gait is becoming popular as a method of person re-identification because of its ability to identify people at a distance. However, most current works in gait recognition do not address the practical problem of occlusions. Among those which do, some require paired tuples of occluded and holistic sequences, which are impractical to collect in the real world. Further, these approaches work on occlusions but fail to retain performance on holistic inputs. To address these challenges, we propose RG-Gait, a method for residual correction for occluded gait recognition with holistic retention. We model the problem as a residual learning task, conceptualizing the occluded gait signature as a residual deviation from the holistic gait representation. Our proposed network adaptively integrates the learned residual, significantly improving performance on occluded gait sequences without compromising the holistic recognition accuracy. We evaluate our approach on the challenging Gait3D, GREW and BRIAR datasets and show that learning the residual can be an effective technique to tackle occluded gait recognition with holistic retention.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 21,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.10999",
        "abs_url": "https://arxiv.org/abs/2507.10999",
        "pdf_url": "https://arxiv.org/pdf/2507.10999",
        "title": "SpaRTAN: Spatial Reinforcement Token-based Aggregation Network for Visual Recognition",
        "authors": [
            "Quan Bi Pay",
            "Vishnu Monn Baskaran",
            "Junn Yong Loo",
            "KokSheik Wong",
            "Simon See"
        ],
        "comments": "Accepted at International Joint Conference on Neural Networks (IJCNN 2025)",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "The resurgence of convolutional neural networks (CNNs) in visual recognition tasks, exemplified by ConvNeXt, has demonstrated their capability to rival transformer-based architectures through advanced training methodologies and ViT-inspired design principles. However, both CNNs and transformers exhibit a simplicity bias, favoring straightforward features over complex structural representations. Furthermore, modern CNNs often integrate MLP-like blocks akin to those in transformers, but these blocks suffer from significant information redundancies, necessitating high expansion ratios to sustain competitive performance. To address these limitations, we propose SpaRTAN, a lightweight architectural design that enhances spatial and channel-wise information processing. SpaRTAN employs kernels with varying receptive fields, controlled by kernel size and dilation factor, to capture discriminative multi-order spatial features effectively. A wave-based channel aggregation module further modulates and reinforces pixel interactions, mitigating channel-wise redundancies. Combining the two modules, the proposed network can efficiently gather and dynamically contextualize discriminative features. Experimental results in ImageNet and COCO demonstrate that SpaRTAN achieves remarkable parameter efficiency while maintaining competitive performance. In particular, on the ImageNet-1k benchmark, SpaRTAN achieves 77. 7% accuracy with only 3.8M parameters and approximately 1.0 GFLOPs, demonstrating its ability to deliver strong performance through an efficient design. On the COCO benchmark, it achieves 50.0% AP, surpassing the previous benchmark by 1.2% with only 21.5M parameters. The code is publicly available at [this https URL].",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 22,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11003",
        "abs_url": "https://arxiv.org/abs/2507.11003",
        "pdf_url": "https://arxiv.org/pdf/2507.11003",
        "title": "Bridge Feature Matching and Cross-Modal Alignment with Mutual-filtering for Zero-shot Anomaly Detection",
        "authors": [
            "Yuhu Bai",
            "Jiangning Zhang",
            "Yunkang Cao",
            "Guangyuan Lu",
            "Qingdong He",
            "Xiangtai Li",
            "Guanzhong Tian"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "With the advent of vision-language models (e.g., CLIP) in zero- and few-shot settings, CLIP has been widely applied to zero-shot anomaly detection (ZSAD) in recent research, where the rare classes are essential and expected in many applications. This study introduces \\textbf{FiSeCLIP} for ZSAD with training-free \\textbf{CLIP}, combining the feature matching with the cross-modal alignment. Testing with the entire dataset is impractical, while batch-based testing better aligns with real industrial needs, and images within a batch can serve as mutual reference points. Accordingly, FiSeCLIP utilizes other images in the same batch as reference information for the current image. However, the lack of labels for these references can introduce ambiguity, we apply text information to \\textbf{fi}lter out noisy features. In addition, we further explore CLIP's inherent potential to restore its local \\textbf{se}mantic correlation, adapting it for fine-grained anomaly detection tasks to enable a more accurate filtering process. Our approach exhibits superior performance for both anomaly classification and segmentation on anomaly detection benchmarks, building a stronger baseline for the direction, e.g., on MVTec-AD, FiSeCLIP outperforms the SOTA AdaCLIP by +4.6\\%$\\uparrow$/+5.7\\%$\\uparrow$ in segmentation metrics AU-ROC/$F_1$-max.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 23,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11015",
        "abs_url": "https://arxiv.org/abs/2507.11015",
        "pdf_url": "https://arxiv.org/pdf/2507.11015",
        "title": "Semantically Informed Salient Regions Guided Radiology Report Generation",
        "authors": [
            "Zeyi Hou",
            "Zeqiang Wei",
            "Ruixin Yan",
            "Ning Lang",
            "Xiuzhuang Zhou"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Recent advances in automated radiology report generation from chest X-rays using deep learning algorithms have the potential to significantly reduce the arduous workload of radiologists. However, due to the inherent massive data bias in radiology images, where abnormalities are typically subtle and sparsely distributed, existing methods often produce fluent yet medically inaccurate reports, limiting their applicability in clinical practice. To address this issue effectively, we propose a Semantically Informed Salient Regions-guided (SISRNet) report generation method. Specifically, our approach explicitly identifies salient regions with medically critical characteristics using fine-grained cross-modal semantics. Then, SISRNet systematically focuses on these high-information regions during both image modeling and report generation, effectively capturing subtle abnormal findings, mitigating the negative impact of data bias, and ultimately generating clinically accurate reports. Compared to its peers, SISRNet demonstrates superior performance on widely used IU-Xray and MIMIC-CXR datasets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 24,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11025",
        "abs_url": "https://arxiv.org/abs/2507.11025",
        "pdf_url": "https://arxiv.org/pdf/2507.11025",
        "title": "Human-Guided Shade Artifact Suppression in CBCT-to-MDCT Translation via Schrödinger Bridge with Conditional Diffusion",
        "authors": [
            "Sung Ho Kang",
            "Hyun-Cheol Park"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We present a novel framework for CBCT-to-MDCT translation, grounded in the Schrodinger Bridge (SB) formulation, which integrates GAN-derived priors with human-guided conditional diffusion. Unlike conventional GANs or diffusion models, our approach explicitly enforces boundary consistency between CBCT inputs and pseudo targets, ensuring both anatomical fidelity and perceptual controllability. Binary human feedback is incorporated via classifier-free guidance (CFG), effectively steering the generative process toward clinically preferred outcomes. Through iterative refinement and tournament-based preference selection, the model internalizes human preferences without relying on a reward model. Subtraction image visualizations reveal that the proposed method selectively attenuates shade artifacts in key anatomical regions while preserving fine structural detail. Quantitative evaluations further demonstrate superior performance across RMSE, SSIM, LPIPS, and Dice metrics on clinical datasets -- outperforming prior GAN- and fine-tuning-based feedback methods -- while requiring only 10 sampling steps. These findings underscore the effectiveness and efficiency of our framework for real-time, preference-aligned medical image translation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 25,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11030",
        "abs_url": "https://arxiv.org/abs/2507.11030",
        "pdf_url": "https://arxiv.org/pdf/2507.11030",
        "title": "Personalized OVSS: Understanding Personal Concept in Open-Vocabulary Semantic Segmentation",
        "authors": [
            "Sunghyun Park",
            "Jungsoo Lee",
            "Shubhankar Borse",
            "Munawar Hayat",
            "Sungha Choi",
            "Kyuwoong Hwang",
            "Fatih Porikli"
        ],
        "comments": "Accepted to ICCV 2025; 15 pages",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "While open-vocabulary semantic segmentation (OVSS) can segment an image into semantic regions based on arbitrarily given text descriptions even for classes unseen during training, it fails to understand personal texts (e.g., `my mug cup') for segmenting regions of specific interest to users. This paper addresses challenges like recognizing `my mug cup' among `multiple mug cups'. To overcome this challenge, we introduce a novel task termed \\textit{personalized open-vocabulary semantic segmentation} and propose a text prompt tuning-based plug-in method designed to recognize personal visual concepts using a few pairs of images and masks, while maintaining the performance of the original OVSS. Based on the observation that reducing false predictions is essential when applying text prompt tuning to this task, our proposed method employs `negative mask proposal' that captures visual concepts other than the personalized concept. We further improve the performance by enriching the representation of text prompts by injecting visual embeddings of the personal concept into them. This approach enhances personalized OVSS without compromising the original OVSS performance. We demonstrate the superiority of our method on our newly established benchmarks for this task, including FSS$^\\text{per}$, CUB$^\\text{per}$, and ADE$^\\text{per}$.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 26,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11035",
        "abs_url": "https://arxiv.org/abs/2507.11035",
        "pdf_url": "https://arxiv.org/pdf/2507.11035",
        "title": "Efficient Dual-domain Image Dehazing with Haze Prior Perception",
        "authors": [
            "Lirong Zheng",
            "Yanshan Li",
            "Rui Yu",
            "Kaihao Zhang"
        ],
        "comments": "12 pages",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Transformer-based models exhibit strong global modeling capabilities in single-image dehazing, but their high computational cost limits real-time applicability. Existing methods predominantly rely on spatial-domain features to capture long-range dependencies, which are computationally expensive and often inadequate under complex haze conditions. While some approaches introduce frequency-domain cues, the weak coupling between spatial and frequency branches limits the overall performance. To overcome these limitations, we propose the Dark Channel Guided Frequency-aware Dehazing Network (DGFDNet), a novel dual-domain framework that performs physically guided degradation alignment across spatial and frequency domains. At its core, the DGFDBlock comprises two key modules: 1) the Haze-Aware Frequency Modulator (HAFM), which generates a pixel-level haze confidence map from dark channel priors to adaptively enhance haze-relevant frequency components, thereby achieving global degradation-aware spectral modulation; 2) the Multi-level Gating Aggregation Module (MGAM), which fuses multi-scale features through diverse convolutional kernels and hybrid gating mechanisms to recover fine structural details. Additionally, a Prior Correction Guidance Branch (PCGB) incorporates a closed-loop feedback mechanism, enabling iterative refinement of the prior by intermediate dehazed features and significantly improving haze localization accuracy, especially in challenging outdoor scenes. Extensive experiments on four benchmark haze datasets demonstrate that DGFDNet achieves state-of-the-art performance with superior robustness and real-time efficiency. Code is available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 27,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11037",
        "abs_url": "https://arxiv.org/abs/2507.11037",
        "pdf_url": "https://arxiv.org/pdf/2507.11037",
        "title": "A Multi-View High-Resolution Foot-Ankle Complex Point Cloud Dataset During Gait for Occlusion-Robust 3D Completion",
        "authors": [
            "Jie-Wen Li",
            "Zi-Han Ye",
            "Qingyuan Zhou",
            "Jiayi Song",
            "Ying He",
            "Ben Fei",
            "Wen-Ming Chen"
        ],
        "comments": "15 pages, 10 figures, 2 tables",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The kinematics analysis of foot-ankle complex during gait is essential for advancing biomechanical research and clinical assessment. Collecting accurate surface geometry data from the foot and ankle during dynamic gait conditions is inherently challenging due to swing foot occlusions and viewing limitations. Thus, this paper introduces FootGait3D, a novel multi-view dataset of high-resolution ankle-foot surface point clouds captured during natural gait. Different from existing gait datasets that typically target whole-body or lower-limb motion, FootGait3D focuses specifically on the detailed modeling of the ankle-foot region, offering a finer granularity of motion data. To address this, FootGait3D consists of 8,403 point cloud frames collected from 46 subjects using a custom five-camera depth sensing system. Each frame includes a complete 5-view reconstruction of the foot and ankle (serving as ground truth) along with partial point clouds obtained from only four, three, or two views. This structured variation enables rigorous evaluation of 3D point cloud completion methods under varying occlusion levels and viewpoints. Our dataset is designed for shape completion tasks, facilitating the benchmarking of state-of-the-art single-modal (e.g., PointTr, SnowflakeNet, Anchorformer) and multi-modal (e.g., SVDFormer, PointSea, CSDN) completion networks on the challenge of recovering the full foot geometry from occluded inputs. FootGait3D has significant potential to advance research in biomechanics and multi-segment foot modeling, offering a valuable testbed for clinical gait analysis, prosthetic design, and robotics applications requiring detailed 3D models of the foot during motion. The dataset is now available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 28,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11040",
        "abs_url": "https://arxiv.org/abs/2507.11040",
        "pdf_url": "https://arxiv.org/pdf/2507.11040",
        "title": "Combining Transformers and CNNs for Efficient Object Detection in High-Resolution Satellite Imagery",
        "authors": [
            "Nicolas Drapier",
            "Aladine Chetouani",
            "Aurélien Chateigner"
        ],
        "comments": "11 pages, 9 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We present GLOD, a transformer-first architecture for object detection in high-resolution satellite imagery. GLOD replaces CNN backbones with a Swin Transformer for end-to-end feature extraction, combined with novel UpConvMixer blocks for robust upsampling and Fusion Blocks for multi-scale feature integration. Our approach achieves 32.95\\% on xView, outperforming SOTA methods by 11.46\\%. Key innovations include asymmetric fusion with CBAM attention and a multi-path head design capturing objects across scales. The architecture is optimized for satellite imagery challenges, leveraging spatial priors while maintaining computational efficiency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 29,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11055",
        "abs_url": "https://arxiv.org/abs/2507.11055",
        "pdf_url": "https://arxiv.org/pdf/2507.11055",
        "title": "Alleviating Textual Reliance in Medical Language-guided Segmentation via Prototype-driven Semantic Approximation",
        "authors": [
            "Shuchang Ye",
            "Usman Naseem",
            "Mingyuan Meng",
            "Jinman Kim"
        ],
        "comments": "Accepted to ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Medical language-guided segmentation, integrating textual clinical reports as auxiliary guidance to enhance image segmentation, has demonstrated significant improvements over unimodal approaches. However, its inherent reliance on paired image-text input, which we refer to as ``textual reliance\", presents two fundamental limitations: 1) many medical segmentation datasets lack paired reports, leaving a substantial portion of image-only data underutilized for training; and 2) inference is limited to retrospective analysis of cases with paired reports, limiting its applicability in most clinical scenarios where segmentation typically precedes reporting. To address these limitations, we propose ProLearn, the first Prototype-driven Learning framework for language-guided segmentation that fundamentally alleviates textual reliance. At its core, we introduce a novel Prototype-driven Semantic Approximation (PSA) module to enable approximation of semantic guidance from textual input. PSA initializes a discrete and compact prototype space by distilling segmentation-relevant semantics from textual reports. Once initialized, it supports a query-and-respond mechanism which approximates semantic guidance for images without textual input, thereby alleviating textual reliance. Extensive experiments on QaTa-COV19, MosMedData+ and Kvasir-SEG demonstrate that ProLearn outperforms state-of-the-art language-guided methods when limited text is available.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 30,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11061",
        "abs_url": "https://arxiv.org/abs/2507.11061",
        "pdf_url": "https://arxiv.org/pdf/2507.11061",
        "title": "Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with Regularized Score Distillation Sampling",
        "authors": [
            "Hayeon Kim",
            "Ji Ha Jang",
            "Se Young Chun"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Recent advances in 3D neural representations and instance-level editing models have enabled the efficient creation of high-quality 3D content. However, achieving precise local 3D edits remains challenging, especially for Gaussian Splatting, due to inconsistent multi-view 2D part segmentations and inherently ambiguous nature of Score Distillation Sampling (SDS) loss. To address these limitations, we propose RoMaP, a novel local 3D Gaussian editing framework that enables precise and drastic part-level modifications. First, we introduce a robust 3D mask generation module with our 3D-Geometry Aware Label Prediction (3D-GALP), which uses spherical harmonics (SH) coefficients to model view-dependent label variations and soft-label property, yielding accurate and consistent part segmentations across viewpoints. Second, we propose a regularized SDS loss that combines the standard SDS loss with additional regularizers. In particular, an L1 anchor loss is introduced via our Scheduled Latent Mixing and Part (SLaMP) editing method, which generates high-quality part-edited 2D images and confines modifications only to the target region while preserving contextual coherence. Additional regularizers, such as Gaussian prior removal, further improve flexibility by allowing changes beyond the existing context, and robust 3D masking prevents unintended edits. Experimental results demonstrate that our RoMaP achieves state-of-the-art local 3D editing on both reconstructed and generated Gaussian scenes and objects qualitatively and quantitatively, making it possible for more robust and flexible part-level 3D Gaussian editing.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 31,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11075",
        "abs_url": "https://arxiv.org/abs/2507.11075",
        "pdf_url": "https://arxiv.org/pdf/2507.11075",
        "title": "Joint angle model based learning to refine kinematic human pose estimation",
        "authors": [
            "Chang Peng",
            "Yifei Zhou",
            "Huifeng Xi",
            "Shiqing Huang",
            "Chuangye Chen",
            "Jianming Yang",
            "Bao Yang",
            "Zhenyu Jiang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Marker-free human pose estimation (HPE) has found increasing applications in various fields. Current HPE suffers from occasional errors in keypoint recognition and random fluctuation in keypoint trajectories when analyzing kinematic human poses. The performance of existing deep learning-based models for HPE refinement is considerably limited by inaccurate training datasets in which the keypoints are manually annotated. This paper proposed a novel method to overcome the difficulty through joint angle-based modeling. The key techniques include: (i) A joint angle-based model of human pose, which is robust to describe kinematic human poses; (ii) Approximating temporal variation of joint angles through high order Fourier series to get reliable \"ground truth\"; (iii) A bidirectional recurrent network is designed as a post-processing module to refine the estimation of well-established HRNet. Trained with the high-quality dataset constructed using our method, the network demonstrates outstanding performance to correct wrongly recognized joints and smooth their spatiotemporal trajectories. Tests show that joint angle-based refinement (JAR) outperforms the state-of-the-art HPE refinement network in challenging cases like figure skating and breaking.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 32,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11077",
        "abs_url": "https://arxiv.org/abs/2507.11077",
        "pdf_url": "https://arxiv.org/pdf/2507.11077",
        "title": "GKNet: Graph-based Keypoints Network for Monocular Pose Estimation of Non-cooperative Spacecraft",
        "authors": [
            "Weizhao Ma",
            "Dong Zhou",
            "Yuhui Hu",
            "Zipeng He"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Monocular pose estimation of non-cooperative spacecraft is significant for on-orbit service (OOS) tasks, such as satellite maintenance, space debris removal, and station assembly. Considering the high demands on pose estimation accuracy, mainstream monocular pose estimation methods typically consist of keypoint detectors and PnP solver. However, current keypoint detectors remain vulnerable to structural symmetry and partial occlusion of non-cooperative spacecraft. To this end, we propose a graph-based keypoints network for the monocular pose estimation of non-cooperative spacecraft, GKNet, which leverages the geometric constraint of keypoints graph. In order to better validate keypoint detectors, we present a moderate-scale dataset for the spacecraft keypoint detection, named SKD, which consists of 3 spacecraft targets, 90,000 simulated images, and corresponding high-precise keypoint annotations. Extensive experiments and an ablation study have demonstrated the high accuracy and effectiveness of our GKNet, compared to the state-of-the-art spacecraft keypoint detectors. The code for GKNet and the SKD dataset is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 33,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11081",
        "abs_url": "https://arxiv.org/abs/2507.11081",
        "pdf_url": "https://arxiv.org/pdf/2507.11081",
        "title": "Automatic Road Subsurface Distress Recognition from Ground Penetrating Radar Images using Deep Learning-based Cross-verification",
        "authors": [
            "Chang Peng",
            "Bao Yang",
            "Meiqi Li",
            "Ge Zhang",
            "Hui Sun",
            "Zhenyu Jiang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Ground penetrating radar (GPR) has become a rapid and non-destructive solution for road subsurface distress (RSD) detection. However, RSD recognition from GPR images is labor-intensive and heavily relies on inspectors' expertise. Deep learning offers the possibility for automatic RSD recognition, but its current performance is limited by two factors: Scarcity of high-quality dataset for network training and insufficient capability of network to distinguish RSD. In this study, a rigorously validated 3D GPR dataset containing 2134 samples of diverse types was constructed through field scanning. Based on the finding that the YOLO model trained with one of the three scans of GPR images exhibits varying sensitivity to specific type of RSD, we proposed a novel cross-verification strategy with outstanding accuracy in RSD recognition, achieving recall over 98.6% in field tests. The approach, integrated into an online RSD detection system, can reduce the labor of inspection by around 90%.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 34,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11085",
        "abs_url": "https://arxiv.org/abs/2507.11085",
        "pdf_url": "https://arxiv.org/pdf/2507.11085",
        "title": "Atmos-Bench: 3D Atmospheric Structures for Climate Insight",
        "authors": [
            "Tianchi Xu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)",
        "abstract": "Atmospheric structure, represented by backscatter coefficients (BC) recovered from satellite LiDAR attenuated backscatter (ATB), provides a volumetric view of clouds, aerosols, and molecules, playing a critical role in human activities, climate understanding, and extreme weather forecasting. Existing methods often rely on auxiliary inputs and simplified physics-based approximations, and lack a standardized 3D benchmark for fair evaluation. However, such approaches may introduce additional uncertainties and insufficiently capture realistic radiative transfer and atmospheric scattering-absorption effects. To bridge these gaps, we present Atmos-Bench: the first 3D atmospheric benchmark, along with a novel FourCastX: Frequency-enhanced Spatio-Temporal Mixture-of-Experts Network that (a) generates 921,600 image slices from 3D scattering volumes simulated at 532 nm and 355 nm by coupling WRF with an enhanced COSP simulator over 384 land-ocean time steps, yielding high-quality voxel-wise references; (b) embeds ATB-BC physical constraints into the model architecture, promoting energy consistency during restoration; (c) achieves consistent improvements on the Atmos-Bench dataset across both 355 nm and 532 nm bands, outperforming state-of-the-art baseline models without relying on auxiliary inputs. Atmos-Bench establishes a new standard for satellite-based 3D atmospheric structure recovery and paves the way for deeper climate insight.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 35,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11099",
        "abs_url": "https://arxiv.org/abs/2507.11099",
        "pdf_url": "https://arxiv.org/pdf/2507.11099",
        "title": "A Survey on Interpretability in Visual Recognition",
        "authors": [
            "Qiyang Wan",
            "Chengzhi Gao",
            "Ruiping Wang",
            "Xilin Chen"
        ],
        "comments": "20 pages, 7 figures, 2 tables. Under review",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In recent years, visual recognition methods have advanced significantly, finding applications across diverse fields. While researchers seek to understand the mechanisms behind the success of these models, there is also a growing impetus to deploy them in critical areas like autonomous driving and medical diagnostics to better diagnose failures, which promotes the development of interpretability research. This paper systematically reviews existing research on the interpretability of visual recognition models and proposes a taxonomy of methods from a human-centered perspective. The proposed taxonomy categorizes interpretable recognition methods based on Intent, Object, Presentation, and Methodology, thereby establishing a systematic and coherent set of grouping criteria for these XAI methods. Additionally, we summarize the requirements for evaluation metrics and explore new opportunities enabled by recent technologies, such as large multimodal models. We aim to organize existing research in this domain and inspire future investigations into the interpretability of visual recognition models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 36,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11102",
        "abs_url": "https://arxiv.org/abs/2507.11102",
        "pdf_url": "https://arxiv.org/pdf/2507.11102",
        "title": "KptLLM++: Towards Generic Keypoint Comprehension with Large Language Model",
        "authors": [
            "Jie Yang",
            "Wang Zeng",
            "Sheng Jin",
            "Lumin Xu",
            "Wentao Liu",
            "Chen Qian",
            "Zhen Li",
            "Ruimao Zhang"
        ],
        "comments": "Extended Version of KptLLM. arXiv admin note: text overlap with arXiv:2411.01846",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "The emergence of Multimodal Large Language Models (MLLMs) has revolutionized image understanding by bridging textual and visual modalities. However, these models often struggle with capturing fine-grained semantic information, such as the precise identification and analysis of object keypoints. Keypoints, as structure-aware, pixel-level, and compact representations of objects, particularly articulated ones, play a crucial role in applications such as fine-grained image analysis, object retrieval, and behavior recognition. In this paper, we propose KptLLM++, a novel multimodal large language model that specifically designed for generic keypoint comprehension through the integration of diverse input modalities guided by user-defined instructions. By unifying keypoint detection across varied contexts, KptLLM++ establishes itself as an advanced interface, fostering more effective human-AI collaboration. The model is built upon a novel identify-then-detect paradigm, which first interprets keypoint semantics and subsequently localizes their precise positions through a structured chain-of-thought reasoning mechanism. To push the boundaries of performance, we have scaled up the training dataset to over 500K samples, encompassing diverse objects, keypoint categories, image styles, and scenarios with complex occlusions. This extensive scaling enables KptLLM++ to unlock its potential, achieving remarkable accuracy and generalization. Comprehensive experiments on multiple keypoint detection benchmarks demonstrate its state-of-the-art performance, underscoring its potential as a unified solution for fine-grained image understanding and its transformative implications for human-AI interaction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 37,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11116",
        "abs_url": "https://arxiv.org/abs/2507.11116",
        "pdf_url": "https://arxiv.org/pdf/2507.11116",
        "title": "Jellyfish Species Identification: A CNN Based Artificial Neural Network Approach",
        "authors": [
            "Md. Sabbir Hossen",
            "Md. Saiduzzaman",
            "Pabon Shaha",
            "Mostofa Kamal Nasir"
        ],
        "comments": "This paper has been accepted at the IEEE QPAIN 2025. The final version will be available in the IEEE Xplore Digital Library",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Jellyfish, a diverse group of gelatinous marine organisms, play a crucial role in maintaining marine ecosystems but pose significant challenges for biodiversity and conservation due to their rapid proliferation and ecological impact. Accurate identification of jellyfish species is essential for ecological monitoring and management. In this study, we proposed a deep learning framework for jellyfish species detection and classification using an underwater image dataset. The framework integrates advanced feature extraction techniques, including MobileNetV3, ResNet50, EfficientNetV2-B0, and VGG16, combined with seven traditional machine learning classifiers and three Feedforward Neural Network classifiers for precise species identification. Additionally, we activated the softmax function to directly classify jellyfish species using the convolutional neural network models. The combination of the Artificial Neural Network with MobileNetV3 is our best-performing model, achieving an exceptional accuracy of 98%, significantly outperforming other feature extractor-classifier combinations. This study demonstrates the efficacy of deep learning and hybrid frameworks in addressing biodiversity challenges and advancing species detection in marine environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 38,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11119",
        "abs_url": "https://arxiv.org/abs/2507.11119",
        "pdf_url": "https://arxiv.org/pdf/2507.11119",
        "title": "Try Harder: Hard Sample Generation and Learning for Clothes-Changing Person Re-ID",
        "authors": [
            "Hankun Liu",
            "Yujian Zhao",
            "Guanglin Niu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Hard samples pose a significant challenge in person re-identification (ReID) tasks, particularly in clothing-changing person Re-ID (CC-ReID). Their inherent ambiguity or similarity, coupled with the lack of explicit definitions, makes them a fundamental bottleneck. These issues not only limit the design of targeted learning strategies but also diminish the model's robustness under clothing or viewpoint changes. In this paper, we propose a novel multimodal-guided Hard Sample Generation and Learning (HSGL) framework, which is the first effort to unify textual and visual modalities to explicitly define, generate, and optimize hard samples within a unified paradigm. HSGL comprises two core components: (1) Dual-Granularity Hard Sample Generation (DGHSG), which leverages multimodal cues to synthesize semantically consistent samples, including both coarse- and fine-grained hard positives and negatives for effectively increasing the hardness and diversity of the training data. (2) Hard Sample Adaptive Learning (HSAL), which introduces a hardness-aware optimization strategy that adjusts feature distances based on textual semantic labels, encouraging the separation of hard positives and drawing hard negatives closer in the embedding space to enhance the model's discriminative capability and robustness to hard samples. Extensive experiments on multiple CC-ReID benchmarks demonstrate the effectiveness of our approach and highlight the potential of multimodal-guided hard sample generation and learning for robust CC-ReID. Notably, HSAL significantly accelerates the convergence of the targeted learning procedure and achieves state-of-the-art performance on both PRCC and LTCC datasets. The code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 39,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11129",
        "abs_url": "https://arxiv.org/abs/2507.11129",
        "pdf_url": "https://arxiv.org/pdf/2507.11129",
        "title": "MMOne: Representing Multiple Modalities in One Scene",
        "authors": [
            "Zhifeng Gu",
            "Bing Wang"
        ],
        "comments": "Accepted to ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Humans perceive the world through multimodal cues to understand and interact with the environment. Learning a scene representation for multiple modalities enhances comprehension of the physical world. However, modality conflicts, arising from inherent distinctions among different modalities, present two critical challenges: property disparity and granularity disparity. To address these challenges, we propose a general framework, MMOne, to represent multiple modalities in one scene, which can be readily extended to additional modalities. Specifically, a modality modeling module with a novel modality indicator is proposed to capture the unique properties of each modality. Additionally, we design a multimodal decomposition mechanism to separate multi-modal Gaussians into single-modal Gaussians based on modality differences. We address the essential distinctions among modalities by disentangling multimodal information into shared and modality-specific components, resulting in a more compact and efficient multimodal scene representation. Extensive experiments demonstrate that our method consistently enhances the representation capability for each modality and is scalable to additional modalities. The code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 40,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11143",
        "abs_url": "https://arxiv.org/abs/2507.11143",
        "pdf_url": "https://arxiv.org/pdf/2507.11143",
        "title": "RMAU-NET: A Residual-Multihead-Attention U-Net Architecture for Landslide Segmentation and Detection from Remote Sensing Images",
        "authors": [
            "Lam Pham",
            "Cam Le",
            "Hieu Tang",
            "Khang Truong",
            "Truong Nguyen",
            "Jasmin Lampert",
            "Alexander Schindler",
            "Martin Boyer",
            "Son Phan"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Image and Video Processing (eess.IV)",
        "abstract": "In recent years, landslide disasters have reported frequently due to the extreme weather events of droughts, floods , storms, or the consequence of human activities such as deforestation, excessive exploitation of natural resources. However, automatically observing landslide is challenging due to the extremely large observing area and the rugged topography such as mountain or highland. This motivates us to propose an end-to-end deep-learning-based model which explores the remote sensing images for automatically observing landslide events. By considering remote sensing images as the input data, we can obtain free resource, observe large and rough terrains by time. To explore the remote sensing images, we proposed a novel neural network architecture which is for two tasks of landslide detection and landslide segmentation. We evaluated our proposed model on three different benchmark datasets of LandSlide4Sense, Bijie, and Nepal. By conducting extensive experiments, we achieve F1 scores of 98.23, 93.83 for the landslide detection task on LandSlide4Sense, Bijie datasets; mIoU scores of 63.74, 76.88 on the segmentation tasks regarding LandSlide4Sense, Nepal datasets. These experimental results prove potential to integrate our proposed model into real-life landslide observation systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 41,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11153",
        "abs_url": "https://arxiv.org/abs/2507.11153",
        "pdf_url": "https://arxiv.org/pdf/2507.11153",
        "title": "Assessing Color Vision Test in Large Vision-language Models",
        "authors": [
            "Hongfei Ye",
            "Bin Chen",
            "Wenxi Liu",
            "Yu Zhang",
            "Zhao Li",
            "Dandan Ni",
            "Hongyang Chen"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "With the widespread adoption of large vision-language models, the capacity for color vision in these models is crucial. However, the color vision abilities of large visual-language models have not yet been thoroughly explored. To address this gap, we define a color vision testing task for large vision-language models and construct a dataset \\footnote{Anonymous Github Showing some of the data this https URL} that covers multiple categories of test questions and tasks of varying difficulty levels. Furthermore, we analyze the types of errors made by large vision-language models and propose fine-tuning strategies to enhance their performance in color vision tests.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 42,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11171",
        "abs_url": "https://arxiv.org/abs/2507.11171",
        "pdf_url": "https://arxiv.org/pdf/2507.11171",
        "title": "Clustering-Guided Multi-Layer Contrastive Representation Learning for Citrus Disease Classification",
        "authors": [
            "Jun Chen",
            "Yonghua Yu",
            "Weifu Li",
            "Yaohui Chen",
            "Hong Chen"
        ],
        "comments": "11 pages, 5 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Citrus, as one of the most economically important fruit crops globally, suffers severe yield depressions due to various diseases. Accurate disease detection and classification serve as critical prerequisites for implementing targeted control measures. Recent advancements in artificial intelligence, particularly deep learning-based computer vision algorithms, have substantially decreased time and labor requirements while maintaining the accuracy of detection and classification. Nevertheless, these methods predominantly rely on massive, high-quality annotated training examples to attain promising performance. By introducing two key designs: contrasting with cluster centroids and a multi-layer contrastive training (MCT) paradigm, this paper proposes a novel clustering-guided self-supervised multi-layer contrastive representation learning (CMCRL) algorithm. The proposed method demonstrates several advantages over existing counterparts: (1) optimizing with massive unannotated samples; (2) effective adaptation to the symptom similarity across distinct citrus diseases; (3) hierarchical feature representation learning. The proposed method achieves state-of-the-art performance on the public citrus image set CDD, outperforming existing methods by 4.5\\%-30.1\\% accuracy. Remarkably, our method narrows the performance gap with fully supervised counterparts (all samples are labeled). Beyond classification accuracy, our method shows great performance on other evaluation metrics (F1 score, precision, and recall), highlighting the robustness against the class imbalance challenge.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 43,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11200",
        "abs_url": "https://arxiv.org/abs/2507.11200",
        "pdf_url": "https://arxiv.org/pdf/2507.11200",
        "title": "How Far Have Medical Vision-Language Models Come? A Comprehensive Benchmarking Study",
        "authors": [
            "Che Liu",
            "Jiazhen Pan",
            "Weixiang Shen",
            "Wenjia Bai",
            "Daniel Rueckert",
            "Rossella Arcucci"
        ],
        "comments": "Accepted by the International Conference on AI in Healthcare 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Vision-Language Models (VLMs) trained on web-scale corpora excel at natural image tasks and are increasingly repurposed for healthcare; however, their competence in medical tasks remains underexplored. We present a comprehensive evaluation of open-source general-purpose and medically specialised VLMs, ranging from 3B to 72B parameters, across eight benchmarks: MedXpert, OmniMedVQA, PMC-VQA, PathVQA, MMMU, SLAKE, and VQA-RAD. To observe model performance across different aspects, we first separate it into understanding and reasoning components. Three salient findings emerge. First, large general-purpose models already match or surpass medical-specific counterparts on several benchmarks, demonstrating strong zero-shot transfer from natural to medical images. Second, reasoning performance is consistently lower than understanding, highlighting a critical barrier to safe decision support. Third, performance varies widely across benchmarks, reflecting differences in task design, annotation quality, and knowledge demands. No model yet reaches the reliability threshold for clinical deployment, underscoring the need for stronger multimodal alignment and more rigorous, fine-grained evaluation protocols.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 44,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11202",
        "abs_url": "https://arxiv.org/abs/2507.11202",
        "pdf_url": "https://arxiv.org/pdf/2507.11202",
        "title": "A Robust Incomplete Multimodal Low-Rank Adaptation Approach for Emotion Recognition",
        "authors": [
            "Xinkui Zhao",
            "Jinsong Shu",
            "Yangyang Wu",
            "Guanjie Cheng",
            "Zihe Liu",
            "Naibo Wang",
            "Shuiguang Deng",
            "Zhongle Xie",
            "Jianwei Yin"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Multimodal Emotion Recognition (MER) often encounters incomplete multimodality in practical applications due to sensor failures or privacy protection requirements. While existing methods attempt to address various incomplete multimodal scenarios by balancing the training of each modality combination through additional gradients, these approaches face a critical limitation: training gradients from different modality combinations conflict with each other, ultimately degrading the performance of the final prediction model. In this paper, we propose a unimodal decoupled dynamic low-rank adaptation method based on modality combinations, named MCULoRA, which is a novel framework for the parameter-efficient training of incomplete multimodal learning models. MCULoRA consists of two key modules, modality combination aware low-rank adaptation (MCLA) and dynamic parameter fine-tuning (DPFT). The MCLA module effectively decouples the shared information from the distinct characteristics of individual modality combinations. The DPFT module adjusts the training ratio of modality combinations based on the separability of each modality's representation space, optimizing the learning efficiency across different modality combinations. Our extensive experimental evaluation in multiple benchmark datasets demonstrates that MCULoRA substantially outperforms previous incomplete multimodal learning approaches in downstream task accuracy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 45,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11245",
        "abs_url": "https://arxiv.org/abs/2507.11245",
        "pdf_url": "https://arxiv.org/pdf/2507.11245",
        "title": "NarrLV: Towards a Comprehensive Narrative-Centric Evaluation for Long Video Generation Models",
        "authors": [
            "X. Feng",
            "H. Yu",
            "M. Wu",
            "S. Hu",
            "J. Chen",
            "C. Zhu",
            "J. Wu",
            "X. Chu",
            "K. Huang"
        ],
        "comments": "Project Page: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "With the rapid development of foundation video generation technologies, long video generation models have exhibited promising research potential thanks to expanded content creation space. Recent studies reveal that the goal of long video generation tasks is not only to extend video duration but also to accurately express richer narrative content within longer videos. However, due to the lack of evaluation benchmarks specifically designed for long video generation models, the current assessment of these models primarily relies on benchmarks with simple narrative prompts (e.g., VBench). To the best of our knowledge, our proposed NarrLV is the first benchmark to comprehensively evaluate the Narrative expression capabilities of Long Video generation models. Inspired by film narrative theory, (i) we first introduce the basic narrative unit maintaining continuous visual presentation in videos as Temporal Narrative Atom (TNA), and use its count to quantitatively measure narrative richness. Guided by three key film narrative elements influencing TNA changes, we construct an automatic prompt generation pipeline capable of producing evaluation prompts with a flexibly expandable number of TNAs. (ii) Then, based on the three progressive levels of narrative content expression, we design an effective evaluation metric using the MLLM-based question generation and answering framework. (iii) Finally, we conduct extensive evaluations on existing long video generation models and the foundation generation models. Experimental results demonstrate that our metric aligns closely with human judgments. The derived evaluation outcomes reveal the detailed capability boundaries of current video generation models in narrative content expression.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 46,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11247",
        "abs_url": "https://arxiv.org/abs/2507.11247",
        "pdf_url": "https://arxiv.org/pdf/2507.11247",
        "title": "Fairness-Aware Grouping for Continuous Sensitive Variables: Application for Debiasing Face Analysis with respect to Skin Tone",
        "authors": [
            "Veronika Shilova",
            "Emmanuel Malherbe",
            "Giovanni Palma",
            "Laurent Risser",
            "Jean-Michel Loubes"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Within a legal framework, fairness in datasets and models is typically assessed by dividing observations into predefined groups and then computing fairness measures (e.g., Disparate Impact or Equality of Odds with respect to gender). However, when sensitive attributes such as skin color are continuous, dividing into default groups may overlook or obscure the discrimination experienced by certain minority subpopulations. To address this limitation, we propose a fairness-based grouping approach for continuous (possibly multidimensional) sensitive attributes. By grouping data according to observed levels of discrimination, our method identifies the partition that maximizes a novel criterion based on inter-group variance in discrimination, thereby isolating the most critical subgroups. We validate the proposed approach using multiple synthetic datasets and demonstrate its robustness under changing population distributions - revealing how discrimination is manifested within the space of sensitive attributes. Furthermore, we examine a specialized setting of monotonic fairness for the case of skin color. Our empirical results on both CelebA and FFHQ, leveraging the skin tone as predicted by an industrial proprietary algorithm, show that the proposed segmentation uncovers more nuanced patterns of discrimination than previously reported, and that these findings remain stable across datasets for a given model. Finally, we leverage our grouping model for debiasing purpose, aiming at predicting fair scores with group-by-group post-processing. The results demonstrate that our approach improves fairness while having minimal impact on accuracy, thus confirming our partition method and opening the door for industrial deployment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 47,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11252",
        "abs_url": "https://arxiv.org/abs/2507.11252",
        "pdf_url": "https://arxiv.org/pdf/2507.11252",
        "title": "MFGDiffusion: Mask-Guided Smoke Synthesis for Enhanced Forest Fire Detection",
        "authors": [
            "Guanghao Wu",
            "Chen Xu",
            "Hai Song",
            "Chong Wang",
            "Qixing Zhang"
        ],
        "comments": "18 pages, 11 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)",
        "abstract": "Smoke is the first visible indicator of a this http URL the advancement of deep learning, image-based smoke detection has become a crucial method for detecting and preventing forest fires. However, the scarcity of smoke image data from forest fires is one of the significant factors hindering the detection of forest fire smoke. Image generation models offer a promising solution for synthesizing realistic smoke images. However, current inpainting models exhibit limitations in generating high-quality smoke representations, particularly manifesting as inconsistencies between synthesized smoke and background contexts. To solve these problems, we proposed a comprehensive framework for generating forest fire smoke images. Firstly, we employed the pre-trained segmentation model and the multimodal model to obtain smoke masks and image this http URL, to address the insufficient utilization of masks and masked images by inpainting models, we introduced a network architecture guided by mask and masked image features. We also proposed a new loss function, the mask random difference loss, which enhances the consistency of the generated effects around the mask by randomly expanding and eroding the mask this http URL, to generate a smoke image dataset using random masks for subsequent detection tasks, we incorporated smoke characteristics and use a multimodal large language model as a filtering tool to select diverse and reasonable smoke images, thereby improving the quality of the synthetic dataset. Experiments showed that our generated smoke images are realistic and diverse, and effectively enhance the performance of forest fire smoke detection models. Code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 48,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11261",
        "abs_url": "https://arxiv.org/abs/2507.11261",
        "pdf_url": "https://arxiv.org/pdf/2507.11261",
        "title": "ViewSRD: 3D Visual Grounding via Structured Multi-View Decomposition",
        "authors": [
            "Ronggang Huang",
            "Haoxin Yang",
            "Yan Cai",
            "Xuemiao Xu",
            "Huaidong Zhang",
            "Shengfeng He"
        ],
        "comments": "Accepted by ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "3D visual grounding aims to identify and localize objects in a 3D space based on textual descriptions. However, existing methods struggle with disentangling targets from anchors in complex multi-anchor queries and resolving inconsistencies in spatial descriptions caused by perspective variations. To tackle these challenges, we propose ViewSRD, a framework that formulates 3D visual grounding as a structured multi-view decomposition process. First, the Simple Relation Decoupling (SRD) module restructures complex multi-anchor queries into a set of targeted single-anchor statements, generating a structured set of perspective-aware descriptions that clarify positional relationships. These decomposed representations serve as the foundation for the Multi-view Textual-Scene Interaction (Multi-TSI) module, which integrates textual and scene features across multiple viewpoints using shared, Cross-modal Consistent View Tokens (CCVTs) to preserve spatial correlations. Finally, a Textual-Scene Reasoning module synthesizes multi-view predictions into a unified and robust 3D visual grounding. Experiments on 3D visual grounding datasets show that ViewSRD significantly outperforms state-of-the-art methods, particularly in complex queries requiring precise spatial differentiation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 49,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11267",
        "abs_url": "https://arxiv.org/abs/2507.11267",
        "pdf_url": "https://arxiv.org/pdf/2507.11267",
        "title": "YOLOatr : Deep Learning Based Automatic Target Detection and Localization in Thermal Infrared Imagery",
        "authors": [
            "Aon Safdar",
            "Usman Akram",
            "Waseem Anwar",
            "Basit Malik",
            "Mian Ibad Ali"
        ],
        "comments": "Published in 25th Irish Machine Vision and Image Processing Conf., Galway, Ireland, Aug 30-Sep 1 2023 Also available at this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Automatic Target Detection (ATD) and Recognition (ATR) from Thermal Infrared (TI) imagery in the defense and surveillance domain is a challenging computer vision (CV) task in comparison to the commercial autonomous vehicle perception domain. Limited datasets, peculiar domain-specific and TI modality-specific challenges, i.e., limited hardware, scale invariance issues due to greater distances, deliberate occlusion by tactical vehicles, lower sensor resolution and resultant lack of structural information in targets, effects of weather, temperature, and time of day variations, and varying target to clutter ratios all result in increased intra-class variability and higher inter-class similarity, making accurate real-time ATR a challenging CV task. Resultantly, contemporary state-of-the-art (SOTA) deep learning architectures underperform in the ATR domain. We propose a modified anchor-based single-stage detector, called YOLOatr, based on a modified YOLOv5s, with optimal modifications to the detection heads, feature fusion in the neck, and a custom augmentation profile. We evaluate the performance of our proposed model on a comprehensive DSIAC MWIR dataset for real-time ATR over both correlated and decorrelated testing protocols. The results demonstrate that our proposed model achieves state-of-the-art ATR performance of up to 99.6%.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 50,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11279",
        "abs_url": "https://arxiv.org/abs/2507.11279",
        "pdf_url": "https://arxiv.org/pdf/2507.11279",
        "title": "Tomato Multi-Angle Multi-Pose Dataset for Fine-Grained Phenotyping",
        "authors": [
            "Yujie Zhang",
            "Sabine Struckmeyer",
            "Andreas Kolb",
            "Sven Reichardt"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Observer bias and inconsistencies in traditional plant phenotyping methods limit the accuracy and reproducibility of fine-grained plant analysis. To overcome these challenges, we developed TomatoMAP, a comprehensive dataset for Solanum lycopersicum using an Internet of Things (IoT) based imaging system with standardized data acquisition protocols. Our dataset contains 64,464 RGB images that capture 12 different plant poses from four camera elevation angles. Each image includes manually annotated bounding boxes for seven regions of interest (ROIs), including leaves, panicle, batch of flowers, batch of fruits, axillary shoot, shoot and whole plant area, along with 50 fine-grained growth stage classifications based on the BBCH scale. Additionally, we provide 3,616 high-resolution image subset with pixel-wise semantic and instance segmentation annotations for fine-grained phenotyping. We validated our dataset using a cascading model deep learning framework combining MobileNetv3 for classification, YOLOv11 for object detection, and MaskRCNN for segmentation. Through AI vs. Human analysis involving five domain experts, we demonstrate that the models trained on our dataset achieve accuracy and speed comparable to the experts. Cohen's Kappa and inter-rater agreement heatmap confirm the reliability of automated fine-grained phenotyping using our approach.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 51,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11287",
        "abs_url": "https://arxiv.org/abs/2507.11287",
        "pdf_url": "https://arxiv.org/pdf/2507.11287",
        "title": "Task-Oriented Human Grasp Synthesis via Context- and Task-Aware Diffusers",
        "authors": [
            "An-Lun Liu",
            "Yu-Wei Chao",
            "Yi-Ting Chen"
        ],
        "comments": "Accepted by ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)",
        "abstract": "In this paper, we study task-oriented human grasp synthesis, a new grasp synthesis task that demands both task and context awareness. At the core of our method is the task-aware contact maps. Unlike traditional contact maps that only reason about the manipulated object and its relation with the hand, our enhanced maps take into account scene and task information. This comprehensive map is critical for hand-object interaction, enabling accurate grasping poses that align with the task. We propose a two-stage pipeline that first constructs a task-aware contact map informed by the scene and task. In the subsequent stage, we use this contact map to synthesize task-oriented human grasps. We introduce a new dataset and a metric for the proposed task to evaluate our approach. Our experiments validate the importance of modeling both scene and task, demonstrating significant improvements over existing methods in both grasp quality and task performance. See our project page for more details: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 52,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11301",
        "abs_url": "https://arxiv.org/abs/2507.11301",
        "pdf_url": "https://arxiv.org/pdf/2507.11301",
        "title": "Detección y Cuantificación de Erosión Fluvial con Visión Artificial",
        "authors": [
            "Paúl Maji",
            "Marlon Túquerres",
            "Stalin Valencia",
            "Marcela Valenzuela",
            "Christian Mejia-Escobar"
        ],
        "comments": "18 pages, in Spanish language, 13 figures, 4 tables",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Fluvial erosion is a natural process that can generate significant impacts on soil stability and strategic infrastructures. The detection and monitoring of this phenomenon is traditionally addressed by photogrammetric methods and analysis in geographic information systems. These tasks require specific knowledge and intensive manual processing. This study proposes an artificial intelligence-based approach for automatic identification of eroded zones and estimation of their area. The state-of-the-art computer vision model YOLOv11, adjusted by fine-tuning and trained with photographs and LiDAR images, is used. This combined dataset was segmented and labeled using the Roboflow platform. Experimental results indicate efficient detection of erosion patterns with an accuracy of 70%, precise identification of eroded areas and reliable calculation of their extent in pixels and square meters. As a final product, the EROSCAN system has been developed, an interactive web application that allows users to upload images and obtain automatic segmentations of fluvial erosion, together with the estimated area. This tool optimizes the detection and quantification of the phenomenon, facilitating decision making in risk management and territorial planning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 53,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11321",
        "abs_url": "https://arxiv.org/abs/2507.11321",
        "pdf_url": "https://arxiv.org/pdf/2507.11321",
        "title": "A Mixed-Primitive-based Gaussian Splatting Method for Surface Reconstruction",
        "authors": [
            "Haoxuan Qu",
            "Yujun Cai",
            "Hossein Rahmani",
            "Ajay Kumar",
            "Junsong Yuan",
            "Jun Liu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Recently, Gaussian Splatting (GS) has received a lot of attention in surface reconstruction. However, while 3D objects can be of complex and diverse shapes in the real world, existing GS-based methods only limitedly use a single type of splatting primitive (Gaussian ellipse or Gaussian ellipsoid) to represent object surfaces during their reconstruction. In this paper, we highlight that this can be insufficient for object surfaces to be represented in high quality. Thus, we propose a novel framework that, for the first time, enables Gaussian Splatting to incorporate multiple types of (geometrical) primitives during its surface reconstruction process. Specifically, in our framework, we first propose a compositional splatting strategy, enabling the splatting and rendering of different types of primitives in the Gaussian Splatting pipeline. In addition, we also design our framework with a mixed-primitive-based initialization strategy and a vertex pruning mechanism to further promote its surface representation learning process to be well executed leveraging different types of primitives. Extensive experiments show the efficacy of our framework and its accurate surface reconstruction performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 54,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11333",
        "abs_url": "https://arxiv.org/abs/2507.11333",
        "pdf_url": "https://arxiv.org/pdf/2507.11333",
        "title": "MonoMVSNet: Monocular Priors Guided Multi-View Stereo Network",
        "authors": [
            "Jianfei Jiang",
            "Qiankun Liu",
            "Haochen Yu",
            "Hongyuan Liu",
            "Liyong Wang",
            "Jiansheng Chen",
            "Huimin Ma"
        ],
        "comments": "Accepted by ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Learning-based Multi-View Stereo (MVS) methods aim to predict depth maps for a sequence of calibrated images to recover dense point clouds. However, existing MVS methods often struggle with challenging regions, such as textureless regions and reflective surfaces, where feature matching fails. In contrast, monocular depth estimation inherently does not require feature matching, allowing it to achieve robust relative depth estimation in these regions. To bridge this gap, we propose MonoMVSNet, a novel monocular feature and depth guided MVS network that integrates powerful priors from a monocular foundation model into multi-view geometry. Firstly, the monocular feature of the reference view is integrated into source view features by the attention mechanism with a newly designed cross-view position encoding. Then, the monocular depth of the reference view is aligned to dynamically update the depth candidates for edge regions during the sampling procedure. Finally, a relative consistency loss is further designed based on the monocular depth to supervise the depth prediction. Extensive experiments demonstrate that MonoMVSNet achieves state-of-the-art performance on the DTU and Tanks-and-Temples datasets, ranking first on the Tanks-and-Temples Intermediate and Advanced benchmarks. The source code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 55,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11336",
        "abs_url": "https://arxiv.org/abs/2507.11336",
        "pdf_url": "https://arxiv.org/pdf/2507.11336",
        "title": "UGC-VideoCaptioner: An Omni UGC Video Detail Caption Model and New Benchmarks",
        "authors": [
            "Peiran Wu",
            "Yunze Liu",
            "Zhengdong Zhu",
            "Enmin Zhou",
            "Shawn Shen"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Real-world user-generated videos, especially on platforms like TikTok, often feature rich and intertwined audio visual content. However, existing video captioning benchmarks and models remain predominantly visual centric, overlooking the crucial role of audio in conveying scene dynamics, speaker intent, and narrative context. This lack of omni datasets and lightweight, capable models hampers progress in fine grained, multimodal video understanding. To address these challenges, we introduce UGC-VideoCap, a new benchmark and model framework specifically designed for detailed omnimodal captioning of short form user-generated videos. Unlike prior datasets, UGC-VideoCap emphasizes balanced integration of audio and visual modalities, featuring 1000 TikTok videos annotated through a structured three stage human-in-the-loop pipeline covering audio only, visual only, and joint audio visual semantics. The benchmark also includes 4000 carefully crafted QA pairs probing both unimodal and cross modal understanding. Alongside the dataset, we propose UGC-VideoCaptioner(3B), a 3B parameter captioning model distilled from Gemini 2.5 Flash. Using a novel two-stage training strategy supervised fine tuning followed by Group Relative Policy Optimization (GRPO), our approach enables efficient adaptation from limited data while maintaining competitive performance. Together, our benchmark and model offer a high-quality foundation and a data-efficient solution for advancing omnimodal video captioning in unconstrained real-world UGC settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 56,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11372",
        "abs_url": "https://arxiv.org/abs/2507.11372",
        "pdf_url": "https://arxiv.org/pdf/2507.11372",
        "title": "Attributes Shape the Embedding Space of Face Recognition Models",
        "authors": [
            "Pierrick Leroy",
            "Antonio Mastropietro",
            "Marco Nurisso",
            "Francesco Vaccarino"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Face Recognition (FR) tasks have made significant progress with the advent of Deep Neural Networks, particularly through margin-based triplet losses that embed facial images into high-dimensional feature spaces. During training, these contrastive losses focus exclusively on identity information as labels. However, we observe a multiscale geometric structure emerging in the embedding space, influenced by interpretable facial (e.g., hair color) and image attributes (e.g., contrast). We propose a geometric approach to describe the dependence or invariance of FR models to these attributes and introduce a physics-inspired alignment metric. We evaluate the proposed metric on controlled, simplified models and widely used FR models fine-tuned with synthetic data for targeted attribute augmentation. Our findings reveal that the models exhibit varying degrees of invariance across different attributes, providing insight into their strengths and weaknesses and enabling deeper interpretability. Code available here: this https URL}{this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 57,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11441",
        "abs_url": "https://arxiv.org/abs/2507.11441",
        "pdf_url": "https://arxiv.org/pdf/2507.11441",
        "title": "Implementing Adaptations for Vision AutoRegressive Model",
        "authors": [
            "Kaif Shaikh",
            "Antoni Kowalczuk",
            "Franziska Boenisch",
            "Adam Dziedzic"
        ],
        "comments": "Accepted at DIG-BUGS: Data in Generative Models Workshop @ ICML 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Vision AutoRegressive model (VAR) was recently introduced as an alternative to Diffusion Models (DMs) in image generation domain. In this work we focus on its adaptations, which aim to fine-tune pre-trained models to perform specific downstream tasks, like medical data generation. While for DMs there exist many techniques, adaptations for VAR remain underexplored. Similarly, differentially private (DP) adaptations-ones that aim to preserve privacy of the adaptation data-have been extensively studied for DMs, while VAR lacks such solutions. In our work, we implement and benchmark many strategies for VAR, and compare them to state-of-the-art DM adaptation strategies. We observe that VAR outperforms DMs for non-DP adaptations, however, the performance of DP suffers, which necessitates further research in private adaptations for VAR. Code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 58,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11443",
        "abs_url": "https://arxiv.org/abs/2507.11443",
        "pdf_url": "https://arxiv.org/pdf/2507.11443",
        "title": "COLI: A Hierarchical Efficient Compressor for Large Images",
        "authors": [
            "Haoran Wang",
            "Hanyu Pei",
            "Yang Lyu",
            "Kai Zhang",
            "Li Li",
            "Feng-Lei Fan"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "The escalating adoption of high-resolution, large-field-of-view imagery amplifies the need for efficient compression methodologies. Conventional techniques frequently fail to preserve critical image details, while data-driven approaches exhibit limited generalizability. Implicit Neural Representations (INRs) present a promising alternative by learning continuous mappings from spatial coordinates to pixel intensities for individual images, thereby storing network weights rather than raw pixels and avoiding the generalization problem. However, INR-based compression of large images faces challenges including slow compression speed and suboptimal compression ratios. To address these limitations, we introduce COLI (Compressor for Large Images), a novel framework leveraging Neural Representations for Videos (NeRV). First, recognizing that INR-based compression constitutes a training process, we accelerate its convergence through a pretraining-finetuning paradigm, mixed-precision training, and reformulation of the sequential loss into a parallelizable objective. Second, capitalizing on INRs' transformation of image storage constraints into weight storage, we implement Hyper-Compression, a novel post-training technique to substantially enhance compression ratios while maintaining minimal output distortion. Evaluations across two medical imaging datasets demonstrate that COLI consistently achieves competitive or superior PSNR and SSIM metrics at significantly reduced bits per pixel (bpp), while accelerating NeRV training by up to 4 times.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 59,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11474",
        "abs_url": "https://arxiv.org/abs/2507.11474",
        "pdf_url": "https://arxiv.org/pdf/2507.11474",
        "title": "HUG-VAS: A Hierarchical NURBS-Based Generative Model for Aortic Geometry Synthesis and Controllable Editing",
        "authors": [
            "Pan Du",
            "Mingqi Xu",
            "Xiaozhi Zhu",
            "Jian-xun Wang"
        ],
        "comments": "59 pages, 9 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Accurate characterization of vascular geometry is essential for cardiovascular diagnosis and treatment planning. Traditional statistical shape modeling (SSM) methods rely on linear assumptions, limiting their expressivity and scalability to complex topologies such as multi-branch vascular structures. We introduce HUG-VAS, a Hierarchical NURBS Generative model for Vascular geometry Synthesis, which integrates NURBS surface parameterization with diffusion-based generative modeling to synthesize realistic, fine-grained aortic geometries. Trained with 21 patient-specific samples, HUG-VAS generates anatomically faithful aortas with supra-aortic branches, yielding biomarker distributions that closely match those of the original dataset. HUG-VAS adopts a hierarchical architecture comprising a denoising diffusion model that generates centerlines and a guided diffusion model that synthesizes radial profiles conditioned on those centerlines, thereby capturing two layers of anatomical variability. Critically, the framework supports zero-shot conditional generation from image-derived priors, enabling practical applications such as interactive semi-automatic segmentation, robust reconstruction under degraded imaging conditions, and implantable device optimization. To our knowledge, HUG-VAS is the first SSM framework to bridge image-derived priors with generative shape modeling via a unified integration of NURBS parameterization and hierarchical diffusion processes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 60,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11476",
        "abs_url": "https://arxiv.org/abs/2507.11476",
        "pdf_url": "https://arxiv.org/pdf/2507.11476",
        "title": "3C-FBI: A Combinatorial method using Convolutions for Circle Fitting in Blurry Images",
        "authors": [
            "Esteban Román Catafau",
            "Torbjörn E.M. Nordling"
        ],
        "comments": "22 pages, 16 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "This paper addresses the fundamental computer vision challenge of robust circle detection and fitting in degraded imaging conditions. We present Combinatorial Convolution-based Circle Fitting for Blurry Images (3C-FBI), an algorithm that bridges the gap between circle detection and precise parametric fitting by combining (1) efficient combinatorial edge pixel (edgel) sampling and (2) convolution-based density estimation in parameter space. We evaluate 3C-FBI across three experimental frameworks: (1) real-world medical data from Parkinson's disease assessments (144 frames from 36 videos), (2) controlled synthetic data following established circle-fitting benchmarks, and (3) systematic analysis across varying spatial resolutions and outlier contamination levels. Results show that 3C-FBI achieves state-of-the-art accuracy (Jaccard index 0.896) while maintaining real-time performance (40.3 fps), significantly outperforming classical methods like RCD (6.8 fps) on a standard CPU (i7-10875H). It maintains near-perfect accuracy (Jaccard almost 1.0) at high resolutions (480x480) and reliable performance (Jaccard higher than 0.95) down to 160x160 with up to 20% outliers. In extensive synthetic testing, 3C-FBI achieves a mean Jaccard Index of 0.989 across contamination levels, comparable to modern methods like Qi et al. (2024, 0.991), and surpassing RHT (0.964). This combination of accuracy, speed, and robustness makes 3C-FBI ideal for medical imaging, robotics, and industrial inspection under challenging conditions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 61,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11488",
        "abs_url": "https://arxiv.org/abs/2507.11488",
        "pdf_url": "https://arxiv.org/pdf/2507.11488",
        "title": "COLIBRI Fuzzy Model: Color Linguistic-Based Representation and Interpretation",
        "authors": [
            "Pakizar Shamoi",
            "Nuray Toganas",
            "Muragul Muratbekova",
            "Elnara Kadyrgali",
            "Adilet Yerkin",
            "Ayan Igali",
            "Malika Ziyada",
            "Ayana Adilova",
            "Aron Karatayev",
            "Yerdauit Torekhan"
        ],
        "comments": "submitted to IEEE for consideration",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Colors are omnipresent in today's world and play a vital role in how humans perceive and interact with their surroundings. However, it is challenging for computers to imitate human color perception. This paper introduces the Human Perception-Based Fuzzy Color Model, COLIBRI (Color Linguistic-Based Representation and Interpretation), designed to bridge the gap between computational color representations and human visual perception. The proposed model uses fuzzy sets and logic to create a framework for color categorization. Using a three-phase experimental approach, the study first identifies distinguishable color stimuli for hue, saturation, and intensity through preliminary experiments, followed by a large-scale human categorization survey involving more than 1000 human subjects. The resulting data are used to extract fuzzy partitions and generate membership functions that reflect real-world perceptual uncertainty. The model incorporates a mechanism for adaptation that allows refinement based on feedback and contextual changes. Comparative evaluations demonstrate the model's alignment with human perception compared to traditional color models, such as RGB, HSV, and LAB. To the best of our knowledge, no previous research has documented the construction of a model for color attribute specification based on a sample of this size or a comparable sample of the human population (n = 2496). Our findings are significant for fields such as design, artificial intelligence, marketing, and human-computer interaction, where perceptually relevant color representation is critical.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 62,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11522",
        "abs_url": "https://arxiv.org/abs/2507.11522",
        "pdf_url": "https://arxiv.org/pdf/2507.11522",
        "title": "CATVis: Context-Aware Thought Visualization",
        "authors": [
            "Tariq Mehmood",
            "Hamza Ahmad",
            "Muhammad Haroon Shakeel",
            "Murtaza Taj"
        ],
        "comments": "Accepted at MICCAI 2025. This is the submitted version prior to peer review. The final Version of Record will appear in the MICCAI 2025 proceedings (Springer LNCS)",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "EEG-based brain-computer interfaces (BCIs) have shown promise in various applications, such as motor imagery and cognitive state monitoring. However, decoding visual representations from EEG signals remains a significant challenge due to their complex and noisy nature. We thus propose a novel 5-stage framework for decoding visual representations from EEG signals: (1) an EEG encoder for concept classification, (2) cross-modal alignment of EEG and text embeddings in CLIP feature space, (3) caption refinement via re-ranking, (4) weighted interpolation of concept and caption embeddings for richer semantics, and (5) image generation using a pre-trained Stable Diffusion model. We enable context-aware EEG-to-image generation through cross-modal alignment and re-ranking. Experimental results demonstrate that our method generates high-quality images aligned with visual stimuli, outperforming SOTA approaches by 13.43% in Classification Accuracy, 15.21% in Generation Accuracy and reducing Fréchet Inception Distance by 36.61%, indicating superior semantic alignment and image quality.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 63,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11533",
        "abs_url": "https://arxiv.org/abs/2507.11533",
        "pdf_url": "https://arxiv.org/pdf/2507.11533",
        "title": "CharaConsist: Fine-Grained Consistent Character Generation",
        "authors": [
            "Mengyu Wang",
            "Henghui Ding",
            "Jianing Peng",
            "Yao Zhao",
            "Yunpeng Chen",
            "Yunchao Wei"
        ],
        "comments": "ICCV 2025 accepted paper, project page: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In text-to-image generation, producing a series of consistent contents that preserve the same identity is highly valuable for real-world applications. Although a few works have explored training-free methods to enhance the consistency of generated subjects, we observe that they suffer from the following problems. First, they fail to maintain consistent background details, which limits their applicability. Furthermore, when the foreground character undergoes large motion variations, inconsistencies in identity and clothing details become evident. To address these problems, we propose CharaConsist, which employs point-tracking attention and adaptive token merge along with decoupled control of the foreground and background. CharaConsist enables fine-grained consistency for both foreground and background, supporting the generation of one character in continuous shots within a fixed scene or in discrete shots across different scenes. Moreover, CharaConsist is the first consistent generation method tailored for text-to-image DiT model. Its ability to maintain fine-grained consistency, combined with the larger capacity of latest base model, enables it to produce high-quality visual outputs, broadening its applicability to a wider range of real-world scenarios. The source code has been released at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 64,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11539",
        "abs_url": "https://arxiv.org/abs/2507.11539",
        "pdf_url": "https://arxiv.org/pdf/2507.11539",
        "title": "Streaming 4D Visual Geometry Transformer",
        "authors": [
            "Dong Zhuo",
            "Wenzhao Zheng",
            "Jiahe Guo",
            "Yuqi Wu",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "comments": "Code is available at: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a fundamental yet challenging computer vision task. To facilitate interactive and real-time applications, we propose a streaming 4D visual geometry transformer that shares a similar philosophy with autoregressive large language models. We explore a simple and efficient design and employ a causal transformer architecture to process the input sequence in an online manner. We use temporal causal attention and cache the historical keys and values as implicit memory to enable efficient streaming long-term 4D reconstruction. This design can handle real-time 4D reconstruction by incrementally integrating historical information while maintaining high-quality spatial consistency. For efficient training, we propose to distill knowledge from the dense bidirectional visual geometry grounded transformer (VGGT) to our causal model. For inference, our model supports the migration of optimized efficient attention operator (e.g., FlashAttention) from the field of large language models. Extensive experiments on various 4D geometry perception benchmarks demonstrate that our model increases the inference speed in online scenarios while maintaining competitive performance, paving the way for scalable and interactive 4D vision systems. Code is available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 65,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11540",
        "abs_url": "https://arxiv.org/abs/2507.11540",
        "pdf_url": "https://arxiv.org/pdf/2507.11540",
        "title": "Towards Depth Foundation Model: Recent Trends in Vision-Based Depth Estimation",
        "authors": [
            "Zhen Xu",
            "Hongyu Zhou",
            "Sida Peng",
            "Haotong Lin",
            "Haoyu Guo",
            "Jiahao Shao",
            "Peishan Yang",
            "Qinglin Yang",
            "Sheng Miao",
            "Xingyi He",
            "Yifan Wang",
            "Yue Wang",
            "Ruizhen Hu",
            "Yiyi Liao",
            "Xiaowei Zhou",
            "Hujun Bao"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Depth estimation is a fundamental task in 3D computer vision, crucial for applications such as 3D reconstruction, free-viewpoint rendering, robotics, autonomous driving, and AR/VR technologies. Traditional methods relying on hardware sensors like LiDAR are often limited by high costs, low resolution, and environmental sensitivity, limiting their applicability in real-world scenarios. Recent advances in vision-based methods offer a promising alternative, yet they face challenges in generalization and stability due to either the low-capacity model architectures or the reliance on domain-specific and small-scale datasets. The emergence of scaling laws and foundation models in other domains has inspired the development of \"depth foundation models\": deep neural networks trained on large datasets with strong zero-shot generalization capabilities. This paper surveys the evolution of deep learning architectures and paradigms for depth estimation across the monocular, stereo, multi-view, and monocular video settings. We explore the potential of these models to address existing challenges and provide a comprehensive overview of large-scale datasets that can facilitate their development. By identifying key architectures and training strategies, we aim to highlight the path towards robust depth foundation models, offering insights into their future research and applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 66,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.10560",
        "abs_url": "https://arxiv.org/abs/2507.10560",
        "pdf_url": "https://arxiv.org/pdf/2507.10560",
        "title": "Tangma: A Tanh-Guided Activation Function with Learnable Parameters",
        "authors": [
            "Shreel Golwala"
        ],
        "comments": "",
        "subjects": "Neural and Evolutionary Computing (cs.NE); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Activation functions are key to effective backpropagation and expressiveness in deep neural networks. This work introduces Tangma, a new activation function that combines the smooth shape of the hyperbolic tangent with two learnable parameters: $\\alpha$, which shifts the curve's inflection point to adjust neuron activation, and $\\gamma$, which adds linearity to preserve weak gradients and improve training stability. Tangma was evaluated on MNIST and CIFAR-10 using custom networks composed of convolutional and linear layers, and compared against ReLU, Swish, and GELU. On MNIST, Tangma achieved the highest validation accuracy of 99.09% and the lowest validation loss, demonstrating faster and more stable convergence than the baselines. On CIFAR-10, Tangma reached a top validation accuracy of 78.15%, outperforming all other activation functions while maintaining a competitive training loss. Tangma also showed improved training efficiency, with lower average epoch runtimes compared to Swish and GELU. These results suggest that Tangma performs well on standard vision tasks and enables reliable, efficient training. Its learnable design gives more control over activation behavior, which may benefit larger models in tasks such as image recognition or language modeling.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 67,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.10561",
        "abs_url": "https://arxiv.org/abs/2507.10561",
        "pdf_url": "https://arxiv.org/pdf/2507.10561",
        "title": "SFATTI: Spiking FPGA Accelerator for Temporal Task-driven Inference -- A Case Study on MNIST",
        "authors": [
            "Alessio Caviglia",
            "Filippo Marostica",
            "Alessio Carpegna",
            "Alessandro Savino",
            "Stefano Di Carlo"
        ],
        "comments": "",
        "subjects": "Neural and Evolutionary Computing (cs.NE); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Hardware accelerators are essential for achieving low-latency, energy-efficient inference in edge applications like image recognition. Spiking Neural Networks (SNNs) are particularly promising due to their event-driven and temporally sparse nature, making them well-suited for low-power Field Programmable Gate Array (FPGA)-based deployment. This paper explores using the open-source Spiker+ framework to generate optimized SNNs accelerators for handwritten digit recognition on the MNIST dataset. Spiker+ enables high-level specification of network topologies, neuron models, and quantization, automatically generating deployable HDL. We evaluate multiple configurations and analyze trade-offs relevant to edge computing constraints.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 68,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.10589",
        "abs_url": "https://arxiv.org/abs/2507.10589",
        "pdf_url": "https://arxiv.org/pdf/2507.10589",
        "title": "Comparative Analysis of Vision Transformers and Traditional Deep Learning Approaches for Automated Pneumonia Detection in Chest X-Rays",
        "authors": [
            "Gaurav Singh"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Neural and Evolutionary Computing (cs.NE)",
        "abstract": "Pneumonia, particularly when induced by diseases like COVID-19, remains a critical global health challenge requiring rapid and accurate diagnosis. This study presents a comprehensive comparison of traditional machine learning and state-of-the-art deep learning approaches for automated pneumonia detection using chest X-rays (CXRs). We evaluate multiple methodologies, ranging from conventional machine learning techniques (PCA-based clustering, Logistic Regression, and Support Vector Classification) to advanced deep learning architectures including Convolutional Neural Networks (Modified LeNet, DenseNet-121) and various Vision Transformer (ViT) implementations (Deep-ViT, Compact Convolutional Transformer, and Cross-ViT). Using a dataset of 5,856 pediatric CXR images, we demonstrate that Vision Transformers, particularly the Cross-ViT architecture, achieve superior performance with 88.25% accuracy and 99.42% recall, surpassing traditional CNN approaches. Our analysis reveals that architectural choices impact performance more significantly than model size, with Cross-ViT's 75M parameters outperforming larger models. The study also addresses practical considerations including computational efficiency, training requirements, and the critical balance between precision and recall in medical diagnostics. Our findings suggest that Vision Transformers offer a promising direction for automated pneumonia detection, potentially enabling more rapid and accurate diagnosis during health crises.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 69,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.10601",
        "abs_url": "https://arxiv.org/abs/2507.10601",
        "pdf_url": "https://arxiv.org/pdf/2507.10601",
        "title": "AGFS-Tractometry: A Novel Atlas-Guided Fine-Scale Tractometry Approach for Enhanced Along-Tract Group Statistical Comparison Using Diffusion MRI Tractography",
        "authors": [
            "Ruixi Zheng",
            "Wei Zhang",
            "Yijie Li",
            "Xi Zhu",
            "Zhou Lan",
            "Jarrett Rushmore",
            "Yogesh Rathi",
            "Nikos Makris",
            "Lauren J. O'Donnell",
            "Fan Zhang"
        ],
        "comments": "31 pages and 7 figures",
        "subjects": "Quantitative Methods (q-bio.QM); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Image and Video Processing (eess.IV); Methodology (stat.ME)",
        "abstract": "Diffusion MRI (dMRI) tractography is currently the only method for in vivo mapping of the brain's white matter (WM) connections. Tractometry is an advanced tractography analysis technique for along-tract profiling to investigate the morphology and microstructural properties along the fiber tracts. Tractometry has become an essential tool for studying local along-tract differences between different populations (e.g., health vs disease). In this study, we propose a novel atlas-guided fine-scale tractometry method, namely AGFS-Tractometry, that leverages tract spatial information and permutation testing to enhance the along-tract statistical analysis between populations. There are two major contributions in AGFS-Tractometry. First, we create a novel atlas-guided tract profiling template that enables consistent, fine-scale, along-tract parcellation of subject-specific fiber tracts. Second, we propose a novel nonparametric permutation testing group comparison method to enable simultaneous analysis across all along-tract parcels while correcting for multiple comparisons. We perform experimental evaluations on synthetic datasets with known group differences and in vivo real data. We compare AGFS-Tractometry with two state-of-the-art tractometry methods, including Automated Fiber-tract Quantification (AFQ) and BUndle ANalytics (BUAN). Our results show that the proposed AGFS-Tractometry obtains enhanced sensitivity and specificity in detecting local WM differences. In the real data analysis experiments, AGFS-Tractometry can identify more regions with significant differences, which are anatomically consistent with the existing literature. Overall, these demonstrate the ability of AGFS-Tractometry to detect subtle or spatially localized WM group-level differences. The created tract profiling template and related code are available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 70,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.10611",
        "abs_url": "https://arxiv.org/abs/2507.10611",
        "pdf_url": "https://arxiv.org/pdf/2507.10611",
        "title": "FedGSCA: Medical Federated Learning with Global Sample Selector and Client Adaptive Adjuster under Label Noise",
        "authors": [
            "Mengwen Ye",
            "Yingzi Huangfu",
            "Shujian Gao",
            "Wei Ren",
            "Weifan Liu",
            "Zekuan Yu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Federated Learning (FL) emerged as a solution for collaborative medical image classification while preserving data privacy. However, label noise, which arises from inter-institutional data variability, can cause training instability and degrade model performance. Existing FL methods struggle with noise heterogeneity and the imbalance in medical data. Motivated by these challenges, we propose FedGSCA, a novel framework for enhancing robustness in noisy medical FL. FedGSCA introduces a Global Sample Selector that aggregates noise knowledge from all clients, effectively addressing noise heterogeneity and improving global model stability. Furthermore, we develop a Client Adaptive Adjustment (CAA) mechanism that combines adaptive threshold pseudo-label generation and Robust Credal Labeling Loss. CAA dynamically adjusts to class distributions, ensuring the inclusion of minority samples and carefully managing noisy labels by considering multiple plausible labels. This dual approach mitigates the impact of noisy data and prevents overfitting during local training, which improves the generalizability of the model. We evaluate FedGSCA on one real-world colon slides dataset and two synthetic medical datasets under various noise conditions, including symmetric, asymmetric, extreme, and heterogeneous types. The results show that FedGSCA outperforms the state-of-the-art methods, excelling in extreme and heterogeneous noise scenarios. Moreover, FedGSCA demonstrates significant advantages in improving model stability and handling complex noise, making it well-suited for real-world medical federated learning scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 71,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.10623",
        "abs_url": "https://arxiv.org/abs/2507.10623",
        "pdf_url": "https://arxiv.org/pdf/2507.10623",
        "title": "Flows and Diffusions on the Neural Manifold",
        "authors": [
            "Daniel Saragih",
            "Deyu Cao",
            "Tejas Balaji"
        ],
        "comments": "40 pages, 6 figures, 13 tables",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Diffusion and flow-based generative models have achieved remarkable success in domains such as image synthesis, video generation, and natural language modeling. In this work, we extend these advances to weight space learning by leveraging recent techniques to incorporate structural priors derived from optimization dynamics. Central to our approach is modeling the trajectory induced by gradient descent as a trajectory inference problem. We unify several trajectory inference techniques under the framework of gradient flow matching, providing a theoretical framework for treating optimization paths as inductive bias. We further explore architectural and algorithmic choices, including reward fine-tuning by adjoint matching, the use of autoencoders for latent weight representation, conditioning on task-specific context data, and adopting informative source distributions such as Kaiming uniform. Experiments demonstrate that our method matches or surpasses baselines in generating in-distribution weights, improves initialization for downstream training, and supports fine-tuning to enhance performance. Finally, we illustrate a practical application in safety-critical systems: detecting harmful covariate shifts, where our method outperforms the closest comparable baseline.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 72,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.10637",
        "abs_url": "https://arxiv.org/abs/2507.10637",
        "pdf_url": "https://arxiv.org/pdf/2507.10637",
        "title": "A Simple Baseline for Stable and Plastic Neural Networks",
        "authors": [
            "É. Künzel",
            "A. Jaziri",
            "V. Ramesh"
        ],
        "comments": "11 pages, 50 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Continual learning in computer vision requires that models adapt to a continuous stream of tasks without forgetting prior knowledge, yet existing approaches often tip the balance heavily toward either plasticity or stability. We introduce RDBP, a simple, low-overhead baseline that unites two complementary mechanisms: ReLUDown, a lightweight activation modification that preserves feature sensitivity while preventing neuron dormancy, and Decreasing Backpropagation, a biologically inspired gradient-scheduling scheme that progressively shields early layers from catastrophic updates. Evaluated on the Continual ImageNet benchmark, RDBP matches or exceeds the plasticity and stability of state-of-the-art methods while reducing computational cost. RDBP thus provides both a practical solution for real-world continual learning and a clear benchmark against which future continual learning strategies can be measured.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 73,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.10672",
        "abs_url": "https://arxiv.org/abs/2507.10672",
        "pdf_url": "https://arxiv.org/pdf/2507.10672",
        "title": "Vision Language Action Models in Robotic Manipulation: A Systematic Review",
        "authors": [
            "Muhayy Ud Din",
            "Waseem Akram",
            "Lyes Saad Saoud",
            "Jan Rosell",
            "Irfan Hussain"
        ],
        "comments": "submitted to annual review in control",
        "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Vision Language Action (VLA) models represent a transformative shift in robotics, with the aim of unifying visual perception, natural language understanding, and embodied control within a single learning framework. This review presents a comprehensive and forward-looking synthesis of the VLA paradigm, with a particular emphasis on robotic manipulation and instruction-driven autonomy. We comprehensively analyze 102 VLA models, 26 foundational datasets, and 12 simulation platforms that collectively shape the development and evaluation of VLAs models. These models are categorized into key architectural paradigms, each reflecting distinct strategies for integrating vision, language, and control in robotic systems. Foundational datasets are evaluated using a novel criterion based on task complexity, variety of modalities, and dataset scale, allowing a comparative analysis of their suitability for generalist policy learning. We introduce a two-dimensional characterization framework that organizes these datasets based on semantic richness and multimodal alignment, showing underexplored regions in the current data landscape. Simulation environments are evaluated for their effectiveness in generating large-scale data, as well as their ability to facilitate transfer from simulation to real-world settings and the variety of supported tasks. Using both academic and industrial contributions, we recognize ongoing challenges and outline strategic directions such as scalable pretraining protocols, modular architectural design, and robust multimodal alignment strategies. This review serves as both a technical reference and a conceptual roadmap for advancing embodiment and robotic control, providing insights that span from dataset generation to real world deployment of generalist robotic agents.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 74,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.10768",
        "abs_url": "https://arxiv.org/abs/2507.10768",
        "pdf_url": "https://arxiv.org/pdf/2507.10768",
        "title": "Spatial Reasoners for Continuous Variables in Any Domain",
        "authors": [
            "Bart Pogodzinski",
            "Christopher Wewer",
            "Bernt Schiele",
            "Jan Eric Lenssen"
        ],
        "comments": "For the project documentation see this https URL . The SRM project website is available at this https URL . The work was published on ICML 2025 CODEML workshop",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We present Spatial Reasoners, a software framework to perform spatial reasoning over continuous variables with generative denoising models. Denoising generative models have become the de-facto standard for image generation, due to their effectiveness in sampling from complex, high-dimensional distributions. Recently, they have started being explored in the context of reasoning over multiple continuous variables. Providing infrastructure for generative reasoning with such models requires a high effort, due to a wide range of different denoising formulations, samplers, and inference strategies. Our presented framework aims to facilitate research in this area, providing easy-to-use interfaces to control variable mapping from arbitrary data domains, generative model paradigms, and inference strategies. Spatial Reasoners are openly available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 75,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.10776",
        "abs_url": "https://arxiv.org/abs/2507.10776",
        "pdf_url": "https://arxiv.org/pdf/2507.10776",
        "title": "rt-RISeg: Real-Time Model-Free Robot Interactive Segmentation for Active Instance-Level Object Understanding",
        "authors": [
            "Howard H. Qian",
            "Yiting Chen",
            "Gaotian Wang",
            "Podshara Chanrungmaneekul",
            "Kaiyu Hang"
        ],
        "comments": "8 pages, IROS 2025, Interactive Perception, Segmentation, Robotics, Computer Vision",
        "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Successful execution of dexterous robotic manipulation tasks in new environments, such as grasping, depends on the ability to proficiently segment unseen objects from the background and other objects. Previous works in unseen object instance segmentation (UOIS) train models on large-scale datasets, which often leads to overfitting on static visual features. This dependency results in poor generalization performance when confronted with out-of-distribution scenarios. To address this limitation, we rethink the task of UOIS based on the principle that vision is inherently interactive and occurs over time. We propose a novel real-time interactive perception framework, rt-RISeg, that continuously segments unseen objects by robot interactions and analysis of a designed body frame-invariant feature (BFIF). We demonstrate that the relative rotational and linear velocities of randomly sampled body frames, resulting from selected robot interactions, can be used to identify objects without any learned segmentation model. This fully self-contained segmentation pipeline generates and updates object segmentation masks throughout each robot interaction without the need to wait for an action to finish. We showcase the effectiveness of our proposed interactive perception method by achieving an average object segmentation accuracy rate 27.5% greater than state-of-the-art UOIS methods. Furthermore, although rt-RISeg is a standalone framework, we show that the autonomously generated segmentation masks can be used as prompts to vision foundation models for significantly improved performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 76,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.10787",
        "abs_url": "https://arxiv.org/abs/2507.10787",
        "pdf_url": "https://arxiv.org/pdf/2507.10787",
        "title": "Can Multimodal Foundation Models Understand Schematic Diagrams? An Empirical Study on Information-Seeking QA over Scientific Papers",
        "authors": [
            "Yilun Zhao",
            "Chengye Wang",
            "Chuhan Li",
            "Arman Cohan"
        ],
        "comments": "ACL 2025 Findings",
        "subjects": "Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "This paper introduces MISS-QA, the first benchmark specifically designed to evaluate the ability of models to interpret schematic diagrams within scientific literature. MISS-QA comprises 1,500 expert-annotated examples over 465 scientific papers. In this benchmark, models are tasked with interpreting schematic diagrams that illustrate research overviews and answering corresponding information-seeking questions based on the broader context of the paper. We assess the performance of 18 frontier multimodal foundation models, including o4-mini, Gemini-2.5-Flash, and Qwen2.5-VL. We reveal a significant performance gap between these models and human experts on MISS-QA. Our analysis of model performance on unanswerable questions and our detailed error analysis further highlight the strengths and limitations of current models, offering key insights to enhance models in comprehending multimodal scientific literature.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 77,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.10869",
        "abs_url": "https://arxiv.org/abs/2507.10869",
        "pdf_url": "https://arxiv.org/pdf/2507.10869",
        "title": "Focus on Texture: Rethinking Pre-training in Masked Autoencoders for Medical Image Classification",
        "authors": [
            "Chetan Madan",
            "Aarjav Satia",
            "Soumen Basu",
            "Pankaj Gupta",
            "Usha Dutta",
            "Chetan Arora"
        ],
        "comments": "To appear at MICCAI 2025",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Masked Autoencoders (MAEs) have emerged as a dominant strategy for self-supervised representation learning in natural images, where models are pre-trained to reconstruct masked patches with a pixel-wise mean squared error (MSE) between original and reconstructed RGB values as the loss. We observe that MSE encourages blurred image re-construction, but still works for natural images as it preserves dominant edges. However, in medical imaging, when the texture cues are more important for classification of a visual abnormality, the strategy fails. Taking inspiration from Gray Level Co-occurrence Matrix (GLCM) feature in Radiomics studies, we propose a novel MAE based pre-training framework, GLCM-MAE, using reconstruction loss based on matching GLCM. GLCM captures intensity and spatial relationships in an image, hence proposed loss helps preserve morphological features. Further, we propose a novel formulation to convert matching GLCM matrices into a differentiable loss function. We demonstrate that unsupervised pre-training on medical images with the proposed GLCM loss improves representations for downstream tasks. GLCM-MAE outperforms the current state-of-the-art across four tasks - gallbladder cancer detection from ultrasound images by 2.1%, breast cancer detection from ultrasound by 3.1%, pneumonia detection from x-rays by 0.5%, and COVID detection from CT by 0.6%. Source code and pre-trained models are available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 78,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.10894",
        "abs_url": "https://arxiv.org/abs/2507.10894",
        "pdf_url": "https://arxiv.org/pdf/2507.10894",
        "title": "NavComposer: Composing Language Instructions for Navigation Trajectories through Action-Scene-Object Modularization",
        "authors": [
            "Zongtao He",
            "Liuyi Wang",
            "Lu Chen",
            "Chengju Liu",
            "Qijun Chen"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Language-guided navigation is a cornerstone of embodied AI, enabling agents to interpret language instructions and navigate complex environments. However, expert-provided instructions are limited in quantity, while synthesized annotations often lack quality, making them insufficient for large-scale research. To address this, we propose NavComposer, a novel framework for automatically generating high-quality navigation instructions. NavComposer explicitly decomposes semantic entities such as actions, scenes, and objects, and recomposes them into natural language instructions. Its modular architecture allows flexible integration of state-of-the-art techniques, while the explicit use of semantic entities enhances both the richness and accuracy of instructions. Moreover, it operates in a data-agnostic manner, supporting adaptation to diverse navigation trajectories without domain-specific training. Complementing NavComposer, we introduce NavInstrCritic, a comprehensive annotation-free evaluation system that assesses navigation instructions on three dimensions: contrastive matching, semantic consistency, and linguistic diversity. NavInstrCritic provides a holistic evaluation of instruction quality, addressing limitations of traditional metrics that rely heavily on expert annotations. By decoupling instruction generation and evaluation from specific navigation agents, our method enables more scalable and generalizable research. Extensive experiments provide direct and practical evidence for the effectiveness of our method.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 79,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.10960",
        "abs_url": "https://arxiv.org/abs/2507.10960",
        "pdf_url": "https://arxiv.org/pdf/2507.10960",
        "title": "Whom to Respond To? A Transformer-Based Model for Multi-Party Social Robot Interaction",
        "authors": [
            "He Zhu",
            "Ryo Miyoshi",
            "Yuki Okafuji"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Prior human-robot interaction (HRI) research has primarily focused on single-user interactions, where robots do not need to consider the timing or recipient of their responses. However, in multi-party interactions, such as at malls and hospitals, social robots must understand the context and decide both when and to whom they should respond. In this paper, we propose a Transformer-based multi-task learning framework to improve the decision-making process of social robots, particularly in multi-user environments. Considering the characteristics of HRI, we propose two novel loss functions: one that enforces constraints on active speakers to improve scene modeling, and another that guides response selection towards utterances specifically directed at the robot. Additionally, we construct a novel multi-party HRI dataset that captures real-world complexities, such as gaze misalignment. Experimental results demonstrate that our model achieves state-of-the-art performance in respond decisions, outperforming existing heuristic-based and single-task approaches. Our findings contribute to the development of socially intelligent social robots capable of engaging in natural and context-aware multi-party interactions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 80,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.10972",
        "abs_url": "https://arxiv.org/abs/2507.10972",
        "pdf_url": "https://arxiv.org/pdf/2507.10972",
        "title": "Teach Me Sign: Stepwise Prompting LLM for Sign Language Production",
        "authors": [
            "Zhaoyi An",
            "Rei Kawakami"
        ],
        "comments": "Accepted by IEEE ICIP 2025",
        "subjects": "Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM)",
        "abstract": "Large language models, with their strong reasoning ability and rich knowledge, have brought revolution to many tasks of AI, but their impact on sign language generation remains limited due to its complexity and unique rules. In this paper, we propose TEAch Me Sign (TEAM-Sign), treating sign language as another natural language. By fine-tuning an LLM, we enable it to learn the correspondence between text and sign language, and facilitate generation. Considering the differences between sign and spoken language, we employ a stepwise prompting strategy to extract the inherent sign language knowledge within the LLM, thereby supporting the learning and generation process. Experimental results on How2Sign and Phoenix14T datasets demonstrate that our approach effectively leverages both the sign language knowledge and reasoning capabilities of LLM to align the different distribution and grammatical rules between sign and spoken language.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 81,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11001",
        "abs_url": "https://arxiv.org/abs/2507.11001",
        "pdf_url": "https://arxiv.org/pdf/2507.11001",
        "title": "Learning to Tune Like an Expert: Interpretable and Scene-Aware Navigation via MLLM Reasoning and CVAE-Based Adaptation",
        "authors": [
            "Yanbo Wang",
            "Zipeng Fang",
            "Lei Zhao",
            "Weidong Chen"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Service robots are increasingly deployed in diverse and dynamic environments, where both physical layouts and social contexts change over time and across locations. In these unstructured settings, conventional navigation systems that rely on fixed parameters often fail to generalize across scenarios, resulting in degraded performance and reduced social acceptance. Although recent approaches have leveraged reinforcement learning to enhance traditional planners, these methods often fail in real-world deployments due to poor generalization and limited simulation diversity, which hampers effective sim-to-real transfer. To tackle these issues, we present LE-Nav, an interpretable and scene-aware navigation framework that leverages multi-modal large language model reasoning and conditional variational autoencoders to adaptively tune planner hyperparameters. To achieve zero-shot scene understanding, we utilize one-shot exemplars and chain-of-thought prompting strategies. Additionally, a conditional variational autoencoder captures the mapping between natural language instructions and navigation hyperparameters, enabling expert-level tuning. Experiments show that LE-Nav can generate hyperparameters achieving human-level tuning across diverse planners and scenarios. Real-world navigation trials and a user study on a smart wheelchair platform demonstrate that it outperforms state-of-the-art methods on quantitative metrics such as success rate, efficiency, safety, and comfort, while receiving higher subjective scores for perceived safety and social acceptance. Code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 82,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11017",
        "abs_url": "https://arxiv.org/abs/2507.11017",
        "pdf_url": "https://arxiv.org/pdf/2507.11017",
        "title": "First-Order Error Matters: Accurate Compensation for Quantized Large Language Models",
        "authors": [
            "Xingyu Zheng",
            "Haotong Qin",
            "Yuye Li",
            "Jiakai Wang",
            "Jinyang Guo",
            "Michele Magno",
            "Xianglong Liu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Post-training quantization (PTQ) offers an efficient approach to compressing large language models (LLMs), significantly reducing memory access and computational costs. Existing compensation-based weight calibration methods often rely on a second-order Taylor expansion to model quantization error, under the assumption that the first-order term is negligible in well-trained full-precision models. However, we reveal that the progressive compensation process introduces accumulated first-order deviations between latent weights and their full-precision counterparts, making this assumption fundamentally flawed. To address this, we propose FOEM, a novel PTQ method that explicitly incorporates first-order gradient terms to improve quantization error compensation. FOEM approximates gradients by directly computing the difference between latent and full-precision weights, avoiding the high cost and limited generalization of backpropagation-based gradient computation. This approach introduces minimal additional computational overhead. Moreover, FOEM leverages precomputed Cholesky factors to efficiently recover the inverse of Hessian submatrices in real time. Extensive experiments across a wide range of models and benchmarks demonstrate that FOEM consistently outperforms the classical GPTQ method. In 3-bit weight-only quantization, FOEM reduces the perplexity of Llama3-8B by 89.6%, and improves the 5-shot MMLU accuracy of Llama3-70B from 51.7% to 74.9%, approaching the full-precision performance of 78.6%. Furthermore, FOEM can be seamlessly integrated with advanced techniques such as GPTAQ and SpinQuant, yielding additional improvements under the challenging W4A4KV4 setting, and further narrowing the accuracy gap with full-precision baselines beyond what current state-of-the-art methods achieve. The code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 83,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11069",
        "abs_url": "https://arxiv.org/abs/2507.11069",
        "pdf_url": "https://arxiv.org/pdf/2507.11069",
        "title": "TRAN-D: 2D Gaussian Splatting-based Sparse-view Transparent Object Depth Reconstruction via Physics Simulation for Scene Update",
        "authors": [
            "Jeongyun Kim",
            "Seunghoon Jeong",
            "Giseop Kim",
            "Myung-Hwan Jeon",
            "Eunji Jun",
            "Ayoung Kim"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Understanding the 3D geometry of transparent objects from RGB images is challenging due to their inherent physical properties, such as reflection and refraction. To address these difficulties, especially in scenarios with sparse views and dynamic environments, we introduce TRAN-D, a novel 2D Gaussian Splatting-based depth reconstruction method for transparent objects. Our key insight lies in separating transparent objects from the background, enabling focused optimization of Gaussians corresponding to the object. We mitigate artifacts with an object-aware loss that places Gaussians in obscured regions, ensuring coverage of invisible surfaces while reducing overfitting. Furthermore, we incorporate a physics-based simulation that refines the reconstruction in just a few seconds, effectively handling object removal and chain-reaction movement of remaining objects without the need for rescanning. TRAN-D is evaluated on both synthetic and real-world sequences, and it consistently demonstrated robust improvements over existing GS-based state-of-the-art methods. In comparison with baselines, TRAN-D reduces the mean absolute error by over 39% for the synthetic TRansPose sequences. Furthermore, despite being updated using only one image, TRAN-D reaches a {\\delta} < 2.5 cm accuracy of 48.46%, over 1.5 times that of baselines, which uses six images. Code and more results are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 84,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11071",
        "abs_url": "https://arxiv.org/abs/2507.11071",
        "pdf_url": "https://arxiv.org/pdf/2507.11071",
        "title": "LogTinyLLM: Tiny Large Language Models Based Contextual Log Anomaly Detection",
        "authors": [
            "Isaiah Thompson Ocansey",
            "Ritwik Bhattacharya",
            "Tanmay Sen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Log anomaly detection using traditional rule based or deep learning based methods is often challenging due to the large volume and highly complex nature of log sequence. So effective way of detection of anomalous sequence of logs is crucial for system maintenance and development. This paper proposes parameter efficient finetuning specifically low rank adaptation (LoRA) and adapter based approaches for finding contextual anomalies in sequence of logs in large log data set. It compares different tiny large language models (LLMs) on the Thunderbird dataset. The results show that LoRA based finetuning provides substantial performance improvements of 18 to 19 percentage over LogBert based full finetuning approach, achieving accuracy scores between 97.76% and 98.83% compared to 79.37%.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 85,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11152",
        "abs_url": "https://arxiv.org/abs/2507.11152",
        "pdf_url": "https://arxiv.org/pdf/2507.11152",
        "title": "Latent Space Consistency for Sparse-View CT Reconstruction",
        "authors": [
            "Duoyou Chen",
            "Yunqing Chen",
            "Can Zhang",
            "Zhou Wang",
            "Cheng Chen",
            "Ruoxiu Xiao"
        ],
        "comments": "ACMMM2025 Accepted",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Computed Tomography (CT) is a widely utilized imaging modality in clinical settings. Using densely acquired rotational X-ray arrays, CT can capture 3D spatial features. However, it is confronted with challenged such as significant time consumption and high radiation exposure. CT reconstruction methods based on sparse-view X-ray images have garnered substantial attention from researchers as they present a means to mitigate costs and risks. In recent years, diffusion models, particularly the Latent Diffusion Model (LDM), have demonstrated promising potential in the domain of 3D CT reconstruction. Nonetheless, due to the substantial differences between the 2D latent representation of X-ray modalities and the 3D latent representation of CT modalities, the vanilla LDM is incapable of achieving effective alignment within the latent space. To address this issue, we propose the Consistent Latent Space Diffusion Model (CLS-DM), which incorporates cross-modal feature contrastive learning to efficiently extract latent 3D information from 2D X-ray images and achieve latent space alignment between modalities. Experimental results indicate that CLS-DM outperforms classical and state-of-the-art generative models in terms of standard voxel-level metrics (PSNR, SSIM) on the LIDC-IDRI and CTSpine1K datasets. This methodology not only aids in enhancing the effectiveness and economic viability of sparse X-ray reconstructed CT but can also be generalized to other cross-modal transformation tasks, such as text-to-image synthesis. We have made our code publicly available at this https URL to facilitate further research and applications in other domains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 86,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11293",
        "abs_url": "https://arxiv.org/abs/2507.11293",
        "pdf_url": "https://arxiv.org/pdf/2507.11293",
        "title": "3D Magnetic Inverse Routine for Single-Segment Magnetic Field Images",
        "authors": [
            "J. Senthilnath",
            "Chen Hao",
            "F. C. Wellstood"
        ],
        "comments": "copyright 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works",
        "subjects": "Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In semiconductor packaging, accurately recovering 3D information is crucial for non-destructive testing (NDT) to localize circuit defects. This paper presents a novel approach called the 3D Magnetic Inverse Routine (3D MIR), which leverages Magnetic Field Images (MFI) to retrieve the parameters for the 3D current flow of a single-segment. The 3D MIR integrates a deep learning (DL)-based Convolutional Neural Network (CNN), spatial-physics-based constraints, and optimization techniques. The method operates in three stages: i) The CNN model processes the MFI data to predict ($\\ell/z_o$), where $\\ell$ is the wire length and $z_o$ is the wire's vertical depth beneath the magnetic sensors and classify segment type ($c$). ii) By leveraging spatial-physics-based constraints, the routine provides initial estimates for the position ($x_o$, $y_o$, $z_o$), length ($\\ell$), current ($I$), and current flow direction (positive or negative) of the current segment. iii) An optimizer then adjusts these five parameters ($x_o$, $y_o$, $z_o$, $\\ell$, $I$) to minimize the difference between the reconstructed MFI and the actual MFI. The results demonstrate that the 3D MIR method accurately recovers 3D information with high precision, setting a new benchmark for magnetic image reconstruction in semiconductor packaging. This method highlights the potential of combining DL and physics-driven optimization in practical applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 87,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11302",
        "abs_url": "https://arxiv.org/abs/2507.11302",
        "pdf_url": "https://arxiv.org/pdf/2507.11302",
        "title": "All Eyes, no IMU: Learning Flight Attitude from Vision Alone",
        "authors": [
            "Jesse J. Hagenaars",
            "Stein Stroobants",
            "Sander M. Bohte",
            "Guido C.H.E. De Croon"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Vision is an essential part of attitude control for many flying animals, some of which have no dedicated sense of gravity. Flying robots, on the other hand, typically depend heavily on accelerometers and gyroscopes for attitude stabilization. In this work, we present the first vision-only approach to flight control for use in generic environments. We show that a quadrotor drone equipped with a downward-facing event camera can estimate its attitude and rotation rate from just the event stream, enabling flight control without inertial sensors. Our approach uses a small recurrent convolutional neural network trained through supervised learning. Real-world flight tests demonstrate that our combination of event camera and low-latency neural network is capable of replacing the inertial measurement unit in a traditional flight control loop. Furthermore, we investigate the network's generalization across different environments, and the impact of memory and different fields of view. While networks with memory and access to horizon-like visual cues achieve best performance, variants with a narrower field of view achieve better relative generalization. Our work showcases vision-only flight control as a promising candidate for enabling autonomous, insect-scale flying robots.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 88,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11325",
        "abs_url": "https://arxiv.org/abs/2507.11325",
        "pdf_url": "https://arxiv.org/pdf/2507.11325",
        "title": "HANS-Net: Hyperbolic Convolution and Adaptive Temporal Attention for Accurate and Generalizable Liver and Tumor Segmentation in CT Imaging",
        "authors": [
            "Arefin Ittesafun Abian",
            "Ripon Kumar Debnath",
            "Md. Abdur Rahman",
            "Mohaimenul Azam Khan Raiaan",
            "Md Rafiqul Islam",
            "Asif Karim",
            "Reem E. Mohamed",
            "Sami Azam"
        ],
        "comments": "10 figures. Will be submitted to IEEE Transactions on Radiation and Plasma Medical Sciences",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Accurate liver and tumor segmentation on abdominal CT images is critical for reliable diagnosis and treatment planning, but remains challenging due to complex anatomical structures, variability in tumor appearance, and limited annotated data. To address these issues, we introduce Hyperbolic-convolutions Adaptive-temporal-attention with Neural-representation and Synaptic-plasticity Network (HANS-Net), a novel segmentation framework that synergistically combines hyperbolic convolutions for hierarchical geometric representation, a wavelet-inspired decomposition module for multi-scale texture learning, a biologically motivated synaptic plasticity mechanism for adaptive feature enhancement, and an implicit neural representation branch to model fine-grained and continuous anatomical boundaries. Additionally, we incorporate uncertainty-aware Monte Carlo dropout to quantify prediction confidence and lightweight temporal attention to improve inter-slice consistency without sacrificing efficiency. Extensive evaluations of the LiTS dataset demonstrate that HANS-Net achieves a mean Dice score of 93.26%, an IoU of 88.09%, an average symmetric surface distance (ASSD) of 0.72 mm, and a volume overlap error (VOE) of 11.91%. Furthermore, cross-dataset validation on the 3D-IRCADb-01 dataset obtains an average Dice of 87.45%, IoU of 80.30%, ASSD of 1.525 mm, and VOE of 19.71%, indicating strong generalization across different datasets. These results confirm the effectiveness and robustness of HANS-Net in providing anatomically consistent, accurate, and confident liver and tumor segmentation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 89,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11401",
        "abs_url": "https://arxiv.org/abs/2507.11401",
        "pdf_url": "https://arxiv.org/pdf/2507.11401",
        "title": "Stochastic Entanglement Configuration for Constructive Entanglement Topologies in Quantum Machine Learning with Application to Cardiac MRI",
        "authors": [
            "Mehri Mehrnia",
            "Mohammed S.M. Elbaz"
        ],
        "comments": "Accepted for publication at IEEE International Conference on Quantum Computing and Engineering (QCE) 2025",
        "subjects": "Quantum Physics (quant-ph); Computer Vision and Pattern Recognition (cs.CV); Emerging Technologies (cs.ET); Machine Learning (cs.LG)",
        "abstract": "Efficient entanglement strategies are essential for advancing variational quantum circuits (VQCs) for quantum machine learning (QML). However, most current approaches use fixed entanglement topologies that are not adaptive to task requirements, limiting potential gains over classical models. We introduce a novel stochastic entanglement configuration method that systematically generates diverse entanglement topologies to identify a subspace of constructive entanglement configurations, defined as entanglement topologies that boost hybrid model performance (e.g., classification accuracy) beyond classical baselines. Each configuration is encoded as a stochastic binary matrix, denoting directed entanglement between qubits. This enables scalable exploration of the hyperspace of candidate entanglement topologies using entanglement density and per-qubit constraints as key metrics. We define unconstrained and constrained sampling modes, controlling entanglement per qubit. Using our method, 400 stochastic configurations were generated and evaluated in a hybrid QML for cardiac MRI disease classification. We identified 64 (16%) novel constructive entanglement configurations that consistently outperformed the classical baseline. Ensemble aggregation of top-performing configurations achieved ~0.92 classification accuracy, exceeding the classical model (~0.87) by over 5%. Compared to four conventional topologies (ring, nearest neighbor, no entanglement, fully entangled), none surpassed the classical baseline (maximum accuracy ~0.82), while our configurations delivered up to ~20% higher accuracy. Thus, highlighting the robustness and generalizability of the identified constructive entanglements.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 90,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11415",
        "abs_url": "https://arxiv.org/abs/2507.11415",
        "pdf_url": "https://arxiv.org/pdf/2507.11415",
        "title": "U-RWKV: Lightweight medical image segmentation with direction-adaptive RWKV",
        "authors": [
            "Hongbo Ye",
            "Fenghe Tang",
            "Peiang Zhao",
            "Zhen Huang",
            "Dexin Zhao",
            "Minghao Bian",
            "S.Kevin Zhou"
        ],
        "comments": "Accepted by MICCAI2025",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Achieving equity in healthcare accessibility requires lightweight yet high-performance solutions for medical image segmentation, particularly in resource-limited settings. Existing methods like U-Net and its variants often suffer from limited global Effective Receptive Fields (ERFs), hindering their ability to capture long-range dependencies. To address this, we propose U-RWKV, a novel framework leveraging the Recurrent Weighted Key-Value(RWKV) architecture, which achieves efficient long-range modeling at O(N) computational cost. The framework introduces two key innovations: the Direction-Adaptive RWKV Module(DARM) and the Stage-Adaptive Squeeze-and-Excitation Module(SASE). DARM employs Dual-RWKV and QuadScan mechanisms to aggregate contextual cues across images, mitigating directional bias while preserving global context and maintaining high computational efficiency. SASE dynamically adapts its architecture to different feature extraction stages, balancing high-resolution detail preservation and semantic relationship capture. Experiments demonstrate that U-RWKV achieves state-of-the-art segmentation performance with high computational efficiency, offering a practical solution for democratizing advanced medical imaging technologies in resource-constrained environments. The code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 91,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11461",
        "abs_url": "https://arxiv.org/abs/2507.11461",
        "pdf_url": "https://arxiv.org/pdf/2507.11461",
        "title": "Deep Equilibrium models for Poisson Imaging Inverse problems via Mirror Descent",
        "authors": [
            "Christian Daniele",
            "Silvia Villa",
            "Samuel Vaiter",
            "Luca Calatroni"
        ],
        "comments": "",
        "subjects": "Optimization and Control (math.OC); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Deep Equilibrium Models (DEQs) are implicit neural networks with fixed points, which have recently gained attention for learning image regularization functionals, particularly in settings involving Gaussian fidelities, where assumptions on the forward operator ensure contractiveness of standard (proximal) Gradient Descent operators. In this work, we extend the application of DEQs to Poisson inverse problems, where the data fidelity term is more appropriately modeled by the Kullback-Leibler divergence. To this end, we introduce a novel DEQ formulation based on Mirror Descent defined in terms of a tailored non-Euclidean geometry that naturally adapts with the structure of the data term. This enables the learning of neural regularizers within a principled training framework. We derive sufficient conditions to guarantee the convergence of the learned reconstruction scheme and propose computational strategies that enable both efficient training and fully parameter-free inference. Numerical experiments show that our method outperforms traditional model-based approaches and it is comparable to the performance of Bregman Plug-and-Play methods, while mitigating their typical drawbacks - namely, sensitivity to initialization and careful tuning of hyperparameters. The code is publicly available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 92,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.CV/2025-07-16?abs=True",
        "arxiv_id": "2507.11465",
        "abs_url": "https://arxiv.org/abs/2507.11465",
        "pdf_url": "https://arxiv.org/pdf/2507.11465",
        "title": "Elevating 3D Models: High-Quality Texture and Geometry Refinement from a Low-Quality Model",
        "authors": [
            "Nuri Ryu",
            "Jiyun Won",
            "Jooeun Son",
            "Minsu Gong",
            "Joo-Haeng Lee",
            "Sunghyun Cho"
        ],
        "comments": "Accepted to SIGGRAPH 2025. For the project page, see this https URL",
        "subjects": "Graphics (cs.GR); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "High-quality 3D assets are essential for various applications in computer graphics and 3D vision but remain scarce due to significant acquisition costs. To address this shortage, we introduce Elevate3D, a novel framework that transforms readily accessible low-quality 3D assets into higher quality. At the core of Elevate3D is HFS-SDEdit, a specialized texture enhancement method that significantly improves texture quality while preserving the appearance and geometry while fixing its degradations. Furthermore, Elevate3D operates in a view-by-view manner, alternating between texture and geometry refinement. Unlike previous methods that have largely overlooked geometry refinement, our framework leverages geometric cues from images refined with HFS-SDEdit by employing state-of-the-art monocular geometry predictors. This approach ensures detailed and accurate geometry that aligns seamlessly with the enhanced texture. Elevate3D outperforms recent competitors by achieving state-of-the-art quality in 3D model refinement, effectively addressing the scarcity of high-quality open-source 3D assets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    }
]