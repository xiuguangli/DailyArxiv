[
    {
        "order": 1,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10564",
        "abs_url": "https://arxiv.org/abs/2507.10564",
        "pdf_url": "https://arxiv.org/pdf/2507.10564",
        "title": "Tool-to-Tool Matching Analysis Based Difference Score Computation Methods for Semiconductor Manufacturing",
        "authors": [
            "Sameera Bharadwaja H.",
            "Siddhrath Jandial",
            "Shashank S. Agashe",
            "Rajesh Kumar Reddy Moore",
            "Youngkwan Kim"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Signal Processing (eess.SP); Machine Learning (stat.ML)",
        "abstract": "We consider the problem of tool-to-tool matching (TTTM), also called, chamber matching in the context of a semiconductor manufacturing equipment. Traditional TTTM approaches utilize static configuration data or depend on a golden reference which are difficult to obtain in a commercial manufacturing line. Further, existing methods do not extend very well to a heterogeneous setting, where equipment are of different make-and-model, sourced from different equipment vendors. We propose novel TTTM analysis pipelines to overcome these issues. We hypothesize that a mismatched equipment would have higher variance and/or higher number of modes in the data. Our best univariate method achieves a correlation coefficient >0.95 and >0.5 with the variance and number of modes, respectively showing that the proposed methods are effective. Also, the best multivariate method achieves a correlation coefficient >0.75 with the top-performing univariate methods, showing its effectiveness. Finally, we analyze the sensitivity of the multivariate algorithms to the algorithm hyper-parameters.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 2,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10574",
        "abs_url": "https://arxiv.org/abs/2507.10574",
        "pdf_url": "https://arxiv.org/pdf/2507.10574",
        "title": "Enhancing Cross Entropy with a Linearly Adaptive Loss Function for Optimized Classification Performance",
        "authors": [
            "Jae Wan Shim"
        ],
        "comments": "13 pages, 2 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)",
        "abstract": "We propose the Linearly Adaptive Cross Entropy Loss function. This is a novel measure derived from the information theory. In comparison to the standard cross entropy loss function, the proposed one has an additional term that depends on the predicted probability of the true class. This feature serves to enhance the optimization process in classification tasks involving one-hot encoded class labels. The proposed one has been evaluated on a ResNet-based model using the CIFAR-100 dataset. Preliminary results show that the proposed one consistently outperforms the standard cross entropy loss function in terms of classification accuracy. Moreover, the proposed one maintains simplicity, achieving practically the same efficiency to the traditional cross entropy loss. These findings suggest that our approach could broaden the scope for future research into loss function design.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 3,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10575",
        "abs_url": "https://arxiv.org/abs/2507.10575",
        "pdf_url": "https://arxiv.org/pdf/2507.10575",
        "title": "An Adaptive Volatility-based Learning Rate Scheduler",
        "authors": [
            "Kieran Chai Kai Ren"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Effective learning rate (LR) scheduling is crucial for training deep neural networks. However, popular pre-defined and adaptive schedulers can still lead to suboptimal generalization. This paper introduces VolSched, a novel adaptive LR scheduler inspired by the concept of volatility in stochastic processes like Geometric Brownian Motion to dynamically adjust the learning rate. By calculating the ratio between long-term and short-term accuracy volatility, VolSched increases the LR to escape plateaus and decreases it to stabilize training, allowing the model to explore the loss landscape more effectively. We evaluate VolSched on the CIFAR-100 dataset against a strong baseline using a standard augmentation pipeline. When paired with ResNet-18 and ResNet-34, our scheduler delivers consistent performance gains, improving top-1 accuracy by 1.4 and 1.3 percentage points respectively. Analysis of the loss curves reveals that VolSched promotes a longer exploration phase. A quantitative analysis of the Hessian shows that VolSched finds a final solution that is 38% flatter than the next-best baseline, allowing the model to obtain wider minima and hence better generalization performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 4,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10581",
        "abs_url": "https://arxiv.org/abs/2507.10581",
        "pdf_url": "https://arxiv.org/pdf/2507.10581",
        "title": "Universal Approximation Theorem for a Single-Layer Transformer",
        "authors": [
            "Esmail Gumaan"
        ],
        "comments": "7 pages, 2 figures, 1 theorem, 10 formulas",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "Deep learning employs multi-layer neural networks trained via the backpropagation algorithm. This approach has achieved success across many domains and relies on adaptive gradient methods such as the Adam optimizer. Sequence modeling evolved from recurrent neural networks to attention-based models, culminating in the Transformer architecture. Transformers have achieved state-of-the-art performance in natural language processing (for example, BERT and GPT-3) and have been applied in computer vision and computational biology. However, theoretical understanding of these models remains limited. In this paper, we examine the mathematical foundations of deep learning and Transformers and present a novel theoretical result. We review key concepts from linear algebra, probability, and optimization that underpin deep learning, and we analyze the multi-head self-attention mechanism and the backpropagation algorithm in detail. Our main contribution is a universal approximation theorem for Transformers: we prove that a single-layer Transformer, comprising one self-attention layer followed by a position-wise feed-forward network with ReLU activation, can approximate any continuous sequence-to-sequence mapping on a compact domain to arbitrary precision. We provide a formal statement and a complete proof. Finally, we present case studies that demonstrate the practical implications of this result. Our findings advance the theoretical understanding of Transformer models and help bridge the gap between theory and practice.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 5,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10591",
        "abs_url": "https://arxiv.org/abs/2507.10591",
        "pdf_url": "https://arxiv.org/pdf/2507.10591",
        "title": "MH-FSF: A Unified Framework for Overcoming Benchmarking and Reproducibility Limitations in Feature Selection Evaluation",
        "authors": [
            "Vanderson Rocha",
            "Diego Kreutz",
            "Gabriel Canto",
            "Hendrio Bragan√ßa",
            "Eduardo Feitosa"
        ],
        "comments": "11 pages; 4 figures; 5 tables; submitted to JBCS",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Performance (cs.PF)",
        "abstract": "Feature selection is vital for building effective predictive models, as it reduces dimensionality and emphasizes key features. However, current research often suffers from limited benchmarking and reliance on proprietary datasets. This severely hinders reproducibility and can negatively impact overall performance. To address these limitations, we introduce the MH-FSF framework, a comprehensive, modular, and extensible platform designed to facilitate the reproduction and implementation of feature selection methods. Developed through collaborative research, MH-FSF provides implementations of 17 methods (11 classical, 6 domain-specific) and enables systematic evaluation on 10 publicly available Android malware datasets. Our results reveal performance variations across both balanced and imbalanced datasets, highlighting the critical need for data preprocessing and selection criteria that account for these asymmetries. We demonstrate the importance of a unified platform for comparing diverse feature selection techniques, fostering methodological consistency and rigor. By providing this framework, we aim to significantly broaden the existing literature and pave the way for new research directions in feature selection, particularly within the context of Android malware detection.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 6,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10594",
        "abs_url": "https://arxiv.org/abs/2507.10594",
        "pdf_url": "https://arxiv.org/pdf/2507.10594",
        "title": "Extension OL-MDISF: Online Learning from Mix-Typed, Drifted, and Incomplete Streaming Features",
        "authors": [
            "Shengda Zhuo",
            "Di Wu",
            "Yi He",
            "Shuqiang Huang",
            "Xindong Wu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Online learning, where feature spaces can change over time, offers a flexible learning paradigm that has attracted considerable attention. However, it still faces three significant challenges. First, the heterogeneity of real-world data streams with mixed feature types presents challenges for traditional parametric modeling. Second, data stream distributions can shift over time, causing an abrupt and substantial decline in model performance. Additionally, the time and cost constraints make it infeasible to label every data instance in a supervised setting. To overcome these challenges, we propose a new algorithm Online Learning from Mix-typed, Drifted, and Incomplete Streaming Features (OL-MDISF), which aims to relax restrictions on both feature types, data distribution, and supervision information. Our approach involves utilizing copula models to create a comprehensive latent space, employing an adaptive sliding window for detecting drift points to ensure model stability, and establishing label proximity information based on geometric structural relationships. To demonstrate the model's efficiency and effectiveness, we provide theoretical analysis and comprehensive experimental results. This extension serves as a standalone technical reference to the original OL-MDISF method. It provides (i) a contextual analysis of OL-MDISF within the broader landscape of online learning, covering recent advances in mixed-type feature modeling, concept drift adaptation, and weak supervision, and (ii) a comprehensive set of experiments across 14 real-world datasets under two types of drift scenarios. These include full CER trends, ablation studies, sensitivity analyses, and temporal ensemble dynamics. We hope this document can serve as a reproducible benchmark and technical resource for researchers working on nonstationary, heterogeneous, and weakly supervised data streams.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 7,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10595",
        "abs_url": "https://arxiv.org/abs/2507.10595",
        "pdf_url": "https://arxiv.org/pdf/2507.10595",
        "title": "Divide-Then-Rule: A Cluster-Driven Hierarchical Interpolator for Attribute-Missing Graphs",
        "authors": [
            "Yaowen Hu",
            "Wenxuan Tu",
            "Yue Liu",
            "Miaomiao Li",
            "Wenpeng Lu",
            "Zhigang Luo",
            "Xinwang Liu",
            "Ping Chen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Deep graph clustering (DGC) for attribute-missing graphs is an unsupervised task aimed at partitioning nodes with incomplete attributes into distinct clusters. Addressing this challenging issue is vital for practical applications. However, research in this area remains underexplored. Existing imputation methods for attribute-missing graphs often fail to account for the varying amounts of information available across node neighborhoods, leading to unreliable results, especially for nodes with insufficient known neighborhood. To address this issue, we propose a novel method named Divide-Then-Rule Graph Completion (DTRGC). This method first addresses nodes with sufficient known neighborhood information and treats the imputed results as new knowledge to iteratively impute more challenging nodes, while leveraging clustering information to correct imputation errors. Specifically, Dynamic Cluster-Aware Feature Propagation (DCFP) initializes missing node attributes by adjusting propagation weights based on the clustering structure. Subsequently, Hierarchical Neighborhood-aware Imputation (HNAI) categorizes attribute-missing nodes into three groups based on the completeness of their neighborhood attributes. The imputation is performed hierarchically, prioritizing the groups with nodes that have the most available neighborhood information. The cluster structure is then used to refine the imputation and correct potential errors. Finally, Hop-wise Representation Enhancement (HRE) integrates information across multiple hops, thereby enriching the expressiveness of node representations. Experimental results on six widely used graph datasets show that DTRGC significantly improves the clustering performance of various DGC methods under attribute-missing graphs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 8,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10605",
        "abs_url": "https://arxiv.org/abs/2507.10605",
        "pdf_url": "https://arxiv.org/pdf/2507.10605",
        "title": "RedOne: Revealing Domain-specific LLM Post-Training in Social Networking Services",
        "authors": [
            "Fei Zhao",
            "Chonggang Lu",
            "Yue Wang",
            "Zheyong Xie",
            "Ziyan Liu",
            "Haofu Qian",
            "JianZhao Huang",
            "Fangcheng Shi",
            "Zijie Meng",
            "Hongcheng Guo",
            "Mingqian He",
            "Xinze Lyu",
            "Yiming Lu",
            "Ziyang Xiang",
            "Zheyu Ye",
            "Chengqiang Lu",
            "Zhe Xu",
            "Yi Wu",
            "Yao Hu",
            "Yan Gao",
            "Jun Fan",
            "Xiaolong Jiang",
            "Weiting Liu",
            "Boyang Wang",
            "Shaosheng Cao"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Social and Information Networks (cs.SI)",
        "abstract": "As a primary medium for modern information dissemination, social networking services (SNS) have experienced rapid growth, which has proposed significant challenges for platform content management and interaction quality improvement. Recently, the development of large language models (LLMs) has offered potential solutions but existing studies focus on isolated tasks, which not only encounter diminishing benefit from the data scaling within individual scenarios but also fail to flexibly adapt to diverse real-world context. To address these challenges, we introduce RedOne, a domain-specific LLM designed to break the performance bottleneck of single-task baselines and establish a comprehensive foundation for the SNS. RedOne was developed through a three-stage training strategy consisting of continue pretraining, supervised fine-tuning, and preference optimization, using a large-scale real-world dataset. Through extensive experiments, RedOne maintains strong general capabilities, and achieves an average improvement up to 14.02% across 8 major SNS tasks and 7.56% in SNS bilingual evaluation benchmark, compared with base models. Furthermore, through online testing, RedOne reduced the exposure rate in harmful content detection by 11.23% and improved the click page rate in post-view search by 14.95% compared with single-tasks finetuned baseline models. These results establish RedOne as a robust domain-specific LLM for SNS, demonstrating excellent generalization across various tasks and promising applicability in real-world scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 9,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10606",
        "abs_url": "https://arxiv.org/abs/2507.10606",
        "pdf_url": "https://arxiv.org/pdf/2507.10606",
        "title": "DALI-PD: Diffusion-based Synthetic Layout Heatmap Generation for ML in Physical Design",
        "authors": [
            "Bing-Yue Wu",
            "Vidya A. Chhabria"
        ],
        "comments": "Under review at Asia and South Pacific Design Automation Conference (ASP-DAC'26)",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Hardware Architecture (cs.AR)",
        "abstract": "Machine learning (ML) has demonstrated significant promise in various physical design (PD) tasks. However, model generalizability remains limited by the availability of high-quality, large-scale training datasets. Creating such datasets is often computationally expensive and constrained by IP. While very few public datasets are available, they are typically static, slow to generate, and require frequent updates. To address these limitations, we present DALI-PD, a scalable framework for generating synthetic layout heatmaps to accelerate ML in PD research. DALI-PD uses a diffusion model to generate diverse layout heatmaps via fast inference in seconds. The heatmaps include power, IR drop, congestion, macro placement, and cell density maps. Using DALI-PD, we created a dataset comprising over 20,000 layout configurations with varying macro counts and placements. These heatmaps closely resemble real layouts and improve ML accuracy on downstream ML tasks such as IR drop or congestion prediction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 10,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10609",
        "abs_url": "https://arxiv.org/abs/2507.10609",
        "pdf_url": "https://arxiv.org/pdf/2507.10609",
        "title": "A Feed-Forward Artificial Intelligence Pipeline for Sustainable Desalination under Climate Uncertainties: UAE Insights",
        "authors": [
            "Obumneme Nwafor",
            "Chioma Nwafor",
            "Amro Zakaria",
            "Nkechi Nwankwo"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Systems and Control (eess.SY)",
        "abstract": "The United Arab Emirates (UAE) relies heavily on seawater desalination to meet over 90% of its drinking water needs. Desalination processes are highly energy intensive and account for approximately 15% of the UAE's electricity consumption, contributing to over 22% of the country's energy-related CO2 emissions. Moreover, these processes face significant sustainability challenges in the face of climate uncertainties such as rising seawater temperatures, salinity, and aerosol optical depth (AOD). AOD greatly affects the operational and economic performance of solar-powered desalination systems through photovoltaic soiling, membrane fouling, and water turbidity cycles. This study proposes a novel pipelined two-stage predictive modelling architecture: the first stage forecasts AOD using satellite-derived time series and meteorological data; the second stage uses the predicted AOD and other meteorological factors to predict desalination performance efficiency losses. The framework achieved 98% accuracy, and SHAP (SHapley Additive exPlanations) was used to reveal key drivers of system degradation. Furthermore, this study proposes a dust-aware rule-based control logic for desalination systems based on predicted values of AOD and solar efficiency. This control logic is used to adjust the desalination plant feed water pressure, adapt maintenance scheduling, and regulate energy source switching. To enhance the practical utility of the research findings, the predictive models and rule-based controls were packaged into an interactive dashboard for scenario and predictive analytics. This provides a management decision-support system for climate-adaptive planning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 11,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10611",
        "abs_url": "https://arxiv.org/abs/2507.10611",
        "pdf_url": "https://arxiv.org/pdf/2507.10611",
        "title": "FedGSCA: Medical Federated Learning with Global Sample Selector and Client Adaptive Adjuster under Label Noise",
        "authors": [
            "Mengwen Ye",
            "Yingzi Huangfu",
            "Shujian Gao",
            "Wei Ren",
            "Weifan Liu",
            "Zekuan Yu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Federated Learning (FL) emerged as a solution for collaborative medical image classification while preserving data privacy. However, label noise, which arises from inter-institutional data variability, can cause training instability and degrade model performance. Existing FL methods struggle with noise heterogeneity and the imbalance in medical data. Motivated by these challenges, we propose FedGSCA, a novel framework for enhancing robustness in noisy medical FL. FedGSCA introduces a Global Sample Selector that aggregates noise knowledge from all clients, effectively addressing noise heterogeneity and improving global model stability. Furthermore, we develop a Client Adaptive Adjustment (CAA) mechanism that combines adaptive threshold pseudo-label generation and Robust Credal Labeling Loss. CAA dynamically adjusts to class distributions, ensuring the inclusion of minority samples and carefully managing noisy labels by considering multiple plausible labels. This dual approach mitigates the impact of noisy data and prevents overfitting during local training, which improves the generalizability of the model. We evaluate FedGSCA on one real-world colon slides dataset and two synthetic medical datasets under various noise conditions, including symmetric, asymmetric, extreme, and heterogeneous types. The results show that FedGSCA outperforms the state-of-the-art methods, excelling in extreme and heterogeneous noise scenarios. Moreover, FedGSCA demonstrates significant advantages in improving model stability and handling complex noise, making it well-suited for real-world medical federated learning scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 12,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10613",
        "abs_url": "https://arxiv.org/abs/2507.10613",
        "pdf_url": "https://arxiv.org/pdf/2507.10613",
        "title": "Sub-Scaling Laws: On the Role of Data Density and Training Strategies in LLMs",
        "authors": [
            "Zhengyu Chen",
            "Siqi Wang",
            "Teng Xiao",
            "Yudong Wang",
            "Shiqi Chen",
            "Xunliang Cai",
            "Junxian He",
            "Jingang Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Traditional scaling laws in natural language processing suggest that increasing model size and training data enhances performance. However, recent studies reveal deviations, particularly in large language models, where performance improvements decelerate, which is a phenomenon known as sub-scaling. This paper revisits these scaling laws by examining the impact of data quality and training strategies on model performance. Through extensive empirical analysis of over 400 models, we identify high data density and non-optimal resource allocation as key factors contributing to sub-scaling. High data density leads to diminishing returns due to redundant information, while optimal resource allocation is crucial for sustained performance improvements. We propose a sub-optimal scaling law that better predicts performance in sub-scaling regimes, highlighting the importance of data quality and diversity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 13,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10614",
        "abs_url": "https://arxiv.org/abs/2507.10614",
        "pdf_url": "https://arxiv.org/pdf/2507.10614",
        "title": "Fine-tuning Large Language Model for Automated Algorithm Design",
        "authors": [
            "Fei Liu",
            "Rui Zhang",
            "Xi Lin",
            "Zhichao Lu",
            "Qingfu Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "The integration of large language models (LLMs) into automated algorithm design has shown promising potential. A prevalent approach embeds LLMs within search routines to iteratively generate and refine candidate algorithms. However, most existing methods rely on off-the-shelf LLMs trained for general coding tasks,leaving a key question open: Do we need LLMs specifically tailored for algorithm design? If so, how can such LLMs be effectively obtained and how well can they generalize across different algorithm design tasks? In this paper, we take a first step toward answering these questions by exploring fine-tuning of LLMs for algorithm design. We introduce a Diversity-Aware Rank based (DAR) sampling strategy to balance training data diversity and quality, then we leverage direct preference optimization to efficiently align LLM outputs with task objectives. Our experiments, conducted on Llama-3.2-1B-Instruct and Llama- 3.1-8B-Instruct, span three distinct algorithm design tasks. Results suggest that finetuned LLMs can significantly outperform their off-the-shelf counterparts with the smaller Llama-3.2-1B-Instruct and match the larger Llama-3.1-8B-Instruct on the admissible set problem. Moreover, we observe promising generalization: LLMs finetuned on specific algorithm design tasks also improve performance on related tasks with varying settings. These findings highlight the value of task-specific adaptation for LLMs in algorithm design and open new avenues for future research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 14,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10616",
        "abs_url": "https://arxiv.org/abs/2507.10616",
        "pdf_url": "https://arxiv.org/pdf/2507.10616",
        "title": "Scalpel vs. Hammer: GRPO Amplifies Existing Capabilities, SFT Replaces Them",
        "authors": [
            "Neel Rajani",
            "Aryo Pradipta Gema",
            "Seraphina Goldfarb-Tarrant",
            "Ivan Titov"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Training large language models (LLMs) for reasoning via maths and code datasets has become a major new focus in LLM post-training. Two particularly popular approaches are reinforcement learning (RL) and supervised fine-tuning (SFT), but their training dynamics are poorly understood. We present a comparative analysis of RL and SFT on the same maths problems with the same model and similar hyperparameters. We find that RL yields minor in-domain gains on maths and slight degradation on knowledge-intensive benchmarks like MMLU, while both trends are more pronounced in SFT. We also analyse model parameters across checkpoints, observing that both algorithms modify query and key weights the most. Meanwhile, SFT exhibits greater updates and also affects mid-layer MLPs more, leading us to hypothesise that this may have caused the out-of-domain degradation. We therefore investigate whether freezing parts of the model during training can mitigate the reduced performance on knowledge-intensive benchmarks. However, our results are inconclusive, with benefits on GPQA:Diamond and degradation on other benchmarks. Taken together, our observations provide a preliminary indication for why RL amplifies existing capabilities, while SFT replaces old skills with new ones.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 15,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10618",
        "abs_url": "https://arxiv.org/abs/2507.10618",
        "pdf_url": "https://arxiv.org/pdf/2507.10618",
        "title": "Compute Requirements for Algorithmic Innovation in Frontier AI Models",
        "authors": [
            "Peter Barnett"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Algorithmic innovation in the pretraining of large language models has driven a massive reduction in the total compute required to reach a given level of capability. In this paper we empirically investigate the compute requirements for developing algorithmic innovations. We catalog 36 pre-training algorithmic innovations used in Llama 3 and DeepSeek-V3. For each innovation we estimate both the total FLOP used in development and the FLOP/s of the hardware utilized. Innovations using significant resources double in their requirements each year. We then use this dataset to investigate the effect of compute caps on innovation. Our analysis suggests that compute caps alone are unlikely to dramatically slow AI algorithmic progress. Even stringent compute caps -- such as capping total operations to the compute used to train GPT-2 or capping hardware capacity to 8 H100 GPUs -- could still have allowed for half of the cataloged innovations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 16,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10619",
        "abs_url": "https://arxiv.org/abs/2507.10619",
        "pdf_url": "https://arxiv.org/pdf/2507.10619",
        "title": "Meta-Reinforcement Learning for Fast and Data-Efficient Spectrum Allocation in Dynamic Wireless Networks",
        "authors": [
            "Oluwaseyi Giwa",
            "Tobi Awodunmila",
            "Muhammad Ahmed Mohsin",
            "Ahsan Bilal",
            "Muhammad Ali Jamshed"
        ],
        "comments": "5 pages, 6 figures, under review at IEEE Wireless Communications Letters",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Networking and Internet Architecture (cs.NI)",
        "abstract": "The dynamic allocation of spectrum in 5G / 6G networks is critical to efficient resource utilization. However, applying traditional deep reinforcement learning (DRL) is often infeasible due to its immense sample complexity and the safety risks associated with unguided exploration, which can cause severe network interference. To address these challenges, we propose a meta-learning framework that enables agents to learn a robust initial policy and rapidly adapt to new wireless scenarios with minimal data. We implement three meta-learning architectures, model-agnostic meta-learning (MAML), recurrent neural network (RNN), and an attention-enhanced RNN, and evaluate them against a non-meta-learning DRL algorithm, proximal policy optimization (PPO) baseline, in a simulated dynamic integrated access/backhaul (IAB) environment. Our results show a clear performance gap. The attention-based meta-learning agent reaches a peak mean network throughput of 48 Mbps, while the PPO baseline decreased drastically to 10 Mbps. Furthermore, our method reduces SINR and latency violations by more than 50% compared to PPO. It also shows quick adaptation, with a fairness index 0.7, showing better resource allocation. This work proves that meta-learning is a very effective and safer option for intelligent control in complex wireless systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 17,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10620",
        "abs_url": "https://arxiv.org/abs/2507.10620",
        "pdf_url": "https://arxiv.org/pdf/2507.10620",
        "title": "LLMs Meet Cross-Modal Time Series Analytics: Overview and Directions",
        "authors": [
            "Chenxi Liu",
            "Hao Miao",
            "Cheng Long",
            "Yan Zhao",
            "Ziyue Li",
            "Panos Kalnis"
        ],
        "comments": "Accepted at SSTD 2025 (Tutorial). arXiv admin note: text overlap with arXiv:2505.02583",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Large Language Models (LLMs) have emerged as a promising paradigm for time series analytics, leveraging their massive parameters and the shared sequential nature of textual and time series data. However, a cross-modality gap exists between time series and textual data, as LLMs are pre-trained on textual corpora and are not inherently optimized for time series. In this tutorial, we provide an up-to-date overview of LLM-based cross-modal time series analytics. We introduce a taxonomy that classifies existing approaches into three groups based on cross-modal modeling strategies, e.g., conversion, alignment, and fusion, and then discuss their applications across a range of downstream tasks. In addition, we summarize several open challenges. This tutorial aims to expand the practical application of LLMs in solving real-world problems in cross-modal time series analytics while balancing effectiveness and efficiency. Participants will gain a thorough understanding of current advancements, methodologies, and future research directions in cross-modal time series analytics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 18,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10623",
        "abs_url": "https://arxiv.org/abs/2507.10623",
        "pdf_url": "https://arxiv.org/pdf/2507.10623",
        "title": "Flows and Diffusions on the Neural Manifold",
        "authors": [
            "Daniel Saragih",
            "Deyu Cao",
            "Tejas Balaji"
        ],
        "comments": "40 pages, 6 figures, 13 tables",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Diffusion and flow-based generative models have achieved remarkable success in domains such as image synthesis, video generation, and natural language modeling. In this work, we extend these advances to weight space learning by leveraging recent techniques to incorporate structural priors derived from optimization dynamics. Central to our approach is modeling the trajectory induced by gradient descent as a trajectory inference problem. We unify several trajectory inference techniques under the framework of gradient flow matching, providing a theoretical framework for treating optimization paths as inductive bias. We further explore architectural and algorithmic choices, including reward fine-tuning by adjoint matching, the use of autoencoders for latent weight representation, conditioning on task-specific context data, and adopting informative source distributions such as Kaiming uniform. Experiments demonstrate that our method matches or surpasses baselines in generating in-distribution weights, improves initialization for downstream training, and supports fine-tuning to enhance performance. Finally, we illustrate a practical application in safety-critical systems: detecting harmful covariate shifts, where our method outperforms the closest comparable baseline.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 19,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10626",
        "abs_url": "https://arxiv.org/abs/2507.10626",
        "pdf_url": "https://arxiv.org/pdf/2507.10626",
        "title": "Player-Team Heterogeneous Interaction Graph Transformer for Soccer Outcome Prediction",
        "authors": [
            "Lintao Wang",
            "Shiwen Xu",
            "Michael Horton",
            "Joachim Gudmundsson",
            "Zhiyong Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Predicting soccer match outcomes is a challenging task due to the inherently unpredictable nature of the game and the numerous dynamic factors influencing results. While it conventionally relies on meticulous feature engineering, deep learning techniques have recently shown a great promise in learning effective player and team representations directly for soccer outcome prediction. However, existing methods often overlook the heterogeneous nature of interactions among players and teams, which is crucial for accurately modeling match dynamics. To address this gap, we propose HIGFormer (Heterogeneous Interaction Graph Transformer), a novel graph-augmented transformer-based deep learning model for soccer outcome prediction. HIGFormer introduces a multi-level interaction framework that captures both fine-grained player dynamics and high-level team interactions. Specifically, it comprises (1) a Player Interaction Network, which encodes player performance through heterogeneous interaction graphs, combining local graph convolutions with a global graph-augmented transformer; (2) a Team Interaction Network, which constructs interaction graphs from a team-to-team perspective to model historical match relationships; and (3) a Match Comparison Transformer, which jointly analyzes both team and player-level information to predict match outcomes. Extensive experiments on the WyScout Open Access Dataset, a large-scale real-world soccer dataset, demonstrate that HIGFormer significantly outperforms existing methods in prediction accuracy. Furthermore, we provide valuable insights into leveraging our model for player performance evaluation, offering a new perspective on talent scouting and team strategy analysis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 20,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10628",
        "abs_url": "https://arxiv.org/abs/2507.10628",
        "pdf_url": "https://arxiv.org/pdf/2507.10628",
        "title": "GHPO: Adaptive Guidance for Stable and Efficient LLM Reinforcement Learning",
        "authors": [
            "Ziru Liu",
            "Cheng Gong",
            "Xinyu Fu",
            "Yaofang Liu",
            "Ran Chen",
            "Shoubo Hu",
            "Suiyun Zhang",
            "Rui Liu",
            "Qingfu Zhang",
            "Dandan Tu"
        ],
        "comments": "Code avaiable at this https URL",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a powerful paradigm for facilitating the self-improvement of large language models (LLMs), particularly in the domain of complex reasoning tasks. However, prevailing on-policy RL methods often contend with significant training instability and inefficiency. This is primarily due to a capacity-difficulty mismatch, where the complexity of training data frequently outpaces the model's current capabilities, leading to critically sparse reward signals and stalled learning progress. This challenge is particularly acute for smaller, more resource-efficient LLMs. To overcome this, we introduce the Guided Hybrid Policy Optimization (GHPO), a novel difficulty-aware reinforcement learning framework. GHPO dynamically calibrates task difficulty by employing adaptive prompt refinement to provide targeted guidance. This unique approach adaptively balances direct imitation learning for problems currently beyond the model's reach with exploration-based reinforcement learning for more manageable tasks, effectively creating a smooth and optimized learning curriculum. Extensive experiments demonstrate that GHPO achieves an average performance gain of approximately 5% across six challenging mathematics benchmarks, consistently outperforming strong on-policy reinforcement learning and curriculum learning baselines. Further analysis confirms that our framework significantly enhances both training stability and final reasoning performance, thus offering a scalable and efficient solution for developing powerful and robust reasoning models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 21,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10632",
        "abs_url": "https://arxiv.org/abs/2507.10632",
        "pdf_url": "https://arxiv.org/pdf/2507.10632",
        "title": "Scalable Unsupervised Segmentation via Random Fourier Feature-based Gaussian Process",
        "authors": [
            "Issei Saito",
            "Masatoshi Nagano",
            "Tomoaki Nakamura",
            "Daichi Mochihashi",
            "Koki Mimura"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "In this paper, we propose RFF-GP-HSMM, a fast unsupervised time-series segmentation method that incorporates random Fourier features (RFF) to address the high computational cost of the Gaussian process hidden semi-Markov model (GP-HSMM). GP-HSMM models time-series data using Gaussian processes, requiring inversion of an N times N kernel matrix during training, where N is the number of data points. As the scale of the data increases, matrix inversion incurs a significant computational cost. To address this, the proposed method approximates the Gaussian process with linear regression using RFF, preserving expressive power while eliminating the need for inversion of the kernel matrix. Experiments on the Carnegie Mellon University (CMU) motion-capture dataset demonstrate that the proposed method achieves segmentation performance comparable to that of conventional methods, with approximately 278 times faster segmentation on time-series data comprising 39,200 frames.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 22,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10636",
        "abs_url": "https://arxiv.org/abs/2507.10636",
        "pdf_url": "https://arxiv.org/pdf/2507.10636",
        "title": "GeoHopNet: Hopfield-Augmented Sparse Spatial Attention for Dynamic UAV Site Location Problem",
        "authors": [
            "Jianing Zhi",
            "Xinghua Li",
            "Zidong Chen"
        ],
        "comments": "12 Pages, 5 Figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE); Robotics (cs.RO)",
        "abstract": "The rapid development of urban low-altitude unmanned aerial vehicle (UAV) economy poses new challenges for dynamic site selection of UAV landing points and supply stations. Traditional deep reinforcement learning methods face computational complexity bottlenecks, particularly with standard attention mechanisms, when handling large-scale urban-level location problems. This paper proposes GeoHopNet, a Hopfield-augmented sparse spatial attention network specifically designed for dynamic UAV site location problems. Our approach introduces four core innovations: (1) distance-biased multi-head attention mechanism that explicitly encodes spatial geometric information; (2) K-nearest neighbor sparse attention that reduces computational complexity from $O(N^2)$ to $O(NK)$; (3) a modern Hopfield external memory module; and (4) a memory regularization strategy. Experimental results demonstrate that GeoHopNet extends the boundary of solvable problem sizes. For large-scale instances with 1,000 nodes, where standard attention models become prohibitively slow (over 3 seconds per instance) and traditional solvers fail, GeoHopNet finds high-quality solutions (0.22\\% optimality gap) in under 0.1 seconds. Compared to the state-of-the-art ADNet baseline on 100-node instances, our method improves solution quality by 22.2\\% and is 1.8$\\times$ faster.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 23,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10637",
        "abs_url": "https://arxiv.org/abs/2507.10637",
        "pdf_url": "https://arxiv.org/pdf/2507.10637",
        "title": "A Simple Baseline for Stable and Plastic Neural Networks",
        "authors": [
            "√â. K√ºnzel",
            "A. Jaziri",
            "V. Ramesh"
        ],
        "comments": "11 pages, 50 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Continual learning in computer vision requires that models adapt to a continuous stream of tasks without forgetting prior knowledge, yet existing approaches often tip the balance heavily toward either plasticity or stability. We introduce RDBP, a simple, low-overhead baseline that unites two complementary mechanisms: ReLUDown, a lightweight activation modification that preserves feature sensitivity while preventing neuron dormancy, and Decreasing Backpropagation, a biologically inspired gradient-scheduling scheme that progressively shields early layers from catastrophic updates. Evaluated on the Continual ImageNet benchmark, RDBP matches or exceeds the plasticity and stability of state-of-the-art methods while reducing computational cost. RDBP thus provides both a practical solution for real-world continual learning and a clear benchmark against which future continual learning strategies can be measured.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 24,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10638",
        "abs_url": "https://arxiv.org/abs/2507.10638",
        "pdf_url": "https://arxiv.org/pdf/2507.10638",
        "title": "ZClassifier: Temperature Tuning and Manifold Approximation via KL Divergence on Logit Space",
        "authors": [
            "Shim Soon Yong"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We introduce a novel classification framework, ZClassifier, that replaces conventional deterministic logits with diagonal Gaussian-distributed logits. Our method simultaneously addresses temperature scaling and manifold approximation by minimizing the Kullback-Leibler (KL) divergence between the predicted Gaussian distributions and a unit isotropic Gaussian. This unifies uncertainty calibration and latent control in a principled probabilistic manner, enabling a natural interpretation of class confidence and geometric consistency. Experiments on CIFAR-10 show that ZClassifier improves over softmax classifiers in robustness, calibration, and latent separation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 25,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10642",
        "abs_url": "https://arxiv.org/abs/2507.10642",
        "pdf_url": "https://arxiv.org/pdf/2507.10642",
        "title": "First-of-its-kind AI model for bioacoustic detection using a lightweight associative memory Hopfield neural network",
        "authors": [
            "Andrew Gascoyne",
            "Wendy Lomas"
        ],
        "comments": "12 pages, 5 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "A growing issue within conservation bioacoustics is the task of analysing the vast amount of data generated from the use of passive acoustic monitoring devices. In this paper, we present an alternative AI model which has the potential to help alleviate this problem. Our model formulation addresses the key issues encountered when using current AI models for bioacoustic analysis, namely the: limited training data available; environmental impact, particularly in energy consumption and carbon footprint of training and implementing these models; and associated hardware requirements. The model developed in this work uses associative memory via a transparent, explainable Hopfield neural network to store signals and detect similar signals which can then be used to classify species. Training is rapid ($3$\\,ms), as only one representative signal is required for each target sound within a dataset. The model is fast, taking only $5.4$\\,s to pre-process and classify all $10384$ publicly available bat recordings, on a standard Apple MacBook Air. The model is also lightweight with a small memory footprint of $144.09$\\,MB of RAM usage. Hence, the low computational demands make the model ideal for use on a variety of standard personal devices with potential for deployment in the field via edge-processing devices. It is also competitively accurate, with up to $86\\%$ precision on the dataset used to evaluate the model. In fact, we could not find a single case of disagreement between model and manual identification via expert field guides. Although a dataset of bat echolocation calls was chosen to demo this first-of-its-kind AI model, trained on only two representative calls, the model is not species specific. In conclusion, we propose an equitable AI model that has the potential to be a game changer for fast, lightweight, sustainable, transparent, explainable and accurate bioacoustic analysis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 26,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10678",
        "abs_url": "https://arxiv.org/abs/2507.10678",
        "pdf_url": "https://arxiv.org/pdf/2507.10678",
        "title": "A Group Theoretic Analysis of the Symmetries Underlying Base Addition and Their Learnability by Neural Networks",
        "authors": [
            "Cutter Dawes",
            "Simon Segert",
            "Kamesh Krishnamurthy",
            "Jonathan D. Cohen"
        ],
        "comments": "22 pages, 6 figures; typos corrected",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE); Neurons and Cognition (q-bio.NC)",
        "abstract": "A major challenge in the use of neural networks both for modeling human cognitive function and for artificial intelligence is the design of systems with the capacity to efficiently learn functions that support radical generalization. At the roots of this is the capacity to discover and implement symmetry functions. In this paper, we investigate a paradigmatic example of radical generalization through the use of symmetry: base addition. We present a group theoretic analysis of base addition, a fundamental and defining characteristic of which is the carry function -- the transfer of the remainder, when a sum exceeds the base modulus, to the next significant place. Our analysis exposes a range of alternative carry functions for a given base, and we introduce quantitative measures to characterize these. We then exploit differences in carry functions to probe the inductive biases of neural networks in symmetry learning, by training neural networks to carry out base addition using different carries, and comparing efficacy and rate of learning as a function of their structure. We find that even simple neural networks can achieve radical generalization with the right input format and carry function, and that learnability is closely correlated with carry function structure. We then discuss the relevance this has for cognitive science and machine learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 27,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10714",
        "abs_url": "https://arxiv.org/abs/2507.10714",
        "pdf_url": "https://arxiv.org/pdf/2507.10714",
        "title": "A Simple Approximate Bayesian Inference Neural Surrogate for Stochastic Petri Net Models",
        "authors": [
            "Bright Kwaku Manu",
            "Trevor Reckell",
            "Beckett Sterner",
            "Petar Jevtic"
        ],
        "comments": "12 pages, 10 figures, for all associated codes and files, see this https URL",
        "subjects": "Machine Learning (cs.LG); Quantitative Methods (q-bio.QM); Machine Learning (stat.ML)",
        "abstract": "Stochastic Petri Nets (SPNs) are an increasingly popular tool of choice for modeling discrete-event dynamics in areas such as epidemiology and systems biology, yet their parameter estimation remains challenging in general and in particular when transition rates depend on external covariates and explicit likelihoods are unavailable. We introduce a neural-surrogate (neural-network--based approximation of the posterior distribution) framework that predicts the coefficients of known covariate-dependent rate functions directly from noisy, partially observed token trajectories. Our model employs a lightweight 1D Convolutional Residual Network trained end-to-end on Gillespie-simulated SPN realizations, learning to invert system dynamics under realistic conditions of event dropout. During inference, Monte Carlo dropout provides calibrated uncertainty bounds together with point estimates. On synthetic SPNs with 20% missing events, our surrogate recovers rate-function coefficients with an RMSE = 0.108 and substantially runs faster than traditional Bayesian approaches. These results demonstrate that data-driven, likelihood-free surrogates can enable accurate, robust, and real-time parameter recovery in complex, partially observed discrete-event systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 28,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10718",
        "abs_url": "https://arxiv.org/abs/2507.10718",
        "pdf_url": "https://arxiv.org/pdf/2507.10718",
        "title": "Distributionally Robust Optimization with Adversarial Data Contamination",
        "authors": [
            "Shuyao Li",
            "Ilias Diakonikolas",
            "Jelena Diakonikolas"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Data Structures and Algorithms (cs.DS); Optimization and Control (math.OC)",
        "abstract": "Distributionally Robust Optimization (DRO) provides a framework for decision-making under distributional uncertainty, yet its effectiveness can be compromised by outliers in the training data. This paper introduces a principled approach to simultaneously address both challenges. We focus on optimizing Wasserstein-1 DRO objectives for generalized linear models with convex Lipschitz loss functions, where an $\\epsilon$-fraction of the training data is adversarially corrupted. Our primary contribution lies in a novel modeling framework that integrates robustness against training data contamination with robustness against distributional shifts, alongside an efficient algorithm inspired by robust statistics to solve the resulting optimization problem. We prove that our method achieves an estimation error of $O(\\sqrt{\\epsilon})$ for the true DRO objective value using only the contaminated data under the bounded covariance assumption. This work establishes the first rigorous guarantees, supported by efficient computation, for learning under the dual challenges of data contamination and distributional shifts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 29,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10741",
        "abs_url": "https://arxiv.org/abs/2507.10741",
        "pdf_url": "https://arxiv.org/pdf/2507.10741",
        "title": "Ground-Compose-Reinforce: Tasking Reinforcement Learning Agents through Formal Language",
        "authors": [
            "Andrew C. Li",
            "Toryn Q. Klassen",
            "Andrew Wang",
            "Parand A. Alamdari",
            "Sheila A. McIlraith"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Grounding language in complex perception (e.g. pixels) and action is a key challenge when building situated agents that can interact with humans via language. In past works, this is often solved via manual design of the language grounding or by curating massive datasets relating language to elements of the environment. We propose Ground-Compose-Reinforce, a neurosymbolic framework for grounding formal language from data, and eliciting behaviours by directly tasking RL agents through this language. By virtue of data-driven learning, our framework avoids the manual design of domain-specific elements like reward functions or symbol detectors. By virtue of compositional formal language semantics, our framework achieves data-efficient grounding and generalization to arbitrary language compositions. Experiments on an image-based gridworld and a MuJoCo robotics domain show that our approach reliably maps formal language instructions to behaviours with limited data while end-to-end, data-driven approaches fail.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 30,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10747",
        "abs_url": "https://arxiv.org/abs/2507.10747",
        "pdf_url": "https://arxiv.org/pdf/2507.10747",
        "title": "A Benchmarking Framework for AI models in Automotive Aerodynamics",
        "authors": [
            "Kaustubh Tangsali",
            "Rishikesh Ranade",
            "Mohammad Amin Nabian",
            "Alexey Kamenev",
            "Peter Sharpe",
            "Neil Ashton",
            "Ram Cherukuri",
            "Sanjay Choudhry"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In this paper, we introduce a benchmarking framework within the open-source NVIDIA PhysicsNeMo-CFD framework designed to systematically assess the accuracy, performance, scalability, and generalization capabilities of AI models for automotive aerodynamics predictions. The open extensible framework enables incorporation of a diverse set of metrics relevant to the Computer-Aided Engineering (CAE) community. By providing a standardized methodology for comparing AI models, the framework enhances transparency and consistency in performance assessment, with the overarching goal of improving the understanding and development of these models to accelerate research and innovation in the field. To demonstrate its utility, the framework includes evaluation of both surface and volumetric flow field predictions on three AI models: DoMINO, X-MeshGraphNet, and FIGConvNet using the DrivAerML dataset. It also includes guidelines for integrating additional models and datasets, making it extensible for physically consistent metrics. This benchmarking study aims to enable researchers and industry professionals in selecting, refining, and advancing AI-driven aerodynamic modeling approaches, ultimately fostering the development of more efficient, accurate, and interpretable solutions in automotive aerodynamics",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 31,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10768",
        "abs_url": "https://arxiv.org/abs/2507.10768",
        "pdf_url": "https://arxiv.org/pdf/2507.10768",
        "title": "Spatial Reasoners for Continuous Variables in Any Domain",
        "authors": [
            "Bart Pogodzinski",
            "Christopher Wewer",
            "Bernt Schiele",
            "Jan Eric Lenssen"
        ],
        "comments": "For the project documentation see this https URL . The SRM project website is available at this https URL . The work was published on ICML 2025 CODEML workshop",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We present Spatial Reasoners, a software framework to perform spatial reasoning over continuous variables with generative denoising models. Denoising generative models have become the de-facto standard for image generation, due to their effectiveness in sampling from complex, high-dimensional distributions. Recently, they have started being explored in the context of reasoning over multiple continuous variables. Providing infrastructure for generative reasoning with such models requires a high effort, due to a wide range of different denoising formulations, samplers, and inference strategies. Our presented framework aims to facilitate research in this area, providing easy-to-use interfaces to control variable mapping from arbitrary data domains, generative model paradigms, and inference strategies. Spatial Reasoners are openly available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 32,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10792",
        "abs_url": "https://arxiv.org/abs/2507.10792",
        "pdf_url": "https://arxiv.org/pdf/2507.10792",
        "title": "A Generalizable Physics-Enhanced State Space Model for Long-Term Dynamics Forecasting in Complex Environments",
        "authors": [
            "Yuchen Wang",
            "Hongjue Zhao",
            "Haohong Lin",
            "Enze Xu",
            "Lifang He",
            "Huajie Shao"
        ],
        "comments": "8 pages, 6 figures, accepted in ICML 2025",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "This work aims to address the problem of long-term dynamic forecasting in complex environments where data are noisy and irregularly sampled. While recent studies have introduced some methods to improve prediction performance, these approaches still face a significant challenge in handling long-term extrapolation tasks under such complex scenarios. To overcome this challenge, we propose Phy-SSM, a generalizable method that integrates partial physics knowledge into state space models (SSMs) for long-term dynamics forecasting in complex environments. Our motivation is that SSMs can effectively capture long-range dependencies in sequential data and model continuous dynamical systems, while the incorporation of physics knowledge improves generalization ability. The key challenge lies in how to seamlessly incorporate partially known physics into SSMs. To achieve this, we decompose partially known system dynamics into known and unknown state matrices, which are integrated into a Phy-SSM unit. To further enhance long-term prediction performance, we introduce a physics state regularization term to make the estimated latent states align with system dynamics. Besides, we theoretically analyze the uniqueness of the solutions for our method. Extensive experiments on three real-world applications, including vehicle motion prediction, drone state prediction, and COVID-19 epidemiology forecasting, demonstrate the superior performance of Phy-SSM over the baselines in both long-term interpolation and extrapolation tasks. The code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 33,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10797",
        "abs_url": "https://arxiv.org/abs/2507.10797",
        "pdf_url": "https://arxiv.org/pdf/2507.10797",
        "title": "Multi-Armed Sampling Problem and the End of Exploration",
        "authors": [
            "Mohammad Pedramfar",
            "Siamak Ravanbakhsh"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Optimization and Control (math.OC); Machine Learning (stat.ML)",
        "abstract": "This paper introduces the framework of multi-armed sampling, as the sampling counterpart to the optimization problem of multi-arm bandits. Our primary motivation is to rigorously examine the exploration-exploitation trade-off in the context of sampling. We systematically define plausible notions of regret for this framework and establish corresponding lower bounds. We then propose a simple algorithm that achieves these optimal regret bounds. Our theoretical results demonstrate that in contrast to optimization, sampling does not require exploration. To further connect our findings with those of multi-armed bandits, we define a continuous family of problems and associated regret measures that smoothly interpolates and unifies multi-armed sampling and multi-armed bandit problems using a temperature parameter. We believe the multi-armed sampling framework, and our findings in this setting can have a foundational role in the study of sampling including recent neural samplers, akin to the role of multi-armed bandits in reinforcement learning. In particular, our work sheds light on the need for exploration and the convergence properties of algorithm for entropy-regularized reinforcement learning, fine-tuning of pretrained models and reinforcement learning with human feedback (RLHF).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 34,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10809",
        "abs_url": "https://arxiv.org/abs/2507.10809",
        "pdf_url": "https://arxiv.org/pdf/2507.10809",
        "title": "Uncovering Causal Relation Shifts in Event Sequences under Out-of-Domain Interventions",
        "authors": [
            "Kazi Tasnim Zinat",
            "Yun Zhou",
            "Xiang Lyu",
            "Yawei Wang",
            "Zhicheng Liu",
            "Panpan Xu"
        ],
        "comments": "Accepted at ICANN 2025",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Inferring causal relationships between event pairs in a temporal sequence is applicable in many domains such as healthcare, manufacturing, and transportation. Most existing work on causal inference primarily focuses on event types within the designated domain, without considering the impact of exogenous out-of-domain interventions. In real-world settings, these out-of-domain interventions can significantly alter causal dynamics. To address this gap, we propose a new causal framework to define average treatment effect (ATE), beyond independent and identically distributed (i.i.d.) data in classic Rubin's causal framework, to capture the causal relation shift between events of temporal process under out-of-domain intervention. We design an unbiased ATE estimator, and devise a Transformer-based neural network model to handle both long-range temporal dependencies and local patterns while integrating out-of-domain intervention information into process modeling. Extensive experiments on both simulated and real-world datasets demonstrate that our method outperforms baselines in ATE estimation and goodness-of-fit under out-of-domain-augmented point processes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 35,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10820",
        "abs_url": "https://arxiv.org/abs/2507.10820",
        "pdf_url": "https://arxiv.org/pdf/2507.10820",
        "title": "Semantic Context for Tool Orchestration",
        "authors": [
            "Robert M√ºller"
        ],
        "comments": "Workshop on Computer Use Agents @ ICML2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "This paper demonstrates that Semantic Context (SC), leveraging descriptive tool information, is a foundational component for robust tool orchestration. Our contributions are threefold. First, we provide a theoretical foundation using contextual bandits, introducing SC-LinUCB and proving it achieves lower regret and adapts favourably in dynamic action spaces. Second, we provide parallel empirical validation with Large Language Models, showing that SC is critical for successful in-context learning in both static (efficient learning) and non-stationary (robust adaptation) settings. Third, we propose the FiReAct pipeline, and demonstrate on a benchmark with over 10,000 tools that SC-based retrieval enables an LLM to effectively orchestrate over a large action space. These findings provide a comprehensive guide to building more sample-efficient, adaptive, and scalable orchestration agents.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 36,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10834",
        "abs_url": "https://arxiv.org/abs/2507.10834",
        "pdf_url": "https://arxiv.org/pdf/2507.10834",
        "title": "From Small to Large: A Graph Convolutional Network Approach for Solving Assortment Optimization Problems",
        "authors": [
            "Guokai Li",
            "Pin Gao",
            "Stefanus Jasin",
            "Zizhuo Wang"
        ],
        "comments": "Conference version. The journal version will be updated soon",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Assortment optimization involves selecting a subset of substitutable products (subject to certain constraints) to maximize the expected revenue. It is a classic problem in revenue management and finds applications across various industries. However, the problem is usually NP-hard due to its combinatorial and non-linear nature. In this work, we explore how graph concolutional networks (GCNs) can be leveraged to efficiently solve constrained assortment optimization under the mixed multinomial logit choice model. We first develop a graph representation of the assortment problem, then train a GCN to learn the patterns of optimal assortments, and lastly propose two inference policies based on the GCN's output. Due to the GCN's inherent ability to generalize across inputs of varying sizes, we can use a GCN trained on small-scale instances to facilitate large-scale instances. Extensive numerical experiments demonstrate that given a GCN trained on small-scale instances (e.g., with 20 products), the proposed policies can achieve superior performance (90%+ optimality) on large-scale instances (with up to 2,000 products) within seconds, which outperform existing heuristic policies in both performance and efficiency. Furthermore, we extend our framework to a model-free setting where the underlying choice model is unknown but transaction data is available. We also conduct numerical experiments to demonstrate the effectiveness and efficiency of our proposed policies in this setting.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 37,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10843",
        "abs_url": "https://arxiv.org/abs/2507.10843",
        "pdf_url": "https://arxiv.org/pdf/2507.10843",
        "title": "Offline Reinforcement Learning with Wasserstein Regularization via Optimal Transport Maps",
        "authors": [
            "Motoki Omura",
            "Yusuke Mukuta",
            "Kazuki Ota",
            "Takayuki Osa",
            "Tatsuya Harada"
        ],
        "comments": "Accepted at RLC 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Robotics (cs.RO)",
        "abstract": "Offline reinforcement learning (RL) aims to learn an optimal policy from a static dataset, making it particularly valuable in scenarios where data collection is costly, such as robotics. A major challenge in offline RL is distributional shift, where the learned policy deviates from the dataset distribution, potentially leading to unreliable out-of-distribution actions. To mitigate this issue, regularization techniques have been employed. While many existing methods utilize density ratio-based measures, such as the $f$-divergence, for regularization, we propose an approach that utilizes the Wasserstein distance, which is robust to out-of-distribution data and captures the similarity between actions. Our method employs input-convex neural networks (ICNNs) to model optimal transport maps, enabling the computation of the Wasserstein distance in a discriminator-free manner, thereby avoiding adversarial training and ensuring stable learning. Our approach demonstrates comparable or superior performance to widely used existing methods on the D4RL benchmark dataset. The code is available at this https URL .",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 38,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10861",
        "abs_url": "https://arxiv.org/abs/2507.10861",
        "pdf_url": "https://arxiv.org/pdf/2507.10861",
        "title": "Visually grounded emotion regulation via diffusion models and user-driven reappraisal",
        "authors": [
            "Edoardo Pinzuti",
            "Oliver T√ºscher",
            "Andr√© Ferreira Castro"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Cognitive reappraisal is a key strategy in emotion regulation, involving reinterpretation of emotionally charged stimuli to alter affective responses. Despite its central role in clinical and cognitive science, real-world reappraisal interventions remain cognitively demanding, abstract, and primarily verbal. This reliance on higher-order cognitive and linguistic processes is often impaired in individuals with trauma or depression, limiting the effectiveness of standard approaches. Here, we propose a novel, visually based augmentation of cognitive reappraisal by integrating large-scale text-to-image diffusion models into the emotional regulation process. Specifically, we introduce a system in which users reinterpret emotionally negative images via spoken reappraisals, which are transformed into supportive, emotionally congruent visualizations using stable diffusion models with a fine-tuned IP-adapter. This generative transformation visually instantiates users' reappraisals while maintaining structural similarity to the original stimuli, externalizing and reinforcing regulatory intent. To test this approach, we conducted a within-subject experiment (N = 20) using a modified cognitive emotion regulation (CER) task. Participants reappraised or described aversive images from the International Affective Picture System (IAPS), with or without AI-generated visual feedback. Results show that AI-assisted reappraisal significantly reduced negative affect compared to both non-AI and control conditions. Further analyses reveal that sentiment alignment between participant reappraisals and generated images correlates with affective relief, suggesting that multimodal coherence enhances regulatory efficacy. These findings demonstrate that generative visual input can support cogitive reappraisal and open new directions at the intersection of generative AI, affective computing, and therapeutic technology.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 39,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10871",
        "abs_url": "https://arxiv.org/abs/2507.10871",
        "pdf_url": "https://arxiv.org/pdf/2507.10871",
        "title": "GALDS: A Graph-Autoencoder-based Latent Dynamics Surrogate model to predict neurite material transport",
        "authors": [
            "Tsung Yeh Hsieh",
            "Yongjie Jessica Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Numerical Analysis (math.NA); Medical Physics (physics.med-ph)",
        "abstract": "Neurons exhibit intricate geometries within their neurite networks, which play a crucial role in processes such as signaling and nutrient transport. Accurate simulation of material transport in the networks is essential for understanding these biological phenomena but poses significant computational challenges because of the complex tree-like structures involved. Traditional approaches are time-intensive and resource-demanding, yet the inherent properties of neuron trees, which consists primarily of pipes with steady-state parabolic velocity profiles and bifurcations, provide opportunities for computational optimization. To address these challenges, we propose a Graph-Autoencoder-based Latent Dynamics Surrogate (GALDS) model, which is specifically designed to streamline the simulation of material transport in neural trees. GALDS employs a graph autoencoder to encode latent representations of the network's geometry, velocity fields, and concentration profiles. These latent space representations are then assembled into a global graph, which is subsequently used to predict system dynamics in the latent space via a trained graph latent space system dynamic model, inspired by the Neural Ordinary Differential Equations (Neural ODEs) concept. The integration of an autoencoder allows for the use of smaller graph neural network models with reduced training data requirements. Furthermore, the Neural ODE component effectively mitigates the issue of error accumulation commonly encountered in recurrent neural networks. The effectiveness of the GALDS model is demonstrated through results on eight unseen geometries and four abnormal transport examples, where our approach achieves mean relative error of 3% with maximum relative error <8% and demonstrates a 10-fold speed improvement compared to previous surrogate model approaches.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 40,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10880",
        "abs_url": "https://arxiv.org/abs/2507.10880",
        "pdf_url": "https://arxiv.org/pdf/2507.10880",
        "title": "Domain-Adaptive Small Language Models for Structured Tax Code Prediction",
        "authors": [
            "Souvik Nath",
            "Sumit Wadhwa",
            "Luiz Perez"
        ],
        "comments": "10 pages, 3 figures",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
        "abstract": "Every day, multinational firms process thousands of transactions, each of which must adhere to tax regulations that vary by jurisdiction and are often nuanced. The determination of product and service tax codes, such as HSN or SAC is a major use case in Tax compliance. An accurate determination of such codes is imperative to avoid any tax penalties. This paper proposes a domain-adaptive small language model (SLM) with an encoder-decoder architecture for the enhanced prediction of product and service tax codes. In this approach, we address the problem of predicting hierarchical tax code sequences using unstructured product and services data. We employ an SLM based upon encoder-decoder architecture as this enables sequential generation of tax codes to capture the hierarchical dependencies present within the tax codes. Our experiments demonstrate that encoder-decoder SLMs can be successfully applied to the sequential prediction of structured tax codes, a domain that remains comparatively unexplored in current NLP research. In this paper, we demonstrate the superior performance of the domain-adaptive encoder-decoder SLMs over flat classifiers when applied to the Harmonized System of Nomenclature (HSN), and achieve superior results compared to decoder-only and encoder-only architectures for structured sequence generation tasks. This approach can also be scaled to other government-mandated tax commodity codes, such as United Nations Standard Products and Services Codes (UNSPSC), or Brazil's Nomenclatura Comum do Mercosul (NCM).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 41,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10884",
        "abs_url": "https://arxiv.org/abs/2507.10884",
        "pdf_url": "https://arxiv.org/pdf/2507.10884",
        "title": "Learning from Imperfect Data: Robust Inference of Dynamic Systems using Simulation-based Generative Model",
        "authors": [
            "Hyunwoo Cho",
            "Hyeontae Jo",
            "Hyung Ju Hwang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Dynamical Systems (math.DS)",
        "abstract": "System inference for nonlinear dynamic models, represented by ordinary differential equations (ODEs), remains a significant challenge in many fields, particularly when the data are noisy, sparse, or partially observable. In this paper, we propose a Simulation-based Generative Model for Imperfect Data (SiGMoID) that enables precise and robust inference for dynamic systems. The proposed approach integrates two key methods: (1) physics-informed neural networks with hyper-networks that constructs an ODE solver, and (2) Wasserstein generative adversarial networks that estimates ODE parameters by effectively capturing noisy data distributions. We demonstrate that SiGMoID quantifies data noise, estimates system parameters, and infers unobserved system components. Its effectiveness is validated validated through realistic experimental examples, showcasing its broad applicability in various domains, from scientific research to engineered systems, and enabling the discovery of full system dynamics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 42,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10886",
        "abs_url": "https://arxiv.org/abs/2507.10886",
        "pdf_url": "https://arxiv.org/pdf/2507.10886",
        "title": "How to Protect Models against Adversarial Unlearning?",
        "authors": [
            "Patryk Jasiorski",
            "Marek Klonowski",
            "Micha≈Ç Wo≈∫niak"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "AI models need to be unlearned to fulfill the requirements of legal acts such as the AI Act or GDPR, and also because of the need to remove toxic content, debiasing, the impact of malicious instances, or changes in the data distribution structure in which a model works. Unfortunately, removing knowledge may cause undesirable side effects, such as a deterioration in model performance. In this paper, we investigate the problem of adversarial unlearning, where a malicious party intentionally sends unlearn requests to deteriorate the model's performance maximally. We show that this phenomenon and the adversary's capabilities depend on many factors, primarily on the backbone model itself and strategy/limitations in selecting data to be unlearned. The main result of this work is a new method of protecting model performance from these side effects, both in the case of unlearned behavior resulting from spontaneous processes and adversary actions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 43,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10890",
        "abs_url": "https://arxiv.org/abs/2507.10890",
        "pdf_url": "https://arxiv.org/pdf/2507.10890",
        "title": "Outbound Modeling for Inventory Management",
        "authors": [
            "Riccardo Savorgnan",
            "Udaya Ghai",
            "Carson Eisenach",
            "Dean Foster"
        ],
        "comments": "KDD - AI for Supply Chain Workshop",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We study the problem of forecasting the number of units fulfilled (or ``drained'') from each inventory warehouse to meet customer demand, along with the associated outbound shipping costs. The actual drain and shipping costs are determined by complex production systems that manage the planning and execution of customers' orders fulfillment, i.e. from where and how to ship a unit to be delivered to a customer. Accurately modeling these processes is critical for regional inventory planning, especially when using Reinforcement Learning (RL) to develop control policies. For the RL usecase, a drain model is incorporated into a simulator to produce long rollouts, which we desire to be differentiable. While simulating the calls to the internal software systems can be used to recover this transition, they are non-differentiable and too slow and costly to run within an RL training environment. Accordingly, we frame this as a probabilistic forecasting problem, modeling the joint distribution of outbound drain and shipping costs across all warehouses at each time period, conditioned on inventory positions and exogenous customer demand. To ensure robustness in an RL environment, the model must handle out-of-distribution scenarios that arise from off-policy trajectories. We propose a validation scheme that leverages production systems to evaluate the drain model on counterfactual inventory states induced by RL policies. Preliminary results demonstrate the model's accuracy within the in-distribution setting.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 44,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10904",
        "abs_url": "https://arxiv.org/abs/2507.10904",
        "pdf_url": "https://arxiv.org/pdf/2507.10904",
        "title": "Class-Proportional Coreset Selection for Difficulty-Separable Data",
        "authors": [
            "Elisa Tsai",
            "Haizhong Zheng",
            "Atul Prakash"
        ],
        "comments": "This paper has been accepted to the ICCV 2025 Workshop on Curated Data for Efficient Learning (CDEL)",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "High-quality training data is essential for building reliable and efficient machine learning systems. One-shot coreset selection addresses this by pruning the dataset while maintaining or even improving model performance, often relying on training-dynamics-based data difficulty scores. However, most existing methods implicitly assume class-wise homogeneity in data difficulty, overlooking variation in data difficulty across different classes. In this work, we challenge this assumption by showing that, in domains such as network intrusion detection and medical imaging, data difficulty often clusters by class. We formalize this as class-difficulty separability and introduce the Class Difficulty Separability Coefficient (CDSC) as a quantitative measure. We demonstrate that high CDSC values correlate with performance degradation in class-agnostic coreset methods, which tend to overrepresent easy majority classes while neglecting rare but informative ones. To address this, we introduce class-proportional variants of multiple sampling strategies. Evaluated on five diverse datasets spanning security and medical domains, our methods consistently achieve state-of-the-art data efficiency. For instance, on CTU-13, at an extreme 99% pruning rate, a class-proportional variant of Coverage-centric Coreset Selection (CCS-CP) shows remarkable stability, with accuracy dropping only 2.58%, precision 0.49%, and recall 0.19%. In contrast, the class-agnostic CCS baseline, the next best method, suffers sharper declines of 7.59% in accuracy, 4.57% in precision, and 4.11% in recall. We further show that aggressive pruning enhances generalization in noisy, imbalanced, and large-scale datasets. Our results underscore that explicitly modeling class-difficulty separability leads to more effective, robust, and generalizable data pruning, particularly in high-stakes scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 45,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10955",
        "abs_url": "https://arxiv.org/abs/2507.10955",
        "pdf_url": "https://arxiv.org/pdf/2507.10955",
        "title": "Diffusion Decoding for Peptide De Novo Sequencing",
        "authors": [
            "Chi-en Amy Tai",
            "Alexander Wong"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Peptide de novo sequencing is a method used to reconstruct amino acid sequences from tandem mass spectrometry data without relying on existing protein sequence databases. Traditional deep learning approaches, such as Casanovo, mainly utilize autoregressive decoders and predict amino acids sequentially. Subsequently, they encounter cascading errors and fail to leverage high-confidence regions effectively. To address these issues, this paper investigates using diffusion decoders adapted for the discrete data domain. These decoders provide a different approach, allowing sequence generation to start from any peptide segment, thereby enhancing prediction accuracy. We experiment with three different diffusion decoder designs, knapsack beam search, and various loss functions. We find knapsack beam search did not improve performance metrics and simply replacing the transformer decoder with a diffusion decoder lowered performance. Although peptide precision and recall were still 0, the best diffusion decoder design with the DINOISER loss function obtained a statistically significant improvement in amino acid recall by 0.373 compared to the baseline autoregressive decoder-based Casanovo model. These findings highlight the potential of diffusion decoders to not only enhance model sensitivity but also drive significant advancements in peptide de novo sequencing.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 46,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10983",
        "abs_url": "https://arxiv.org/abs/2507.10983",
        "pdf_url": "https://arxiv.org/pdf/2507.10983",
        "title": "Physics-Informed Neural Networks For Semiconductor Film Deposition: A Review",
        "authors": [
            "Tao Han",
            "Zahra Taheri",
            "Hyunwoong Ko"
        ],
        "comments": "11 pages, 1 figure, 3 tables, IDETC-CIE 2025",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Semiconductor manufacturing relies heavily on film deposition processes, such as Chemical Vapor Deposition and Physical Vapor Deposition. These complex processes require precise control to achieve film uniformity, proper adhesion, and desired functionality. Recent advancements in Physics-Informed Neural Networks (PINNs), an innovative machine learning (ML) approach, have shown significant promise in addressing challenges related to process control, quality assurance, and predictive modeling within semiconductor film deposition and other manufacturing domains. This paper provides a comprehensive review of ML applications targeted at semiconductor film deposition processes. Through a thematic analysis, we identify key trends, existing limitations, and research gaps, offering insights into both the advantages and constraints of current methodologies. Our structured analysis aims to highlight the potential integration of these ML techniques to enhance interpretability, accuracy, and robustness in film deposition processes. Additionally, we examine state-of-the-art PINN methods, discussing strategies for embedding physical knowledge, governing laws, and partial differential equations into advanced neural network architectures tailored for semiconductor manufacturing. Based on this detailed review, we propose novel research directions that integrate the strengths of PINNs to significantly advance film deposition processes. The contributions of this study include establishing a clear pathway for future research in integrating physics-informed ML frameworks, addressing existing methodological gaps, and ultimately improving precision, scalability, and operational efficiency within semiconductor manufacturing.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 47,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10986",
        "abs_url": "https://arxiv.org/abs/2507.10986",
        "pdf_url": "https://arxiv.org/pdf/2507.10986",
        "title": "StellarF: A Lora-Adapter Integrated Large Model Framework for Stellar Flare Forecasting with Historical & Statistical Data",
        "authors": [
            "Tianyu Su",
            "Zhiqiang Zou",
            "Ali Luo",
            "Xiao Kong",
            "Qingyu Lu",
            "Min Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Stellar flare forecasting, a critical research frontier in astronomy, offers profound insights into stellar activity. However, the field is constrained by both the sparsity of recorded flare events and the absence of domain-specific large-scale predictive models. To address these challenges, this study introduces StellarF (Stellar Flare Forecasting), a novel large model that leverages Low-Rank (LoRA) and Adapter techniques to parameter-efficient learning for stellar flare forecasting. At its core, StellarF integrates an flare statistical information module with a historical flare record module, enabling multi-scale pattern recognition from observational data. Extensive experiments on our self-constructed datasets (derived from Kepler and TESS light curves) demonstrate that StellarF achieves state-of-the-art performance compared to existing methods. The proposed prediction paradigm establishes a novel methodological framework for advancing astrophysical research and cross-disciplinary applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 48,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10990",
        "abs_url": "https://arxiv.org/abs/2507.10990",
        "pdf_url": "https://arxiv.org/pdf/2507.10990",
        "title": "High-Throughput Distributed Reinforcement Learning via Adaptive Policy Synchronization",
        "authors": [
            "Rodney Lafuente-Mercado"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Scaling reinforcement learning (RL) workloads often requires distributing environment simulation across compute clusters. Existing frameworks entangle simulation, learning logic, and orchestration into monolithic systems, limiting modularity and reusability. We present ClusterEnv, a lightweight, learner-agnostic interface for distributed environment execution that mirrors the Gymnasium API. ClusterEnv introduces the DETACH pattern, which decouples simulation from training by offloading reset() and step() operations to remote workers while keeping learning centralized. To address policy staleness in distributed execution, we propose Adaptive Actor Policy Synchronization (AAPS), a divergence-triggered update mechanism that reduces synchronization overhead without sacrificing performance. ClusterEnv integrates cleanly into existing RL pipelines, supports both on-policy and off-policy methods, and requires minimal code changes. Experiments on discrete control tasks demonstrate that AAPS achieves high sample efficiency with significantly fewer weight updates. Source code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 49,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10995",
        "abs_url": "https://arxiv.org/abs/2507.10995",
        "pdf_url": "https://arxiv.org/pdf/2507.10995",
        "title": "Misalignment from Treating Means as Ends",
        "authors": [
            "Henrik Marklund",
            "Alex Infanger",
            "Benjamin Van Roy"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Reward functions, learned or manually specified, are rarely perfect. Instead of accurately expressing human goals, these reward functions are often distorted by human beliefs about how best to achieve those goals. Specifically, these reward functions often express a combination of the human's terminal goals -- those which are ends in themselves -- and the human's instrumental goals -- those which are means to an end. We formulate a simple example in which even slight conflation of instrumental and terminal goals results in severe misalignment: optimizing the misspecified reward function results in poor performance when measured by the true reward function. This example distills the essential properties of environments that make reinforcement learning highly sensitive to conflation of instrumental and terminal goals. We discuss how this issue can arise with a common approach to reward learning and how it can manifest in real environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 50,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10998",
        "abs_url": "https://arxiv.org/abs/2507.10998",
        "pdf_url": "https://arxiv.org/pdf/2507.10998",
        "title": "Crafting Imperceptible On-Manifold Adversarial Attacks for Tabular Data",
        "authors": [
            "Zhipeng He",
            "Alexander Stevens",
            "Chun Ouyang",
            "Johannes De Smedt",
            "Alistair Barros",
            "Catarina Moreira"
        ],
        "comments": "32 pages",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Adversarial attacks on tabular data present fundamental challenges distinct from image or text domains due to the heterogeneous nature of mixed categorical and numerical features. Unlike images where pixel perturbations maintain visual similarity, tabular data lacks intuitive similarity metrics, making it difficult to define imperceptible modifications. Additionally, traditional gradient-based methods prioritise $\\ell_p$-norm constraints, often producing adversarial examples that deviate from the original data distributions, making them detectable. We propose a latent space perturbation framework using a mixed-input Variational Autoencoder (VAE) to generate imperceptible adversarial examples. The proposed VAE integrates categorical embeddings and numerical features into a unified latent manifold, enabling perturbations that preserve statistical consistency. We specify In-Distribution Success Rate (IDSR) to measure the proportion of adversarial examples that remain statistically indistinguishable from the input distribution. Evaluation across six publicly available datasets and three model architectures demonstrates that our method achieves substantially lower outlier rates and more consistent performance compared to traditional input-space attacks and other VAE-based methods adapted from image domain approaches. Our comprehensive analysis includes hyperparameter sensitivity, sparsity control mechanisms, and generative architectural comparisons, revealing that VAE-based attacks depend critically on reconstruction quality but offer superior practical utility when sufficient training data is available. This work highlights the importance of on-manifold perturbations for realistic adversarial attacks on tabular data, offering a robust approach for practical deployment. The source code can be accessed through this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 51,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11005",
        "abs_url": "https://arxiv.org/abs/2507.11005",
        "pdf_url": "https://arxiv.org/pdf/2507.11005",
        "title": "AdaMuon: Adaptive Muon Optimizer",
        "authors": [
            "Chongjie Si",
            "Debing Zhang",
            "Wei Shen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We propose AdaMuon, an adaptive learning-rate framework built upon the recently validated Muon optimizer, which has demonstrated substantial efficiency gains over AdamW in large-scale model training. AdaMuon augments Muon with two mutually dependent modules: (1) a per-parameter second-moment modulation that captures orthogonal gradient updates to ensure update-level adaptivity, and (2) a RMS-aligned rescaling that regulates the overall update magnitude by aligning it with the intrinsic structure of the parameter space. Empirical results on multiple model scales and learning-rate regimes confirm that AdaMuon consistently outperforms the original Muon, delivering higher acceleration in convergence while maintaining training stability. Our method introduces no additional tuning burden and can be seamlessly integrated into existing Muon training pipelines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 52,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11012",
        "abs_url": "https://arxiv.org/abs/2507.11012",
        "pdf_url": "https://arxiv.org/pdf/2507.11012",
        "title": "Leveraging Advanced Machine Learning to Predict Turbulence Dynamics from Temperature Observations at an Experimental Prescribed Fire",
        "authors": [
            "Dipak Dulal",
            "Joseph J. Charney",
            "Michael R. Gallagher",
            "Pitambar Acharya",
            "Carmeliza Navasca",
            "Nicholas S. Skowronski"
        ],
        "comments": "arXiv admin note: text overlap with arXiv:2311.05128",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "This study explores the potential for predicting turbulent kinetic energy (TKE) from more readily acquired temperature data using temperature profiles and turbulence data collected concurrently at 10 Hz during a small experimental prescribed burn in the New Jersey Pine Barrens. Machine learning models, including Deep Neural Networks, Random Forest Regressor, Gradient Boosting, and Gaussian Process Regressor, were employed to assess the potential to predict TKE from temperature perturbations and explore temporal and spatial dynamics of correlations. Data visualization and correlation analyses revealed patterns and relationships between thermocouple temperatures and TKE, providing insight into the underlying dynamics. More accurate predictions of TKE were achieved by employing various machine learning models despite a weak correlation between the predictors and the target variable. The results demonstrate significant success, particularly from regression models, in accurately predicting the TKE. The findings of this study demonstrate a novel numerical approach to identifying new relationships between temperature and airflow processes in and around the fire environment. These relationships can help refine our understanding of combustion environment processes and the coupling and decoupling of fire environment processes necessary for improving fire operations strategy and fire and smoke model predictions. The findings of this study additionally highlight the valuable role of machine learning techniques in analyzing the complex large datasets of the fire environments, showcasing their potential to advance fire research and management practices.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 53,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11017",
        "abs_url": "https://arxiv.org/abs/2507.11017",
        "pdf_url": "https://arxiv.org/pdf/2507.11017",
        "title": "First-Order Error Matters: Accurate Compensation for Quantized Large Language Models",
        "authors": [
            "Xingyu Zheng",
            "Haotong Qin",
            "Yuye Li",
            "Jiakai Wang",
            "Jinyang Guo",
            "Michele Magno",
            "Xianglong Liu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Post-training quantization (PTQ) offers an efficient approach to compressing large language models (LLMs), significantly reducing memory access and computational costs. Existing compensation-based weight calibration methods often rely on a second-order Taylor expansion to model quantization error, under the assumption that the first-order term is negligible in well-trained full-precision models. However, we reveal that the progressive compensation process introduces accumulated first-order deviations between latent weights and their full-precision counterparts, making this assumption fundamentally flawed. To address this, we propose FOEM, a novel PTQ method that explicitly incorporates first-order gradient terms to improve quantization error compensation. FOEM approximates gradients by directly computing the difference between latent and full-precision weights, avoiding the high cost and limited generalization of backpropagation-based gradient computation. This approach introduces minimal additional computational overhead. Moreover, FOEM leverages precomputed Cholesky factors to efficiently recover the inverse of Hessian submatrices in real time. Extensive experiments across a wide range of models and benchmarks demonstrate that FOEM consistently outperforms the classical GPTQ method. In 3-bit weight-only quantization, FOEM reduces the perplexity of Llama3-8B by 89.6%, and improves the 5-shot MMLU accuracy of Llama3-70B from 51.7% to 74.9%, approaching the full-precision performance of 78.6%. Furthermore, FOEM can be seamlessly integrated with advanced techniques such as GPTAQ and SpinQuant, yielding additional improvements under the challenging W4A4KV4 setting, and further narrowing the accuracy gap with full-precision baselines beyond what current state-of-the-art methods achieve. The code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 54,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11019",
        "abs_url": "https://arxiv.org/abs/2507.11019",
        "pdf_url": "https://arxiv.org/pdf/2507.11019",
        "title": "Relative Entropy Pathwise Policy Optimization",
        "authors": [
            "Claas Voelcker",
            "Axel Brunnbauer",
            "Marcel Hussing",
            "Michal Nauman",
            "Pieter Abbeel",
            "Eric Eaton",
            "Radu Grosu",
            "Amir-massoud Farahmand",
            "Igor Gilitschenski"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Score-function policy gradients have delivered strong results in game-playing, robotics and language-model fine-tuning. Yet its high-variance often undermines training stability. On the other hand, pathwise policy gradients alleviate the training variance, but are reliable only when driven by an accurate action-conditioned value function which is notoriously hard to train without relying on past off-policy data. In this paper, we discuss how to construct a value-gradient driven, on-policy algorithm that allow training Q-value models purely from on-policy data, unlocking the possibility of using pathwise policy updates in the context of on-policy learning. We show how to balance stochastic policies for exploration with constrained policy updates for stable training, and evaluate important architectural components that facilitate accurate value function learning. Building on these insights, we propose Relative Entropy Pathwise Policy Optimization (REPPO), an efficient on-policy algorithm that combines the sample-efficiency of pathwise policy gradients with the simplicity and minimal memory footprint of standard on-policy learning. We demonstrate that REPPO provides strong empirical performance at decreased sample requirements, wall-clock time, memory footprint as well as high hyperparameter robustness in a set of experiments on two standard GPU-parallelized benchmarks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 55,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11053",
        "abs_url": "https://arxiv.org/abs/2507.11053",
        "pdf_url": "https://arxiv.org/pdf/2507.11053",
        "title": "GATE: Graph Attention Neural Networks with Real-Time Edge Construction for Robust Indoor Localization using Mobile Embedded Devices",
        "authors": [
            "Danish Gufran",
            "Sudeep Pasricha"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Accurate indoor localization is crucial for enabling spatial context in smart environments and navigation systems. Wi-Fi Received Signal Strength (RSS) fingerprinting is a widely used indoor localization approach due to its compatibility with mobile embedded devices. Deep Learning (DL) models improve accuracy in localization tasks by learning RSS variations across locations, but they assume fingerprint vectors exist in a Euclidean space, failing to incorporate spatial relationships and the non-uniform distribution of real-world RSS noise. This results in poor generalization across heterogeneous mobile devices, where variations in hardware and signal processing distort RSS readings. Graph Neural Networks (GNNs) can improve upon conventional DL models by encoding indoor locations as nodes and modeling their spatial and signal relationships as edges. However, GNNs struggle with non-Euclidean noise distributions and suffer from the GNN blind spot problem, leading to degraded accuracy in environments with dense access points (APs). To address these challenges, we propose GATE, a novel framework that constructs an adaptive graph representation of fingerprint vectors while preserving an indoor state-space topology, modeling the non-Euclidean structure of RSS noise to mitigate environmental noise and address device heterogeneity. GATE introduces 1) a novel Attention Hyperspace Vector (AHV) for enhanced message passing, 2) a novel Multi-Dimensional Hyperspace Vector (MDHV) to mitigate the GNN blind spot, and 3) an new Real-Time Edge Construction (RTEC) approach for dynamic graph adaptation. Extensive real-world evaluations across multiple indoor spaces with varying path lengths, AP densities, and heterogeneous devices demonstrate that GATE achieves 1.6x to 4.72x lower mean localization errors and 1.85x to 4.57x lower worst-case errors compared to state-of-the-art indoor localization frameworks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 56,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11063",
        "abs_url": "https://arxiv.org/abs/2507.11063",
        "pdf_url": "https://arxiv.org/pdf/2507.11063",
        "title": "A Distance Metric for Mixed Integer Programming Instances",
        "authors": [
            "Gwen Maudet",
            "Gr√©goire Danoy"
        ],
        "comments": "Accepted to ECAI 2025",
        "subjects": "Machine Learning (cs.LG); Optimization and Control (math.OC)",
        "abstract": "Mixed-integer linear programming (MILP) is a powerful tool for addressing a wide range of real-world problems, but it lacks a clear structure for comparing instances. A reliable similarity metric could establish meaningful relationships between instances, enabling more effective evaluation of instance set heterogeneity and providing better guidance to solvers, particularly when machine learning is involved. Existing similarity metrics often lack precision in identifying instance classes or rely heavily on labeled data, which limits their applicability and generalization. To bridge this gap, this paper introduces the first mathematical distance metric for MILP instances, derived directly from their mathematical formulations. By discretizing right-hand sides, weights, and variables into classes, the proposed metric draws inspiration from the Earth mover's distance to quantify mismatches in weight-variable distributions for constraint comparisons. This approach naturally extends to enable instance-level comparisons. We evaluate both an exact and a greedy variant of our metric under various parameter settings, using the StrIPLIB dataset. Results show that all components of the metric contribute to class identification, and that the greedy version achieves accuracy nearly identical to the exact formulation while being nearly 200 times faster. Compared to state-of-the-art baselines, including feature-based, image-based, and neural network models, our unsupervised method consistently outperforms all non-learned approaches and rivals the performance of a supervised classifier on class and subclass grouping tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 57,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11071",
        "abs_url": "https://arxiv.org/abs/2507.11071",
        "pdf_url": "https://arxiv.org/pdf/2507.11071",
        "title": "LogTinyLLM: Tiny Large Language Models Based Contextual Log Anomaly Detection",
        "authors": [
            "Isaiah Thompson Ocansey",
            "Ritwik Bhattacharya",
            "Tanmay Sen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Log anomaly detection using traditional rule based or deep learning based methods is often challenging due to the large volume and highly complex nature of log sequence. So effective way of detection of anomalous sequence of logs is crucial for system maintenance and development. This paper proposes parameter efficient finetuning specifically low rank adaptation (LoRA) and adapter based approaches for finding contextual anomalies in sequence of logs in large log data set. It compares different tiny large language models (LLMs) on the Thunderbird dataset. The results show that LoRA based finetuning provides substantial performance improvements of 18 to 19 percentage over LogBert based full finetuning approach, achieving accuracy scores between 97.76% and 98.83% compared to 79.37%.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 58,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11173",
        "abs_url": "https://arxiv.org/abs/2507.11173",
        "pdf_url": "https://arxiv.org/pdf/2507.11173",
        "title": "Real-Time Bayesian Detection of Drift-Evasive GNSS Spoofing in Reinforcement Learning Based UAV Deconfliction",
        "authors": [
            "Deepak Kumar Panda",
            "Weisi Guo"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Autonomous unmanned aerial vehicles (UAVs) rely on global navigation satellite system (GNSS) pseudorange measurements for accurate real-time localization and navigation. However, this dependence exposes them to sophisticated spoofing threats, where adversaries manipulate pseudoranges to deceive UAV receivers. Among these, drift-evasive spoofing attacks subtly perturb measurements, gradually diverting the UAVs trajectory without triggering conventional signal-level anti-spoofing mechanisms. Traditional distributional shift detection techniques often require accumulating a threshold number of samples, causing delays that impede rapid detection and timely response. Consequently, robust temporal-scale detection methods are essential to identify attack onset and enable contingency planning with alternative sensing modalities, improving resilience against stealthy adversarial manipulations. This study explores a Bayesian online change point detection (BOCPD) approach that monitors temporal shifts in value estimates from a reinforcement learning (RL) critic network to detect subtle behavioural deviations in UAV navigation. Experimental results show that this temporal value-based framework outperforms conventional GNSS spoofing detectors, temporal semi-supervised learning frameworks, and the Page-Hinkley test, achieving higher detection accuracy and lower false-positive and false-negative rates for drift-evasive spoofing attacks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 59,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11178",
        "abs_url": "https://arxiv.org/abs/2507.11178",
        "pdf_url": "https://arxiv.org/pdf/2507.11178",
        "title": "Gradient Regularization-based Neural Granger Causality",
        "authors": [
            "Meiliang Liu",
            "Huiwen Dong",
            "Xiaoxiao Yang",
            "Yunfang Xu",
            "Zijin Li",
            "Zhengye Si",
            "Xinyue Yang",
            "Zhiwen Zhao"
        ],
        "comments": "9 pages,3 figures, conference",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "With the advancement of deep learning technologies, various neural network-based Granger causality models have been proposed. Although these models have demonstrated notable improvements, several limitations remain. Most existing approaches adopt the component-wise architecture, necessitating the construction of a separate model for each time series, which results in substantial computational costs. In addition, imposing the sparsity-inducing penalty on the first-layer weights of the neural network to extract causal relationships weakens the model's ability to capture complex interactions. To address these limitations, we propose Gradient Regularization-based Neural Granger Causality (GRNGC), which requires only one time series prediction model and applies $L_{1}$ regularization to the gradient between model's input and output to infer Granger causality. Moreover, GRNGC is not tied to a specific time series forecasting model and can be implemented with diverse architectures such as KAN, MLP, and LSTM, offering enhanced flexibility. Numerical simulations on DREAM, Lorenz-96, fMRI BOLD, and CausalTime show that GRNGC outperforms existing baselines and significantly reduces computational overhead. Meanwhile, experiments on real-world DNA, Yeast, HeLa, and bladder urothelial carcinoma datasets further validate the model's effectiveness in reconstructing gene regulatory networks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 60,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11181",
        "abs_url": "https://arxiv.org/abs/2507.11181",
        "pdf_url": "https://arxiv.org/pdf/2507.11181",
        "title": "Mixture of Experts in Large Language Models",
        "authors": [
            "Danyang Zhang",
            "Junhao Song",
            "Ziqian Bi",
            "Yingfang Yuan",
            "Tianyang Wang",
            "Joe Yeong",
            "Junfeng Hao"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "This paper presents a comprehensive review of the Mixture-of-Experts (MoE) architecture in large language models, highlighting its ability to significantly enhance model performance while maintaining minimal computational overhead. Through a systematic analysis spanning theoretical foundations, core architectural designs, and large language model (LLM) applications, we examine expert gating and routing mechanisms, hierarchical and sparse MoE configurations, meta-learning approaches, multimodal and multitask learning scenarios, real-world deployment cases, and recent advances and challenges in deep learning. Our analysis identifies key advantages of MoE, including superior model capacity compared to equivalent Bayesian approaches, improved task-specific performance, and the ability to scale model capacity efficiently. We also underscore the importance of ensuring expert diversity, accurate calibration, and reliable inference aggregation, as these are essential for maximizing the effectiveness of MoE architectures. Finally, this review outlines current research limitations, open challenges, and promising future directions, providing a foundation for continued innovation in MoE architecture and its applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 61,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11183",
        "abs_url": "https://arxiv.org/abs/2507.11183",
        "pdf_url": "https://arxiv.org/pdf/2507.11183",
        "title": "Quantized Rank Reduction: A Communications-Efficient Federated Learning Scheme for Network-Critical Applications",
        "authors": [
            "Dimitrios Kritsiolis",
            "Constantine Kotropoulos"
        ],
        "comments": "In Proceedings of the 2025 IARIA Annual Congress on Frontiers in Science, Technology, Services, and Applications (IARIA Congress 2025), Venice, Italy, July 6-10, 2025",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Federated learning is a machine learning approach that enables multiple devices (i.e., agents) to train a shared model cooperatively without exchanging raw data. This technique keeps data localized on user devices, ensuring privacy and security, while each agent trains the model on their own data and only shares model updates. The communication overhead is a significant challenge due to the frequent exchange of model updates between the agents and the central server. In this paper, we propose a communication-efficient federated learning scheme that utilizes low-rank approximation of neural network gradients and quantization to significantly reduce the network load of the decentralized learning process with minimal impact on the model's accuracy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 62,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11185",
        "abs_url": "https://arxiv.org/abs/2507.11185",
        "pdf_url": "https://arxiv.org/pdf/2507.11185",
        "title": "An Explainable AI-Enhanced Machine Learning Approach for Cardiovascular Disease Detection and Risk Assessment",
        "authors": [
            "Md. Emon Akter Sourov",
            "Md. Sabbir Hossen",
            "Pabon Shaha",
            "Mohammad Minoar Hossain",
            "Md Sadiq Iqbal"
        ],
        "comments": "This paper has been accepted at the IEEE QPAIN 2025. The final version will be available in the IEEE Xplore Digital Library",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Heart disease remains a major global health concern, particularly in regions with limited access to medical resources and diagnostic facilities. Traditional diagnostic methods often fail to accurately identify and manage heart disease risks, leading to adverse outcomes. Machine learning has the potential to significantly enhance the accuracy, efficiency, and speed of heart disease diagnosis. In this study, we proposed a comprehensive framework that combines classification models for heart disease detection and regression models for risk prediction. We employed the Heart Disease dataset, which comprises 1,035 cases. To address the issue of class imbalance, the Synthetic Minority Oversampling Technique (SMOTE) was applied, resulting in the generation of an additional 100,000 synthetic data points. Performance metrics, including accuracy, precision, recall, F1-score, R2, MSE, RMSE, and MAE, were used to evaluate the model's effectiveness. Among the classification models, Random Forest emerged as the standout performer, achieving an accuracy of 97.2% on real data and 97.6% on synthetic data. For regression tasks, Linear Regression demonstrated the highest R2 values of 0.992 and 0.984 on real and synthetic datasets, respectively, with the lowest error metrics. Additionally, Explainable AI techniques were employed to enhance the interpretability of the models. This study highlights the potential of machine learning to revolutionize heart disease diagnosis and risk prediction, thereby facilitating early intervention and enhancing clinical decision-making.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 63,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11187",
        "abs_url": "https://arxiv.org/abs/2507.11187",
        "pdf_url": "https://arxiv.org/pdf/2507.11187",
        "title": "Striking the Perfect Balance: Preserving Privacy While Boosting Utility in Collaborative Medical Prediction Platforms",
        "authors": [
            "Shao-Bo Lin",
            "Xiaotong Liu",
            "Yao Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Online collaborative medical prediction platforms offer convenience and real-time feedback by leveraging massive electronic health records. However, growing concerns about privacy and low prediction quality can deter patient participation and doctor cooperation. In this paper, we first clarify the privacy attacks, namely attribute attacks targeting patients and model extraction attacks targeting doctors, and specify the corresponding privacy principles. We then propose a privacy-preserving mechanism and integrate it into a novel one-shot distributed learning framework, aiming to simultaneously meet both privacy requirements and prediction performance objectives. Within the framework of statistical learning theory, we theoretically demonstrate that the proposed distributed learning framework can achieve the optimal prediction performance under specific privacy requirements. We further validate the developed privacy-preserving collaborative medical prediction platform through both toy simulations and real-world data experiments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 64,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11228",
        "abs_url": "https://arxiv.org/abs/2507.11228",
        "pdf_url": "https://arxiv.org/pdf/2507.11228",
        "title": "Gradient Descent on Logistic Regression: Do Large Step-Sizes Work with Data on the Sphere?",
        "authors": [
            "Si Yi Meng",
            "Baptiste Goujaud",
            "Antonio Orvieto",
            "Christopher De Sa"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Optimization and Control (math.OC)",
        "abstract": "Gradient descent (GD) on logistic regression has many fascinating properties. When the dataset is linearly separable, it is known that the iterates converge in direction to the maximum-margin separator regardless of how large the step size is. In the non-separable case, however, it has been shown that GD can exhibit a cycling behaviour even when the step sizes is still below the stability threshold $2/\\lambda$, where $\\lambda$ is the largest eigenvalue of the Hessian at the solution. This short paper explores whether restricting the data to have equal magnitude is a sufficient condition for global convergence, under any step size below the stability threshold. We prove that this is true in a one dimensional space, but in higher dimensions cycling behaviour can still occur. We hope to inspire further studies on quantifying how common these cycles are in realistic datasets, as well as finding sufficient conditions to guarantee global convergence with large step sizes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 65,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11246",
        "abs_url": "https://arxiv.org/abs/2507.11246",
        "pdf_url": "https://arxiv.org/pdf/2507.11246",
        "title": "Generative Click-through Rate Prediction with Applications to Search Advertising",
        "authors": [
            "Lingwei Kong",
            "Lu Wang",
            "Changping Peng",
            "Zhangang Lin",
            "Ching Law",
            "Jingping Shao"
        ],
        "comments": "This work was first submitted on February 9, 2024",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Click-Through Rate (CTR) prediction models are integral to a myriad of industrial settings, such as personalized search advertising. Current methods typically involve feature extraction from users' historical behavior sequences combined with product information, feeding into a discriminative model that is trained on user feedback to estimate CTR. With the success of models such as GPT, the potential for generative models to enrich expressive power beyond discriminative models has become apparent. In light of this, we introduce a novel model that leverages generative models to enhance the precision of CTR predictions in discriminative models. To reconcile the disparate data aggregation needs of both model types, we design a two-stage training process: 1) Generative pre-training for next-item prediction with the given item category in user behavior sequences; 2) Fine-tuning the well-trained generative model within a discriminative CTR prediction framework. Our method's efficacy is substantiated through extensive experiments on a new dataset, and its significant utility is further corroborated by online A/B testing results. Currently, the model is deployed on one of the world's largest e-commerce platforms, and we intend to release the associated code and dataset in the future.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 66,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11262",
        "abs_url": "https://arxiv.org/abs/2507.11262",
        "pdf_url": "https://arxiv.org/pdf/2507.11262",
        "title": "LyAm: Robust Non-Convex Optimization for Stable Learning in Noisy Environments",
        "authors": [
            "Elmira Mirzabeigi",
            "Sepehr Rezaee",
            "Kourosh Parand"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Optimization and Control (math.OC)",
        "abstract": "Training deep neural networks, particularly in computer vision tasks, often suffers from noisy gradients and unstable convergence, which hinder performance and generalization. In this paper, we propose LyAm, a novel optimizer that integrates Adam's adaptive moment estimation with Lyapunov-based stability mechanisms. LyAm dynamically adjusts the learning rate using Lyapunov stability theory to enhance convergence robustness and mitigate training noise. We provide a rigorous theoretical framework proving the convergence guarantees of LyAm in complex, non-convex settings. Extensive experiments on like as CIFAR-10 and CIFAR-100 show that LyAm consistently outperforms state-of-the-art optimizers in terms of accuracy, convergence speed, and stability, establishing it as a strong candidate for robust deep learning optimization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 67,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11269",
        "abs_url": "https://arxiv.org/abs/2507.11269",
        "pdf_url": "https://arxiv.org/pdf/2507.11269",
        "title": "Turning Sand to Gold: Recycling Data to Bridge On-Policy and Off-Policy Learning via Causal Bound",
        "authors": [
            "Tal Fiskus",
            "Uri Shaham"
        ],
        "comments": "51 pages, 16 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Deep reinforcement learning (DRL) agents excel in solving complex decision-making tasks across various domains. However, they often require a substantial number of training steps and a vast experience replay buffer, leading to significant computational and resource demands. To address these challenges, we introduce a novel theoretical result that leverages the Neyman-Rubin potential outcomes framework into DRL. Unlike most methods that focus on bounding the counterfactual loss, we establish a causal bound on the factual loss, which is analogous to the on-policy loss in DRL. This bound is computed by storing past value network outputs in the experience replay buffer, effectively utilizing data that is usually discarded. Extensive experiments across the Atari 2600 and MuJoCo domains on various agents, such as DQN and SAC, achieve up to 2,427% higher reward ratio, outperforming the same agents without our proposed term, and reducing the experience replay buffer size by up to 96%, significantly improving sample efficiency at negligible cost.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 68,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11274",
        "abs_url": "https://arxiv.org/abs/2507.11274",
        "pdf_url": "https://arxiv.org/pdf/2507.11274",
        "title": "Fast Last-Iterate Convergence of SGD in the Smooth Interpolation Regime",
        "authors": [
            "Amit Attia",
            "Matan Schliserman",
            "Uri Sherman",
            "Tomer Koren"
        ],
        "comments": "27 pages",
        "subjects": "Machine Learning (cs.LG); Optimization and Control (math.OC); Machine Learning (stat.ML)",
        "abstract": "We study population convergence guarantees of stochastic gradient descent (SGD) for smooth convex objectives in the interpolation regime, where the noise at optimum is zero or near zero. The behavior of the last iterate of SGD in this setting -- particularly with large (constant) stepsizes -- has received growing attention in recent years due to implications for the training of over-parameterized models, as well as to analyzing forgetting in continual learning and to understanding the convergence of the randomized Kaczmarz method for solving linear systems. We establish that after $T$ steps of SGD on $\\beta$-smooth convex loss functions with stepsize $\\eta \\leq 1/\\beta$, the last iterate exhibits expected excess risk $\\widetilde{O}(1/(\\eta T^{1-\\beta\\eta/2}) + \\eta T^{\\beta\\eta/2} \\sigma_\\star^2)$, where $\\sigma_\\star^2$ denotes the variance of the stochastic gradients at the optimum. In particular, for a well-tuned stepsize we obtain a near optimal $\\widetilde{O}(1/T + \\sigma_\\star/\\sqrt{T})$ rate for the last iterate, extending the results of Varre et al. (2021) beyond least squares regression; and when $\\sigma_\\star=0$ we obtain a rate of $O(1/\\sqrt{T})$ with $\\eta=1/\\beta$, improving upon the best-known $O(T^{-1/4})$ rate recently established by Evron et al. (2025) in the special case of realizable linear regression.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 69,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11344",
        "abs_url": "https://arxiv.org/abs/2507.11344",
        "pdf_url": "https://arxiv.org/pdf/2507.11344",
        "title": "Guiding LLM Decision-Making with Fairness Reward Models",
        "authors": [
            "Zara Hall",
            "Melanie Subbiah",
            "Thomas P Zollo",
            "Kathleen McKeown",
            "Richard Zemel"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Large language models are increasingly used to support high-stakes decisions, potentially influencing who is granted bail or receives a loan. Naive chain-of-thought sampling can improve average decision accuracy, but has also been shown to amplify unfair bias. To address this challenge and enable the trustworthy use of reasoning models in high-stakes decision-making, we propose a framework for training a generalizable Fairness Reward Model (FRM). Our model assigns a fairness score to LLM reasoning, enabling the system to down-weight biased trajectories and favor equitable ones when aggregating decisions across reasoning chains. We show that a single Fairness Reward Model, trained on weakly supervised, LLM-annotated examples of biased versus unbiased reasoning, transfers across tasks, domains, and model families without additional fine-tuning. Applied to real-world decision-making tasks including recidivism prediction and social media moderation, we show that our approach consistently improves fairness while matching, or even surpassing, baseline accuracy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 70,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11357",
        "abs_url": "https://arxiv.org/abs/2507.11357",
        "pdf_url": "https://arxiv.org/pdf/2507.11357",
        "title": "Neurosymbolic Reasoning Shortcuts under the Independence Assumption",
        "authors": [
            "Emile van Krieken",
            "Pasquale Minervini",
            "Edoardo Ponti",
            "Antonio Vergari"
        ],
        "comments": "Accepted at NeSy 2025",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "The ubiquitous independence assumption among symbolic concepts in neurosymbolic (NeSy) predictors is a convenient simplification: NeSy predictors use it to speed up probabilistic reasoning. Recent works like van Krieken et al. (2024) and Marconato et al. (2024) argued that the independence assumption can hinder learning of NeSy predictors and, more crucially, prevent them from correctly modelling uncertainty. There is, however, scepticism in the NeSy community around the scenarios in which the independence assumption actually limits NeSy systems (Faronius and Dos Martires, 2025). In this work, we settle this question by formally showing that assuming independence among symbolic concepts entails that a model can never represent uncertainty over certain concept combinations. Thus, the model fails to be aware of reasoning shortcuts, i.e., the pathological behaviour of NeSy predictors that predict correct downstream tasks but for the wrong reasons.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 71,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11367",
        "abs_url": "https://arxiv.org/abs/2507.11367",
        "pdf_url": "https://arxiv.org/pdf/2507.11367",
        "title": "Local Pairwise Distance Matching for Backpropagation-Free Reinforcement Learning",
        "authors": [
            "Daniel Tanneberg"
        ],
        "comments": "accepted at the European Conference on Artificial Intelligence (ECAI 2025)",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "Training neural networks with reinforcement learning (RL) typically relies on backpropagation (BP), necessitating storage of activations from the forward pass for subsequent backward updates. Furthermore, backpropagating error signals through multiple layers often leads to vanishing or exploding gradients, which can degrade learning performance and stability. We propose a novel approach that trains each layer of the neural network using local signals during the forward pass in RL settings. Our approach introduces local, layer-wise losses leveraging the principle of matching pairwise distances from multi-dimensional scaling, enhanced with optional reward-driven guidance. This method allows each hidden layer to be trained using local signals computed during forward propagation, thus eliminating the need for backward passes and storing intermediate activations. Our experiments, conducted with policy gradient methods across common RL benchmarks, demonstrate that this backpropagation-free method achieves competitive performance compared to their classical BP-based counterpart. Additionally, the proposed method enhances stability and consistency within and across runs, and improves performance especially in challenging environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 72,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11371",
        "abs_url": "https://arxiv.org/abs/2507.11371",
        "pdf_url": "https://arxiv.org/pdf/2507.11371",
        "title": "Step-wise Policy for Rare-tool Knowledge (SPaRK): Offline RL that Drives Diverse Tool Use in LLMs",
        "authors": [
            "Gabriel Bo",
            "Koa Chang",
            "Justin Gu"
        ],
        "comments": "12 pages, 4 figures",
        "subjects": "Machine Learning (cs.LG); Multiagent Systems (cs.MA)",
        "abstract": "We present Step-wise Policy for Rare-tool Knowledge (SPaRK), a novel reinforcement learning framework that teaches large language models to explore diverse tool usage patterns beyond conventional high-temperature sampling. Building on recent advances in step-wise reinforcement learning, we introduce a dual-objective reward system that simultaneously optimizes for answer quality and tool diversity, training a Llama-3.1 8B model through offline PPO on synthetically generated trajectories from the MMLU-Pro dataset. Our approach uniquely employs a rarity-first exploitation strategy where a GPT-4o judge scores candidate actions across eight distinct tools plus chain-of-thought reasoning, with the policy favoring less-frequently used but still viable tools to encourage systematic exploration. Empirical results demonstrate that SPaRK achieves competitive performance across 14 MMLU-Pro categories while exhibiting significantly higher entropy in tool selection compared to both baseline and supervised fine-tuning approaches, suggesting that algorithmic exploration through explicit tool diversity can enhance reasoning capabilities without sacrificing accuracy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 73,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11393",
        "abs_url": "https://arxiv.org/abs/2507.11393",
        "pdf_url": "https://arxiv.org/pdf/2507.11393",
        "title": "A Neural Network Model of Complementary Learning Systems: Pattern Separation and Completion for Continual Learning",
        "authors": [
            "James P Jun",
            "Vijay Marupudi",
            "Raj Sanjay Shah",
            "Sashank Varma"
        ],
        "comments": "Accepted to CogSci 2025. 7 pages, 7 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Learning new information without forgetting prior knowledge is central to human intelligence. In contrast, neural network models suffer from catastrophic forgetting: a significant degradation in performance on previously learned tasks when acquiring new information. The Complementary Learning Systems (CLS) theory offers an explanation for this human ability, proposing that the brain has distinct systems for pattern separation (encoding distinct memories) and pattern completion (retrieving complete memories from partial cues). To capture these complementary functions, we leverage the representational generalization capabilities of variational autoencoders (VAEs) and the robust memory storage properties of Modern Hopfield networks (MHNs), combining them into a neurally plausible continual learning model. We evaluate this model on the Split-MNIST task, a popular continual learning benchmark, and achieve close to state-of-the-art accuracy (~90%), substantially reducing forgetting. Representational analyses empirically confirm the functional dissociation: the VAE underwrites pattern completion, while the MHN drives pattern separation. By capturing pattern separation and completion in scalable architectures, our work provides a functional template for modeling memory consolidation, generalization, and continual learning in both biological and artificial systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 74,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11411",
        "abs_url": "https://arxiv.org/abs/2507.11411",
        "pdf_url": "https://arxiv.org/pdf/2507.11411",
        "title": "Robust-Multi-Task Gradient Boosting",
        "authors": [
            "Seyedsaman Emami",
            "Gonzalo Mart√≠nez-Mu√±oz",
            "Daniel Hern√°ndez-Lobato"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Multi-task learning (MTL) has shown effectiveness in exploiting shared information across tasks to improve generalization. MTL assumes tasks share similarities that can improve performance. In addition, boosting algorithms have demonstrated exceptional performance across diverse learning problems, primarily due to their ability to focus on hard-to-learn instances and iteratively reduce residual errors. This makes them a promising approach for learning multi-task problems. However, real-world MTL scenarios often involve tasks that are not well-aligned (known as outlier or adversarial tasks), which do not share beneficial similarities with others and can, in fact, deteriorate the performance of the overall model. To overcome this challenge, we propose Robust-Multi-Task Gradient Boosting (R-MTGB), a novel boosting framework that explicitly models and adapts to task heterogeneity during training. R-MTGB structures the learning process into three sequential blocks: (1) learning shared patterns, (2) partitioning tasks into outliers and non-outliers with regularized parameters, and (3) fine-tuning task-specific predictors. This architecture enables R-MTGB to automatically detect and penalize outlier tasks while promoting effective knowledge transfer among related tasks. Our method integrates these mechanisms seamlessly within gradient boosting, allowing robust handling of noisy or adversarial tasks without sacrificing accuracy. Extensive experiments on both synthetic benchmarks and real-world datasets demonstrate that our approach successfully isolates outliers, transfers knowledge, and consistently reduces prediction errors for each task individually, and achieves overall performance gains across all tasks. These results highlight robustness, adaptability, and reliable convergence of R-MTGB in challenging MTL environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 75,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11436",
        "abs_url": "https://arxiv.org/abs/2507.11436",
        "pdf_url": "https://arxiv.org/pdf/2507.11436",
        "title": "Toward Improving fNIRS Classification: A Study on Activation Functions in Deep Neural Architectures",
        "authors": [
            "Behtom Adeli",
            "John McLinden",
            "Pankaj Pandey",
            "Ming Shao",
            "Yalda Shahriari"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Activation functions are critical to the performance of deep neural networks, particularly in domains such as functional near-infrared spectroscopy (fNIRS), where nonlinearity, low signal-to-noise ratio (SNR), and signal variability poses significant challenges to model accuracy. However, the impact of activation functions on deep learning (DL) performance in the fNIRS domain remains underexplored and lacks systematic investigation in the current literature. This study evaluates a range of conventional and field-specific activation functions for fNIRS classification tasks using multiple deep learning architectures, including the domain-specific fNIRSNet, AbsoluteNet, MDNN, and shallowConvNet (as the baseline), all tested on a single dataset recorded during an auditory task. To ensure fair a comparison, all networks were trained and tested using standardized preprocessing and consistent training parameters. The results show that symmetrical activation functions such as Tanh and the Absolute value function Abs(x) can outperform commonly used functions like the Rectified Linear Unit (ReLU), depending on the architecture. Additionally, a focused analysis of the role of symmetry was conducted using a Modified Absolute Function (MAF), with results further supporting the effectiveness of symmetrical activation functions on performance gains. These findings underscore the importance of selecting proper activation functions that align with the signal characteristics of fNIRS data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 76,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11439",
        "abs_url": "https://arxiv.org/abs/2507.11439",
        "pdf_url": "https://arxiv.org/pdf/2507.11439",
        "title": "Data Augmentation in Time Series Forecasting through Inverted Framework",
        "authors": [
            "Hongming Tan",
            "Ting Chen",
            "Ruochong Jin",
            "Wai Kin Chan"
        ],
        "comments": "The paper is under consideration at Pattern Recognition Letters",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Currently, iTransformer is one of the most popular and effective models for multivariate time series (MTS) forecasting. Thanks to its inverted framework, iTransformer effectively captures multivariate correlation. However, the inverted framework still has some limitations. It diminishes temporal interdependency information, and introduces noise in cases of nonsignificant variable correlation. To address these limitations, we introduce a novel data augmentation method on inverted framework, called DAIF. Unlike previous data augmentation methods, DAIF stands out as the first real-time augmentation specifically designed for the inverted framework in MTS forecasting. We first define the structure of the inverted sequence-to-sequence framework, then propose two different DAIF strategies, Frequency Filtering and Cross-variation Patching to address the existing challenges of the inverted framework. Experiments across multiple datasets and inverted models have demonstrated the effectiveness of our DAIF.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 77,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11457",
        "abs_url": "https://arxiv.org/abs/2507.11457",
        "pdf_url": "https://arxiv.org/pdf/2507.11457",
        "title": "LRMR: LLM-Driven Relational Multi-node Ranking for Lymph Node Metastasis Assessment in Rectal Cancer",
        "authors": [
            "Yaoxian Dong",
            "Yifan Gao",
            "Haoyue Li",
            "Yanfen Cui",
            "Xin Gao"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Image and Video Processing (eess.IV)",
        "abstract": "Accurate preoperative assessment of lymph node (LN) metastasis in rectal cancer guides treatment decisions, yet conventional MRI evaluation based on morphological criteria shows limited diagnostic performance. While some artificial intelligence models have been developed, they often operate as black boxes, lacking the interpretability needed for clinical trust. Moreover, these models typically evaluate nodes in isolation, overlooking the patient-level context. To address these limitations, we introduce LRMR, an LLM-Driven Relational Multi-node Ranking framework. This approach reframes the diagnostic task from a direct classification problem into a structured reasoning and ranking process. The LRMR framework operates in two stages. First, a multimodal large language model (LLM) analyzes a composite montage image of all LNs from a patient, generating a structured report that details ten distinct radiological features. Second, a text-based LLM performs pairwise comparisons of these reports between different patients, establishing a relative risk ranking based on the severity and number of adverse features. We evaluated our method on a retrospective cohort of 117 rectal cancer patients. LRMR achieved an area under the curve (AUC) of 0.7917 and an F1-score of 0.7200, outperforming a range of deep learning baselines, including ResNet50 (AUC 0.7708). Ablation studies confirmed the value of our two main contributions: removing the relational ranking stage or the structured prompting stage led to a significant performance drop, with AUCs falling to 0.6875 and 0.6458, respectively. Our work demonstrates that decoupling visual perception from cognitive reasoning through a two-stage LLM framework offers a powerful, interpretable, and effective new paradigm for assessing lymph node metastasis in rectal cancer.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 78,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11471",
        "abs_url": "https://arxiv.org/abs/2507.11471",
        "pdf_url": "https://arxiv.org/pdf/2507.11471",
        "title": "D3FL: Data Distribution and Detrending for Robust Federated Learning in Non-linear Time-series Data",
        "authors": [
            "Harsha Varun Marisetty",
            "Manik Gupta",
            "Yogesh Simmhan"
        ],
        "comments": "Preprint of paper to appear in the proceedings of IEEE INTERNATIONAL CONFERENCE ON EDGE COMPUTING & COMMUNICATIONS EDGE 2025",
        "subjects": "Machine Learning (cs.LG); Distributed, Parallel, and Cluster Computing (cs.DC)",
        "abstract": "With advancements in computing and communication technologies, the Internet of Things (IoT) has seen significant growth. IoT devices typically collect data from various sensors, such as temperature, humidity, and energy meters. Much of this data is temporal in nature. Traditionally, data from IoT devices is centralized for analysis, but this approach introduces delays and increased communication costs. Federated learning (FL) has emerged as an effective alternative, allowing for model training across distributed devices without the need to centralize data. In many applications, such as smart home energy and environmental monitoring, the data collected by IoT devices across different locations can exhibit significant variation in trends and seasonal patterns. Accurately forecasting such non-stationary, non-linear time-series data is crucial for applications like energy consumption estimation and weather forecasting. However, these data variations can severely impact prediction accuracy. The key contributions of this paper are: (1) Investigating how non-linear, non-stationary time-series data distributions, like generalized extreme value (gen-extreme) and log norm distributions, affect FL performance. (2) Analyzing how different detrending techniques for non-linear time-series data influence the forecasting model's performance in a FL setup. We generated several synthetic time-series datasets using non-linear data distributions and trained an LSTM-based forecasting model using both centralized and FL approaches. Additionally, we evaluated the impact of detrending on real-world datasets with non-linear time-series data distributions. Our experimental results show that: (1) FL performs worse than centralized approaches when dealing with non-linear data distributions. (2) The use of appropriate detrending techniques improves FL performance, reducing loss across different data distributions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 79,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11486",
        "abs_url": "https://arxiv.org/abs/2507.11486",
        "pdf_url": "https://arxiv.org/pdf/2507.11486",
        "title": "Exploring the robustness of TractOracle methods in RL-based tractography",
        "authors": [
            "Jeremi Levesque",
            "Antoine Th√©berge",
            "Maxime Descoteaux",
            "Pierre-Marc Jodoin"
        ],
        "comments": "38 pages, 8 figures. Submitted to Medical Image Analysis",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Tractography algorithms leverage diffusion MRI to reconstruct the fibrous architecture of the brain's white matter. Among machine learning approaches, reinforcement learning (RL) has emerged as a promising framework for tractography, outperforming traditional methods in several key aspects. TractOracle-RL, a recent RL-based approach, reduces false positives by incorporating anatomical priors into the training process via a reward-based mechanism. In this paper, we investigate four extensions of the original TractOracle-RL framework by integrating recent advances in RL, and we evaluate their performance across five diverse diffusion MRI datasets. Results demonstrate that combining an oracle with the RL framework consistently leads to robust and reliable tractography, regardless of the specific method or dataset used. We also introduce a novel RL training scheme called Iterative Reward Training (IRT), inspired by the Reinforcement Learning from Human Feedback (RLHF) paradigm. Instead of relying on human input, IRT leverages bundle filtering methods to iteratively refine the oracle's guidance throughout training. Experimental results show that RL methods trained with oracle feedback significantly outperform widely used tractography techniques in terms of accuracy and anatomical validity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 80,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11493",
        "abs_url": "https://arxiv.org/abs/2507.11493",
        "pdf_url": "https://arxiv.org/pdf/2507.11493",
        "title": "A parametric activation function based on Wendland RBF",
        "authors": [
            "Majid Darehmiraki"
        ],
        "comments": "11 pages, 2 figures",
        "subjects": "Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)",
        "abstract": "This paper introduces a novel parametric activation function based on Wendland radial basis functions (RBFs) for deep neural networks. Wendland RBFs, known for their compact support, smoothness, and positive definiteness in approximation theory, are adapted to address limitations of traditional activation functions like ReLU, sigmoid, and tanh. The proposed enhanced Wendland activation combines a standard Wendland component with linear and exponential terms, offering tunable locality, improved gradient propagation, and enhanced stability during training. Theoretical analysis highlights its mathematical properties, including smoothness and adaptability, while empirical experiments on synthetic tasks (e.g., sine wave approximation) and benchmark datasets (MNIST, Fashion-MNIST) demonstrate competitive performance. Results show that the Wendland-based activation achieves superior accuracy in certain scenarios, particularly in regression tasks, while maintaining computational efficiency. The study bridges classical RBF theory with modern deep learning, suggesting that Wendland activations can mitigate overfitting and improve generalization through localized, smooth transformations. Future directions include hybrid architectures and domain-specific adaptations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 81,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11515",
        "abs_url": "https://arxiv.org/abs/2507.11515",
        "pdf_url": "https://arxiv.org/pdf/2507.11515",
        "title": "AirLLM: Diffusion Policy-based Adaptive LoRA for Remote Fine-Tuning of LLM over the Air",
        "authors": [
            "Shiyi Yang",
            "Xiaoxue Yu",
            "Rongpeng Li",
            "Jianhang Zhu",
            "Zhifeng Zhao",
            "Honggang Zhang"
        ],
        "comments": "11 pages, 8 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Operating Large Language Models (LLMs) on edge devices is increasingly challenged by limited communication bandwidth and strained computational and memory costs. Thus, cloud-assisted remote fine-tuning becomes indispensable. Nevertheless, existing Low-Rank Adaptation (LoRA) approaches typically employ fixed or heuristic rank configurations, and the subsequent over-the-air transmission of all LoRA parameters could be rather inefficient. To address this limitation, we develop AirLLM, a hierarchical diffusion policy framework for communication-aware LoRA adaptation. Specifically, AirLLM models the rank configuration as a structured action vector that spans all LoRA-inserted projections. To solve the underlying high-dimensional sequential decision-making problem, a Proximal Policy Optimization (PPO) agent generates coarse-grained decisions by jointly observing wireless states and linguistic complexity, which are then refined via Denoising Diffusion Implicit Models (DDIM) to produce high-resolution, task- and channel-adaptive rank vectors. The two modules are optimized alternatively, with the DDIM trained under the Classifier-Free Guidance (CFG) paradigm to maintain alignment with PPO rewards. Experiments under varying signal-to-noise ratios demonstrate that AirLLM consistently enhances fine-tuning performance while significantly reducing transmission costs, highlighting the effectiveness of reinforcement-driven, diffusion-refined rank adaptation for scalable and efficient remote fine-tuning over the air.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 82,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11531",
        "abs_url": "https://arxiv.org/abs/2507.11531",
        "pdf_url": "https://arxiv.org/pdf/2507.11531",
        "title": "Langevin Flows for Modeling Neural Latent Dynamics",
        "authors": [
            "Yue Song",
            "T. Anderson Keller",
            "Yisong Yue",
            "Pietro Perona",
            "Max Welling"
        ],
        "comments": "Full version of the Cognitive Computational Neuroscience (CCN) 2025 poster",
        "subjects": "Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)",
        "abstract": "Neural populations exhibit latent dynamical structures that drive time-evolving spiking activities, motivating the search for models that capture both intrinsic network dynamics and external unobserved influences. In this work, we introduce LangevinFlow, a sequential Variational Auto-Encoder where the time evolution of latent variables is governed by the underdamped Langevin equation. Our approach incorporates physical priors -- such as inertia, damping, a learned potential function, and stochastic forces -- to represent both autonomous and non-autonomous processes in neural systems. Crucially, the potential function is parameterized as a network of locally coupled oscillators, biasing the model toward oscillatory and flow-like behaviors observed in biological neural populations. Our model features a recurrent encoder, a one-layer Transformer decoder, and Langevin dynamics in the latent space. Empirically, our method outperforms state-of-the-art baselines on synthetic neural populations generated by a Lorenz attractor, closely matching ground-truth firing rates. On the Neural Latents Benchmark (NLB), the model achieves superior held-out neuron likelihoods (bits per spike) and forward prediction accuracy across four challenging datasets. It also matches or surpasses alternative methods in decoding behavioral metrics such as hand velocity. Overall, this work introduces a flexible, physics-inspired, high-performing framework for modeling complex neural population dynamics and their unobserved influences.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 83,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10560",
        "abs_url": "https://arxiv.org/abs/2507.10560",
        "pdf_url": "https://arxiv.org/pdf/2507.10560",
        "title": "Tangma: A Tanh-Guided Activation Function with Learnable Parameters",
        "authors": [
            "Shreel Golwala"
        ],
        "comments": "",
        "subjects": "Neural and Evolutionary Computing (cs.NE); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Activation functions are key to effective backpropagation and expressiveness in deep neural networks. This work introduces Tangma, a new activation function that combines the smooth shape of the hyperbolic tangent with two learnable parameters: $\\alpha$, which shifts the curve's inflection point to adjust neuron activation, and $\\gamma$, which adds linearity to preserve weak gradients and improve training stability. Tangma was evaluated on MNIST and CIFAR-10 using custom networks composed of convolutional and linear layers, and compared against ReLU, Swish, and GELU. On MNIST, Tangma achieved the highest validation accuracy of 99.09% and the lowest validation loss, demonstrating faster and more stable convergence than the baselines. On CIFAR-10, Tangma reached a top validation accuracy of 78.15%, outperforming all other activation functions while maintaining a competitive training loss. Tangma also showed improved training efficiency, with lower average epoch runtimes compared to Swish and GELU. These results suggest that Tangma performs well on standard vision tasks and enables reliable, efficient training. Its learnable design gives more control over activation behavior, which may benefit larger models in tasks such as image recognition or language modeling.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 84,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10562",
        "abs_url": "https://arxiv.org/abs/2507.10562",
        "pdf_url": "https://arxiv.org/pdf/2507.10562",
        "title": "SAMEP: A Secure Protocol for Persistent Context Sharing Across AI Agents",
        "authors": [
            "Hari Masoor"
        ],
        "comments": "7 pages, 4 figures, 3 implementation examples. Original work submitted as a preprint",
        "subjects": "Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Databases (cs.DB); Machine Learning (cs.LG)",
        "abstract": "Current AI agent architectures suffer from ephemeral memory limitations, preventing effective collaboration and knowledge sharing across sessions and agent boundaries. We introduce SAMEP (Secure Agent Memory Exchange Protocol), a novel framework that enables persistent, secure, and semantically searchable memory sharing among AI agents. Our protocol addresses three critical challenges: (1) persistent context preservation across agent sessions, (2) secure multi-agent collaboration with fine-grained access control, and (3) efficient semantic discovery of relevant historical context. SAMEP implements a distributed memory repository with vector-based semantic search, cryptographic access controls (AES-256-GCM), and standardized APIs compatible with existing agent communication protocols (MCP, A2A). We demonstrate SAMEP's effectiveness across diverse domains including multi-agent software development, healthcare AI with HIPAA compliance, and multi-modal processing pipelines. Experimental results show 73% reduction in redundant computations, 89% improvement in context relevance scores, and complete compliance with regulatory requirements including audit trail generation. SAMEP enables a new paradigm of persistent, collaborative AI agent ecosystems while maintaining security and privacy guarantees.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 85,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10566",
        "abs_url": "https://arxiv.org/abs/2507.10566",
        "pdf_url": "https://arxiv.org/pdf/2507.10566",
        "title": "AI Mother Tongue: Self-Emergent Communication in MARL via Endogenous Symbol Systems",
        "authors": [
            "Hung Ming Liu"
        ],
        "comments": "30 pages, 4 figures",
        "subjects": "Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG); Multiagent Systems (cs.MA); Neural and Evolutionary Computing (cs.NE)",
        "abstract": "In Decentralized Multi-Agent Reinforcement Learning (MARL), the development of Emergent Communication has long been constrained by the ``Joint Exploration Dilemma'', leading agents to fall into a ``Communication Vacuum Equilibrium'' . Traditional methods address this by introducing inductive biases to facilitate communication emergence . This study fundamentally questions whether such artificial inductive biases are, in fact, over-engineering. Through experiments with the ``AI Mother Tongue'' (AIM) framework, based on a Vector Quantized Variational Autoencoder (VQ-VAE), we demonstrate that when agents possess an endogenous symbol system, their neural representations naturally exhibit spontaneous semantic compression and Nash equilibrium-driven semantic convergence, achieving effective symbolic communication without external inductive biases. This aligns with recent neuroscience findings suggesting that the human brain does not directly use human language for internal thought , and resonates with research on ``soft thinking'' capabilities in Large Language Models (LLMs) . Compared to traditional explicit communication methods, AIM demonstrates stronger generality and efficiency. The interpretable analysis toolkit developed in this study confirms that symbol usage exhibits a significant power-law distribution, leading to three major theoretical insights: the ``Neural Communication Hypothesis'', the ``Tool-First Principle'', and the ``Semantic Interpretability Paradigm''. Future research will explore the integration of Hierarchical Quantized Variational Autoencoders (HQ-VAE) to enhance AIM's complex expressive capabilities and investigate the potential for ``Reinforcement Learning (RL) Low-Level Pre-training''. This discovery offers new avenues for bridging symbolism and connectionism.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 86,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10567",
        "abs_url": "https://arxiv.org/abs/2507.10567",
        "pdf_url": "https://arxiv.org/pdf/2507.10567",
        "title": "Protocols for Verifying Smooth Strategies in Bandits and Games",
        "authors": [
            "Miranda Christ",
            "Daniel Reichman",
            "Jonathan Shafer"
        ],
        "comments": "",
        "subjects": "Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG)",
        "abstract": "We study protocols for verifying approximate optimality of strategies in multi-armed bandits and normal-form games. As the number of actions available to each player is often large, we seek protocols where the number of queries to the utility oracle is sublinear in the number of actions. We prove that such verification is possible for sufficiently smooth strategies that do not put too much probability mass on any specific action. We provide protocols for verifying that a smooth policy for a multi-armed bandit is $\\varepsilon$-optimal. Our verification protocols require provably fewer arm queries than learning. Furthermore, we establish a nearly-tight lower bound on the query complexity of verification in our settings. As an application, we show how to use verification for bandits to achieve verification in normal-form games. This gives a protocol for verifying whether a given strategy profile is an approximate strong smooth Nash equilibrium, with a query complexity that is sublinear in the number of actions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 87,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10599",
        "abs_url": "https://arxiv.org/abs/2507.10599",
        "pdf_url": "https://arxiv.org/pdf/2507.10599",
        "title": "Emergence of Hierarchical Emotion Organization in Large Language Models",
        "authors": [
            "Bo Zhao",
            "Maya Okawa",
            "Eric J. Bigelow",
            "Rose Yu",
            "Tomer Ullman",
            "Ekdeep Singh Lubana",
            "Hidenori Tanaka"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "As large language models (LLMs) increasingly power conversational agents, understanding how they model users' emotional states is critical for ethical deployment. Inspired by emotion wheels -- a psychological framework that argues emotions organize hierarchically -- we analyze probabilistic dependencies between emotional states in model outputs. We find that LLMs naturally form hierarchical emotion trees that align with human psychological models, and larger models develop more complex hierarchies. We also uncover systematic biases in emotion recognition across socioeconomic personas, with compounding misclassifications for intersectional, underrepresented groups. Human studies reveal striking parallels, suggesting that LLMs internalize aspects of social perception. Beyond highlighting emergent emotional reasoning in LLMs, our results hint at the potential of using cognitively-grounded theories for developing better model evaluations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 88,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10601",
        "abs_url": "https://arxiv.org/abs/2507.10601",
        "pdf_url": "https://arxiv.org/pdf/2507.10601",
        "title": "AGFS-Tractometry: A Novel Atlas-Guided Fine-Scale Tractometry Approach for Enhanced Along-Tract Group Statistical Comparison Using Diffusion MRI Tractography",
        "authors": [
            "Ruixi Zheng",
            "Wei Zhang",
            "Yijie Li",
            "Xi Zhu",
            "Zhou Lan",
            "Jarrett Rushmore",
            "Yogesh Rathi",
            "Nikos Makris",
            "Lauren J. O'Donnell",
            "Fan Zhang"
        ],
        "comments": "31 pages and 7 figures",
        "subjects": "Quantitative Methods (q-bio.QM); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Image and Video Processing (eess.IV); Methodology (stat.ME)",
        "abstract": "Diffusion MRI (dMRI) tractography is currently the only method for in vivo mapping of the brain's white matter (WM) connections. Tractometry is an advanced tractography analysis technique for along-tract profiling to investigate the morphology and microstructural properties along the fiber tracts. Tractometry has become an essential tool for studying local along-tract differences between different populations (e.g., health vs disease). In this study, we propose a novel atlas-guided fine-scale tractometry method, namely AGFS-Tractometry, that leverages tract spatial information and permutation testing to enhance the along-tract statistical analysis between populations. There are two major contributions in AGFS-Tractometry. First, we create a novel atlas-guided tract profiling template that enables consistent, fine-scale, along-tract parcellation of subject-specific fiber tracts. Second, we propose a novel nonparametric permutation testing group comparison method to enable simultaneous analysis across all along-tract parcels while correcting for multiple comparisons. We perform experimental evaluations on synthetic datasets with known group differences and in vivo real data. We compare AGFS-Tractometry with two state-of-the-art tractometry methods, including Automated Fiber-tract Quantification (AFQ) and BUndle ANalytics (BUAN). Our results show that the proposed AGFS-Tractometry obtains enhanced sensitivity and specificity in detecting local WM differences. In the real data analysis experiments, AGFS-Tractometry can identify more regions with significant differences, which are anatomically consistent with the existing literature. Overall, these demonstrate the ability of AGFS-Tractometry to detect subtle or spatially localized WM group-level differences. The created tract profiling template and related code are available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 89,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10607",
        "abs_url": "https://arxiv.org/abs/2507.10607",
        "pdf_url": "https://arxiv.org/pdf/2507.10607",
        "title": "Neural Expectation Operators",
        "authors": [
            "Qian Qi"
        ],
        "comments": "",
        "subjects": "Probability (math.PR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "This paper introduces \\textbf{Measure Learning}, a paradigm for modeling ambiguity via non-linear expectations. We define Neural Expectation Operators as solutions to Backward Stochastic Differential Equations (BSDEs) whose drivers are parameterized by neural networks. The main mathematical contribution is a rigorous well-posedness theorem for BSDEs whose drivers satisfy a local Lipschitz condition in the state variable $y$ and quadratic growth in its martingale component $z$. This result circumvents the classical global Lipschitz assumption, is applicable to common neural network architectures (e.g., with ReLU activations), and holds for exponentially integrable terminal data, which is the sharp condition for this setting. Our primary innovation is to build a constructive bridge between the abstract, and often restrictive, assumptions of the deep theory of quadratic BSDEs and the world of machine learning, demonstrating that these conditions can be met by concrete, verifiable neural network designs. We provide constructive methods for enforcing key axiomatic properties, such as convexity, by architectural design. The theory is extended to the analysis of fully coupled Forward-Backward SDE systems and to the asymptotic analysis of large interacting particle systems, for which we establish both a Law of Large Numbers (propagation of chaos) and a Central Limit Theorem. This work provides the foundational mathematical framework for data-driven modeling under ambiguity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 90,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10608",
        "abs_url": "https://arxiv.org/abs/2507.10608",
        "pdf_url": "https://arxiv.org/pdf/2507.10608",
        "title": "The Shape of Deceit: Behavioral Consistency and Fragility in Money Laundering Patterns",
        "authors": [
            "Danny Butvinik",
            "Ofir Yakobi",
            "Michal Einhorn Cohen",
            "Elina Maliarsky"
        ],
        "comments": "",
        "subjects": "Social and Information Networks (cs.SI); Machine Learning (cs.LG); Applications (stat.AP)",
        "abstract": "Conventional anti-money laundering (AML) systems predominantly focus on identifying anomalous entities or transactions, flagging them for manual investigation based on statistical deviation or suspicious behavior. This paradigm, however, misconstrues the true nature of money laundering, which is rarely anomalous but often deliberate, repeated, and concealed within consistent behavioral routines. In this paper, we challenge the entity-centric approach and propose a network-theoretic perspective that emphasizes detecting predefined laundering patterns across directed transaction networks. We introduce the notion of behavioral consistency as the core trait of laundering activity, and argue that such patterns are better captured through subgraph structures expressing semantic and functional roles - not solely geometry. Crucially, we explore the concept of pattern fragility: the sensitivity of laundering patterns to small attribute changes and, conversely, their semantic robustness even under drastic topological transformations. We claim that laundering detection should not hinge on statistical outliers, but on preservation of behavioral essence, and propose a reconceptualization of pattern similarity grounded in this insight. This philosophical and practical shift has implications for how AML systems model, scan, and interpret networks in the fight against financial crime.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 91,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10622",
        "abs_url": "https://arxiv.org/abs/2507.10622",
        "pdf_url": "https://arxiv.org/pdf/2507.10622",
        "title": "Spectral Feature Extraction for Robust Network Intrusion Detection Using MFCCs",
        "authors": [
            "HyeYoung Lee",
            "Muhammad Nadeem",
            "Pavel Tsoi"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Disordered Systems and Neural Networks (cond-mat.dis-nn); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "The rapid expansion of Internet of Things (IoT) networks has led to a surge in security vulnerabilities, emphasizing the critical need for robust anomaly detection and classification techniques. In this work, we propose a novel approach for identifying anomalies in IoT network traffic by leveraging the Mel-frequency cepstral coefficients (MFCC) and ResNet-18, a deep learning model known for its effectiveness in feature extraction and image-based tasks. Learnable MFCCs enable adaptive spectral feature representation, capturing the temporal patterns inherent in network traffic more effectively than traditional fixed MFCCs. We demonstrate that transforming raw signals into MFCCs maps the data into a higher-dimensional space, enhancing class separability and enabling more effective multiclass classification. Our approach combines the strengths of MFCCs with the robust feature extraction capabilities of ResNet-18, offering a powerful framework for anomaly detection. The proposed model is evaluated on three widely used IoT intrusion detection datasets: CICIoT2023, NSL-KDD, and IoTID20. The experimental results highlight the potential of integrating adaptive signal processing techniques with deep learning architectures to achieve robust and scalable anomaly detection in heterogeneous IoT network landscapes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 92,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10624",
        "abs_url": "https://arxiv.org/abs/2507.10624",
        "pdf_url": "https://arxiv.org/pdf/2507.10624",
        "title": "Comprehension Without Competence: Architectural Limits of LLMs in Symbolic Computation and Reasoning",
        "authors": [
            "Zheng Zhang"
        ],
        "comments": "Substantial change to previous version (experiments, theorem, analysis and related work); currently under review at TMLR",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Large Language Models (LLMs) display striking surface fluency yet systematically fail at tasks requiring symbolic reasoning, arithmetic accuracy, and logical consistency. This paper offers a structural diagnosis of such failures, revealing a persistent gap between \\textit{comprehension} and \\textit{competence}. Through controlled experiments and architectural analysis, we demonstrate that LLMs often articulate correct principles without reliably applying them--a failure rooted not in knowledge access, but in computational execution. We term this phenomenon the computational \\textit{split-brain syndrome}, where instruction and action pathways are geometrically and functionally dissociated. This core limitation recurs across domains, from mathematical operations to relational inferences, and explains why model behavior remains brittle even under idealized prompting. We argue that LLMs function as powerful pattern completion engines, but lack the architectural scaffolding for principled, compositional reasoning. Our findings delineate the boundary of current LLM capabilities and motivate future models with metacognitive control, principle lifting, and structurally grounded execution. This diagnosis also clarifies why mechanistic interpretability findings may reflect training-specific pattern coordination rather than universal computational principles, and why the geometric separation between instruction and execution pathways suggests limitations in neural introspection and mechanistic analysis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 93,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10634",
        "abs_url": "https://arxiv.org/abs/2507.10634",
        "pdf_url": "https://arxiv.org/pdf/2507.10634",
        "title": "Learning to Quantize and Precode in Massive MIMO Systems for Energy Reduction: a Graph Neural Network Approach",
        "authors": [
            "Thomas Feys",
            "Liesbet Van der Perre",
            "Fran√ßois Rottenberg"
        ],
        "comments": "",
        "subjects": "Systems and Control (eess.SY); Machine Learning (cs.LG); Signal Processing (eess.SP); Machine Learning (stat.ML)",
        "abstract": "Massive MIMO systems are moving toward increased numbers of radio frequency chains, higher carrier frequencies and larger bandwidths. As such, digital-to-analog converters (DACs) are becoming a bottleneck in terms of hardware complexity and power consumption. In this work, non-linear precoding for coarsely quantized downlink massive MIMO is studied. Given the NP-hard nature of this problem, a graph neural network (GNN) is proposed that directly outputs the precoded quantized vector based on the channel matrix and the intended transmit symbols. The model is trained in a self-supervised manner, by directly maximizing the achievable rate. To overcome the non-differentiability of the objective function, introduced due to the non-differentiable DAC functions, a straight-through Gumbel-softmax estimation of the gradient is proposed. The proposed method achieves a significant increase in achievable sum rate under coarse quantization. For instance, in the single-user case, the proposed method can achieve the same sum rate as maximum ratio transmission (MRT) by using one-bit DAC's as compared to 3 bits for MRT. This reduces the DAC's power consumption by a factor 4-7 and 3 for baseband and RF DACs respectively. This, however, comes at the cost of increased digital signal processing power consumption. When accounting for this, the reduction in overall power consumption holds for a system bandwidth up to 3.5 MHz for baseband DACs, while the RF DACs can maintain a power reduction of 2.9 for higher bandwidths. Notably, indirect effects, which further reduce the power consumption, such as a reduced fronthaul consumption and reduction in other components, are not considered in this analysis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 94,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10635",
        "abs_url": "https://arxiv.org/abs/2507.10635",
        "pdf_url": "https://arxiv.org/pdf/2507.10635",
        "title": "Formal Verification of Variational Quantum Circuits",
        "authors": [
            "Nicola Assolini",
            "Luca Marzari",
            "Isabella Mastroeni",
            "Alessandra di Pierro"
        ],
        "comments": "Assolini and Marzari contributed equally to the paper",
        "subjects": "Quantum Physics (quant-ph); Machine Learning (cs.LG); Programming Languages (cs.PL)",
        "abstract": "Variational quantum circuits (VQCs) are a central component of many quantum machine learning algorithms, offering a hybrid quantum-classical framework that, under certain aspects, can be considered similar to classical deep neural networks. A shared aspect is, for instance, their vulnerability to adversarial inputs, small perturbations that can lead to incorrect predictions. While formal verification techniques have been extensively developed for classical models, no comparable framework exists for certifying the robustness of VQCs. Here, we present the first in-depth theoretical and practical study of the formal verification problem for VQCs. Inspired by abstract interpretation methods used in deep learning, we analyze the applicability and limitations of interval-based reachability techniques in the quantum setting. We show that quantum-specific aspects, such as state normalization, introduce inter-variable dependencies that challenge existing approaches. We investigate these issues by introducing a novel semantic framework based on abstract interpretation, where the verification problem for VQCs can be formally defined, and its complexity analyzed. Finally, we demonstrate our approach on standard verification benchmarks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 95,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10640",
        "abs_url": "https://arxiv.org/abs/2507.10640",
        "pdf_url": "https://arxiv.org/pdf/2507.10640",
        "title": "SENSOR: An ML-Enhanced Online Annotation Tool to Uncover Privacy Concerns from User Reviews in Social-Media Applications",
        "authors": [
            "Labiba Farah",
            "Mohammad Ridwan Kabir",
            "Shohel Ahmed",
            "MD Mohaymen Ul Anam",
            "Md. Sakibul Islam"
        ],
        "comments": "26 pages, 9 figures, 5 tables",
        "subjects": "Software Engineering (cs.SE); Machine Learning (cs.LG); Social and Information Networks (cs.SI)",
        "abstract": "The widespread use of social media applications has raised significant privacy concerns, often highlighted in user reviews. These reviews also provide developers with valuable insights into improving apps by addressing issues and introducing better features. However, the sheer volume and nuanced nature of reviews make manual identification and prioritization of privacy-related concerns challenging for developers. Previous studies have developed software utilities to automatically classify user reviews as privacy-relevant, privacy-irrelevant, bug reports, feature requests, etc., using machine learning. Notably, there is a lack of focus on classifying reviews specifically as privacy-related feature requests, privacy-related bug reports, or privacy-irrelevant. This paper introduces SENtinel SORt (SENSOR), an automated online annotation tool designed to help developers annotate and classify user reviews into these categories. For automating the annotation of such reviews, this paper introduces the annotation model, GRACE (GRU-based Attention with CBOW Embedding), using Gated Recurrent Units (GRU) with Continuous Bag of Words (CBOW) and Attention mechanism. Approximately 16000 user reviews from seven popular social media apps on Google Play Store, including Instagram, Facebook, WhatsApp, Snapchat, X (formerly Twitter), Facebook Lite, and Line were analyzed. Two annotators manually labelled the reviews, achieving a Cohen's Kappa value of 0.87, ensuring a labeled dataset with high inter-rater agreement for training machine learning models. Among the models tested, GRACE demonstrated the best performance (macro F1-score: 0.9434, macro ROC-AUC: 0.9934, and accuracy: 95.10%) despite class imbalance. SENSOR demonstrates significant potential to assist developers with extracting and addressing privacy-related feature requests or bug reports from user reviews, enhancing user privacy and trust.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 96,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10643",
        "abs_url": "https://arxiv.org/abs/2507.10643",
        "pdf_url": "https://arxiv.org/pdf/2507.10643",
        "title": "TaylorPODA: A Taylor Expansion-Based Method to Improve Post-Hoc Attributions for Opaque Models",
        "authors": [
            "Yuchi Tang",
            "I√±aki Esnaola",
            "Suzanne Mason",
            "George Panoutsos"
        ],
        "comments": "17 pages, 6 figures, Submitted to NeurIPS 2025",
        "subjects": "Machine Learning (stat.ML); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Existing post-hoc model-agnostic methods generate external explanations for opaque models, primarily by locally attributing the model output to its input features. However, they often lack an explicit and systematic framework for quantifying the contribution of individual features. Building on the Taylor expansion framework introduced by Deng et al. (2024) to unify existing local attribution methods, we propose a rigorous set of postulates -- \"precision\", \"federation\", and \"zero-discrepancy\" -- to govern Taylor term-specific attribution. Guided by these postulates, we introduce TaylorPODA (Taylor expansion-derived imPortance-Order aDapted Attribution), which incorporates an additional \"adaptation\" property. This property enables alignment with task-specific goals, especially in post-hoc settings lacking ground-truth explanations. Empirical evaluations demonstrate that TaylorPODA achieves competitive results against baseline methods, providing principled and visualization-friendly explanations. This work represents a step toward the trustworthy deployment of opaque models by offering explanations with stronger theoretical grounding.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 97,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10701",
        "abs_url": "https://arxiv.org/abs/2507.10701",
        "pdf_url": "https://arxiv.org/pdf/2507.10701",
        "title": "Kernel Learning for Mean-Variance Trading Strategies",
        "authors": [
            "Owen Futter",
            "Nicola Muca Cirone",
            "Blanka Horvath"
        ],
        "comments": "49 pages",
        "subjects": "Trading and Market Microstructure (q-fin.TR); Machine Learning (cs.LG); Mathematical Finance (q-fin.MF); Portfolio Management (q-fin.PM)",
        "abstract": "In this article, we develop a kernel-based framework for constructing dynamic, pathdependent trading strategies under a mean-variance optimisation criterion. Building on the theoretical results of (Muca Cirone and Salvi, 2025), we parameterise trading strategies as functions in a reproducing kernel Hilbert space (RKHS), enabling a flexible and non-Markovian approach to optimal portfolio problems. We compare this with the signature-based framework of (Futter, Horvath, Wiese, 2023) and demonstrate that both significantly outperform classical Markovian methods when the asset dynamics or predictive signals exhibit temporal dependencies for both synthetic and market-data examples. Using kernels in this context provides significant modelling flexibility, as the choice of feature embedding can range from randomised signatures to the final layers of neural network architectures. Crucially, our framework retains closed-form solutions and provides an alternative to gradient-based optimisation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 98,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10710",
        "abs_url": "https://arxiv.org/abs/2507.10710",
        "pdf_url": "https://arxiv.org/pdf/2507.10710",
        "title": "Robust Multi-Manifold Clustering via Simplex Paths",
        "authors": [
            "Haoyu Chen",
            "Anna Little",
            "Akin Narayan"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "This article introduces a novel, geometric approach for multi-manifold clustering (MMC), i.e. for clustering a collection of potentially intersecting, d-dimensional manifolds into the individual manifold components. We first compute a locality graph on d-simplices, using the dihedral angle in between adjacent simplices as the graph weights, and then compute infinity path distances in this simplex graph. This procedure gives a metric on simplices which we refer to as the largest angle path distance (LAPD). We analyze the properties of LAPD under random sampling, and prove that with an appropriate denoising procedure, this metric separates the manifold components with high probability. We validate the proposed methodology with extensive numerical experiments on both synthetic and real-world data sets. These experiments demonstrate that the method is robust to noise, curvature, and small intersection angle, and generally out-performs other MMC algorithms. In addition, we provide a highly scalable implementation of the proposed algorithm, which leverages approximation schemes for infinity path distance to achieve quasi-linear computational complexity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 99,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10715",
        "abs_url": "https://arxiv.org/abs/2507.10715",
        "pdf_url": "https://arxiv.org/pdf/2507.10715",
        "title": "Real-time, Adaptive Radiological Anomaly Detection and Isotope Identification Using Non-negative Matrix Factorization",
        "authors": [
            "Chandler Jones",
            "Mark Bandstra",
            "Stefan Faaland",
            "Yue Shi Lai",
            "Nico Abgrall",
            "Scott Suchyta",
            "Reynold Cooper"
        ],
        "comments": "11 pages, 8 figures",
        "subjects": "Applied Physics (physics.app-ph); Machine Learning (cs.LG)",
        "abstract": "Spectroscopic anomaly detection and isotope identification algorithms are integral components in nuclear nonproliferation applications such as search operations. The task is especially challenging in the case of mobile detector systems due to the fact that the observed gamma-ray background changes more than for a static detector system, and a pretrained background model can easily find itself out of domain. The result is that algorithms may exceed their intended false alarm rate, or sacrifice detection sensitivity in order to maintain the desired false alarm rate. Non-negative matrix factorization (NMF) has been shown to be a powerful tool for spectral anomaly detection and identification, but, like many similar algorithms that rely on data-driven background models, in its conventional implementation it is unable to update in real time to account for environmental changes that affect the background spectroscopic signature. We have developed a novel NMF-based algorithm that periodically updates its background model to accommodate changing environmental conditions. The Adaptive NMF algorithm involves fewer assumptions about its environment, making it more generalizable than existing NMF-based methods while maintaining or exceeding detection performance on simulated and real-world datasets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 100,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10726",
        "abs_url": "https://arxiv.org/abs/2507.10726",
        "pdf_url": "https://arxiv.org/pdf/2507.10726",
        "title": "Extracting Document Relations from Search Corpus by Marginalizing over User Queries",
        "authors": [
            "Yuki Iwamoto",
            "Kaoru Tsunoda",
            "Ken Kaneiwa"
        ],
        "comments": "9 pages, 6 figures",
        "subjects": "Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "Understanding relationships between documents in large-scale corpora is essential for knowledge discovery and information organization. However, existing approaches rely heavily on manual annotation or predefined relationship taxonomies. We propose EDR-MQ (Extracting Document Relations by Marginalizing over User Queries), a novel framework that discovers document relationships through query marginalization. EDR-MQ is based on the insight that strongly related documents often co-occur in results across diverse user queries, enabling us to estimate joint probabilities between document pairs by marginalizing over a collection of queries. To enable this query marginalization approach, we develop Multiply Conditioned Retrieval-Augmented Generation (MC-RAG), which employs conditional retrieval where subsequent document retrievals depend on previously retrieved content. By observing co-occurrence patterns across diverse queries, EDR-MQ estimates joint probabilities between document pairs without requiring labeled training data or predefined taxonomies. Experimental results show that our query marginalization approach successfully identifies meaningful document relationships, revealing topical clusters, evidence chains, and cross-domain connections that are not apparent through traditional similarity-based methods. Our query-driven framework offers a practical approach to document organization that adapts to different user perspectives and information needs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 101,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10743",
        "abs_url": "https://arxiv.org/abs/2507.10743",
        "pdf_url": "https://arxiv.org/pdf/2507.10743",
        "title": "Language Models for Adult Service Website Text Analysis",
        "authors": [
            "Nickolas Freeman",
            "Thanh Nguyen",
            "Gregory Bott",
            "Jason Parton",
            "Collin Francel"
        ],
        "comments": "32 pages, 12 figures, 1 table",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Sex trafficking refers to the use of force, fraud, or coercion to compel an individual to perform in commercial sex acts against their will. Adult service websites (ASWs) have and continue to be linked to sex trafficking, offering a platform for traffickers to advertise their victims. Thus, organizations involved in the fight against sex trafficking often use ASW data when attempting to identify potential sex trafficking victims. A critical challenge in transforming ASW data into actionable insight is text analysis. Previous research using ASW data has shown that ASW ad text is important for linking ads. However, working with this text is challenging due to its extensive use of emojis, poor grammar, and deliberate obfuscation to evade law enforcement scrutiny. We conduct a comprehensive study of language modeling approaches for this application area, including simple information retrieval methods, pre-trained transformers, and custom transformer models. We demonstrate that characteristics of ASW text data allow efficient custom transformer models to be trained with relatively small GPU resources and used efficiently for inference on consumer hardware. Our custom models outperform fine-tuned variants of well-known encoder-only transformer models, including BERT-base, RoBERTa, and ModernBERT, on accuracy, recall, F1 score, and ROC AUC. We demonstrate the use of our best-performing custom configuration on three tasks related to ASW data analysis: (i) decomposing the giant component in a graph representation of ASW data, (ii) clustering ASW ad text, and (iii) using the learned token embeddings to understand the use of emojis in the illicit context we study. The models we develop represent a significant advancement in ASW text analysis, which can be leveraged in a variety of downstream applications and research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 102,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10758",
        "abs_url": "https://arxiv.org/abs/2507.10758",
        "pdf_url": "https://arxiv.org/pdf/2507.10758",
        "title": "IoT Malware Network Traffic Detection using Deep Learning and GraphSAGE Models",
        "authors": [
            "Nikesh Prajapati",
            "Bimal Karki",
            "Saroj Gopali",
            "Akbar Siami Namin"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "This paper intends to detect IoT malicious attacks through deep learning models and demonstrates a comprehensive evaluation of the deep learning and graph-based models regarding malicious network traffic detection. The models particularly are based on GraphSAGE, Bidirectional encoder representations from transformers (BERT), Temporal Convolutional Network (TCN) as well as Multi-Head Attention, together with Bidirectional Long Short-Term Memory (BI-LSTM) Multi-Head Attention and BI-LSTM and LSTM models. The chosen models demonstrated great performance to model temporal patterns and detect feature significance. The observed performance are mainly due to the fact that IoT system traffic patterns are both sequential and diverse, leaving a rich set of temporal patterns for the models to learn. Experimental results showed that BERT maintained the best performance. It achieved 99.94% accuracy rate alongside high precision and recall, F1-score and AUC-ROC score of 99.99% which demonstrates its capabilities through temporal dependency capture. The Multi-Head Attention offered promising results by providing good detection capabilities with interpretable results. On the other side, the Multi-Head Attention model required significant processing time like BI-LSTM variants. The GraphSAGE model achieved good accuracy while requiring the shortest training time but yielded the lowest accuracy, precision, and F1 score compared to the other models",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 103,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10795",
        "abs_url": "https://arxiv.org/abs/2507.10795",
        "pdf_url": "https://arxiv.org/pdf/2507.10795",
        "title": "Multilayer Artificial Benchmark for Community Detection (mABCD)",
        "authors": [
            "≈Åukasz Krai≈Ñski",
            "Micha≈Ç Czuba",
            "Piotr Br√≥dka",
            "Pawe≈Ç Pra≈Çat",
            "Bogumi≈Ç Kami≈Ñski",
            "Fran√ßois Th√©berge"
        ],
        "comments": "28 pages, 15 figures, 7 tables",
        "subjects": "Social and Information Networks (cs.SI); Machine Learning (cs.LG)",
        "abstract": "The Artificial Benchmark for Community Detection (ABCD) model is a random graph model with community structure and power-law distribution for both degrees and community sizes. The model generates graphs similar to the well-known LFR model but it is faster, more interpretable, and can be investigated analytically. In this paper, we use the underlying ingredients of the ABCD model and introduce its variant for multilayer networks, mABCD.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 104,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10835",
        "abs_url": "https://arxiv.org/abs/2507.10835",
        "pdf_url": "https://arxiv.org/pdf/2507.10835",
        "title": "Functional Neural Wavefunction Optimization",
        "authors": [
            "Victor Armegioiu",
            "Juan Carrasquilla",
            "Siddhartha Mishra",
            "Johannes M√ºller",
            "Jannes Nys",
            "Marius Zeinhofer",
            "Hang Zhang"
        ],
        "comments": "",
        "subjects": "Strongly Correlated Electrons (cond-mat.str-el); Machine Learning (cs.LG); Optimization and Control (math.OC); Computational Physics (physics.comp-ph); Quantum Physics (quant-ph)",
        "abstract": "We propose a framework for the design and analysis of optimization algorithms in variational quantum Monte Carlo, drawing on geometric insights into the corresponding function space. The framework translates infinite-dimensional optimization dynamics into tractable parameter-space algorithms through a Galerkin projection onto the tangent space of the variational ansatz. This perspective unifies existing methods such as stochastic reconfiguration and Rayleigh-Gauss-Newton, provides connections to classic function-space algorithms, and motivates the derivation of novel algorithms with geometrically principled hyperparameter choices. We validate our framework with numerical experiments demonstrating its practical relevance through the accurate estimation of ground-state energies for several prototypical models in condensed matter physics modeled with neural network wavefunctions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 105,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10846",
        "abs_url": "https://arxiv.org/abs/2507.10846",
        "pdf_url": "https://arxiv.org/pdf/2507.10846",
        "title": "Winsor-CAM: Human-Tunable Visual Explanations from Deep Networks via Layer-Wise Winsorization",
        "authors": [
            "Casey Wall",
            "Longwei Wang",
            "Rodrigue Rizk",
            "KC Santosh"
        ],
        "comments": "15 pages, 10 figures, 7 tables. Submitted to IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Interpreting the decision-making process of Convolutional Neural Networks (CNNs) is critical for deploying models in high-stakes domains. Gradient-weighted Class Activation Mapping (Grad-CAM) is a widely used method for visual explanations, yet it typically focuses on the final convolutional layer or na√Øvely averages across layers, strategies that can obscure important semantic cues or amplify irrelevant noise. We propose Winsor-CAM, a novel, human-tunable extension of Grad-CAM that generates robust and coherent saliency maps by aggregating information across all convolutional layers. To mitigate the influence of noisy or extreme attribution values, Winsor-CAM applies Winsorization, a percentile-based outlier attenuation technique. A user-controllable threshold allows for semantic-level tuning, enabling flexible exploration of model behavior across representational hierarchies. Evaluations on standard architectures (ResNet50, DenseNet121, VGG16, InceptionV3) using the PASCAL VOC 2012 dataset demonstrate that Winsor-CAM produces more interpretable heatmaps and achieves superior performance in localization metrics, including intersection-over-union and center-of-mass alignment, when compared to Grad-CAM and uniform layer-averaging baselines. Winsor-CAM advances the goal of trustworthy AI by offering interpretable, multi-layer insights with human-in-the-loop control.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 106,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10850",
        "abs_url": "https://arxiv.org/abs/2507.10850",
        "pdf_url": "https://arxiv.org/pdf/2507.10850",
        "title": "HEIMDALL: a grapH-based sEIsMic Detector And Locator for microseismicity",
        "authors": [
            "Matteo Bagagli",
            "Francesco Grigoli",
            "Davide Bacciu"
        ],
        "comments": "",
        "subjects": "Geophysics (physics.geo-ph); Machine Learning (cs.LG)",
        "abstract": "In this work, we present a new deep-learning model for microseismicity monitoring that utilizes continuous spatiotemporal relationships between seismic station recordings, forming an end-to-end pipeline for seismic catalog creation. It employs graph theory and state-of-the-art graph neural network architectures to perform phase picking, association, and event location simultaneously over rolling windows, making it suitable for both playback and near-real-time monitoring. As part of the global strategy to reduce carbon emissions within the broader context of a green-energy transition, there has been growing interest in exploiting enhanced geothermal systems. Tested in the complex geothermal area of Iceland's Hengill region using open-access data from a temporary experiment, our model was trained and validated using both manually revised and automatic seismic catalogs. Results showed a significant increase in event detection compared to previously published automatic systems and reference catalogs, including a $4 M_w$ seismic sequence in December 2018 and a single-day sequence in February 2019. Our method reduces false events, minimizes manual oversight, and decreases the need for extensive tuning of pipelines or transfer learning of deep-learning models. Overall, it validates a robust monitoring tool for geothermal seismic regions, complementing existing systems and enhancing operational risk mitigation during geothermal energy exploitation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 107,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10854",
        "abs_url": "https://arxiv.org/abs/2507.10854",
        "pdf_url": "https://arxiv.org/pdf/2507.10854",
        "title": "PhreshPhish: A Real-World, High-Quality, Large-Scale Phishing Website Dataset and Benchmark",
        "authors": [
            "Thomas Dalton",
            "Hemanth Gowda",
            "Girish Rao",
            "Sachin Pargi",
            "Alireza Hadj Khodabakhshi",
            "Joseph Rombs",
            "Stephan Jou",
            "Manish Marwah"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Phishing remains a pervasive and growing threat, inflicting heavy economic and reputational damage. While machine learning has been effective in real-time detection of phishing attacks, progress is hindered by lack of large, high-quality datasets and benchmarks. In addition to poor-quality due to challenges in data collection, existing datasets suffer from leakage and unrealistic base rates, leading to overly optimistic performance results. In this paper, we introduce PhreshPhish, a large-scale, high-quality dataset of phishing websites that addresses these limitations. Compared to existing public datasets, PhreshPhish is substantially larger and provides significantly higher quality, as measured by the estimated rate of invalid or mislabeled data points. Additionally, we propose a comprehensive suite of benchmark datasets specifically designed for realistic model evaluation by minimizing leakage, increasing task difficulty, enhancing dataset diversity, and adjustment of base rates more likely to be seen in the real world. We train and evaluate multiple solution approaches to provide baseline performance on the benchmark sets. We believe the availability of this dataset and benchmarks will enable realistic, standardized model comparison and foster further advances in phishing detection. The datasets and benchmarks are available on Hugging Face (this https URL).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 108,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10877",
        "abs_url": "https://arxiv.org/abs/2507.10877",
        "pdf_url": "https://arxiv.org/pdf/2507.10877",
        "title": "BioScore: A Foundational Scoring Function For Diverse Biomolecular Complexes",
        "authors": [
            "Yuchen Zhu",
            "Jihong Chen",
            "Yitong Li",
            "Xiaomin Fang",
            "Xianbin Ye",
            "Jingzhou He",
            "Xujun Zhang",
            "Jingxuan Ge",
            "Chao Shen",
            "Xiaonan Zhang",
            "Tingjun Hou",
            "Chang-Yu Hsieh"
        ],
        "comments": "",
        "subjects": "Chemical Physics (physics.chem-ph); Machine Learning (cs.LG); Biological Physics (physics.bio-ph)",
        "abstract": "Structural assessment of biomolecular complexes is vital for translating molecular models into functional insights, shaping our understanding of biology and aiding drug discovery. However, current structure-based scoring functions often lack generalizability across diverse biomolecular systems. We present BioScore, a foundational scoring function that addresses key challenges -- data sparsity, cross-system representation, and task compatibility -- through a dual-scale geometric graph learning framework with tailored modules for structure assessment and affinity prediction. BioScore supports a wide range of tasks, including affinity prediction, conformation ranking, and structure-based virtual screening. Evaluated on 16 benchmarks spanning proteins, nucleic acids, small molecules, and carbohydrates, BioScore consistently outperforms or matches 70 traditional and deep learning methods. Our newly proposed PPI Benchmark further enables comprehensive evaluation of protein-protein complex scoring. BioScore demonstrates broad applicability: (1) pretraining on mixed-structure data boosts protein-protein affinity prediction by up to 40% and antigen-antibody binding correlation by over 90%; (2) cross-system generalizability enables zero- and few-shot prediction with up to 71% correlation gain; and (3) its unified representation captures chemically challenging systems such as cyclic peptides, improving affinity prediction by over 60%. BioScore establishes a robust and generalizable framework for structural assessment across complex biomolecular landscapes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 109,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10893",
        "abs_url": "https://arxiv.org/abs/2507.10893",
        "pdf_url": "https://arxiv.org/pdf/2507.10893",
        "title": "Modernizing CNN-based Weather Forecast Model towards Higher Computational Efficiency",
        "authors": [
            "Minjong Cheon",
            "Eunhan Goo",
            "Su-Hyeon Shin",
            "Muhammad Ahmed",
            "Hyungjun Kim"
        ],
        "comments": "26pages, 9 Figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Atmospheric and Oceanic Physics (physics.ao-ph)",
        "abstract": "Recently, AI-based weather forecast models have achieved impressive advances. These models have reached accuracy levels comparable to traditional NWP systems, marking a significant milestone in data-driven weather prediction. However, they mostly leverage Transformer-based architectures, which often leads to high training complexity and resource demands due to the massive parameter sizes. In this study, we introduce a modernized CNN-based model for global weather forecasting that delivers competitive accuracy while significantly reducing computational requirements. To present a systematic modernization roadmap, we highlight key architectural enhancements across multiple design scales from an earlier CNN-based approach. KAI-a incorporates a scale-invariant architecture and InceptionNeXt-based blocks within a geophysically-aware design, tailored to the structure of Earth system data. Trained on the ERA5 daily dataset with 67 atmospheric variables, the model contains about 7 million parameters and completes training in just 12 hours on a single NVIDIA L40s GPU. Our evaluation shows that KAI-a matches the performance of state-of-the-art models in medium-range weather forecasting, while offering a significantly lightweight design. Furthermore, case studies on the 2018 European heatwave and the East Asian summer monsoon demonstrate KAI-a's robust skill in capturing extreme events, reinforcing its practical utility.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 110,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10895",
        "abs_url": "https://arxiv.org/abs/2507.10895",
        "pdf_url": "https://arxiv.org/pdf/2507.10895",
        "title": "Commuting Distance Regularization for Timescale-Dependent Label Inconsistency in EEG Emotion Recognition",
        "authors": [
            "Xiaocong Zeng",
            "Craig Michoski",
            "Yan Pang",
            "Dongyang Kuang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Signal Processing (eess.SP)",
        "abstract": "In this work, we address the often-overlooked issue of Timescale Dependent Label Inconsistency (TsDLI) in training neural network models for EEG-based human emotion recognition. To mitigate TsDLI and enhance model generalization and explainability, we propose two novel regularization strategies: Local Variation Loss (LVL) and Local-Global Consistency Loss (LGCL). Both methods incorporate classical mathematical principles--specifically, functions of bounded variation and commute-time distances--within a graph theoretic framework. Complementing our regularizers, we introduce a suite of new evaluation metrics that better capture the alignment between temporally local predictions and their associated global emotion labels. We validate our approach through comprehensive experiments on two widely used EEG emotion datasets, DREAMER and DEAP, across a range of neural architectures including LSTM and transformer-based models. Performance is assessed using five distinct metrics encompassing both quantitative accuracy and qualitative consistency. Results consistently show that our proposed methods outperform state-of-the-art baselines, delivering superior aggregate performance and offering a principled trade-off between interpretability and predictive power under label inconsistency. Notably, LVL achieves the best aggregate rank across all benchmarked backbones and metrics, while LGCL frequently ranks the second, highlighting the effectiveness of our framework.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 111,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10903",
        "abs_url": "https://arxiv.org/abs/2507.10903",
        "pdf_url": "https://arxiv.org/pdf/2507.10903",
        "title": "LiLM-RDB-SFC: Lightweight Language Model with Relational Database-Guided DRL for Optimized SFC Provisioning",
        "authors": [
            "Parisa Fard Moshiri",
            "Xinyu Zhu",
            "Poonam Lohan",
            "Burak Kantarci",
            "Emil Janulewicz"
        ],
        "comments": "9 pages, 6 figures, Accepted to IEEE 16th International Conference on Network of the Future (NoF) 2025",
        "subjects": "Networking and Internet Architecture (cs.NI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Effective management of Service Function Chains (SFCs) and optimal Virtual Network Function (VNF) placement are critical challenges in modern Software-Defined Networking (SDN) and Network Function Virtualization (NFV) environments. Although Deep Reinforcement Learning (DRL) is widely adopted for dynamic network decision-making, its inherent dependency on structured data and fixed action rules often limits adaptability and responsiveness, particularly under unpredictable network conditions. This paper introduces LiLM-RDB-SFC, a novel approach combining Lightweight Language Model (LiLM) with Relational Database (RDB) to answer network state queries to guide DRL model for efficient SFC provisioning. Our proposed approach leverages two LiLMs, Bidirectional and Auto-Regressive Transformers (BART) and the Fine-tuned Language Net T5 (FLAN-T5), to interpret network data and support diverse query types related to SFC demands, data center resources, and VNF availability. Results demonstrate that FLAN-T5 outperforms BART with a lower test loss (0.00161 compared to 0.00734), higher accuracy (94.79% compared to 80.2%), and less processing time (2h 2min compared to 2h 38min). Moreover, when compared to the large language model SQLCoder, FLAN-T5 matches the accuracy of SQLCoder while cutting processing time by 96% (SQLCoder: 54 h 43 min; FLAN-T5: 2 h 2 min).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 112,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10913",
        "abs_url": "https://arxiv.org/abs/2507.10913",
        "pdf_url": "https://arxiv.org/pdf/2507.10913",
        "title": "A Learning Framework For Cooperative Collision Avoidance of UAV Swarms Leveraging Domain Knowledge",
        "authors": [
            "Shuangyao Huang",
            "Haibo Zhang",
            "Zhiyi Huang"
        ],
        "comments": "Under review at AAAI 2026",
        "subjects": "Multiagent Systems (cs.MA); Machine Learning (cs.LG); Robotics (cs.RO)",
        "abstract": "This paper presents a multi-agent reinforcement learning (MARL) framework for cooperative collision avoidance of UAV swarms leveraging domain knowledge-driven reward. The reward is derived from knowledge in the domain of image processing, approximating contours on a two-dimensional field. By modeling obstacles as maxima on the field, collisions are inherently avoided as contours never go through peaks or intersect. Additionally, counters are smooth and energy-efficient. Our framework enables training with large swarm sizes as the agent interaction is minimized and the need for complex credit assignment schemes or observation sharing mechanisms in state-of-the-art MARL approaches are eliminated. Moreover, UAVs obtain the ability to adapt to complex environments where contours may be non-viable or non-existent through intensive training. Extensive experiments are conducted to evaluate the performances of our framework against state-of-the-art MARL algorithms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 113,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10934",
        "abs_url": "https://arxiv.org/abs/2507.10934",
        "pdf_url": "https://arxiv.org/pdf/2507.10934",
        "title": "Towards Practical Benchmarking of Data Cleaning Techniques: On Generating Authentic Errors via Large Language Models",
        "authors": [
            "Xinyuan Liu",
            "Jiahui Chen",
            "Bocheng Hu",
            "Yu Sun",
            "Xinyang Chen",
            "Shaoxu Song"
        ],
        "comments": "",
        "subjects": "Databases (cs.DB); Machine Learning (cs.LG)",
        "abstract": "Data quality remains an important challenge in data-driven systems, as errors in tabular data can severely compromise downstream analytics and machine learning performance. Although numerous error detection algorithms have been proposed, the lack of diverse, real-world error datasets limits comprehensive evaluation. Manual error annotation is both time-consuming and inconsistent, motivating the exploration of synthetic error generation as an alternative. In this work, we introduce TableEG, a framework that leverages large language models (LLMs) to generate authentic errors. By employing a table fine-tuning strategy and a triplet representation $(I, T, O)$ to model error generation, detection, and correction tasks, TableEG captures the complex dependencies inherent in two-dimensional tables. Trained on 12 real-world datasets spanning 10 diverse domains, TableEG ensures that the synthesized errors faithfully reflect authentic error distributions. Experimental results indicate that errors generated by TableEG exhibit superior pattern and distribution similarity compared to both rule-based methods and LLM-generated errors without fine-tuning. Furthermore, performance metrics on TableEG-generated errors closely align with those on real-world errors across nearly all datasets and detection algorithms, particularly for machine learning based detection techniques. Overall, TableEG not only bridges the gap between synthetic and real-world errors but also establishes a robust benchmark for subsequent error detection and correction tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 114,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.10956",
        "abs_url": "https://arxiv.org/abs/2507.10956",
        "pdf_url": "https://arxiv.org/pdf/2507.10956",
        "title": "GOLFS: Feature Selection via Combining Both Global and Local Information for High Dimensional Clustering",
        "authors": [
            "Zhaoyu Xing",
            "Yang Wan",
            "Juan Wen",
            "Wei Zhong"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "It is important to identify the discriminative features for high dimensional clustering. However, due to the lack of cluster labels, the regularization methods developed for supervised feature selection can not be directly applied. To learn the pseudo labels and select the discriminative features simultaneously, we propose a new unsupervised feature selection method, named GlObal and Local information combined Feature Selection (GOLFS), for high dimensional clustering problems. The GOLFS algorithm combines both local geometric structure via manifold learning and global correlation structure of samples via regularized self-representation to select the discriminative features. The combination improves the accuracy of both feature selection and clustering by exploiting more comprehensive information. In addition, an iterative algorithm is proposed to solve the optimization problem and the convergency is proved. Simulations and two real data applications demonstrate the excellent finite-sample performance of GOLFS on both feature selection and clustering.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 115,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11106",
        "abs_url": "https://arxiv.org/abs/2507.11106",
        "pdf_url": "https://arxiv.org/pdf/2507.11106",
        "title": "A Mathematical Optimization Approach to Multisphere Support Vector Data Description",
        "authors": [
            "V√≠ctor Blanco",
            "Inmaculada Espejo",
            "Ra√∫l P√°ez",
            "Antonio M. Rodr√≠guez-Ch√≠a"
        ],
        "comments": "18 pages, 5 figures, 3 tables",
        "subjects": "Optimization and Control (math.OC); Machine Learning (cs.LG)",
        "abstract": "We present a novel mathematical optimization framework for outlier detection in multimodal datasets, extending Support Vector Data Description approaches. We provide a primal formulation, in the shape of a Mixed Integer Second Order Cone model, that constructs Euclidean hyperspheres to identify anomalous observations. Building on this, we develop a dual model that enables the application of the kernel trick, thus allowing for the detection of outliers within complex, non-linear data structures. An extensive computational study demonstrates the effectiveness of our exact method, showing clear advantages over existing heuristic techniques in terms of accuracy and robustness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 116,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11112",
        "abs_url": "https://arxiv.org/abs/2507.11112",
        "pdf_url": "https://arxiv.org/pdf/2507.11112",
        "title": "Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs",
        "authors": [
            "Sanhanat Sivapiromrat",
            "Caiqi Zhang",
            "Marco Basaldella",
            "Nigel Collier"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
        "abstract": "Recent studies have shown that Large Language Models (LLMs) are vulnerable to data poisoning attacks, where malicious training examples embed hidden behaviours triggered by specific input patterns. However, most existing works assume a phrase and focus on the attack's effectiveness, offering limited understanding of trigger mechanisms and how multiple triggers interact within the model. In this paper, we present a framework for studying poisoning in LLMs. We show that multiple distinct backdoor triggers can coexist within a single model without interfering with each other, enabling adversaries to embed several triggers concurrently. Using multiple triggers with high embedding similarity, we demonstrate that poisoned triggers can achieve robust activation even when tokens are substituted or separated by long token spans. Our findings expose a broader and more persistent vulnerability surface in LLMs. To mitigate this threat, we propose a post hoc recovery method that selectively retrains specific model components based on a layer-wise weight difference analysis. Our method effectively removes the trigger behaviour with minimal parameter updates, presenting a practical and efficient defence against multi-trigger poisoning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 117,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11128",
        "abs_url": "https://arxiv.org/abs/2507.11128",
        "pdf_url": "https://arxiv.org/pdf/2507.11128",
        "title": "What Should LLMs Forget? Quantifying Personal Data in LLMs for Right-to-Be-Forgotten Requests",
        "authors": [
            "Dimitri Staufer"
        ],
        "comments": "16 pages, 3 figures. Accepted at the 7th Workshop on eXplainable Knowledge Discovery in Data Mining (XKDD 2025), ECML PKDD 2025, Porto, Portugal",
        "subjects": "Computation and Language (cs.CL); Computers and Society (cs.CY); Machine Learning (cs.LG)",
        "abstract": "Large Language Models (LLMs) can memorize and reveal personal information, raising concerns regarding compliance with the EU's GDPR, particularly the Right to Be Forgotten (RTBF). Existing machine unlearning methods assume the data to forget is already known but do not address how to identify which individual-fact associations are stored in the model. Privacy auditing techniques typically operate at the population level or target a small set of identifiers, limiting applicability to individual-level data inquiries. We introduce WikiMem, a dataset of over 5,000 natural language canaries covering 243 human-related properties from Wikidata, and a model-agnostic metric to quantify human-fact associations in LLMs. Our approach ranks ground-truth values against counterfactuals using calibrated negative log-likelihood across paraphrased prompts. We evaluate 200 individuals across 15 LLMs (410M-70B parameters), showing that memorization correlates with subject web presence and model scale. We provide a foundation for identifying memorized personal data in LLMs at the individual level, enabling the dynamic construction of forget sets for machine unlearning and RTBF requests.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 118,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11129",
        "abs_url": "https://arxiv.org/abs/2507.11129",
        "pdf_url": "https://arxiv.org/pdf/2507.11129",
        "title": "MMOne: Representing Multiple Modalities in One Scene",
        "authors": [
            "Zhifeng Gu",
            "Bing Wang"
        ],
        "comments": "Accepted to ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Humans perceive the world through multimodal cues to understand and interact with the environment. Learning a scene representation for multiple modalities enhances comprehension of the physical world. However, modality conflicts, arising from inherent distinctions among different modalities, present two critical challenges: property disparity and granularity disparity. To address these challenges, we propose a general framework, MMOne, to represent multiple modalities in one scene, which can be readily extended to additional modalities. Specifically, a modality modeling module with a novel modality indicator is proposed to capture the unique properties of each modality. Additionally, we design a multimodal decomposition mechanism to separate multi-modal Gaussians into single-modal Gaussians based on modality differences. We address the essential distinctions among modalities by disentangling multimodal information into shared and modality-specific components, resulting in a more compact and efficient multimodal scene representation. Extensive experiments demonstrate that our method consistently enhances the representation capability for each modality and is scalable to additional modalities. The code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 119,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11136",
        "abs_url": "https://arxiv.org/abs/2507.11136",
        "pdf_url": "https://arxiv.org/pdf/2507.11136",
        "title": "Interpretable Bayesian Tensor Network Kernel Machines with Automatic Rank and Feature Selection",
        "authors": [
            "Afra Kilic",
            "Kim Batselier"
        ],
        "comments": "39 pages, 5 figures, 4 tables. Submitted to Journal of Machine Learning Research. The code is available at: this https URL. arXiv admin note: text overlap with arXiv:1401.6497 by other authors",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Tensor Network (TN) Kernel Machines speed up model learning by representing parameters as low-rank TNs, reducing computation and memory use. However, most TN-based Kernel methods are deterministic and ignore parameter uncertainty. Further, they require manual tuning of model complexity hyperparameters like tensor rank and feature dimensions, often through trial-and-error or computationally costly methods like cross-validation. We propose Bayesian Tensor Network Kernel Machines, a fully probabilistic framework that uses sparsity-inducing hierarchical priors on TN factors to automatically infer model complexity. This enables automatic inference of tensor rank and feature dimensions, while also identifying the most relevant features for prediction, thereby enhancing model interpretability. All the model parameters and hyperparameters are treated as latent variables with corresponding priors. Given the Bayesian approach and latent variable dependencies, we apply a mean-field variational inference to approximate their posteriors. We show that applying a mean-field approximation to TN factors yields a Bayesian ALS algorithm with the same computational complexity as its deterministic counterpart, enabling uncertainty quantification at no extra computational cost. Experiments on synthetic and real-world datasets demonstrate the superior performance of our model in prediction accuracy, uncertainty quantification, interpretability, and scalability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 120,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11137",
        "abs_url": "https://arxiv.org/abs/2507.11137",
        "pdf_url": "https://arxiv.org/pdf/2507.11137",
        "title": "Hashed Watermark as a Filter: Defeating Forging and Overwriting Attacks in Weight-based Neural Network Watermarking",
        "authors": [
            "Yuan Yao",
            "Jin Song",
            "Jian Jin"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
        "abstract": "As valuable digital assets, deep neural networks necessitate robust ownership protection, positioning neural network watermarking (NNW) as a promising solution. Among various NNW approaches, weight-based methods are favored for their simplicity and practicality; however, they remain vulnerable to forging and overwriting attacks. To address those challenges, we propose NeuralMark, a robust method built around a hashed watermark filter. Specifically, we utilize a hash function to generate an irreversible binary watermark from a secret key, which is then used as a filter to select the model parameters for embedding. This design cleverly intertwines the embedding parameters with the hashed watermark, providing a robust defense against both forging and overwriting attacks. An average pooling is also incorporated to resist fine-tuning and pruning attacks. Furthermore, it can be seamlessly integrated into various neural network architectures, ensuring broad applicability. Theoretically, we analyze its security boundary. Empirically, we verify its effectiveness and robustness across 13 distinct Convolutional and Transformer architectures, covering five image classification tasks and one text generation task. The source codes are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 121,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11143",
        "abs_url": "https://arxiv.org/abs/2507.11143",
        "pdf_url": "https://arxiv.org/pdf/2507.11143",
        "title": "RMAU-NET: A Residual-Multihead-Attention U-Net Architecture for Landslide Segmentation and Detection from Remote Sensing Images",
        "authors": [
            "Lam Pham",
            "Cam Le",
            "Hieu Tang",
            "Khang Truong",
            "Truong Nguyen",
            "Jasmin Lampert",
            "Alexander Schindler",
            "Martin Boyer",
            "Son Phan"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Image and Video Processing (eess.IV)",
        "abstract": "In recent years, landslide disasters have reported frequently due to the extreme weather events of droughts, floods , storms, or the consequence of human activities such as deforestation, excessive exploitation of natural resources. However, automatically observing landslide is challenging due to the extremely large observing area and the rugged topography such as mountain or highland. This motivates us to propose an end-to-end deep-learning-based model which explores the remote sensing images for automatically observing landslide events. By considering remote sensing images as the input data, we can obtain free resource, observe large and rough terrains by time. To explore the remote sensing images, we proposed a novel neural network architecture which is for two tasks of landslide detection and landslide segmentation. We evaluated our proposed model on three different benchmark datasets of LandSlide4Sense, Bijie, and Nepal. By conducting extensive experiments, we achieve F1 scores of 98.23, 93.83 for the landslide detection task on LandSlide4Sense, Bijie datasets; mIoU scores of 63.74, 76.88 on the segmentation tasks regarding LandSlide4Sense, Nepal datasets. These experimental results prove potential to integrate our proposed model into real-life landslide observation systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 122,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11161",
        "abs_url": "https://arxiv.org/abs/2507.11161",
        "pdf_url": "https://arxiv.org/pdf/2507.11161",
        "title": "How does Labeling Error Impact Contrastive Learning? A Perspective from Data Dimensionality Reduction",
        "authors": [
            "Jun Chen",
            "Hong Chen",
            "Yonghua Yu",
            "Yiming Ying"
        ],
        "comments": "Published as ICML2025 poster. The arXiv version is a modified version",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "In recent years, contrastive learning has achieved state-of-the-art performance in the territory of self-supervised representation learning. Many previous works have attempted to provide the theoretical understanding underlying the success of contrastive learning. Almost all of them rely on a default assumption, i.e., the label consistency assumption, which may not hold in practice (the probability of failure is called labeling error) due to the strength and randomness of common augmentation strategies, such as random resized crop (RRC). This paper investigates the theoretical impact of labeling error on the downstream classification performance of contrastive learning. We first reveal several significant negative impacts of labeling error on downstream classification risk. To mitigate these impacts, data dimensionality reduction method (e.g., singular value decomposition, SVD) is applied on original data to reduce false positive samples, and establish both theoretical and empirical evaluations. Moreover, it is also found that SVD acts as a double-edged sword, which may lead to the deterioration of downstream classification accuracy due to the reduced connectivity of the augmentation graph. Based on the above observations, we give the augmentation suggestion that we should use some moderate embedding dimension (such as $512, 1024$ in our experiments), data inflation, weak augmentation, and SVD to ensure large graph connectivity and small labeling error to improve model performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 123,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11168",
        "abs_url": "https://arxiv.org/abs/2507.11168",
        "pdf_url": "https://arxiv.org/pdf/2507.11168",
        "title": "Improving Wi-Fi Network Performance Prediction with Deep Learning Models",
        "authors": [
            "Gabriele Formis",
            "Amanda Ericson",
            "Stefan Forsstrom",
            "Kyi Thar",
            "Gianluca Cena",
            "Stefano Scanzio"
        ],
        "comments": "preprint accepted, 8 pages, 2025",
        "subjects": "Networking and Internet Architecture (cs.NI); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Signal Processing (eess.SP)",
        "abstract": "The increasing need for robustness, reliability, and determinism in wireless networks for industrial and mission-critical applications is the driver for the growth of new innovative methods. The study presented in this work makes use of machine learning techniques to predict channel quality in a Wi-Fi network in terms of the frame delivery ratio. Predictions can be used proactively to adjust communication parameters at runtime and optimize network operations for industrial applications. Methods including convolutional neural networks and long short-term memory were analyzed on datasets acquired from a real Wi-Fi setup across multiple channels. The models were compared in terms of prediction accuracy and computational complexity. Results show that the frame delivery ratio can be reliably predicted, and convolutional neural networks, although slightly less effective than other models, are more efficient in terms of CPU usage and memory consumption. This enhances the model's usability on embedded and industrial systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 124,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11191",
        "abs_url": "https://arxiv.org/abs/2507.11191",
        "pdf_url": "https://arxiv.org/pdf/2507.11191",
        "title": "Data-Driven Differential Evolution in Tire Industry Extrusion: Leveraging Surrogate Models",
        "authors": [
            "Eider Garate-Perez",
            "Kerman L√≥pez de Calle-Etxabe",
            "Susana Ferreiro"
        ],
        "comments": "22 pages, 15 figures",
        "subjects": "Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)",
        "abstract": "The optimization of industrial processes remains a critical challenge, particularly when no mathematical formulation of objective functions or constraints is available. This study addresses this issue by proposing a surrogate-based, data-driven methodology for optimizing complex real-world manufacturing systems using only historical process data. Machine learning models are employed to approximate system behavior and construct surrogate models, which are integrated into a tailored metaheuristic approach: Data-Driven Differential Evolution with Multi-Level Penalty Functions and Surrogate Models, an adapted version of Differential Evolution suited to the characteristics of the studied process. The methodology is applied to an extrusion process in the tire manufacturing industry, with the goal of optimizing initialization parameters to reduce waste and production time. Results show that the surrogate-based optimization approach outperforms historical best configurations, achieving a 65\\% reduction in initialization and setup time, while also significantly minimizing material waste. These findings highlight the potential of combining data-driven modeling and metaheuristic optimization for industrial processes where explicit formulations are unavailable.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 125,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11192",
        "abs_url": "https://arxiv.org/abs/2507.11192",
        "pdf_url": "https://arxiv.org/pdf/2507.11192",
        "title": "Recent Advances in Simulation-based Inference for Gravitational Wave Data Analysis",
        "authors": [
            "Bo Liang",
            "He Wang"
        ],
        "comments": "30 pages, 6 figures, 1 table. Minor clarifications added on page 3. Literature covered up to early 2025",
        "subjects": "General Relativity and Quantum Cosmology (gr-qc); High Energy Astrophysical Phenomena (astro-ph.HE); Instrumentation and Methods for Astrophysics (astro-ph.IM); Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "The detection of gravitational waves by the LIGO-Virgo-KAGRA collaboration has ushered in a new era of observational astronomy, emphasizing the need for rapid and detailed parameter estimation and population-level analyses. Traditional Bayesian inference methods, particularly Markov chain Monte Carlo, face significant computational challenges when dealing with the high-dimensional parameter spaces and complex noise characteristics inherent in gravitational wave data. This review examines the emerging role of simulation-based inference methods in gravitational wave astronomy, with a focus on approaches that leverage machine-learning techniques such as normalizing flows and neural posterior estimation. We provide a comprehensive overview of the theoretical foundations underlying various simulation-based inference methods, including neural posterior estimation, neural ratio estimation, neural likelihood estimation, flow matching, and consistency models. We explore the applications of these methods across diverse gravitational wave data processing scenarios, from single-source parameter estimation and overlapping signal analysis to testing general relativity and conducting population studies. Although these techniques demonstrate speed improvements over traditional methods in controlled studies, their model-dependent nature and sensitivity to prior assumptions are barriers to their widespread adoption. Their accuracy, which is similar to that of conventional methods, requires further validation across broader parameter spaces and noise conditions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 126,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11202",
        "abs_url": "https://arxiv.org/abs/2507.11202",
        "pdf_url": "https://arxiv.org/pdf/2507.11202",
        "title": "A Robust Incomplete Multimodal Low-Rank Adaptation Approach for Emotion Recognition",
        "authors": [
            "Xinkui Zhao",
            "Jinsong Shu",
            "Yangyang Wu",
            "Guanjie Cheng",
            "Zihe Liu",
            "Naibo Wang",
            "Shuiguang Deng",
            "Zhongle Xie",
            "Jianwei Yin"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Multimodal Emotion Recognition (MER) often encounters incomplete multimodality in practical applications due to sensor failures or privacy protection requirements. While existing methods attempt to address various incomplete multimodal scenarios by balancing the training of each modality combination through additional gradients, these approaches face a critical limitation: training gradients from different modality combinations conflict with each other, ultimately degrading the performance of the final prediction model. In this paper, we propose a unimodal decoupled dynamic low-rank adaptation method based on modality combinations, named MCULoRA, which is a novel framework for the parameter-efficient training of incomplete multimodal learning models. MCULoRA consists of two key modules, modality combination aware low-rank adaptation (MCLA) and dynamic parameter fine-tuning (DPFT). The MCLA module effectively decouples the shared information from the distinct characteristics of individual modality combinations. The DPFT module adjusts the training ratio of modality combinations based on the separability of each modality's representation space, optimizing the learning efficiency across different modality combinations. Our extensive experimental evaluation in multiple benchmark datasets demonstrates that MCULoRA substantially outperforms previous incomplete multimodal learning approaches in downstream task accuracy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 127,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11229",
        "abs_url": "https://arxiv.org/abs/2507.11229",
        "pdf_url": "https://arxiv.org/pdf/2507.11229",
        "title": "DuetGraph: Coarse-to-Fine Knowledge Graph Reasoning with Dual-Pathway Global-Local Fusion",
        "authors": [
            "Jin Li",
            "Zezhong Ding",
            "Xike Xie"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Knowledge graphs (KGs) are vital for enabling knowledge reasoning across various domains. Recent KG reasoning methods that integrate both global and local information have achieved promising results. However, existing methods often suffer from score over-smoothing, which blurs the distinction between correct and incorrect answers and hinders reasoning effectiveness. To address this, we propose DuetGraph, a coarse-to-fine KG reasoning mechanism with dual-pathway global-local fusion. DuetGraph tackles over-smoothing by segregating -- rather than stacking -- the processing of local (via message passing) and global (via attention) information into two distinct pathways, preventing mutual interference and preserving representational discrimination. In addition, DuetGraph introduces a coarse-to-fine optimization, which partitions entities into high- and low-score subsets. This strategy narrows the candidate space and sharpens the score gap between the two subsets, which alleviates over-smoothing and enhances inference quality. Extensive experiments on various datasets demonstrate that DuetGraph achieves state-of-the-art (SOTA) performance, with up to an 8.7% improvement in reasoning quality and a 1.8$\\times$ acceleration in training efficiency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 128,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11236",
        "abs_url": "https://arxiv.org/abs/2507.11236",
        "pdf_url": "https://arxiv.org/pdf/2507.11236",
        "title": "Improved sampling algorithms and Poincar√© inequalities for non-log-concave distributions",
        "authors": [
            "Yuchen He",
            "Zhehan Lei",
            "Jianan Shao",
            "Chihao Zhang"
        ],
        "comments": "",
        "subjects": "Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG); Probability (math.PR); Machine Learning (stat.ML)",
        "abstract": "We study the problem of sampling from a distribution $\\mu$ with density $\\propto e^{-V}$ for some potential function $V:\\mathbb R^d\\to \\mathbb R$ with query access to $V$ and $\\nabla V$. We start with the following standard assumptions: (1) The potential function $V$ is $L$-smooth. (2) The second moment $\\mathbf{E}_{X\\sim \\mu}[\\|X\\|^2]\\leq M$. Recently, He and Zhang (COLT'25) showed that the query complexity of sampling from such distributions is at least $\\left(\\frac{LM}{d\\epsilon}\\right)^{\\Omega(d)}$ where $\\epsilon$ is the desired accuracy in total variation distance, and the Poincar√© constant can be arbitrarily large. Meanwhile, another common assumption in the study of diffusion based samplers (see e.g., the work of Chen, Chewi, Li, Li, Salim and Zhang (ICLR'23)) strengthens the smoothness condition (1) to the following: (1*) The potential function of *every* distribution along the Ornstein-Uhlenbeck process starting from $\\mu$ is $L$-smooth. We show that under the assumptions (1*) and (2), the query complexity of sampling from $\\mu$ can be $\\mathrm{poly}(L,d)\\cdot \\left(\\frac{Ld+M}{\\epsilon^2}\\right)^{\\mathcal{O}(L+1)}$, which is polynomial in $d$ and $\\frac{1}{\\epsilon}$ when $L=\\mathcal{O}(1)$ and $M=\\mathrm{poly}(d)$. This improves the algorithm with quasi-polynomial query complexity developed by Huang et al. (COLT'24). Our results imply that the seemly moderate strengthening of the smoothness condition (1) to (1*) can lead to an exponential gap in the query complexity of sampling algorithms. Moreover, we show that together with the assumption (1*) and the stronger moment assumption that $\\|X\\|$ is $\\lambda$-sub-Gaussian for $X\\sim\\mu$, the Poincar√© constant of $\\mu$ is at most $\\mathcal{O}(\\lambda)^{2(L+1)}$. As an application of our technique, we obtain improved estimate of the Poincar√© constant for mixture of Gaussians with the same covariance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 129,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11247",
        "abs_url": "https://arxiv.org/abs/2507.11247",
        "pdf_url": "https://arxiv.org/pdf/2507.11247",
        "title": "Fairness-Aware Grouping for Continuous Sensitive Variables: Application for Debiasing Face Analysis with respect to Skin Tone",
        "authors": [
            "Veronika Shilova",
            "Emmanuel Malherbe",
            "Giovanni Palma",
            "Laurent Risser",
            "Jean-Michel Loubes"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Within a legal framework, fairness in datasets and models is typically assessed by dividing observations into predefined groups and then computing fairness measures (e.g., Disparate Impact or Equality of Odds with respect to gender). However, when sensitive attributes such as skin color are continuous, dividing into default groups may overlook or obscure the discrimination experienced by certain minority subpopulations. To address this limitation, we propose a fairness-based grouping approach for continuous (possibly multidimensional) sensitive attributes. By grouping data according to observed levels of discrimination, our method identifies the partition that maximizes a novel criterion based on inter-group variance in discrimination, thereby isolating the most critical subgroups. We validate the proposed approach using multiple synthetic datasets and demonstrate its robustness under changing population distributions - revealing how discrimination is manifested within the space of sensitive attributes. Furthermore, we examine a specialized setting of monotonic fairness for the case of skin color. Our empirical results on both CelebA and FFHQ, leveraging the skin tone as predicted by an industrial proprietary algorithm, show that the proposed segmentation uncovers more nuanced patterns of discrimination than previously reported, and that these findings remain stable across datasets for a given model. Finally, we leverage our grouping model for debiasing purpose, aiming at predicting fair scores with group-by-group post-processing. The results demonstrate that our approach improves fairness while having minimal impact on accuracy, thus confirming our partition method and opening the door for industrial deployment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 130,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11316",
        "abs_url": "https://arxiv.org/abs/2507.11316",
        "pdf_url": "https://arxiv.org/pdf/2507.11316",
        "title": "Internal Value Alignment in Large Language Models through Controlled Value Vector Activation",
        "authors": [
            "Haoran Jin",
            "Meng Li",
            "Xiting Wang",
            "Zhihao Xu",
            "Minlie Huang",
            "Yantao Jia",
            "Defu Lian"
        ],
        "comments": "25 pages, 14 figures. Accepted by ACL 2025 (main conference)",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Aligning Large Language Models (LLMs) with human values has attracted increasing attention since it provides clarity, transparency, and the ability to adapt to evolving scenarios. In this paper, we introduce a Controlled Value Vector Activation (ConVA) method that directly aligns the internal values of LLMs by interpreting how a value is encoded in their latent representations and modifies relevant activations to ensure consistent values in LLMs. To ensure an accurate and unbiased interpretation, we propose a context-controlled value vector identification method. To consistently control values without sacrificing model performance, we introduce a gated value vector activation method for effective and minimum degree of value control. Experiments show that our method achieves the highest control success rate across 10 basic values without hurting LLM performance and fluency, and ensures target values even with opposite and potentially malicious input prompts. Source code and data are available at~ this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 131,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11366",
        "abs_url": "https://arxiv.org/abs/2507.11366",
        "pdf_url": "https://arxiv.org/pdf/2507.11366",
        "title": "A Parallelizable Approach for Characterizing NE in Zero-Sum Games After a Linear Number of Iterations of Gradient Descent",
        "authors": [
            "Taemin Kim",
            "James P. Bailey"
        ],
        "comments": "",
        "subjects": "Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG)",
        "abstract": "We study online optimization methods for zero-sum games, a fundamental problem in adversarial learning in machine learning, economics, and many other domains. Traditional methods approximate Nash equilibria (NE) using either regret-based methods (time-average convergence) or contraction-map-based methods (last-iterate convergence). We propose a new method based on Hamiltonian dynamics in physics and prove that it can characterize the set of NE in a finite (linear) number of iterations of alternating gradient descent in the unbounded setting, modulo degeneracy, a first in online optimization. Unlike standard methods for computing NE, our proposed approach can be parallelized and works with arbitrary learning rates, both firsts in algorithmic game theory. Experimentally, we support our results by showing our approach drastically outperforms standard methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 132,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11381",
        "abs_url": "https://arxiv.org/abs/2507.11381",
        "pdf_url": "https://arxiv.org/pdf/2507.11381",
        "title": "From Observational Data to Clinical Recommendations: A Causal Framework for Estimating Patient-level Treatment Effects and Learning Policies",
        "authors": [
            "Rom Gutman",
            "Shimon Sheiba",
            "Omer Noy Klein",
            "Naama Dekel Bird",
            "Amit Gruber",
            "Doron Aronson",
            "Oren Caspi",
            "Uri Shalit"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Applications (stat.AP)",
        "abstract": "We propose a framework for building patient-specific treatment recommendation models, building on the large recent literature on learning patient-level causal models and inspired by the target trial paradigm of Hernan and Robins. We focus on safety and validity, including the crucial issue of causal identification when using observational data. We do not provide a specific model, but rather a way to integrate existing methods and know-how into a practical pipeline. We further provide a real world use-case of treatment optimization for patients with heart failure who develop acute kidney injury during hospitalization. The results suggest our pipeline can improve patient outcomes over the current treatment regime.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 133,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11385",
        "abs_url": "https://arxiv.org/abs/2507.11385",
        "pdf_url": "https://arxiv.org/pdf/2507.11385",
        "title": "Joint space-time wind field data extrapolation and uncertainty quantification using nonparametric Bayesian dictionary learning",
        "authors": [
            "George D. Pasparakis",
            "Ioannis A. Kougioumtzoglou",
            "Michael D. Shields"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "A methodology is developed, based on nonparametric Bayesian dictionary learning, for joint space-time wind field data extrapolation and estimation of related statistics by relying on limited/incomplete measurements. Specifically, utilizing sparse/incomplete measured data, a time-dependent optimization problem is formulated for determining the expansion coefficients of an associated low-dimensional representation of the stochastic wind field. Compared to an alternative, standard, compressive sampling treatment of the problem, the developed methodology exhibits the following advantages. First, the Bayesian formulation enables also the quantification of the uncertainty in the estimates. Second, the requirement in standard CS-based applications for an a priori selection of the expansion basis is circumvented. Instead, this is done herein in an adaptive manner based on the acquired data. Overall, the methodology exhibits enhanced extrapolation accuracy, even in cases of high-dimensional data of arbitrary form, and of relatively large extrapolation distances. Thus, it can be used, potentially, in a wide range of wind engineering applications where various constraints dictate the use of a limited number of sensors. The efficacy of the methodology is demonstrated by considering two case studies. The first relates to the extrapolation of simulated wind velocity records consistent with a prescribed joint wavenumber-frequency power spectral density in a three-dimensional domain (2D and time). The second pertains to the extrapolation of four-dimensional (3D and time) boundary layer wind tunnel experimental data that exhibit significant spatial variability and non-Gaussian characteristics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 134,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11387",
        "abs_url": "https://arxiv.org/abs/2507.11387",
        "pdf_url": "https://arxiv.org/pdf/2507.11387",
        "title": "From Kinetic Theory to AI: a Rediscovery of High-Dimensional Divergences and Their Properties",
        "authors": [
            "Gennaro Auricchio",
            "Giovanni Brigati",
            "Paolo Giudici",
            "Giuseppe Toscani"
        ],
        "comments": "",
        "subjects": "Mathematical Physics (math-ph); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multiagent Systems (cs.MA)",
        "abstract": "Selecting an appropriate divergence measure is a critical aspect of machine learning, as it directly impacts model performance. Among the most widely used, we find the Kullback-Leibler (KL) divergence, originally introduced in kinetic theory as a measure of relative entropy between probability distributions. Just as in machine learning, the ability to quantify the proximity of probability distributions plays a central role in kinetic theory. In this paper, we present a comparative review of divergence measures rooted in kinetic theory, highlighting their theoretical foundations and exploring their potential applications in machine learning and artificial intelligence.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 135,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11400",
        "abs_url": "https://arxiv.org/abs/2507.11400",
        "pdf_url": "https://arxiv.org/pdf/2507.11400",
        "title": "The model is the message: Lightweight convolutional autoencoders applied to noisy imaging data for planetary science and astrobiology",
        "authors": [
            "Caleb Scharf"
        ],
        "comments": "28 pages, 8 figures, accepted for publication in Icarus",
        "subjects": "Earth and Planetary Astrophysics (astro-ph.EP); Instrumentation and Methods for Astrophysics (astro-ph.IM); Machine Learning (cs.LG)",
        "abstract": "The application of convolutional autoencoder deep learning to imaging data for planetary science and astrobiological use is briefly reviewed and explored with a focus on the need to understand algorithmic rationale, process, and results when machine learning is utilized. Successful autoencoders train to build a model that captures the features of data in a dimensionally reduced form (the latent representation) that can then be used to recreate the original input. One application is the reconstruction of incomplete or noisy data. Here a baseline, lightweight convolutional autoencoder is used to examine the utility for planetary image reconstruction or inpainting in situations where there is destructive random noise (i.e., either luminance noise with zero returned data in some image pixels, or color noise with random additive levels across pixel channels). It is shown that, in certain use cases, multi-color image reconstruction can be usefully applied even with extensive random destructive noise with 90% areal coverage and higher. This capability is discussed in the context of intentional masking to reduce data bandwidth, or situations with low-illumination levels and other factors that obscure image data (e.g., sensor degradation or atmospheric conditions). It is further suggested that for some scientific use cases the model latent space and representations have more utility than large raw imaging datasets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 136,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11401",
        "abs_url": "https://arxiv.org/abs/2507.11401",
        "pdf_url": "https://arxiv.org/pdf/2507.11401",
        "title": "Stochastic Entanglement Configuration for Constructive Entanglement Topologies in Quantum Machine Learning with Application to Cardiac MRI",
        "authors": [
            "Mehri Mehrnia",
            "Mohammed S.M. Elbaz"
        ],
        "comments": "Accepted for publication at IEEE International Conference on Quantum Computing and Engineering (QCE) 2025",
        "subjects": "Quantum Physics (quant-ph); Computer Vision and Pattern Recognition (cs.CV); Emerging Technologies (cs.ET); Machine Learning (cs.LG)",
        "abstract": "Efficient entanglement strategies are essential for advancing variational quantum circuits (VQCs) for quantum machine learning (QML). However, most current approaches use fixed entanglement topologies that are not adaptive to task requirements, limiting potential gains over classical models. We introduce a novel stochastic entanglement configuration method that systematically generates diverse entanglement topologies to identify a subspace of constructive entanglement configurations, defined as entanglement topologies that boost hybrid model performance (e.g., classification accuracy) beyond classical baselines. Each configuration is encoded as a stochastic binary matrix, denoting directed entanglement between qubits. This enables scalable exploration of the hyperspace of candidate entanglement topologies using entanglement density and per-qubit constraints as key metrics. We define unconstrained and constrained sampling modes, controlling entanglement per qubit. Using our method, 400 stochastic configurations were generated and evaluated in a hybrid QML for cardiac MRI disease classification. We identified 64 (16%) novel constructive entanglement configurations that consistently outperformed the classical baseline. Ensemble aggregation of top-performing configurations achieved ~0.92 classification accuracy, exceeding the classical model (~0.87) by over 5%. Compared to four conventional topologies (ring, nearest neighbor, no entanglement, fully entangled), none surpassed the classical baseline (maximum accuracy ~0.82), while our configurations delivered up to ~20% higher accuracy. Thus, highlighting the robustness and generalizability of the identified constructive entanglements.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 137,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11412",
        "abs_url": "https://arxiv.org/abs/2507.11412",
        "pdf_url": "https://arxiv.org/pdf/2507.11412",
        "title": "Seq vs Seq: An Open Suite of Paired Encoders and Decoders",
        "authors": [
            "Orion Weller",
            "Kathryn Ricci",
            "Marc Marone",
            "Antoine Chaffin",
            "Dawn Lawrie",
            "Benjamin Van Durme"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "The large language model (LLM) community focuses almost exclusively on decoder-only language models, since they are easier to use for text generation. However, a large subset of the community still uses encoder-only models for tasks such as classification or retrieval. Previous work has attempted to compare these architectures, but is forced to make comparisons with models that have different numbers of parameters, training techniques, and datasets. We introduce the SOTA open-data Ettin suite of models: paired encoder-only and decoder-only models ranging from 17 million parameters to 1 billion, trained on up to 2 trillion tokens. Using the same recipe for both encoder-only and decoder-only models produces SOTA recipes in both categories for their respective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as decoders. Like previous work, we find that encoder-only models excel at classification and retrieval tasks while decoders excel at generative tasks. However, we show that adapting a decoder model to encoder tasks (and vice versa) through continued training is subpar compared to using only the reverse objective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa for generative tasks). We open-source all artifacts of this study including training data, training order segmented by checkpoint, and 200+ checkpoints to allow future work to analyze or extend all aspects of training.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 138,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11419",
        "abs_url": "https://arxiv.org/abs/2507.11419",
        "pdf_url": "https://arxiv.org/pdf/2507.11419",
        "title": "Better Regret Rates in Bilateral Trade via Sublinear Budget Violation",
        "authors": [
            "Anna Lunghi",
            "Matteo Castiglioni",
            "Alberto Marchesi"
        ],
        "comments": "",
        "subjects": "Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG)",
        "abstract": "Bilateral trade is a central problem in algorithmic economics, and recent work has explored how to design trading mechanisms using no-regret learning algorithms. However, no-regret learning is impossible when budget balance has to be enforced at each time step. Bernasconi et al. [Ber+24] show how this impossibility can be circumvented by relaxing the budget balance constraint to hold only globally over all time steps. In particular, they design an algorithm achieving regret of the order of $\\tilde O(T^{3/4})$ and provide a lower bound of $\\Omega(T^{5/7})$. In this work, we interpolate between these two extremes by studying how the optimal regret rate varies with the allowed violation of the global budget balance constraint. Specifically, we design an algorithm that, by violating the constraint by at most $T^{\\beta}$ for any given $\\beta \\in [\\frac{3}{4}, \\frac{6}{7}]$, attains regret $\\tilde O(T^{1 - \\beta/3})$. We complement this result with a matching lower bound, thus fully characterizing the trade-off between regret and budget violation. Our results show that both the $\\tilde O(T^{3/4})$ upper bound in the global budget balance case and the $\\Omega(T^{5/7})$ lower bound under unconstrained budget balance violation obtained by Bernasconi et al. [Ber+24] are tight.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 139,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11430",
        "abs_url": "https://arxiv.org/abs/2507.11430",
        "pdf_url": "https://arxiv.org/pdf/2507.11430",
        "title": "FLsim: A Modular and Library-Agnostic Simulation Framework for Federated Learning",
        "authors": [
            "Arnab Mukherjee",
            "Raju Halder",
            "Joydeep Chandra"
        ],
        "comments": "",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)",
        "abstract": "Federated Learning (FL) has undergone significant development since its inception in 2016, advancing from basic algorithms to complex methodologies tailored to address diverse challenges and use cases. However, research and benchmarking of novel FL techniques against a plethora of established state-of-the-art solutions remain challenging. To streamline this process, we introduce FLsim, a comprehensive FL simulation framework designed to meet the diverse requirements of FL workflows in the literature. FLsim is characterized by its modularity, scalability, resource efficiency, and controlled reproducibility of experimental outcomes. Its easy to use interface allows users to specify customized FL requirements through job configuration, which supports: (a) customized data distributions, ranging from non-independent and identically distributed (non-iid) data to independent and identically distributed (iid) data, (b) selection of local learning algorithms according to user preferences, with complete agnosticism to ML libraries, (c) choice of network topology illustrating communication patterns among nodes, (d) definition of model aggregation and consensus algorithms, and (e) pluggable blockchain support for enhanced robustness. Through a series of experimental evaluations, we demonstrate the effectiveness and versatility of FLsim in simulating a diverse range of state-of-the-art FL experiments. We envisage that FLsim would mark a significant advancement in FL simulation frameworks, offering unprecedented flexibility and functionality for researchers and practitioners alike.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 140,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11441",
        "abs_url": "https://arxiv.org/abs/2507.11441",
        "pdf_url": "https://arxiv.org/pdf/2507.11441",
        "title": "Implementing Adaptations for Vision AutoRegressive Model",
        "authors": [
            "Kaif Shaikh",
            "Antoni Kowalczuk",
            "Franziska Boenisch",
            "Adam Dziedzic"
        ],
        "comments": "Accepted at DIG-BUGS: Data in Generative Models Workshop @ ICML 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Vision AutoRegressive model (VAR) was recently introduced as an alternative to Diffusion Models (DMs) in image generation domain. In this work we focus on its adaptations, which aim to fine-tune pre-trained models to perform specific downstream tasks, like medical data generation. While for DMs there exist many techniques, adaptations for VAR remain underexplored. Similarly, differentially private (DP) adaptations-ones that aim to preserve privacy of the adaptation data-have been extensively studied for DMs, while VAR lacks such solutions. In our work, we implement and benchmark many strategies for VAR, and compare them to state-of-the-art DM adaptation strategies. We observe that VAR outperforms DMs for non-DP adaptations, however, the performance of DP suffers, which necessitates further research in private adaptations for VAR. Code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 141,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11473",
        "abs_url": "https://arxiv.org/abs/2507.11473",
        "pdf_url": "https://arxiv.org/pdf/2507.11473",
        "title": "Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety",
        "authors": [
            "Tomek Korbak",
            "Mikita Balesni",
            "Elizabeth Barnes",
            "Yoshua Bengio",
            "Joe Benton",
            "Joseph Bloom",
            "Mark Chen",
            "Alan Cooney",
            "Allan Dafoe",
            "Anca Dragan",
            "Scott Emmons",
            "Owain Evans",
            "David Farhi",
            "Ryan Greenblatt",
            "Dan Hendrycks",
            "Marius Hobbhahn",
            "Evan Hubinger",
            "Geoffrey Irving",
            "Erik Jenner",
            "Daniel Kokotajlo",
            "Victoria Krakovna",
            "Shane Legg",
            "David Lindner",
            "David Luan",
            "Aleksander MƒÖdry",
            "Julian Michael",
            "Neel Nanda",
            "Dave Orr",
            "Jakub Pachocki",
            "Ethan Perez",
            "Mary Phuong",
            "Fabien Roger",
            "Joshua Saxe",
            "Buck Shlegeris",
            "Mart√≠n Soto",
            "Eric Steinberger",
            "Jasmine Wang",
            "Wojciech Zaremba",
            "Bowen Baker",
            "Rohin Shah",
            "Vlad Mikulik"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "AI systems that \"think\" in human language offer a unique opportunity for AI safety: we can monitor their chains of thought (CoT) for the intent to misbehave. Like all other known AI oversight methods, CoT monitoring is imperfect and allows some misbehavior to go unnoticed. Nevertheless, it shows promise and we recommend further research into CoT monitorability and investment in CoT monitoring alongside existing safety methods. Because CoT monitorability may be fragile, we recommend that frontier model developers consider the impact of development decisions on CoT monitorability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 142,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11502",
        "abs_url": "https://arxiv.org/abs/2507.11502",
        "pdf_url": "https://arxiv.org/pdf/2507.11502",
        "title": "HKGAI-V1: Towards Regional Sovereign Large Language Model for Hong Kong",
        "authors": [
            "Sirui Han",
            "Junqi Zhu",
            "Ruiyuan Zhang",
            "Yike Guo"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)",
        "abstract": "This paper presents the development of HKGAI-V1, a foundational sovereign large language model (LLM), developed as part of an initiative to establish value-aligned AI infrastructure specifically tailored for Hong Kong. Addressing the region's unique multilingual environment (Cantonese, Mandarin, and English), its distinct socio-legal context under the \"one country, two systems\" framework, and specific local cultural and value considerations, the model is built upon the DeepSeek architecture and systematically aligned with regional norms through a multifaceted full parameter fine-tuning process. It is further integrated with a retrieval-augmented generation (RAG) system to ensure timely and factually grounded information access. The core contribution lies in the design and implementation of a comprehensive, region-specific AI alignment and safety framework, demonstrated through two key achievements: 1) The successful development of HKGAI-V1 itself - which outper-forms general-purpose models in handling Hong Kong-specific culturally sensitive queries, and embodies a \"governance-embedded\" approach to digital sovereignty - empowers Hong Kong to exercise control over AI applications in critical sectors including public services, legal systems, and edu-cation. 2) The development of the proprietary Adversarial HK Value Benchmark, a rigorous tool for evaluating model alignment with local ethical and legal stand-ards under challenging conditions. By documenting these achievements, the paper provides not only a technological artifact but also a replicable blueprint for developing advanced, regionally focused AI systems deeply rooted in their local identities.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 143,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11506",
        "abs_url": "https://arxiv.org/abs/2507.11506",
        "pdf_url": "https://arxiv.org/pdf/2507.11506",
        "title": "Elk: Exploring the Efficiency of Inter-core Connected AI Chips with Deep Learning Compiler Techniques",
        "authors": [
            "Yiqi Liu",
            "Yuqi Xue",
            "Noelle Crawford",
            "Jilong Xue",
            "Jian Huang"
        ],
        "comments": "This paper is accepted at the 58th IEEE/ACM International Symposium on Microarchitecture (MICRO'25)",
        "subjects": "Hardware Architecture (cs.AR); Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)",
        "abstract": "To meet the increasing demand of deep learning (DL) models, AI chips are employing both off-chip memory (e.g., HBM) and high-bandwidth low-latency interconnect for direct inter-core data exchange. However, it is not easy to explore the efficiency of these inter-core connected AI (ICCA) chips, due to a fundamental tussle among compute (per-core execution), communication (inter-core data exchange), and I/O (off-chip data access). In this paper, we develop Elk, a DL compiler framework to maximize the efficiency of ICCA chips by jointly trading off all the three performance factors discussed above. Elk structures these performance factors into configurable parameters and forms a global trade-off space in the DL compiler. To systematically explore this space and maximize overall efficiency, Elk employs a new inductive operator scheduling policy and a cost-aware on-chip memory allocation algorithm. It generates globally optimized execution plans that best overlap off-chip data loading and on-chip execution. To examine the efficiency of Elk, we build a full-fledged emulator based on a real ICCA chip IPU-POD4, and an ICCA chip simulator for sensitivity analysis with different interconnect network topologies. Elk achieves 94% of the ideal roofline performance of ICCA chips on average, showing the benefits of supporting large DL models on ICCA chips. We also show Elk's capability of enabling architecture design space exploration for new ICCA chip development.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 144,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11522",
        "abs_url": "https://arxiv.org/abs/2507.11522",
        "pdf_url": "https://arxiv.org/pdf/2507.11522",
        "title": "CATVis: Context-Aware Thought Visualization",
        "authors": [
            "Tariq Mehmood",
            "Hamza Ahmad",
            "Muhammad Haroon Shakeel",
            "Murtaza Taj"
        ],
        "comments": "Accepted at MICCAI 2025. This is the submitted version prior to peer review. The final Version of Record will appear in the MICCAI 2025 proceedings (Springer LNCS)",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "EEG-based brain-computer interfaces (BCIs) have shown promise in various applications, such as motor imagery and cognitive state monitoring. However, decoding visual representations from EEG signals remains a significant challenge due to their complex and noisy nature. We thus propose a novel 5-stage framework for decoding visual representations from EEG signals: (1) an EEG encoder for concept classification, (2) cross-modal alignment of EEG and text embeddings in CLIP feature space, (3) caption refinement via re-ranking, (4) weighted interpolation of concept and caption embeddings for richer semantics, and (5) image generation using a pre-trained Stable Diffusion model. We enable context-aware EEG-to-image generation through cross-modal alignment and re-ranking. Experimental results demonstrate that our method generates high-quality images aligned with visual stimuli, outperforming SOTA approaches by 13.43% in Classification Accuracy, 15.21% in Generation Accuracy and reducing Fr√©chet Inception Distance by 36.61%, indicating superior semantic alignment and image quality.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 145,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11535",
        "abs_url": "https://arxiv.org/abs/2507.11535",
        "pdf_url": "https://arxiv.org/pdf/2507.11535",
        "title": "Canonical Bayesian Linear System Identification",
        "authors": [
            "Andrey Bryutkin",
            "Matthew E. Levine",
            "I√±igo Urteaga",
            "Youssef Marzouk"
        ],
        "comments": "46 pages, 9 figures",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Systems and Control (eess.SY); Computation (stat.CO)",
        "abstract": "Standard Bayesian approaches for linear time-invariant (LTI) system identification are hindered by parameter non-identifiability; the resulting complex, multi-modal posteriors make inference inefficient and impractical. We solve this problem by embedding canonical forms of LTI systems within the Bayesian framework. We rigorously establish that inference in these minimal parameterizations fully captures all invariant system dynamics (e.g., transfer functions, eigenvalues, predictive distributions of system outputs) while resolving identifiability. This approach unlocks the use of meaningful, structure-aware priors (e.g., enforcing stability via eigenvalues) and ensures conditions for a Bernstein--von Mises theorem -- a link between Bayesian and frequentist large-sample asymptotics that is broken in standard forms. Extensive simulations with modern MCMC methods highlight advantages over standard parameterizations: canonical forms achieve higher computational efficiency, generate interpretable and well-behaved posteriors, and provide robust uncertainty estimates, particularly from limited data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 146,
        "date": "2025-07-16",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-16?abs=True",
        "arxiv_id": "2507.11539",
        "abs_url": "https://arxiv.org/abs/2507.11539",
        "pdf_url": "https://arxiv.org/pdf/2507.11539",
        "title": "Streaming 4D Visual Geometry Transformer",
        "authors": [
            "Dong Zhuo",
            "Wenzhao Zheng",
            "Jiahe Guo",
            "Yuqi Wu",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "comments": "Code is available at: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a fundamental yet challenging computer vision task. To facilitate interactive and real-time applications, we propose a streaming 4D visual geometry transformer that shares a similar philosophy with autoregressive large language models. We explore a simple and efficient design and employ a causal transformer architecture to process the input sequence in an online manner. We use temporal causal attention and cache the historical keys and values as implicit memory to enable efficient streaming long-term 4D reconstruction. This design can handle real-time 4D reconstruction by incrementally integrating historical information while maintaining high-quality spatial consistency. For efficient training, we propose to distill knowledge from the dense bidirectional visual geometry grounded transformer (VGGT) to our causal model. For inference, our model supports the migration of optimized efficient attention operator (e.g., FlashAttention) from the field of large language models. Extensive experiments on various 4D geometry perception benchmarks demonstrate that our model increases the inference speed in online scenarios while maintaining competitive performance, paving the way for scalable and interactive 4D vision systems. Code is available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    }
]