[
    {
        "order": 1,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04718",
        "abs_url": "https://arxiv.org/abs/2511.04718",
        "pdf_url": "https://arxiv.org/pdf/2511.04718",
        "title": "Ada-FCN: Adaptive Frequency-Coupled Network for fMRI-Based Brain Disorder Classification",
        "authors": [
            "Yue Xun",
            "Jiaxing Xu",
            "Wenbo Gao",
            "Chen Yang",
            "Shujun Wang"
        ],
        "comments": "11 pages, 2 figures, conference",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Resting-state fMRI has become a valuable tool for classifying brain disorders and constructing brain functional connectivity networks by tracking BOLD signals across brain regions. However, existing mod els largely neglect the multi-frequency nature of neuronal oscillations, treating BOLD signals as monolithic time series. This overlooks the cru cial fact that neurological disorders often manifest as disruptions within specific frequency bands, limiting diagnostic sensitivity and specificity. While some methods have attempted to incorporate frequency informa tion, they often rely on predefined frequency bands, which may not be optimal for capturing individual variability or disease-specific alterations. To address this, we propose a novel framework featuring Adaptive Cas cade Decomposition to learn task-relevant frequency sub-bands for each brain region and Frequency-Coupled Connectivity Learning to capture both intra- and nuanced cross-band interactions in a unified functional network. This unified network informs a novel message-passing mecha nism within our Unified-GCN, generating refined node representations for diagnostic prediction. Experimental results on the ADNI and ABIDE datasets demonstrate superior performance over existing methods. The code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 2,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04722",
        "abs_url": "https://arxiv.org/abs/2511.04722",
        "pdf_url": "https://arxiv.org/pdf/2511.04722",
        "title": "AWEMixer: Adaptive Wavelet-Enhanced Mixer Network for Long-Term Time Series Forecasting",
        "authors": [
            "Qianyang Li",
            "Xingjun Zhang",
            "Peng Tao",
            "Shaoxun Wang",
            "Yancheng Pan",
            "Jia Wei"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Forecasting long-term time series in IoT environments remains a significant challenge due to the non-stationary and multi-scale characteristics of sensor signals. Furthermore, error accumulation causes a decrease in forecast quality when predicting further into the future. Traditional methods are restricted to operate in time-domain, while the global frequency information achieved by Fourier transform would be regarded as stationary signals leading to blur the temporal patterns of transient events. We propose AWEMixer, an Adaptive Wavelet-Enhanced Mixer Network including two innovative components: 1) a Frequency Router designs to utilize the global periodicity pattern achieved by Fast Fourier Transform to adaptively weight localized wavelet subband, and 2) a Coherent Gated Fusion Block to achieve selective integration of prominent frequency features with multi-scale temporal representation through cross-attention and gating mechanism, which realizes accurate time-frequency localization while remaining robust to noise. Seven public benchmarks validate that our model is more effective than recent state-of-the-art models. Specifically, our model consistently achieves performance improvement compared with transformer-based and MLP-based state-of-the-art models in long-sequence time series forecasting. Code is available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 3,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04751",
        "abs_url": "https://arxiv.org/abs/2511.04751",
        "pdf_url": "https://arxiv.org/pdf/2511.04751",
        "title": "Regularized GLISp for sensor-guided human-in-the-loop optimization",
        "authors": [
            "Matteo Cercola",
            "Michele Lomuscio",
            "Dario Piga",
            "Simone Formentin"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Human-in-the-loop calibration is often addressed via preference-based optimization, where algorithms learn from pairwise comparisons rather than explicit cost evaluations. While effective, methods such as Preferential Bayesian Optimization or Global optimization based on active preference learning with radial basis functions (GLISp) treat the system as a black box and ignore informative sensor measurements. In this work, we introduce a sensor-guided regularized extension of GLISp that integrates measurable descriptors into the preference-learning loop through a physics-informed hypothesis function and a least-squares regularization term. This injects grey-box structure, combining subjective feedback with quantitative sensor information while preserving the flexibility of preference-based search. Numerical evaluations on an analytical benchmark and on a human-in-the-loop vehicle suspension tuning task show faster convergence and superior final solutions compared to baseline GLISp.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 4,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04760",
        "abs_url": "https://arxiv.org/abs/2511.04760",
        "pdf_url": "https://arxiv.org/pdf/2511.04760",
        "title": "When Data Falls Short: Grokking Below the Critical Threshold",
        "authors": [
            "Vaibhav Singh",
            "Eugene Belilovsky",
            "Rahaf Aljundi"
        ],
        "comments": "6 pages",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In this paper, we investigate the phenomenon of grokking, where models exhibit delayed generalization following overfitting on training data. We focus on data-scarce regimes where the number of training samples falls below the critical threshold, making grokking unobservable, and on practical scenarios involving distribution shift. We first show that Knowledge Distillation (KD) from a model that has already grokked on a distribution (p1) can induce and accelerate grokking on a different distribution (p2), even when the available data lies below the critical threshold. This highlights the value of KD for deployed models that must adapt to new distributions under limited data. We then study training on the joint distribution (p1, p2) and demonstrate that while standard supervised training fails when either distribution has insufficient data, distilling from models grokked on the individual distributions enables generalization. Finally, we examine a continual pretraining setup, where a grokked model transitions from p1 to p2, and find that KD both accelerates generalization and mitigates catastrophic forgetting, achieving strong performance even with only 10% of the data. Together, our results provide new insights into the mechanics of grokking under knowledge transfer and underscore the central role of KD in enabling generalization in low-data and evolving distribution settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 5,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04768",
        "abs_url": "https://arxiv.org/abs/2511.04768",
        "pdf_url": "https://arxiv.org/pdf/2511.04768",
        "title": "FuseFlow: A Fusion-Centric Compilation Framework for Sparse Deep Learning on Streaming Dataflow",
        "authors": [
            "Rubens Lacouture",
            "Nathan Zhang",
            "Ritvik Sharma",
            "Marco Siracusa",
            "Fredrik Kjolstad",
            "Kunle Olukotun",
            "Olivia Hsu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Hardware Architecture (cs.AR); Programming Languages (cs.PL)",
        "abstract": "As deep learning models scale, sparse computation and specialized dataflow hardware have emerged as powerful solutions to address efficiency. We propose FuseFlow, a compiler that converts sparse machine learning models written in PyTorch to fused sparse dataflow graphs for reconfigurable dataflow architectures (RDAs). FuseFlow is the first compiler to support general cross-expression fusion of sparse operations. In addition to fusion across kernels (expressions), FuseFlow also supports optimizations like parallelization, dataflow ordering, and sparsity blocking. It targets a cycle-accurate dataflow simulator for microarchitectural analysis of fusion strategies. We use FuseFlow for design-space exploration across four real-world machine learning applications with sparsity, showing that full fusion (entire cross-expression fusion across all computation in an end-to-end model) is not always optimal for sparse models-fusion granularity depends on the model itself. FuseFlow also provides a heuristic to identify and prune suboptimal configurations. Using Fuseflow, we achieve performance improvements, including a ~2.7x speedup over an unfused baseline for GPT-3 with BigBird block-sparse attention.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 6,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04774",
        "abs_url": "https://arxiv.org/abs/2511.04774",
        "pdf_url": "https://arxiv.org/pdf/2511.04774",
        "title": "SLOFetch: Compressed-Hierarchical Instruction Prefetching for Cloud Microservices",
        "authors": [
            "Liu Jiang",
            "Zerui Bao",
            "Shiqi Sheng",
            "Di Zhu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Hardware Architecture (cs.AR)",
        "abstract": "Large-scale networked services rely on deep soft-ware stacks and microservice orchestration, which increase instruction footprints and create frontend stalls that inflate tail latency and energy. We revisit instruction prefetching for these cloud workloads and present a design that aligns with SLO driven and self optimizing systems. Building on the Entangling Instruction Prefetcher (EIP), we introduce a Compressed Entry that captures up to eight destinations around a base using 36 bits by exploiting spatial clustering, and a Hierarchical Metadata Storage scheme that keeps only L1 resident and frequently queried entries on chip while virtualizing bulk metadata into lower levels. We further add a lightweight Online ML Controller that scores prefetch profitability using context features and a bandit adjusted threshold. On data center applications, our approach preserves EIP like speedups with smaller on chip state and improves efficiency for networked services in the ML era.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 7,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04789",
        "abs_url": "https://arxiv.org/abs/2511.04789",
        "pdf_url": "https://arxiv.org/pdf/2511.04789",
        "title": "Conditional Neural ODE for Longitudinal Parkinson's Disease Progression Forecasting",
        "authors": [
            "Xiaoda Wang",
            "Yuji Zhao",
            "Kaiqiao Han",
            "Xiao Luo",
            "Sanne van Rooij",
            "Jennifer Stevens",
            "Lifang He",
            "Liang Zhan",
            "Yizhou Sun",
            "Wei Wang",
            "Carl Yang"
        ],
        "comments": "Accepted to IEEE International Conference on Bioinformatics and Biomedicine (BIBM) 2025",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Parkinson's disease (PD) shows heterogeneous, evolving brain-morphometry patterns. Modeling these longitudinal trajectories enables mechanistic insight, treatment development, and individualized 'digital-twin' forecasting. However, existing methods usually adopt recurrent neural networks and transformer architectures, which rely on discrete, regularly sampled data while struggling to handle irregular and sparse magnetic resonance imaging (MRI) in PD cohorts. Moreover, these methods have difficulty capturing individual heterogeneity including variations in disease onset, progression rate, and symptom severity, which is a hallmark of PD. To address these challenges, we propose CNODE (Conditional Neural ODE), a novel framework for continuous, individualized PD progression forecasting. The core of CNODE is to model morphological brain changes as continuous temporal processes using a neural ODE model. In addition, we jointly learn patient-specific initial time and progress speed to align individual trajectories into a shared progression trajectory. We validate CNODE on the Parkinson's Progression Markers Initiative (PPMI) dataset. Experimental results show that our method outperforms state-of-the-art baselines in forecasting longitudinal PD progression.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 8,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04791",
        "abs_url": "https://arxiv.org/abs/2511.04791",
        "pdf_url": "https://arxiv.org/pdf/2511.04791",
        "title": "DuetServe: Harmonizing Prefill and Decode for LLM Serving via Adaptive GPU Multiplexing",
        "authors": [
            "Lei Gao",
            "Chaoyi Jiang",
            "Hossein Entezari Zarch",
            "Daniel Wong",
            "Murali Annavaram"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Modern LLM serving systems must sustain high throughput while meeting strict latency SLOs across two distinct inference phases: compute-intensive prefill and memory-bound decode phases. Existing approaches either (1) aggregate both phases on shared GPUs, leading to interference between prefill and decode phases, which degrades time-between-tokens (TBT); or (2) disaggregate the two phases across GPUs, improving latency but wasting resources through duplicated models and KV cache transfers. We present DuetServe, a unified LLM serving framework that achieves disaggregation-level isolation within a single GPU. DuetServe operates in aggregated mode by default and dynamically activates SM-level GPU spatial multiplexing when TBT degradation is predicted. Its key idea is to decouple prefill and decode execution only when needed through fine-grained, adaptive SM partitioning that provides phase isolation only when contention threatens latency service level objectives (SLOs). DuetServe integrates (1) an attention-aware roofline model to forecast iteration latency, (2) a partitioning optimizer that selects the optimal SM split to maximize throughput under TBT constraints, and (3) an interruption-free execution engine that eliminates CPU-GPU synchronization overhead. Evaluations show that DuetServe improves total throughput by up to 1.3x while maintaining low generation latency compared to state-of-the-art frameworks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 9,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04804",
        "abs_url": "https://arxiv.org/abs/2511.04804",
        "pdf_url": "https://arxiv.org/pdf/2511.04804",
        "title": "Simplex-FEM Networks (SiFEN): Learning A Triangulated Function Approximator",
        "authors": [
            "Chaymae Yahyati",
            "Ismail Lamaakal",
            "Khalid El Makkaoui",
            "Ibrahim Ouahbi",
            "Yassine Maleh"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We introduce Simplex-FEM Networks (SiFEN), a learned piecewise-polynomial predictor that represents f: R^d -> R^k as a globally C^r finite-element field on a learned simplicial mesh in an optionally warped input space. Each query activates exactly one simplex and at most d+1 basis functions via barycentric coordinates, yielding explicit locality, controllable smoothness, and cache-friendly sparsity. SiFEN pairs degree-m Bernstein-Bezier polynomials with a light invertible warp and trains end-to-end with shape regularization, semi-discrete OT coverage, and differentiable edge flips. Under standard shape-regularity and bi-Lipschitz warp assumptions, SiFEN achieves the classic FEM approximation rate M^(-m/d) with M mesh vertices. Empirically, on synthetic approximation tasks, tabular regression/classification, and as a drop-in head on compact CNNs, SiFEN matches or surpasses MLPs and KANs at matched parameter budgets, improves calibration (lower ECE/Brier), and reduces inference latency due to geometric locality. These properties make SiFEN a compact, interpretable, and theoretically grounded alternative to dense MLPs and edge-spline networks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 10,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04805",
        "abs_url": "https://arxiv.org/abs/2511.04805",
        "pdf_url": "https://arxiv.org/pdf/2511.04805",
        "title": "PuzzleMoE: Efficient Compression of Large Mixture-of-Experts Models via Sparse Expert Merging and Bit-packed inference",
        "authors": [
            "Yushu Zhao",
            "Zheng Wang",
            "Minjia Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Mixture-of-Experts (MoE) models have shown strong potential in scaling language models efficiently by activating only a small subset of experts per input. However, their widespread deployment remains limited due to the high memory overhead associated with storing all expert parameters, particularly as the number of experts increases. To address this challenge, prior works have explored expert dropping and merging strategies, yet they often suffer from performance drop at high compression ratios. In this paper, we introduce PuzzleMoE, a training-free MoE compression method that achieves both high accuracy and efficient inference through two key innovations: First, PuzzleMoE performs sparse expert merging by identifying element-wise weight redundancy and specialization. It uses a dual-mask to capture both shared and expert-specific parameters. Second, to avoid the overhead of storing binary masks and signs, PuzzleMoE introduces a bit-packed encoding scheme that reuses underutilized exponent bits, enabling efficient MoE inference on GPUs. Extensive experiments demonstrate that PuzzleMoE can compress MoE models by up to 50% while maintaining accuracy across various tasks. Specifically, it outperforms prior MoE compression methods by up to 16.7% on MMLU at 50% compression ratio, and achieves up to 1.28\\times inference speedup.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 11,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04807",
        "abs_url": "https://arxiv.org/abs/2511.04807",
        "pdf_url": "https://arxiv.org/pdf/2511.04807",
        "title": "Autoencoding Dynamics: Topological Limitations and Capabilities",
        "authors": [
            "Matthew D. Kvalheim",
            "Eduardo D. Sontag"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Dynamical Systems (math.DS)",
        "abstract": "Given a \"data manifold\" $M\\subset \\mathbb{R}^n$ and \"latent space\" $\\mathbb{R}^\\ell$, an autoencoder is a pair of continuous maps consisting of an \"encoder\" $E\\colon \\mathbb{R}^n\\to \\mathbb{R}^\\ell$ and \"decoder\" $D\\colon \\mathbb{R}^\\ell\\to \\mathbb{R}^n$ such that the \"round trip\" map $D\\circ E$ is as close as possible to the identity map $\\mbox{id}_M$ on $M$. We present various topological limitations and capabilites inherent to the search for an autoencoder, and describe capabilities for autoencoding dynamical systems having $M$ as an invariant manifold.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 12,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04808",
        "abs_url": "https://arxiv.org/abs/2511.04808",
        "pdf_url": "https://arxiv.org/pdf/2511.04808",
        "title": "Sharp Minima Can Generalize: A Loss Landscape Perspective On Data",
        "authors": [
            "Raymond Fan",
            "Bryce Sandlund",
            "Lin Myat Ko"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The volume hypothesis suggests deep learning is effective because it is likely to find flat minima due to their large volumes, and flat minima generalize well. This picture does not explain the role of large datasets in generalization. Measuring minima volumes under varying amounts of training data reveals sharp minima which generalize well exist, but are unlikely to be found due to their small volumes. Increasing data changes the loss landscape, such that previously small generalizing minima become (relatively) large.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 13,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04825",
        "abs_url": "https://arxiv.org/abs/2511.04825",
        "pdf_url": "https://arxiv.org/pdf/2511.04825",
        "title": "Persistent reachability homology in machine learning applications",
        "authors": [
            "Luigi Caputi",
            "Nicholas Meadows",
            "Henri Riihim√§ki"
        ],
        "comments": "19 pages; any comments welcome",
        "subjects": "Machine Learning (cs.LG); Algebraic Topology (math.AT); Quantitative Methods (q-bio.QM)",
        "abstract": "We explore the recently introduced persistent reachability homology (PRH) of digraph data, i.e. data in the form of directed graphs. In particular, we study the effectiveness of PRH in network classification task in a key neuroscience problem: epilepsy detection. PRH is a variation of the persistent homology of digraphs, more traditionally based on the directed flag complex (DPH). A main advantage of PRH is that it considers the condensations of the digraphs appearing in the persistent filtration and thus is computed from smaller digraphs. We compare the effectiveness of PRH to that of DPH and we show that PRH outperforms DPH in the classification task. We use the Betti curves and their integrals as topological features and implement our pipeline on support vector machine.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 14,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04838",
        "abs_url": "https://arxiv.org/abs/2511.04838",
        "pdf_url": "https://arxiv.org/pdf/2511.04838",
        "title": "SPECTRA: Spectral Target-Aware Graph Augmentation for Imbalanced Molecular Property Regression",
        "authors": [
            "Brenda Nogueira",
            "Meng Jiang",
            "Nitesh V. Chawla",
            "Nuno Moniz"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Spectral Theory (math.SP); Molecular Networks (q-bio.MN)",
        "abstract": "In molecular property prediction, the most valuable compounds (e.g., high potency) often occupy sparse regions of the target space. Standard Graph Neural Networks (GNNs) commonly optimize for the average error, underperforming on these uncommon but critical cases, with existing oversampling methods often distorting molecular topology. In this paper, we introduce SPECTRA, a Spectral Target-Aware graph augmentation framework that generates realistic molecular graphs in the spectral domain. SPECTRA (i) reconstructs multi-attribute molecular graphs from SMILES; (ii) aligns molecule pairs via (Fused) Gromov-Wasserstein couplings to obtain node correspondences; (iii) interpolates Laplacian eigenvalues, eigenvectors and node features in a stable share-basis; and (iv) reconstructs edges to synthesize physically plausible intermediates with interpolated targets. A rarity-aware budgeting scheme, derived from a kernel density estimation of labels, concentrates augmentation where data are scarce. Coupled with a spectral GNN using edge-aware Chebyshev convolutions, SPECTRA densifies underrepresented regions without degrading global accuracy. On benchmarks, SPECTRA consistently improves error in relevant target ranges while maintaining competitive overall MAE, and yields interpretable synthetic molecules whose structure reflects the underlying spectral geometry. Our results demonstrate that spectral, geometry-aware augmentation is an effective and efficient strategy for imbalanced molecular property regression.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 15,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04844",
        "abs_url": "https://arxiv.org/abs/2511.04844",
        "pdf_url": "https://arxiv.org/pdf/2511.04844",
        "title": "Sublinear iterations can suffice even for DDPMs",
        "authors": [
            "Matthew S. Zhang",
            "Stephen Huan",
            "Jerry Huang",
            "Nicholas M. Boffi",
            "Sitan Chen",
            "Sinho Chewi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Statistics Theory (math.ST)",
        "abstract": "SDE-based methods such as denoising diffusion probabilistic models (DDPMs) have shown remarkable success in real-world sample generation tasks. Prior analyses of DDPMs have been focused on the exponential Euler discretization, showing guarantees that generally depend at least linearly on the dimension or initial Fisher information. Inspired by works in log-concave sampling (Shen and Lee, 2019), we analyze an integrator -- the denoising diffusion randomized midpoint method (DDRaM) -- that leverages an additional randomized midpoint to better approximate the SDE. Using a recently-developed analytic framework called the \"shifted composition rule\", we show that this algorithm enjoys favorable discretization properties under appropriate smoothness assumptions, with sublinear $\\widetilde{O}(\\sqrt{d})$ score evaluations needed to ensure convergence. This is the first sublinear complexity bound for pure DDPM sampling -- prior works which obtained such bounds worked instead with ODE-based sampling and had to make modifications to the sampler which deviate from how they are used in practice. We also provide experimental validation of the advantages of our method, showing that it performs well in practice with pre-trained image synthesis models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 16,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04845",
        "abs_url": "https://arxiv.org/abs/2511.04845",
        "pdf_url": "https://arxiv.org/pdf/2511.04845",
        "title": "Investigating U.S. Consumer Demand for Food Products with Innovative Transportation Certificates Based on Stated Preferences and Machine Learning Approaches",
        "authors": [
            "Jingchen Bi",
            "Rodrigo Mesa-Arango"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "This paper utilizes a machine learning model to estimate the consumer's behavior for food products with innovative transportation certificates in the U.S. Building on previous research that examined demand for food products with supply chain traceability using stated preference analysis, transportation factors were identified as significant in consumer food purchasing choices. Consequently, a second experiment was conducted to pinpoint the specific transportation attributes valued by consumers. A machine learning model was applied, and five innovative certificates related to transportation were proposed: Transportation Mode, Internet of Things (IoT), Safety measures, Energy Source, and Must Arrive By Dates (MABDs). The preference experiment also incorporated product-specific and decision-maker factors for control purposes. The findings reveal a notable inclination toward safety and energy certificates within the transportation domain of the U.S. food supply chain. Additionally, the study examined the influence of price, product type, certificates, and decision-maker factors on purchasing choices. Ultimately, the study offers data-driven recommendations for improving food supply chain systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 17,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04847",
        "abs_url": "https://arxiv.org/abs/2511.04847",
        "pdf_url": "https://arxiv.org/pdf/2511.04847",
        "title": "Grounded Test-Time Adaptation for LLM Agents",
        "authors": [
            "Arthur Chen",
            "Zuxin Liu",
            "Jianguo Zhang",
            "Akshara Prabhakar",
            "Zhiwei Liu",
            "Shelby Heinecke",
            "Silvio Savarese",
            "Victor Zhong",
            "Caiming Xiong"
        ],
        "comments": "Preprint. Under review",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Large language model (LLM)-based agents struggle to generalize to novel and complex environments, such as unseen websites or new sets of functions, due to a fundamental mismatch between their pre-training and test-time conditions. This challenge stems from two distinct failure modes: a syntactic misunderstanding of environment-specific components like observation formats, and a semantic misunderstanding of state-transition dynamics, which are only revealed at test time. To address these issues, we propose two distinct and complementary strategies for adapting LLM agents by leveraging environment-specific information available during deployment. First, an online distributional adaptation method parameterizes environmental nuances by learning a lightweight adaptation vector that biases the model's output distribution, enabling rapid alignment with an environment response format. Second, a deployment-time dynamics grounding method employs a persona-driven exploration phase to systematically probe and learn the environment's causal dynamics before task execution, equipping the agent with a nonparametric world model. We evaluate these strategies across diverse agentic benchmarks, including function calling and web navigation. Our empirical results show the effectiveness of both strategies across all benchmarks with minimal computational cost. We find that dynamics grounding is particularly effective in complex environments where unpredictable dynamics pose a major obstacle, demonstrating a robust path toward more generalizable and capable LLM-based agents. For example, on the WebArena multi-site split, this method increases the agent's success rate from 2% to 23%.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 18,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04854",
        "abs_url": "https://arxiv.org/abs/2511.04854",
        "pdf_url": "https://arxiv.org/pdf/2511.04854",
        "title": "SigmaDock: Untwisting Molecular Docking With Fragment-Based SE(3) Diffusion",
        "authors": [
            "Alvaro Prat",
            "Leo Zhang",
            "Charlotte M. Deane",
            "Yee Whye Teh",
            "Garrett M. Morris"
        ],
        "comments": "Preprint",
        "subjects": "Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)",
        "abstract": "Determining the binding pose of a ligand to a protein, known as molecular docking, is a fundamental task in drug discovery. Generative approaches promise faster, improved, and more diverse pose sampling than physics-based methods, but are often hindered by chemically implausible outputs, poor generalisability, and high computational cost. To address these challenges, we introduce a novel fragmentation scheme, leveraging inductive biases from structural chemistry, to decompose ligands into rigid-body fragments. Building on this decomposition, we present SigmaDock, an SE(3) Riemannian diffusion model that generates poses by learning to reassemble these rigid bodies within the binding pocket. By operating at the level of fragments in SE(3), SigmaDock exploits well-established geometric priors while avoiding overly complex diffusion processes and unstable training dynamics. Experimentally, we show SigmaDock achieves state-of-the-art performance, reaching Top-1 success rates (RMSD<2 & PB-valid) above 79.9% on the PoseBusters set, compared to 12.7-30.8% reported by recent deep learning approaches, whilst demonstrating consistent generalisation to unseen proteins. SigmaDock is the first deep learning approach to surpass classical physics-based docking under the PB train-test split, marking a significant leap forward in the reliability and feasibility of deep learning for molecular modelling.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 19,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04856",
        "abs_url": "https://arxiv.org/abs/2511.04856",
        "pdf_url": "https://arxiv.org/pdf/2511.04856",
        "title": "Quantum Boltzmann Machines for Sample-Efficient Reinforcement Learning",
        "authors": [
            "Thore Gerlach",
            "Michael Schenk",
            "Verena Kain"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Quantum Physics (quant-ph)",
        "abstract": "We introduce theoretically grounded Continuous Semi-Quantum Boltzmann Machines (CSQBMs) that supports continuous-action reinforcement learning. By combining exponential-family priors over visible units with quantum Boltzmann distributions over hidden units, CSQBMs yield a hybrid quantum-classical model that reduces qubit requirements while retaining strong expressiveness. Crucially, gradients with respect to continuous variables can be computed analytically, enabling direct integration into Actor-Critic algorithms. Building on this, we propose a continuous Q-learning framework that replaces global maximization by efficient sampling from the CSQBM distribution, thereby overcoming instability issues in continuous control.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 20,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04865",
        "abs_url": "https://arxiv.org/abs/2511.04865",
        "pdf_url": "https://arxiv.org/pdf/2511.04865",
        "title": "FoodRL: A Reinforcement Learning Ensembling Framework For In-Kind Food Donation Forecasting",
        "authors": [
            "Esha Sharma",
            "Lauren Davis",
            "Julie Ivy",
            "Min Chi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Food banks are crucial for alleviating food insecurity, but their effectiveness hinges on accurately forecasting highly volatile in-kind donations to ensure equitable and efficient resource distribution. Traditional forecasting models often fail to maintain consistent accuracy due to unpredictable fluctuations and concept drift driven by seasonal variations and natural disasters such as hurricanes in the Southeastern U.S. and wildfires in the West Coast. To address these challenges, we propose FoodRL, a novel reinforcement learning (RL) based metalearning framework that clusters and dynamically weights diverse forecasting models based on recent performance and contextual information. Evaluated on multi-year data from two structurally distinct U.S. food banks-one large regional West Coast food bank affected by wildfires and another state-level East Coast food bank consistently impacted by hurricanes, FoodRL consistently outperforms baseline methods, particularly during periods of disruption or decline. By delivering more reliable and adaptive forecasts, FoodRL can facilitate the redistribution of food equivalent to 1.7 million additional meals annually, demonstrating its significant potential for social impact as well as adaptive ensemble learning for humanitarian supply chains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 21,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04883",
        "abs_url": "https://arxiv.org/abs/2511.04883",
        "pdf_url": "https://arxiv.org/pdf/2511.04883",
        "title": "Self-Interest and Systemic Benefits: Emergence of Collective Rationality in Mixed Autonomy Traffic Through Deep Reinforcement Learning",
        "authors": [
            "Di Chen",
            "Jia Li",
            "Michael Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Autonomous vehicles (AVs) are expected to be commercially available in the near future, leading to mixed autonomy traffic consisting of both AVs and human-driven vehicles (HVs). Although numerous studies have shown that AVs can be deployed to benefit the overall traffic system performance by incorporating system-level goals into their decision making, it is not clear whether the benefits still exist when agents act out of self-interest -- a trait common to all driving agents, both human and autonomous. This study aims to understand whether self-interested AVs can bring benefits to all driving agents in mixed autonomy traffic systems. The research is centered on the concept of collective rationality (CR). This concept, originating from game theory and behavioral economics, means that driving agents may cooperate collectively even when pursuing individual interests. Our recent research has proven the existence of CR in an analytical game-theoretical model and empirically in mixed human-driven traffic. In this paper, we demonstrate that CR can be attained among driving agents trained using deep reinforcement learning (DRL) with a simple reward design. We examine the extent to which self-interested traffic agents can achieve CR without directly incorporating system-level objectives. Results show that CR consistently emerges in various scenarios, which indicates the robustness of this property. We also postulate a mechanism to explain the emergence of CR in the microscopic and dynamic environment and verify it based on simulation evidence. This research suggests the possibility of leveraging advanced learning methods (such as federated learning) to achieve collective cooperation among self-interested driving agents in mixed-autonomy systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 22,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04902",
        "abs_url": "https://arxiv.org/abs/2511.04902",
        "pdf_url": "https://arxiv.org/pdf/2511.04902",
        "title": "You Need Reasoning to Learn Reasoning: The Limitations of Label-Free RL in Weak Base Models",
        "authors": [
            "Shuvendu Roy",
            "Hossein Hajimirsadeghi",
            "Mengyao Zhai",
            "Golnoosh Samei"
        ],
        "comments": "39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: MATH-AI",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Recent advances in large language models have demonstrated the promise of unsupervised reinforcement learning (RL) methods for enhancing reasoning capabilities without external supervision. However, the generalizability of these label-free RL approaches to smaller base models with limited reasoning capabilities remains unexplored. In this work, we systematically investigate the performance of label-free RL methods across different model sizes and reasoning strengths, from 0.5B to 7B parameters. Our empirical analysis reveals critical limitations: label-free RL is highly dependent on the base model's pre-existing reasoning capability, with performance often degrading below baseline levels for weaker models. We find that smaller models fail to generate sufficiently long or diverse chain-of-thought reasoning to enable effective self-reflection, and that training data difficulty plays a crucial role in determining success. To address these challenges, we propose a simple yet effective method for label-free RL that utilizes curriculum learning to progressively introduce harder problems during training and mask no-majority rollouts during training. Additionally, we introduce a data curation pipeline to generate samples with predefined difficulty. Our approach demonstrates consistent improvements across all model sizes and reasoning capabilities, providing a path toward more robust unsupervised RL that can bootstrap reasoning abilities in resource-constrained models. We make our code available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 23,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04904",
        "abs_url": "https://arxiv.org/abs/2511.04904",
        "pdf_url": "https://arxiv.org/pdf/2511.04904",
        "title": "Multi-Agent Craftax: Benchmarking Open-Ended Multi-Agent Reinforcement Learning at the Hyperscale",
        "authors": [
            "Bassel Al Omari",
            "Michael Matthews",
            "Alexander Rutherford",
            "Jakob Nicolaus Foerster"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Multiagent Systems (cs.MA)",
        "abstract": "Progress in multi-agent reinforcement learning (MARL) requires challenging benchmarks that assess the limits of current methods. However, existing benchmarks often target narrow short-horizon challenges that do not adequately stress the long-term dependencies and generalization capabilities inherent in many multi-agent systems. To address this, we first present \\textit{Craftax-MA}: an extension of the popular open-ended RL environment, Craftax, that supports multiple agents and evaluates a wide range of general abilities within a single environment. Written in JAX, \\textit{Craftax-MA} is exceptionally fast with a training run using 250 million environment interactions completing in under an hour. To provide a more compelling challenge for MARL, we also present \\textit{Craftax-Coop}, an extension introducing heterogeneous agents, trading and more mechanics that require complex cooperation among agents for success. We provide analysis demonstrating that existing algorithms struggle with key challenges in this benchmark, including long-horizon credit assignment, exploration and cooperation, and argue for its potential to drive long-term research in MARL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 24,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04907",
        "abs_url": "https://arxiv.org/abs/2511.04907",
        "pdf_url": "https://arxiv.org/pdf/2511.04907",
        "title": "Efficient Swap Multicalibration of Elicitable Properties",
        "authors": [
            "Lunjia Hu",
            "Haipeng Luo",
            "Spandan Senapati",
            "Vatsal Sharan"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Multicalibration [HJKRR18] is an algorithmic fairness perspective that demands that the predictions of a predictor are correct conditional on themselves and membership in a collection of potentially overlapping subgroups of a population. The work of [NR23] established a surprising connection between multicalibration for an arbitrary property $\\Gamma$ (e.g., mean or median) and property elicitation: a property $\\Gamma$ can be multicalibrated if and only if it is elicitable, where elicitability is the notion that the true property value of a distribution can be obtained by solving a regression problem over the distribution. In the online setting, [NR23] proposed an inefficient algorithm that achieves $\\sqrt T$ $\\ell_2$-multicalibration error for a hypothesis class of group membership functions and an elicitable property $\\Gamma$, after $T$ rounds of interaction between a forecaster and adversary. In this paper, we generalize multicalibration for an elicitable property $\\Gamma$ from group membership functions to arbitrary bounded hypothesis classes and introduce a stronger notion -- swap multicalibration, following [GKR23]. Subsequently, we propose an oracle-efficient algorithm which, when given access to an online agnostic learner, achieves $T^{1/(r+1)}$ $\\ell_r$-swap multicalibration error with high probability (for $r\\ge2$) for a hypothesis class with bounded sequential Rademacher complexity and an elicitable property $\\Gamma$. For the special case of $r=2$, this implies an oracle-efficient algorithm that achieves $T^{1/3}$ $\\ell_2$-swap multicalibration error, which significantly improves on the previously established bounds for the problem [NR23, GMS25, LSS25a], and completely resolves an open question raised in [GJRR24] on the possibility of an oracle-efficient algorithm that achieves $\\sqrt{T}$ $\\ell_2$-mean multicalibration error by answering it in a strongly affirmative sense.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 25,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04918",
        "abs_url": "https://arxiv.org/abs/2511.04918",
        "pdf_url": "https://arxiv.org/pdf/2511.04918",
        "title": "Machine Learning Algorithms in Statistical Modelling Bridging Theory and Application",
        "authors": [
            "A. Ganapathi Rao",
            "Sathish Krishna Anumula",
            "Aditya Kumar Singh",
            "Renukhadevi M",
            "Y. Jeevan Nagendra Kumar",
            "Tammineni Rama Tulasi"
        ],
        "comments": "9 Pages, 4 Figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "It involves the completely novel ways of integrating ML algorithms with traditional statistical modelling that has changed the way we analyze data, do predictive analytics or make decisions in the fields of the data. In this paper, we study some ML and statistical model connections to understand ways in which some modern ML algorithms help 'enrich' conventional models; we demonstrate how new algorithms improve performance, scale, flexibility and robustness of the traditional models. It shows that the hybrid models are of great improvement in predictive accuracy, robustness, and interpretability",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 26,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04934",
        "abs_url": "https://arxiv.org/abs/2511.04934",
        "pdf_url": "https://arxiv.org/pdf/2511.04934",
        "title": "Leak@$k$: Unlearning Does Not Make LLMs Forget Under Probabilistic Decoding",
        "authors": [
            "Hadi Reisizadeh",
            "Jiajun Ruan",
            "Yiwei Chen",
            "Soumyadeep Pal",
            "Sijia Liu",
            "Mingyi Hong"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Unlearning in large language models (LLMs) is critical for regulatory compliance and for building ethical generative AI systems that avoid producing private, toxic, illegal, or copyrighted content. Despite rapid progress, in this work we show that \\textit{almost all} existing unlearning methods fail to achieve true forgetting in practice. Specifically, while evaluations of these `unlearned' models under deterministic (greedy) decoding often suggest successful knowledge removal using standard benchmarks (as has been done in the literature), we show that sensitive information reliably resurfaces when models are sampled with standard probabilistic decoding. To rigorously capture this vulnerability, we introduce \\texttt{leak@$k$}, a new meta-evaluation metric that quantifies the likelihood of forgotten knowledge reappearing when generating $k$ samples from the model under realistic decoding strategies. Using three widely adopted benchmarks, TOFU, MUSE, and WMDP, we conduct the first large-scale, systematic study of unlearning reliability using our newly defined \\texttt{leak@$k$} metric. Our findings demonstrate that knowledge leakage persists across methods and tasks, underscoring that current state-of-the-art unlearning techniques provide only limited forgetting and highlighting the urgent need for more robust approaches to LLM unlearning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 27,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04937",
        "abs_url": "https://arxiv.org/abs/2511.04937",
        "pdf_url": "https://arxiv.org/pdf/2511.04937",
        "title": "Structural Properties, Cycloid Trajectories and Non-Asymptotic Guarantees of EM Algorithm for Mixed Linear Regression",
        "authors": [
            "Zhankun Luo",
            "Abolfazl Hashemi"
        ],
        "comments": "Preprint of the paper submitted to IEEE Transactions on Information Theory",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "This work investigates the structural properties, cycloid trajectories, and non-asymptotic convergence guarantees of the Expectation-Maximization (EM) algorithm for two-component Mixed Linear Regression (2MLR) with unknown mixing weights and regression parameters. Recent studies have established global convergence for 2MLR with known balanced weights and super-linear convergence in noiseless and high signal-to-noise ratio (SNR) regimes. However, the theoretical behavior of EM in the fully unknown setting remains unclear, with its trajectory and convergence order not yet fully characterized. We derive explicit EM update expressions for 2MLR with unknown mixing weights and regression parameters across all SNR regimes and analyze their structural properties and cycloid trajectories. In the noiseless case, we prove that the trajectory of the regression parameters in EM iterations traces a cycloid by establishing a recurrence relation for the sub-optimality angle, while in high SNR regimes we quantify its discrepancy from the cycloid trajectory. The trajectory-based analysis reveals the order of convergence: linear when the EM estimate is nearly orthogonal to the ground truth, and quadratic when the angle between the estimate and ground truth is small at the population level. Our analysis establishes non-asymptotic guarantees by sharpening bounds on statistical errors between finite-sample and population EM updates, relating EM's statistical accuracy to the sub-optimality angle, and proving convergence with arbitrary initialization at the finite-sample level. This work provides a novel trajectory-based framework for analyzing EM in Mixed Linear Regression.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 28,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04971",
        "abs_url": "https://arxiv.org/abs/2511.04971",
        "pdf_url": "https://arxiv.org/pdf/2511.04971",
        "title": "Risk Prediction of Cardiovascular Disease for Diabetic Patients with Machine Learning and Deep Learning Techniques",
        "authors": [
            "Esha Chowdhury"
        ],
        "comments": "24 pages with 6 table and 8 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Accurate prediction of cardiovascular disease (CVD) risk is crucial for healthcare institutions. This study addresses the growing prevalence of diabetes and its strong link to heart disease by proposing an efficient CVD risk prediction model for diabetic patients using machine learning (ML) and hybrid deep learning (DL) approaches. The BRFSS dataset was preprocessed by removing duplicates, handling missing values, identifying categorical and numerical features, and applying Principal Component Analysis (PCA) for feature extraction. Several ML models, including Decision Trees (DT), Random Forest (RF), k-Nearest Neighbors (KNN), Support Vector Machine (SVM), AdaBoost, and XGBoost, were implemented, with XGBoost achieving the highest accuracy of 0.9050. Various DL models, such as Artificial Neural Networks (ANN), Deep Neural Networks (DNN), Recurrent Neural Networks (RNN), Convolutional Neural Networks (CNN), Long Short-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), and Gated Recurrent Unit (GRU), as well as hybrid models combining CNN with LSTM, BiLSTM, and GRU, were also explored. Some of these models achieved perfect recall (1.00), with the LSTM model achieving the highest accuracy of 0.9050. Our research highlights the effectiveness of ML and DL models in predicting CVD risk among diabetic patients, automating and enhancing clinical decision-making. High accuracy and F1 scores demonstrate these models' potential to improve personalized risk management and preventive strategies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 29,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04973",
        "abs_url": "https://arxiv.org/abs/2511.04973",
        "pdf_url": "https://arxiv.org/pdf/2511.04973",
        "title": "Less Is More: Generating Time Series with LLaMA-Style Autoregression in Simple Factorized Latent Spaces",
        "authors": [
            "Siyuan Li",
            "Yifan Sun",
            "Lei Cheng",
            "Lewen Wang",
            "Yang Liu",
            "Weiqing Liu",
            "Jianlong Li",
            "Jiang Bian",
            "Shikai Fang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Generative models for multivariate time series are essential for data augmentation, simulation, and privacy preservation, yet current state-of-the-art diffusion-based approaches are slow and limited to fixed-length windows. We propose FAR-TS, a simple yet effective framework that combines disentangled factorization with an autoregressive Transformer over a discrete, quantized latent space to generate time series. Each time series is decomposed into a data-adaptive basis that captures static cross-channel correlations and temporal coefficients that are vector-quantized into discrete tokens. A LLaMA-style autoregressive Transformer then models these token sequences, enabling fast and controllable generation of sequences with arbitrary length. Owing to its streamlined design, FAR-TS achieves orders-of-magnitude faster generation than Diffusion-TS while preserving cross-channel correlations and an interpretable latent space, enabling high-quality and flexible time series synthesis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 30,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04979",
        "abs_url": "https://arxiv.org/abs/2511.04979",
        "pdf_url": "https://arxiv.org/pdf/2511.04979",
        "title": "Scaling Up ROC-Optimizing Support Vector Machines",
        "authors": [
            "Gimun Bae",
            "Seung Jun Shin"
        ],
        "comments": "15 pages, Submitted to Stat",
        "subjects": "Machine Learning (cs.LG); Computation (stat.CO); Machine Learning (stat.ML)",
        "abstract": "The ROC-SVM, originally proposed by Rakotomamonjy, directly maximizes the area under the ROC curve (AUC) and has become an attractive alternative of the conventional binary classification under the presence of class imbalance. However, its practical use is limited by high computational cost, as training involves evaluating all $O(n^2)$. To overcome this limitation, we develop a scalable variant of the ROC-SVM that leverages incomplete U-statistics, thereby substantially reducing computational complexity. We further extend the framework to nonlinear classification through a low-rank kernel approximation, enabling efficient training in reproducing kernel Hilbert spaces. Theoretical analysis establishes an error bound that justifies the proposed approximation, and empirical results on both synthetic and real datasets demonstrate that the proposed method achieves comparable AUC performance to the original ROC-SVM with drastically reduced training time.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 31,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04980",
        "abs_url": "https://arxiv.org/abs/2511.04980",
        "pdf_url": "https://arxiv.org/pdf/2511.04980",
        "title": "Unlocking the Black Box: A Five-Dimensional Framework for Evaluating Explainable AI in Credit Risk",
        "authors": [
            "Rongbin Ye",
            "Jiaqi Chen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The financial industry faces a significant challenge modeling and risk portfolios: balancing the predictability of advanced machine learning models, neural network models, and explainability required by regulatory entities (such as Office of the Comptroller of the Currency, Consumer Financial Protection Bureau). This paper intends to fill the gap in the application between these \"black box\" models and explainability frameworks, such as LIME and SHAP. Authors elaborate on the application of these frameworks on different models and demonstrates the more complex models with better prediction powers could be applied and reach the same level of the explainability, using SHAP and LIME. Beyond the comparison and discussion of performances, this paper proposes a novel five dimensional framework evaluating Inherent Interpretability, Global Explanations, Local Explanations, Consistency, and Complexity to offer a nuanced method for assessing and comparing model explainability beyond simple accuracy metrics. This research demonstrates the feasibility of employing sophisticated, high performing ML models in regulated financial environments by utilizing modern explainability techniques and provides a structured approach to evaluate the crucial trade offs between model performance and interpretability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 32,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04981",
        "abs_url": "https://arxiv.org/abs/2511.04981",
        "pdf_url": "https://arxiv.org/pdf/2511.04981",
        "title": "Deep Progressive Training: scaling up depth capacity of zero/one-layer models",
        "authors": [
            "Zhiqi Bu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Model depth is a double-edged sword in deep learning: deeper models achieve higher accuracy but require higher computational cost. To efficiently train models at scale, an effective strategy is the progressive training, which scales up model capacity during training, hence significantly reducing computation with little to none performance degradation. In this work, we study the depth expansion of large models through the lens of optimization theory and feature learning, offering insights on the initialization of new layers, hyperparameter transfer, learning rate schedule, and timing of model expansion. Specifically, we propose zero/one-layer progressive training for the optimal tradeoff between computation and loss. For example, zero/one-layer progressive training on GPT2 can save $\\approx 80\\%$ compute, or equivalently accelerate $\\approx 5\\times$ while achieving almost the same loss, compared to to a fully trained 60-layer model with 7B parameters.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 33,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04984",
        "abs_url": "https://arxiv.org/abs/2511.04984",
        "pdf_url": "https://arxiv.org/pdf/2511.04984",
        "title": "Peptide2Mol: A Diffusion Model for Generating Small Molecules as Peptide Mimics for Targeted Protein Binding",
        "authors": [
            "Xinheng He",
            "Yijia Zhang",
            "Haowei Lin",
            "Xingang Peng",
            "Xiangzhe Kong",
            "Mingyu Li",
            "Jianzhu Ma"
        ],
        "comments": "Abstract 1 page, main text 9 pages, references 2 pages, 4 figures. Submitted to RECOMB 2026",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Structure-based drug design has seen significant advancements with the integration of artificial intelligence (AI), particularly in the generation of hit and lead compounds. However, most AI-driven approaches neglect the importance of endogenous protein interactions with peptides, which may result in suboptimal molecule designs. In this work, we present Peptide2Mol, an E(3)-equivariant graph neural network diffusion model that generates small molecules by referencing both the original peptide binders and their surrounding protein pocket environments. Trained on large datasets and leveraging sophisticated modeling techniques, Peptide2Mol not only achieves state-of-the-art performance in non-autoregressive generative tasks, but also produces molecules with similarity to the original peptide binder. Additionally, the model allows for molecule optimization and peptidomimetic design through a partial diffusion process. Our results highlight Peptide2Mol as an effective deep generative model for generating and optimizing bioactive small molecules from protein binding pockets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 34,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04988",
        "abs_url": "https://arxiv.org/abs/2511.04988",
        "pdf_url": "https://arxiv.org/pdf/2511.04988",
        "title": "Carbon Price Forecasting with Structural Breaks: A Comparative Study of Deep Learning Models",
        "authors": [
            "Runsheng Ren",
            "Jing Li",
            "Yanxiu Li",
            "Shixun Huang",
            "Jun Shen",
            "Wanqing Li",
            "John Le",
            "Sheng Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Accurately forecasting carbon prices is essential for informed energy market decision-making, guiding sustainable energy planning, and supporting effective decarbonization strategies. However, it remains challenging due to structural breaks and high-frequency noise caused by frequent policy interventions and market shocks. Existing studies, including the most recent baseline approaches, have attempted to incorporate breakpoints but often treat denoising and modeling as separate processes and lack systematic evaluation across advanced deep learning architectures, limiting the robustness and the generalization capability. To address these gaps, this paper proposes a comprehensive hybrid framework that integrates structural break detection (Bai-Perron, ICSS, and PELT algorithms), wavelet signal denoising, and three state-of-the-art deep learning models (LSTM, GRU, and TCN). Using European Union Allowance (EUA) spot prices from 2007 to 2024 and exogenous features such as energy prices and policy indicators, the framework constructs univariate and multivariate datasets for comparative evaluation. Experimental results demonstrate that our proposed PELT-WT-TCN achieves the highest prediction accuracy, reducing forecasting errors by 22.35% in RMSE and 18.63% in MAE compared to the state-of-the-art baseline model (Breakpoints with Wavelet and LSTM), and by 70.55% in RMSE and 74.42% in MAE compared to the original LSTM without decomposition from the same baseline study. These findings underscore the value of integrating structural awareness and multiscale decomposition into deep learning architectures to enhance accuracy and interpretability in carbon price forecasting and other nonstationary financial time series.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 35,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05005",
        "abs_url": "https://arxiv.org/abs/2511.05005",
        "pdf_url": "https://arxiv.org/pdf/2511.05005",
        "title": "Multi-agent Coordination via Flow Matching",
        "authors": [
            "Dongsu Lee",
            "Daehee Lee",
            "Amy Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Robotics (cs.RO)",
        "abstract": "This work presents MAC-Flow, a simple yet expressive framework for multi-agent coordination. We argue that requirements of effective coordination are twofold: (i) a rich representation of the diverse joint behaviors present in offline data and (ii) the ability to act efficiently in real time. However, prior approaches often sacrifice one for the other, i.e., denoising diffusion-based solutions capture complex coordination but are computationally slow, while Gaussian policy-based solutions are fast but brittle in handling multi-agent interaction. MAC-Flow addresses this trade-off by first learning a flow-based representation of joint behaviors, and then distilling it into decentralized one-step policies that preserve coordination while enabling fast execution. Across four different benchmarks, including $12$ environments and $34$ datasets, MAC-Flow alleviates the trade-off between performance and computational cost, specifically achieving about $\\boldsymbol{\\times14.5}$ faster inference compared to diffusion-based MARL methods, while maintaining good performance. At the same time, its inference speed is similar to that of prior Gaussian policy-based offline multi-agent reinforcement learning (MARL) methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 36,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05114",
        "abs_url": "https://arxiv.org/abs/2511.05114",
        "pdf_url": "https://arxiv.org/pdf/2511.05114",
        "title": "Usando LLMs para Programar Jogos de Tabuleiro e Varia√ß√µes",
        "authors": [
            "√Ålvaro Guglielmin Becker",
            "Lana Bertoldo Rossato",
            "Anderson Rocha Tavares"
        ],
        "comments": "Accepted for presentation at the I Escola Regional de Aprendizado de M√°quina e Intelig√™ncia Artificial da Regi√£o Sul, 2025, in Portuguese language",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Creating programs to represent board games can be a time-consuming task. Large Language Models (LLMs) arise as appealing tools to expedite this process, given their capacity to efficiently generate code from simple contextual information. In this work, we propose a method to test how capable three LLMs (Claude, DeepSeek and ChatGPT) are at creating code for board games, as well as new variants of existing games.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 37,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05124",
        "abs_url": "https://arxiv.org/abs/2511.05124",
        "pdf_url": "https://arxiv.org/pdf/2511.05124",
        "title": "QuAnTS: Question Answering on Time Series",
        "authors": [
            "Felix Divo",
            "Maurice Kraus",
            "Anh Q. Nguyen",
            "Hao Xue",
            "Imran Razzak",
            "Flora D. Salim",
            "Kristian Kersting",
            "Devendra Singh Dhami"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Text offers intuitive access to information. This can, in particular, complement the density of numerical time series, thereby allowing improved interactions with time series models to enhance accessibility and decision-making. While the creation of question-answering datasets and models has recently seen remarkable growth, most research focuses on question answering (QA) on vision and text, with time series receiving minute attention. To bridge this gap, we propose a challenging novel time series QA (TSQA) dataset, QuAnTS, for Question Answering on Time Series data. Specifically, we pose a wide variety of questions and answers about human motion in the form of tracked skeleton trajectories. We verify that the large-scale QuAnTS dataset is well-formed and comprehensive through extensive experiments. Thoroughly evaluating existing and newly proposed baselines then lays the groundwork for a deeper exploration of TSQA using QuAnTS. Additionally, we provide human performances as a key reference for gauging the practical usability of such models. We hope to encourage future research on interacting with time series models through text, enabling better decision-making and more transparent systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 38,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05163",
        "abs_url": "https://arxiv.org/abs/2511.05163",
        "pdf_url": "https://arxiv.org/pdf/2511.05163",
        "title": "Consecutive Preferential Bayesian Optimization",
        "authors": [
            "Aras Erarslan",
            "Carlos Sevilla Salcedo",
            "Ville Tanskanen",
            "Anni Nisov",
            "Eero P√§iv√§kumpu",
            "Heikki Aisala",
            "Kaisu Honkap√§√§",
            "Arto Klami",
            "Petrus Mikkola"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Preferential Bayesian optimization allows optimization of objectives that are either expensive or difficult to measure directly, by relying on a minimal number of comparative evaluations done by a human expert. Generating candidate solutions for evaluation is also often expensive, but this cost is ignored by existing methods. We generalize preference-based optimization to explicitly account for production and evaluation costs with Consecutive Preferential Bayesian Optimization, reducing production cost by constraining comparisons to involve previously generated candidates. We also account for the perceptual ambiguity of the oracle providing the feedback by incorporating a Just-Noticeable Difference threshold into a probabilistic preference model to capture indifference to small utility differences. We adapt an information-theoretic acquisition strategy to this setting, selecting new configurations that are most informative about the unknown optimum under a preference model accounting for the perceptual ambiguity. We empirically demonstrate a notable increase in accuracy in setups with high production costs or with indifference feedback.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 39,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05169",
        "abs_url": "https://arxiv.org/abs/2511.05169",
        "pdf_url": "https://arxiv.org/pdf/2511.05169",
        "title": "Multimodal Deep Learning for Prediction of Progression-Free Survival in Patients with Neuroendocrine Tumors Undergoing 177Lu-based Peptide Receptor Radionuclide Therapy",
        "authors": [
            "Simon Baur",
            "Tristan Ruhwedel",
            "Ekin B√∂ke",
            "Zuzanna Kobus",
            "Gergana Lishkova",
            "Christoph Wetz",
            "Holger Amthauer",
            "Christoph Roderburg",
            "Frank Tacke",
            "Julian M. Rogasch",
            "Wojciech Samek",
            "Henning Jann",
            "Jackie Ma",
            "Johannes Eschrich"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Peptide receptor radionuclide therapy (PRRT) is an established treatment for metastatic neuroendocrine tumors (NETs), yet long-term disease control occurs only in a subset of patients. Predicting progression-free survival (PFS) could support individualized treatment planning. This study evaluates laboratory, imaging, and multimodal deep learning models for PFS prediction in PRRT-treated patients. In this retrospective, single-center study 116 patients with metastatic NETs undergoing 177Lu-DOTATOC were included. Clinical characteristics, laboratory values, and pretherapeutic somatostatin receptor positron emission tomography/computed tomographies (SR-PET/CT) were collected. Seven models were trained to classify low- vs. high-PFS groups, including unimodal (laboratory, SR-PET, or CT) and multimodal fusion approaches. Explainability was evaluated by feature importance analysis and gradient maps. Forty-two patients (36%) had short PFS (< 1 year), 74 patients long PFS (>1 year). Groups were similar in most characteristics, except for higher baseline chromogranin A (p = 0.003), elevated gamma-GT (p = 0.002), and fewer PRRT cycles (p < 0.001) in short-PFS patients. The Random Forest model trained only on laboratory biomarkers reached an AUROC of 0.59 +- 0.02. Unimodal three-dimensional convolutional neural networks using SR-PET or CT performed worse (AUROC 0.42 +- 0.03 and 0.54 +- 0.01, respectively). A multimodal fusion model laboratory values, SR-PET, and CT -augmented with a pretrained CT branch - achieved the best results (AUROC 0.72 +- 0.01, AUPRC 0.80 +- 0.01). Multimodal deep learning combining SR-PET, CT, and laboratory biomarkers outperformed unimodal approaches for PFS prediction after PRRT. Upon external validation, such models may support risk-adapted follow-up strategies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 40,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05177",
        "abs_url": "https://arxiv.org/abs/2511.05177",
        "pdf_url": "https://arxiv.org/pdf/2511.05177",
        "title": "Associative Poisoning to Generative Machine Learning",
        "authors": [
            "Mathias Lundteigen Mohus",
            "Jingyue Li",
            "Zhirong Yang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The widespread adoption of generative models such as Stable Diffusion and ChatGPT has made them increasingly attractive targets for malicious exploitation, particularly through data poisoning. Existing poisoning attacks compromising synthesised data typically either cause broad degradation of generated data or require control over the training process, limiting their applicability in real-world scenarios. In this paper, we introduce a novel data poisoning technique called associative poisoning, which compromises fine-grained features of the generated data without requiring control of the training process. This attack perturbs only the training data to manipulate statistical associations between specific feature pairs in the generated outputs. We provide a formal mathematical formulation of the attack and prove its theoretical feasibility and stealthiness. Empirical evaluations using two state-of-the-art generative models demonstrate that associative poisoning effectively induces or suppresses feature associations while preserving the marginal distributions of the targeted features and maintaining high-quality outputs, thereby evading visual detection. These results suggest that generative systems used in image synthesis, synthetic dataset generation, and natural language processing are susceptible to subtle, stealthy manipulations that compromise their statistical integrity. To address this risk, we examine the limitations of existing defensive strategies and propose a novel countermeasure strategy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 41,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05187",
        "abs_url": "https://arxiv.org/abs/2511.05187",
        "pdf_url": "https://arxiv.org/pdf/2511.05187",
        "title": "Linear Gradient Prediction with Control Variates",
        "authors": [
            "Kamil Ciosek",
            "Nicol√≤ Felicioni",
            "Juan Elenter Litwin"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "We propose a new way of training neural networks, with the goal of reducing training cost. Our method uses approximate predicted gradients instead of the full gradients that require an expensive backward pass. We derive a control-variate-based technique that ensures our updates are unbiased estimates of the true gradient. Moreover, we propose a novel way to derive a predictor for the gradient inspired by the theory of the Neural Tangent Kernel. We empirically show the efficacy of the technique on a vision transformer classification task.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 42,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05221",
        "abs_url": "https://arxiv.org/abs/2511.05221",
        "pdf_url": "https://arxiv.org/pdf/2511.05221",
        "title": "ActiTect: A Generalizable Machine Learning Pipeline for REM Sleep Behavior Disorder Screening through Standardized Actigraphy",
        "authors": [
            "David Bertram",
            "Anja Ophey",
            "Sinah R√∂ttgen",
            "Konstantin Kuffer",
            "Gereon R. Fink",
            "Elke Kalbe",
            "Clint Hansen",
            "Walter Maetzler",
            "Maximilian Kapsecker",
            "Lara M. Reimer",
            "Stephan Jonas",
            "Andreas T. Damgaard",
            "Natasha B. Bertelsen",
            "Casper Skjaerbaek",
            "Per Borghammer",
            "Karolien Groenewald",
            "Pietro-Luca Ratti",
            "Michele T. Hu",
            "No √©mie Moreau",
            "Michael Sommerauer",
            "Katarzyna Bozek"
        ],
        "comments": "30 pages including supplement, 4 core figures, 1 supplement figure",
        "subjects": "Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)",
        "abstract": "Isolated rapid eye movement sleep behavior disorder (iRBD) is a major prodromal marker of $\\alpha$-synucleinopathies, often preceding the clinical onset of Parkinson's disease, dementia with Lewy bodies, or multiple system atrophy. While wrist-worn actimeters hold significant potential for detecting RBD in large-scale screening efforts by capturing abnormal nocturnal movements, they become inoperable without a reliable and efficient analysis pipeline. This study presents ActiTect, a fully automated, open-source machine learning tool to identify RBD from actigraphy recordings. To ensure generalizability across heterogeneous acquisition settings, our pipeline includes robust preprocessing and automated sleep-wake detection to harmonize multi-device data and extract physiologically interpretable motion features characterizing activity patterns. Model development was conducted on a cohort of 78 individuals, yielding strong discrimination under nested cross-validation (AUROC = 0.95). Generalization was confirmed on a blinded local test set (n = 31, AUROC = 0.86) and on two independent external cohorts (n = 113, AUROC = 0.84; n = 57, AUROC = 0.94). To assess real-world robustness, leave-one-dataset-out cross-validation across the internal and external cohorts demonstrated consistent performance (AUROC range = 0.84-0.89). A complementary stability analysis showed that key predictive features remained reproducible across datasets, supporting the final pooled multi-center model as a robust pre-trained resource for broader deployment. By being open-source and easy to use, our tool promotes widespread adoption and facilitates independent validation and collaborative improvements, thereby advancing the field toward a unified and generalizable RBD detection model using wearable devices.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 43,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05236",
        "abs_url": "https://arxiv.org/abs/2511.05236",
        "pdf_url": "https://arxiv.org/pdf/2511.05236",
        "title": "The Causal Round Trip: Generating Authentic Counterfactuals by Eliminating Information Loss",
        "authors": [
            "Rui Wu",
            "Lizheng Wang",
            "Yongjun Li"
        ],
        "comments": "50 pages, 10 figures. Submitted to the Journal of Machine Learning Research (JMLR). Keywords: Causal Inference, Diffusion Models, Causal Information Conservation, Structural Causal Models, Counterfactual Generation, BELM, Structural Reconstruction Error",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Judea Pearl's vision of Structural Causal Models (SCMs) as engines for counterfactual reasoning hinges on faithful abduction: the precise inference of latent exogenous noise. For decades, operationalizing this step for complex, non-linear mechanisms has remained a significant computational challenge. The advent of diffusion models, powerful universal function approximators, offers a promising solution. However, we argue that their standard design, optimized for perceptual generation over logical inference, introduces a fundamental flaw for this classical problem: an inherent information loss we term the Structural Reconstruction Error (SRE). To address this challenge, we formalize the principle of Causal Information Conservation (CIC) as the necessary condition for faithful abduction. We then introduce BELM-MDCM, the first diffusion-based framework engineered to be causally sound by eliminating SRE by construction through an analytically invertible mechanism. To operationalize this framework, a Targeted Modeling strategy provides structural regularization, while a Hybrid Training Objective instills a strong causal inductive bias. Rigorous experiments demonstrate that our Zero-SRE framework not only achieves state-of-the-art accuracy but, more importantly, enables the high-fidelity, individual-level counterfactuals required for deep causal inquiries. Our work provides a foundational blueprint that reconciles the power of modern generative models with the rigor of classical causal theory, establishing a new and more rigorous standard for this emerging field.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 44,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05265",
        "abs_url": "https://arxiv.org/abs/2511.05265",
        "pdf_url": "https://arxiv.org/pdf/2511.05265",
        "title": "An End-to-End Deep Reinforcement Learning Approach for Solving the Traveling Salesman Problem with Drones",
        "authors": [
            "Taihelong Zeng",
            "Yun Lin",
            "Yuhe Shi",
            "Yan Li",
            "Zhiqing Wei",
            "Xuanru Ji"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "The emergence of truck-drone collaborative systems in last-mile logistics has positioned the Traveling Salesman Problem with Drones (TSP-D) as a pivotal extension of classical routing optimization, where synchronized vehicle coordination promises substantial operational efficiency and reduced environmental impact, yet introduces NP-hard combinatorial complexity beyond the reach of conventional optimization paradigms. Deep reinforcement learning offers a theoretically grounded framework to address TSP-D's inherent challenges through self-supervised policy learning and adaptive decision-making. This study proposes a hierarchical Actor-Critic deep reinforcement learning framework for solving the TSP-D problem. The architecture consists of two primary components: a Transformer-inspired encoder and an efficient Minimal Gated Unit decoder. The encoder incorporates a novel, optimized k-nearest neighbors sparse attention mechanism specifically for focusing on relevant spatial relationships, further enhanced by the integration of global node features. The Minimal Gated Unit decoder processes these encoded representations to efficiently generate solution sequences. The entire framework operates within an asynchronous advantage actor-critic paradigm. Experimental results show that, on benchmark TSP-D instances of various scales (N=10 to 100), the proposed model can obtain competitive or even superior solutions in shorter average computation times compared to high-performance heuristic algorithms and existing reinforcement learning methods. Moreover, compared to advanced reinforcement learning algorithm benchmarks, the proposed framework significantly reduces the total training time required while achieving superior final performance, highlighting its notable advantage in training efficiency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 45,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05289",
        "abs_url": "https://arxiv.org/abs/2511.05289",
        "pdf_url": "https://arxiv.org/pdf/2511.05289",
        "title": "Embedding-Space Data Augmentation to Prevent Membership Inference Attacks in Clinical Time Series Forecasting",
        "authors": [
            "Marius Fracarolli",
            "Michael Staniek",
            "Stefan Riezler"
        ],
        "comments": "Accepted as a proceedings paper at Machine Learning for Health (ML4H) symposium 2025, December 1-2, 2025, San Diego, United States, 15 pages",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Balancing strong privacy guarantees with high predictive performance is critical for time series forecasting (TSF) tasks involving Electronic Health Records (EHR). In this study, we explore how data augmentation can mitigate Membership Inference Attacks (MIA) on TSF models. We show that retraining with synthetic data can substantially reduce the effectiveness of loss-based MIAs by reducing the attacker's true-positive to false-positive ratio. The key challenge is generating synthetic samples that closely resemble the original training data to confuse the attacker, while also introducing enough novelty to enhance the model's ability to generalize to unseen data. We examine multiple augmentation strategies - Zeroth-Order Optimization (ZOO), a variant of ZOO constrained by Principal Component Analysis (ZOO-PCA), and MixUp - to strengthen model resilience without sacrificing accuracy. Our experimental results show that ZOO-PCA yields the best reductions in TPR/FPR ratio for MIA attacks without sacrificing performance on test data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 46,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05313",
        "abs_url": "https://arxiv.org/abs/2511.05313",
        "pdf_url": "https://arxiv.org/pdf/2511.05313",
        "title": "Attention and Compression is all you need for Controllably Efficient Language Models",
        "authors": [
            "Jatin Prakash",
            "Aahlad Puli",
            "Rajesh Ranganath"
        ],
        "comments": "Preprint",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The quadratic cost of attention in transformers motivated the development of efficient approaches: namely sparse and sliding window attention, convolutions and linear attention. Although these approaches result in impressive reductions in compute and memory, they often trade-off with quality, specifically in-context recall performance. Moreover, apriori fixing this quality-compute tradeoff means being suboptimal from the get-go: some downstream applications require more memory for in-context recall, while others require lower latency and memory. Further, these approaches rely on heuristic choices that artificially restrict attention, or require handcrafted and complex recurrent state update rules, or they must be carefully composed with attention at specific layers to form a hybrid architecture that complicates the design process, especially at scale. To address above issues, we propose Compress & Attend Transformer (CAT), a conceptually simple architecture employing two simple ingredients only: dense attention and compression. CAT decodes chunks of tokens by attending to compressed chunks of the sequence so far. Compression results in decoding from a reduced sequence length that yields compute and memory savings, while choosing a particular chunk size trades-off quality for efficiency. Moreover, CAT can be trained with multiple chunk sizes at once, unlocking control of quality-compute trade-offs directly at test-time without any retraining, all in a single adaptive architecture. In exhaustive evaluations on common language modeling tasks, in-context recall, and long-context understanding, a single adaptive CAT model outperforms existing efficient baselines, including hybrid architectures, across different compute-memory budgets. Further, a single CAT matches dense transformer in language modeling across model scales while being 1.4-3x faster and requiring 2-9x lower total memory usage.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 47,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05325",
        "abs_url": "https://arxiv.org/abs/2511.05325",
        "pdf_url": "https://arxiv.org/pdf/2511.05325",
        "title": "Turning Adversaries into Allies: Reversing Typographic Attacks for Multimodal E-Commerce Product Retrieval",
        "authors": [
            "Janet Jenq",
            "Hongda Shen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Multimodal product retrieval systems in e-commerce platforms rely on effectively combining visual and textual signals to improve search relevance and user experience. However, vision-language models such as CLIP are vulnerable to typographic attacks, where misleading or irrelevant text embedded in images skews model predictions. In this work, we propose a novel method that reverses the logic of typographic attacks by rendering relevant textual content (e.g., titles, descriptions) directly onto product images to perform vision-text compression, thereby strengthening image-text alignment and boosting multimodal product retrieval performance. We evaluate our method on three vertical-specific e-commerce datasets (sneakers, handbags, and trading cards) using six state-of-the-art vision foundation models. Our experiments demonstrate consistent improvements in unimodal and multimodal retrieval accuracy across categories and model families. Our findings suggest that visually rendering product metadata is a simple yet effective enhancement for zero-shot multimodal retrieval in e-commerce applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 48,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05330",
        "abs_url": "https://arxiv.org/abs/2511.05330",
        "pdf_url": "https://arxiv.org/pdf/2511.05330",
        "title": "Learning Dynamics from Input-Output Data with Hamiltonian Gaussian Processes",
        "authors": [
            "Jan-Hendrik Ewering",
            "Robin E. Herrmann",
            "Niklas Wahlstr√∂m",
            "Thomas B. Sch√∂n",
            "Thomas Seel"
        ],
        "comments": "17 pages, 5 figures",
        "subjects": "Machine Learning (cs.LG); Systems and Control (eess.SY)",
        "abstract": "Embedding non-restrictive prior knowledge, such as energy conservation laws, in learning-based approaches is a key motive to construct physically consistent models from limited data, relevant for, e.g., model-based control. Recent work incorporates Hamiltonian dynamics into Gaussian Process (GP) regression to obtain uncertainty-quantifying models that adhere to the underlying physical principles. However, these works rely on velocity or momentum data, which is rarely available in practice. In this paper, we consider dynamics learning with non-conservative Hamiltonian GPs, and address the more realistic problem setting of learning from input-output data. We provide a fully Bayesian scheme for estimating probability densities of unknown hidden states, of GP hyperparameters, as well as of structural hyperparameters, such as damping coefficients. Considering the computational complexity of GPs, we take advantage of a reduced-rank GP approximation and leverage its properties for computationally efficient prediction and training. The proposed method is evaluated in a nonlinear simulation case study and compared to a state-of-the-art approach that relies on momentum measurements.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 49,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05355",
        "abs_url": "https://arxiv.org/abs/2511.05355",
        "pdf_url": "https://arxiv.org/pdf/2511.05355",
        "title": "SAD-Flower: Flow Matching for Safe, Admissible, and Dynamically Consistent Planning",
        "authors": [
            "Tzu-Yuan Huang",
            "Armin Lederer",
            "Dai-Jie Wu",
            "Xiaobing Dai",
            "Sihua Zhang",
            "Stefan Sosnowski",
            "Shao-Hua Sun",
            "Sandra Hirche"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Robotics (cs.RO); Systems and Control (eess.SY)",
        "abstract": "Flow matching (FM) has shown promising results in data-driven planning. However, it inherently lacks formal guarantees for ensuring state and action constraints, whose satisfaction is a fundamental and crucial requirement for the safety and admissibility of planned trajectories on various systems. Moreover, existing FM planners do not ensure the dynamical consistency, which potentially renders trajectories inexecutable. We address these shortcomings by proposing SAD-Flower, a novel framework for generating Safe, Admissible, and Dynamically consistent trajectories. Our approach relies on an augmentation of the flow with a virtual control input. Thereby, principled guidance can be derived using techniques from nonlinear control theory, providing formal guarantees for state constraints, action constraints, and dynamic consistency. Crucially, SAD-Flower operates without retraining, enabling test-time satisfaction of unseen constraints. Through extensive experiments across several tasks, we demonstrate that SAD-Flower outperforms various generative-model-based baselines in ensuring constraint satisfaction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 50,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05357",
        "abs_url": "https://arxiv.org/abs/2511.05357",
        "pdf_url": "https://arxiv.org/pdf/2511.05357",
        "title": "Diffusion-Based Electromagnetic Inverse Design of Scattering Structured Media",
        "authors": [
            "Mikhail Tsukerman",
            "Konstantin Grotov",
            "Pavel Ginzburg"
        ],
        "comments": "Accepted to Machine Learning and the Physical Sciences Workshop, NeurIPS 2025",
        "subjects": "Machine Learning (cs.LG); Applied Physics (physics.app-ph); Computational Physics (physics.comp-ph)",
        "abstract": "We present a conditional diffusion model for electromagnetic inverse design that generates structured media geometries directly from target differential scattering cross-section profiles, bypassing expensive iterative optimization. Our 1D U-Net architecture with Feature-wise Linear Modulation learns to map desired angular scattering patterns to 2x2 dielectric sphere structure, naturally handling the non-uniqueness of inverse problems by sampling diverse valid designs. Trained on 11,000 simulated metasurfaces, the model achieves median MPE below 19% on unseen targets (best: 1.39%), outperforming CMA-ES evolutionary optimization while reducing design time from hours to seconds. These results demonstrate that employing diffusion models is promising for advancing electromagnetic inverse design research, potentially enabling rapid exploration of complex metasurface architectures and accelerating the development of next-generation photonic and wireless communication systems. The code is publicly available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 51,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05396",
        "abs_url": "https://arxiv.org/abs/2511.05396",
        "pdf_url": "https://arxiv.org/pdf/2511.05396",
        "title": "Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction",
        "authors": [
            "Yiting He",
            "Zhishuai Liu",
            "Weixin Wang",
            "Pan Xu"
        ],
        "comments": "53 pages, 6 figures, 3 tables. Published in Proceedings of the 42nd International Conference on Machine Learning (ICML 2025)",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Robotics (cs.RO); Machine Learning (stat.ML)",
        "abstract": "Off-dynamics reinforcement learning (RL), where training and deployment transition dynamics are different, can be formulated as learning in a robust Markov decision process (RMDP) where uncertainties in transition dynamics are imposed. Existing literature mostly assumes access to generative models allowing arbitrary state-action queries or pre-collected datasets with a good state coverage of the deployment environment, bypassing the challenge of exploration. In this work, we study a more realistic and challenging setting where the agent is limited to online interaction with the training environment. To capture the intrinsic difficulty of exploration in online RMDPs, we introduce the supremal visitation ratio, a novel quantity that measures the mismatch between the training dynamics and the deployment dynamics. We show that if this ratio is unbounded, online learning becomes exponentially hard. We propose the first computationally efficient algorithm that achieves sublinear regret in online RMDPs with $f$-divergence based transition uncertainties. We also establish matching regret lower bounds, demonstrating that our algorithm achieves optimal dependence on both the supremal visitation ratio and the number of interaction episodes. Finally, we validate our theoretical results through comprehensive numerical experiments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 52,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05420",
        "abs_url": "https://arxiv.org/abs/2511.05420",
        "pdf_url": "https://arxiv.org/pdf/2511.05420",
        "title": "ProDER: A Continual Learning Approach for Fault Prediction in Evolving Smart Grids",
        "authors": [
            "Emad Efatinasab",
            "Nahal Azadi",
            "Davide Dalle Pezze",
            "Gian Antonio Susto",
            "Chuadhry Mujeeb Ahmed",
            "Mirco Rampazzo"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "As smart grids evolve to meet growing energy demands and modern operational challenges, the ability to accurately predict faults becomes increasingly critical. However, existing AI-based fault prediction models struggle to ensure reliability in evolving environments where they are required to adapt to new fault types and operational zones. In this paper, we propose a continual learning (CL) framework in the smart grid context to evolve the model together with the environment. We design four realistic evaluation scenarios grounded in class-incremental and domain-incremental learning to emulate evolving grid conditions. We further introduce Prototype-based Dark Experience Replay (ProDER), a unified replay-based approach that integrates prototype-based feature regularization, logit distillation, and a prototype-guided replay memory. ProDER achieves the best performance among tested CL techniques, with only a 0.045 accuracy drop for fault type prediction and 0.015 for fault zone prediction. These results demonstrate the practicality of CL for scalable, real-world fault prediction in smart grids.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 53,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05444",
        "abs_url": "https://arxiv.org/abs/2511.05444",
        "pdf_url": "https://arxiv.org/pdf/2511.05444",
        "title": "Adversarially Robust Multitask Adaptive Control",
        "authors": [
            "Kasra Fallah",
            "Leonardo F. Toso",
            "James Anderson"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Systems and Control (eess.SY); Optimization and Control (math.OC)",
        "abstract": "We study adversarially robust multitask adaptive linear quadratic control; a setting where multiple systems collaboratively learn control policies under model uncertainty and adversarial corruption. We propose a clustered multitask approach that integrates clustering and system identification with resilient aggregation to mitigate corrupted model updates. Our analysis characterizes how clustering accuracy, intra-cluster heterogeneity, and adversarial behavior affect the expected regret of certainty-equivalent (CE) control across LQR tasks. We establish non-asymptotic bounds demonstrating that the regret decreases inversely with the number of honest systems per cluster and that this reduction is preserved under a bounded fraction of adversarial systems within each cluster.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 54,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05456",
        "abs_url": "https://arxiv.org/abs/2511.05456",
        "pdf_url": "https://arxiv.org/pdf/2511.05456",
        "title": "Parameter-Efficient Conditioning for Material Generalization in Graph-Based Simulators",
        "authors": [
            "Naveen Raj Manoharan",
            "Hassan Iqbal",
            "Krishna Kumar"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Graph network-based simulators (GNS) have demonstrated strong potential for learning particle-based physics (such as fluids, deformable solids, and granular flows) while generalizing to unseen geometries due to their inherent inductive biases. However, existing models are typically trained for a single material type and fail to generalize across distinct constitutive behaviors, limiting their applicability in real-world engineering settings. Using granular flows as a running example, we propose a parameter-efficient conditioning mechanism that makes the GNS model adaptive to material parameters. We identify that sensitivity to material properties is concentrated in the early message-passing (MP) layers, a finding we link to the local nature of constitutive models (e.g., Mohr-Coulomb) and their effects on information propagation. We empirically validate this by showing that fine-tuning only the first few (1-5) of 10 MP layers of a pretrained model achieves comparable test performance as compared to fine-tuning the entire network. Building on this insight, we propose a parameter-efficient Feature-wise Linear Modulation (FiLM) conditioning mechanism designed to specifically target these early layers. This approach produces accurate long-term rollouts on unseen, interpolated, or moderately extrapolated values (e.g., up to 2.5 degrees for friction angle and 0.25 kPa for cohesion) when trained exclusively on as few as 12 short simulation trajectories from new materials, representing a 5-fold data reduction compared to a baseline multi-task learning method. Finally, we validate the model's utility by applying it to an inverse problem, successfully identifying unknown cohesion parameters from trajectory data. This approach enables the use of GNS in inverse design and closed-loop control tasks where material properties are treated as design variables.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 55,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05460",
        "abs_url": "https://arxiv.org/abs/2511.05460",
        "pdf_url": "https://arxiv.org/pdf/2511.05460",
        "title": "Synapse: Adaptive Arbitration of Complementary Expertise in Time Series Foundational Models",
        "authors": [
            "Sarkar Snigdha Sarathi Das",
            "Palash Goyal",
            "Mihir Parmar",
            "Yiwen Song",
            "Long T. Le",
            "Lesly Miculicich",
            "Jinsung Yoon",
            "Rui Zhang",
            "Hamid Palangi",
            "Tomas Pfister"
        ],
        "comments": "19 pages, 7 figures, 4 tables",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Pre-trained Time Series Foundational Models (TSFMs) represent a significant advance, capable of forecasting diverse time series with complex characteristics, including varied seasonalities, trends, and long-range dependencies. Despite their primary goal of universal time series forecasting, their efficacy is far from uniform; divergent training protocols and data sources cause individual TSFMs to exhibit highly variable performance across different forecasting tasks, domains, and horizons. Leveraging this complementary expertise by arbitrating existing TSFM outputs presents a compelling strategy, yet this remains a largely unexplored area of research. In this paper, we conduct a thorough examination of how different TSFMs exhibit specialized performance profiles across various forecasting settings, and how we can effectively leverage this behavior in arbitration between different time series models. We specifically analyze how factors such as model selection and forecast horizon distribution can influence the efficacy of arbitration strategies. Based on this analysis, we propose Synapse, a novel arbitration framework for TSFMs. Synapse is designed to dynamically leverage a pool of TSFMs, assign and adjust predictive weights based on their relative, context-dependent performance, and construct a robust forecast distribution by adaptively sampling from the output quantiles of constituent models. Experimental results demonstrate that Synapse consistently outperforms other popular ensembling techniques as well as individual TSFMs, demonstrating Synapse's efficacy in time series forecasting.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 56,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05471",
        "abs_url": "https://arxiv.org/abs/2511.05471",
        "pdf_url": "https://arxiv.org/pdf/2511.05471",
        "title": "Precipitation nowcasting of satellite data using physically conditioned neural networks",
        "authors": [
            "Ant√¥nio Cat√£o",
            "Melvin Poveda",
            "Leonardo Voltarelli",
            "Paulo Orenstein"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Accurate short-term precipitation forecasts predominantly rely on dense weather-radar networks, limiting operational value in places most exposed to climate extremes. We present TUPANN (Transferable and Universal Physics-Aligned Nowcasting Network), a satellite-only model trained on GOES-16 RRQPE. Unlike most deep learning models for nowcasting, TUPANN decomposes the forecast into physically meaningful components: a variational encoder-decoder infers motion and intensity fields from recent imagery under optical-flow supervision, a lead-time-conditioned MaxViT evolves the latent state, and a differentiable advection operator reconstructs future frames. We evaluate TUPANN on both GOES-16 and IMERG data, in up to four distinct climates (Rio de Janeiro, Manaus, Miami, La Paz) at 10-180min lead times using the CSI and HSS metrics over 4-64 mm/h thresholds. Comparisons against optical-flow, deep learning and hybrid baselines show that TUPANN achieves the best or second-best skill in most settings, with pronounced gains at higher thresholds. Training on multiple cities further improves performance, while cross-city experiments show modest degradation and occasional gains for rare heavy-rain regimes. The model produces smooth, interpretable motion fields aligned with numerical optical flow and runs in near real time due to the low latency of GOES-16. These results indicate that physically aligned learning can provide nowcasts that are skillful, transferable and global.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 57,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05482",
        "abs_url": "https://arxiv.org/abs/2511.05482",
        "pdf_url": "https://arxiv.org/pdf/2511.05482",
        "title": "SoilX: Calibration-Free Comprehensive Soil Sensing Through Contrastive Cross-Component Learning",
        "authors": [
            "Kang Yang",
            "Yuanlin Yang",
            "Yuning Chen",
            "Sikai Yang",
            "Xinyu Zhang",
            "Wan Du"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Precision agriculture demands continuous and accurate monitoring of soil moisture (M) and key macronutrients, including nitrogen (N), phosphorus (P), and potassium (K), to optimize yields and conserve resources. Wireless soil sensing has been explored to measure these four components; however, current solutions require recalibration (i.e., retraining the data processing model) to handle variations in soil texture, characterized by aluminosilicates (Al) and organic carbon (C), limiting their practicality. To address this, we introduce SoilX, a calibration-free soil sensing system that jointly measures six key components: {M, N, P, K, C, Al}. By explicitly modeling C and Al, SoilX eliminates texture- and carbon-dependent recalibration. SoilX incorporates Contrastive Cross-Component Learning (3CL), with two customized terms: the Orthogonality Regularizer and the Separation Loss, to effectively disentangle cross-component interference. Additionally, we design a novel tetrahedral antenna array with an antenna-switching mechanism, which can robustly measure soil dielectric permittivity independent of device placement. Extensive experiments demonstrate that SoilX reduces estimation errors by 23.8% to 31.5% over baselines and generalizes well to unseen fields.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 58,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05483",
        "abs_url": "https://arxiv.org/abs/2511.05483",
        "pdf_url": "https://arxiv.org/pdf/2511.05483",
        "title": "DGTN: Graph-Enhanced Transformer with Diffusive Attention Gating Mechanism for Enzyme DDG Prediction",
        "authors": [
            "Abigail Lin"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Predicting the effect of amino acid mutations on enzyme thermodynamic stability (DDG) is fundamental to protein engineering and drug design. While recent deep learning approaches have shown promise, they often process sequence and structure information independently, failing to capture the intricate coupling between local structural geometry and global sequential patterns. We present DGTN (Diffused Graph-Transformer Network), a novel architecture that co-learns graph neural network (GNN) weights for structural priors and transformer attention through a diffusion mechanism. Our key innovation is a bidirectional diffusion process where: (1) GNN-derived structural embeddings guide transformer attention via learnable diffusion kernels, and (2) transformer representations refine GNN message passing through attention-modulated graph updates. We provide rigorous mathematical analysis showing this co-learning scheme achieves provably better approximation bounds than independent processing. On ProTherm and SKEMPI benchmarks, DGTN achieves state-of-the-art performance (Pearson Rho = 0.87, RMSE = 1.21 kcal/mol), with 6.2% improvement over best baselines. Ablation studies confirm the diffusion mechanism contributes 4.8 points to correlation. Our theoretical analysis proves the diffused attention converges to optimal structure-sequence coupling, with convergence rate O(1/sqrt(T) ) where T is diffusion steps. This work establishes a principled framework for integrating heterogeneous protein representations through learnable diffusion.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 59,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2410.07961",
        "abs_url": "https://arxiv.org/abs/2410.07961",
        "pdf_url": "https://arxiv.org/pdf/2410.07961",
        "title": "QCircuitBench: A Large-Scale Dataset for Benchmarking Quantum Algorithm Design",
        "authors": [
            "Rui Yang",
            "Ziruo Wang",
            "Yuntian Gu",
            "Tianyi Chen",
            "Yitao Liang",
            "Tongyang Li"
        ],
        "comments": "45 pages, 17 figures, 15 tables, GitHub repository: this https URL",
        "subjects": "Quantum Physics (quant-ph); Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Quantum computing is an emerging field recognized for the significant speedup it offers over classical computing through quantum algorithms. However, designing and implementing quantum algorithms pose challenges due to the complex nature of quantum mechanics and the necessity for precise control over quantum states. Despite the significant advancements in AI, there has been a lack of datasets specifically tailored for this purpose. In this work, we introduce QCircuitBench, the first benchmark dataset designed to evaluate AI's capability in designing and implementing quantum algorithms using quantum programming languages. Unlike using AI for writing traditional codes, this task is fundamentally more complicated due to highly flexible design space. Our key contributions include: 1. A general framework which formulates the key features of quantum algorithm design for Large Language Models. 2. Implementations for quantum algorithms from basic primitives to advanced applications, spanning 3 task suites, 25 algorithms, and 120,290 data points. 3. Automatic validation and verification functions, allowing for iterative evaluation and interactive reasoning without human inspection. 4. Promising potential as a training dataset through preliminary fine-tuning results. We observed several interesting experimental phenomena: LLMs tend to exhibit consistent error patterns, and fine-tuning does not always outperform few-shot learning. In all, QCircuitBench is a comprehensive benchmark for LLM-driven quantum algorithm design, and it reveals limitations of LLMs in this domain.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 60,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.02401",
        "abs_url": "https://arxiv.org/abs/2511.02401",
        "pdf_url": "https://arxiv.org/pdf/2511.02401",
        "title": "Generalization in Representation Models via Random Matrix Theory: Application to Recurrent Networks",
        "authors": [
            "Yessin Moakher",
            "Malik Tiomoko",
            "Cosme Louart",
            "Zhenyu Liao"
        ],
        "comments": "",
        "subjects": "Statistics Theory (math.ST); Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "We first study the generalization error of models that use a fixed feature representation (frozen intermediate layers) followed by a trainable readout layer. This setting encompasses a range of architectures, from deep random-feature models to echo-state networks (ESNs) with recurrent dynamics. Working in the high-dimensional regime, we apply Random Matrix Theory to derive a closed-form expression for the asymptotic generalization error. We then apply this analysis to recurrent representations and obtain concise formula that characterize their performance. Surprisingly, we show that a linear ESN is equivalent to ridge regression with an exponentially time-weighted (''memory'') input covariance, revealing a clear inductive bias toward recent inputs. Experiments match predictions: ESNs win in low-sample, short-memory regimes, while ridge prevails with more data or long-range dependencies. Our methodology provides a general framework for analyzing overparameterized models and offers insights into the behavior of deep learning networks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 61,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04688",
        "abs_url": "https://arxiv.org/abs/2511.04688",
        "pdf_url": "https://arxiv.org/pdf/2511.04688",
        "title": "Evaluating LLMs' Reasoning Over Ordered Procedural Steps",
        "authors": [
            "Adrita Anika",
            "Md Messal Monem Miah"
        ],
        "comments": "Accepted to IJCNLP-AACL 2025 Findings",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Reasoning over procedural sequences, where the order of steps directly impacts outcomes, is a critical capability for large language models (LLMs). In this work, we study the task of reconstructing globally ordered sequences from shuffled procedural steps, using a curated dataset of food recipes, a domain where correct sequencing is essential for task success. We evaluate several LLMs under zero-shot and few-shot settings and present a comprehensive evaluation framework that adapts established metrics from ranking and sequence alignment. These include Kendall's Tau, Normalized Longest Common Subsequence (NLCS), and Normalized Edit Distance (NED), which capture complementary aspects of ordering quality. Our analysis shows that model performance declines with increasing sequence length, reflecting the added complexity of longer procedures. We also find that greater step displacement in the input, corresponding to more severe shuffling, leads to further degradation. These findings highlight the limitations of current LLMs in procedural reasoning, especially with longer and more disordered inputs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 62,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04702",
        "abs_url": "https://arxiv.org/abs/2511.04702",
        "pdf_url": "https://arxiv.org/pdf/2511.04702",
        "title": "Communication-Constrained Private Decentralized Online Personalized Mean Estimation",
        "authors": [
            "Yauhen Yakimenka",
            "Hsuan-Yin Lin",
            "Eirik Rosnes",
            "J√∂rg Kliewer"
        ],
        "comments": "Paper accepted for presentation at the 2025 IEEE Information Theory Workshop (ITW 2025). Final conference version",
        "subjects": "Social and Information Networks (cs.SI); Information Theory (cs.IT); Machine Learning (cs.LG)",
        "abstract": "We consider the problem of communication-constrained collaborative personalized mean estimation under a privacy constraint in an environment of several agents continuously receiving data according to arbitrary unknown agent-specific distributions. A consensus-based algorithm is studied under the framework of differential privacy in order to protect each agent's data. We give a theoretical convergence analysis of the proposed consensus-based algorithm for any bounded unknown distributions on the agents' data, showing that collaboration provides faster convergence than a fully local approach where agents do not share data, under an oracle decision rule and under some restrictions on the privacy level and the agents' connectivity, which illustrates the benefit of private collaboration in an online setting under a communication restriction on the agents. The theoretical faster-than-local convergence guarantee is backed up by several numerical results.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 63,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04711",
        "abs_url": "https://arxiv.org/abs/2511.04711",
        "pdf_url": "https://arxiv.org/pdf/2511.04711",
        "title": "SWAP: Towards Copyright Auditing of Soft Prompts via Sequential Watermarking",
        "authors": [
            "Wenyuan Yang",
            "Yichen Sun",
            "Changzheng Chen",
            "Zhixuan Chu",
            "Jiaheng Zhang",
            "Yiming Li",
            "Dacheng Tao"
        ],
        "comments": "The first two authors contributed equally to this work. 27 pages",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Large-scale vision-language models, especially CLIP, have demonstrated remarkable performance across diverse downstream tasks. Soft prompts, as carefully crafted modules that efficiently adapt vision-language models to specific tasks, necessitate effective copyright protection. In this paper, we investigate model copyright protection by auditing whether suspicious third-party models incorporate protected soft prompts. While this can be viewed as a special case of model ownership auditing, our analysis shows that existing techniques are ineffective due to prompt learning's unique characteristics. Non-intrusive auditing is inherently prone to false positives when independent models share similar data distributions with victim models. Intrusive approaches also fail: backdoor methods designed for CLIP cannot embed functional triggers, while extending traditional DNN backdoor techniques to prompt learning suffers from harmfulness and ambiguity challenges. We find that these failures in intrusive auditing stem from the same fundamental reason: watermarking operates within the same decision space as the primary task yet pursues opposing objectives. Motivated by these findings, we propose sequential watermarking for soft prompts (SWAP), which implants watermarks into a different and more complex space. SWAP encodes watermarks through a specific order of defender-specified out-of-distribution classes, inspired by the zero-shot prediction capability of CLIP. This watermark, which is embedded in a more complex space, keeps the original prediction label unchanged, making it less opposed to the primary task. We further design a hypothesis-test-guided verification protocol for SWAP and provide theoretical analyses of success conditions. Extensive experiments on 11 datasets demonstrate SWAP's effectiveness, harmlessness, and robustness against potential adaptive attacks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 64,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04727",
        "abs_url": "https://arxiv.org/abs/2511.04727",
        "pdf_url": "https://arxiv.org/pdf/2511.04727",
        "title": "IndicVisionBench: Benchmarking Cultural and Multilingual Understanding in VLMs",
        "authors": [
            "Ali Faraz",
            "Akash",
            "Shaharukh Khan",
            "Raja Kolla",
            "Akshat Patidar",
            "Suranjan Goswami",
            "Abhinav Ravi",
            "Chandra Khatri",
            "Shubham Agarwal"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Vision-language models (VLMs) have demonstrated impressive generalization across multimodal tasks, yet most evaluation benchmarks remain Western-centric, leaving open questions about their performance in culturally diverse and multilingual settings. To address this gap, we introduce IndicVisionBench, the first large-scale benchmark centered on the Indian subcontinent. Covering English and 10 Indian languages, our benchmark spans 3 multimodal tasks, including Optical Character Recognition (OCR), Multimodal Machine Translation (MMT), and Visual Question Answering (VQA), covering 6 kinds of question types. Our final benchmark consists of a total of ~5K images and 37K+ QA pairs across 13 culturally grounded topics. In addition, we release a paired parallel corpus of annotations across 10 Indic languages, creating a unique resource for analyzing cultural and linguistic biases in VLMs. We evaluate a broad spectrum of 8 models, from proprietary closed-source systems to open-weights medium and large-scale models. Our experiments reveal substantial performance gaps, underscoring the limitations of current VLMs in culturally diverse contexts. By centering cultural diversity and multilinguality, IndicVisionBench establishes a reproducible evaluation framework that paves the way for more inclusive multimodal research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 65,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04770",
        "abs_url": "https://arxiv.org/abs/2511.04770",
        "pdf_url": "https://arxiv.org/pdf/2511.04770",
        "title": "Machine Learning-Driven Analysis of kSZ Maps to Predict CMB Optical Depth $œÑ$",
        "authors": [
            "Farshid Farhadi Khouzani",
            "Abinash Kumar Shaw",
            "Paul La Plante",
            "Bryar Mustafa Shareef",
            "Laxmi Gewali"
        ],
        "comments": "12 pages, 5 figures, submitted to PASP",
        "subjects": "Cosmology and Nongalactic Astrophysics (astro-ph.CO); Machine Learning (cs.LG)",
        "abstract": "Upcoming measurements of the kinetic Sunyaev-Zel'dovich (kSZ) effect, which results from Cosmic Microwave Background (CMB) photons scattering off moving electrons, offer a powerful probe of the Epoch of Reionization (EoR). The kSZ signal contains key information about the timing, duration, and spatial structure of the EoR. A precise measurement of the CMB optical depth $\\tau$, a key parameter that characterizes the universe's integrated electron density, would significantly constrain models of early structure formation. However, the weak kSZ signal is difficult to extract from CMB observations due to significant contamination from astrophysical foregrounds. We present a machine learning approach to extract $\\tau$ from simulated kSZ maps. We train advanced machine learning models, including swin transformers, on high-resolution seminumeric simulations of the kSZ signal. To robustly quantify prediction uncertainties of $\\tau$, we employ the Laplace Approximation (LA). This approach provides an efficient and principled Gaussian approximation to the posterior distribution over the model's weights, allowing for reliable error estimation. We investigate and compare two distinct application modes: a post-hoc LA applied to a pre-trained model, and an online LA where model weights and hyperparameters are optimized jointly by maximizing the marginal likelihood. This approach provides a framework for robustly constraining $\\tau$ and its associated uncertainty, which can enhance the analysis of upcoming CMB surveys like the Simons Observatory and CMB-S4.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 66,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04792",
        "abs_url": "https://arxiv.org/abs/2511.04792",
        "pdf_url": "https://arxiv.org/pdf/2511.04792",
        "title": "Blind Strong Gravitational Lensing Inversion: Joint Inference of Source and Lens Mass with Score-Based Models",
        "authors": [
            "Gabriel Missael Barco",
            "Ronan Legin",
            "Connor Stone",
            "Yashar Hezaveh",
            "Laurence Perreault-Levasseur"
        ],
        "comments": "18 pages, 9 figures, 1 table. Accepted to the NeurIPS 2025 Workshop on Machine Learning and the Physical Sciences",
        "subjects": "Instrumentation and Methods for Astrophysics (astro-ph.IM); Cosmology and Nongalactic Astrophysics (astro-ph.CO); Machine Learning (cs.LG)",
        "abstract": "Score-based models can serve as expressive, data-driven priors for scientific inverse problems. In strong gravitational lensing, they enable posterior inference of a background galaxy from its distorted, multiply-imaged observation. Previous work, however, assumes that the lens mass distribution (and thus the forward operator) is known. We relax this assumption by jointly inferring the source and a parametric lens-mass profile, using a sampler based on GibbsDDRM but operating in continuous time. The resulting reconstructions yield residuals consistent with the observational noise, and the marginal posteriors of the lens parameters recover true values without systematic bias. To our knowledge, this is the first successful demonstration of joint source-and-lens inference with a score-based prior.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 67,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04803",
        "abs_url": "https://arxiv.org/abs/2511.04803",
        "pdf_url": "https://arxiv.org/pdf/2511.04803",
        "title": "Data Efficiency and Transfer Robustness in Biomedical Image Segmentation: A Study of Redundancy and Forgetting with Cellpose",
        "authors": [
            "Shuo Zhao",
            "Jianxu Chen"
        ],
        "comments": "Accepted to IEEE BIBM 2025 Workshop; 6 pages; 4 figures; 5 tables; IEEEtran class. Code: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Generalist biomedical image segmentation models such as Cellpose are increasingly applied across diverse imaging modalities and cell types. However, two critical challenges remain underexplored: (1) the extent of training data redundancy and (2) the impact of cross domain transfer on model retention. In this study, we conduct a systematic empirical analysis of these challenges using Cellpose as a case study. First, to assess data redundancy, we propose a simple dataset quantization (DQ) strategy for constructing compact yet diverse training subsets. Experiments on the Cyto dataset show that image segmentation performance saturates with only 10% of the data, revealing substantial redundancy and potential for training with minimal annotations. Latent space analysis using MAE embeddings and t-SNE confirms that DQ selected patches capture greater feature diversity than random sampling. Second, to examine catastrophic forgetting, we perform cross domain finetuning experiments and observe significant degradation in source domain performance, particularly when adapting from generalist to specialist domains. We demonstrate that selective DQ based replay reintroducing just 5-10% of the source data effectively restores source performance, while full replay can hinder target adaptation. Additionally, we find that training domain sequencing improves generalization and reduces forgetting in multi stage transfer. Our findings highlight the importance of data centric design in biomedical image segmentation and suggest that efficient training requires not only compact subsets but also retention aware learning strategies and informed domain ordering. The code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 68,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04812",
        "abs_url": "https://arxiv.org/abs/2511.04812",
        "pdf_url": "https://arxiv.org/pdf/2511.04812",
        "title": "Unified Multimodal Diffusion Forcing for Forceful Manipulation",
        "authors": [
            "Zixuan Huang",
            "Huaidian Hou",
            "Dmitry Berenson"
        ],
        "comments": "Project website: this https URL",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Given a dataset of expert trajectories, standard imitation learning approaches typically learn a direct mapping from observations (e.g., RGB images) to actions. However, such methods often overlook the rich interplay between different modalities, i.e., sensory inputs, actions, and rewards, which is crucial for modeling robot behavior and understanding task outcomes. In this work, we propose Multimodal Diffusion Forcing, a unified framework for learning from multimodal robot trajectories that extends beyond action generation. Rather than modeling a fixed distribution, MDF applies random partial masking and trains a diffusion model to reconstruct the trajectory. This training objective encourages the model to learn temporal and cross-modal dependencies, such as predicting the effects of actions on force signals or inferring states from partial observations. We evaluate MDF on contact-rich, forceful manipulation tasks in simulated and real-world environments. Our results show that MDF not only delivers versatile functionalities, but also achieves strong performance, and robustness under noisy observations. More visualizations can be found on our website this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 69,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04869",
        "abs_url": "https://arxiv.org/abs/2511.04869",
        "pdf_url": "https://arxiv.org/pdf/2511.04869",
        "title": "Trained on Tokens, Calibrated on Concepts: The Emergence of Semantic Calibration in LLMs",
        "authors": [
            "Preetum Nakkiran",
            "Arwen Bradley",
            "Adam Goli≈Ñski",
            "Eugene Ndiaye",
            "Michael Kirchhof",
            "Sinead Williamson"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Large Language Models (LLMs) often lack meaningful confidence estimates for their outputs. While base LLMs are known to exhibit next-token calibration, it remains unclear whether they can assess confidence in the actual meaning of their responses beyond the token level. We find that, when using a certain sampling-based notion of semantic calibration, base LLMs are remarkably well-calibrated: they can meaningfully assess confidence in open-domain question-answering tasks, despite not being explicitly trained to do so. Our main theoretical contribution establishes a mechanism for why semantic calibration emerges as a byproduct of next-token prediction, leveraging a recent connection between calibration and local loss optimality. The theory relies on a general definition of \"B-calibration,\" which is a notion of calibration parameterized by a choice of equivalence classes (semantic or otherwise). This theoretical mechanism leads to a testable prediction: base LLMs will be semantically calibrated when they can easily predict their own distribution over semantic answer classes before generating a response. We state three implications of this prediction, which we validate through experiments: (1) Base LLMs are semantically calibrated across question-answering tasks, (2) RL instruction-tuning systematically breaks this calibration, and (3) chain-of-thought reasoning breaks calibration. To our knowledge, our work provides the first principled explanation of when and why semantic calibration emerges in LLMs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 70,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04873",
        "abs_url": "https://arxiv.org/abs/2511.04873",
        "pdf_url": "https://arxiv.org/pdf/2511.04873",
        "title": "Prototype Selection Using Topological Data Analysis",
        "authors": [
            "Jordan Eckert",
            "Elvan Ceyhan",
            "Henry Schenck"
        ],
        "comments": "Code is found on this http URL",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Recently, there has been an explosion in statistical learning literature to represent data using topological principles to capture structure and relationships. We propose a topological data analysis (TDA)-based framework, named Topological Prototype Selector (TPS), for selecting representative subsets (prototypes) from large datasets. We demonstrate the effectiveness of TPS on simulated data under different data intrinsic characteristics, and compare TPS against other currently used prototype selection methods in real data settings. In all simulated and real data settings, TPS significantly preserves or improves classification performance while substantially reducing data size. These contributions advance both algorithmic and geometric aspects of prototype learning and offer practical tools for parallelized, interpretable, and efficient classification.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 71,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04970",
        "abs_url": "https://arxiv.org/abs/2511.04970",
        "pdf_url": "https://arxiv.org/pdf/2511.04970",
        "title": "Learning Fourier shapes to probe the geometric world of deep neural networks",
        "authors": [
            "Jian Wang",
            "Yixing Yong",
            "Haixia Bi",
            "Lijun He",
            "Fan Li"
        ],
        "comments": "20 pages, 5 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "While both shape and texture are fundamental to visual recognition, research on deep neural networks (DNNs) has predominantly focused on the latter, leaving their geometric understanding poorly probed. Here, we show: first, that optimized shapes can act as potent semantic carriers, generating high-confidence classifications from inputs defined purely by their geometry; second, that they are high-fidelity interpretability tools that precisely isolate a model's salient regions; and third, that they constitute a new, generalizable adversarial paradigm capable of deceiving downstream visual tasks. This is achieved through an end-to-end differentiable framework that unifies a powerful Fourier series to parameterize arbitrary shapes, a winding number-based mapping to translate them into the pixel grid required by DNNs, and signal energy constraints that enhance optimization efficiency while ensuring physically plausible shapes. Our work provides a versatile framework for probing the geometric world of DNNs and opens new frontiers for challenging and understanding machine perception.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 72,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.04983",
        "abs_url": "https://arxiv.org/abs/2511.04983",
        "pdf_url": "https://arxiv.org/pdf/2511.04983",
        "title": "Predicting Cognitive Assessment Scores in Older Adults with Cognitive Impairment Using Wearable Sensors",
        "authors": [
            "Assma Habadi",
            "Milos Zefran",
            "Lijuan Yin",
            "Woojin Song",
            "Maria Caceres",
            "Elise Hu",
            "Naoko Muramatsu"
        ],
        "comments": "40 pages, 2 figures, 3 tables; Supplementary Material: 3 tables (S1-S3). Presented as a poster at the Gerontological Society of America (GSA) Annual Scientific Meeting, November 2025",
        "subjects": "Neurons and Cognition (q-bio.NC); Machine Learning (cs.LG); Signal Processing (eess.SP)",
        "abstract": "Background and Objectives: This paper focuses on using AI to assess the cognitive function of older adults with mild cognitive impairment or mild dementia using physiological data provided by a wearable device. Cognitive screening tools are disruptive, time-consuming, and only capture brief snapshots of activity. Wearable sensors offer an attractive alternative by continuously monitoring physiological signals. This study investigated whether physiological data can accurately predict scores on established cognitive tests. Research Design and Methods: We recorded physiological signals from 23 older adults completing three NIH Toolbox Cognitive Battery tests, which assess working memory, processing speed, and attention. The Empatica EmbracePlus, a wearable device, measured blood volume pulse, skin conductance, temperature, and movement. Statistical features were extracted using wavelet-based and segmentation methods. We then applied supervised learning and validated predictions via cross-validation, hold-out testing, and bootstrapping. Results: Our models showed strong performance with Spearman's \\rho of 0.73-0.82 and mean absolute errors of 0.14-0.16, significantly outperforming a naive mean predictor. Sensor roles varied: heart-related signals combined with movement and temperature best predicted working memory, movement paired with skin conductance was most informative for processing speed, and heart in tandem with skin conductance worked best for attention. Discussion and Implications: These findings suggest that wearable sensors paired with AI tools such as supervised learning and feature engineering can noninvasively track specific cognitive functions in older adults, enabling continuous monitoring. Our study demonstrates how AI can be leveraged when the data sample is small. This approach may support remote assessments and facilitate clinical interventions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 73,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05050",
        "abs_url": "https://arxiv.org/abs/2511.05050",
        "pdf_url": "https://arxiv.org/pdf/2511.05050",
        "title": "Estimating Bidirectional Causal Effects with Large Scale Online Kernel Learning",
        "authors": [
            "Masahiro Tanaka"
        ],
        "comments": "Accepted for publication in Proceedings of the 2025 International Conference on Data Science and Intelligent Systems (DSIS 2025)",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Methodology (stat.ME)",
        "abstract": "In this study, a scalable online kernel learning framework is proposed for estimating bidirectional causal effects in systems characterized by mutual dependence and heteroskedasticity. Traditional causal inference often focuses on unidirectional effects, overlooking the common bidirectional relationships in real-world phenomena. Building on heteroskedasticity-based identification, the proposed method integrates a quasi-maximum likelihood estimator for simultaneous equation models with large scale online kernel learning. It employs random Fourier feature approximations to flexibly model nonlinear conditional means and variances, while an adaptive online gradient descent algorithm ensures computational efficiency for streaming and high-dimensional data. Results from extensive simulations demonstrate that the proposed method achieves superior accuracy and stability than single equation and polynomial approximation baselines, exhibiting lower bias and root mean squared error across various data-generating processes. These results confirm that the proposed approach effectively captures complex bidirectional causal effects with near-linear computational scaling. By combining econometric identification with modern machine learning techniques, the proposed framework offers a practical, scalable, and theoretically grounded solution for large scale causal inference in natural/social science, policy making, business, and industrial applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 74,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05085",
        "abs_url": "https://arxiv.org/abs/2511.05085",
        "pdf_url": "https://arxiv.org/pdf/2511.05085",
        "title": "Iterative Layer-wise Distillation for Efficient Compression of Large Language Models",
        "authors": [
            "Grigory Kovalev",
            "Mikhail Tikhomirov"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "This work investigates distillation methods for large language models (LLMs) with the goal of developing compact models that preserve high performance. Several existing approaches are reviewed, with a discussion of their respective strengths and limitations. An improved method based on the ShortGPT approach has been developed, building upon the idea of incorporating iterative evaluation of layer importance. At each step, importance is assessed by measuring performance degradation when individual layers are removed, using a set of representative datasets. This process is combined with further training using a joint loss function based on KL divergence and mean squared error. Experiments on the Qwen2.5-3B model show that the number of layers can be reduced from 36 to 28 (resulting in a 2.47 billion parameter model) with only a 9.7% quality loss, and to 24 layers with an 18% loss. The findings suggest that the middle transformer layers contribute less to inference, underscoring the potential of the proposed method for creating efficient models. The results demonstrate the effectiveness of iterative distillation and fine-tuning, making the approach suitable for deployment in resource-limited settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 75,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05156",
        "abs_url": "https://arxiv.org/abs/2511.05156",
        "pdf_url": "https://arxiv.org/pdf/2511.05156",
        "title": "SmartSecChain-SDN: A Blockchain-Integrated Intelligent Framework for Secure and Efficient Software-Defined Networks",
        "authors": [
            "Azhar Hussain Mozumder",
            "M. John Basha",
            "Chayapathi A. R"
        ],
        "comments": "20 pages, 12 figures",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)",
        "abstract": "With more and more existing networks being transformed to Software-Defined Networking (SDN), they need to be more secure and demand smarter ways of traffic control. This work, SmartSecChain-SDN, is a platform that combines machine learning based intrusion detection, blockchain-based storage of logs, and application-awareness-based priority in SDN networks. To detect network intrusions in a real-time, precision and low-false positives setup, the framework utilizes the application of advanced machine learning algorithms, namely Random Forest, XGBoost, CatBoost, and CNN-BiLSTM. SmartSecChain-SDN is based on the Hyperledger Fabric, which is a permissioned blockchain technology, to provide secure, scalable, and privacy-preserving storage and, thus, guarantee that the Intrusion Detection System (IDS) records cannot be altered and can be analyzed comprehensively. The system also has Quality of Service (QoS) rules and traffic shaping based on applications, which enables prioritization of critical services, such as VoIP, video conferencing, and business applications, as well as de-prioritization of non-essential traffic, such as downloads and updates. Mininet can simulate real-time SDN scenarios because it is used to prototype whole architectures. It is also compatible with controllers OpenDaylight and Ryu. It has tested the framework using the InSDN dataset and proved that it can identify different kinds of cyberattacks and handle bandwidth allocation efficiently under circumstances of resource constraints. SmartSecChain-SDN comprehensively addresses SDN system protection, securing and enhancing. The proposed study offers an innovative, extensible way to improve cybersecurity, regulatory compliance, and the administration of next-generation programmable networks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 76,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05158",
        "abs_url": "https://arxiv.org/abs/2511.05158",
        "pdf_url": "https://arxiv.org/pdf/2511.05158",
        "title": "Follow-Me in Micro-Mobility with End-to-End Imitation Learning",
        "authors": [
            "Sahar Salimpour",
            "Iacopo Catalano",
            "Tomi Westerlund",
            "Mohsen Falahi",
            "Jorge Pe√±a Queralta"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
        "abstract": "Autonomous micro-mobility platforms face challenges from the perspective of the typical deployment environment: large indoor spaces or urban areas that are potentially crowded and highly dynamic. While social navigation algorithms have progressed significantly, optimizing user comfort and overall user experience over other typical metrics in robotics (e.g., time or distance traveled) is understudied. Specifically, these metrics are critical in commercial applications. In this paper, we show how imitation learning delivers smoother and overall better controllers, versus previously used manually-tuned controllers. We demonstrate how DAAV's autonomous wheelchair achieves state-of-the-art comfort in follow-me mode, in which it follows a human operator assisting persons with reduced mobility (PRM). This paper analyzes different neural network architectures for end-to-end control and demonstrates their usability in real-world production-level deployments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 77,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05159",
        "abs_url": "https://arxiv.org/abs/2511.05159",
        "pdf_url": "https://arxiv.org/pdf/2511.05159",
        "title": "A New Framework for Convex Clustering in Kernel Spaces: Finite Sample Bounds, Consistency and Performance Insights",
        "authors": [
            "Shubhayan Pan",
            "Saptarshi Chakraborty",
            "Debolina Paul",
            "Kushal Bose",
            "Swagatam Das"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Convex clustering is a well-regarded clustering method, resembling the similar centroid-based approach of Lloyd's $k$-means, without requiring a predefined cluster count. It starts with each data point as its centroid and iteratively merges them. Despite its advantages, this method can fail when dealing with data exhibiting linearly non-separable or non-convex structures. To mitigate the limitations, we propose a kernelized extension of the convex clustering method. This approach projects the data points into a Reproducing Kernel Hilbert Space (RKHS) using a feature map, enabling convex clustering in this transformed space. This kernelization not only allows for better handling of complex data distributions but also produces an embedding in a finite-dimensional vector space. We provide a comprehensive theoretical underpinnings for our kernelized approach, proving algorithmic convergence and establishing finite sample bounds for our estimates. The effectiveness of our method is demonstrated through extensive experiments on both synthetic and real-world datasets, showing superior performance compared to state-of-the-art clustering techniques. This work marks a significant advancement in the field, offering an effective solution for clustering in non-linear and non-convex data scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 78,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05231",
        "abs_url": "https://arxiv.org/abs/2511.05231",
        "pdf_url": "https://arxiv.org/pdf/2511.05231",
        "title": "A differentiable model of supply-chain shocks",
        "authors": [
            "Saad Hamid",
            "Jos√© Moran",
            "Luca Mungo",
            "Arnau Quera-Bofarull",
            "Sebastian Towers"
        ],
        "comments": "Accepted to 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Differentiable Systems and Scientific Machine Learning (EurIPS)",
        "subjects": "Physics and Society (physics.soc-ph); Machine Learning (cs.LG); Multiagent Systems (cs.MA)",
        "abstract": "Modelling how shocks propagate in supply chains is an increasingly important challenge in economics. Its relevance has been highlighted in recent years by events such as Covid-19 and the Russian invasion of Ukraine. Agent-based models (ABMs) are a promising approach for this problem. However, calibrating them is hard. We show empirically that it is possible to achieve speed ups of over 3 orders of magnitude when calibrating ABMs of supply networks by running them on GPUs and using automatic differentiation, compared to non-differentiable baselines. This opens the door to scaling ABMs to model the whole global supply network.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 79,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05234",
        "abs_url": "https://arxiv.org/abs/2511.05234",
        "pdf_url": "https://arxiv.org/pdf/2511.05234",
        "title": "Context-aware Learned Mesh-based Simulation via Trajectory-Level Meta-Learning",
        "authors": [
            "Philipp Dahlinger",
            "Niklas Freymuth",
            "Tai Hoang",
            "Tobias W√ºrth",
            "Michael Volpp",
            "Luise K√§rger",
            "Gerhard Neumann"
        ],
        "comments": "35 pages. Submitted to Transactions on Machine Learning Research (TMLR)",
        "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
        "abstract": "Simulating object deformations is a critical challenge across many scientific domains, including robotics, manufacturing, and structural mechanics. Learned Graph Network Simulators (GNSs) offer a promising alternative to traditional mesh-based physics simulators. Their speed and inherent differentiability make them particularly well suited for applications that require fast and accurate simulations, such as robotic manipulation or manufacturing optimization. However, existing learned simulators typically rely on single-step observations, which limits their ability to exploit temporal context. Without this information, these models fail to infer, e.g., material properties. Further, they rely on auto-regressive rollouts, which quickly accumulate error for long trajectories. We instead frame mesh-based simulation as a trajectory-level meta-learning problem. Using Conditional Neural Processes, our method enables rapid adaptation to new simulation scenarios from limited initial data while capturing their latent simulation properties. We utilize movement primitives to directly predict fast, stable and accurate simulations from a single model call. The resulting approach, Movement-primitive Meta-MeshGraphNet (M3GN), provides higher simulation accuracy at a fraction of the runtime cost compared to state-of-the-art GNSs across several tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 80,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05275",
        "abs_url": "https://arxiv.org/abs/2511.05275",
        "pdf_url": "https://arxiv.org/pdf/2511.05275",
        "title": "TwinVLA: Data-Efficient Bimanual Manipulation with Twin Single-Arm Vision-Language-Action Models",
        "authors": [
            "Hokyun Im",
            "Euijin Jeong",
            "Jianlong Fu",
            "Andrey Kolobov",
            "Youngwoon Lee"
        ],
        "comments": "Project webpage : this https URL",
        "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
        "abstract": "Vision-language-action models (VLAs) trained on large-scale robotic datasets have demonstrated strong performance on manipulation tasks, including bimanual tasks. However, because most public datasets focus on single-arm demonstrations, adapting VLAs for bimanual tasks typically requires substantial additional bimanual data and fine-tuning. To address this challenge, we introduce TwinVLA, a modular framework that composes two copies of a pretrained single-arm VLA into a coordinated bimanual VLA. Unlike monolithic cross-embodiment models trained on mixtures of single-arm and bimanual data, TwinVLA improves both data efficiency and performance by composing pretrained single-arm policies. Across diverse bimanual tasks in real-world and simulation settings, TwinVLA outperforms a comparably-sized monolithic RDT-1B model without requiring any bimanual pretraining. Furthermore, it narrows the gap to state-of-the-art model, $\\pi_0$ which rely on extensive proprietary bimanual data and compute cost. These results establish our modular composition approach as a data-efficient and scalable path toward high-performance bimanual manipulation, leveraging public single-arm data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 81,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05295",
        "abs_url": "https://arxiv.org/abs/2511.05295",
        "pdf_url": "https://arxiv.org/pdf/2511.05295",
        "title": "Language Generation and Identification From Partial Enumeration: Tight Density Bounds and Topological Characterizations",
        "authors": [
            "Jon Kleinberg",
            "Fan Wei"
        ],
        "comments": "",
        "subjects": "Data Structures and Algorithms (cs.DS); Computation and Language (cs.CL); Discrete Mathematics (cs.DM); Machine Learning (cs.LG)",
        "abstract": "The success of large language models (LLMs) has motivated formal theories of language generation and learning. We study the framework of \\emph{language generation in the limit}, where an adversary enumerates strings from an unknown language $K$ drawn from a countable class, and an algorithm must generate unseen strings from $K$. Prior work showed that generation is always possible, and that some algorithms achieve positive lower density, revealing a \\emph{validity--breadth} trade-off between correctness and coverage. We resolve a main open question in this line, proving a tight bound of $1/2$ on the best achievable lower density. We then strengthen the model to allow \\emph{partial enumeration}, where the adversary reveals only an infinite subset $C \\subseteq K$. We show that generation in the limit remains achievable, and if $C$ has lower density $\\alpha$ in $K$, the algorithm's output achieves density at least $\\alpha/2$, matching the upper bound. This generalizes the $1/2$ bound to the partial-information setting, where the generator must recover within a factor $1/2$ of the revealed subset's density. We further revisit the classical Gold--Angluin model of \\emph{language identification} under partial enumeration. We characterize when identification in the limit is possible -- when hypotheses $M_t$ eventually satisfy $C \\subseteq M \\subseteq K$ -- and in the process give a new topological formulation of Angluin's characterization, showing that her condition is precisely equivalent to an appropriate topological space having the $T_D$ separation property.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 82,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05297",
        "abs_url": "https://arxiv.org/abs/2511.05297",
        "pdf_url": "https://arxiv.org/pdf/2511.05297",
        "title": "Building Specialized Software-Assistant ChatBot with Graph-Based Retrieval-Augmented Generation",
        "authors": [
            "Mohammed Hilel",
            "Yannis Karmim",
            "Jean De Bodinat",
            "Reda Sarehane",
            "Antoine Gillon"
        ],
        "comments": "",
        "subjects": "Software Engineering (cs.SE); Machine Learning (cs.LG)",
        "abstract": "Digital Adoption Platforms (DAPs) have become essential tools for helping employees navigate complex enterprise software such as CRM, ERP, or HRMS systems. Companies like LemonLearning have shown how digital guidance can reduce training costs and accelerate onboarding. However, building and maintaining these interactive guides still requires extensive manual effort. Leveraging Large Language Models as virtual assistants is an appealing alternative, yet without a structured understanding of the target software, LLMs often hallucinate and produce unreliable answers. Moreover, most production-grade LLMs are black-box APIs, making fine-tuning impractical due to the lack of access to model weights. In this work, we introduce a Graph-based Retrieval-Augmented Generation framework that automatically converts enterprise web applications into state-action knowledge graphs, enabling LLMs to generate grounded and context-aware assistance. The framework was co-developed with the AI enterprise RAKAM, in collaboration with Lemon Learning. We detail the engineering pipeline that extracts and structures software interfaces, the design of the graph-based retrieval process, and the integration of our approach into production DAP workflows. Finally, we discuss scalability, robustness, and deployment lessons learned from industrial use cases.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 83,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05301",
        "abs_url": "https://arxiv.org/abs/2511.05301",
        "pdf_url": "https://arxiv.org/pdf/2511.05301",
        "title": "QUESTER: Query Specification for Generative Retrieval",
        "authors": [
            "Arthur Satouf",
            "Yuxuan Zong",
            "Habiboulaye Amadou-Boubacar",
            "Pablo Piantanida",
            "Benjamin Piwowarski"
        ],
        "comments": "",
        "subjects": "Information Retrieval (cs.IR); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Generative Retrieval (GR) differs from the traditional index-then-retrieve pipeline by storing relevance in model parameters and directly generating document identifiers. However, GR often struggles to generalize and is costly to scale. We introduce QUESTER (QUEry SpecificaTion gEnerative Retrieval), which reframes GR as query specification generation - in this work, a simple keyword query handled by BM25 - using a (small) LLM. The policy is trained using reinforcement learning techniques (GRPO). Across in- and out-of-domain evaluations, we show that our model is more effective than BM25, and competitive with neural IR models, while maintaining a good efficiency",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 84,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05311",
        "abs_url": "https://arxiv.org/abs/2511.05311",
        "pdf_url": "https://arxiv.org/pdf/2511.05311",
        "title": "Cleaning Maintenance Logs with LLM Agents for Improved Predictive Maintenance",
        "authors": [
            "Valeriu Dimidov",
            "Faisal Hawlader",
            "Sasan Jafarnejad",
            "Rapha√´l Frank"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO); Software Engineering (cs.SE)",
        "abstract": "Economic constraints, limited availability of datasets for reproducibility and shortages of specialized expertise have long been recognized as key challenges to the adoption and advancement of predictive maintenance (PdM) in the automotive sector. Recent progress in large language models (LLMs) presents an opportunity to overcome these barriers and speed up the transition of PdM from research to industrial practice. Under these conditions, we explore the potential of LLM-based agents to support PdM cleaning pipelines. Specifically, we focus on maintenance logs, a critical data source for training well-performing machine learning (ML) models, but one often affected by errors such as typos, missing fields, near-duplicate entries, and incorrect dates. We evaluate LLM agents on cleaning tasks involving six distinct types of noise. Our findings show that LLMs are effective at handling generic cleaning tasks and offer a promising foundation for future industrial applications. While domain-specific errors remain challenging, these results highlight the potential for further improvements through specialized training and enhanced agentic capabilities.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 85,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05408",
        "abs_url": "https://arxiv.org/abs/2511.05408",
        "pdf_url": "https://arxiv.org/pdf/2511.05408",
        "title": "Steering Language Models with Weight Arithmetic",
        "authors": [
            "Constanza Fierro",
            "Fabien Roger"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Providing high-quality feedback to Large Language Models (LLMs) on a diverse training distribution can be difficult and expensive, and providing feedback only on a narrow distribution can result in unintended generalizations. To better leverage narrow training data, we propose contrastive weight steering, a simple post-training method that edits the model parameters using weight arithmetic. We isolate a behavior direction in weight-space by subtracting the weight deltas from two small fine-tunes -- one that induces the desired behavior and another that induces its opposite -- and then add or remove this direction to modify the model's weights. We apply this technique to mitigate sycophancy and induce misalignment, and find that weight steering often generalizes further than activation steering, achieving stronger out-of-distribution behavioral control before degrading general capabilities. We also show that, in the context of task-specific fine-tuning, weight steering can partially mitigate undesired behavioral drift: it can reduce sycophancy and under-refusals introduced during fine-tuning while preserving task performance gains. Finally, we provide preliminary evidence that emergent misalignment can be detected by measuring the similarity between fine-tuning updates and an \"evil\" weight direction, suggesting that it may be possible to monitor the evolution of weights during training and detect rare misaligned behaviors that never manifest during training or evaluations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 86,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05452",
        "abs_url": "https://arxiv.org/abs/2511.05452",
        "pdf_url": "https://arxiv.org/pdf/2511.05452",
        "title": "Self-adaptive weighting and sampling for physics-informed neural networks",
        "authors": [
            "Wenqian Chen",
            "Amanda Howard",
            "Panos Stinis"
        ],
        "comments": "11 figures",
        "subjects": "Machine Learning (stat.ML); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Computational Physics (physics.comp-ph)",
        "abstract": "Physics-informed deep learning has emerged as a promising framework for solving partial differential equations (PDEs). Nevertheless, training these models on complex problems remains challenging, often leading to limited accuracy and efficiency. In this work, we introduce a hybrid adaptive sampling and weighting method to enhance the performance of physics-informed neural networks (PINNs). The adaptive sampling component identifies training points in regions where the solution exhibits rapid variation, while the adaptive weighting component balances the convergence rate across training points. Numerical experiments show that applying only adaptive sampling or only adaptive weighting is insufficient to consistently achieve accurate predictions, particularly when training points are scarce. Since each method emphasizes different aspects of the solution, their effectiveness is problem dependent. By combining both strategies, the proposed framework consistently improves prediction accuracy and training efficiency, offering a more robust approach for solving PDEs with PINNs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 87,
        "date": "2025-11-10",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-10?abs=True",
        "arxiv_id": "2511.05476",
        "abs_url": "https://arxiv.org/abs/2511.05476",
        "pdf_url": "https://arxiv.org/pdf/2511.05476",
        "title": "A Metamorphic Testing Perspective on Knowledge Distillation for Language Models of Code: Does the Student Deeply Mimic the Teacher?",
        "authors": [
            "Md. Abdul Awal",
            "Mrigank Rochan",
            "Chanchal K. Roy"
        ],
        "comments": "The paper is currently under review at a peer-reviewed journal",
        "subjects": "Software Engineering (cs.SE); Machine Learning (cs.LG)",
        "abstract": "Transformer-based language models of code have achieved state-of-the-art performance across a wide range of software analytics tasks, but their practical deployment remains limited due to high computational costs, slow inference speeds, and significant environmental impact. To address these challenges, recent research has increasingly explored knowledge distillation as a method for compressing a large language model of code (the teacher) into a smaller model (the student) while maintaining performance. However, the degree to which a student model deeply mimics the predictive behavior and internal representations of its teacher remains largely unexplored, as current accuracy-based evaluation provides only a surface-level view of model quality and often fails to capture more profound discrepancies in behavioral fidelity between the teacher and student models. To address this gap, we empirically show that the student model often fails to deeply mimic the teacher model, resulting in up to 285% greater performance drop under adversarial attacks, which is not captured by traditional accuracy-based evaluation. Therefore, we propose MetaCompress, a metamorphic testing framework that systematically evaluates behavioral fidelity by comparing the outputs of teacher and student models under a set of behavior-preserving metamorphic relations. We evaluate MetaCompress on two widely studied tasks, using compressed versions of popular language models of code, obtained via three different knowledge distillation techniques: Compressor, AVATAR, and MORPH. The results show that MetaCompress identifies up to 62% behavioral discrepancies in student models, underscoring the need for behavioral fidelity evaluation within the knowledge distillation pipeline and establishing MetaCompress as a practical framework for testing compressed language models of code derived through knowledge distillation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    }
]