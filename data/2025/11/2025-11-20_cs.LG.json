[
    {
        "order": 1,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.14813",
        "abs_url": "https://arxiv.org/abs/2511.14813",
        "pdf_url": "https://arxiv.org/pdf/2511.14813",
        "title": "DEVAL: A Framework for Evaluating and Improving the Derivation Capability of Large Language Models",
        "authors": [
            "Yifan Li",
            "Qin Li",
            "Min Zhang",
            "Min Zhang",
            "Peixin Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Assessing the reasoning ability of Large Language Models (LLMs) over data remains an open and pressing research question. Compared with LLMs, human reasoning can derive corresponding modifications to the output based on certain kinds of changes to the input. This reasoning pattern, which relies on abstract rules that govern relationships between changes of data, has not been comprehensively described or evaluated in LLMs. In this paper, we formally define this reasoning pattern as the Derivation Relation (DR) and introduce the concept of Derivation Capability (DC), i.e. applying DR by making the corresponding modification to the output whenever the input takes certain changes. To assess DC, a systematically constructed evaluation framework named DEVAL is proposed and used to evaluate five popular LLMs and one Large Reasoning Model in seven mainstream tasks. The evaluation results show that mainstream LLMs, such as GPT-4o and Claude3.5, exhibit moderate DR recognition capabilities but reveal significant drop-offs on applying DR effectively in problem-solving scenarios. To improve this, we propose a novel prompt engineering approach called Derivation Prompting (DP). It achieves an average improvement of 15.2% in DC for all tested LLMs, outperforming commonly used prompt engineering techniques.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 2,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.14846",
        "abs_url": "https://arxiv.org/abs/2511.14846",
        "pdf_url": "https://arxiv.org/pdf/2511.14846",
        "title": "Empowering Multi-Turn Tool-Integrated Reasoning with Group Turn Policy Optimization",
        "authors": [
            "Yifeng Ding",
            "Hung Le",
            "Songyang Han",
            "Kangrui Ruan",
            "Zhenghui Jin",
            "Varun Kumar",
            "Zijian Wang",
            "Anoop Deoras"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Training Large Language Models (LLMs) for multi-turn Tool-Integrated Reasoning (TIR) - where models iteratively reason, generate code, and verify through execution - remains challenging for existing reinforcement learning (RL) approaches. Current RL methods, exemplified by Group Relative Policy Optimization (GRPO), suffer from coarse-grained, trajectory-level rewards that provide insufficient learning signals for complex multi-turn interactions, leading to training stagnation. To address this issue, we propose Group Turn Policy Optimization (GTPO), a novel RL algorithm specifically designed for training LLMs on multi-turn TIR tasks. GTPO introduces three key innovations: (1) turn-level reward assignment that provides fine-grained feedback for individual turns, (2) return-based advantage estimation where normalized discounted returns are calculated as advantages, and (3) self-supervised reward shaping that exploits self-supervision signals from generated code to densify sparse binary outcome-based rewards. Our comprehensive evaluation demonstrates that GTPO outperforms GRPO by 3.0% on average across diverse reasoning benchmarks, establishing its effectiveness for advancing complex mathematical reasoning in the real world.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 3,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.14865",
        "abs_url": "https://arxiv.org/abs/2511.14865",
        "pdf_url": "https://arxiv.org/pdf/2511.14865",
        "title": "FinTRec: Transformer Based Unified Contextual Ads Targeting and Personalization for Financial Applications",
        "authors": [
            "Dwipam Katariya",
            "Snehita Varma",
            "Akshat Shreemali",
            "Benjamin Wu",
            "Kalanand Mishra",
            "Pranab Mohanty"
        ],
        "comments": "10 pages, 7 figures, Accepted at CARS @ RecSys 2025",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Transformer-based architectures are widely adopted in sequential recommendation systems, yet their application in Financial Services (FS) presents distinct practical and modeling challenges for real-time recommendation. These include:a) long-range user interactions (implicit and explicit) spanning both digital and physical channels generating temporally heterogeneous context, b) the presence of multiple interrelated products require coordinated models to support varied ad placements and personalized feeds, while balancing competing business goals. We propose FinTRec, a transformer-based framework that addresses these challenges and its operational objectives in FS. While tree-based models have traditionally been preferred in FS due to their explainability and alignment with regulatory requirements, our study demonstrate that FinTRec offers a viable and effective shift toward transformer-based architectures. Through historic simulation and live A/B test correlations, we show FinTRec consistently outperforms the production-grade tree-based baseline. The unified architecture, when fine-tuned for product adaptation, enables cross-product signal sharing, reduces training cost and technical debt, while improving offline performance across all products. To our knowledge, this is the first comprehensive study of unified sequential recommendation modeling in FS that addresses both technical and business considerations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 4,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.14887",
        "abs_url": "https://arxiv.org/abs/2511.14887",
        "pdf_url": "https://arxiv.org/pdf/2511.14887",
        "title": "Transformer-Guided Deep Reinforcement Learning for Optimal Takeoff Trajectory Design of an eVTOL Drone",
        "authors": [
            "Nathan M. Roberts II",
            "Xiaosong Du"
        ],
        "comments": "Conference version with 12 pages and 2 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The rapid advancement of electric vertical take-off and landing (eVTOL) aircraft offers a promising opportunity to alleviate urban traffic congestion. Thus, developing optimal takeoff trajectories for minimum energy consumption becomes essential for broader eVTOL aircraft applications. Conventional optimal control methods (such as dynamic programming and linear quadratic regulator) provide highly efficient and well-established solutions but are limited by problem dimensionality and complexity. Deep reinforcement learning (DRL) emerges as a special type of artificial intelligence tackling complex, nonlinear systems; however, the training difficulty is a key bottleneck that limits DRL applications. To address these challenges, we propose the transformer-guided DRL to alleviate the training difficulty by exploring a realistic state space at each time step using a transformer. The proposed transformer-guided DRL was demonstrated on an optimal takeoff trajectory design of an eVTOL drone for minimal energy consumption while meeting takeoff conditions (i.e., minimum vertical displacement and minimum horizontal velocity) by varying control variables (i.e., power and wing angle to the vertical). Results presented that the transformer-guided DRL agent learned to take off with $4.57\\times10^6$ time steps, representing 25% of the $19.79\\times10^6$ time steps needed by a vanilla DRL agent. In addition, the transformer-guided DRL achieved 97.2% accuracy on the optimal energy consumption compared against the simulation-based optimal reference while the vanilla DRL achieved 96.3% accuracy. Therefore, the proposed transformer-guided DRL outperformed vanilla DRL in terms of both training efficiency as well as optimal design verification.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 5,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.14889",
        "abs_url": "https://arxiv.org/abs/2511.14889",
        "pdf_url": "https://arxiv.org/pdf/2511.14889",
        "title": "Bringing Federated Learning to Space",
        "authors": [
            "Grace Kim",
            "Filip Svoboda",
            "Nicholas Lane"
        ],
        "comments": "15 pages, 9 figures, 3 tables accepted to IEEE Aeroconf 2026",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "As Low Earth Orbit (LEO) satellite constellations rapidly expand to hundreds and thousands of spacecraft, the need for distributed on-board machine learning becomes critical to address downlink bandwidth limitations. Federated learning (FL) offers a promising framework to conduct collaborative model training across satellite networks. Realizing its benefits in space naturally requires addressing space-specific constraints, from intermittent connectivity to dynamics imposed by orbital motion. This work presents the first systematic feasibility analysis of adapting off-the-shelf FL algorithms for satellite constellation deployment. We introduce a comprehensive \"space-ification\" framework that adapts terrestrial algorithms (FedAvg, FedProx, FedBuff) to operate under orbital constraints, producing an orbital-ready suite of FL algorithms. We then evaluate these space-ified methods through extensive parameter sweeps across 768 constellation configurations that vary cluster sizes (1-10), satellites per cluster (1-10), and ground station networks (1-13). Our analysis demonstrates that space-adapted FL algorithms efficiently scale to constellations of up to 100 satellites, achieving performance close to the centralized ideal. Multi-month training cycles can be reduced to days, corresponding to a 9x speedup through orbital scheduling and local coordination within satellite clusters. These results provide actionable insights for future mission designers, enabling distributed on-board learning for more autonomous, resilient, and data-driven satellite operations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 6,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.14903",
        "abs_url": "https://arxiv.org/abs/2511.14903",
        "pdf_url": "https://arxiv.org/pdf/2511.14903",
        "title": "It's LIT! Reliability-Optimized LLMs with Inspectable Tools",
        "authors": [
            "Ruixin Zhang",
            "Jon Donnelly",
            "Zhicheng Guo",
            "Ghazal Khalighinejad",
            "Haiyang Huang",
            "Alina Jade Barnett",
            "Cynthia Rudin"
        ],
        "comments": "Accepted to the 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop on Multi-Turn Interactions in Large Language Models",
        "subjects": "Machine Learning (cs.LG); Software Engineering (cs.SE)",
        "abstract": "Large language models (LLMs) have exhibited remarkable capabilities across various domains. The ability to call external tools further expands their capability to handle real-world tasks. However, LLMs often follow an opaque reasoning process, which limits their usefulness in high-stakes domains where solutions need to be trustworthy to end users. LLMs can choose solutions that are unreliable and difficult to troubleshoot, even if better options are available. We address this issue by forcing LLMs to use external -- more reliable -- tools to solve problems when possible. We present a framework built on the tool-calling capabilities of existing LLMs to enable them to select the most reliable and easy-to-troubleshoot solution path, which may involve multiple sequential tool calls. We refer to this framework as LIT (LLMs with Inspectable Tools). In order to support LIT, we introduce a new and challenging benchmark dataset of 1,300 questions and a customizable set of reliability cost functions associated with a collection of specialized tools. These cost functions summarize how reliable each tool is and how easy it is to troubleshoot. For instance, a calculator is reliable across domains, whereas a linear prediction model is not reliable if there is distribution shift, but it is easy to troubleshoot. A tool that constructs a random forest is neither reliable nor easy to troubleshoot. These tools interact with the Harvard USPTO Patent Dataset and a new dataset of NeurIPS 2023 papers to solve mathematical, coding, and modeling problems of varying difficulty levels. We demonstrate that LLMs can achieve more reliable and informed problem-solving while maintaining task performance using our framework.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 7,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.14920",
        "abs_url": "https://arxiv.org/abs/2511.14920",
        "pdf_url": "https://arxiv.org/pdf/2511.14920",
        "title": "Structured Contrastive Learning for Interpretable Latent Representations",
        "authors": [
            "Zhengyang Shen",
            "Hua Tu",
            "Mayue Shi"
        ],
        "comments": "Comments: 10 pages, 6 figures. Applications to medical signal retrieval and activity recognition. Correspondence: m.shi16@imperial.this http URL",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Neural networks exhibit severe brittleness to semantically irrelevant transformations. A mere 75ms electrocardiogram (ECG) phase shift degrades latent cosine similarity from 1.0 to 0.2, while sensor rotations collapse activity recognition performance with inertial measurement units (IMUs). We identify the root cause as \"laissez-faire\" representation learning, where latent spaces evolve unconstrained provided task performance is satisfied. We propose Structured Contrastive Learning (SCL), a framework that partitions latent space representations into three semantic groups: invariant features that remain consistent under given transformations (e.g., phase shifts or rotations), variant features that actively differentiate transformations via a novel variant mechanism, and free features that preserve task flexibility. This creates controllable push-pull dynamics where different latent dimensions serve distinct, interpretable purposes. The variant mechanism enhances contrastive learning by encouraging variant features to differentiate within positive pairs, enabling simultaneous robustness and interpretability. Our approach requires no architectural modifications and integrates seamlessly into existing training pipelines. Experiments on ECG phase invariance and IMU rotation robustness demonstrate superior performance: ECG similarity improves from 0.25 to 0.91 under phase shifts, while WISDM activity recognition achieves 86.65% accuracy with 95.38% rotation consistency, consistently outperforming traditional data augmentation. This work represents a paradigm shift from reactive data augmentation to proactive structural learning, enabling interpretable latent representations in neural networks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 8,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.14922",
        "abs_url": "https://arxiv.org/abs/2511.14922",
        "pdf_url": "https://arxiv.org/pdf/2511.14922",
        "title": "Integrating Causal Inference with Graph Neural Networks for Alzheimer's Disease Analysis",
        "authors": [
            "Pranay Kumar Peddi",
            "Dhrubajyoti Ghosh"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Methodology (stat.ME)",
        "abstract": "Deep graph learning has advanced Alzheimer's (AD) disease classification from MRI, but most models remain correlational, confounding demographic and genetic factors with disease specific features. We present Causal-GCN, an interventional graph convolutional framework that integrates do-calculus-based back-door adjustment to identify brain regions exerting stable causal influence on AD progression. Each subject's MRI is represented as a structural connectome where nodes denote cortical and subcortical regions and edges encode anatomical connectivity. Confounders such as age, sec, and APOE4 genotype are summarized via principal components and included in the causal adjustment set. After training, interventions on individual regions are simulated by serving their incoming edges and altering node features to estimate average causal effects on disease probability. Applied to 484 subjects from the ADNI cohort, Causal-GCN achieves performance comparable to baseline GNNs while providing interpretable causal effect rankings that highlight posterior, cingulate, and insular hubs consistent with established AD neuropathology.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 9,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.14936",
        "abs_url": "https://arxiv.org/abs/2511.14936",
        "pdf_url": "https://arxiv.org/pdf/2511.14936",
        "title": "How to Train Private Clinical Language Models: A Comparative Study of Privacy-Preserving Pipelines for ICD-9 Coding",
        "authors": [
            "Mathieu Dufour",
            "Andrew Duncan"
        ],
        "comments": "10 pages, 5 figures. Accepted to the Privacy-Preserving Machine Learning Workshop at EurIPS 2025",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
        "abstract": "Large language models trained on clinical text risk exposing sensitive patient information, yet differential privacy (DP) methods often severely degrade the diagnostic accuracy needed for deployment. Despite rapid progress in DP optimisation and text generation, it remains unclear which privacy-preserving strategy actually works best for clinical language tasks. We present the first systematic head-to-head comparison of four training pipelines for automated diagnostic coding from hospital discharge summaries. All pipelines use identical 1B-parameter models and matched privacy budgets to predict ICD-9 codes. At moderate and relaxed privacy budgets ($\\varepsilon \\in \\{4, 6\\}$), knowledge distillation from DP-trained teachers outperforms both direct DP-SGD and DP-synthetic data training, recovering up to 63\\% of the non-private performance whilst maintaining strong empirical privacy (membership-inference AUC $\\approx$ 0.5). These findings expose large differences in the privacy-utility trade-off across architectures and identify knowledge distillation as the most practical route to privacy-preserving clinical NLP.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 10,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15004",
        "abs_url": "https://arxiv.org/abs/2511.15004",
        "pdf_url": "https://arxiv.org/pdf/2511.15004",
        "title": "IonCast: A Deep Learning Framework for Forecasting Ionospheric Dynamics",
        "authors": [
            "Halil S. Kelebek",
            "Linnea M. Wolniewicz",
            "Michael D. Vergalla",
            "Simone Mestici",
            "Giacomo Acciarini",
            "Bala Poduval",
            "Olga Verkhoglyadova",
            "Madhulika Guhathakurta",
            "Thomas E. Berger",
            "Frank Soboczenski",
            "Atılım Güneş Baydin"
        ],
        "comments": "11 pages, 7 figures, 3 tables. Accepted as a poster presentation at the Machine Learning for the Physical Sciences Workshop at NeurIPS 2025",
        "subjects": "Machine Learning (cs.LG); Earth and Planetary Astrophysics (astro-ph.EP)",
        "abstract": "The ionosphere is a critical component of near-Earth space, shaping GNSS accuracy, high-frequency communications, and aviation operations. For these reasons, accurate forecasting and modeling of ionospheric variability has become increasingly relevant. To address this gap, we present IonCast, a suite of deep learning models that include a GraphCast-inspired model tailored for ionospheric dynamics. IonCast leverages spatiotemporal learning to forecast global Total Electron Content (TEC), integrating diverse physical drivers and observational datasets. Validating on held-out storm-time and quiet conditions highlights improved skill compared to persistence. By unifying heterogeneous data with scalable graph-based spatiotemporal learning, IonCast demonstrates how machine learning can augment physical understanding of ionospheric variability and advance operational space weather resilience.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 11,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15048",
        "abs_url": "https://arxiv.org/abs/2511.15048",
        "pdf_url": "https://arxiv.org/pdf/2511.15048",
        "title": "Oversampling techniques for predicting COVID-19 patient length of stay",
        "authors": [
            "Zachariah Farahany",
            "Jiawei Wu",
            "K M Sajjadul Islam",
            "Praveen Madiraju"
        ],
        "comments": "10 pages, 2022 IEEE International Conference on Big Data (Big Data)",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "COVID-19 is a respiratory disease that caused a global pandemic in 2019. It is highly infectious and has the following symptoms: fever or chills, cough, shortness of breath, fatigue, muscle or body aches, headache, the new loss of taste or smell, sore throat, congestion or runny nose, nausea or vomiting, and diarrhea. These symptoms vary in severity; some people with many risk factors have been known to have lengthy hospital stays or die from the disease. In this paper, we analyze patients' electronic health records (EHR) to predict the severity of their COVID-19 infection using the length of stay (LOS) as our measurement of severity. This is an imbalanced classification problem, as many people have a shorter LOS rather than a longer one. To combat this problem, we synthetically create alternate oversampled training data sets. Once we have this oversampled data, we run it through an Artificial Neural Network (ANN), which during training has its hyperparameters tuned using Bayesian optimization. We select the model with the best F1 score and then evaluate it and discuss it.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 12,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15062",
        "abs_url": "https://arxiv.org/abs/2511.15062",
        "pdf_url": "https://arxiv.org/pdf/2511.15062",
        "title": "Interpretable temporal fusion network of multi- and multi-class arrhythmia classification",
        "authors": [
            "Yun Kwan Kim"
        ],
        "comments": "[Doctoral dissertation, Korea University, 2025]",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Clinical decision support systems (CDSSs) have been widely utilized to support the decisions made by cardiologists when detecting and classifying arrhythmia from electrocardiograms. However, forming a CDSS for the arrhythmia classification task is challenging due to the varying lengths of arrhythmias. Although the onset time of arrhythmia varies, previously developed methods have not considered such conditions. Thus, we propose a framework that consists of (i) local and global extraction and (ii) local-global information fusion with attention to enable arrhythmia detection and classification within a constrained input length. The framework's performance was evaluated in terms of 10-class and 4-class arrhythmia detection, focusing on identifying the onset and ending point of arrhythmia episodes and their duration using the MIT-BIH arrhythmia database (MITDB) and the MIT-BIH atrial fibrillation database (AFDB). Duration, episode, and Dice score performances resulted in overall F1-scores of 96.45%, 82.05%, and 96.31% on the MITDB and 97.57%, 98.31%, and 97.45% on the AFDB, respectively. The results demonstrated statistically superior performance compared to those of the benchmark models. To assess the generalization capability of the proposed method, an MITDB-trained model and MIT-BIH malignant ventricular arrhythmia database-trained model were tested AFDB and MITDB, respectively. Superior performance was attained compared with that of a state-of-the-art model. The proposed method effectively captures both local and global information and dynamics without significant information loss. Consequently, arrhythmias can be detected with greater accuracy, and their occurrence times can be precisely determined, enabling the clinical field to develop more accurate treatment plans based on the proposed method.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 13,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15083",
        "abs_url": "https://arxiv.org/abs/2511.15083",
        "pdf_url": "https://arxiv.org/pdf/2511.15083",
        "title": "Fourier-KAN-Mamba: A Novel State-Space Equation Approach for Time-Series Anomaly Detection",
        "authors": [
            "Xiancheng Wang",
            "Lin Wang",
            "Rui Wang",
            "Zhibo Zhang",
            "Minghang Zhao"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Signal Processing (eess.SP)",
        "abstract": "Time-series anomaly detection plays a critical role in numerous real-world applications, including industrial monitoring and fault diagnosis. Recently, Mamba-based state-space models have shown remarkable efficiency in long-sequence modeling. However, directly applying Mamba to anomaly detection tasks still faces challenges in capturing complex temporal patterns and nonlinear dynamics. In this paper, we propose Fourier-KAN-Mamba, a novel hybrid architecture that integrates Fourier layer, Kolmogorov-Arnold Networks (KAN), and Mamba selective state-space model. The Fourier layer extracts multi-scale frequency features, KAN enhances nonlinear representation capability, and a temporal gating control mechanism further improves the model's ability to distinguish normal and anomalous patterns. Extensive experiments on MSL, SMAP, and SWaT datasets demonstrate that our method significantly outperforms existing state-of-the-art approaches. Keywords: time-series anomaly detection, state-space model, Mamba, Fourier transform, Kolmogorov-Arnold Network",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 14,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15125",
        "abs_url": "https://arxiv.org/abs/2511.15125",
        "pdf_url": "https://arxiv.org/pdf/2511.15125",
        "title": "Efficient RF Passive Components Modeling with Bayesian Online Learning and Uncertainty Aware Sampling",
        "authors": [
            "Huifan Zhang",
            "Pingqiang Zhou"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Conventional radio frequency (RF) passive components modeling based on machine learning requires extensive electromagnetic (EM) simulations to cover geometric and frequency design spaces, creating computational bottlenecks. In this paper, we introduce an uncertainty-aware Bayesian online learning framework for efficient parametric modeling of RF passive components, which includes: 1) a Bayesian neural network with reconfigurable heads for joint geometric-frequency domain modeling while quantifying uncertainty; 2) an adaptive sampling strategy that simultaneously optimizes training data sampling across geometric parameters and frequency domain using uncertainty guidance. Validated on three RF passive components, the framework achieves accurate modeling while using only 2.86% EM simulation time compared to traditional ML-based flow, achieving a 35 times speedup.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 15,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15136",
        "abs_url": "https://arxiv.org/abs/2511.15136",
        "pdf_url": "https://arxiv.org/pdf/2511.15136",
        "title": "Novel sparse matrix algorithm expands the feasible size of a self-organizing map of the knowledge indexed by a database of peer-reviewed medical literature",
        "authors": [
            "Andrew Amos",
            "Joanne Lee",
            "Tarun Sen Gupta",
            "Bunmi S. Malau-Aduli"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Past efforts to map the Medline database have been limited to small subsets of the available data because of the exponentially increasing memory and processing demands of existing algorithms. We designed a novel algorithm for sparse matrix multiplication that allowed us to apply a self-organizing map to the entire Medline dataset, allowing for a more complete map of existing medical knowledge. The algorithm also increases the feasibility of refining the self-organizing map to account for changes in the dataset over time.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 16,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15138",
        "abs_url": "https://arxiv.org/abs/2511.15138",
        "pdf_url": "https://arxiv.org/pdf/2511.15138",
        "title": "Cross-Modal Consistency-Guided Active Learning for Affective BCI Systems",
        "authors": [
            "Hyo-Jeong Jang",
            "Hye-Bin Shin",
            "Kang Yin"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Human-Computer Interaction (cs.HC)",
        "abstract": "Deep learning models perform best with abundant, high-quality labels, yet such conditions are rarely achievable in EEG-based emotion recognition. Electroencephalogram (EEG) signals are easily corrupted by artifacts and individual variability, while emotional labels often stem from subjective and inconsistent reports-making robust affective decoding particularly difficult. We propose an uncertainty-aware active learning framework that enhances robustness to label noise by jointly leveraging model uncertainty and cross-modal consistency. Instead of relying solely on EEG-based uncertainty estimates, the method evaluates cross-modal alignment to determine whether uncertainty originates from cognitive ambiguity or sensor noise. A representation alignment module embeds EEG and face features into a shared latent space, enforcing semantic coherence between modalities. Residual discrepancies are treated as noise-induced inconsistencies, and these samples are selectively queried for oracle feedback during active learning. This feedback-driven process guides the network toward reliable, informative samples and reduces the impact of noisy labels. Experiments on the ASCERTAIN dataset examine the efficiency and robustness of ours, highlighting its potential as a data-efficient and noise-tolerant approach for EEG-based affective decoding in brain-computer interface systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 17,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15172",
        "abs_url": "https://arxiv.org/abs/2511.15172",
        "pdf_url": "https://arxiv.org/pdf/2511.15172",
        "title": "Complex variational autoencoders admit Kähler structure",
        "authors": [
            "Andrew Gracyk"
        ],
        "comments": "First version",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "It has been discovered that latent-Euclidean variational autoencoders (VAEs) admit, in various capacities, Riemannian structure. We adapt these arguments but for complex VAEs with a complex latent stage. We show that complex VAEs reveal to some level Kähler geometric structure. Our methods will be tailored for decoder geometry. We derive the Fisher information metric in the complex case under a latent complex Gaussian regularization with trivial relation matrix. It is well known from statistical information theory that the Fisher information coincides with the Hessian of the Kullback-Leibler (KL) divergence. Thus, the metric Kähler potential relation is exactly achieved under relative entropy. We propose a Kähler potential derivative of complex Gaussian mixtures that has rough equivalence to the Fisher information metric while still being faithful to the underlying Kähler geometry. Computation of the metric via this potential is efficient, and through our potential, valid as a plurisubharmonic (PSH) function, large scale computational burden of automatic differentiation is displaced to small scale. We show that we can regularize the latent space with decoder geometry, and that we can sample in accordance with a weighted complex volume element. We demonstrate these strategies, at the exchange of sample variation, yield consistently smoother representations and fewer semantic outliers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 18,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15175",
        "abs_url": "https://arxiv.org/abs/2511.15175",
        "pdf_url": "https://arxiv.org/pdf/2511.15175",
        "title": "Vehicle Routing Problems via Quantum Graph Attention Network Deep Reinforcement Learning",
        "authors": [
            "Le Tung Giang",
            "Vu Hoang Viet",
            "Nguyen Xuan Tung",
            "Trinh Van Chien",
            "Won-Joo Hwang"
        ],
        "comments": "11 pages, 3 figures, 2 tables. Accepted by SOICT 2025",
        "subjects": "Machine Learning (cs.LG); Information Theory (cs.IT); Quantum Physics (quant-ph)",
        "abstract": "The vehicle routing problem (VRP) is a fundamental NP-hard task in intelligent transportation systems with broad applications in logistics and distribution. Deep reinforcement learning (DRL) with Graph Neural Networks (GNNs) has shown promise, yet classical models rely on large multi-layer perceptrons (MLPs) that are parameter-heavy and memory-bound. We propose a Quantum Graph Attention Network (Q-GAT) within a DRL framework, where parameterized quantum circuits (PQCs) replace conventional MLPs at critical readout stages. The hybrid model maintains the expressive capacity of graph attention encoders while reducing trainable parameters by more than 50%. Using proximal policy optimization (PPO) with greedy and stochastic decoding, experiments on VRP benchmarks show that Q-GAT achieves faster convergence and reduces routing cost by about 5% compared with classical GAT baselines. These results demonstrate the potential of PQC-enhanced GNNs as compact and effective solvers for large-scale routing and logistics optimization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 19,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15208",
        "abs_url": "https://arxiv.org/abs/2511.15208",
        "pdf_url": "https://arxiv.org/pdf/2511.15208",
        "title": "Reasoning in Diffusion Large Language Models is Concentrated in Dynamic Confusion Zones",
        "authors": [
            "Ranfei Chen",
            "Ming Chen",
            "Kaifei Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Diffusion Large Language Models (dLLMs) are rapidly emerging alongside autoregressive models as a powerful paradigm for complex reasoning, with reinforcement learning increasingly used for downstream alignment. Existing trajectory-based RL methods uniformly allocate policy gradients across denoising steps, implicitly treating all steps as equally important. We challenge this assumption by analyzing trajectories with several step-level metrics: entropy-based uncertainty, Confidence-Margin (CM) uncertainty, and Rate of Entropy Change (RoEC). These reveal structured \"zones of confusion\": transient spikes in uncertainty and instability that strongly predict final success or failure, while most steps remain stable. We propose Adaptive Trajectory Policy Optimization (ATPO), a lightweight step-selection strategy that dynamically reallocates gradient updates to these high-leverage steps without changing the RL objective, rewards, or compute budget. Using a hybrid RoEC+CM rule, ATPO delivers substantial gains in reasoning accuracy and training stability across benchmarks, showing that exploiting trajectory dynamics is key to advancing dLLM RL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 20,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15246",
        "abs_url": "https://arxiv.org/abs/2511.15246",
        "pdf_url": "https://arxiv.org/pdf/2511.15246",
        "title": "D2D Power Allocation via Quantum Graph Neural Network",
        "authors": [
            "Tung Giang Le",
            "Xuan Tung Nguyen",
            "Won-Joo Hwang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Increasing wireless network complexity demands scalable resource management. Classical GNNs excel at graph learning but incur high computational costs in large-scale settings. We present a fully quantum Graph Neural Network (QGNN) that implements message passing via Parameterized Quantum Circuits (PQCs). Our Quantum Graph Convolutional Layers (QGCLs) encode features into quantum states, process graphs with NISQ-compatible unitaries, and retrieve embeddings through measurement. Applied to D2D power control for SINR maximization, our QGNN matches classical performance with fewer parameters and inherent parallelism. This end-to-end PQC-based GNN marks a step toward quantum-accelerated wireless optimization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 21,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15250",
        "abs_url": "https://arxiv.org/abs/2511.15250",
        "pdf_url": "https://arxiv.org/pdf/2511.15250",
        "title": "Optimized scheduling of electricity-heat cooperative system considering wind energy consumption and peak shaving and valley filling",
        "authors": [
            "Jin Ye",
            "Lingmei Wang",
            "Shujian Zhang",
            "Haihang WU"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "With the global energy transition and rapid development of renewable energy, the scheduling optimization challenge for combined power-heat systems under new energy integration and multiple uncertainties has become increasingly prominent. Addressing this challenge, this study proposes an intelligent scheduling method based on the improved Dual-Delay Deep Deterministic Policy Gradient (PVTD3) algorithm. System optimization is achieved by introducing a penalty term for grid power purchase variations. Simulation results demonstrate that under three typical scenarios (10%, 20%, and 30% renewable penetration), the PVTD3 algorithm reduces the system's comprehensive cost by 6.93%, 12.68%, and 13.59% respectively compared to the traditional TD3 algorithm. Concurrently, it reduces the average fluctuation amplitude of grid power purchases by 12.8%. Regarding energy storage management, the PVTD3 algorithm reduces the end-time state values of low-temperature thermal storage tanks by 7.67-17.67 units while maintaining high-temperature tanks within the 3.59-4.25 safety operating range. Multi-scenario comparative validation demonstrates that the proposed algorithm not only excels in economic efficiency and grid stability but also exhibits superior sustainable scheduling capabilities in energy storage device management.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 22,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15251",
        "abs_url": "https://arxiv.org/abs/2511.15251",
        "pdf_url": "https://arxiv.org/pdf/2511.15251",
        "title": "PLATONT: Learning a Platonic Representation for Unified Network Tomography",
        "authors": [
            "Chengze Du",
            "Heng Xu",
            "Zhiwei Yu",
            "Bo Liu",
            "Jialong Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)",
        "abstract": "Network tomography aims to infer hidden network states, such as link performance, traffic load, and topology, from external observations. Most existing methods solve these problems separately and depend on limited task-specific signals, which limits generalization and interpretability. We present PLATONT, a unified framework that models different network indicators (e.g., delay, loss, bandwidth) as projections of a shared latent network state. Guided by the Platonic Representation Hypothesis, PLATONT learns this latent state through multimodal alignment and contrastive learning. By training multiple tomography tasks within a shared latent space, it builds compact and structured representations that improve cross-task generalization. Experiments on synthetic and real-world datasets show that PLATONT consistently outperforms existing methods in link estimation, topology inference, and traffic prediction, achieving higher accuracy and stronger robustness under varying network conditions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 23,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15276",
        "abs_url": "https://arxiv.org/abs/2511.15276",
        "pdf_url": "https://arxiv.org/pdf/2511.15276",
        "title": "SNAP: Low-Latency Test-Time Adaptation with Sparse Updates",
        "authors": [
            "Hyeongheon Cha",
            "Dong Min Kim",
            "Hye Won Chung",
            "Taesik Gong",
            "Sung-Ju Lee"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Test-Time Adaptation (TTA) adjusts models using unlabeled test data to handle dynamic distribution shifts. However, existing methods rely on frequent adaptation and high computational cost, making them unsuitable for resource-constrained edge environments. To address this, we propose SNAP, a sparse TTA framework that reduces adaptation frequency and data usage while preserving accuracy. SNAP maintains competitive accuracy even when adapting based on only 1% of the incoming data stream, demonstrating its robustness under infrequent updates. Our method introduces two key components: (i) Class and Domain Representative Memory (CnDRM), which identifies and stores a small set of samples that are representative of both class and domain characteristics to support efficient adaptation with limited data; and (ii) Inference-only Batch-aware Memory Normalization (IoBMN), which dynamically adjusts normalization statistics at inference time by leveraging these representative samples, enabling efficient alignment to shifting target domains. Integrated with five state-of-the-art TTA algorithms, SNAP reduces latency by up to 93.12%, while keeping the accuracy drop below 3.3%, even across adaptation rates ranging from 1% to 50%. This demonstrates its strong potential for practical use on edge devices serving latency-sensitive applications. The source code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 24,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15300",
        "abs_url": "https://arxiv.org/abs/2511.15300",
        "pdf_url": "https://arxiv.org/pdf/2511.15300",
        "title": "Quant-Trim in Practice: Improved Cross-Platform Low-Bit Deployment on Edge NPUs",
        "authors": [
            "Rayen Dhahri",
            "Steffen Urban"
        ],
        "comments": "Accepted to a Eurips 2025 workshop, work in progress",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Specialized edge accelerators rely on low-bit quantization, but vendor compilers differ in scaling, clipping, and kernel support, often as black boxes. The same floating-point (FP) checkpoint can therefore yield inconsistent accuracy across backends, forcing practitioners to tweak flags or refactor models to vendor-friendly operator subsets. We introduce Quant-Trim, a training-phase method that produces a hardware-neutral checkpoint robust to backend and precision choices. It combines progressive fake quantization to align training with the deployed integer grid and reverse pruning to tame outlier-driven scale inflation while preserving learnability. Quant-Trim is agnostic to quantization schemes (symmetric/asymmetric,per-tensor/per-channel, INT8/INT4) and requires no vendor-specific graph this http URL models and tasks, it narrows the FP,low-bit gap, reduces dependence on compiler heuristics/calibration, and avoids per-backend retraining. We report accuracy and edge metrics latency, throughput, energy/inference, and cost under static/dynamic activation scaling and varying operator coverage.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 25,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15324",
        "abs_url": "https://arxiv.org/abs/2511.15324",
        "pdf_url": "https://arxiv.org/pdf/2511.15324",
        "title": "On the Internal Semantics of Time-Series Foundation Models",
        "authors": [
            "Atharva Pandey",
            "Abhilash Neog",
            "Gautam Jajoo"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Time-series Foundation Models (TSFMs) have recently emerged as a universal paradigm for learning across diverse temporal domains. However, despite their empirical success, the internal mechanisms by which these models represent fundamental time-series concepts remain poorly understood. In this work, we undertake a systematic investigation of concept interpretability in TSFMs. Specifically, we examine: (i) which layers encode which concepts, (ii) whether concept parameters are linearly recoverable, (iii) how representations evolve in terms of concept disentanglement and abstraction across model depth, and (iv) how models process compositions of concepts. We systematically probe these questions using layer-wise analyses, linear recoverability tests, and representation similarity measures, providing a structured account of TSFM semantics. The resulting insights show that early layers mainly capture local, time-domain patterns (e.g., AR(1), level shifts, trends), while deeper layers encode dispersion and change-time signals, with spectral and warping factors remaining the hardest to recover linearly. In compositional settings, however, probe performance degrades, revealing interference between concepts. This highlights that while atomic concepts are reliably localized, composition remains a challenge, underscoring a key limitation in current TSFMs' ability to represent interacting temporal phenomena.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 26,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15327",
        "abs_url": "https://arxiv.org/abs/2511.15327",
        "pdf_url": "https://arxiv.org/pdf/2511.15327",
        "title": "KrawtchoukNet: A Unified GNN Solution for Heterophily and Over-smoothing with Adaptive Bounded Polynomials",
        "authors": [
            "Huseyin Goksu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Signal Processing (eess.SP)",
        "abstract": "Spectral Graph Neural Networks (GNNs) based on polynomial filters, such as ChebyNet, suffer from two critical limitations: 1) performance collapse on \"heterophilic\" graphs and 2) performance collapse at high polynomial degrees (K), known as over-smoothing. Both issues stem from the static, low-pass nature of standard filters. In this work, we propose `KrawtchoukNet`, a GNN filter based on the discrete Krawtchouk polynomials. We demonstrate that `KrawtchoukNet` provides a unified solution to both problems through two key design choices. First, by fixing the polynomial's domain N to a small constant (e.g., N=20), we create the first GNN filter whose recurrence coefficients are \\textit{inherently bounded}, making it exceptionally robust to over-smoothing (achieving SOTA results at K=10). Second, by making the filter's shape parameter p learnable, the filter adapts its spectral response to the graph data. We show this adaptive nature allows `KrawtchoukNet` to achieve SOTA performance on challenging heterophilic benchmarks (Texas, Cornell), decisively outperforming standard GNNs like GAT and APPNP.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 27,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15328",
        "abs_url": "https://arxiv.org/abs/2511.15328",
        "pdf_url": "https://arxiv.org/pdf/2511.15328",
        "title": "LaguerreNet: Advancing a Unified Solution for Heterophily and Over-smoothing with Adaptive Continuous Polynomials",
        "authors": [
            "Huseyin Goksu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Signal Processing (eess.SP)",
        "abstract": "Spectral Graph Neural Networks (GNNs) suffer from two critical limitations: poor performance on \"heterophilic\" graphs and performance collapse at high polynomial degrees (K), known as over-smoothing. Both issues stem from the static, low-pass nature of standard filters (e.g., ChebyNet). While adaptive polynomial filters, such as the discrete MeixnerNet, have emerged as a potential unified solution, their extension to the continuous domain and stability with unbounded coefficients remain open questions. In this work, we propose `LaguerreNet`, a novel GNN filter based on continuous Laguerre polynomials. `LaguerreNet` learns the filter's spectral shape by making its core alpha parameter trainable, thereby advancing the adaptive polynomial approach. We solve the severe O(k^2) numerical instability of these unbounded polynomials using a `LayerNorm`-based stabilization technique. We demonstrate experimentally that this approach is highly effective: 1) `LaguerreNet` achieves state-of-the-art results on challenging heterophilic benchmarks. 2) It is exceptionally robust to over-smoothing, with performance peaking at K=10, an order of magnitude beyond where ChebyNet collapses.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 28,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15350",
        "abs_url": "https://arxiv.org/abs/2511.15350",
        "pdf_url": "https://arxiv.org/pdf/2511.15350",
        "title": "Multi-layer Stack Ensembles for Time Series Forecasting",
        "authors": [
            "Nathanael Bosch",
            "Oleksandr Shchur",
            "Nick Erickson",
            "Michael Bohlke-Schneider",
            "Caner Türkmen"
        ],
        "comments": "Published at AutoML Conference 2025 Methods Track",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Ensembling is a powerful technique for improving the accuracy of machine learning models, with methods like stacking achieving strong results in tabular tasks. In time series forecasting, however, ensemble methods remain underutilized, with simple linear combinations still considered state-of-the-art. In this paper, we systematically explore ensembling strategies for time series forecasting. We evaluate 33 ensemble models -- both existing and novel -- across 50 real-world datasets. Our results show that stacking consistently improves accuracy, though no single stacker performs best across all tasks. To address this, we propose a multi-layer stacking framework for time series forecasting, an approach that combines the strengths of different stacker models. We demonstrate that this method consistently provides superior accuracy across diverse forecasting scenarios. Our findings highlight the potential of stacking-based methods to improve AutoML systems for time series forecasting.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 29,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15357",
        "abs_url": "https://arxiv.org/abs/2511.15357",
        "pdf_url": "https://arxiv.org/pdf/2511.15357",
        "title": "Cost-Aware Prediction (CAP): An LLM-Enhanced Machine Learning Pipeline and Decision Support System for Heart Failure Mortality Prediction",
        "authors": [
            "Yinan Yu",
            "Falk Dippel",
            "Christina E. Lundberg",
            "Martin Lindgren",
            "Annika Rosengren",
            "Martin Adiels",
            "Helen Sjöland"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Objective: Machine learning (ML) predictive models are often developed without considering downstream value trade-offs and clinical interpretability. This paper introduces a cost-aware prediction (CAP) framework that combines cost-benefit analysis assisted by large language model (LLM) agents to communicate the trade-offs involved in applying ML predictions. Materials and Methods: We developed an ML model predicting 1-year mortality in patients with heart failure (N = 30,021, 22% mortality) to identify those eligible for home care. We then introduced clinical impact projection (CIP) curves to visualize important cost dimensions - quality of life and healthcare provider expenses, further divided into treatment and error costs, to assess the clinical consequences of predictions. Finally, we used four LLM agents to generate patient-specific descriptions. The system was evaluated by clinicians for its decision support value. Results: The eXtreme gradient boosting (XGB) model achieved the best performance, with an area under the receiver operating characteristic curve (AUROC) of 0.804 (95% confidence interval (CI) 0.792-0.816), area under the precision-recall curve (AUPRC) of 0.529 (95% CI 0.502-0.558) and a Brier score of 0.135 (95% CI 0.130-0.140). Discussion: The CIP cost curves provided a population-level overview of cost composition across decision thresholds, whereas LLM-generated cost-benefit analysis at individual patient-levels. The system was well received according to the evaluation by clinicians. However, feedback emphasizes the need to strengthen the technical accuracy for speculative tasks. Conclusion: CAP utilizes LLM agents to integrate ML classifier outcomes and cost-benefit analysis for more transparent and interpretable decision support.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 30,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15371",
        "abs_url": "https://arxiv.org/abs/2511.15371",
        "pdf_url": "https://arxiv.org/pdf/2511.15371",
        "title": "CID: Measuring Feature Importance Through Counterfactual Distributions",
        "authors": [
            "Eddie Conti",
            "Álvaro Parafita",
            "Axel Brando"
        ],
        "comments": "Accepted at Northern Lights Deep Learning (NLDL) 2026 Conference",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Assessing the importance of individual features in Machine Learning is critical to understand the model's decision-making process. While numerous methods exist, the lack of a definitive ground truth for comparison highlights the need for alternative, well-founded measures. This paper introduces a novel post-hoc local feature importance method called Counterfactual Importance Distribution (CID). We generate two sets of positive and negative counterfactuals, model their distributions using Kernel Density Estimation, and rank features based on a distributional dissimilarity measure. This measure, grounded in a rigorous mathematical framework, satisfies key properties required to function as a valid metric. We showcase the effectiveness of our method by comparing with well-established local feature importance explainers. Our method not only offers complementary perspectives to existing approaches, but also improves performance on faithfulness metrics (both for comprehensiveness and sufficiency), resulting in more faithful explanations of the system. These results highlight its potential as a valuable tool for model analysis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 31,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15393",
        "abs_url": "https://arxiv.org/abs/2511.15393",
        "pdf_url": "https://arxiv.org/pdf/2511.15393",
        "title": "EVA-Net: Interpretable Brain Age Prediction via Continuous Aging Prototypes from EEG",
        "authors": [
            "Kunyu Zhang",
            "Mingxuan Wang",
            "Xiangjie Shi",
            "Haoxing Xu",
            "Chao Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The brain age is a key indicator of brain health. While electroencephalography (EEG) is a practical tool for this task, existing models struggle with the common challenge of imperfect medical data, such as learning a ``normal'' baseline from weakly supervised, healthy-only cohorts. This is a critical anomaly detection task for identifying disease, but standard models are often black boxes lacking an interpretable structure. We propose EVA-Net, a novel framework that recasts brain age as an interpretable anomaly detection problem. EVA-Net uses an efficient, sparsified-attention Transformer to model long EEG sequences. To handle noise and variability in imperfect data, it employs a Variational Information Bottleneck to learn a robust, compressed representation. For interpretability, this representation is aligned to a continuous prototype network that explicitly learns the normative healthy aging manifold. Trained on 1297 healthy subjects, EVA-Net achieves state-of-the-art accuracy. We validated its anomaly detection capabilities on an unseen cohort of 27 MCI and AD patients. This pathological group showed significantly higher brain-age gaps and a novel Prototype Alignment Error, confirming their deviation from the healthy manifold. EVA-Net provides an interpretable framework for healthcare intelligence using imperfect medical data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 32,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15409",
        "abs_url": "https://arxiv.org/abs/2511.15409",
        "pdf_url": "https://arxiv.org/pdf/2511.15409",
        "title": "Proximal Approximate Inference in State-Space Models",
        "authors": [
            "Hany Abdulsamad",
            "Ángel F. García-Fernández",
            "Simo Särkkä"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Methodology (stat.ME)",
        "abstract": "We present a class of algorithms for state estimation in nonlinear, non-Gaussian state-space models. Our approach is based on a variational Lagrangian formulation that casts Bayesian inference as a sequence of entropic trust-region updates subject to dynamic constraints. This framework gives rise to a family of forward-backward algorithms, whose structure is determined by the chosen factorization of the variational posterior. By focusing on Gauss--Markov approximations, we derive recursive schemes with favorable computational complexity. For general nonlinear, non-Gaussian models we close the recursions using generalized statistical linear regression and Fourier--Hermite moment matching.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 33,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15447",
        "abs_url": "https://arxiv.org/abs/2511.15447",
        "pdf_url": "https://arxiv.org/pdf/2511.15447",
        "title": "TSFM in-context learning for time-series classification of bearing-health status",
        "authors": [
            "Michel Tokic",
            "Slobodan Djukanović",
            "Anja von Beuningen",
            "Cheng Feng"
        ],
        "comments": "Preprint submitted to ESANN 2026",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "This paper introduces a classification method using in-context learning in time-series foundation models (TSFM). We show how data, which was not part of the TSFM training data corpus, can be classified without the need of finetuning the model. Examples are represented in the form of targets (class id) and covariates (data matrix) within the prompt of the model, which enables to classify an unknown covariate data pattern alongside the forecast axis through in-context learning. We apply this method to vibration data for assessing the health state of a bearing within a servo-press motor. The method transforms frequency domain reference signals into pseudo time-series patterns, generates aligned covariate and target signals, and uses the TSFM to predict probabilities how classified data corresponds to predefined labels. Leveraging the scalability of pre-trained models this method demonstrates efficacy across varied operational conditions. This marks significant progress beyond custom narrow AI solutions towards broader, AI-driven maintenance systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 34,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15454",
        "abs_url": "https://arxiv.org/abs/2511.15454",
        "pdf_url": "https://arxiv.org/pdf/2511.15454",
        "title": "FairEnergy: Contribution-Based Fairness meets Energy Efficiency in Federated Learning",
        "authors": [
            "Ouiame Marnissi",
            "Hajar EL Hammouti",
            "El Houcine Bergou"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Federated learning (FL) enables collaborative model training across distributed devices while preserving data privacy. However, balancing energy efficiency and fair participation while ensuring high model accuracy remains challenging in wireless edge systems due to heterogeneous resources, unequal client contributions, and limited communication capacity. To address these challenges, we propose FairEnergy, a fairness-aware energy minimization framework that integrates a contribution score capturing both the magnitude of updates and their compression ratio into the joint optimization of device selection, bandwidth allocation, and compression level. The resulting mixed-integer non-convex problem is solved by relaxing binary selection variables and applying Lagrangian decomposition to handle global bandwidth coupling, followed by per-device subproblem optimization. Experiments on non-IID data show that FairEnergy achieves higher accuracy while reducing energy consumption by up to 79\\% compared to baseline strategies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 35,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15507",
        "abs_url": "https://arxiv.org/abs/2511.15507",
        "pdf_url": "https://arxiv.org/pdf/2511.15507",
        "title": "Sample-Adaptivity Tradeoff in On-Demand Sampling",
        "authors": [
            "Nika Haghtalab",
            "Omar Montasser",
            "Mingda Qiao"
        ],
        "comments": "50 pages, to appear at NeurIPS 2025",
        "subjects": "Machine Learning (cs.LG); Data Structures and Algorithms (cs.DS); Machine Learning (stat.ML)",
        "abstract": "We study the tradeoff between sample complexity and round complexity in on-demand sampling, where the learning algorithm adaptively samples from $k$ distributions over a limited number of rounds. In the realizable setting of Multi-Distribution Learning (MDL), we show that the optimal sample complexity of an $r$-round algorithm scales approximately as $dk^{\\Theta(1/r)} / \\epsilon$. For the general agnostic case, we present an algorithm that achieves near-optimal sample complexity of $\\widetilde O((d + k) / \\epsilon^2)$ within $\\widetilde O(\\sqrt{k})$ rounds. Of independent interest, we introduce a new framework, Optimization via On-Demand Sampling (OODS), which abstracts the sample-adaptivity tradeoff and captures most existing MDL algorithms. We establish nearly tight bounds on the round complexity in the OODS setting. The upper bounds directly yield the $\\widetilde O(\\sqrt{k})$-round algorithm for agnostic MDL, while the lower bounds imply that achieving sub-polynomial round complexity would require fundamentally new techniques that bypass the inherent hardness of OODS.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 36,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15522",
        "abs_url": "https://arxiv.org/abs/2511.15522",
        "pdf_url": "https://arxiv.org/pdf/2511.15522",
        "title": "PCARNN-DCBF: Minimal-Intervention Geofence Enforcement for Ground Vehicles",
        "authors": [
            "Yinan Yu",
            "Samuel Scheidegger"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Robotics (cs.RO)",
        "abstract": "Runtime geofencing for ground vehicles is rapidly emerging as a critical technology for enforcing Operational Design Domains (ODDs). However, existing solutions struggle to reconcile high-fidelity learning with the structural requirements of verifiable control. We address this by introducing PCARNN-DCBF, a novel pipeline integrating a Physics-encoded Control-Affine Residual Neural Network with a preview-based Discrete Control Barrier Function. Unlike generic learned models, PCARNN explicitly preserves the control-affine structure of vehicle dynamics, ensuring the linearity required for reliable optimization. This enables the DCBF to enforce polygonal keep-in constraints via a real-time Quadratic Program (QP) that handles high relative degree and mitigates actuator saturation. Experiments in CARLA across electric and combustion platforms demonstrate that this structure-preserving approach significantly outperforms analytical and unstructured neural baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 37,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15619",
        "abs_url": "https://arxiv.org/abs/2511.15619",
        "pdf_url": "https://arxiv.org/pdf/2511.15619",
        "title": "CODE: A global approach to ODE dynamics learning",
        "authors": [
            "Nils Wildt",
            "Daniel M. Tartakovsky",
            "Sergey Oladyshkin",
            "Wolfgang Nowak"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Ordinary differential equations (ODEs) are a conventional way to describe the observed dynamics of physical systems. Scientists typically hypothesize about dynamical behavior, propose a mathematical model, and compare its predictions to data. However, modern computing and algorithmic advances now enable purely data-driven learning of governing dynamics directly from observations. In data-driven settings, one learns the ODE's right-hand side (RHS). Dense measurements are often assumed, yet high temporal resolution is typically both cumbersome and expensive. Consequently, one usually has only sparsely sampled data. In this work we introduce ChaosODE (CODE), a Polynomial Chaos ODE Expansion in which we use an arbitrary Polynomial Chaos Expansion (aPCE) for the ODE's right-hand side, resulting in a global orthonormal polynomial representation of dynamics. We evaluate the performance of CODE in several experiments on the Lotka-Volterra system, across varying noise levels, initial conditions, and predictions far into the future, even on previously unseen initial conditions. CODE exhibits remarkable extrapolation capabilities even when evaluated under novel initial conditions and shows advantages compared to well-examined methods using neural networks (NeuralODE) or kernel approximators (KernelODE) as the RHS representer. We observe that the high flexibility of NeuralODE and KernelODE degrades extrapolation capabilities under scarce data and measurement noise. Finally, we provide practical guidelines for robust optimization of dynamics-learning problems and illustrate them in the accompanying code.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 38,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15669",
        "abs_url": "https://arxiv.org/abs/2511.15669",
        "pdf_url": "https://arxiv.org/pdf/2511.15669",
        "title": "DeepThinkVLA: Enhancing Reasoning Capability of Vision-Language-Action Models",
        "authors": [
            "Cheng Yin",
            "Yankai Lin",
            "Wang Xu",
            "Sikyuen Tam",
            "Xiangrui Zeng",
            "Zhiyuan Liu",
            "Zhouping Yin"
        ],
        "comments": "16 pages, 6 figures, conference",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Robotics (cs.RO)",
        "abstract": "Enabling Vision-Language-Action (VLA) models to \"think before acting\" via Chain-of-Thought (CoT) is a promising path to overcoming the data-hungry nature of end-to-end robot policies. However, progress is stalled by a fundamental conflict: existing models use a single autoregressive decoder for both sequential CoT reasoning and high-dimensional, parallelizable robot actions. This architectural mismatch degrades motor control and fails to forge a strong causal link between thought and action. We introduce DeepThinkVLA, which resolves this conflict through a tightly integrated architecture and training strategy. Architecturally, our hybrid-attention decoder generates sequential CoT with causal attention and then switches to bidirectional attention for fast, parallel decoding of action vectors. This design is complemented by a two-stage training pipeline: we first use Supervised Fine-Tuning (SFT) to teach the model foundational reasoning, then apply Reinforcement Learning (RL) with task-success rewards to causally align the full reasoning-action sequence with desired outcomes. This synergy leads to state-of-the-art performance, achieving a 97.0% success rate on the LIBERO benchmark. Our ablations confirm the design's effectiveness: the hybrid architecture alone outperforms standard decoders by 15.5%, and the final RL stage provides a crucial 2% boost to secure top performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 39,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15684",
        "abs_url": "https://arxiv.org/abs/2511.15684",
        "pdf_url": "https://arxiv.org/pdf/2511.15684",
        "title": "Walrus: A Cross-Domain Foundation Model for Continuum Dynamics",
        "authors": [
            "Michael McCabe",
            "Payel Mukhopadhyay",
            "Tanya Marwah",
            "Bruno Regaldo-Saint Blancard",
            "Francois Rozet",
            "Cristiana Diaconu",
            "Lucas Meyer",
            "Kaze W. K. Wong",
            "Hadi Sotoudeh",
            "Alberto Bietti",
            "Irina Espejo",
            "Rio Fear",
            "Siavash Golkar",
            "Tom Hehir",
            "Keiya Hirashima",
            "Geraud Krawezik",
            "Francois Lanusse",
            "Rudy Morel",
            "Ruben Ohana",
            "Liam Parker",
            "Mariel Pettee",
            "Jeff Shen",
            "Kyunghyun Cho",
            "Miles Cranmer",
            "Shirley Ho"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE)",
        "abstract": "Foundation models have transformed machine learning for language and vision, but achieving comparable impact in physical simulation remains a challenge. Data heterogeneity and unstable long-term dynamics inhibit learning from sufficiently diverse dynamics, while varying resolutions and dimensionalities challenge efficient training on modern hardware. Through empirical and theoretical analysis, we incorporate new approaches to mitigate these obstacles, including a harmonic-analysis-based stabilization method, load-balanced distributed 2D and 3D training strategies, and compute-adaptive tokenization. Using these tools, we develop Walrus, a transformer-based foundation model developed primarily for fluid-like continuum dynamics. Walrus is pretrained on nineteen diverse scenarios spanning astrophysics, geoscience, rheology, plasma physics, acoustics, and classical fluids. Experiments show that Walrus outperforms prior foundation models on both short and long term prediction horizons on downstream tasks and across the breadth of pretraining data, while ablation studies confirm the value of our contributions to forecast stability, training throughput, and transfer performance over conventional approaches. Code and weights are released for community use.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 40,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15694",
        "abs_url": "https://arxiv.org/abs/2511.15694",
        "pdf_url": "https://arxiv.org/pdf/2511.15694",
        "title": "The Impact of Quantization on Large Reasoning Model Reinforcement Learning",
        "authors": [
            "Medha Kumar",
            "Zifei Xu",
            "Xin Wang",
            "Tristan Webb"
        ],
        "comments": "Accepted to the NeurIPS 2025 Efficient Reasoning Workshop",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Strong reasoning capabilities can now be achieved by large-scale reinforcement learning (RL) without any supervised fine-tuning. Although post-training quantization (PTQ) and quantization-aware training (QAT) are well studied in the context of fine-tuning, how quantization impacts RL in large reasoning models (LRMs) remains an open question. To answer this question, we conducted systematic experiments and discovered a significant gap in reasoning performance on mathematical benchmarks between post-RL quantized models and their quantization-aware RL optimized counterparts. Our findings suggest that quantization-aware RL training negatively impacted the learning process, whereas PTQ and QLoRA led to greater performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 41,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.14775",
        "abs_url": "https://arxiv.org/abs/2511.14775",
        "pdf_url": "https://arxiv.org/pdf/2511.14775",
        "title": "Reservoir Computing via Multi-Scale Random Fourier Features for Forecasting Fast-Slow Dynamical Systems",
        "authors": [
            "S. K. Laha"
        ],
        "comments": "23 pages, 18 Figure",
        "subjects": "Neural and Evolutionary Computing (cs.NE); Machine Learning (cs.LG)",
        "abstract": "Forecasting nonlinear time series with multi-scale temporal structures remains a central challenge in complex systems modeling. We present a novel reservoir computing framework that combines delay embedding with random Fourier feature (RFF) mappings to capture such dynamics. Two formulations are investigated: a single-scale RFF reservoir, which employs a fixed kernel bandwidth, and a multi-scale RFF reservoir, which integrates multiple bandwidths to represent both fast and slow temporal dependencies. The framework is applied to a diverse set of canonical systems: neuronal models such as the Rulkov map, Izhikevich model, Hindmarsh-Rose model, and Morris-Lecar model, which exhibit spiking, bursting, and chaotic behaviors arising from fast-slow interactions; and ecological models including the predator-prey dynamics and Ricker map with seasonal forcing, which display multi-scale oscillations and intermittency. Across all cases, the multi-scale RFF reservoir consistently outperforms its single-scale counterpart, achieving lower normalized root mean square error (NRMSE) and more robust long-horizon predictions. These results highlight the effectiveness of explicitly incorporating multi-scale feature mappings into reservoir computing architectures for modeling complex dynamical systems with intrinsic fast-slow interactions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 42,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.14784",
        "abs_url": "https://arxiv.org/abs/2511.14784",
        "pdf_url": "https://arxiv.org/pdf/2511.14784",
        "title": "Convex Clustering Redefined: Robust Learning with the Median of Means Estimator",
        "authors": [
            "Sourav De",
            "Koustav Chowdhury",
            "Bibhabasu Mandal",
            "Sagar Ghosh",
            "Swagatam Das",
            "Debolina Paul",
            "Saptarshi Chakraborty"
        ],
        "comments": "Accepted in AAAI 2026",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Statistics Theory (math.ST); Computation (stat.CO); Methodology (stat.ME)",
        "abstract": "Clustering approaches that utilize convex loss functions have recently attracted growing interest in the formation of compact data clusters. Although classical methods like k-means and its wide family of variants are still widely used, all of them require the number of clusters k to be supplied as input, and many are notably sensitive to initialization. Convex clustering provides a more stable alternative by formulating the clustering task as a convex optimization problem, ensuring a unique global solution. However, it faces challenges in handling high-dimensional data, especially in the presence of noise and outliers. Additionally, strong fusion regularization, controlled by the tuning parameter, can hinder effective cluster formation within a convex clustering framework. To overcome these challenges, we introduce a robust approach that integrates convex clustering with the Median of Means (MoM) estimator, thus developing an outlier-resistant and efficient clustering framework that does not necessitate prior knowledge of the number of clusters. By leveraging the robustness of MoM alongside the stability of convex clustering, our method enhances both performance and efficiency, especially on large-scale datasets. Theoretical analysis demonstrates weak consistency under specific conditions, while experiments on synthetic and real-world datasets validate the method's superior performance compared to existing approaches.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 43,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.14807",
        "abs_url": "https://arxiv.org/abs/2511.14807",
        "pdf_url": "https://arxiv.org/pdf/2511.14807",
        "title": "Fully Differentiable dMRI Streamline Propagation in PyTorch",
        "authors": [
            "Jongyeon Yoon",
            "Elyssa M. McMaster",
            "Michael E. Kim",
            "Gaurav Rudravaram",
            "Kurt G. Schilling",
            "Bennett A. Landman",
            "Daniel Moyer"
        ],
        "comments": "9 pages, 4 figures. Accepted to SPIE Medical Imaging 2026: Image Processing",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Diffusion MRI (dMRI) provides a distinctive means to probe the microstructural architecture of living tissue, facilitating applications such as brain connectivity analysis, modeling across multiple conditions, and the estimation of macrostructural features. Tractography, which emerged in the final years of the 20th century and accelerated in the early 21st century, is a technique for visualizing white matter pathways in the brain using dMRI. Most diffusion tractography methods rely on procedural streamline propagators or global energy minimization methods. Although recent advancements in deep learning have enabled tasks that were previously challenging, existing tractography approaches are often non-differentiable, limiting their integration in end-to-end learning frameworks. While progress has been made in representing streamlines in differentiable frameworks, no existing method offers fully differentiable propagation. In this work, we propose a fully differentiable solution that retains numerical fidelity with a leading streamline algorithm. The key is that our PyTorch-engineered streamline propagator has no components that block gradient flow, making it fully differentiable. We show that our method matches standard propagators while remaining differentiable. By translating streamline propagation into a differentiable PyTorch framework, we enable deeper integration of tractography into deep learning workflows, laying the foundation for a new category of macrostructural reasoning that is not only computationally robust but also scientifically rigorous.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 44,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.14832",
        "abs_url": "https://arxiv.org/abs/2511.14832",
        "pdf_url": "https://arxiv.org/pdf/2511.14832",
        "title": "How to pick the best anomaly detector?",
        "authors": [
            "Marie Hein",
            "Gregor Kasieczka",
            "Michael Krämer",
            "Louis Moureaux",
            "Alexander Mück",
            "David Shih"
        ],
        "comments": "12 pages, 7 figures",
        "subjects": "High Energy Physics - Phenomenology (hep-ph); Machine Learning (cs.LG); High Energy Physics - Experiment (hep-ex); Data Analysis, Statistics and Probability (physics.data-an)",
        "abstract": "Anomaly detection has the potential to discover new physics in unexplored regions of the data. However, choosing the best anomaly detector for a given data set in a model-agnostic way is an important challenge which has hitherto largely been neglected. In this paper, we introduce the data-driven ARGOS metric, which has a sound theoretical foundation and is empirically shown to robustly select the most sensitive anomaly detection model given the data. Focusing on weakly-supervised, classifier-based anomaly detection methods, we show that the ARGOS metric outperforms other model selection metrics previously used in the literature, in particular the binary cross-entropy loss. We explore several realistic applications, including hyperparameter tuning as well as architecture and feature selection, and in all cases we demonstrate that ARGOS is robust to the noisy conditions of anomaly detection.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 45,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.14868",
        "abs_url": "https://arxiv.org/abs/2511.14868",
        "pdf_url": "https://arxiv.org/pdf/2511.14868",
        "title": "Hierarchical Token Prepending: Enhancing Information Flow in Decoder-based LLM Embeddings",
        "authors": [
            "Xueying Ding",
            "Xingyue Huang",
            "Mingxuan Ju",
            "Liam Collins",
            "Yozen Liu",
            "Leman Akoglu",
            "Neil Shah",
            "Tong Zhao"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Large language models produce powerful text embeddings, but their causal attention mechanism restricts the flow of information from later to earlier tokens, degrading representation quality. While recent methods attempt to solve this by prepending a single summary token, they over-compress information, hence harming performance on long documents. We propose Hierarchical Token Prepending (HTP), a method that resolves two critical bottlenecks. To mitigate attention-level compression, HTP partitions the input into blocks and prepends block-level summary tokens to subsequent blocks, creating multiple pathways for backward information flow. To address readout-level over-squashing, we replace last-token pooling with mean-pooling, a choice supported by theoretical analysis. HTP achieves consistent performance gains across 11 retrieval datasets and 30 general embedding benchmarks, especially in long-context settings. As a simple, architecture-agnostic method, HTP enhances both zero-shot and finetuned models, offering a scalable route to superior long-document embeddings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 46,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.14882",
        "abs_url": "https://arxiv.org/abs/2511.14882",
        "pdf_url": "https://arxiv.org/pdf/2511.14882",
        "title": "Exact Learning of Weighted Graphs Using Composite Queries",
        "authors": [
            "Michael T. Goodrich",
            "Songyu Liu",
            "Ioannis Panageas"
        ],
        "comments": "Full version of the paper published at IWOCA 2025",
        "subjects": "Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG)",
        "abstract": "In this paper, we study the exact learning problem for weighted graphs, where we are given the vertex set, $V$, of a weighted graph, $G=(V,E,w)$, but we are not given $E$. The problem, which is also known as graph reconstruction, is to determine all the edges of $E$, including their weights, by asking queries about $G$ from an oracle. As we observe, using simple shortest-path length queries is not sufficient, in general, to learn a weighted graph. So we study a number of scenarios where it is possible to learn $G$ using a subquadratic number of composite queries, which combine two or three simple queries.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 47,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.14908",
        "abs_url": "https://arxiv.org/abs/2511.14908",
        "pdf_url": "https://arxiv.org/pdf/2511.14908",
        "title": "On-Premise SLMs vs. Commercial LLMs: Prompt Engineering and Incident Classification in SOCs and CSIRTs",
        "authors": [
            "Gefté Almeida",
            "Marcio Pohlmann",
            "Alex Severo",
            "Diego Kreutz",
            "Tiago Heinrich",
            "Lourenço Pereira"
        ],
        "comments": "5 pages, 3 figures, 3 tables, submitted to ERRC/WRSeg 2025",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "In this study, we evaluate open-source models for security incident classification, comparing them with proprietary models. We utilize a dataset of anonymized real incidents, categorized according to the NIST SP 800-61r3 taxonomy and processed using five prompt-engineering techniques (PHP, SHP, HTP, PRP, and ZSL). The results indicate that, although proprietary models still exhibit higher accuracy, locally deployed open-source models provide advantages in privacy, cost-effectiveness, and data sovereignty.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 48,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.14939",
        "abs_url": "https://arxiv.org/abs/2511.14939",
        "pdf_url": "https://arxiv.org/pdf/2511.14939",
        "title": "Fine-tuning Pre-trained Audio Models for COVID-19 Detection: A Technical Report",
        "authors": [
            "Daniel Oliveira de Brito",
            "Letícia Gabriella de Souza",
            "Marcelo Matheus Gauy",
            "Marcelo Finger",
            "Arnaldo Candido Junior"
        ],
        "comments": "11 pages",
        "subjects": "Sound (cs.SD); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)",
        "abstract": "This technical report investigates the performance of pre-trained audio models on COVID-19 detection tasks using established benchmark datasets. We fine-tuned Audio-MAE and three PANN architectures (CNN6, CNN10, CNN14) on the Coswara and COUGHVID datasets, evaluating both intra-dataset and cross-dataset generalization. We implemented a strict demographic stratification by age and gender to prevent models from exploiting spurious correlations between demographic characteristics and COVID-19 status. Intra-dataset results showed moderate performance, with Audio-MAE achieving the strongest result on Coswara (0.82 AUC, 0.76 F1-score), while all models demonstrated limited performance on Coughvid (AUC 0.58-0.63). Cross-dataset evaluation revealed severe generalization failure across all models (AUC 0.43-0.68), with Audio-MAE showing strong performance degradation (F1-score 0.00-0.08). Our experiments demonstrate that demographic balancing, while reducing apparent model performance, provides more realistic assessment of COVID-19 detection capabilities by eliminating demographic leakage - a confounding factor that inflate performance metrics. Additionally, the limited dataset sizes after balancing (1,219-2,160 samples) proved insufficient for deep learning models that typically require substantially larger training sets. These findings highlight fundamental challenges in developing generalizable audio-based COVID-19 detection systems and underscore the importance of rigorous demographic controls for clinically robust model evaluation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 49,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.14953",
        "abs_url": "https://arxiv.org/abs/2511.14953",
        "pdf_url": "https://arxiv.org/pdf/2511.14953",
        "title": "Compiling to recurrent neurons",
        "authors": [
            "Joey Velez-Ginorio",
            "Nada Amin",
            "Konrad Kording",
            "Steve Zdancewic"
        ],
        "comments": "",
        "subjects": "Programming Languages (cs.PL); Machine Learning (cs.LG)",
        "abstract": "Discrete structures are currently second-class in differentiable programming. Since functions over discrete structures lack overt derivatives, differentiable programs do not differentiate through them and limit where they can be used. For example, when programming a neural network, conditionals and iteration cannot be used everywhere; they can break the derivatives necessary for gradient-based learning to work. This limits the class of differentiable algorithms we can directly express, imposing restraints on how we build neural networks and differentiable programs more generally. However, these restraints are not fundamental. Recent work shows conditionals can be first-class, by compiling them into differentiable form as linear neurons. Similarly, this work shows iteration can be first-class -- by compiling to linear recurrent neurons. We present a minimal typed, higher-order and linear programming language with iteration called $\\textsf{Cajal}\\scriptstyle(\\mathbb{\\multimap}, \\mathbb{2}, \\mathbb{N})$. We prove its programs compile correctly to recurrent neurons, allowing discrete algorithms to be expressed in a differentiable form compatible with gradient-based learning. With our implementation, we conduct two experiments where we link these recurrent neurons against a neural network solving an iterative image transformation task. This determines part of its function prior to learning. As a result, the network learns faster and with greater data-efficiency relative to a neural network programmed without first-class iteration. A key lesson is that recurrent neurons enable a rich interplay between learning and the discrete structures of ordinary programming.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 50,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.14962",
        "abs_url": "https://arxiv.org/abs/2511.14962",
        "pdf_url": "https://arxiv.org/pdf/2511.14962",
        "title": "Reconstruction of three-dimensional shapes of normal and disease-related erythrocytes from partial observations using multi-fidelity neural networks",
        "authors": [
            "Haizhou Wen",
            "He Li",
            "Zhen Li"
        ],
        "comments": "29 pages, 10 figures, 3 appendices",
        "subjects": "Computational Physics (physics.comp-ph); Machine Learning (cs.LG); Image and Video Processing (eess.IV); Biological Physics (physics.bio-ph); Quantitative Methods (q-bio.QM)",
        "abstract": "Reconstruction of 3D erythrocyte or red blood cell (RBC) morphology from partial observations, such as microscope images, is essential for understanding the physiology of RBC aging and the pathology of various RBC disorders. In this study, we propose a multi-fidelity neural network (MFNN) approach to fuse high-fidelity cross-sections of an RBC, with a morphologically similar low-fidelity reference 3D RBC shape to recover its full 3D surface. The MFNN predictor combines a convolutional neural network trained on low-fidelity reference RBC data with a feedforward neural network that captures nonlinear morphological correlations, and augments training with surface area and volume constraints for regularization in the low-fidelity branch. This approach is theoretically grounded by a topological homeomorphism between a sphere and 3D RBC surfaces, with training data generated by dissipative particle dynamics simulations of stomatocyte-discocyte-echinocyte transformation. Benchmarking across diverse RBC shapes observed in normal and aged populations, our results show that the MFNN predictor can reconstruct complex RBC morphologies with over 95% coordinate accuracy when provided with at least two orthogonal cross-sections. It is observed that informative oblique cross-sections intersecting spicule tips of echinocytes improve both local and global feature reconstruction, highlighting the value of feature-aware sampling. Our study further evaluates the influence of sampling strategies, shape dissimilarity, and noise, showing enhanced robustness under physically constrained training. Altogether, these results demonstrate the capability of MFNN to reconstruct the 3D shape of normal and aged RBCs from partial cross-sections as observed in conventional microscope images, which could facilitate the quantitative analysis of RBC morphological parameters in normal and disease-related RBC samples.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 51,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.14969",
        "abs_url": "https://arxiv.org/abs/2511.14969",
        "pdf_url": "https://arxiv.org/pdf/2511.14969",
        "title": "Quality-Controlled Multimodal Emotion Recognition in Conversations with Identity-Based Transfer Learning and MAMBA Fusion",
        "authors": [
            "Zanxu Wang",
            "Homayoon Beigi"
        ],
        "comments": "8 pages, 14 images, 3 tables, Recognition Technologies, Inc. Technical Report RTI-20251118-01",
        "subjects": "Audio and Speech Processing (eess.AS); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Image and Video Processing (eess.IV); Signal Processing (eess.SP)",
        "abstract": "This paper addresses data quality issues in multimodal emotion recognition in conversation (MERC) through systematic quality control and multi-stage transfer learning. We implement a quality control pipeline for MELD and IEMOCAP datasets that validates speaker identity, audio-text alignment, and face detection. We leverage transfer learning from speaker and face recognition, assuming that identity-discriminative embeddings capture not only stable acoustic and Facial traits but also person-specific patterns of emotional expression. We employ RecoMadeEasy(R) engines for extracting 512-dimensional speaker and face embeddings, fine-tune MPNet-v2 for emotion-aware text representations, and adapt these features through emotion-specific MLPs trained on unimodal datasets. MAMBA-based trimodal fusion achieves 64.8% accuracy on MELD and 74.3% on IEMOCAP. These results show that combining identity-based audio and visual embeddings with emotion-tuned text representations on a quality-controlled subset of data yields consistent competitive performance for multimodal emotion recognition in conversation and provides a basis for further improvement on challenging, low-frequency emotion classes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 52,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.14980",
        "abs_url": "https://arxiv.org/abs/2511.14980",
        "pdf_url": "https://arxiv.org/pdf/2511.14980",
        "title": "Selective Forgetting in Option Calibration: An Operator-Theoretic Gauss-Newton Framework",
        "authors": [
            "Ahmet Umur Özsoy"
        ],
        "comments": "",
        "subjects": "Mathematical Finance (q-fin.MF); Machine Learning (cs.LG)",
        "abstract": "Calibration of option pricing models is routinely repeated as markets evolve, yet modern systems lack an operator for removing data from a calibrated model without full retraining. When quotes become stale, corrupted, or subject to deletion requirements, existing calibration pipelines must rebuild the entire nonlinear least-squares problem, even if only a small subset of data must be excluded. In this work, we introduce a principled framework for selective forgetting (machine unlearning) in parametric option calibration. We provide stability guarantees, perturbation bounds, and show that the proposed operators satisfy local exactness under standard regularity assumptions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 53,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.14993",
        "abs_url": "https://arxiv.org/abs/2511.14993",
        "pdf_url": "https://arxiv.org/pdf/2511.14993",
        "title": "Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation",
        "authors": [
            "Vladimir Arkhipkin",
            "Vladimir Korviakov",
            "Nikolai Gerasimenko",
            "Denis Parkhomenko",
            "Viacheslav Vasilev",
            "Alexey Letunovskiy",
            "Maria Kovaleva",
            "Nikolai Vaulin",
            "Ivan Kirillov",
            "Lev Novitskiy",
            "Denis Koposov",
            "Nikita Kiselev",
            "Alexander Varlamov",
            "Dmitrii Mikhailov",
            "Vladimir Polovnikov",
            "Andrey Shutkin",
            "Ilya Vasiliev",
            "Julia Agafonova",
            "Anastasiia Kargapoltseva",
            "Anna Dmitrienko",
            "Anastasia Maltseva",
            "Anna Averchenkova",
            "Olga Kim",
            "Tatiana Nikulina",
            "Denis Dimitrov"
        ],
        "comments": "Website: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 54,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15003",
        "abs_url": "https://arxiv.org/abs/2511.15003",
        "pdf_url": "https://arxiv.org/pdf/2511.15003",
        "title": "Resource-Based Time and Cost Prediction in Project Networks: From Statistical Modeling to Graph Neural Networks",
        "authors": [
            "Reza Mirjalili",
            "Behrad Braghi",
            "Shahram Shadrokh Sikari"
        ],
        "comments": "52 pages, 12 figures",
        "subjects": "Applications (stat.AP); Machine Learning (cs.LG)",
        "abstract": "Accurate prediction of project duration and cost remains one of the most challenging aspects of project management, particularly in resource-constrained and interdependent task networks. Traditional analytical techniques such as the Critical Path Method (CPM) and Program Evaluation and Review Technique (PERT) rely on simplified and often static assumptions regarding task interdependencies and resource performance. This study proposes a novel resource-based predictive framework that integrates network representations of project activities with graph neural networks (GNNs) to capture structural and contextual relationships among tasks, resources, and time-cost dynamics. The model represents the project as a heterogeneous activity-resource graph in which nodes denote activities and resources, and edges encode temporal and resource dependencies. We evaluate multiple learning paradigms, including GraphSAGE and Temporal Graph Networks, on both synthetic and benchmark project datasets. Experimental results show that the proposed GNN framework achieves an average 23 to 31 percent reduction in mean absolute error compared to traditional regression and tree-based methods, while improving the coefficient of determination R2 from approximately 0.78 to 0.91 for large and complex project networks. Furthermore, the learned embeddings provide interpretable insights into resource bottlenecks and critical dependencies, enabling more explainable and adaptive scheduling decisions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 55,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15010",
        "abs_url": "https://arxiv.org/abs/2511.15010",
        "pdf_url": "https://arxiv.org/pdf/2511.15010",
        "title": "Latent space analysis and generalization to out-of-distribution data",
        "authors": [
            "Katie Rainey",
            "Erin Hausmann",
            "Donald Waagen",
            "David Gray",
            "Donald Hulsey"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Understanding the relationships between data points in the latent decision space derived by the deep learning system is critical to evaluating and interpreting the performance of the system on real world data. Detecting \\textit{out-of-distribution} (OOD) data for deep learning systems continues to be an active research topic. We investigate the connection between latent space OOD detection and classification accuracy of the model. Using open source simulated and measured Synthetic Aperture RADAR (SAR) datasets, we empirically demonstrate that the OOD detection cannot be used as a proxy measure for model performance. We hope to inspire additional research into the geometric properties of the latent space that may yield future insights into deep learning robustness and generalizability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 56,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15022",
        "abs_url": "https://arxiv.org/abs/2511.15022",
        "pdf_url": "https://arxiv.org/pdf/2511.15022",
        "title": "Complex-Valued 2D Gaussian Representation for Computer-Generated Holography",
        "authors": [
            "Yicheng Zhan",
            "Xiangjun Gao",
            "Long Quan",
            "Kaan Akşit"
        ],
        "comments": "8 pages, 11 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR); Machine Learning (cs.LG)",
        "abstract": "We propose a new hologram representation based on structured complex-valued 2D Gaussian primitives, which replaces per-pixel information storage and reduces the parameter search space by up to 10:1. To enable end-to-end training, we develop a differentiable rasterizer for our representation, integrated with a GPU-optimized light propagation kernel in free space. Our extensive experiments show that our method achieves up to 2.5x lower VRAM usage and 50% faster optimization while producing higher-fidelity reconstructions than existing methods. We further introduce a conversion procedure that adapts our representation to practical hologram formats, including smooth and random phase-only holograms. Our experiments show that this procedure can effectively suppress noise artifacts observed in previous methods. By reducing the hologram parameter search space, our representation enables a more scalable hologram estimation in the next-generation computer-generated holography systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 57,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15061",
        "abs_url": "https://arxiv.org/abs/2511.15061",
        "pdf_url": "https://arxiv.org/pdf/2511.15061",
        "title": "Beyond GeneGPT: A Multi-Agent Architecture with Open-Source LLMs for Enhanced Genomic Question Answering",
        "authors": [
            "Haodong Chen",
            "Guido Zuccon",
            "Teerapong Leelanupab"
        ],
        "comments": "This paper has been accepted to SIGIR-AP 2025",
        "subjects": "Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "Genomic question answering often requires complex reasoning and integration across diverse biomedical sources. GeneGPT addressed this challenge by combining domain-specific APIs with OpenAI's code-davinci-002 large language model to enable natural language interaction with genomic databases. However, its reliance on a proprietary model limits scalability, increases operational costs, and raises concerns about data privacy and generalization. In this work, we revisit and reproduce GeneGPT in a pilot study using open source models, including Llama 3.1, Qwen2.5, and Qwen2.5 Coder, within a monolithic architecture; this allows us to identify the limitations of this approach. Building on this foundation, we then develop OpenBioLLM, a modular multi-agent framework that extends GeneGPT by introducing agent specialization for tool routing, query generation, and response validation. This enables coordinated reasoning and role-based task execution. OpenBioLLM matches or outperforms GeneGPT on over 90% of the benchmark tasks, achieving average scores of 0.849 on Gene-Turing and 0.830 on GeneHop, while using smaller open-source models without additional fine-tuning or tool-specific pretraining. OpenBioLLM's modular multi-agent design reduces latency by 40-50% across benchmark tasks, significantly improving efficiency without compromising model capability. The results of our comprehensive evaluation highlight the potential of open-source multi-agent systems for genomic question answering. Code and resources are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 58,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15076",
        "abs_url": "https://arxiv.org/abs/2511.15076",
        "pdf_url": "https://arxiv.org/pdf/2511.15076",
        "title": "GPU-Initiated Networking for NCCL",
        "authors": [
            "Khaled Hamidouche",
            "John Bachan",
            "Pak Markthub",
            "Peter-Jan Gootzen",
            "Elena Agostini",
            "Sylvain Jeaugey",
            "Aamir Shafi",
            "Georgios Theodorakis",
            "Manjunath Gorentla Venkata"
        ],
        "comments": "13 pages, 9 figures, 3 tables",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Artificial Intelligence (cs.AI); Hardware Architecture (cs.AR); Machine Learning (cs.LG)",
        "abstract": "Modern AI workloads, especially Mixture-of-Experts (MoE) architectures, increasingly demand low-latency, fine-grained GPU-to-GPU communication with device-side control. Traditional GPU communication follows a host-initiated model, where the CPU orchestrates all communication operations - a characteristic of the CUDA runtime. Although robust for collective operations, applications requiring tight integration of computation and communication can benefit from device-initiated communication that eliminates CPU coordination overhead. NCCL 2.28 introduces the Device API with three operation modes: Load/Store Accessible (LSA) for NVLink/PCIe, Multimem for NVLink SHARP, and GPU-Initiated Networking (GIN) for network RDMA. This paper presents the GIN architecture, design, semantics, and highlights its impact on MoE communication. GIN builds on a three-layer architecture: i) NCCL Core host-side APIs for device communicator setup and collective memory window registration; ii) Device-side APIs for remote memory operations callable from CUDA kernels; and iii) A network plugin architecture with dual semantics (GPUDirect Async Kernel-Initiated and Proxy) for broad hardware support. The GPUDirect Async Kernel-Initiated backend leverages DOCA GPUNetIO for direct GPU-to-NIC communication, while the Proxy backend provides equivalent functionality via lock-free GPU-to-CPU queues over standard RDMA networks. We demonstrate GIN's practicality through integration with DeepEP, an MoE communication library. Comprehensive benchmarking shows that GIN provides device-initiated communication within NCCL's unified runtime, combining low-latency operations with NCCL's collective algorithms and production infrastructure.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 59,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15132",
        "abs_url": "https://arxiv.org/abs/2511.15132",
        "pdf_url": "https://arxiv.org/pdf/2511.15132",
        "title": "WaveFuse-AL: Cyclical and Performance-Adaptive Multi-Strategy Active Learning for Medical Images",
        "authors": [
            "Nishchala Thakur",
            "Swati Kochhar",
            "Deepti R. Bathula",
            "Sukrit Gupta"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Active learning reduces annotation costs in medical imaging by strategically selecting the most informative samples for labeling. However, individual acquisition strategies often exhibit inconsistent behavior across different stages of the active learning cycle. We propose Cyclical and Performance-Adaptive Multi-Strategy Active Learning (WaveFuse-AL), a novel framework that adaptively fuses multiple established acquisition strategies-BALD, BADGE, Entropy, and CoreSet throughout the learning process. WaveFuse-AL integrates cyclical (sinusoidal) temporal priors with performance-driven adaptation to dynamically adjust strategy importance over time. We evaluate WaveFuse-AL on three medical imaging benchmarks: APTOS-2019 (multi-class classification), RSNA Pneumonia Detection (binary classification), and ISIC-2018 (skin lesion segmentation). Experimental results demonstrate that WaveFuse-AL consistently outperforms both single-strategy and alternating-strategy baselines, achieving statistically significant performance improvements (on ten out of twelve metric measurements) while maximizing the utility of limited annotation budgets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 60,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15146",
        "abs_url": "https://arxiv.org/abs/2511.15146",
        "pdf_url": "https://arxiv.org/pdf/2511.15146",
        "title": "Beyond Uncertainty Sets: Leveraging Optimal Transport to Extend Conformal Predictive Distribution to Multivariate Settings",
        "authors": [
            "Eugene Ndiaye"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Statistics Theory (math.ST)",
        "abstract": "Conformal prediction (CP) constructs uncertainty sets for model outputs with finite-sample coverage guarantees. A candidate output is included in the prediction set if its non-conformity score is not considered extreme relative to the scores observed on a set of calibration examples. However, this procedure is only straightforward when scores are scalar-valued, which has limited CP to real-valued scores or ad-hoc reductions to one dimension. The problem of ordering vectors has been studied via optimal transport (OT), which provides a principled method for defining vector-ranks and multivariate quantile regions, though typically with only asymptotic coverage guarantees. We restore finite-sample, distribution-free coverage by conformalizing the vector-valued OT quantile region. Here, a candidate's rank is defined via a transport map computed for the calibration scores augmented with that candidate's score. This defines a continuum of OT problems for which we prove that the resulting optimal assignment is piecewise-constant across a fixed polyhedral partition of the score space. This allows us to characterize the entire prediction set tractably, and provides the machinery to address a deeper limitation of prediction sets: that they only indicate which outcomes are plausible, but not their relative likelihood. In one dimension, conformal predictive distributions (CPDs) fill this gap by producing a predictive distribution with finite-sample calibration. Extending CPDs beyond one dimension remained an open problem. We construct, to our knowledge, the first multivariate CPDs with finite-sample calibration, i.e., they define a valid multivariate distribution where any derived uncertainty region automatically has guaranteed coverage. We present both conservative and exact randomized versions, the latter resulting in a multivariate generalization of the classical Dempster-Hill procedure.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 61,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15159",
        "abs_url": "https://arxiv.org/abs/2511.15159",
        "pdf_url": "https://arxiv.org/pdf/2511.15159",
        "title": "Generating Natural-Language Surgical Feedback: From Structured Representation to Domain-Grounded Evaluation",
        "authors": [
            "Firdavs Nasriddinov",
            "Rafal Kocielnik",
            "Anima Anandkumar",
            "Andrew J. Hung"
        ],
        "comments": "Accepted as proceedings paper for ML4H 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "High-quality intraoperative feedback from a surgical trainer is pivotal for improving trainee performance and long-term skill acquisition. Automating natural, trainer-style feedback promises timely, accessible, and consistent guidance at scale but requires models that understand clinically relevant representations. We present a structure-aware pipeline that learns a surgical action ontology from real trainer-to-trainee transcripts (33 surgeries) and uses it to condition feedback generation. We contribute by (1) mining Instrument-Action-Target (IAT) triplets from real-world feedback text and clustering surface forms into normalized categories, (2) fine-tuning a video-to-IAT model that leverages the surgical procedure and task contexts as well as fine-grained temporal instrument motion, and (3) demonstrating how to effectively use IAT triplet representations to guide GPT-4o in generating clinically grounded, trainer-style feedback. We show that, on Task 1: Video-to-IAT recognition, our context injection and temporal tracking deliver consistent AUC gains (Instrument: 0.67 to 0.74; Action: 0.60 to 0.63; Tissue: 0.74 to 0.79). For Task 2: feedback text generation (rated on a 1-5 fidelity rubric where 1 = opposite/unsafe, 3 = admissible, and 5 = perfect match to a human trainer), GPT-4o from video alone scores 2.17, while IAT conditioning reaches 2.44 (+12.4%), doubling the share of admissible generations with score >= 3 from 21% to 42%. Traditional text-similarity metrics also improve: word error rate decreases by 15-31% and ROUGE (phrase/substring overlap) increases by 9-64%. Grounding generation in explicit IAT structure improves fidelity and yields clinician-verifiable rationales, supporting auditable use in surgical training.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 62,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15163",
        "abs_url": "https://arxiv.org/abs/2511.15163",
        "pdf_url": "https://arxiv.org/pdf/2511.15163",
        "title": "Teaching According to Students' Aptitude: Personalized Mathematics Tutoring via Persona-, Memory-, and Forgetting-Aware LLMs",
        "authors": [
            "Yang Wu",
            "Rujing Yao",
            "Tong Zhang",
            "Yufei Shi",
            "Zhuoren Jiang",
            "Zhushan Li",
            "Xiaozhong Liu"
        ],
        "comments": "AAAI 2026 Workshop",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)",
        "abstract": "Large Language Models (LLMs) are increasingly integrated into intelligent tutoring systems to provide human-like and adaptive instruction. However, most existing approaches fail to capture how students' knowledge evolves dynamically across their proficiencies, conceptual gaps, and forgetting patterns. This challenge is particularly acute in mathematics tutoring, where effective instruction requires fine-grained scaffolding precisely calibrated to each student's mastery level and cognitive retention. To address this issue, we propose TASA (Teaching According to Students' Aptitude), a student-aware tutoring framework that integrates persona, memory, and forgetting dynamics for personalized mathematics learning. Specifically, TASA maintains a structured student persona capturing proficiency profiles and an event memory recording prior learning interactions. By incorporating a continuous forgetting curve with knowledge tracing, TASA dynamically updates each student's mastery state and generates contextually appropriate, difficulty-calibrated questions and explanations. Empirical results demonstrate that TASA achieves superior learning outcomes and more adaptive tutoring behavior compared to representative baselines, underscoring the importance of modeling temporal forgetting and learner profiles in LLM-based tutoring systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 63,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15183",
        "abs_url": "https://arxiv.org/abs/2511.15183",
        "pdf_url": "https://arxiv.org/pdf/2511.15183",
        "title": "HinTel-AlignBench: A Framework and Benchmark for Hindi-Telugu with English-Aligned Samples",
        "authors": [
            "Rishikant Chigrupaatii",
            "Ponnada Sai Tulasi Kanishka",
            "Lalit Chandra Routhu",
            "Martin Patel Sama Supratheek Reddy",
            "Divyam Gupta",
            "Dasari Srikar",
            "Krishna Teja Kuchimanchi",
            "Rajiv Misra",
            "Rohun Tripathi"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "With nearly 1.5 billion people and more than 120 major languages, India represents one of the most diverse regions in the world. As multilingual Vision-Language Models (VLMs) gain prominence, robust evaluation methodologies are essential to drive progress toward equitable AI for low-resource languages. Current multilingual VLM evaluations suffer from four major limitations: reliance on unverified auto-translations, narrow task/domain coverage, limited sample sizes, and lack of cultural and natively sourced Question-Answering (QA). To address these gaps, we present a scalable framework to evaluate VLMs in Indian languages and compare it with performance in English. Using the framework, we generate HinTel-AlignBench, a benchmark that draws from diverse sources in Hindi and Telugu with English-aligned samples. Our contributions are threefold: (1) a semi-automated dataset creation framework combining back-translation, filtering, and human verification; (2) the most comprehensive vision-language benchmark for Hindi and and Telugu, including adapted English datasets (VQAv2, RealWorldQA, CLEVR-Math) and native novel Indic datasets (JEE for STEM, VAANI for cultural grounding) with approximately 4,000 QA pairs per language; and (3) a detailed performance analysis of various State-of-the-Art (SOTA) open-weight and closed-source VLMs. We find a regression in performance for tasks in English versus in Indian languages for 4 out of 5 tasks across all the models, with an average regression of 8.3 points in Hindi and 5.5 points for Telugu. We categorize common failure modes to highlight concrete areas of improvement in multilingual multimodal understanding.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 64,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15196",
        "abs_url": "https://arxiv.org/abs/2511.15196",
        "pdf_url": "https://arxiv.org/pdf/2511.15196",
        "title": "Particle Monte Carlo methods for Lattice Field Theory",
        "authors": [
            "David Yallup"
        ],
        "comments": "To appear in the NeurIPS 2025 workshop, Frontiers in Probabilistic Inference: Sampling Meets Learning",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); High Energy Physics - Lattice (hep-lat)",
        "abstract": "High-dimensional multimodal sampling problems from lattice field theory (LFT) have become important benchmarks for machine learning assisted sampling methods. We show that GPU-accelerated particle methods, Sequential Monte Carlo (SMC) and nested sampling, provide a strong classical baseline that matches or outperforms state-of-the-art neural samplers in sample quality and wall-clock time on standard scalar field theory benchmarks, while also estimating the partition function. Using only a single data-driven covariance for tuning, these methods achieve competitive performance without problem-specific structure, raising the bar for when learned proposals justify their training cost.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 65,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15199",
        "abs_url": "https://arxiv.org/abs/2511.15199",
        "pdf_url": "https://arxiv.org/pdf/2511.15199",
        "title": "Learning Where, What and How to Transfer: A Multi-Role Reinforcement Learning Approach for Evolutionary Multitasking",
        "authors": [
            "Jiajun Zhan",
            "Zeyuan Ma",
            "Yue-Jiao Gong",
            "Kay Chen Tan"
        ],
        "comments": "",
        "subjects": "Neural and Evolutionary Computing (cs.NE); Machine Learning (cs.LG)",
        "abstract": "Evolutionary multitasking (EMT) algorithms typically require tailored designs for knowledge transfer, in order to assure convergence and optimality in multitask optimization. In this paper, we explore designing a systematic and generalizable knowledge transfer policy through Reinforcement Learning. We first identify three major challenges: determining the task to transfer (where), the knowledge to be transferred (what) and the mechanism for the transfer (how). To address these challenges, we formulate a multi-role RL system where three (groups of) policy networks act as specialized agents: a task routing agent incorporates an attention-based similarity recognition module to determine source-target transfer pairs via attention scores; a knowledge control agent determines the proportion of elite solutions to transfer; and a group of strategy adaptation agents control transfer strength by dynamically controlling hyper-parameters in the underlying EMT framework. Through pre-training all network modules end-to-end over an augmented multitask problem distribution, a generalizable meta-policy is obtained. Comprehensive validation experiments show state-of-the-art performance of our method against representative baselines. Further in-depth analysis not only reveals the rationale behind our proposal but also provide insightful interpretations on what the system have learned.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 66,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15210",
        "abs_url": "https://arxiv.org/abs/2511.15210",
        "pdf_url": "https://arxiv.org/pdf/2511.15210",
        "title": "Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story",
        "authors": [
            "Vladislav Pedashenko",
            "Laida Kushnareva",
            "Yana Khassan Nibal",
            "Eduard Tulchinskii",
            "Kristian Kuznetsov",
            "Vladislav Zharchinskii",
            "Yury Maximov",
            "Irina Piontkovskaya"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Intrinsic dimension (ID) is an important tool in modern LLM analysis, informing studies of training dynamics, scaling behavior, and dataset structure, yet its textual determinants remain underexplored. We provide the first comprehensive study grounding ID in interpretable text properties through cross-encoder analysis, linguistic features, and sparse autoencoders (SAEs). In this work, we establish three key findings. First, ID is complementary to entropy-based metrics: after controlling for length, the two are uncorrelated, with ID capturing geometric complexity orthogonal to prediction quality. Second, ID exhibits robust genre stratification: scientific prose shows low ID (~8), encyclopedic content medium ID (~9), and creative/opinion writing high ID (~10.5) across all models tested. This reveals that contemporary LLMs find scientific text \"representationally simple\" while fiction requires additional degrees of freedom. Third, using SAEs, we identify causal features: scientific signals (formal tone, report templates, statistics) reduce ID; humanized signals (personalization, emotion, narrative) increase it. Steering experiments confirm these effects are causal. Thus, for contemporary models, scientific writing appears comparatively \"easy\", whereas fiction, opinion, and affect add representational degrees of freedom. Our multi-faceted analysis provides practical guidance for the proper use of ID and the sound interpretation of ID-based results.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 67,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15222",
        "abs_url": "https://arxiv.org/abs/2511.15222",
        "pdf_url": "https://arxiv.org/pdf/2511.15222",
        "title": "Why Physics Still Matters: Improving Machine Learning Prediction of Material Properties with Phonon-Informed Datasets",
        "authors": [
            "Pol Benítez",
            "Cibrán López",
            "Edgardo Saucedo",
            "Teruyasu Mizoguchi",
            "Claudio Cazorla"
        ],
        "comments": "12 pages; 5 figures",
        "subjects": "Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG)",
        "abstract": "Machine learning (ML) methods have become powerful tools for predicting material properties with near first-principles accuracy and vastly reduced computational cost. However, the performance of ML models critically depends on the quality, size, and diversity of the training dataset. In materials science, this dependence is particularly important for learning from low-symmetry atomistic configurations that capture thermal excitations, structural defects, and chemical disorder, features that are ubiquitous in real materials but underrepresented in most datasets. The absence of systematic strategies for generating representative training data may therefore limit the predictive power of ML models in technologically critical fields such as energy conversion and photonics. In this work, we assess the effectiveness of graph neural network (GNN) models trained on two fundamentally different types of datasets: one composed of randomly generated atomic configurations and another constructed using physically informed sampling based on lattice vibrations. As a case study, we address the challenging task of predicting electronic and mechanical properties of a prototypical family of optoelectronic materials under realistic finite-temperature conditions. We find that the phonons-informed model consistently outperforms the randomly trained counterpart, despite relying on fewer data points. Explainability analyses further reveal that high-performing models assign greater weight to chemically meaningful bonds that control property variations, underscoring the importance of physically guided data generation. Overall, this work demonstrates that larger datasets do not necessarily yield better GNN predictive models and introduces a simple and general strategy for efficiently constructing high-quality training data in materials informatics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 68,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15262",
        "abs_url": "https://arxiv.org/abs/2511.15262",
        "pdf_url": "https://arxiv.org/pdf/2511.15262",
        "title": "Reinforcement Learning in Queue-Reactive Models: Application to Optimal Execution",
        "authors": [
            "Tomas Espana",
            "Yadh Hafsi",
            "Fabrizio Lillo",
            "Edoardo Vittori"
        ],
        "comments": "",
        "subjects": "Trading and Market Microstructure (q-fin.TR); Machine Learning (cs.LG)",
        "abstract": "We investigate the use of Reinforcement Learning for the optimal execution of meta-orders, where the objective is to execute incrementally large orders while minimizing implementation shortfall and market impact over an extended period of time. Departing from traditional parametric approaches to price dynamics and impact modeling, we adopt a model-free, data-driven framework. Since policy optimization requires counterfactual feedback that historical data cannot provide, we employ the Queue-Reactive Model to generate realistic and tractable limit order book simulations that encompass transient price impact, and nonlinear and dynamic order flow responses. Methodologically, we train a Double Deep Q-Network agent on a state space comprising time, inventory, price, and depth variables, and evaluate its performance against established benchmarks. Numerical simulation results show that the agent learns a policy that is both strategic and tactical, adapting effectively to order book conditions and outperforming standard approaches across multiple training configurations. These findings provide strong evidence that model-free Reinforcement Learning can yield adaptive and robust solutions to the optimal execution problem.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 69,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15315",
        "abs_url": "https://arxiv.org/abs/2511.15315",
        "pdf_url": "https://arxiv.org/pdf/2511.15315",
        "title": "Robust Bayesian Optimisation with Unbounded Corruptions",
        "authors": [
            "Abdelhamid Ezzerg",
            "Ilija Bogunovic",
            "Jeremias Knoblauch"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Bayesian Optimization is critically vulnerable to extreme outliers. Existing provably robust methods typically assume a bounded cumulative corruption budget, which makes them defenseless against even a single corruption of sufficient magnitude. To address this, we introduce a new adversary whose budget is only bounded in the frequency of corruptions, not in their magnitude. We then derive RCGP-UCB, an algorithm coupling the famous upper confidence bound (UCB) approach with a Robust Conjugate Gaussian Process (RCGP). We present stable and adaptive versions of RCGP-UCB, and prove that they achieve sublinear regret in the presence of up to $O(T^{1/2})$ and $O(T^{1/3})$ corruptions with possibly infinite magnitude. This robustness comes at near zero cost: without outliers, RCGP-UCB's regret bounds match those of the standard GP-UCB algorithm.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 70,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15332",
        "abs_url": "https://arxiv.org/abs/2511.15332",
        "pdf_url": "https://arxiv.org/pdf/2511.15332",
        "title": "Exponential Lasso: robust sparse penalization under heavy-tailed noise and outliers with exponential-type loss",
        "authors": [
            "Tien Mai"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Methodology (stat.ME)",
        "abstract": "In high-dimensional statistics, the Lasso is a cornerstone method for simultaneous variable selection and parameter estimation. However, its reliance on the squared loss function renders it highly sensitive to outliers and heavy-tailed noise, potentially leading to unreliable model selection and biased estimates. To address this limitation, we introduce the Exponential Lasso, a novel robust method that integrates an exponential-type loss function within the Lasso framework. This loss function is designed to achieve a smooth trade-off between statistical efficiency under Gaussian noise and robustness against data contamination. Unlike other methods that cap the influence of large residuals, the exponential loss smoothly redescends, effectively downweighting the impact of extreme outliers while preserving near-quadratic behavior for small errors. We establish theoretical guarantees showing that the Exponential Lasso achieves strong statistical convergence rates, matching the classical Lasso under ideal conditions while maintaining its robustness in the presence of heavy-tailed contamination. Computationally, the estimator is optimized efficiently via a Majorization-Minimization (MM) algorithm that iteratively solves a series of weighted Lasso subproblems. Numerical experiments demonstrate that the proposed method is highly competitive, outperforming the classical Lasso in contaminated settings and maintaining strong performance even under Gaussian noise. Our method is implemented in the \\texttt{R} package \\texttt{heavylasso} available on Github: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 71,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15406",
        "abs_url": "https://arxiv.org/abs/2511.15406",
        "pdf_url": "https://arxiv.org/pdf/2511.15406",
        "title": "Controlling False Positives in Image Segmentation via Conformal Prediction",
        "authors": [
            "Luca Mossina",
            "Corentin Friedrich"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Reliable semantic segmentation is essential for clinical decision making, yet deep models rarely provide explicit statistical guarantees on their errors. We introduce a simple post-hoc framework that constructs confidence masks with distribution-free, image-level control of false-positive predictions. Given any pretrained segmentation model, we define a nested family of shrunken masks obtained either by increasing the score threshold or by applying morphological erosion. A labeled calibration set is used to select a single shrink parameter via conformal prediction, ensuring that, for new images that are exchangeable with the calibration data, the proportion of false positives retained in the confidence mask stays below a user-specified tolerance with high probability. The method is model-agnostic, requires no retraining, and provides finite-sample guarantees regardless of the underlying predictor. Experiments on a polyp-segmentation benchmark demonstrate target-level empirical validity. Our framework enables practical, risk-aware segmentation in settings where over-segmentation can have clinical consequences. Code at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 72,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15445",
        "abs_url": "https://arxiv.org/abs/2511.15445",
        "pdf_url": "https://arxiv.org/pdf/2511.15445",
        "title": "Neural network-driven domain decomposition for efficient solutions to the Helmholtz equation",
        "authors": [
            "Victorita Dolean",
            "Daria Hrebenshchykova",
            "Stéphane Lanteri",
            "Victor Michel-Dansac"
        ],
        "comments": "",
        "subjects": "Numerical Analysis (math.NA); Machine Learning (cs.LG)",
        "abstract": "Accurately simulating wave propagation is crucial in fields such as acoustics, electromagnetism, and seismic analysis. Traditional numerical methods, like finite difference and finite element approaches, are widely used to solve governing partial differential equations (PDEs) such as the Helmholtz equation. However, these methods face significant computational challenges when applied to high-frequency wave problems in complex two-dimensional domains. This work investigates Finite Basis Physics-Informed Neural Networks (FBPINNs) and their multilevel extensions as a promising alternative. These methods leverage domain decomposition, partitioning the computational domain into overlapping sub-domains, each governed by a local neural network. We assess their accuracy and computational efficiency in solving the Helmholtz equation for the homogeneous case, demonstrating their potential to mitigate the limitations of traditional approaches.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 73,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15446",
        "abs_url": "https://arxiv.org/abs/2511.15446",
        "pdf_url": "https://arxiv.org/pdf/2511.15446",
        "title": "Gini Score under Ties and Case Weights",
        "authors": [
            "Alexej Brauer",
            "Mario V. Wüthrich"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Applications (stat.AP)",
        "abstract": "The Gini score is a popular tool in statistical modeling and machine learning for model validation and model selection. It is a purely rank based score that allows one to assess risk rankings. The Gini score for statistical modeling has mainly been used in a binary context, in which it has many equivalent reformulations such as the receiver operating characteristic (ROC) or the area under the curve (AUC). In the actuarial literature, this rank based score for binary responses has been extended to general real-valued random variables using Lorenz curves and concentration curves. While these initial concepts assume that the risk ranking is generated by a continuous distribution function, we discuss in this paper how the Gini score can be used in the case of ties in the risk ranking. Moreover, we adapt the Gini score to the common actuarial situation of having case weights.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 74,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15476",
        "abs_url": "https://arxiv.org/abs/2511.15476",
        "pdf_url": "https://arxiv.org/pdf/2511.15476",
        "title": "RS-CA-HSICT: A Residual and Spatial Channel Augmented CNN Transformer Framework for Monkeypox Detection",
        "authors": [
            "Rashid Iqbal",
            "Saddam Hussain Khan"
        ],
        "comments": "33 Pages, 12 Figure, 4 Tables",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "This work proposes a hybrid deep learning approach, namely Residual and Spatial Learning based Channel Augmented Integrated CNN-Transformer architecture, that leverages the strengths of CNN and Transformer towards enhanced MPox detection. The proposed RS-CA-HSICT framework is composed of an HSICT block, a residual CNN module, a spatial CNN block, and a CA, which enhances the diverse feature space, detailed lesion information, and long-range dependencies. The new HSICT module first integrates an abstract representation of the stem CNN and customized ICT blocks for efficient multihead attention and structured CNN layers with homogeneous (H) and structural (S) operations. The customized ICT blocks learn global contextual interactions and local texture extraction. Additionally, H and S layers learn spatial homogeneity and fine structural details by reducing noise and modeling complex morphological variations. Moreover, inverse residual learning enhances vanishing gradient, and stage-wise resolution reduction ensures scale invariance. Furthermore, the RS-CA-HSICT framework augments the learned HSICT channels with the TL-driven Residual and Spatial CNN maps for enhanced multiscale feature space capturing global and localized structural cues, subtle texture, and contrast variations. These channels, preceding augmentation, are refined through the Channel-Fusion-and-Attention block, which preserves discriminative channels while suppressing redundant ones, thereby enabling efficient computation. Finally, the spatial attention mechanism refines pixel selection to detect subtle patterns and intra-class contrast variations in Mpox. Experimental results on both the Kaggle benchmark and a diverse MPox dataset reported classification accuracy as high as 98.30% and an F1-score of 98.13%, which outperforms the existing CNNs and ViTs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 75,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15503",
        "abs_url": "https://arxiv.org/abs/2511.15503",
        "pdf_url": "https://arxiv.org/pdf/2511.15503",
        "title": "A Tensor Compiler for Processing-In-Memory Architectures",
        "authors": [
            "Peiming Yang",
            "Sankeerth Durvasula",
            "Ivan Fernandez",
            "Mohammad Sadrosadati",
            "Onur Mutlu",
            "Gennady Pekhimenko",
            "Christina Giannoula"
        ],
        "comments": "",
        "subjects": "Hardware Architecture (cs.AR); Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG); Performance (cs.PF)",
        "abstract": "Processing-In-Memory (PIM) devices integrated with high-performance Host processors (e.g., GPUs) can accelerate memory-intensive kernels in Machine Learning (ML) models, including Large Language Models (LLMs), by leveraging high memory bandwidth at PIM cores. However, Host processors and PIM cores require different data layouts: Hosts need consecutive elements distributed across DRAM banks, while PIM cores need them within local banks. This necessitates data rearrangements in ML kernel execution that pose significant performance and programmability challenges, further exacerbated by the need to support diverse PIM backends. Current compilation approaches lack systematic optimization for diverse ML kernels across multiple PIM backends and may largely ignore data rearrangements during compute code optimization. We demonstrate that data rearrangements and compute code optimization are interdependent, and need to be jointly optimized during the tuning process. To address this, we design DCC, the first data-centric ML compiler for PIM systems that jointly co-optimizes data rearrangements and compute code in a unified tuning process. DCC integrates a multi-layer PIM abstraction that enables various data distribution and processing strategies on different PIM backends. DCC enables effective co-optimization by mapping data partitioning strategies to compute loop partitions, applying PIM-specific code optimizations and leveraging a fast and accurate performance prediction model to select optimal configurations. Our evaluations in various individual ML kernels demonstrate that DCC achieves up to 7.68x speedup (2.7x average) on HBM-PIM and up to 13.17x speedup (5.75x average) on AttAcc PIM backend over GPU-only execution. In end-to-end LLM inference, DCC on AttAcc accelerates GPT-3 and LLaMA-2 by up to 7.71x (4.88x average) over GPU.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 76,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15529",
        "abs_url": "https://arxiv.org/abs/2511.15529",
        "pdf_url": "https://arxiv.org/pdf/2511.15529",
        "title": "Decentralized Gaussian Process Classification and an Application in Subsea Robotics",
        "authors": [
            "Yifei Gao",
            "Hans J. He",
            "Daniel J. Stilwell",
            "James McMahon"
        ],
        "comments": "8 pages, 8 figures, IROS 2025 conference",
        "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
        "abstract": "Teams of cooperating autonomous underwater vehicles (AUVs) rely on acoustic communication for coordination, yet this communication medium is constrained by limited range, multi-path effects, and low bandwidth. One way to address the uncertainty associated with acoustic communication is to learn the communication environment in real-time. We address the challenge of a team of robots building a map of the probability of communication success from one location to another in real-time. This is a decentralized classification problem -- communication events are either successful or unsuccessful -- where AUVs share a subset of their communication measurements to build the map. The main contribution of this work is a rigorously derived data sharing policy that selects measurements to be shared among AUVs. We experimentally validate our proposed sharing policy using real acoustic communication data collected from teams of Virginia Tech 690 AUVs, demonstrating its effectiveness in underwater environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 77,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15530",
        "abs_url": "https://arxiv.org/abs/2511.15530",
        "pdf_url": "https://arxiv.org/pdf/2511.15530",
        "title": "Convergence and Sketching-Based Efficient Computation of Neural Tangent Kernel Weights in Physics-Based Loss",
        "authors": [
            "Max Hirsch",
            "Federico Pichi"
        ],
        "comments": "",
        "subjects": "Numerical Analysis (math.NA); Machine Learning (cs.LG)",
        "abstract": "In multi-objective optimization, multiple loss terms are weighted and added together to form a single objective. These weights are chosen to properly balance the competing losses according to some meta-goal. For example, in physics-informed neural networks (PINNs), these weights are often adaptively chosen to improve the network's generalization error. A popular choice of adaptive weights is based on the neural tangent kernel (NTK) of the PINN, which describes the evolution of the network in predictor space during training. The convergence of such an adaptive weighting algorithm is not clear a priori. Moreover, these NTK-based weights would be updated frequently during training, further increasing the computational burden of the learning process. In this paper, we prove that under appropriate conditions, gradient descent enhanced with adaptive NTK-based weights is convergent in a suitable sense. We then address the problem of computational efficiency by developing a randomized algorithm inspired by a predictor-corrector approach and matrix sketching, which produces unbiased estimates of the NTK up to an arbitrarily small discretization error. Finally, we provide numerical experiments to support our theoretical findings and to show the efficacy of our randomized algorithm. Code Availability: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 78,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15543",
        "abs_url": "https://arxiv.org/abs/2511.15543",
        "pdf_url": "https://arxiv.org/pdf/2511.15543",
        "title": "A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation",
        "authors": [
            "Georgios Venianakis",
            "Constantinos Theodoropoulos",
            "Michail Kavousanakis"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Parameter estimation remains a challenging task across many areas of engineering. Because data acquisition can often be costly, limited, or prone to inaccuracies (noise, uncertainty) it is crucial to identify sensor configurations that provide the maximum amount of information about the unknown parameters, in particular for the case of distributed-parameter systems, where spatial variations are important. Physics-Informed Neural Networks (PINNs) have recently emerged as a powerful machine-learning (ML) tool for parameter estimation, particularly in cases with sparse or noisy measurements, overcoming some of the limitations of traditional optimization-based and Bayesian approaches. Despite the widespread use of PINNs for solving inverse problems, relatively little attention has been given to how their performance depends on sensor placement. This study addresses this gap by introducing a comprehensive PINN-based framework that simultaneously tackles optimal sensor placement and parameter estimation. Our approach involves training a PINN model in which the parameters of interest are included as additional inputs. This enables the efficient computation of sensitivity functions through automatic differentiation, which are then used to determine optimal sensor locations exploiting the D-optimality criterion. The framework is validated on two illustrative distributed-parameter reaction-diffusion-advection problems of increasing complexity. The results demonstrate that our PINNs-based methodology consistently achieves higher accuracy compared to parameter values estimated from intuitively or randomly selected sensor positions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 79,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15615",
        "abs_url": "https://arxiv.org/abs/2511.15615",
        "pdf_url": "https://arxiv.org/pdf/2511.15615",
        "title": "Near-optimal delta-convex estimation of Lipschitz functions",
        "authors": [
            "Gábor Balázs"
        ],
        "comments": "41 pages, 7 figures",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "This paper presents a tractable algorithm for estimating an unknown Lipschitz function from noisy observations and establishes an upper bound on its convergence rate. The approach extends max-affine methods from convex shape-restricted regression to the more general Lipschitz setting. A key component is a nonlinear feature expansion that maps max-affine functions into a subclass of delta-convex functions, which act as universal approximators of Lipschitz functions while preserving their Lipschitz constants. Leveraging this property, the estimator attains the minimax convergence rate (up to logarithmic factors) with respect to the intrinsic dimension of the data under squared loss and subgaussian distributions in the random design setting. The algorithm integrates adaptive partitioning to capture intrinsic dimension, a penalty-based regularization mechanism that removes the need to know the true Lipschitz constant, and a two-stage optimization procedure combining a convex initialization with local refinement. The framework is also straightforward to adapt to convex shape-restricted regression. Experiments demonstrate competitive performance relative to other theoretically justified methods, including nearest-neighbor and kernel-based regressors.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 80,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15632",
        "abs_url": "https://arxiv.org/abs/2511.15632",
        "pdf_url": "https://arxiv.org/pdf/2511.15632",
        "title": "CODE-II: A large-scale dataset for artificial intelligence in ECG analysis",
        "authors": [
            "Petrus E. O. G. B. Abreu",
            "Gabriela M. M. Paixão",
            "Jiawei Li",
            "Paulo R. Gomes",
            "Peter W. Macfarlane",
            "Ana C. S. Oliveira",
            "Vinicius T. Carvalho",
            "Thomas B. Schön",
            "Antonio Luiz P. Ribeiro",
            "Antônio H. Ribeiro"
        ],
        "comments": "",
        "subjects": "Signal Processing (eess.SP); Machine Learning (cs.LG)",
        "abstract": "Data-driven methods for electrocardiogram (ECG) interpretation are rapidly progressing. Large datasets have enabled advances in artificial intelligence (AI) based ECG analysis, yet limitations in annotation quality, size, and scope remain major challenges. Here we present CODE-II, a large-scale real-world dataset of 2,735,269 12-lead ECGs from 2,093,807 adult patients collected by the Telehealth Network of Minas Gerais (TNMG), Brazil. Each exam was annotated using standardized diagnostic criteria and reviewed by cardiologists. A defining feature of CODE-II is a set of 66 clinically meaningful diagnostic classes, developed with cardiologist input and routinely used in telehealth practice. We additionally provide an open available subset: CODE-II-open, a public subset of 15,000 patients, and the CODE-II-test, a non-overlapping set of 8,475 exams reviewed by multiple cardiologists for blinded evaluation. A neural network pre-trained on CODE-II achieved superior transfer performance on external benchmarks (PTB-XL and CPSC 2018) and outperformed alternatives trained on larger datasets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 81,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15634",
        "abs_url": "https://arxiv.org/abs/2511.15634",
        "pdf_url": "https://arxiv.org/pdf/2511.15634",
        "title": "Rényi Differential Privacy for Heavy-Tailed SDEs via Fractional Poincaré Inequalities",
        "authors": [
            "Benjamin Dupuis",
            "Mert Gürbüzbalaban",
            "Umut Şimşekli",
            "Jian Wang",
            "Sinan Yildirim",
            "Lingjiong Zhu"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Characterizing the differential privacy (DP) of learning algorithms has become a major challenge in recent years. In parallel, many studies suggested investigating the behavior of stochastic gradient descent (SGD) with heavy-tailed noise, both as a model for modern deep learning models and to improve their performance. However, most DP bounds focus on light-tailed noise, where satisfactory guarantees have been obtained but the proposed techniques do not directly extend to the heavy-tailed setting. Recently, the first DP guarantees for heavy-tailed SGD were obtained. These results provide $(0,\\delta)$-DP guarantees without requiring gradient clipping. Despite casting new light on the link between DP and heavy-tailed algorithms, these results have a strong dependence on the number of parameters and cannot be extended to other DP notions like the well-established Rényi differential privacy (RDP). In this work, we propose to address these limitations by deriving the first RDP guarantees for heavy-tailed SDEs, as well as their discretized counterparts. Our framework is based on new Rényi flow computations and the use of well-established fractional Poincaré inequalities. Under the assumption that such inequalities are satisfied, we obtain DP guarantees that have a much weaker dependence on the dimension compared to prior art.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 82,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15679",
        "abs_url": "https://arxiv.org/abs/2511.15679",
        "pdf_url": "https://arxiv.org/pdf/2511.15679",
        "title": "Front-door Reducibility: Reducing ADMGs to the Standard Front-door Setting via a Graphical Criterion",
        "authors": [
            "Jianqiao Mao",
            "Max A. Little"
        ],
        "comments": "16 pages, 3 figures",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Front-door adjustment provides a simple closed-form identification formula under the classical front-door criterion, but its applicability is often viewed as narrow and strict. Although ID algorithm is very useful and is proved effective for causal relation identification in general causal graphs (if it is identifiable), performing ID algorithm does not guarantee to obtain a practical, easy-to-estimate interventional distribution expression. We argue that the applicability of the front-door criterion is not as limited as it seems: many more complicated causal graphs can be reduced to the front-door criterion. In this paper, We introduce front-door reducibility (FDR), a graphical condition on acyclic directed mixed graphs (ADMGs) that extends the applicability of the classic front-door criterion to reduce a large family of complicated causal graphs to a front-door setting by aggregating variables into super-nodes (FDR triple) $\\left(\\boldsymbol{X}^{*},\\boldsymbol{Y}^{*},\\boldsymbol{M}^{*}\\right)$. After characterizing FDR criterion, we prove a graph-level equivalence between the satisfication of FDR criterion and the applicability of FDR adjustment. Meanwhile, we then present FDR-TID, an exact algorithm that detects an admissible FDR triple, together with established the algorithm's correctness, completeness, and finite termination. Empirically-motivated examples illustrate that many graphs outside the textbook front-door setting are FDR, yielding simple, estimable adjustments where general ID expressions would be cumbersome. FDR thus complements existing identification method by prioritizing interpretability and computational simplicity without sacrificing generality across mixed graphs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 83,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15698",
        "abs_url": "https://arxiv.org/abs/2511.15698",
        "pdf_url": "https://arxiv.org/pdf/2511.15698",
        "title": "RescueLens: LLM-Powered Triage and Action on Volunteer Feedback for Food Rescue",
        "authors": [
            "Naveen Raman",
            "Jingwu Tang",
            "Zhiyu Chen",
            "Zheyuan Ryan Shi",
            "Sean Hudson",
            "Ameesh Kapoor",
            "Fei Fang"
        ],
        "comments": "Accepted at IAAI'26",
        "subjects": "Computers and Society (cs.CY); Machine Learning (cs.LG)",
        "abstract": "Food rescue organizations simultaneously tackle food insecurity and waste by working with volunteers to redistribute food from donors who have excess to recipients who need it. Volunteer feedback allows food rescue organizations to identify issues early and ensure volunteer satisfaction. However, food rescue organizations monitor feedback manually, which can be cumbersome and labor-intensive, making it difficult to prioritize which issues are most important. In this work, we investigate how large language models (LLMs) assist food rescue organizers in understanding and taking action based on volunteer experiences. We work with 412 Food Rescue, a large food rescue organization based in Pittsburgh, Pennsylvania, to design RescueLens, an LLM-powered tool that automatically categorizes volunteer feedback, suggests donors and recipients to follow up with, and updates volunteer directions based on feedback. We evaluate the performance of RescueLens on an annotated dataset, and show that it can recover 96% of volunteer issues at 71% precision. Moreover, by ranking donors and recipients according to their rates of volunteer issues, RescueLens allows organizers to focus on 0.5% of donors responsible for more than 30% of volunteer issues. RescueLens is now deployed at 412 Food Rescue and through semi-structured interviews with organizers, we find that RescueLens streamlines the feedback process so organizers better allocate their time.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 84,
        "date": "2025-11-20",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-20?abs=True",
        "arxiv_id": "2511.15709",
        "abs_url": "https://arxiv.org/abs/2511.15709",
        "pdf_url": "https://arxiv.org/pdf/2511.15709",
        "title": "Tokenisation over Bounded Alphabets is Hard",
        "authors": [
            "Violeta Kastreva",
            "Philip Whittington",
            "Dennis Komm",
            "Tiago Pimentel"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG)",
        "abstract": "Recent works have shown that tokenisation is NP-complete. However, these works assume tokenisation is applied to inputs with unboundedly large alphabets -- an unrealistic assumption, given that in practice tokenisers operate over fixed-size alphabets, such as bytes or Unicode characters. We close this gap by analysing tokenisation over bounded $n$-ary alphabets, considering two natural variants: bottom-up tokenisation and direct tokenisation, where we must, respectively, select a sequence of merge operations or a vocabulary whose application optimally compresses a dataset. First, we note that proving hardness results for an $n$-ary alphabet proves the same results for alphabets of any larger size. We then prove that even with binary alphabets, both variants are not only NP-complete, but admit no polynomial-time approximation scheme (unless P=NP). We further show that direct tokenisation remains NP-complete even when applied to unary alphabets. While unary alphabets may not be practically useful, this result establishes that the computational intractability of tokenisation is not an artifact of large alphabets or complex constructions, but a fundamental barrier. Overall, our results explain why practical algorithms such as BPE and UnigramLM are heuristic, and points toward approximation algorithms being an important path going forward for tokenisation research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    }
]