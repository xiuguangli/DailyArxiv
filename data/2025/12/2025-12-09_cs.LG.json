[
    {
        "order": 1,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.05989",
        "abs_url": "https://arxiv.org/abs/2512.05989",
        "pdf_url": "https://arxiv.org/pdf/2512.05989",
        "title": "A self-driving lab for solution-processed electrochromic thin films",
        "authors": [
            "Selma Dahms",
            "Luca Torresi",
            "Shahbaz Tareq Bandesha",
            "Jan Hansmann",
            "Holger Röhm",
            "Alexander Colsmann",
            "Marco Schott",
            "Pascal Friederich"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Materials Science (cond-mat.mtrl-sci)",
        "abstract": "Solution-processed electrochromic materials offer high potential for energy-efficient smart windows and displays. Their performance varies with material choice and processing conditions. Electrochromic thin film electrodes require a smooth, defect-free coating for optimal contrast between bleached and colored states. The complexity of optimizing the spin-coated electrochromic thin layer poses challenges for rapid development. This study demonstrates the use of self-driving laboratories to accelerate the development of electrochromic coatings by coupling automation with machine learning. Our system combines automated data acquisition, image processing, spectral analysis, and Bayesian optimization to explore processing parameters efficiently. This approach not only increases throughput but also enables a pointed search for optimal processing parameters. The approach can be applied to various solution-processed materials, highlighting the potential of self-driving labs in enhancing materials discovery and process optimization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 2,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.05990",
        "abs_url": "https://arxiv.org/abs/2512.05990",
        "pdf_url": "https://arxiv.org/pdf/2512.05990",
        "title": "Memory-Amortized Inference: A Topological Unification of Search, Closure, and Structure",
        "authors": [
            "Xin Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)",
        "abstract": "Contemporary ML separates the static structure of parameters from the dynamic flow of inference, yielding systems that lack the sample efficiency and thermodynamic frugality of biological cognition. In this theoretical work, we propose \\textbf{Memory-Amortized Inference (MAI)}, a formal framework rooted in algebraic topology that unifies learning and memory as phase transitions of a single geometric substrate. Central to our theory is the \\textbf{Homological Parity Principle}, which posits a fundamental dichotomy: even-dimensional homology ($H_{even}$) physically instantiates stable \\textbf{Content} (stable scaffolds or ``what''), while odd-dimensional homology ($H_{odd}$) instantiates dynamic \\textbf{Context} (dynamic flows or ``where''). We derive the logical flow of MAI as a topological trinity transformation: \\textbf{Search $\\to$ Closure $\\to$ Structure}. Specifically, we demonstrate that cognition operates by converting high-complexity recursive search (modeled by \\textit{Savitch's Theorem} in NPSPACE) into low-complexity lookup (modeled by \\textit{Dynamic Programming} in P) via the mechanism of \\textbf{Topological Cycle Closure}. We further show that this consolidation process is governed by a topological generalization of the Wake-Sleep algorithm, functioning as a coordinate descent that alternates between optimizing the $H_{odd}$ flow (inference/wake) and condensing persistent cycles into the $H_{even}$ scaffold (learning/sleep). This framework offers a rigorous explanation for the emergence of fast-thinking (intuition) from slow-thinking (reasoning) and provides a blueprint for post-Turing architectures that compute via topological resonance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 3,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06059",
        "abs_url": "https://arxiv.org/abs/2512.06059",
        "pdf_url": "https://arxiv.org/pdf/2512.06059",
        "title": "Deep learning recognition and analysis of Volatile Organic Compounds based on experimental and synthetic infrared absorption spectra",
        "authors": [
            "Andrea Della Valle",
            "Annalisa D'Arco",
            "Tiziana Mancini",
            "Rosanna Mosetti",
            "Maria Chiara Paolozzi",
            "Stefano Lupi",
            "Sebastiano Pilati",
            "Andrea Perali"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Applied Physics (physics.app-ph); Chemical Physics (physics.chem-ph)",
        "abstract": "Volatile Organic Compounds (VOCs) are organic molecules that have low boiling points and therefore easily evaporate into the air. They pose significant risks to human health, making their accurate detection the crux of efforts to monitor and minimize exposure. Infrared (IR) spectroscopy enables the ultrasensitive detection at low-concentrations of VOCs in the atmosphere by measuring their IR absorption spectra. However, the complexity of the IR spectra limits the possibility to implement VOC recognition and quantification in real-time. While deep neural networks (NNs) are increasingly used for the recognition of complex data structures, they typically require massive datasets for the training phase. Here, we create an experimental VOC dataset for nine different classes of compounds at various concentrations, using their IR absorption spectra. To further increase the amount of spectra and their diversity in term of VOC concentration, we augment the experimental dataset with synthetic spectra created via conditional generative NNs. This allows us to train robust discriminative NNs, able to reliably identify the nine VOCs, as well as to precisely predict their concentrations. The trained NN is suitable to be incorporated into sensing devices for VOCs recognition and analysis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 4,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06102",
        "abs_url": "https://arxiv.org/abs/2512.06102",
        "pdf_url": "https://arxiv.org/pdf/2512.06102",
        "title": "JaxWildfire: A GPU-Accelerated Wildfire Simulator for Reinforcement Learning",
        "authors": [
            "Ufuk Çakır",
            "Victor-Alexandru Darvariu",
            "Bruno Lacerda",
            "Nick Hawes"
        ],
        "comments": "To be presented at the NeurIPS 2025 Workshop on Machine Learning and the Physical Sciences (ML4PS)",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Artificial intelligence methods are increasingly being explored for managing wildfires and other natural hazards. In particular, reinforcement learning (RL) is a promising path towards improving outcomes in such uncertain decision-making scenarios and moving beyond reactive strategies. However, training RL agents requires many environment interactions, and the speed of existing wildfire simulators is a severely limiting factor. We introduce $\\texttt{JaxWildfire}$, a simulator underpinned by a principled probabilistic fire spread model based on cellular automata. It is implemented in JAX and enables vectorized simulations using $\\texttt{vmap}$, allowing high throughput of simulations on GPUs. We demonstrate that $\\texttt{JaxWildfire}$ achieves 6-35x speedup over existing software and enables gradient-based optimization of simulator parameters. Furthermore, we show that $\\texttt{JaxWildfire}$ can be used to train RL agents to learn wildfire suppression policies. Our work is an important step towards enabling the advancement of RL techniques for managing natural hazards.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 5,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06104",
        "abs_url": "https://arxiv.org/abs/2512.06104",
        "pdf_url": "https://arxiv.org/pdf/2512.06104",
        "title": "ARC-AGI Without Pretraining",
        "authors": [
            "Isaac Liao",
            "Albert Gu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Conventional wisdom in the age of LLMs dictates that solving IQ-test-like visual puzzles from the ARC-AGI-1 benchmark requires capabilities derived from massive pretraining. To counter this, we introduce CompressARC, a 76K parameter model without any pretraining that solves 20% of evaluation puzzles by minimizing the description length (MDL) of the target puzzle purely during inference time. The MDL endows CompressARC with extreme generalization abilities typically unheard of in deep learning. To our knowledge, CompressARC is the only deep learning method for ARC-AGI where training happens only on a single sample: the target inference puzzle itself, with the final solution information removed. Moreover, CompressARC does not train on the pre-provided ARC-AGI \"training set\". Under these extremely data-limited conditions, we do not ordinarily expect any puzzles to be solvable at all. Yet CompressARC still solves a diverse distribution of creative ARC-AGI puzzles, suggesting MDL to be an alternative feasible way to produce intelligence, besides conventional pretraining.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 6,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06111",
        "abs_url": "https://arxiv.org/abs/2512.06111",
        "pdf_url": "https://arxiv.org/pdf/2512.06111",
        "title": "A Prescriptive Framework for Determining Optimal Days for Short-Term Traffic Counts",
        "authors": [
            "Arthur Mukwaya",
            "Nancy Kasamala",
            "Nana Kankam Gyimah",
            "Judith Mwakalonge",
            "Gurcan Comert",
            "Saidi Siuhi",
            "Denis Ruganuza",
            "Mark Ngotonie"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The Federal Highway Administration (FHWA) mandates that state Departments of Transportation (DOTs) collect reliable Annual Average Daily Traffic (AADT) data. However, many U.S. DOTs struggle to obtain accurate AADT, especially for unmonitored roads. While continuous count (CC) stations offer accurate traffic volume data, their implementation is expensive and difficult to deploy widely, compelling agencies to rely on short-duration traffic counts. This study proposes a machine learning framework, the first to our knowledge, to identify optimal representative days for conducting short count (SC) data collection to improve AADT prediction accuracy. Using 2022 and 2023 traffic volume data from the state of Texas, we compare two scenarios: an 'optimal day' approach that iteratively selects the most informative days for AADT estimation and a 'no optimal day' baseline reflecting current practice by most DOTs. To align with Texas DOT's traffic monitoring program, continuous count data were utilized to simulate the 24 hour short counts. The actual field short counts were used to enhance feature engineering through using a leave-one-out (LOO) technique to generate unbiased representative daily traffic features across similar road segments. Our proposed methodology outperforms the baseline across the top five days, with the best day (Day 186) achieving lower errors (RMSE: 7,871.15, MAE: 3,645.09, MAPE: 11.95%) and higher R^2 (0.9756) than the baseline (RMSE: 11,185.00, MAE: 5,118.57, MAPE: 14.42%, R^2: 0.9499). This research offers DOTs an alternative to conventional short-duration count practices, improving AADT estimation, supporting Highway Performance Monitoring System compliance, and reducing the operational costs of statewide traffic data collection.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 7,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06143",
        "abs_url": "https://arxiv.org/abs/2512.06143",
        "pdf_url": "https://arxiv.org/pdf/2512.06143",
        "title": "gp2Scale: A Class of Compactly-Supported Non-Stationary Kernels and Distributed Computing for Exact Gaussian Processes on 10 Million Data Points",
        "authors": [
            "Marcus M. Noack",
            "Mark D. Risser",
            "Hengrui Luo",
            "Vardaan Tekriwal",
            "Ronald J. Pandolfi"
        ],
        "comments": "None",
        "subjects": "Machine Learning (cs.LG); Probability (math.PR)",
        "abstract": "Despite a large corpus of recent work on scaling up Gaussian processes, a stubborn trade-off between computational speed, prediction and uncertainty quantification accuracy, and customizability persists. This is because the vast majority of existing methodologies exploit various levels of approximations that lower accuracy and limit the flexibility of kernel and noise-model designs -- an unacceptable drawback at a time when expressive non-stationary kernels are on the rise in many fields. Here, we propose a methodology we term \\emph{gp2Scale} that scales exact Gaussian processes to more than 10 million data points without relying on inducing points, kernel interpolation, or neighborhood-based approximations, and instead leveraging the existing capabilities of a GP: its kernel design. Highly flexible, compactly supported, and non-stationary kernels lead to the identification of naturally occurring sparse structure in the covariance matrix, which is then exploited for the calculations of the linear system solution and the log-determinant for training. We demonstrate our method's functionality on several real-world datasets and compare it with state-of-the-art approximation algorithms. Although we show superior approximation performance in many cases, the method's real power lies in its agnosticism toward arbitrary GP customizations -- core kernel design, noise, and mean functions -- and the type of input space, making it optimally suited for modern Gaussian process applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 8,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06183",
        "abs_url": "https://arxiv.org/abs/2512.06183",
        "pdf_url": "https://arxiv.org/pdf/2512.06183",
        "title": "PMA-Diffusion: A Physics-guided Mask-Aware Diffusion Framework for TSE from Sparse Observations",
        "authors": [
            "Lindong Liu",
            "Zhixiong Jin",
            "Seongjin Choi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "High-resolution highway traffic state information is essential for Intelligent Transportation Systems, but typical traffic data acquired from loop detectors and probe vehicles are often too sparse and noisy to capture the detailed dynamics of traffic flow. We propose PMA-Diffusion, a physics-guided mask-aware diffusion framework that reconstructs unobserved highway speed fields from sparse, incomplete observations. Our approach trains a diffusion prior directly on sparsely observed speed fields using two mask-aware training strategies: Single-Mask and Double-Mask. At the inference phase, the physics-guided posterior sampler alternates reverse-diffusion updates, observation projection, and physics-guided projection based on adaptive anisotropic smoothing to reconstruct the missing speed fields. The proposed framework is tested on the I-24 MOTION dataset with varying visibility ratios. Even under severe sparsity, with only 5% visibility, PMA-Diffusion outperforms other baselines across three reconstruction error metrics. Furthermore, PMA-diffusion trained with sparse observation nearly matches the performance of the baseline model trained on fully observed speed fields. The results indicate that combining mask-aware diffusion priors with a physics-guided posterior sampler provides a reliable and flexible solution for traffic state estimation under realistic sensing sparsity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 9,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06200",
        "abs_url": "https://arxiv.org/abs/2512.06200",
        "pdf_url": "https://arxiv.org/pdf/2512.06200",
        "title": "How Should We Evaluate Data Deletion in Graph-Based ANN Indexes?",
        "authors": [
            "Tomohiro Yamashita",
            "Daichi Amagata",
            "Yusuke Matsui"
        ],
        "comments": "4 pages, 4 figures. Accepted at NeurIPS 2025 Workshop on Machine Learning for Systems",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Approximate Nearest Neighbor Search (ANNS) has recently gained significant attention due to its many applications, such as Retrieval-Augmented Generation. Such applications require ANNS algorithms that support dynamic data, so the ANNS problem on dynamic data has attracted considerable interest. However, a comprehensive evaluation methodology for data deletion in ANNS has yet to be established. This study proposes an experimental framework and comprehensive evaluation metrics to assess the efficiency of data deletion for ANNS indexes under practical use cases. Specifically, we categorize data deletion methods in graph-based ANNS into three approaches and formalize them mathematically. The performance is assessed in terms of accuracy, query speed, and other relevant metrics. Finally, we apply the proposed evaluation framework to Hierarchical Navigable Small World, one of the state-of-the-art ANNS methods, to analyze the effects of data deletion, and propose Deletion Control, a method which dynamically selects the appropriate deletion method under a required search accuracy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 10,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06201",
        "abs_url": "https://arxiv.org/abs/2512.06201",
        "pdf_url": "https://arxiv.org/pdf/2512.06201",
        "title": "K2-V2: A 360-Open, Reasoning-Enhanced LLM",
        "authors": [
            "K2 Team",
            "Zhengzhong Liu",
            "Liping Tang",
            "Linghao Jin",
            "Haonan Li",
            "Nikhil Ranjan",
            "Desai Fan",
            "Shaurya Rohatgi",
            "Richard Fan",
            "Omkar Pangarkar",
            "Huijuan Wang",
            "Zhoujun Cheng",
            "Suqi Sun",
            "Seungwook Han",
            "Bowen Tan",
            "Gurpreet Gosal",
            "Xudong Han",
            "Varad Pimpalkhute",
            "Shibo Hao",
            "Ming Shan Hee",
            "Joel Hestness",
            "Haolong Jia",
            "Liqun Ma",
            "Aaryamonvikram Singh",
            "Daria Soboleva",
            "Natalia Vassilieva",
            "Renxi Wang",
            "Yingquan Wu",
            "Yuekai Sun",
            "Taylor Killian",
            "Alexander Moreno",
            "John Maggs",
            "Hector Ren",
            "Guowei He",
            "Hongyi Wang",
            "Xuezhe Ma",
            "Yuqi Wang",
            "Mikhail Yurochkin",
            "Eric P. Xing"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We introduce K2-V2, a 360-open LLM built from scratch as a superior base for reasoning adaptation, in addition to functions such as conversation and knowledge retrieval from general LLMs. It stands as the strongest fully open model, rivals open-weight leaders in its size class, outperforms Qwen2.5-72B and approaches the performance of Qwen3-235B. We actively infuse domain knowledge, reasoning, long-context, and tool use throughout the training process. This explicitly prepares the model for complex reasoning tasks. We demonstrate this potential using simple supervised fine-tuning, establishing a strong baseline that indicates significant headroom for advanced alignment. By releasing the full training history and data composition, we maximize the effectiveness of continuous training, a key open source production scenario. We release the model weights and signature LLM360 artifacts, such as complete training data, to empower the community with a capable, reasoning-centric foundation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 11,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06218",
        "abs_url": "https://arxiv.org/abs/2512.06218",
        "pdf_url": "https://arxiv.org/pdf/2512.06218",
        "title": "Average-reward reinforcement learning in semi-Markov decision processes via relative value iteration",
        "authors": [
            "Huizhen Yu",
            "Yi Wan",
            "Richard S. Sutton"
        ],
        "comments": "24 pages. This paper presents the reinforcement-learning material previously contained in version 2 of arXiv:2409.03915, which is now being split into two stand-alone papers. Minor corrections and improvements to the main results have also been made in the course of this reformatting",
        "subjects": "Machine Learning (cs.LG); Optimization and Control (math.OC)",
        "abstract": "This paper applies the authors' recent results on asynchronous stochastic approximation (SA) in the Borkar-Meyn framework to reinforcement learning in average-reward semi-Markov decision processes (SMDPs). We establish the convergence of an asynchronous SA analogue of Schweitzer's classical relative value iteration algorithm, RVI Q-learning, for finite-space, weakly communicating SMDPs. In particular, we show that the algorithm converges almost surely to a compact, connected subset of solutions to the average-reward optimality equation, with convergence to a unique, sample path-dependent solution under additional stepsize and asynchrony conditions. Moreover, to make full use of the SA framework, we introduce new monotonicity conditions for estimating the optimal reward rate in RVI Q-learning. These conditions substantially expand the previously considered algorithmic framework and are addressed through novel arguments in the stability and convergence analysis of RVI Q-learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 12,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06236",
        "abs_url": "https://arxiv.org/abs/2512.06236",
        "pdf_url": "https://arxiv.org/pdf/2512.06236",
        "title": "Back to Author Console Empowering GNNs for Domain Adaptation via Denoising Target Graph",
        "authors": [
            "Haiyang Yu",
            "Meng-Chieh Lee",
            "Xiang song",
            "Qi Zhu",
            "Christos Faloutsos"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We explore the node classification task in the context of graph domain adaptation, which uses both source and target graph structures along with source labels to enhance the generalization capabilities of Graph Neural Networks (GNNs) on target graphs. Structure domain shifts frequently occur, especially when graph data are collected at different times or from varying areas, resulting in poor performance of GNNs on target graphs. Surprisingly, we find that simply incorporating an auxiliary loss function for denoising graph edges on target graphs can be extremely effective in enhancing GNN performance on target graphs. Based on this insight, we propose our framework, GraphDeT, a framework that integrates this auxiliary edge task into GNN training for node classification under domain adaptation. Our theoretical analysis connects this auxiliary edge task to the graph generalization bound with -distance, demonstrating such auxiliary task can imposes a constraint which tightens the bound and thereby improves generalization. The experimental results demonstrate superior performance compared to the existing baselines in handling both time and regional domain graph shifts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 13,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06243",
        "abs_url": "https://arxiv.org/abs/2512.06243",
        "pdf_url": "https://arxiv.org/pdf/2512.06243",
        "title": "Quantization Blindspots: How Model Compression Breaks Backdoor Defenses",
        "authors": [
            "Rohan Pandey",
            "Eric Ye"
        ],
        "comments": "10 pages",
        "subjects": "Machine Learning (cs.LG); Cryptography and Security (cs.CR)",
        "abstract": "Backdoor attacks embed input-dependent malicious behavior into neural networks while preserving high clean accuracy, making them a persistent threat for deployed ML systems. At the same time, real-world deployments almost never serve full-precision models: post-training quantization to INT8 or lower precision is now standard practice for reducing memory and latency. This work asks a simple question: how do existing backdoor defenses behave under standard quantization pipelines? We conduct a systematic empirical study of five representative defenses across three precision settings (FP32, INT8 dynamic, INT4 simulated) and two standard vision benchmarks using a canonical BadNet attack. We observe that INT8 quantization reduces the detection rate of all evaluated defenses to 0% while leaving attack success rates above 99%. For INT4, we find a pronounced dataset dependence: Neural Cleanse remains effective on GTSRB but fails on CIFAR-10, even though backdoors continue to survive quantization with attack success rates above 90%. Our results expose a mismatch between how defenses are commonly evaluated (on FP32 models) and how models are actually deployed (in quantized form), and they highlight quantization robustness as a necessary axis in future evaluations and designs of backdoor defenses.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 14,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06250",
        "abs_url": "https://arxiv.org/abs/2512.06250",
        "pdf_url": "https://arxiv.org/pdf/2512.06250",
        "title": "Learning When to Switch: Adaptive Policy Selection via Reinforcement Learning",
        "authors": [
            "Chris Tava"
        ],
        "comments": "7 pages",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Autonomous agents often require multiple strategies to solve complex tasks, but determining when to switch between strategies remains challenging. This research introduces a reinforcement learning technique to learn switching thresholds between two orthogonal navigation policies. Using maze navigation as a case study, this work demonstrates how an agent can dynamically transition between systematic exploration (coverage) and goal-directed pathfinding (convergence) to improve task performance. Unlike fixed-threshold approaches, the agent uses Q-learning to adapt switching behavior based on coverage percentage and distance to goal, requiring only minimal domain knowledge: maze dimensions and target location. The agent does not require prior knowledge of wall positions, optimal threshold values, or hand-crafted heuristics; instead, it discovers effective switching strategies dynamically during each run. The agent discretizes its state space into coverage and distance buckets, then adapts which coverage threshold (20-60\\%) to apply based on observed progress signals. Experiments across 240 test configurations (4 maze sizes from 16$\\times$16 to 128$\\times$128 $\\times$ 10 unique mazes $\\times$ 6 agent variants) demonstrate that adaptive threshold learning outperforms both single-strategy agents and fixed 40\\% threshold baselines. Results show 23-55\\% improvements in completion time, 83\\% reduction in runtime variance, and 71\\% improvement in worst-case scenarios. The learned switching behavior generalizes within each size class to unseen wall configurations. Performance gains scale with problem complexity: 23\\% improvement for 16$\\times$16 mazes, 34\\% for 32$\\times$32, and 55\\% for 64$\\times$64, demonstrating that as the space of possible maze structures grows, the value of adaptive policy selection over fixed heuristics increases proportionally.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 15,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06252",
        "abs_url": "https://arxiv.org/abs/2512.06252",
        "pdf_url": "https://arxiv.org/pdf/2512.06252",
        "title": "Learning Without Time-Based Embodiment Resets in Soft-Actor Critic",
        "authors": [
            "Homayoon Farrahi",
            "A. Rupam Mahmood"
        ],
        "comments": "In Proceedings of the 4th Conference on Lifelong Learning Agents (CoLLAs)",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "When creating new reinforcement learning tasks, practitioners often accelerate the learning process by incorporating into the task several accessory components, such as breaking the environment interaction into independent episodes and frequently resetting the environment. Although they can enable the learning of complex intelligent behaviors, such task accessories can result in unnatural task setups and hinder long-term performance in the real world. In this work, we explore the challenges of learning without episode terminations and robot embodiment resets using the Soft Actor-Critic (SAC) algorithm. To learn without terminations, we present a continuing version of the SAC algorithm and show that, with simple modifications to the reward functions of existing tasks, continuing SAC can perform as well as or better than episodic SAC while reducing the sensitivity of performance to the value of the discount rate $\\gamma$. On a modified Gym Reacher task, we investigate possible explanations for the failure of continuing SAC when learning without embodiment resets. Our results suggest that embodiment resets help with exploration of the state space in the SAC algorithm, and removing embodiment resets can lead to poor exploration of the state space and failure of or significantly slower learning. Finally, on additional simulated tasks and a real-robot vision task, we show that increasing the entropy of the policy when performance trends worse or remains static is an effective intervention for recovering the performance lost due to not using embodiment resets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 16,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06288",
        "abs_url": "https://arxiv.org/abs/2512.06288",
        "pdf_url": "https://arxiv.org/pdf/2512.06288",
        "title": "Theoretical Compression Bounds for Wide Multilayer Perceptrons",
        "authors": [
            "Houssam El Cheairi",
            "David Gamarnik",
            "Rahul Mazumder"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Statistics Theory (math.ST)",
        "abstract": "Pruning and quantization techniques have been broadly successful in reducing the number of parameters needed for large neural networks, yet theoretical justification for their empirical success falls short. We consider a randomized greedy compression algorithm for pruning and quantization post-training and use it to rigorously show the existence of pruned/quantized subnetworks of multilayer perceptrons (MLPs) with competitive performance. We further extend our results to structured pruning of MLPs and convolutional neural networks (CNNs), thus providing a unified analysis of pruning in wide networks. Our results are free of data assumptions, and showcase a tradeoff between compressibility and network width. The algorithm we consider bears some similarities with Optimal Brain Damage (OBD) and can be viewed as a post-training randomized version of it. The theoretical results we derive bridge the gap between theory and application for pruning/quantization, and provide a justification for the empirical success of compression in wide multilayer perceptrons.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 17,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06293",
        "abs_url": "https://arxiv.org/abs/2512.06293",
        "pdf_url": "https://arxiv.org/pdf/2512.06293",
        "title": "Importance-aware Topic Modeling for Discovering Public Transit Risk from Noisy Social Media",
        "authors": [
            "Fatima Ashraf",
            "Muhammad Ayub Sabir",
            "Jiaxin Deng",
            "Junbiao Pang",
            "Haitao Yu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Urban transit agencies increasingly turn to social media to monitor emerging service risks such as crowding, delays, and safety incidents, yet the signals of concern are sparse, short, and easily drowned by routine chatter. We address this challenge by jointly modeling linguistic interactions and user influence. First, we construct an influence-weighted keyword co-occurrence graph from cleaned posts so that socially impactful posts contributes proportionally to the underlying evidence. The core of our framework is a Poisson Deconvolution Factorization (PDF) that decomposes this graph into a low-rank topical structure and topic-localized residual interactions, producing an interpretable topic--keyword basis together with topic importance scores. A decorrelation regularizer \\emph{promotes} distinct topics, and a lightweight optimization procedure ensures stable convergence under nonnegativity and normalization constraints. Finally, the number of topics is selected through a coherence-driven sweep that evaluates the quality and distinctness of the learned topics. On large-scale social streams, the proposed model achieves state-of-the-art topic coherence and strong diversity compared with leading baselines. The code and dataset are publicly available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 18,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06301",
        "abs_url": "https://arxiv.org/abs/2512.06301",
        "pdf_url": "https://arxiv.org/pdf/2512.06301",
        "title": "Chemistry Integrated Language Model using Hierarchical Molecular Representation for Polymer Informatics",
        "authors": [
            "Jihun Ahn",
            "Gabriella Pasya Irianti",
            "Vikram Thapar",
            "Su-Mi Hur"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Machine learning has transformed material discovery for inorganic compounds and small molecules, yet polymers remain largely inaccessible to these methods. While data scarcity is often cited as the primary bottleneck, we demonstrate that strategic molecular representations can overcome this limitation. We introduce CI-LLM (Chemically Informed Language Model), a framework combining HAPPY (Hierarchically Abstracted rePeat unit of PolYmer), which encodes chemical substructures as tokens, with numerical descriptors within transformer architectures. For property prediction, De$^3$BERTa, our descriptor-enriched encoder, achieves 3.5x faster inference than SMILES-based models with improved accuracy ($R^2$ score gains of 0.9-4.1 percent across four properties), while providing interpretable structure-property insights at the subgroup level. For inverse design, our GPT-based generator produces polymers with targeted properties, achieving 100 percent scaffold retention and successful multi-property optimization for negatively correlated objectives. This comprehensive framework demonstrates both forward prediction and inverse design capabilities, showcasing how strategic molecular representation advances machine learning applications in polymer science.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 19,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06303",
        "abs_url": "https://arxiv.org/abs/2512.06303",
        "pdf_url": "https://arxiv.org/pdf/2512.06303",
        "title": "Multimodal Graph Neural Networks for Prognostic Modeling of Brain Network Reorganization",
        "authors": [
            "Preksha Girish",
            "Rachana Mysore",
            "Kiran K. N.",
            "Hiranmayee R.",
            "Shipra Prashanth",
            "Shrey Kumar"
        ],
        "comments": "5 pages, 2 figures. IEEE conference-style format",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Understanding the dynamic reorganization of brain networks is critical for predicting cognitive decline, neurological progression, and individual variability in clinical outcomes. This work proposes a multimodal graph neural network framework that integrates structural MRI, diffusion tensor imaging, and functional MRI to model spatiotemporal brain network reorganization. Brain regions are represented as nodes and structural and functional connectivity as edges, forming longitudinal brain graphs for each subject. Temporal evolution is captured via fractional stochastic differential operators embedded within graph-based recurrent networks, enabling the modeling of long-term dependencies and stochastic fluctuations in network dynamics. Attention mechanisms fuse multimodal information and generate interpretable biomarkers, including network energy entropy, graph curvature, fractional memory indices, and modality-specific attention scores. These biomarkers are combined into a composite prognostic index to quantify individual risk of network instability or cognitive decline. Experiments on longitudinal neuroimaging datasets demonstrate both predictive accuracy and interpretability. The results highlight the potential of mathematically rigorous, multimodal graph-based approaches for deriving clinically meaningful biomarkers from existing imaging data without requiring new data collection.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 20,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06341",
        "abs_url": "https://arxiv.org/abs/2512.06341",
        "pdf_url": "https://arxiv.org/pdf/2512.06341",
        "title": "Interpretive Efficiency: Information-Geometric Foundations of Data Usefulness",
        "authors": [
            "Ronald Katende"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Information Retrieval (cs.IR); Information Theory (cs.IT)",
        "abstract": "Interpretability is central to trustworthy machine learning, yet existing metrics rarely quantify how effectively data support an interpretive representation. We propose Interpretive Efficiency, a normalized, task-aware functional that measures the fraction of task-relevant information transmitted through an interpretive channel. The definition is grounded in five axioms ensuring boundedness, Blackwell-style monotonicity, data-processing stability, admissible invariance, and asymptotic consistency. We relate the functional to mutual information and derive a local Fisher-geometric expansion, then establish asymptotic and finite-sample estimation guarantees using standard empirical-process tools. Experiments on controlled image and signal tasks demonstrate that the measure recovers theoretical orderings, exposes representational redundancy masked by accuracy, and correlates with robustness, making it a practical, theory-backed diagnostic for representation design.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 21,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06343",
        "abs_url": "https://arxiv.org/abs/2512.06343",
        "pdf_url": "https://arxiv.org/pdf/2512.06343",
        "title": "When Distance Distracts: Representation Distance Bias in BT-Loss for Reward Models",
        "authors": [
            "Tong Xie",
            "Andrew Bai",
            "Yuanhao Ban",
            "Yunqi Hong",
            "Haoyu Li",
            "Cho-jui Hsieh"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Reward models are central to Large Language Model (LLM) alignment within the framework of RLHF. The standard objective used in reward modeling is the Bradley-Terry (BT) loss, which learns from pairwise data consisting of a pair of chosen and rejected responses. In this work, we analyze the per-sample gradient of BT-loss and show that its norm scales with two distinct components: (1) the difference in predicted rewards between chosen and rejected responses, which reflects the prediction error, and critically, (2) representation distance between the pair measured in the output space of the final layer. While the first term captures the intended training signal, we show that the second term can significantly impact the update magnitude and misalign learning. Specifically, pairs with small representation distance often receive vanishingly weak updates, even when misranked, while pairs with large distance receive disproportionately strong updates. This leads to gradients from large-distance pairs to overshadow those from small-distance pairs, where fine-grained distinctions are especially important. To overcome this limitation, we propose NormBT, an adaptive pair-wise normalization scheme that balances representation-driven effects and focuses learning signals on prediction error. NormBT is a lightweight, drop-in integration to BT loss with negligible overhead. Across various LLM backbones and datasets, NormBT improves reward model performance consistently, with notable gains of over 5% on the Reasoning category of RewardBench, which contains numerous small-distance pairs. This work reveals a key limitation in the widely used BT objective and provides a simple, effective correction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 22,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06347",
        "abs_url": "https://arxiv.org/abs/2512.06347",
        "pdf_url": "https://arxiv.org/pdf/2512.06347",
        "title": "Zero Generalization Error Theorem for Random Interpolators via Algebraic Geometry",
        "authors": [
            "Naoki Yoshida",
            "Isao Ishikawa",
            "Masaaki Imaizumi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "We theoretically demonstrate that the generalization error of interpolators for machine learning models under teacher-student settings becomes 0 once the number of training samples exceeds a certain threshold. Understanding the high generalization ability of large-scale models such as deep neural networks (DNNs) remains one of the central open problems in machine learning theory. While recent theoretical studies have attributed this phenomenon to the implicit bias of stochastic gradient descent (SGD) toward well-generalizing solutions, empirical evidences indicate that it primarily stems from properties of the model itself. Specifically, even randomly sampled interpolators, which are parameters that achieve zero training error, have been observed to generalize effectively. In this study, under a teacher-student framework, we prove that the generalization error of randomly sampled interpolators becomes exactly zero once the number of training samples exceeds a threshold determined by the geometric structure of the interpolator set in parameter space. As a proof technique, we leverage tools from algebraic geometry to mathematically characterize this geometric structure.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 23,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06351",
        "abs_url": "https://arxiv.org/abs/2512.06351",
        "pdf_url": "https://arxiv.org/pdf/2512.06351",
        "title": "LLM-Upgraded Graph Reinforcement Learning for Carbon-Aware Job Scheduling in Smart Manufacturing",
        "authors": [
            "Zhiying Yang",
            "Fang Liu",
            "Wei Zhang",
            "Xin Lou",
            "Malcolm Yoke Hean Low",
            "Boon Ping Gan"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
        "abstract": "This paper presents \\textsc{Luca}, a \\underline{l}arge language model (LLM)-\\underline{u}pgraded graph reinforcement learning framework for \\underline{c}arbon-\\underline{a}ware flexible job shop scheduling. \\textsc{Luca} addresses the challenges of dynamic and sustainable scheduling in smart manufacturing systems by integrating a graph neural network and an LLM, guided by a carefully designed in-house prompting strategy, to produce a fused embedding that captures both structural characteristics and contextual semantics of the latest scheduling state. This expressive embedding is then processed by a deep reinforcement learning policy network, which generates real-time scheduling decisions optimized for both makespan and carbon emission objectives. To support sustainability goals, \\textsc{Luca} incorporates a dual-objective reward function that encourages both energy efficiency and scheduling timeliness. Experimental results on both synthetic and public datasets demonstrate that \\textsc{Luca} consistently outperforms comparison algorithms. For instance, on the synthetic dataset, it achieves an average of 4.1\\% and up to 12.2\\% lower makespan compared to the best-performing comparison algorithm while maintaining the same emission level. On public datasets, additional gains are observed for both makespan and emission. These results demonstrate that \\textsc{Luca} is effective and practical for carbon-aware scheduling in smart manufacturing.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 24,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06356",
        "abs_url": "https://arxiv.org/abs/2512.06356",
        "pdf_url": "https://arxiv.org/pdf/2512.06356",
        "title": "DDFI: Diverse and Distribution-aware Missing Feature Imputation via Two-step Reconstruction",
        "authors": [
            "Yifan Song",
            "Fenglin Yu",
            "Yihong Luo",
            "Xingjian Tao",
            "Siya Qiu",
            "Kai Han",
            "Jing Tang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Social and Information Networks (cs.SI)",
        "abstract": "Incomplete node features are ubiquitous in real-world scenarios, e.g., the attributes of web users may be partly private, which causes the performance of Graph Neural Networks (GNNs) to decline significantly. Feature propagation (FP) is a well-known method that performs well for imputation of missing node features on graphs, but it still has the following three issues: 1) it struggles with graphs that are not fully connected, 2) imputed features face the over-smoothing problem, and 3) FP is tailored for transductive tasks, overlooking the feature distribution shift in inductive tasks. To address these challenges, we introduce DDFI, a Diverse and Distribution-aware Missing Feature Imputation method that combines feature propagation with a graph-based Masked AutoEncoder (MAE) in a nontrivial manner. It first designs a simple yet effective algorithm, namely Co-Label Linking (CLL), that randomly connects nodes in the training set with the same label to enhance the performance on graphs with numerous connected components. Then we develop a novel two-step representation generation process at the inference stage. Specifically, instead of directly using FP-imputed features as input during inference, DDFI further reconstructs the features through the whole MAE to reduce feature distribution shift in the inductive tasks and enhance the diversity of node features. Meanwhile, since existing feature imputation methods for graphs only evaluate by simulating the missing scenes with manually masking the features, we collect a new dataset called Sailing from the records of voyages that contains naturally missing features to help better evaluate the effectiveness. Extensive experiments conducted on six public datasets and Sailing show that DDFI outperforms the state-of-the-art methods under both transductive and inductive settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 25,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06370",
        "abs_url": "https://arxiv.org/abs/2512.06370",
        "pdf_url": "https://arxiv.org/pdf/2512.06370",
        "title": "Optimizing Optimizers for Fast Gradient-Based Learning",
        "authors": [
            "Jaerin Lee",
            "Kyoung Mu Lee"
        ],
        "comments": "49 pages, 5 figures",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "We lay the theoretical foundation for automating optimizer design in gradient-based learning. Based on the greedy principle, we formulate the problem of designing optimizers as maximizing the instantaneous decrease in loss. By treating an optimizer as a function that translates loss gradient signals into parameter motions, the problem reduces to a family of convex optimization problems over the space of optimizers. Solving these problems under various constraints not only recovers a wide range of popular optimizers as closed-form solutions, but also produces the optimal hyperparameters of these optimizers with respect to the problems at hand. This enables a systematic approach to design optimizers and tune their hyperparameters according to the gradient statistics that are collected during the training process. Furthermore, this optimization of optimization can be performed dynamically during training.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 26,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06417",
        "abs_url": "https://arxiv.org/abs/2512.06417",
        "pdf_url": "https://arxiv.org/pdf/2512.06417",
        "title": "Hankel-FNO: Fast Underwater Acoustic Charting Via Physics-Encoded Fourier Neural Operator",
        "authors": [
            "Yifan Sun",
            "Lei Cheng",
            "Jianlong Li",
            "Peter Gerstoft"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Sound (cs.SD)",
        "abstract": "Fast and accurate underwater acoustic charting is crucial for downstream tasks such as environment-aware sensor placement optimization and autonomous vehicle path planning. Conventional methods rely on computationally expensive while accurate numerical solvers, which are not scalable for large-scale or real-time applications. Although deep learning-based surrogate models can accelerate these computations, they often suffer from limitations such as fixed-resolution constraints or dependence on explicit partial differential equation formulations. These issues hinder their applicability and generalization across diverse environments. We propose Hankel-FNO, a Fourier Neural Operator (FNO)-based model for efficient and accurate acoustic charting. By incorporating sound propagation knowledge and bathymetry, our method has high accuracy while maintaining high computational speed. Results demonstrate that Hankel-FNO outperforms traditional solvers in speed and surpasses data-driven alternatives in accuracy, especially in long-range predictions. Experiments show the model's adaptability to diverse environments and sound source settings with minimal fine-tuning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 27,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06427",
        "abs_url": "https://arxiv.org/abs/2512.06427",
        "pdf_url": "https://arxiv.org/pdf/2512.06427",
        "title": "A new initialisation to Control Gradients in Sinusoidal Neural network",
        "authors": [
            "Andrea Combette",
            "Antoine Venaille",
            "Nelly Pustelnik"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Proper initialisation strategy is of primary importance to mitigate gradient explosion or vanishing when training neural networks. Yet, the impact of initialisation parameters still lacks a precise theoretical understanding for several well-established architectures. Here, we propose a new initialisation for networks with sinusoidal activation functions such as \\texttt{SIREN}, focusing on gradients control, their scaling with network depth, their impact on training and on generalization. To achieve this, we identify a closed-form expression for the initialisation of the parameters, differing from the original \\texttt{SIREN} scheme. This expression is derived from fixed points obtained through the convergence of pre-activation distribution and the variance of Jacobian sequences. Controlling both gradients and targeting vanishing pre-activation helps preventing the emergence of inappropriate frequencies during estimation, thereby improving generalization. We further show that this initialisation strongly influences training dynamics through the Neural Tangent Kernel framework (NTK). Finally, we benchmark \\texttt{SIREN} with the proposed initialisation against the original scheme and other baselines on function fitting and image reconstruction. The new initialisation consistently outperforms state-of-the-art methods across a wide range of reconstruction tasks, including those involving physics-informed neural networks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 28,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06440",
        "abs_url": "https://arxiv.org/abs/2512.06440",
        "pdf_url": "https://arxiv.org/pdf/2512.06440",
        "title": "Neural expressiveness for beyond importance model compression",
        "authors": [
            "Angelos-Christos Maroudis",
            "Sotirios Xydis"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Neural Network Pruning has been established as driving force in the exploration of memory and energy efficient solutions with high throughput both during training and at test time. In this paper, we introduce a novel criterion for model compression, named \"Expressiveness\". Unlike existing pruning methods that rely on the inherent \"Importance\" of neurons' and filters' weights, ``Expressiveness\" emphasizes a neuron's or group of neurons ability to redistribute informational resources effectively, based on the overlap of activations. This characteristic is strongly correlated to a network's initialization state, establishing criterion autonomy from the learning state stateless and thus setting a new fundamental basis for the expansion of compression strategies in regards to the \"When to Prune\" question. We show that expressiveness is effectively approximated with arbitrary data or limited dataset's representative samples, making ground for the exploration of Data-Agnostic strategies. Our work also facilitates a \"hybrid\" formulation of expressiveness and importance-based pruning strategies, illustrating their complementary benefits and delivering up to 10x extra gains w.r.t. weight-based approaches in parameter compression ratios, with an average of 1% in performance degradation. We also show that employing expressiveness (independently) for pruning leads to an improvement over top-performing and foundational methods in terms of compression efficiency. Finally, on YOLOv8, we achieve a 46.1% MACs reduction by removing 55.4\\% of the parameters, with an increase of 3% in the mean Absolute Precision ($mAP_{50-95}$) for object detection on COCO dataset.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 29,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06457",
        "abs_url": "https://arxiv.org/abs/2512.06457",
        "pdf_url": "https://arxiv.org/pdf/2512.06457",
        "title": "BitStopper: An Efficient Transformer Attention Accelerator via Stage-fusion and Early Termination",
        "authors": [
            "Huizheng Wang",
            "Hongbin Wang",
            "Shaojun Wei",
            "Yang Hu",
            "Shouyi Yin"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Signal Processing (eess.SP)",
        "abstract": "Attention-based large language models (LLMs) have transformed modern AI applications, but the quadratic cost of self-attention imposes significant compute and memory overhead. Dynamic sparsity (DS) attention mitigates this, yet its hardware efficiency is limited by the added prediction stage and the heavy memory traffic it entails. To address these limitations, this paper proposes BitStopper, a fine-grained algorithm-architecture co-design that operates without a sparsity predictor. First, a bit-serial enable stage fusion (BESF) mechanism is proposed to reuse and minimize the memory access by progressively terminating trivial tokens and merging the prediction stage into the execution stage. Second, a lightweight and adaptive token selection (LATS) strategy is developed to work in concert with the bit-level sparsity speculation. Third, a bit-level asynchronous processing (BAP) strategy is employed to improve compute utilization during the on-demand bit-grained memory fetching. Finally, an elaborate architecture is designed to translate the theoretical complexity reduction into practical performance improvement. Extensive evaluations demonstrate that, compared to state-of-the-art (SOTA) Transformer accelerators, BitStopper achieves 2.03x and 1.89x speedups over Sanger and SOFA, respectively, while delivering 2.4x and 2.1x improvements in energy efficiency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 30,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06490",
        "abs_url": "https://arxiv.org/abs/2512.06490",
        "pdf_url": "https://arxiv.org/pdf/2512.06490",
        "title": "Optimizing LLMs Using Quantization for Mobile Execution",
        "authors": [
            "Agatsya Yadav",
            "Renta Chintala Bhargavi"
        ],
        "comments": "11 pages, 1 equation, 2 tables. Author Accepted Manuscript (AAM) of a paper published in Springer LNNS, ICT4SD 2025. DOI: https://doi.org/10.1007/978-3-032-06697-8_33",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Large Language Models (LLMs) offer powerful capabilities, but their significant size and computational requirements hinder deployment on resource-constrained mobile devices. This paper investigates Post-Training Quantization (PTQ) for compressing LLMs for mobile execution. We apply 4-bit PTQ using the BitsAndBytes library with the Hugging Face Transformers framework to Meta's Llama 3.2 3B model. The quantized model is converted to GGUF format using this http URL tools for optimized mobile inference. The PTQ workflow achieves a 68.66% reduction in model size through 4-bit quantization, enabling the Llama 3.2 3B model to run efficiently on an Android device. Qualitative validation shows that the 4-bit quantized model can perform inference tasks successfully. We demonstrate the feasibility of running the quantized GGUF model on an Android device using the Termux environment and the Ollama framework. PTQ, especially at 4-bit precision combined with mobile-optimized formats like GGUF, provides a practical pathway for deploying capable LLMs on mobile devices, balancing model size and performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 31,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06511",
        "abs_url": "https://arxiv.org/abs/2512.06511",
        "pdf_url": "https://arxiv.org/pdf/2512.06511",
        "title": "Diagnosis-based mortality prediction for intensive care unit patients via transfer learning",
        "authors": [
            "Mengqi Xu",
            "Subha Maity",
            "Joel Dubin"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Applications (stat.AP)",
        "abstract": "In the intensive care unit, the underlying causes of critical illness vary substantially across diagnoses, yet prediction models accounting for diagnostic heterogeneity have not been systematically studied. To address the gap, we evaluate transfer learning approaches for diagnosis-specific mortality prediction and apply both GLM- and XGBoost-based models to the eICU Collaborative Research Database. Our results demonstrate that transfer learning consistently outperforms models trained only on diagnosis-specific data and those using a well-known ICU severity-of-illness score, i.e., APACHE IVa, alone, while also achieving better calibration than models trained on the pooled data. Our findings also suggest that the Youden cutoff is a more appropriate decision threshold than the conventional 0.5 for binary outcomes, and that transfer learning maintains consistently high predictive performance across various cutoff criteria.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 32,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06520",
        "abs_url": "https://arxiv.org/abs/2512.06520",
        "pdf_url": "https://arxiv.org/pdf/2512.06520",
        "title": "Hierarchical geometric deep learning enables scalable analysis of molecular dynamics",
        "authors": [
            "Zihan Pengmei",
            "Spencer C. Guo",
            "Chatipat Lorpaiboon",
            "Aaron R. Dinner"
        ],
        "comments": "17 pages, 12 figures",
        "subjects": "Machine Learning (cs.LG); Statistical Mechanics (cond-mat.stat-mech); Computational Physics (physics.comp-ph); Data Analysis, Statistics and Probability (physics.data-an)",
        "abstract": "Molecular dynamics simulations can generate atomically detailed trajectories of complex systems, but analyzing these dynamics can be challenging when systems lack well-established quantitative descriptors (features). Graph neural networks (GNNs) in which messages are passed between nodes that represent atoms that are spatial neighbors promise to obviate manual feature engineering, but the use of GNNs with biomolecular systems of more than a few hundred residues has been limited in the context of analyzing dynamics by both difficulties in capturing the details of long-range interactions with message passing and the memory and runtime requirements associated with large graphs. Here, we show how local information can be aggregated to reduce memory and runtime requirements without sacrificing atomic detail. We demonstrate that this approach opens the door to analyzing simulations of protein-nucleic acid complexes with thousands of residues on single GPUs within minutes. For systems with hundreds of residues, for which there are sufficient data to make quantitative comparisons, we show that the approach improves performance and interpretability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 33,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06547",
        "abs_url": "https://arxiv.org/abs/2512.06547",
        "pdf_url": "https://arxiv.org/pdf/2512.06547",
        "title": "A-3PO: Accelerating Asynchronous LLM Training with Staleness-aware Proximal Policy Approximation",
        "authors": [
            "Xiaocan Li",
            "Shiliang Wu",
            "Zheng Shen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)",
        "abstract": "Decoupled loss has been a successful reinforcement learning (RL) algorithm to deal with the high data staleness under the asynchronous RL setting. Decoupled loss improves coupled-loss style of algorithms' (e.g., PPO, GRPO) learning stability by introducing a proximal policy to decouple the off-policy corrections (importance weight) from the controlling policy updates (trust region). However, the proximal policy requires an extra forward pass through the network at each training step, creating a computational bottleneck for large language models. We observe that since the proximal policy only serves as a trust region anchor between the behavior and target policies, we can approximate it through simple interpolation without explicit computation. We call this approach A-3PO (APproximated Proximal Policy Optimization). A-3PO eliminates this overhead, reducing training time by 18% while maintaining comparable performance. Code & off-the-shelf example are available at: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 34,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06563",
        "abs_url": "https://arxiv.org/abs/2512.06563",
        "pdf_url": "https://arxiv.org/pdf/2512.06563",
        "title": "Deep Manifold Part 2: Neural Network Mathematics",
        "authors": [
            "Max Y. Ma",
            "Gen-Hua Shi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "This work develops the global equations of neural networks through stacked piecewise manifolds, fixed--point theory, and boundary--conditioned iteration. Once fixed coordinates and operators are removed, a neural network appears as a learnable numerical computation shaped by manifold complexity, high--order nonlinearity, and boundary conditions. Real--world data impose strong data complexity, near-infinite scope, scale, and minibatch fragmentation, while training dynamics produce learning complexity through shifting node covers, curvature accumulation, and the rise and decay of plasticity. These forces constrain learnability and explain why capability emerges only when fixed--point regions stabilize. Neural networks do not begin with fixed points; they construct them through residual--driven iteration. This perspective clarifies the limits of monolithic models under geometric and data--induced plasticity and motivates architectures and federated systems that distribute manifold complexity across many elastic models, forming a coherent world--modeling framework grounded in geometry, algebra, fixed points, and real--data complexity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 35,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06592",
        "abs_url": "https://arxiv.org/abs/2512.06592",
        "pdf_url": "https://arxiv.org/pdf/2512.06592",
        "title": "On fine-tuning Boltz-2 for protein-protein affinity prediction",
        "authors": [
            "James King",
            "Lewis Cornwall",
            "Andrei Cristian Nica",
            "James Day",
            "Aaron Sim",
            "Neil Dalchau",
            "Lilly Wollman",
            "Joshua Meyers"
        ],
        "comments": "MLSB 2025",
        "subjects": "Machine Learning (cs.LG); Biomolecules (q-bio.BM)",
        "abstract": "Accurate prediction of protein-protein binding affinity is vital for understanding molecular interactions and designing therapeutics. We adapt Boltz-2, a state-of-the-art structure-based protein-ligand affinity predictor, for protein-protein affinity regression and evaluate it on two datasets, TCR3d and PPB-affinity. Despite high structural accuracy, Boltz-2-PPI underperforms relative to sequence-based alternatives in both small- and larger-scale data regimes. Combining embeddings from Boltz-2-PPI with sequence-based embeddings yields complementary improvements, particularly for weaker sequence models, suggesting different signals are learned by sequence- and structure-based models. Our results echo known biases associated with training with structural data and suggest that current structure-based representations are not primed for performant affinity prediction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 36,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06607",
        "abs_url": "https://arxiv.org/abs/2512.06607",
        "pdf_url": "https://arxiv.org/pdf/2512.06607",
        "title": "A Fast and Effective Solution to the Problem of Look-ahead Bias in LLMs",
        "authors": [
            "Humzah Merchant",
            "Bradford Levy"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
        "abstract": "Applying LLMs to predictive tasks in finance is challenging due to look-ahead bias resulting from their training on long time-series data. This precludes the backtests typically employed in finance since retraining frontier models from scratch with a specific knowledge cutoff is prohibitive. In this paper, we introduce a fast, effective, and low-cost alternative. Our method guides generation at inference time by adjusting the logits of a large base model using a pair of smaller, specialized models -- one fine-tuned on information to be forgotten and another on information to be retained. We demonstrate that our method effectively removes both verbatim and semantic knowledge, corrects biases, and outperforms prior methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 37,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06630",
        "abs_url": "https://arxiv.org/abs/2512.06630",
        "pdf_url": "https://arxiv.org/pdf/2512.06630",
        "title": "Quantum Temporal Convolutional Neural Networks for Cross-Sectional Equity Return Prediction: A Comparative Benchmark Study",
        "authors": [
            "Chi-Sheng Chen",
            "Xinyu Zhang",
            "Rong Fu",
            "Qiuzhe Xie",
            "Fan Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Quantum Physics (quant-ph)",
        "abstract": "Quantum machine learning offers a promising pathway for enhancing stock market prediction, particularly under complex, noisy, and highly dynamic financial environments. However, many classical forecasting models struggle with noisy input, regime shifts, and limited generalization capacity. To address these challenges, we propose a Quantum Temporal Convolutional Neural Network (QTCNN) that combines a classical temporal encoder with parameter-efficient quantum convolution circuits for cross-sectional equity return prediction. The temporal encoder extracts multi-scale patterns from sequential technical indicators, while the quantum processing leverages superposition and entanglement to enhance feature representation and suppress overfitting. We conduct a comprehensive benchmarking study on the JPX Tokyo Stock Exchange dataset and evaluate predictions through long-short portfolio construction using out-of-sample Sharpe ratio as the primary performance metric. QTCNN achieves a Sharpe ratio of 0.538, outperforming the best classical baseline by approximately 72\\%. These results highlight the practical potential of quantum-enhanced forecasting model, QTCNN, for robust decision-making in quantitative finance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 38,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06638",
        "abs_url": "https://arxiv.org/abs/2512.06638",
        "pdf_url": "https://arxiv.org/pdf/2512.06638",
        "title": "The Impact of Data Characteristics on GNN Evaluation for Detecting Fake News",
        "authors": [
            "Isha Karn",
            "David Jensen"
        ],
        "comments": "Preprint. Approximately 15 pages, 5 figures, 3 tables",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Graph neural networks (GNNs) are widely used for the detection of fake news by modeling the content and propagation structure of news articles on social media. We show that two of the most commonly used benchmark data sets - GossipCop and PolitiFact - are poorly suited to evaluating the utility of models that use propagation structure. Specifically, these data sets exhibit shallow, ego-like graph topologies that provide little or no ability to differentiate among modeling methods. We systematically benchmark five GNN architectures against a structure-agnostic multilayer perceptron (MLP) that uses the same node features. We show that MLPs match or closely trail the performance of GNNs, with performance gaps often within 1-2% and overlapping confidence intervals. To isolate the contribution of structure in these datasets, we conduct controlled experiments where node features are shuffled or edge structures randomized. We find that performance collapses under feature shuffling but remains stable under edge randomization. This suggests that structure plays a negligible role in these benchmarks. Structural analysis further reveals that over 75% of nodes are only one hop from the root, exhibiting minimal structural diversity. In contrast, on synthetic datasets where node features are noisy and structure is informative, GNNs significantly outperform MLPs. These findings provide strong evidence that widely used benchmarks do not meaningfully test the utility of modeling structural features, and they motivate the development of datasets with richer, more diverse graph topologies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 39,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06649",
        "abs_url": "https://arxiv.org/abs/2512.06649",
        "pdf_url": "https://arxiv.org/pdf/2512.06649",
        "title": "Estimating Black Carbon Concentration from Urban Traffic Using Vision-Based Machine Learning",
        "authors": [
            "Camellia Zakaria",
            "Aryan Sadeghi",
            "Weaam Jaafar",
            "Junshi Xu",
            "Alex Mariakakis",
            "Marianne Hatzopoulou"
        ],
        "comments": "12 pages, 16 figures, 4 tables, 4 pages Appendix, in submission and under review for ACM MobiSys 2026 as of December 6th, 2025",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV); Computers and Society (cs.CY); Emerging Technologies (cs.ET)",
        "abstract": "Black carbon (BC) emissions in urban areas are primarily driven by traffic, with hotspots near major roads disproportionately affecting marginalized communities. Because BC monitoring is typically performed using costly and specialized instruments. there is little to no available data on BC from local traffic sources that could help inform policy interventions targeting local factors. By contrast, traffic monitoring systems are widely deployed in cities around the world, highlighting the imbalance between what we know about traffic conditions and what do not know about their environmental consequences. To bridge this gap, we propose a machine learning-driven system that extracts visual information from traffic video to capture vehicles behaviors and conditions. Combining these features with weather data, our model estimates BC at street level, achieving an R-squared value of 0.72 and RMSE of 129.42 ng/m3 (nanogram per cubic meter). From a sustainability perspective, this work leverages resources already supported by urban infrastructure and established modeling techniques to generate information relevant to traffic emission. Obtaining BC concentration data provides actionable insights to support pollution reduction, urban planning, public health, and environmental justice at the local municipal level.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 40,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06655",
        "abs_url": "https://arxiv.org/abs/2512.06655",
        "pdf_url": "https://arxiv.org/pdf/2512.06655",
        "title": "GSAE: Graph-Regularized Sparse Autoencoders for Robust LLM Safety Steering",
        "authors": [
            "Jehyeok Yeon",
            "Federico Cinus",
            "Yifan Wu",
            "Luca Luceri"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Large language models (LLMs) face critical safety challenges, as they can be manipulated to generate harmful content through adversarial prompts and jailbreak attacks. Many defenses are typically either black-box guardrails that filter outputs, or internals-based methods that steer hidden activations by operationalizing safety as a single latent feature or dimension. While effective for simple concepts, this assumption is limiting, as recent evidence shows that abstract concepts such as refusal and temporality are distributed across multiple features rather than isolated in one. To address this limitation, we introduce Graph-Regularized Sparse Autoencoders (GSAEs), which extends SAEs with a Laplacian smoothness penalty on the neuron co-activation graph. Unlike standard SAEs that assign each concept to a single latent feature, GSAEs recover smooth, distributed safety representations as coherent patterns spanning multiple features. We empirically demonstrate that GSAE enables effective runtime safety steering, assembling features into a weighted set of safety-relevant directions and controlling them with a two-stage gating mechanism that activates interventions only when harmful prompts or continuations are detected during generation. This approach enforces refusals adaptively while preserving utility on benign queries. Across safety and QA benchmarks, GSAE steering achieves an average 82% selective refusal rate, substantially outperforming standard SAE steering (42%), while maintaining strong task accuracy (70% on TriviaQA, 65% on TruthfulQA, 74% on GSM8K). Robustness experiments further show generalization across LLaMA-3, Mistral, Qwen, and Phi families and resilience against jailbreak attacks (GCG, AutoDAN), consistently maintaining >= 90% refusal of harmful content.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 41,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06666",
        "abs_url": "https://arxiv.org/abs/2512.06666",
        "pdf_url": "https://arxiv.org/pdf/2512.06666",
        "title": "The Meta-Learning Gap: Combining Hydra and Quant for Large-Scale Time Series Classification",
        "authors": [
            "Urav Maniar"
        ],
        "comments": "Link to the repository: this https URL",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Time series classification faces a fundamental trade-off between accuracy and computational efficiency. While comprehensive ensembles like HIVE-COTE 2.0 achieve state-of-the-art accuracy, their 340-hour training time on the UCR benchmark renders them impractical for large-scale datasets. We investigate whether targeted combinations of two efficient algorithms from complementary paradigms can capture ensemble benefits while maintaining computational feasibility. Combining Hydra (competing convolutional kernels) and Quant (hierarchical interval quantiles) across six ensemble configurations, we evaluate performance on 10 large-scale MONSTER datasets (7,898 to 1,168,774 training instances). Our strongest configuration improves mean accuracy from 0.829 to 0.836, succeeding on 7 of 10 datasets. However, prediction-combination ensembles capture only 11% of theoretical oracle potential, revealing a substantial meta-learning optimization gap. Feature-concatenation approaches exceeded oracle bounds by learning novel decision boundaries, while prediction-level complementarity shows moderate correlation with ensemble gains. The central finding: the challenge has shifted from ensuring algorithms are different to learning how to combine them effectively. Current meta-learning strategies struggle to exploit the complementarity that oracle analysis confirms exists. Improved combination strategies could potentially double or triple ensemble gains across diverse time series classification applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 42,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06678",
        "abs_url": "https://arxiv.org/abs/2512.06678",
        "pdf_url": "https://arxiv.org/pdf/2512.06678",
        "title": "GradientSpace: Unsupervised Data Clustering for Improved Instruction Tuning",
        "authors": [
            "Shrihari Sridharan",
            "Deepak Ravikumar",
            "Anand Raghunathan",
            "Kaushik Roy"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Instruction tuning is one of the key steps required for adapting large language models (LLMs) to a broad spectrum of downstream applications. However, this procedure is difficult because real-world datasets are rarely homogeneous; they consist of a mixture of diverse information, causing gradient interference, where conflicting gradients pull the model in opposing directions, degrading performance. A common strategy to mitigate this issue is to group data based on semantic or embedding similarity. However, this fails to capture how data influences model parameters during learning. While recent works have attempted to cluster gradients directly, they randomly project gradients into lower dimensions to manage memory, which leads to accuracy loss. Moreover, these methods rely on expert ensembles which necessitates multiple inference passes and expensive on-the-fly gradient computations during inference. To address these limitations, we propose GradientSpace, a framework that clusters samples directly in full-dimensional gradient space. We introduce an online SVD-based algorithm that operates on LoRA gradients to identify latent skills without the infeasible cost of storing all sample gradients. Each cluster is used to train a specialized LoRA expert along with a lightweight router trained to select the best expert during inference. We show that routing to a single, appropriate expert outperforms expert ensembles used in prior work, while significantly reducing inference latency. Our experiments across mathematical reasoning, code generation, finance, and creative writing tasks demonstrate that GradientSpace leads to coherent expert specialization and consistent accuracy gains over state-of-the-art clustering methods and finetuning techniques.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 43,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06692",
        "abs_url": "https://arxiv.org/abs/2512.06692",
        "pdf_url": "https://arxiv.org/pdf/2512.06692",
        "title": "State Diversity Matters in Offline Behavior Distillation",
        "authors": [
            "Shiye Lei",
            "Zhihao Cheng",
            "Dacheng Tao"
        ],
        "comments": "12 pages, 5 figures, 5 tables",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Offline Behavior Distillation (OBD), which condenses massive offline RL data into a compact synthetic behavioral dataset, offers a promising approach for efficient policy training and can be applied across various downstream RL tasks. In this paper, we uncover a misalignment between original and distilled datasets, observing that a high-quality original dataset does not necessarily yield a superior synthetic dataset. Through an empirical analysis of policy performance under varying levels of training loss, we show that datasets with greater state diversity outperforms those with higher state quality when training loss is substantial, as is often the case in OBD, whereas the relationship reverses under minimal loss, which contributes to the misalignment. By associating state quality and diversity in reducing pivotal and surrounding error, respectively, our theoretical analysis establishes that surrounding error plays a more crucial role in policy performance when pivotal error is large, thereby highlighting the importance of state diversity in OBD scenario. Furthermore, we propose a novel yet simple algorithm, state density weighted (SDW) OBD, which emphasizes state diversity by weighting the distillation objective using the reciprocal of state density, thereby distilling a more diverse state information into synthetic data. Extensive experiments across multiple D4RL datasets confirm that SDW significantly enhances OBD performance when the original dataset exhibits limited state diversity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 44,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06695",
        "abs_url": "https://arxiv.org/abs/2512.06695",
        "pdf_url": "https://arxiv.org/pdf/2512.06695",
        "title": "Mitigating Barren plateaus in quantum denoising diffusion probabilistic models",
        "authors": [
            "Haipeng Cao",
            "Kaining Zhang",
            "Dacheng Tao",
            "Zhaofeng Su"
        ],
        "comments": "22 pages, 9 figures",
        "subjects": "Machine Learning (cs.LG); Quantum Physics (quant-ph)",
        "abstract": "Quantum generative models leverage quantum superposition and entanglement to enhance learning efficiency for both classical and quantum data. The quantum denoising diffusion probabilistic model (QuDDPM), inspired by its classical counterpart, has been proposed as a promising framework for quantum generative learning. QuDDPM is capable of efficiently learning and generating quantum data, and it demonstrates excellent performance in learning correlated quantum noise models, quantum many-body phases, and the topological structure of quantum data. However, we show that barren plateaus emerge in QuDDPMs due to the use of 2-design states as the input for the denoising process, which severely undermines the performance of QuDDPM. Through theoretical analysis and experimental validation, we confirm the presence of barren plateaus in the original QuDDPM. To address this issue, we introduce an improved QuDDPM that utilizes a distribution maintaining a certain distance from the Haar distribution, ensuring better trainability. Experimental results demonstrate that our approach effectively mitigates the barren plateau problem and generates samples with higher quality, paving the way for scalable and efficient quantum generative learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 45,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06702",
        "abs_url": "https://arxiv.org/abs/2512.06702",
        "pdf_url": "https://arxiv.org/pdf/2512.06702",
        "title": "Pathway to $O(\\sqrt{d})$ Complexity bound under Wasserstein metric of flow-based models",
        "authors": [
            "Xiangjun Meng",
            "Zhongjian Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We provide attainable analytical tools to estimate the error of flow-based generative models under the Wasserstein metric and to establish the optimal sampling iteration complexity bound with respect to dimension as $O(\\sqrt{d})$. We show this error can be explicitly controlled by two parts: the Lipschitzness of the push-forward maps of the backward flow which scales independently of the dimension; and a local discretization error scales $O(\\sqrt{d})$ in terms of dimension. The former one is related to the existence of Lipschitz changes of variables induced by the (heat) flow. The latter one consists of the regularity of the score function in both spatial and temporal directions. These assumptions are valid in the flow-based generative model associated with the Föllmer process and $1$-rectified flow under the Gaussian tail assumption. As a consequence, we show that the sampling iteration complexity grows linearly with the square root of the trace of the covariance operator, which is related to the invariant distribution of the forward process.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 46,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06714",
        "abs_url": "https://arxiv.org/abs/2512.06714",
        "pdf_url": "https://arxiv.org/pdf/2512.06714",
        "title": "A Novel Deep Neural Network Architecture for Real-Time Water Demand Forecasting",
        "authors": [
            "Tony Salloom",
            "Okyay Kaynak",
            "Wei He"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Robotics (cs.RO)",
        "abstract": "Short-term water demand forecasting (StWDF) is the foundation stone in the derivation of an optimal plan for controlling water supply systems. Deep learning (DL) approaches provide the most accurate solutions for this purpose. However, they suffer from complexity problem due to the massive number of parameters, in addition to the high forecasting error at the extreme points. In this work, an effective method to alleviate the error at these points is proposed. It is based on extending the data by inserting virtual data within the actual data to relieve the nonlinearity around them. To our knowledge, this is the first work that considers the problem related to the extreme points. Moreover, the water demand forecasting model proposed in this work is a novel DL model with relatively low complexity. The basic model uses the gated recurrent unit (GRU) to handle the sequential relationship in the historical demand data, while an unsupervised classification method, K-means, is introduced for the creation of new features to enhance the prediction accuracy with less number of parameters. Real data obtained from two different water plants in China are used to train and verify the model proposed. The prediction results and the comparison with the state-of-the-art illustrate that the method proposed reduces the complexity of the model six times of what achieved in the literature while conserving the same accuracy. Furthermore, it is found that extending the data set significantly reduces the error by about 30%. However, it increases the training time.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 47,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06725",
        "abs_url": "https://arxiv.org/abs/2512.06725",
        "pdf_url": "https://arxiv.org/pdf/2512.06725",
        "title": "Decoding Motor Behavior Using Deep Learning and Reservoir Computing",
        "authors": [
            "Tian Lan"
        ],
        "comments": "10 pages, 3 figures",
        "subjects": "Machine Learning (cs.LG); Signal Processing (eess.SP)",
        "abstract": "We present a novel approach to EEG decoding for non-invasive brain machine interfaces (BMIs), with a focus on motor-behavior classification. While conventional convolutional architectures such as EEGNet and DeepConvNet are effective in capturing local spatial patterns, they are markedly less suited for modeling long-range temporal dependencies and nonlinear dynamics. To address this limitation, we integrate an Echo State Network (ESN), a prominent paradigm in reservoir computing into the decoding pipeline. ESNs construct a high-dimensional, sparsely connected recurrent reservoir that excels at tracking temporal dynamics, thereby complementing the spatial representational power of CNNs. Evaluated on a skateboard-trick EEG dataset preprocessed via the PREP pipeline and implemented in MNE-Python, our ESNNet achieves 83.2% within-subject and 51.3% LOSO accuracies, surpassing widely used CNN-based baselines. Code is available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 48,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06727",
        "abs_url": "https://arxiv.org/abs/2512.06727",
        "pdf_url": "https://arxiv.org/pdf/2512.06727",
        "title": "KV-CAR: KV Cache Compression using Autoencoders and KV Reuse in Large Language Models",
        "authors": [
            "Sourjya Roy",
            "Shrihari Sridharan",
            "Surya Selvam",
            "Anand Raghunathan"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "As Large Language Models (LLMs) scale in size and context length, the memory requirements of the key value (KV) cache have emerged as a major bottleneck during autoregressive decoding. The KV cache grows with sequence length and embedding dimension, often exceeding the memory footprint of the model itself and limiting achievable batch sizes and context windows. To address this challenge, we present KV CAR, a unified and architecture agnostic framework that significantly reduces KV cache storage while maintaining model fidelity. KV CAR combines two complementary techniques. First, a lightweight autoencoder learns compact representations of key and value tensors along the embedding dimension, compressing them before they are stored in the KV cache and restoring them upon retrieval. Second, a similarity driven reuse mechanism identifies opportunities to reuse KV tensors of specific attention heads across adjacent layers. Together, these methods reduce the dimensional and structural redundancy in KV tensors without requiring changes to the transformer architecture. Evaluations on GPT 2 and TinyLLaMA models across Wikitext, C4, PIQA, and Winogrande datasets demonstrate that KV CAR achieves up to 47.85 percent KV cache memory reduction with minimal impact on perplexity and zero shot accuracy. System level measurements on an NVIDIA A40 GPU show that the reduced KV footprint directly translates into longer sequence lengths and larger batch sizes during inference. These results highlight the effectiveness of KV CAR in enabling memory efficient LLM inference.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 49,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06737",
        "abs_url": "https://arxiv.org/abs/2512.06737",
        "pdf_url": "https://arxiv.org/pdf/2512.06737",
        "title": "Arc Gradient Descent: A Mathematically Derived Reformulation of Gradient Descent with Phase-Aware, User-Controlled Step Dynamics",
        "authors": [
            "Nikhil Verma",
            "Joonas Linnosmaa",
            "Espinosa-Leal Leonardo",
            "Napat Vajragupta"
        ],
        "comments": "80 pages, 6 tables, 2 figures, 5 appendices, proof-of-concept",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Neural and Evolutionary Computing (cs.NE)",
        "abstract": "The paper presents the formulation, implementation, and evaluation of the ArcGD optimiser. The evaluation is conducted initially on a non-convex benchmark function and subsequently on a real-world ML dataset. The initial comparative study using the Adam optimiser is conducted on a stochastic variant of the highly non-convex and notoriously challenging Rosenbrock function, renowned for its narrow, curved valley, across dimensions ranging from 2D to 1000D and an extreme case of 50,000D. Two configurations were evaluated to eliminate learning-rate bias: (i) both using ArcGD's effective learning rate and (ii) both using Adam's default learning rate. ArcGD consistently outperformed Adam under the first setting and, although slower under the second, achieved super ior final solutions in most cases. In the second evaluation, ArcGD is evaluated against state-of-the-art optimizers (Adam, AdamW, Lion, SGD) on the CIFAR-10 image classification dataset across 8 diverse MLP architectures ranging from 1 to 5 hidden layers. ArcGD achieved the highest average test accuracy (50.7%) at 20,000 iterations, outperforming AdamW (46.6%), Adam (46.8%), SGD (49.6%), and Lion (43.4%), winning or tying on 6 of 8 architectures. Notably, while Adam and AdamW showed strong early convergence at 5,000 iterations, but regressed with extended training, whereas ArcGD continued improving, demonstrating generalization and resistance to overfitting without requiring early stopping tuning. Strong performance on geometric stress tests and standard deep-learning benchmarks indicates broad applicability, highlighting the need for further exploration. Moreover, it is also shown that a variant of ArcGD can be interpreted as a special case of the Lion optimiser, highlighting connections between the inherent mechanisms of such optimisation methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 50,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06752",
        "abs_url": "https://arxiv.org/abs/2512.06752",
        "pdf_url": "https://arxiv.org/pdf/2512.06752",
        "title": "Multi-Scale Protein Structure Modelling with Geometric Graph U-Nets",
        "authors": [
            "Chang Liu",
            "Vivian Li",
            "Linus Leong",
            "Vladimir Radenkovic",
            "Pietro Liò",
            "Chaitanya K. Joshi"
        ],
        "comments": "Presented at Machine Learning in Structural Biology, 2025. Open-source code: this https URL",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Geometric Graph Neural Networks (GNNs) and Transformers have become state-of-the-art for learning from 3D protein structures. However, their reliance on message passing prevents them from capturing the hierarchical interactions that govern protein function, such as global domains and long-range allosteric regulation. In this work, we argue that the network architecture itself should mirror this biological hierarchy. We introduce Geometric Graph U-Nets, a new class of models that learn multi-scale representations by recursively coarsening and refining the protein graph. We prove that this hierarchical design can theoretically more expressive than standard Geometric GNNs. Empirically, on the task of protein fold classification, Geometric U-Nets substantially outperform invariant and equivariant baselines, demonstrating their ability to learn the global structural patterns that define protein folds. Our work provides a principled foundation for designing geometric deep learning architectures that can learn the multi-scale structure of biomolecules.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 51,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06758",
        "abs_url": "https://arxiv.org/abs/2512.06758",
        "pdf_url": "https://arxiv.org/pdf/2512.06758",
        "title": "Optimal Analysis for Bandit Learning in Matching Markets with Serial Dictatorship",
        "authors": [
            "Zilong Wang",
            "Shuai Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The problem of two-sided matching markets is well-studied in computer science and economics, owing to its diverse applications across numerous domains. Since market participants are usually uncertain about their preferences in various online matching platforms, an emerging line of research is dedicated to the online setting where one-side participants (players) learn their unknown preferences through multiple rounds of interactions with the other side (arms). Sankararaman et al. provide an $\\Omega\\left( \\frac{N\\log(T)}{\\Delta^2} + \\frac{K\\log(T)}{\\Delta} \\right)$ regret lower bound for this problem under serial dictatorship assumption, where $N$ is the number of players, $K (\\geq N)$ is the number of arms, $\\Delta$ is the minimum reward gap across players and arms, and $T$ is the time horizon. Serial dictatorship assumes arms have the same preferences, which is common in reality when one side participants have a unified evaluation standard. Recently, the work of Kong and Li proposes the ET-GS algorithm and achieves an $O\\left( \\frac{K\\log(T)}{\\Delta^2} \\right)$ regret upper bound, which is the best upper bound attained so far. Nonetheless, a gap between the lower and upper bounds, ranging from $N$ to $K$, persists. It remains unclear whether the lower bound or the upper bound needs to be improved. In this paper, we propose a multi-level successive selection algorithm that obtains an $O\\left( \\frac{N\\log(T)}{\\Delta^2} + \\frac{K\\log(T)}{\\Delta} \\right)$ regret bound when the market satisfies serial dictatorship. To the best of our knowledge, we are the first to propose an algorithm that matches the lower bound in the problem of matching markets with bandits.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 52,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06782",
        "abs_url": "https://arxiv.org/abs/2512.06782",
        "pdf_url": "https://arxiv.org/pdf/2512.06782",
        "title": "Measuring Over-smoothing beyond Dirichlet energy",
        "authors": [
            "Weiqi Guan",
            "Zihao Shi"
        ],
        "comments": "17 pages, 1 figure",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "While Dirichlet energy serves as a prevalent metric for quantifying over-smoothing, it is inherently restricted to capturing first-order feature derivatives. To address this limitation, we propose a generalized family of node similarity measures based on the energy of higher-order feature derivatives. Through a rigorous theoretical analysis of the relationships among these measures, we establish the decay rates of Dirichlet energy under both continuous heat diffusion and discrete aggregation operators. Furthermore, our analysis reveals an intrinsic connection between the over-smoothing decay rate and the spectral gap of the graph Laplacian. Finally, empirical results demonstrate that attention-based Graph Neural Networks (GNNs) suffer from over-smoothing when evaluated under these proposed metrics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 53,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06791",
        "abs_url": "https://arxiv.org/abs/2512.06791",
        "pdf_url": "https://arxiv.org/pdf/2512.06791",
        "title": "Small-Gain Nash: Certified Contraction to Nash Equilibria in Differentiable Games",
        "authors": [
            "Vedansh Sharma"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Science and Game Theory (cs.GT); Optimization and Control (math.OC)",
        "abstract": "Classical convergence guarantees for gradient-based learning in games require the pseudo-gradient to be (strongly) monotone in Euclidean geometry as shown by rosen(1965), a condition that often fails even in simple games with strong cross-player couplings. We introduce Small-Gain Nash (SGN), a block small-gain condition in a custom block-weighted geometry. SGN converts local curvature and cross-player Lipschitz coupling bounds into a tractable certificate of contraction. It constructs a weighted block metric in which the pseudo-gradient becomes strongly monotone on any region where these bounds hold, even when it is non-monotone in the Euclidean sense. The continuous flow is exponentially contracting in this designed geometry, and projected Euler and RK4 discretizations converge under explicit step-size bounds derived from the SGN margin and a local Lipschitz constant. Our analysis reveals a certified ``timescale band'', a non-asymptotic, metric-based certificate that plays a TTUR-like role: rather than forcing asymptotic timescale separation via vanishing, unequal step sizes, SGN identifies a finite band of relative metric weights for which a single-step-size dynamics is provably contractive. We validate the framework on quadratic games where Euclidean monotonicity analysis fails to predict convergence, but SGN successfully certifies it, and extend the construction to mirror/Fisher geometries for entropy-regularized policy gradient in Markov games. The result is an offline certification pipeline that estimates curvature, coupling, and Lipschitz parameters on compact regions, optimizes block weights to enlarge the SGN margin, and returns a structural, computable convergence certificate consisting of a metric, contraction rate, and safe step-sizes for non-monotone games.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 54,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06813",
        "abs_url": "https://arxiv.org/abs/2512.06813",
        "pdf_url": "https://arxiv.org/pdf/2512.06813",
        "title": "Partial Inverse Design of High-Performance Concrete Using Cooperative Neural Networks for Constraint-Aware Mix Generation",
        "authors": [
            "Agung Nugraha",
            "Heungjun Im",
            "Jihwan Lee"
        ],
        "comments": "19 pages, 12 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "High-performance concrete offers exceptional strength and durability but requires complex mix designs involving many interdependent variables and practical constraints. While data-driven methods have advanced predictive modeling for forward design, inverse design, which focuses on determining mix compositions that achieve target performance, remains limited, particularly in design situations where some mix variables are fixed by constraints and only the remaining variables must be determined. This study proposes a cooperative neural network framework for the partial inverse design of high-performance concrete. The framework combines two coupled neural network models, an imputation model that infers the undetermined variables and a surrogate model that predicts compressive strength. Through cooperative learning, the model generates valid and performance-consistent mix designs in a single forward pass while accommodating different constraint combinations without retraining. Its performance is compared with both probabilistic and generative approaches, including Bayesian inference based on a Gaussian process surrogate and autoencoder-based models. Evaluated on a benchmark dataset, the proposed model achieves stable and higher R-squared values of 0.87-0.92 and reduces mean squared error by an average of 50 percent compared with autoencoder baselines and by an average of 70 percent compared with Bayesian inference. The results demonstrate that the cooperative neural network provides an accurate, robust, and computationally efficient foundation for constraint-aware, data-driven mix proportioning in concrete engineering.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 55,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06837",
        "abs_url": "https://arxiv.org/abs/2512.06837",
        "pdf_url": "https://arxiv.org/pdf/2512.06837",
        "title": "Neural Factorization-based Bearing Fault Diagnosis",
        "authors": [
            "Zhenhao Li",
            "Xu Cheng",
            "Yi Zhou"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "This paper studies the key problems of bearing fault diagnosis of high-speed train. As the core component of the train operation system, the health of bearings is directly related to the safety of train operation. The traditional diagnostic methods are facing the challenge of insufficient diagnostic accuracy under complex conditions. To solve these problems, we propose a novel Neural Factorization-based Classification (NFC) framework for bearing fault diagnosis. It is built on two core idea: 1) Embedding vibration time series into multiple mode-wise latent feature vectors to capture diverse fault-related patterns; 2) Leveraging neural factorization principles to fuse these vectors into a unified vibration representation. This design enables effective mining of complex latent fault characteristics from raw time-series data. We further instantiate the framework with two models CP-NFC and Tucker-NFC based on CP and Tucker fusion schemes, respectively. Experimental results show that both models achieve superior diagnostic performance compared with traditional machine learning methods. The comparative analysis provides valuable empirical evidence and practical guidance for selecting effective diagnostic strategies in high-speed train bearing monitoring.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 56,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06917",
        "abs_url": "https://arxiv.org/abs/2512.06917",
        "pdf_url": "https://arxiv.org/pdf/2512.06917",
        "title": "Know your Trajectory -- Trustworthy Reinforcement Learning deployment through Importance-Based Trajectory Analysis",
        "authors": [
            "Clifford F",
            "Devika Jay",
            "Abhishek Sarkar",
            "Satheesh K Perepu",
            "Santhosh G S",
            "Kaushik Dey",
            "Balaraman Ravindran"
        ],
        "comments": "Accepted at 4th Deployable AI Workshop at AAAI 2026",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "As Reinforcement Learning (RL) agents are increasingly deployed in real-world applications, ensuring their behavior is transparent and trustworthy is paramount. A key component of trust is explainability, yet much of the work in Explainable RL (XRL) focuses on local, single-step decisions. This paper addresses the critical need for explaining an agent's long-term behavior through trajectory-level analysis. We introduce a novel framework that ranks entire trajectories by defining and aggregating a new state-importance metric. This metric combines the classic Q-value difference with a \"radical term\" that captures the agent's affinity to reach its goal, providing a more nuanced measure of state criticality. We demonstrate that our method successfully identifies optimal trajectories from a heterogeneous collection of agent experiences. Furthermore, by generating counterfactual rollouts from critical states within these trajectories, we show that the agent's chosen path is robustly superior to alternatives, thereby providing a powerful \"Why this, and not that?\" explanation. Our experiments in standard OpenAI Gym environments validate that our proposed importance metric is more effective at identifying optimal behaviors compared to classic approaches, offering a significant step towards trustworthy autonomous systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 57,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06920",
        "abs_url": "https://arxiv.org/abs/2512.06920",
        "pdf_url": "https://arxiv.org/pdf/2512.06920",
        "title": "Parent-Guided Semantic Reward Model (PGSRM): Embedding-Based Reward Functions for Reinforcement Learning of Transformer Language Models",
        "authors": [
            "Alexandr Plashchinsky"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We introduce the Parent-Guided Semantic Reward Model (PGSRM), a lightweight reward framework for reinforcement learning (RL) of transformer language models. PGSRM replaces binary correctness signals, human preference data, and trained reward models with a simple signal: cosine similarity between a parent model's reference output embedding and a child model's generated output for the same input. This yields a dense, semantically meaningful reward with no human annotation or additional model training. We apply PGSRM on five language tasks and find that it produces smoother reward improvement and more stable PPO dynamics than a binary reward baseline, suggesting that embedding-based semantic rewards are a practical alternative to RLHF-style reward modeling for parent-guided alignment in smaller transformer models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 58,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06925",
        "abs_url": "https://arxiv.org/abs/2512.06925",
        "pdf_url": "https://arxiv.org/pdf/2512.06925",
        "title": "Deep Reinforcement Learning for Phishing Detection with Transformer-Based Semantic Features",
        "authors": [
            "Aseer Al Faisal"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)",
        "abstract": "Phishing is a cybercrime in which individuals are deceived into revealing personal information, often resulting in financial loss. These attacks commonly occur through fraudulent messages, misleading advertisements, and compromised legitimate websites. This study proposes a Quantile Regression Deep Q-Network (QR-DQN) approach that integrates RoBERTa semantic embeddings with handcrafted lexical features to enhance phishing detection while accounting for uncertainties. Unlike traditional DQN methods that estimate single scalar Q-values, QR-DQN leverages quantile regression to model the distribution of returns, improving stability and generalization on unseen phishing data. A diverse dataset of 105,000 URLs was curated from PhishTank, OpenPhish, Cloudflare, and other sources, and the model was evaluated using an 80/20 train-test split. The QR-DQN framework achieved a test accuracy of 99.86%, precision of 99.75%, recall of 99.96%, and F1-score of 99.85%, demonstrating high effectiveness. Compared to standard DQN with lexical features, the hybrid QR-DQN with lexical and semantic features reduced the generalization gap from 1.66% to 0.04%, indicating significant improvement in robustness. Five-fold cross-validation confirmed model reliability, yielding a mean accuracy of 99.90% with a standard deviation of 0.04%. These results suggest that the proposed hybrid approach effectively identifies phishing threats, adapts to evolving attack strategies, and generalizes well to unseen data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 59,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06929",
        "abs_url": "https://arxiv.org/abs/2512.06929",
        "pdf_url": "https://arxiv.org/pdf/2512.06929",
        "title": "Adaptive Normalization Mamba with Multi Scale Trend Decomposition and Patch MoE Encoding",
        "authors": [
            "MinCheol Jeon"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Time series forecasting in real world environments faces significant challenges non stationarity, multi scale temporal patterns, and distributional shifts that degrade model stability and accuracy. This study propose AdaMamba, a unified forecasting architecture that integrates adaptive normalization, multi scale trend extraction, and contextual sequence modeling to address these challenges. AdaMamba begins with an Adaptive Normalization Block that removes non stationary components through multi scale convolutional trend extraction and channel wise recalibration, enabling consistent detrending and variance stabilization. The normalized sequence is then processed by a Context Encoder that combines patch wise embeddings, positional encoding, and a Mamba enhanced Transformer layer with a mixture of experts feed forward module, allowing efficient modeling of both long range dependencies and local temporal dynamics. A lightweight prediction head generates multi horizon forecasts, and a denormalization mechanism reconstructs outputs by reintegrating local trends to ensure robustness under varying temporal conditions. AdaMamba provides strong representational capacity with modular extensibility, supporting deterministic prediction and compatibility with probabilistic extensions. Its design effectively mitigates covariate shift and enhances predictive reliability across heterogeneous datasets. Experimental evaluations demonstrate that AdaMamba's combination of adaptive normalization and expert augmented contextual modeling yields consistent improvements in stability and accuracy over conventional Transformer based baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 60,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06944",
        "abs_url": "https://arxiv.org/abs/2512.06944",
        "pdf_url": "https://arxiv.org/pdf/2512.06944",
        "title": "A Unifying Human-Centered AI Fairness Framework",
        "authors": [
            "Munshi Mahbubur Rahman",
            "Shimei Pan",
            "James R. Foulds"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computers and Society (cs.CY)",
        "abstract": "The increasing use of Artificial Intelligence (AI) in critical societal domains has amplified concerns about fairness, particularly regarding unequal treatment across sensitive attributes such as race, gender, and socioeconomic status. While there has been substantial work on ensuring AI fairness, navigating trade-offs between competing notions of fairness as well as predictive accuracy remains challenging, creating barriers to the practical deployment of fair AI systems. To address this, we introduce a unifying human-centered fairness framework that systematically covers eight distinct fairness metrics, formed by combining individual and group fairness, infra-marginal and intersectional assumptions, and outcome-based and equality-of-opportunity (EOO) perspectives. This structure allows stakeholders to align fairness interventions with their values and contextual considerations. The framework uses a consistent and easy-to-understand formulation for all metrics to reduce the learning curve for non-experts. Rather than privileging a single fairness notion, the framework enables stakeholders to assign weights across multiple fairness objectives, reflecting their priorities and facilitating multi-stakeholder compromises. We apply this approach to four real-world datasets: the UCI Adult census dataset for income prediction, the COMPAS dataset for criminal recidivism, the German Credit dataset for credit risk assessment, and the MEPS dataset for healthcare utilization. We show that adjusting weights reveals nuanced trade-offs between different fairness metrics. Finally, through case studies in judicial decision-making and healthcare, we demonstrate how the framework can inform practical and value-sensitive deployment of fair AI systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 61,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06971",
        "abs_url": "https://arxiv.org/abs/2512.06971",
        "pdf_url": "https://arxiv.org/pdf/2512.06971",
        "title": "Prediction with Expert Advice under Local Differential Privacy",
        "authors": [
            "Ben Jacobsen",
            "Kassem Fawaz"
        ],
        "comments": "19 pages, 3 figures",
        "subjects": "Machine Learning (cs.LG); Cryptography and Security (cs.CR); Data Structures and Algorithms (cs.DS); Machine Learning (stat.ML)",
        "abstract": "We study the classic problem of prediction with expert advice under the constraint of local differential privacy (LDP). In this context, we first show that a classical algorithm naturally satisfies LDP and then design two new algorithms that improve it: RW-AdaBatch and RW-Meta. For RW-AdaBatch, we exploit the limited-switching behavior induced by LDP to provide a novel form of privacy amplification that grows stronger on easier data, analogous to the shuffle model in offline learning. Drawing on the theory of random walks, we prove that this improvement carries essentially no utility cost. For RW-Meta, we develop a general method for privately selecting between experts that are themselves non-trivial learning algorithms, and we show that in the context of LDP this carries no extra privacy cost. In contrast, prior work has only considered data-independent experts. We also derive formal regret bounds that scale inversely with the degree of independence between experts. Our analysis is supplemented by evaluation on real-world data reported by hospitals during the COVID-19 pandemic; RW-Meta outperforms both the classical baseline and a state-of-the-art \\textit{central} DP algorithm by 1.5-3$\\times$ on the task of predicting which hospital will report the highest density of COVID patients each week.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 62,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06982",
        "abs_url": "https://arxiv.org/abs/2512.06982",
        "pdf_url": "https://arxiv.org/pdf/2512.06982",
        "title": "LLM-Driven Composite Neural Architecture Search for Multi-Source RL State Encoding",
        "authors": [
            "Yu Yu",
            "Qian Xie",
            "Nairen Cao",
            "Li Jin"
        ],
        "comments": "NeurIPS 2025 Workshop on Bridging Language, Agent, and World Models for Reasoning and Planning",
        "subjects": "Machine Learning (cs.LG); Systems and Control (eess.SY)",
        "abstract": "Designing state encoders for reinforcement learning (RL) with multiple information sources -- such as sensor measurements, time-series signals, image observations, and textual instructions -- remains underexplored and often requires manual design. We formalize this challenge as a problem of composite neural architecture search (NAS), where multiple source-specific modules and a fusion module are jointly optimized. Existing NAS methods overlook useful side information from the intermediate outputs of these modules -- such as their representation quality -- limiting sample efficiency in multi-source RL settings. To address this, we propose an LLM-driven NAS pipeline that leverages language-model priors and intermediate-output signals to guide sample-efficient search for high-performing composite state encoders. On a mixed-autonomy traffic control task, our approach discovers higher-performing architectures with fewer candidate evaluations than traditional NAS baselines and the LLM-based GENIUS framework.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 63,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06987",
        "abs_url": "https://arxiv.org/abs/2512.06987",
        "pdf_url": "https://arxiv.org/pdf/2512.06987",
        "title": "OXtal: An All-Atom Diffusion Model for Organic Crystal Structure Prediction",
        "authors": [
            "Emily Jin",
            "Andrei Cristian Nica",
            "Mikhail Galkin",
            "Jarrid Rector-Brooks",
            "Kin Long Kelvin Lee",
            "Santiago Miret",
            "Frances H. Arnold",
            "Michael Bronstein",
            "Avishek Joey Bose",
            "Alexander Tong",
            "Cheng-Hao Liu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Materials Science (cond-mat.mtrl-sci)",
        "abstract": "Accurately predicting experimentally-realizable 3D molecular crystal structures from their 2D chemical graphs is a long-standing open challenge in computational chemistry called crystal structure prediction (CSP). Efficiently solving this problem has implications ranging from pharmaceuticals to organic semiconductors, as crystal packing directly governs the physical and chemical properties of organic solids. In this paper, we introduce OXtal, a large-scale 100M parameter all-atom diffusion model that directly learns the conditional joint distribution over intramolecular conformations and periodic packing. To efficiently scale OXtal, we abandon explicit equivariant architectures imposing inductive bias arising from crystal symmetries in favor of data augmentation strategies. We further propose a novel crystallization-inspired lattice-free training scheme, Stoichiometric Stochastic Shell Sampling ($S^4$), that efficiently captures long-range interactions while sidestepping explicit lattice parametrization -- thus enabling more scalable architectural choices at all-atom resolution. By leveraging a large dataset of 600K experimentally validated crystal structures (including rigid and flexible molecules, co-crystals, and solvates), OXtal achieves orders-of-magnitude improvements over prior ab initio machine learning CSP methods, while remaining orders of magnitude cheaper than traditional quantum-chemical approaches. Specifically, OXtal recovers experimental structures with conformer $\\text{RMSD}_1<0.5$ Å and attains over 80\\% packing similarity rate, demonstrating its ability to model both thermodynamic and kinetic regularities of molecular crystallization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 64,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06993",
        "abs_url": "https://arxiv.org/abs/2512.06993",
        "pdf_url": "https://arxiv.org/pdf/2512.06993",
        "title": "Toward Reliable Machine Unlearning: Theory, Algorithms, and Evaluation",
        "authors": [
            "Ali Ebrahimpour-Boroojeny"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We propose new methodologies for both unlearning random set of samples and class unlearning and show that they outperform existing methods. The main driver of our unlearning methods is the similarity of predictions to a retrained model on both the forget and remain samples. We introduce Adversarial Machine UNlearning (AMUN), which surpasses prior state-of-the-art methods for image classification based on SOTA MIA scores. AMUN lowers the model's confidence on forget samples by fine-tuning on their corresponding adversarial examples. Through theoretical analysis, we identify factors governing AMUN's performance, including smoothness. To facilitate training of smooth models with a controlled Lipschitz constant, we propose FastClip, a scalable method that performs layer-wise spectral-norm clipping of affine layers. In a separate study, we show that increased smoothness naturally improves adversarial example transfer, thereby supporting the second factor above. Following the same principles for class unlearning, we show that existing methods fail in replicating a retrained model's behavior by introducing a nearest-neighbor membership inference attack (MIA-NN) that uses the probabilities assigned to neighboring classes to detect unlearned samples and demonstrate the vulnerability of such methods. We then propose a fine-tuning objective that mitigates this leakage by approximating, for forget-class inputs, the distribution over remaining classes that a model retrained from scratch would produce. To construct this approximation, we estimate inter-class similarity and tilt the target model's distribution accordingly. The resulting Tilted ReWeighting(TRW) distribution serves as the desired target during fine-tuning. Across multiple benchmarks, TRW matches or surpasses existing unlearning methods on prior metrics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 65,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07010",
        "abs_url": "https://arxiv.org/abs/2512.07010",
        "pdf_url": "https://arxiv.org/pdf/2512.07010",
        "title": "Always Keep Your Promises: DynamicLRP, A Model-Agnostic Solution To Layer-Wise Relevance Propagation",
        "authors": [
            "Kevin Lee",
            "Pablo Millan Arias"
        ],
        "comments": "Work in progress, (12 pages manuscript, 6 figures, 6 tables, 3 pages references, 14 pages appendix)",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Layer-wise Relevance Propagation (LRP) provides principled attribution for neural networks through conservation properties and foundations in Deep Taylor Decomposition. However, existing implementations operate at the module level, requiring architecture-specific propagation rules and modifications. These limit the generality of target model and sustainability of implementations as architectures evolve. We introduce DynamicLRP, a model-agnostic LRP framework operating at the tensor operation level. By decomposing attribution to individual operations within computation graphs and introducing a novel mechanism for deferred activation resolution, named the Promise System, our approach achieves true architecture agnosticity while maintaining LRP's theoretical guarantees. This design operates independently of backpropagation machinery, enabling operation on arbitrary computation graphs without model modification and side-by-side execution with gradient backpropagation. Being based on computation graphs, this method is theoretically extensible to other deep learning libraries that support auto-differentiation. We demonstrate faithfulness matching or exceeding specialized implementations (1.77 vs 1.69 ABPC on VGG, equivalent performance on ViT, 93.70\\% and 95.06\\% top-1 attribution accuracy for explaining RoBERTa-large and Flan-T5-large answers on SQuADv2, respectively) while maintaining practical efficiency on models with hundreds of millions of parameters. We achieved 99.92\\% node coverage across 31,465 computation graph nodes from 15 diverse architectures, including state-space models (Mamba), audio transformers (Whisper), and multimodal systems (DePlot) without any model-specific code with rules for 47 fundamental operations implemented. Our operation-level decomposition and Promise System establish a sustainable, extensible foundation for LRP across evolving architectures.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 66,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07011",
        "abs_url": "https://arxiv.org/abs/2512.07011",
        "pdf_url": "https://arxiv.org/pdf/2512.07011",
        "title": "Block Sparse Flash Attention",
        "authors": [
            "Daniel Ohayon",
            "Itay Lamprecht",
            "Itay Hubara",
            "Israel Cohen",
            "Daniel Soudry",
            "Noam Elata"
        ],
        "comments": "10 pages, 5 figures. Code: this https URL",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL); Performance (cs.PF)",
        "abstract": "Modern large language models increasingly require long contexts for reasoning and multi-document tasks, but attention's quadratic complexity creates a severe computational bottleneck. We present Block-Sparse FlashAttention (BSFA), a drop-in replacement that accelerates long-context inference while preserving model quality. Unlike methods that predict importance before computing scores, BSFA computes exact query-key similarities to select the top-k most important value blocks for each query. By comparing per-block maximum scores against calibrated thresholds, we skip approximately 50% of the computation and memory transfers for pruned blocks. Our training-free approach requires only a one-time threshold calibration on a small dataset to learn the per-layer and per-head attention score distributions. We provide a CUDA kernel implementation that can be used as a drop-in replacement for FlashAttention. On Llama-3.1-8B, BSFA achieves up to 1.10x speedup on real-world reasoning benchmarks and up to 1.24x for needle-in-a-haystack retrieval tasks while maintaining above 99% baseline accuracy, with certain configurations even improving accuracy by focusing on the most relevant content, substantially outperforming existing sparse attention methods. The implementation is available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 67,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07040",
        "abs_url": "https://arxiv.org/abs/2512.07040",
        "pdf_url": "https://arxiv.org/pdf/2512.07040",
        "title": "Transformation of Biological Networks into Images via Semantic Cartography for Visual Interpretation and Scalable Deep Analysis",
        "authors": [
            "Sakib Mostafa",
            "Lei Xing",
            "Md. Tauhidul Islam"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Complex biological networks are fundamental to biomedical science, capturing interactions among molecules, cells, genes, and tissues. Deciphering these networks is critical for understanding health and disease, yet their scale and complexity represent a daunting challenge for current computational methods. Traditional biological network analysis methods, including deep learning approaches, while powerful, face inherent challenges such as limited scalability, oversmoothing long-range dependencies, difficulty in multimodal integration, expressivity bounds, and poor interpretability. We present Graph2Image, a framework that transforms large biological networks into sets of two-dimensional images by spatially arranging representative network nodes on a 2D grid. This transformation decouples the nodes as images, enabling the use of convolutional neural networks (CNNs) with global receptive fields and multi-scale pyramids, thus overcoming limitations of existing biological network analysis methods in scalability, memory efficiency, and long-range context capture. Graph2Image also facilitates seamless integration with other imaging and omics modalities and enhances interpretability through direct visualization of node-associated images. When applied to several large-scale biological network datasets, Graph2Image improved classification accuracy by up to 67.2% over existing methods and provided interpretable visualizations that revealed biologically coherent patterns. It also allows analysis of very large biological networks (nodes > 1 billion) on a personal computer. Graph2Image thus provides a scalable, interpretable, and multimodal-ready approach for biological network analysis, offering new opportunities for disease diagnosis and the study of complex biological systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 68,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07064",
        "abs_url": "https://arxiv.org/abs/2512.07064",
        "pdf_url": "https://arxiv.org/pdf/2512.07064",
        "title": "Self-Supervised Learning on Molecular Graphs: A Systematic Investigation of Masking Design",
        "authors": [
            "Jiannan Yang",
            "Veronika Thost",
            "Tengfei Ma"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Quantitative Methods (q-bio.QM)",
        "abstract": "Self-supervised learning (SSL) plays a central role in molecular representation learning. Yet, many recent innovations in masking-based pretraining are introduced as heuristics and lack principled evaluation, obscuring which design choices are genuinely effective. This work cast the entire pretrain-finetune workflow into a unified probabilistic framework, enabling a transparent comparison and deeper understanding of masking strategies. Building on this formalism, we conduct a controlled study of three core design dimensions: masking distribution, prediction target, and encoder architecture, under rigorously controlled settings. We further employ information-theoretic measures to assess the informativeness of pretraining signals and connect them to empirically benchmarked downstream performance. Our findings reveal a surprising insight: sophisticated masking distributions offer no consistent benefit over uniform sampling for common node-level prediction tasks. Instead, the choice of prediction target and its synergy with the encoder architecture are far more critical. Specifically, shifting to semantically richer targets yields substantial downstream improvements, particularly when paired with expressive Graph Transformer encoders. These insights offer practical guidance for developing more effective SSL methods for molecular graphs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 69,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07082",
        "abs_url": "https://arxiv.org/abs/2512.07082",
        "pdf_url": "https://arxiv.org/pdf/2512.07082",
        "title": "TRACE: A Generalizable Drift Detector for Streaming Data-Driven Optimization",
        "authors": [
            "Yuan-Ting Zhong",
            "Ting Huang",
            "Xiaolin Xiao",
            "Yue-Jiao Gong"
        ],
        "comments": "Accepted by AAAI 2026",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Many optimization tasks involve streaming data with unknown concept drifts, posing a significant challenge as Streaming Data-Driven Optimization (SDDO). Existing methods, while leveraging surrogate model approximation and historical knowledge transfer, are often under restrictive assumptions such as fixed drift intervals and fully environmental observability, limiting their adaptability to diverse dynamic environments. We propose TRACE, a TRAnsferable C}oncept-drift Estimator that effectively detects distributional changes in streaming data with varying time scales. TRACE leverages a principled tokenization strategy to extract statistical features from data streams and models drift patterns using attention-based sequence learning, enabling accurate detection on unseen datasets and highlighting the transferability of learned drift patterns. Further, we showcase TRACE's plug-and-play nature by integrating it into a streaming optimizer, facilitating adaptive optimization under unknown drifts. Comprehensive experimental results on diverse benchmarks demonstrate the superior generalization, robustness, and effectiveness of our approach in SDDO scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 70,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07100",
        "abs_url": "https://arxiv.org/abs/2512.07100",
        "pdf_url": "https://arxiv.org/pdf/2512.07100",
        "title": "Dual Refinement Cycle Learning: Unsupervised Text Classification of Mamba and Community Detection on Text Attributed Graph",
        "authors": [
            "Hong Wang",
            "Yinglong Zhang",
            "Hanhan Guo",
            "Xuewen Xia",
            "Xing Xu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Pretrained language models offer strong text understanding capabilities but remain difficult to deploy in real-world text-attributed networks due to their heavy dependence on labeled data. Meanwhile, community detection methods typically ignore textual semantics, limiting their usefulness in downstream applications such as content organization, recommendation, and risk monitoring. To overcome these limitations, we present Dual Refinement Cycle Learning (DRCL), a fully unsupervised framework designed for practical scenarios where no labels or category definitions are available. DRCL integrates structural and semantic information through a warm-start initialization and a bidirectional refinement cycle between a GCN-based Community Detection Module (GCN-CDM) and a Text Semantic Modeling Module (TSMM). The two modules iteratively exchange pseudo-labels, allowing semantic cues to enhance structural clustering and structural patterns to guide text representation learning without manual supervision. Across several text-attributed graph datasets, DRCL consistently improves the structural and semantic quality of discovered communities. Moreover, a Mamba-based classifier trained solely from DRCL's community signals achieves accuracy comparable to supervised models, demonstrating its potential for deployment in large-scale systems where labeled data are scarce or costly.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 71,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07113",
        "abs_url": "https://arxiv.org/abs/2512.07113",
        "pdf_url": "https://arxiv.org/pdf/2512.07113",
        "title": "PlantBiMoE: A Bidirectional Foundation Model with SparseMoE for Plant Genomes",
        "authors": [
            "Kepeng Lin",
            "Qizhe Zhang",
            "Rui Wang",
            "Xuehai Hu",
            "Wei Xu"
        ],
        "comments": "6 pages, 5 figures, accept to BIBM",
        "subjects": "Machine Learning (cs.LG); Genomics (q-bio.GN)",
        "abstract": "Understanding the underlying linguistic rules of plant genomes remains a fundamental challenge in computational biology. Recent advances including AgroNT and PDLLMs have made notable progress although, they suffer from excessive parameter size and limited ability to model the bidirectional nature of DNA strands respectively. To address these limitations, we propose PlantBiMoE, a lightweight and expressive plant genome language model that integrates bidirectional Mamba and a Sparse Mixture-of-Experts (SparseMoE) framework. The bidirectional Mamba enables the model to effectively capture structural dependencies across both the forward and reverse DNA strands, while SparseMoE significantly reduces the number of active parameters, improving computational efficiency without sacrificing modeling capacity. We evaluated and tested our model on the Modified Plants Genome Benchmark (MPGB), an enhanced genomic benchmark, which consolidates 31 datasets across 11 representative tasks, with input sequence lengths ranging from 50 to 6,000 bp. Experimental results demonstrate that PlantBiMoE achieves the best performance on 20 out of 31 datasets and the average best when comparing with existing models. In summary, all above results demonstrate that our model can effectively represent plant genomic sequences, serving as a robust computational tool for diverse genomic tasks, while making substantive contributions to plant genomics, gene editing, and synthetic biology. The code is available at: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 72,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07150",
        "abs_url": "https://arxiv.org/abs/2512.07150",
        "pdf_url": "https://arxiv.org/pdf/2512.07150",
        "title": "FlowLPS: Langevin-Proximal Sampling for Flow-based Inverse Problem Solvers",
        "authors": [
            "Jonghyun Park",
            "Jong Chul Ye"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Deep generative models have become powerful priors for solving inverse problems, and various training-free methods have been developed. However, when applied to latent flow models, existing methods often fail to converge to the posterior mode or suffer from manifold deviation within latent spaces. To mitigate this, here we introduce a novel training-free framework, FlowLPS, that solves inverse problems with pretrained flow models via a Langevin Proximal Sampling (LPS) strategy. Our method integrates Langevin dynamics for manifold-consistent exploration with proximal optimization for precise mode seeking, achieving a superior balance between reconstruction fidelity and perceptual quality across multiple inverse tasks on FFHQ and DIV2K, outperforming state of the art inverse solvers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 73,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07173",
        "abs_url": "https://arxiv.org/abs/2512.07173",
        "pdf_url": "https://arxiv.org/pdf/2512.07173",
        "title": "Improving the Throughput of Diffusion-based Large Language Models via a Training-Free Confidence-Aware Calibration",
        "authors": [
            "Jucheng Shen",
            "Gaurav Sarkar",
            "Yeonju Ro",
            "Sharath Nittur Sridhar",
            "Zhangyang Wang",
            "Aditya Akella",
            "Souvik Kundu"
        ],
        "comments": "8 pages, 3 figures. Preprint under review",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We present CadLLM, a training-free method to accelerate the inference throughput of diffusion-based LLMs (dLLMs). We first investigate the dynamic nature of token unmasking confidence across blocks and steps. Based on this observation, we present a lightweight adaptive approach that controls the generation block size, step size, and threshold based on the average confidence of unmasked tokens. We further reduce softmax overhead by dynamically leveraging a subset of the vocabulary to regulate sampling breadth. CadLLM is a plug-and-play, model-agnostic method compatible with KV-cache-based dLLMs. Extensive experiments on four popular tasks demonstrate that CadLLM yields up to 2.28x throughput improvement over the state-of-the-art baseline with competitive accuracy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 74,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07175",
        "abs_url": "https://arxiv.org/abs/2512.07175",
        "pdf_url": "https://arxiv.org/pdf/2512.07175",
        "title": "SPACE: Noise Contrastive Estimation Stabilizes Self-Play Fine-Tuning for Large Language Models",
        "authors": [
            "Yibo Wang",
            "Qing-Guo Chen",
            "Zhao Xu",
            "Weihua Luo",
            "Kaifu Zhang",
            "Lijun Zhang"
        ],
        "comments": "NeurIPS 2025",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Self-play fine-tuning has demonstrated promising abilities in adapting large language models (LLMs) to downstream tasks with limited real-world data. The basic principle is to iteratively refine the model with real samples and synthetic ones generated from itself. However, the existing methods primarily focus on the relative gaps between the rewards for two types of data, neglecting their absolute values. Through theoretical analysis, we identify that the gap-based methods suffer from unstable evolution, due to the potentially degenerated objectives. To address this limitation, we introduce a novel self-play fine-tuning method, namely Self-PlAy via Noise Contrastive Estimation (SPACE), which leverages noise contrastive estimation to capture the real-world data distribution. Specifically, SPACE treats synthetic samples as auxiliary components, and discriminates them from the real ones in a binary classification manner. As a result, SPACE independently optimizes the absolute reward values for each type of data, ensuring a consistently meaningful objective and thereby avoiding the instability issue. Theoretically, we show that the optimal solution of the objective in SPACE aligns with the underlying distribution of real-world data, and SPACE guarantees a provably stable convergence to the optimal distribution. Empirically, we show that SPACE significantly improves the performance of LLMs over various tasks, and outperforms supervised fine-tuning that employs much more real-world samples. Compared to gap-based self-play fine-tuning methods, SPACE exhibits remarkable superiority and stable evolution.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 75,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07184",
        "abs_url": "https://arxiv.org/abs/2512.07184",
        "pdf_url": "https://arxiv.org/pdf/2512.07184",
        "title": "UniDiff: A Unified Diffusion Framework for Multimodal Time Series Forecasting",
        "authors": [
            "Da Zhang",
            "Bingyu Li",
            "Zhuyuan Zhao",
            "Junyu Gao",
            "Feiping Nie",
            "Xuelong Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "As multimodal data proliferates across diverse real-world applications, leveraging heterogeneous information such as texts and timestamps for accurate time series forecasting (TSF) has become a critical challenge. While diffusion models demonstrate exceptional performance in generation tasks, their application to TSF remains largely confined to modeling single-modality numerical sequences, overlooking the abundant cross-modal signals inherent in complex heterogeneous data. To address this gap, we propose UniDiff, a unified diffusion framework for multimodal time series forecasting. To process the numerical sequence, our framework first tokenizes the time series into patches, preserving local temporal dynamics by mapping each patch to an embedding space via a lightweight MLP. At its core lies a unified and parallel fusion module, where a single cross-attention mechanism adaptively weighs and integrates structural information from timestamps and semantic context from texts in one step, enabling a flexible and efficient interplay between modalities. Furthermore, we introduce a novel classifier-free guidance mechanism designed for multi-source conditioning, allowing for decoupled control over the guidance strength of textual and temporal information during inference, which significantly enhances model robustness. Extensive experiments on real-world benchmark datasets across eight domains demonstrate that the proposed UniDiff model achieves state-of-the-art performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 76,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07200",
        "abs_url": "https://arxiv.org/abs/2512.07200",
        "pdf_url": "https://arxiv.org/pdf/2512.07200",
        "title": "Less is More: Non-uniform Road Segments are Efficient for Bus Arrival Prediction",
        "authors": [
            "Zhen Huang",
            "Jiaxin Deng",
            "Jiayu Xu",
            "Junbiao Pang",
            "Haitao Yu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In bus arrival time prediction, the process of organizing road infrastructure network data into homogeneous entities is known as segmentation. Segmenting a road network is widely recognized as the first and most critical step in developing an arrival time prediction system, particularly for auto-regressive-based approaches. Traditional methods typically employ a uniform segmentation strategy, which fails to account for varying physical constraints along roads, such as road conditions, intersections, and points of interest, thereby limiting prediction efficiency. In this paper, we propose a Reinforcement Learning (RL)-based approach to efficiently and adaptively learn non-uniform road segments for arrival time prediction. Our method decouples the prediction process into two stages: 1) Non-uniform road segments are extracted based on their impact scores using the proposed RL framework; and 2) A linear prediction model is applied to the selected segments to make predictions. This method ensures optimal segment selection while maintaining computational efficiency, offering a significant improvement over traditional uniform approaches. Furthermore, our experimental results suggest that the linear approach can even achieve better performance than more complex methods. Extensive experiments demonstrate the superiority of the proposed method, which not only enhances efficiency but also improves learning performance on large-scale benchmarks. The dataset and the code are publicly accessible at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 77,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07222",
        "abs_url": "https://arxiv.org/abs/2512.07222",
        "pdf_url": "https://arxiv.org/pdf/2512.07222",
        "title": "Pay Less Attention to Function Words for Free Robustness of Vision-Language Models",
        "authors": [
            "Qiwei Tian",
            "Chenhao Lin",
            "Zhengyu Zhao",
            "Chao Shen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
        "abstract": "To address the trade-off between robustness and performance for robust VLM, we observe that function words could incur vulnerability of VLMs against cross-modal adversarial attacks, and propose Function-word De-Attention (FDA) accordingly to mitigate the impact of function words. Similar to differential amplifiers, our FDA calculates the original and the function-word cross-attention within attention heads, and differentially subtracts the latter from the former for more aligned and robust VLMs. Comprehensive experiments include 2 SOTA baselines under 6 different attacks on 2 downstream tasks, 3 datasets, and 3 models. Overall, our FDA yields an average 18/13/53% ASR drop with only 0.2/0.3/0.6% performance drops on the 3 tested models on retrieval, and a 90% ASR drop with a 0.3% performance gain on visual grounding. We demonstrate the scalability, generalization, and zero-shot performance of FDA experimentally, as well as in-depth ablation studies and analysis. Code will be made publicly at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 78,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07244",
        "abs_url": "https://arxiv.org/abs/2512.07244",
        "pdf_url": "https://arxiv.org/pdf/2512.07244",
        "title": "PINE: Pipeline for Important Node Exploration in Attributed Networks",
        "authors": [
            "Elizaveta Kovtun",
            "Maksim Makarenko",
            "Natalia Semenova",
            "Alexey Zaytsev",
            "Semen Budennyy"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "A graph with semantically attributed nodes are a common data structure in a wide range of domains. It could be interlinked web data or citation networks of scientific publications. The essential problem for such a data type is to determine nodes that carry greater importance than all the others, a task that markedly enhances system monitoring and management. Traditional methods to identify important nodes in networks introduce centrality measures, such as node degree or more complex PageRank. However, they consider only the network structure, neglecting the rich node attributes. Recent methods adopt neural networks capable of handling node features, but they require supervision. This work addresses the identified gap--the absence of approaches that are both unsupervised and attribute-aware--by introducing a Pipeline for Important Node Exploration (PINE). At the core of the proposed framework is an attention-based graph model that incorporates node semantic features in the learning process of identifying the structural graph properties. The PINE's node importance scores leverage the obtained attention distribution. We demonstrate the superior performance of the proposed PINE method on various homogeneous and heterogeneous attributed networks. As an industry-implemented system, PINE tackles the real-world challenge of unsupervised identification of key entities within large-scale enterprise graphs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 79,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07287",
        "abs_url": "https://arxiv.org/abs/2512.07287",
        "pdf_url": "https://arxiv.org/pdf/2512.07287",
        "title": "SIT-Graph: State Integrated Tool Graph for Multi-Turn Agents",
        "authors": [
            "Sijia Li",
            "Yuchen Huang",
            "Zifan Liu",
            "Zijian Li",
            "Jingjing fu",
            "Lei Song",
            "Jiang Bian",
            "Jun Zhang",
            "Rui Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Despite impressive advances in agent systems, multi-turn tool-use scenarios remain challenging. It is mainly because intent is clarified progressively and the environment evolves with each tool call. While reusing past experience is natural, current LLM agents either treat entire trajectories or pre-defined subtasks as indivisible units, or solely exploit tool-to-tool dependencies, hindering adaptation as states and information evolve across turns. In this paper, we propose a State Integrated Tool Graph (SIT-Graph), which enhances multi-turn tool use by exploiting partially overlapping experience. Inspired by human decision-making that integrates episodic and procedural memory, SIT-Graph captures both compact state representations (episodic-like fragments) and tool-to-tool dependencies (procedural-like routines) from historical trajectories. Specifically, we first build a tool graph from accumulated tool-use sequences, and then augment each edge with a compact state summary of the dialog and tool history that may shape the next action. At inference time, SIT-Graph enables a human-like balance between episodic recall and procedural execution: when the next decision requires recalling prior context, the agent retrieves the state summaries stored on relevant edges and uses them to guide its next action; when the step is routine, it follows high-confidence tool dependencies without explicit recall. Experiments across multiple stateful multi-turn tool-use benchmarks show that SIT-Graph consistently outperforms strong memory- and graph-based baselines, delivering more robust tool selection and more effective experience transfer.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 80,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07310",
        "abs_url": "https://arxiv.org/abs/2512.07310",
        "pdf_url": "https://arxiv.org/pdf/2512.07310",
        "title": "Towards a Relationship-Aware Transformer for Tabular Data",
        "authors": [
            "Andrei V. Konstantinov",
            "Valerii A. Zuev",
            "Lev V. Utkin"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Deep learning models for tabular data typically do not allow for imposing a graph of external dependencies between samples, which can be useful for accounting for relatedness in tasks such as treatment effect estimation. Graph neural networks only consider adjacent nodes, making them difficult to apply to sparse graphs. This paper proposes several solutions based on a modified attention mechanism, which accounts for possible relationships between data points by adding a term to the attention matrix. Our models are compared with each other and the gradient boosting decision trees in a regression task on synthetic and real-world datasets, as well as in a treatment effect estimation task on the IHDP dataset.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 81,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07313",
        "abs_url": "https://arxiv.org/abs/2512.07313",
        "pdf_url": "https://arxiv.org/pdf/2512.07313",
        "title": "Learning-Augmented Ski Rental with Discrete Distributions: A Bayesian Approach",
        "authors": [
            "Bosun Kang",
            "Hyejun Park",
            "Chenglin Fan"
        ],
        "comments": "7 pages",
        "subjects": "Machine Learning (cs.LG); Data Structures and Algorithms (cs.DS)",
        "abstract": "We revisit the classic ski rental problem through the lens of Bayesian decision-making and machine-learned predictions. While traditional algorithms minimize worst-case cost without assumptions, and recent learning-augmented approaches leverage noisy forecasts with robustness guarantees, our work unifies these perspectives. We propose a discrete Bayesian framework that maintains exact posterior distributions over the time horizon, enabling principled uncertainty quantification and seamless incorporation of expert priors. Our algorithm achieves prior-dependent competitive guarantees and gracefully interpolates between worst-case and fully-informed settings. Our extensive experimental evaluation demonstrates superior empirical performance across diverse scenarios, achieving near-optimal results under accurate priors while maintaining robust worst-case guarantees. This framework naturally extends to incorporate multiple predictions, non-uniform priors, and contextual information, highlighting the practical advantages of Bayesian reasoning in online decision problems with imperfect predictions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 82,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07332",
        "abs_url": "https://arxiv.org/abs/2512.07332",
        "pdf_url": "https://arxiv.org/pdf/2512.07332",
        "title": "Local-Curvature-Aware Knowledge Graph Embedding: An Extended Ricci Flow Approach",
        "authors": [
            "Zhengquan Luo",
            "Guy Tadmor",
            "Or Amar",
            "David Zeevi",
            "Zhiqiang Xu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Knowledge graph embedding (KGE) relies on the geometry of the embedding space to encode semantic and structural relations. Existing methods place all entities on one homogeneous manifold, Euclidean, spherical, hyperbolic, or their product/multi-curvature variants, to model linear, symmetric, or hierarchical patterns. Yet a predefined, homogeneous manifold cannot accommodate the sharply varying curvature that real-world graphs exhibit across local regions. Since this geometry is imposed a priori, any mismatch with the knowledge graph's local curvatures will distort distances between entities and hurt the expressiveness of the resulting KGE. To rectify this, we propose RicciKGE to have the KGE loss gradient coupled with local curvatures in an extended Ricci flow such that entity embeddings co-evolve dynamically with the underlying manifold geometry towards mutual adaptation. Theoretically, when the coupling coefficient is bounded and properly selected, we rigorously prove that i) all the edge-wise curvatures decay exponentially, meaning that the manifold is driven toward the Euclidean flatness; and ii) the KGE distances strictly converge to a global optimum, which indicates that geometric flattening and embedding optimization are promoting each other. Experimental improvements on link prediction and node classification benchmarks demonstrate RicciKGE's effectiveness in adapting to heterogeneous knowledge graph structures.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 83,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07374",
        "abs_url": "https://arxiv.org/abs/2512.07374",
        "pdf_url": "https://arxiv.org/pdf/2512.07374",
        "title": "Recover-to-Forget: Gradient Reconstruction from LoRA for Efficient LLM Unlearning",
        "authors": [
            "Yezi Liu",
            "Hanning Chen",
            "Wenjun Huang",
            "Yang Ni",
            "Mohsen Imani"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
        "abstract": "Unlearning in large foundation models (e.g., LLMs) is essential for enabling dynamic knowledge updates, enforcing data deletion rights, and correcting model behavior. However, existing unlearning methods often require full-model fine-tuning or access to the original training data, which limits their scalability and practicality. In this work, we introduce Recover-to-Forget (R2F), a novel framework for efficient unlearning in LLMs based on reconstructing full-model gradient directions from low-rank LoRA adapter updates. Rather than performing backpropagation through the full model, we compute gradients with respect to LoRA parameters using multiple paraphrased prompts and train a gradient decoder to approximate the corresponding full-model gradients. To ensure applicability to larger or black-box models, the decoder is trained on a proxy model and transferred to target models. We provide a theoretical analysis of cross-model generalization and demonstrate that our method achieves effective unlearning while preserving general model performance. Experimental results demonstrate that R2F offers a scalable and lightweight alternative for unlearning in pretrained LLMs without requiring full retraining or access to internal parameters.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 84,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07375",
        "abs_url": "https://arxiv.org/abs/2512.07375",
        "pdf_url": "https://arxiv.org/pdf/2512.07375",
        "title": "LUNE: Efficient LLM Unlearning via LoRA Fine-Tuning with Negative Examples",
        "authors": [
            "Yezi Liu",
            "Hanning Chen",
            "Wenjun Huang",
            "Yang Ni",
            "Mohsen Imani"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
        "abstract": "Large language models (LLMs) possess vast knowledge acquired from extensive training corpora, but they often cannot remove specific pieces of information when needed, which makes it hard to handle privacy, bias mitigation, and knowledge correction. Traditional model unlearning approaches require computationally expensive fine-tuning or direct weight editing, making them impractical for real-world deployment. In this work, we introduce LoRA-based Unlearning with Negative Examples (LUNE), a lightweight framework that performs negative-only unlearning by updating only low-rank adapters while freezing the backbone, thereby localizing edits and avoiding disruptive global changes. Leveraging Low-Rank Adaptation (LoRA), LUNE targets intermediate representations to suppress (or replace) requested knowledge with an order-of-magnitude lower compute and memory than full fine-tuning or direct weight editing. Extensive experiments on multiple factual unlearning tasks show that LUNE: (I) achieves effectiveness comparable to full fine-tuning and memory-editing methods, and (II) reduces computational cost by about an order of magnitude.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 85,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07393",
        "abs_url": "https://arxiv.org/abs/2512.07393",
        "pdf_url": "https://arxiv.org/pdf/2512.07393",
        "title": "Empirical Results for Adjusting Truncated Backpropagation Through Time while Training Neural Audio Effects",
        "authors": [
            "Yann Bourdin",
            "Pierrick Legrand",
            "Fanny Roche"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "This paper investigates the optimization of Truncated Backpropagation Through Time (TBPTT) for training neural networks in digital audio effect modeling, with a focus on dynamic range compression. The study evaluates key TBPTT hyperparameters -- sequence number, batch size, and sequence length -- and their influence on model performance. Using a convolutional-recurrent architecture, we conduct extensive experiments across datasets with and without conditionning by user controls. Results demonstrate that carefully tuning these parameters enhances model accuracy and training stability, while also reducing computational demands. Objective evaluations confirm improved performance with optimized settings, while subjective listening tests indicate that the revised TBPTT configuration maintains high perceptual quality.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 86,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07417",
        "abs_url": "https://arxiv.org/abs/2512.07417",
        "pdf_url": "https://arxiv.org/pdf/2512.07417",
        "title": "Adaptive Tuning of Parameterized Traffic Controllers via Multi-Agent Reinforcement Learning",
        "authors": [
            "Giray Önür",
            "Azita Dabiri",
            "Bart De Schutter"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Effective traffic control is essential for mitigating congestion in transportation networks. Conventional traffic management strategies, including route guidance, ramp metering, and traffic signal control, often rely on state feedback controllers, used for their simplicity and reactivity; however, they lack the adaptability required to cope with complex and time-varying traffic dynamics. This paper proposes a multi-agent reinforcement learning framework in which each agent adaptively tunes the parameters of a state feedback traffic controller, combining the reactivity of state feedback controllers with the adaptability of reinforcement learning. By tuning parameters at a lower frequency rather than directly determining control actions at a high frequency, the reinforcement learning agents achieve improved training efficiency while maintaining adaptability to varying traffic conditions. The multi-agent structure further enhances system robustness, as local controllers can operate independently in the event of partial failures. The proposed framework is evaluated on a simulated multi-class transportation network under varying traffic conditions. Results show that the proposed multi-agent framework outperforms the no control and fixed-parameter state feedback control cases, while performing on par with the single-agent RL-based adaptive state feedback control, with a much better resilience to partial failures.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 87,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07430",
        "abs_url": "https://arxiv.org/abs/2512.07430",
        "pdf_url": "https://arxiv.org/pdf/2512.07430",
        "title": "MIDG: Mixture of Invariant Experts with knowledge injection for Domain Generalization in Multimodal Sentiment Analysis",
        "authors": [
            "Yangle Li",
            "Danli Luo",
            "Haifeng Hu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Existing methods in domain generalization for Multimodal Sentiment Analysis (MSA) often overlook inter-modal synergies during invariant features extraction, which prevents the accurate capture of the rich semantic information within multimodal data. Additionally, while knowledge injection techniques have been explored in MSA, they often suffer from fragmented cross-modal knowledge, overlooking specific representations that exist beyond the confines of unimodal. To address these limitations, we propose a novel MSA framework designed for domain generalization. Firstly, the framework incorporates a Mixture of Invariant Experts model to extract domain-invariant features, thereby enhancing the model's capacity to learn synergistic relationships between modalities. Secondly, we design a Cross-Modal Adapter to augment the semantic richness of multimodal representations through cross-modal knowledge injection. Extensive domain experiments conducted on three datasets demonstrate that the proposed MIDG achieves superior performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 88,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07433",
        "abs_url": "https://arxiv.org/abs/2512.07433",
        "pdf_url": "https://arxiv.org/pdf/2512.07433",
        "title": "Mitigating Bias in Graph Hyperdimensional Computing",
        "authors": [
            "Yezi Liu",
            "William Youngwoo Chung",
            "Yang Ni",
            "Hanning Chen",
            "Mohsen Imani"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Social and Information Networks (cs.SI)",
        "abstract": "Graph hyperdimensional computing (HDC) has emerged as a promising paradigm for cognitive tasks, emulating brain-like computation with high-dimensional vectors known as hypervectors. While HDC offers robustness and efficiency on graph-structured data, its fairness implications remain largely unexplored. In this paper, we study fairness in graph HDC, where biases in data representation and decision rules can lead to unequal treatment of different groups. We show how hypervector encoding and similarity-based classification can propagate or even amplify such biases, and we propose a fairness-aware training framework, FairGHDC, to mitigate them. FairGHDC introduces a bias correction term, derived from a gap-based demographic-parity regularizer, and converts it into a scalar fairness factor that scales the update of the class hypervector for the ground-truth label. This enables debiasing directly in the hypervector space without modifying the graph encoder or requiring backpropagation. Experimental results on six benchmark datasets demonstrate that FairGHDC substantially reduces demographic-parity and equal-opportunity gaps while maintaining accuracy comparable to standard GNNs and fairness-aware GNNs. At the same time, FairGHDC preserves the computational advantages of HDC, achieving up to about one order of magnitude ($\\approx 10\\times$) speedup in training time on GPU compared to GNN and fairness-aware GNN baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 89,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07450",
        "abs_url": "https://arxiv.org/abs/2512.07450",
        "pdf_url": "https://arxiv.org/pdf/2512.07450",
        "title": "Forget and Explain: Transparent Verification of GNN Unlearning",
        "authors": [
            "Imran Ahsan",
            "Hyunwook Yu",
            "Jinsung Kim",
            "Mucheol Kim"
        ],
        "comments": "To appear in WSDM 2026 (ACM International Conference on Web Search and Data Mining). Code is available at this https URL",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Graph neural networks (GNNs) are increasingly used to model complex patterns in graph-structured data. However, enabling them to \"forget\" designated information remains challenging, especially under privacy regulations such as the GDPR. Existing unlearning methods largely optimize for efficiency and scalability, yet they offer little transparency, and the black-box nature of GNNs makes it difficult to verify whether forgetting has truly occurred. We propose an explainability-driven verifier for GNN unlearning that snapshots the model before and after deletion, using attribution shifts and localized structural changes (for example, graph edit distance) as transparent evidence. The verifier uses five explainability metrics: residual attribution, heatmap shift, explainability score deviation, graph edit distance, and a diagnostic graph rule shift. We evaluate two backbones (GCN, GAT) and four unlearning strategies (Retrain, GraphEditor, GNNDelete, IDEA) across five benchmarks (Cora, Citeseer, Pubmed, Coauthor-CS, Coauthor-Physics). Results show that Retrain and GNNDelete achieve near-complete forgetting, GraphEditor provides partial erasure, and IDEA leaves residual signals. These explanation deltas provide the primary, human-readable evidence of forgetting; we also report membership-inference ROC-AUC as a complementary, graph-wide privacy signal.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 90,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07463",
        "abs_url": "https://arxiv.org/abs/2512.07463",
        "pdf_url": "https://arxiv.org/pdf/2512.07463",
        "title": "Parallel Algorithms for Combined Regularized Support Vector Machines: Application in Music Genre Classification",
        "authors": [
            "Rongmei Liang",
            "Zizheng Liu",
            "Xiaofei Wu",
            "Jingwen Tu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Applications (stat.AP); Computation (stat.CO)",
        "abstract": "In the era of rapid development of artificial intelligence, its applications span across diverse fields, relying heavily on effective data processing and model optimization. Combined Regularized Support Vector Machines (CR-SVMs) can effectively handle the structural information among data features, but there is a lack of efficient algorithms in distributed-stored big data. To address this issue, we propose a unified optimization framework based on consensus structure. This framework is not only applicable to various loss functions and combined regularization terms but can also be effectively extended to non-convex regularization terms, showing strong scalability. Based on this framework, we develop a distributed parallel alternating direction method of multipliers (ADMM) algorithm to efficiently compute CR-SVMs when data is stored in a distributed manner. To ensure the convergence of the algorithm, we also introduce the Gaussian back-substitution method. Meanwhile, for the integrity of the paper, we introduce a new model, the sparse group lasso support vector machine (SGL-SVM), and apply it to music information retrieval. Theoretical analysis confirms that the computational complexity of the proposed algorithm is not affected by different regularization terms and loss functions, highlighting the universality of the parallel algorithm. Experiments on synthetic and free music archiv datasets demonstrate the reliability, stability, and efficiency of the algorithm.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 91,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07486",
        "abs_url": "https://arxiv.org/abs/2512.07486",
        "pdf_url": "https://arxiv.org/pdf/2512.07486",
        "title": "Materium: An Autoregressive Approach for Material Generation",
        "authors": [
            "Niklas Dobberstein",
            "Jan Hamaekers"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Materials Science (cond-mat.mtrl-sci)",
        "abstract": "We present Materium: an autoregressive transformer for generating crystal structures that converts 3D material representations into token sequences. These sequences include elements with oxidation states, fractional coordinates and lattice parameters. Unlike diffusion approaches, which refine atomic positions iteratively through many denoising steps, Materium places atoms at precise fractional coordinates, enabling fast, scalable generation. With this design, the model can be trained in a few hours on a single GPU and generate samples much faster on GPUs and CPUs than diffusion-based approaches. The model was trained and evaluated using multiple properties as conditions, including fundamental properties, such as density and space group, as well as more practical targets, such as band gap and magnetic density. In both single and combined conditions, the model performs consistently well, producing candidates that align with the requested inputs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 92,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07490",
        "abs_url": "https://arxiv.org/abs/2512.07490",
        "pdf_url": "https://arxiv.org/pdf/2512.07490",
        "title": "Efficient Low-Tubal-Rank Tensor Estimation via Alternating Preconditioned Gradient Descent",
        "authors": [
            "Zhiyu Liu",
            "Zhi Han",
            "Yandong Tang",
            "Jun Fan",
            "Yao Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Optimization and Control (math.OC)",
        "abstract": "The problem of low-tubal-rank tensor estimation is a fundamental task with wide applications across high-dimensional signal processing, machine learning, and image science. Traditional approaches tackle such a problem by performing tensor singular value decomposition, which is computationally expensive and becomes infeasible for large-scale tensors. Recent approaches address this issue by factorizing the tensor into two smaller factor tensors and solving the resulting problem using gradient descent. However, this kind of approach requires an accurate estimate of the tensor rank, and when the rank is overestimated, the convergence of gradient descent and its variants slows down significantly or even diverges. To address this problem, we propose an Alternating Preconditioned Gradient Descent (APGD) algorithm, which accelerates convergence in the over-parameterized setting by adding a preconditioning term to the original gradient and updating these two factors alternately. Based on certain geometric assumptions on the objective function, we establish linear convergence guarantees for more general low-tubal-rank tensor estimation problems. Then we further analyze the specific cases of low-tubal-rank tensor factorization and low-tubal-rank tensor recovery. Our theoretical results show that APGD achieves linear convergence even under over-parameterization, and the convergence rate is independent of the tensor condition number. Extensive simulations on synthetic data are carried out to validate our theoretical assertions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 93,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07519",
        "abs_url": "https://arxiv.org/abs/2512.07519",
        "pdf_url": "https://arxiv.org/pdf/2512.07519",
        "title": "Machine Learning: Progress and Prospects",
        "authors": [
            "Alexander Gammerman"
        ],
        "comments": "Inaugural Lecture. 18 pages, 13 figures, Published in 1997 by Royal Holloway, University of London, ISBN 0 900145 93 5",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "This Inaugural Lecture was given at Royal Holloway University of London in 1996. It covers an introduction to machine learning and describes various theoretical advances and practical projects in the field. The Lecture here is presented in its original format, but a few remarks have been added in 2025 to reflect recent developments, and the list of references has been updated to enhance the convenience and accuracy for readers. When did machine learning start? Maybe a good starting point is 1949, when Claude Shannon proposed a learning algorithm for chess-playing programs. Or maybe we should go back to the 1930s when Ronald Fisher developed discriminant analysis - a type of learning where the problem is to construct a decision rule that separates two types of vectors. Or could it be the 18th century when David Hume discussed the idea of induction? Or the 14th century, when William of Ockham formulated the principle of \"simplicity\" known as \"Ockham's razor\" (Ockham, by the way, is a small village not far from Royal Holloway). Or it may be that, like almost everything else in Western civilisation and culture, the origin of these ideas lies in the Mediterranean. After all, it was Aristotle who said that \"we learn some things only by doing things\". The field of machine learning has been greatly influenced by other disciplines and the subject is in itself not a very homogeneous discipline, but includes separate, overlapping subfields. There are many parallel lines of research in ML: inductive learning, neural networks, clustering, and theories of learning. They are all part of the more general field of machine learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 94,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07539",
        "abs_url": "https://arxiv.org/abs/2512.07539",
        "pdf_url": "https://arxiv.org/pdf/2512.07539",
        "title": "FRWKV:Frequency-Domain Linear Attention for Long-Term Time Series Forecasting",
        "authors": [
            "Qingyuan Yang",
            "Shizhuo",
            "Dongyue Chen",
            "Da Teng",
            "Zehua Gan"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Traditional Transformers face a major bottleneck in long-sequence time series forecasting due to their quadratic complexity $(\\mathcal{O}(T^2))$ and their limited ability to effectively exploit frequency-domain information. Inspired by RWKV's $\\mathcal{O}(T)$ linear attention and frequency-domain modeling, we propose FRWKV, a frequency-domain linear-attention framework that overcomes these limitations. Our model integrates linear attention mechanisms with frequency-domain analysis, achieving $\\mathcal{O}(T)$ computational complexity in the attention path while exploiting spectral information to enhance temporal feature representations for scalable long-sequence modeling. Across eight real-world datasets, FRWKV achieves a first-place average rank. Our ablation studies confirm the critical roles of both the linear attention and frequency-encoder components. This work demonstrates the powerful synergy between linear attention and frequency analysis, establishing a new paradigm for scalable time series modeling. Code is available at this repository: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 95,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07542",
        "abs_url": "https://arxiv.org/abs/2512.07542",
        "pdf_url": "https://arxiv.org/pdf/2512.07542",
        "title": "RRAEDy: Adaptive Latent Linearization of Nonlinear Dynamical Systems",
        "authors": [
            "Jad Mounayer",
            "Sebastian Rodriguez",
            "Jerome Tomezyk",
            "Chady Ghnatios",
            "Francisco Chinesta"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Most existing latent-space models for dynamical systems require fixing the latent dimension in advance, they rely on complex loss balancing to approximate linear dynamics, and they don't regularize the latent variables. We introduce RRAEDy, a model that removes these limitations by discovering the appropriate latent dimension, while enforcing both regularized and linearized dynamics in the latent space. Built upon Rank-Reduction Autoencoders (RRAEs), RRAEDy automatically rank and prune latent variables through their singular values while learning a latent Dynamic Mode Decomposition (DMD) operator that governs their temporal progression. This structure-free yet linearly constrained formulation enables the model to learn stable and low-dimensional dynamics without auxiliary losses or manual tuning. We provide theoretical analysis demonstrating the stability of the learned operator and showcase the generality of our model by proposing an extension that handles parametric ODEs. Experiments on canonical benchmarks, including the Van der Pol oscillator, Burgers' equation, 2D Navier-Stokes, and Rotating Gaussians, show that RRAEDy achieves accurate and robust predictions. Our code is open-source and available at this https URL. We also provide a video summarizing the main results at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 96,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07569",
        "abs_url": "https://arxiv.org/abs/2512.07569",
        "pdf_url": "https://arxiv.org/pdf/2512.07569",
        "title": "Weighted Contrastive Learning for Anomaly-Aware Time-Series Forecasting",
        "authors": [
            "Joel Ekstrand",
            "Tor Mattsson",
            "Zahra Taghiyarrenani",
            "Slawomir Nowaczyk",
            "Jens Lundström",
            "Mikael Lindén"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Reliable forecasting of multivariate time series under anomalous conditions is crucial in applications such as ATM cash logistics, where sudden demand shifts can disrupt operations. Modern deep forecasters achieve high accuracy on normal data but often fail when distribution shifts occur. We propose Weighted Contrastive Adaptation (WECA), a Weighted contrastive objective that aligns normal and anomaly-augmented representations, preserving anomaly-relevant information while maintaining consistency under benign variations. Evaluations on a nationwide ATM transaction dataset with domain-informed anomaly injection show that WECA improves SMAPE on anomaly-affected data by 6.1 percentage points compared to a normally trained baseline, with negligible degradation on normal data. These results demonstrate that WECA enhances forecasting reliability under anomalies without sacrificing performance during regular operations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 97,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07647",
        "abs_url": "https://arxiv.org/abs/2512.07647",
        "pdf_url": "https://arxiv.org/pdf/2512.07647",
        "title": "A Mathematical Theory of Top-$k$ Sparse Attention via Total Variation Distance",
        "authors": [
            "Georgios Tzachristas",
            "Lei Deng",
            "Ioannis Tzachristas",
            "Gong Zhang",
            "Renhai Chen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "We develop a unified mathematical framework for certified Top-$k$ attention truncation that quantifies approximation error at both the distribution and output levels. For a single attention distribution $P$ and its Top-$k$ truncation $\\hat P$, we show that the total-variation distance coincides with the discarded softmax tail mass and satisfies $\\mathrm{TV}(P,\\hat P)=1-e^{-\\mathrm{KL}(\\hat P\\Vert P)}$, yielding sharp Top-$k$-specific bounds in place of generic inequalities. From this we derive non-asymptotic deterministic bounds -- from a single boundary gap through multi-gap and blockwise variants -- that control $\\mathrm{TV}(P,\\hat P)$ using only the ordered logits. Using an exact head-tail decomposition, we prove that the output error factorizes as $\\|\\mathrm{Attn}(q,K,V)-\\mathrm{Attn}_k(q,K,V)\\|_2=\\tau\\|\\mu_{\\mathrm{tail}}-\\mu_{\\mathrm{head}}\\|_2$ with $\\tau=\\mathrm{TV}(P,\\hat P)$, yielding a new head-tail diameter bound $\\|\\mathrm{Attn}(q,K,V)-\\mathrm{Attn}_k(q,K,V)\\|_2\\le\\tau\\,\\mathrm{diam}_{H,T}$ and refinements linking the error to $\\mathrm{Var}_P(V)$. Under an i.i.d. Gaussian score model $s_i\\sim\\mathcal N(\\mu,\\sigma^2)$ we derive closed-form tail masses and an asymptotic rule for the minimal $k_\\varepsilon$ ensuring $\\mathrm{TV}(P,\\hat P)\\le\\varepsilon$, namely $k_\\varepsilon/n\\approx\\Phi_c(\\sigma+\\Phi^{-1}(\\varepsilon))$. Experiments on bert-base-uncased and synthetic logits confirm the predicted scaling of $k_\\varepsilon/n$ and show that certified Top-$k$ can reduce scored keys by 2-4$\\times$ on average while meeting the prescribed total-variation budget.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 98,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07667",
        "abs_url": "https://arxiv.org/abs/2512.07667",
        "pdf_url": "https://arxiv.org/pdf/2512.07667",
        "title": "Depth-Wise Activation Steering for Honest Language Models",
        "authors": [
            "Gracjan Góral",
            "Marysia Winkels",
            "Steven Basart"
        ],
        "comments": "See \\url{this https URL}. for code and experiments",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Large language models sometimes assert falsehoods despite internally representing the correct answer, failures of honesty rather than accuracy, which undermines auditability and safety. Existing approaches largely optimize factual correctness or depend on retraining and brittle single-layer edits, offering limited leverage over truthful reporting. We present a training-free activation steering method that weights steering strength across network depth using a Gaussian schedule. On the MASK benchmark, which separates honesty from knowledge, we evaluate seven models spanning the LLaMA, Qwen, and Mistral families and find that Gaussian scheduling improves honesty over no-steering and single-layer baselines in six of seven models. Equal-budget ablations on LLaMA-3.1-8B-Instruct and Qwen-2.5-7B-Instruct show the Gaussian schedule outperforms random, uniform, and box-filter depth allocations, indicating that how intervention is distributed across depth materially affects outcomes beyond total strength. The method is simple, model-agnostic, requires no finetuning, and provides a low-cost control knob for eliciting truthful reporting from models' existing capabilities.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 99,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07676",
        "abs_url": "https://arxiv.org/abs/2512.07676",
        "pdf_url": "https://arxiv.org/pdf/2512.07676",
        "title": "A Bootstrap Perspective on Stochastic Gradient Descent",
        "authors": [
            "Hongjian Lan",
            "Yucong Liu",
            "Florian Schäfer"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Machine learning models trained with \\emph{stochastic} gradient descent (SGD) can generalize better than those trained with deterministic gradient descent (GD). In this work, we study SGD's impact on generalization through the lens of the statistical bootstrap: SGD uses gradient variability under batch sampling as a proxy for solution variability under the randomness of the data collection process. We use empirical results and theoretical analysis to substantiate this claim. In idealized experiments on empirical risk minimization, we show that SGD is drawn to parameter choices that are robust under resampling and thus avoids spurious solutions even if they lie in wider and deeper minima of the training loss. We prove rigorously that by implicitly regularizing the trace of the gradient covariance matrix, SGD controls the algorithmic variability. This regularization leads to solutions that are less sensitive to sampling noise, thereby improving generalization. Numerical experiments on neural network training show that explicitly incorporating the estimate of the algorithmic variability as a regularizer improves test performance. This fact supports our claim that bootstrap estimation underpins SGD's generalization advantages.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 100,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07705",
        "abs_url": "https://arxiv.org/abs/2512.07705",
        "pdf_url": "https://arxiv.org/pdf/2512.07705",
        "title": "In-Context and Few-Shots Learning for Forecasting Time Series Data based on Large Language Models",
        "authors": [
            "Saroj Gopali",
            "Bipin Chhetri",
            "Deepika Giri",
            "Sima Siami-Namini",
            "Akbar Siami Namin"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Existing data-driven approaches in modeling and predicting time series data include ARIMA (Autoregressive Integrated Moving Average), Transformer-based models, LSTM (Long Short-Term Memory) and TCN (Temporal Convolutional Network). These approaches, and in particular deep learning-based models such as LSTM and TCN, have shown great results in predicting time series data. With the advancement of leveraging pre-trained foundation models such as Large Language Models (LLMs) and more notably Google's recent foundation model for time series data, {\\it TimesFM} (Time Series Foundation Model), it is of interest to investigate whether these foundation models have the capability of outperforming existing modeling approaches in analyzing and predicting time series data. This paper investigates the performance of using LLM models for time series data prediction. We investigate the in-context learning methodology in the training of LLM models that are specific to the underlying application domain. More specifically, the paper explores training LLMs through in-context, zero-shot and few-shot learning and forecasting time series data with OpenAI {\\tt o4-mini} and Gemini 2.5 Flash Lite, as well as the recent Google's Transformer-based TimesFM, a time series-specific foundation model, along with two deep learning models, namely TCN and LSTM networks. The findings indicate that TimesFM has the best overall performance with the lowest RMSE value (0.3023) and the competitive inference time (266 seconds). Furthermore, OpenAI's o4-mini also exhibits a good performance based on Zero Shot learning. These findings highlight pre-trained time series foundation models as a promising direction for real-time forecasting, enabling accurate and scalable deployment with minimal model adaptation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 101,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07741",
        "abs_url": "https://arxiv.org/abs/2512.07741",
        "pdf_url": "https://arxiv.org/pdf/2512.07741",
        "title": "A multimodal Bayesian Network for symptom-level depression and anxiety prediction from voice and speech data",
        "authors": [
            "Agnes Norbury",
            "George Fairs",
            "Alexandra L. Georgescu",
            "Matthew M. Nour",
            "Emilia Molimpakis",
            "Stefano Goria"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Sound (cs.SD)",
        "abstract": "During psychiatric assessment, clinicians observe not only what patients report, but important nonverbal signs such as tone, speech rate, fluency, responsiveness, and body language. Weighing and integrating these different information sources is a challenging task and a good candidate for support by intelligence-driven tools - however this is yet to be realized in the clinic. Here, we argue that several important barriers to adoption can be addressed using Bayesian network modelling. To demonstrate this, we evaluate a model for depression and anxiety symptom prediction from voice and speech features in large-scale datasets (30,135 unique speakers). Alongside performance for conditions and symptoms (for depression, anxiety ROC-AUC=0.842,0.831 ECE=0.018,0.015; core individual symptom ROC-AUC>0.74), we assess demographic fairness and investigate integration across and redundancy between different input modality types. Clinical usefulness metrics and acceptability to mental health service users are explored. When provided with sufficiently rich and large-scale multimodal data streams and specified to represent common mental conditions at the symptom rather than disorder level, such models are a principled approach for building robust assessment support tools: providing clinically-relevant outputs in a transparent and explainable format that is directly amenable to expert clinical supervision.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 102,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07766",
        "abs_url": "https://arxiv.org/abs/2512.07766",
        "pdf_url": "https://arxiv.org/pdf/2512.07766",
        "title": "Formalized Hopfield Networks and Boltzmann Machines",
        "authors": [
            "Matteo Cipollina",
            "Michail Karatarakis",
            "Freek Wiedijk"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Logic in Computer Science (cs.LO)",
        "abstract": "Neural networks are widely used, yet their analysis and verification remain challenging. In this work, we present a Lean 4 formalization of neural networks, covering both deterministic and stochastic models. We first formalize Hopfield networks, recurrent networks that store patterns as stable states. We prove convergence and the correctness of Hebbian learning, a training rule that updates network parameters to encode patterns, here limited to the case of pairwise-orthogonal patterns. We then consider stochastic networks, where updates are probabilistic and convergence is to a stationary distribution. As a canonical example, we formalize the dynamics of Boltzmann machines and prove their ergodicity, showing convergence to a unique stationary distribution using a new formalization of the Perron-Frobenius theorem.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 103,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07782",
        "abs_url": "https://arxiv.org/abs/2512.07782",
        "pdf_url": "https://arxiv.org/pdf/2512.07782",
        "title": "GatedFWA: Linear Flash Windowed Attention with Gated Associative Memory",
        "authors": [
            "Jiaxu Liu",
            "Yuhe Bai",
            "Christos-Savvas Bouganis"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Modern autoregressive models rely on attention, yet the Softmax full attention in Transformers scales quadratically with sequence length. Sliding Window Attention (SWA) achieves linear-time encoding/decoding by constraining the attention pattern, but under an \\textit{Associative Memory} interpretation, its difference-style update renders the training objective effectively \\emph{unbounded}. In contrast, Softmax attention normalizes updates, leading to \\emph{memory shrinkage and gradient vanishing}. We propose GatedFWA: a Memory-\\underline{Gated} (\\underline{F}lash) \\underline{W}indowed \\underline{A}ttention mechanism that preserves SWAs efficiency while stabilizing memory updates and making gradient flow controllable. In essence, GatedFWA accumulate a per-token/head gate into a decay bias added to the attention logits, acting as a learnable contraction in the memory recurrence. We implement a fused one-pass gate preprocessing and a FlashAttention-compatible kernel that injects the gate under a sliding mask, ensuring I/O efficiency and numerical stability. On language modelling benchmarks, GatedFWA delivers competitive throughput with negligible overhead and better use of global context, and it integrates cleanly with token compression/selection methods such as NSA and generalizes to various autoregressive domains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 104,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07818",
        "abs_url": "https://arxiv.org/abs/2512.07818",
        "pdf_url": "https://arxiv.org/pdf/2512.07818",
        "title": "Provable Long-Range Benefits of Next-Token Prediction",
        "authors": [
            "Xinyuan Cao",
            "Santosh S. Vempala"
        ],
        "comments": "66 pages, 5 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "Why do modern language models, trained to do well on next-word prediction, appear to generate coherent documents and capture long-range structure? Here we show that next-token prediction is provably powerful for learning longer-range structure, even with common neural network architectures. Specifically, we prove that optimizing next-token prediction over a Recurrent Neural Network (RNN) yields a model that closely approximates the training distribution: for held-out documents sampled from the training distribution, no algorithm of bounded description length limited to examining the next $k$ tokens, for any $k$, can distinguish between $k$ consecutive tokens of such documents and $k$ tokens generated by the learned language model following the same prefix. We provide polynomial bounds (in $k$, independent of the document length) on the model size needed to achieve such $k$-token indistinguishability, offering a complexity-theoretic explanation for the long-range coherence observed in practice.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 105,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07828",
        "abs_url": "https://arxiv.org/abs/2512.07828",
        "pdf_url": "https://arxiv.org/pdf/2512.07828",
        "title": "The Adoption and Usage of AI Agents: Early Evidence from Perplexity",
        "authors": [
            "Jeremy Yang",
            "Noah Yonack",
            "Kate Zyskowski",
            "Denis Yarats",
            "Johnny Ho",
            "Jerry Ma"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); General Economics (econ.GN)",
        "abstract": "This paper presents the first large-scale field study of the adoption, usage intensity, and use cases of general-purpose AI agents operating in open-world web environments. Our analysis centers on Comet, an AI-powered browser developed by Perplexity, and its integrated agent, Comet Assistant. Drawing on hundreds of millions of anonymized user interactions, we address three fundamental questions: Who is using AI agents? How intensively are they using them? And what are they using them for? Our findings reveal substantial heterogeneity in adoption and usage across user segments. Earlier adopters, users in countries with higher GDP per capita and educational attainment, and individuals working in digital or knowledge-intensive sectors -- such as digital technology, academia, finance, marketing, and entrepreneurship -- are more likely to adopt or actively use the agent. To systematically characterize the substance of agent usage, we introduce a hierarchical agentic taxonomy that organizes use cases across three levels: topic, subtopic, and task. The two largest topics, Productivity & Workflow and Learning & Research, account for 57% of all agentic queries, while the two largest subtopics, Courses and Shopping for Goods, make up 22%. The top 10 out of 90 tasks represent 55% of queries. Personal use constitutes 55% of queries, while professional and educational contexts comprise 30% and 16%, respectively. In the short term, use cases exhibit strong stickiness, but over time users tend to shift toward more cognitively oriented topics. The diffusion of increasingly capable AI agents carries important implications for researchers, businesses, policymakers, and educators, inviting new lines of inquiry into this rapidly emerging class of AI capabilities.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 106,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.02272",
        "abs_url": "https://arxiv.org/abs/2512.02272",
        "pdf_url": "https://arxiv.org/pdf/2512.02272",
        "title": "Intrusion Detection on Resource-Constrained IoT Devices with Hardware-Aware ML and DL",
        "authors": [
            "Ali Diab",
            "Adel Chehade",
            "Edoardo Ragusa",
            "Paolo Gastaldo",
            "Rodolfo Zunino",
            "Amer Baghdadi",
            "Mostafa Rizk"
        ],
        "comments": "Accepted at the 2025 IEEE International Conference on Emerging Trends in Engineering and Computing (ETECOM). Recipient of the ETECOM 2025 Best Paper Award",
        "subjects": "Networking and Internet Architecture (cs.NI); Machine Learning (cs.LG)",
        "abstract": "This paper proposes a hardware-aware intrusion detection system (IDS) for Internet of Things (IoT) and Industrial IoT (IIoT) networks; it targets scenarios where classification is essential for fast, privacy-preserving, and resource-efficient threat detection. The goal is to optimize both tree-based machine learning (ML) models and compact deep neural networks (DNNs) within strict edge-device constraints. This allows for a fair comparison and reveals trade-offs between model families. We apply constrained grid search for tree-based classifiers and hardware-aware neural architecture search (HW-NAS) for 1D convolutional neural networks (1D-CNNs). Evaluation on the Edge-IIoTset benchmark shows that selected models meet tight flash, RAM, and compute limits: LightGBM achieves 95.3% accuracy using 75 KB flash and 1.2 K operations, while the HW-NAS-optimized CNN reaches 97.2% with 190 KB flash and 840 K floating-point operations (FLOPs). We deploy the full pipeline on a Raspberry Pi 3 B Plus, confirming that tree-based models operate within 30 ms and that CNNs remain suitable when accuracy outweighs latency. These results highlight the practicality of hardware-constrained model design for real-time IDS at the edge.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 107,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.05976",
        "abs_url": "https://arxiv.org/abs/2512.05976",
        "pdf_url": "https://arxiv.org/pdf/2512.05976",
        "title": "Physics Enhanced Deep Surrogates for the Phonon Boltzmann Transport Equation",
        "authors": [
            "Antonio Varagnolo",
            "Giuseppe Romano",
            "Raphaël Pestourie"
        ],
        "comments": "",
        "subjects": "Computational Physics (physics.comp-ph); Machine Learning (cs.LG)",
        "abstract": "Designing materials with controlled heat flow at the nano-scale is central to advances in microelectronics, thermoelectrics, and energy-conversion technologies. At these scales, phonon transport follows the Boltzmann Transport Equation (BTE), which captures non-diffusive (ballistic) effects but is too costly to solve repeatedly in inverse-design loops. Existing surrogate approaches trade speed for accuracy: fast macroscopic solvers can overestimate conductivities by hundreds of percent, while recent data-driven operator learners often require thousands of high-fidelity simulations. This creates a need for a fast, data-efficient surrogate that remains reliable across ballistic and diffusive regimes. We introduce a Physics-Enhanced Deep Surrogate (PEDS) that combines a differentiable Fourier solver with a neural generator and couples it with uncertainty-driven active learning. The Fourier solver acts as a physical inductive bias, while the network learns geometry-dependent corrections and a mixing coefficient that interpolates between macroscopic and nano-scale behavior. PEDS reduces training-data requirements by up to 70% compared with purely data-driven baselines, achieves roughly 5% fractional error with only 300 high-fidelity BTE simulations, and enables efficient design of porous geometries spanning 12-85 W m$^{-1}$ K$^{-1}$ with average design errors of 4%. The learned mixing parameter recovers the ballistic-diffusive transition and improves out of distribution robustness. These results show that embedding simple, differentiable low-fidelity physics can dramatically increase surrogate data-efficiency and interpretability, making repeated PDE-constrained optimization practical for nano-scale thermal-materials design.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 108,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.05988",
        "abs_url": "https://arxiv.org/abs/2512.05988",
        "pdf_url": "https://arxiv.org/pdf/2512.05988",
        "title": "VG3T: Visual Geometry Grounded Gaussian Transformer",
        "authors": [
            "Junho Kim",
            "Seongwon Lee"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Generating a coherent 3D scene representation from multi-view images is a fundamental yet challenging task. Existing methods often struggle with multi-view fusion, leading to fragmented 3D representations and sub-optimal performance. To address this, we introduce VG3T, a novel multi-view feed-forward network that predicts a 3D semantic occupancy via a 3D Gaussian representation. Unlike prior methods that infer Gaussians from single-view images, our model directly predicts a set of semantically attributed Gaussians in a joint, multi-view fashion. This novel approach overcomes the fragmentation and inconsistency inherent in view-by-view processing, offering a unified paradigm to represent both geometry and semantics. We also introduce two key components, Grid-Based Sampling and Positional Refinement, to mitigate the distance-dependent density bias common in pixel-aligned Gaussian initialization methods. Our VG3T shows a notable 1.7%p improvement in mIoU while using 46% fewer primitives than the previous state-of-the-art on the nuScenes benchmark, highlighting its superior efficiency and performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 109,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06031",
        "abs_url": "https://arxiv.org/abs/2512.06031",
        "pdf_url": "https://arxiv.org/pdf/2512.06031",
        "title": "Multi-resolution Physics-Aware Recurrent Convolutional Neural Network for Complex Flows",
        "authors": [
            "Xinlun Cheng",
            "Joseph Choi",
            "H.S. Udaykumar",
            "Stephen Baek"
        ],
        "comments": "",
        "subjects": "Fluid Dynamics (physics.flu-dyn); Machine Learning (cs.LG); Computational Physics (physics.comp-ph)",
        "abstract": "We present MRPARCv2, Multi-resolution Physics-Aware Recurrent Convolutional Neural Network, designed to model complex flows by embedding the structure of advection-diffusion-reaction equations and leveraging a multi-resolution architecture. MRPARCv2 introduces hierarchical discretization and cross-resolution feature communication to improve the accuracy and efficiency of flow simulations. We evaluate the model on a challenging 2D turbulent radiative layer dataset from The Well multi-physics benchmark repository and demonstrate significant improvements when compared to the single resolution baseline model, in both Variance Scaled Root Mean Squared Error and physics-driven metrics, including turbulent kinetic energy spectra and mass-temperature distributions. Despite having 30% fewer trainable parameters, MRPARCv2 outperforms its predecessor by up to 50% in roll-out prediction error and 86% in spectral error. A preliminary study on uncertainty quantification was performed, and we also analyzed the model's performance under different levels of abstractions of the flow, specifically on sampling subsets of field variables. We find that the absence of physical constraints on the equation of state (EOS) in the network architecture leads to degraded accuracy. A variable substitution experiment confirms that this issue persists regardless of which physical quantity is predicted directly. Our findings highlight the advantages of multi-resolution inductive bias for capturing multi-scale flow dynamics and suggest the need for future PIML models to embed EOS knowledge to enhance physical fidelity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 110,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06038",
        "abs_url": "https://arxiv.org/abs/2512.06038",
        "pdf_url": "https://arxiv.org/pdf/2512.06038",
        "title": "Closed-Loop Robotic Manipulation of Transparent Substrates for Self-Driving Laboratories using Deep Learning Micro-Error Correction",
        "authors": [
            "Kelsey Fontenot",
            "Anjali Gorti",
            "Iva Goel",
            "Tonio Buonassisi",
            "Alexander E. Siemenn"
        ],
        "comments": "15 pages, 8 figures",
        "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
        "abstract": "Self-driving laboratories (SDLs) have accelerated the throughput and automation capabilities for discovering and improving chemistries and materials. Although these SDLs have automated many of the steps required to conduct chemical and materials experiments, a commonly overlooked step in the automation pipeline is the handling and reloading of substrates used to transfer or deposit materials onto for downstream characterization. Here, we develop a closed-loop method of Automated Substrate Handling and Exchange (ASHE) using robotics, dual-actuated dispensers, and deep learning-driven computer vision to detect and correct errors in the manipulation of fragile and transparent substrates for SDLs. Using ASHE, we demonstrate a 98.5% first-time placement accuracy across 130 independent trials of reloading transparent glass substrates into an SDL, where only two substrate misplacements occurred and were successfully detected as errors and automatically corrected. Through the development of more accurate and reliable methods for handling various types of substrates, we move toward an improvement in the automation capabilities of self-driving laboratories, furthering the acceleration of novel chemical and materials discoveries.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 111,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06109",
        "abs_url": "https://arxiv.org/abs/2512.06109",
        "pdf_url": "https://arxiv.org/pdf/2512.06109",
        "title": "Unifying Entropy Regularization in Optimal Control: From and Back to Classical Objectives via Iterated Soft Policies and Path Integral Solutions",
        "authors": [
            "Ajinkya Bhole",
            "Mohammad Mahmoudi Filabadi",
            "Guillaume Crevecoeur",
            "Tom Lefebvre"
        ],
        "comments": "",
        "subjects": "Optimization and Control (math.OC); Machine Learning (cs.LG); Robotics (cs.RO); Systems and Control (eess.SY)",
        "abstract": "This paper develops a unified perspective on several stochastic optimal control formulations through the lens of Kullback-Leibler regularization. We propose a central problem that separates the KL penalties on policies and transitions, assigning them independent weights, thereby generalizing the standard trajectory-level KL-regularization commonly used in probabilistic and KL-regularized control. This generalized formulation acts as a generative structure allowing to recover various control problems. These include the classical Stochastic Optimal Control (SOC), Risk-Sensitive Optimal Control (RSOC), and their policy-based KL-regularized counterparts. The latter we refer to as soft-policy SOC and RSOC, facilitating alternative problems with tractable solutions. Beyond serving as regularized variants, we show that these soft-policy formulations majorize the original SOC and RSOC problem. This means that the regularized solution can be iterated to retrieve the original solution. Furthermore, we identify a structurally synchronized case of the risk-seeking soft-policy RSOC formulation, wherein the policy and transition KL-regularization weights coincide. Remarkably, this specific setting gives rise to several powerful properties such as a linear Bellman equation, path integral solution, and, compositionality, thereby extending these computationally favourable properties to a broad class of control problems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 112,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06113",
        "abs_url": "https://arxiv.org/abs/2512.06113",
        "pdf_url": "https://arxiv.org/pdf/2512.06113",
        "title": "Hardware Software Optimizations for Fast Model Recovery on Reconfigurable Architectures",
        "authors": [
            "Bin Xu",
            "Ayan Banerjee",
            "Sandeep Gupta"
        ],
        "comments": "",
        "subjects": "Hardware Architecture (cs.AR); Machine Learning (cs.LG)",
        "abstract": "Model Recovery (MR) is a core primitive for physical AI and real-time digital twins, but GPUs often execute MR inefficiently due to iterative dependencies, kernel-launch overheads, underutilized memory bandwidth, and high data-movement latency. We present MERINDA, an FPGA-accelerated MR framework that restructures computation as a streaming dataflow pipeline. MERINDA exploits on-chip locality through BRAM tiling, fixed-point kernels, and the concurrent use of LUT fabric and carry-chain adders to expose fine-grained spatial parallelism while minimizing off-chip traffic. This hardware-aware formulation removes synchronization bottlenecks and sustains high throughput across the iterative updates in MR. On representative MR workloads, MERINDA delivers up to 6.3x fewer cycles than an FPGA-based LTC baseline, enabling real-time performance for time-critical physical systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 113,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06181",
        "abs_url": "https://arxiv.org/abs/2512.06181",
        "pdf_url": "https://arxiv.org/pdf/2512.06181",
        "title": "Beyond Lux thresholds: a systematic pipeline for classifying biologically relevant light contexts from wearable data",
        "authors": [
            "Yanuo Zhou"
        ],
        "comments": "16 pages, 8 figures. Reproducible pipeline for classifying biologically light from wearable spectral data. Manuscript in preparation for journal submission",
        "subjects": "Quantitative Methods (q-bio.QM); Machine Learning (cs.LG)",
        "abstract": "Background: Wearable spectrometers enable field quantification of biologically relevant light, yet reproducible pipelines for contextual classification remain under-specified. Objective: To establish and validate a subject-wise evaluated, reproducible pipeline and actionable design rules for classifying natural vs. artificial light from wearable spectral data. Methods: We analysed ActLumus recordings from 26 participants, each monitored for at least 7 days at 10-second sampling, paired with daily exposure diaries. The pipeline fixes the sequence: domain selection, log-base-10 transform, L2 normalisation excluding total intensity (to avoid brightness shortcuts), hour-level medoid aggregation, sine/cosine hour encoding, and MLP classifier, evaluated under participant-wise cross-validation. Results: The proposed sequence consistently achieved high performance on the primary task, with representative configurations reaching AUC = 0.938 (accuracy 88%) for natural vs. artificial classification on the held-out subject split. In contrast, indoor vs. outdoor classification remained at feasibility level due to spectral overlap and class imbalance (best AUC approximately 0.75; majority-class collapse without contextual sensors). Threshold baselines were insufficient on our data, supporting the need for spectral-temporal modelling beyond illuminance cut-offs. Conclusions: We provide a reproducible, auditable baseline pipeline and design rules for contextual light classification under subject-wise generalisation. All code, configuration files, and derived artefacts will be openly archived (GitHub + Zenodo DOI) to support reuse and benchmarking.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 114,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06205",
        "abs_url": "https://arxiv.org/abs/2512.06205",
        "pdf_url": "https://arxiv.org/pdf/2512.06205",
        "title": "On measuring grounding and generalizing grounding problems",
        "authors": [
            "Daniel Quigley",
            "Eric Maynard"
        ],
        "comments": "36 pages, 85 sources",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "The symbol grounding problem asks how tokens like cat can be about cats, as opposed to mere shapes manipulated in a calculus. We recast grounding from a binary judgment into an audit across desiderata, each indexed by an evaluation tuple (context, meaning type, threat model, reference distribution): authenticity (mechanisms reside inside the agent and, for strong claims, were acquired through learning or evolution); preservation (atomic meanings remain intact); faithfulness, both correlational (realized meanings match intended ones) and etiological (internal mechanisms causally contribute to success); robustness (graceful degradation under declared perturbations); compositionality (the whole is built systematically from the parts). We apply this framework to four grounding modes (symbolic; referential; vectorial; relational) and three case studies: model-theoretic semantics achieves exact composition but lacks etiological warrant; large language models show correlational fit and local robustness for linguistic tasks, yet lack selection-for-success on world tasks without grounded interaction; human language meets the desiderata under strong authenticity through evolutionary and developmental acquisition. By operationalizing a philosophical inquiry about representation, we equip philosophers of science, computer scientists, linguists, and mathematicians with a common language and technical framework for systematic investigation of grounding and meaning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 115,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06208",
        "abs_url": "https://arxiv.org/abs/2512.06208",
        "pdf_url": "https://arxiv.org/pdf/2512.06208",
        "title": "SparsePixels: Efficient Convolution for Sparse Data on FPGAs",
        "authors": [
            "Ho Fung Tsoi",
            "Dylan Rankin",
            "Vladimir Loncar",
            "Philip Harris"
        ],
        "comments": "Under review",
        "subjects": "Hardware Architecture (cs.AR); Machine Learning (cs.LG); High Energy Physics - Experiment (hep-ex)",
        "abstract": "Inference of standard CNNs on FPGAs often incurs high latency and a long initiation interval due to the deep nested loops required to densely convolve every input pixel regardless of its feature value, especially when the image size is large. However, in some image data, input features can be spatially sparse, and semantic information may occupy only a small fraction of the input pixels. In this case most computation would be wasted on empty regions. In this work, we introduce SparsePixels, a framework for efficient convolution for spatially sparse image data on FPGAs, targeting fast inference applications in constrained environments with latency requirements of microseconds or below. Our approach implements a special class of CNNs that selectively retain and compute on a small subset of pixels that are active while ignoring the rest. We show that, for example, in a neutrino physics dataset for identifying neutrino interactions in LArTPC images that have around 4k input pixels but are naturally very sparse, a standard CNN with a compact size of 4k parameters incurs an inference latency of 48.665 $\\mu$s on an FPGA, whereas a sparse CNN of the same base architecture computing on less than 1% of the input pixels results in a $\\times 73$ inference speedup to 0.665 $\\mu$s, with resource utilization well within on-chip budgets, trading only a small percent-level performance loss. At least one-order-of magnitude speedups with comparable performance are also demonstrated in similar datasets with sparse image patterns. This work aims to benefit future algorithm developments for fast and efficient data readout in modern experiments such as the trigger and data acquisition systems at the CERN Large Hadron Collider. For easy adoption, we have developed a library to support building sparse CNNs with quantization-aware training, as well as an HLS implementation for FPGA deployment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 116,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06210",
        "abs_url": "https://arxiv.org/abs/2512.06210",
        "pdf_url": "https://arxiv.org/pdf/2512.06210",
        "title": "Forests of Uncertaint(r)ees: Using tree-based ensembles to estimate probability distributions of future conflict",
        "authors": [
            "Daniel Mittermaier",
            "Tobias Bohne",
            "Martin Hofer",
            "Daniel Racek"
        ],
        "comments": "18 pages, 4 figures, 3 tables. Replication code available at this https URL",
        "subjects": "Applications (stat.AP); Machine Learning (cs.LG)",
        "abstract": "Predictions of fatalities from violent conflict on the PRIO-GRID-month (pgm) level are characterized by high levels of uncertainty, limiting their usefulness in practical applications. We discuss the two main sources of uncertainty for this prediction task, the nature of violent conflict and data limitations, embedding this in the wider literature on uncertainty quantification in machine learning. We develop a strategy to quantify uncertainty in conflict forecasting, shifting from traditional point predictions to full predictive distributions. Our approach compares and combines multiple tree-based classifiers and distributional regressors in a custom auto-ML setup, estimating distributions for each pgm individually. We also test the integration of regional models in spatial ensembles as a potential avenue to reduce uncertainty. The models are able to consistently outperform a suite of benchmarks derived from conflict history in predictions up to one year in advance, with performance driven by regions where conflict was observed. With our evaluation, we emphasize the need to understand how a metric behaves for a given prediction problem, in our case characterized by extremely high zero-inflatedness. While not resulting in better predictions, the integration of smaller models does not decrease performance for this prediction task, opening avenues to integrate data sources with less spatial coverage in the future.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 117,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06211",
        "abs_url": "https://arxiv.org/abs/2512.06211",
        "pdf_url": "https://arxiv.org/pdf/2512.06211",
        "title": "A Broader View on Clustering under Cluster-Aware Norm Objectives",
        "authors": [
            "Martin G. Herold",
            "Evangelos Kipouridis",
            "Joachim Spoerhase"
        ],
        "comments": "",
        "subjects": "Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG)",
        "abstract": "We revisit the $(f,g)$-clustering problem that we introduced in a recent work [SODA'25], and which subsumes fundamental clustering problems such as $k$-Center, $k$-Median, Min-Sum of Radii, and Min-Load $k$-Clustering. This problem assigns each of the $k$ clusters a cost determined by the monotone, symmetric norm $f$ applied to the vector distances in the cluster, and aims at minimizing the norm $g$ applied to the vector of cluster costs. Previously, we focused on certain special cases for which we designed constant-factor approximation algorithms. Our bounds for more general settings left, however, large gaps to the known bounds for the basic problems they capture. In this work, we provide a clearer picture of the approximability of these more general settings. First, we design an $O(\\log^2 n)$-approximation algorithm for $(f, L_{1})$-clustering for any $f$. This improves upon our previous $\\widetilde{O}(\\sqrt{n})$-approximation. Second, we provide an $O(k)$-approximation for the general $(f,g)$-clustering problem, which improves upon our previous $\\widetilde{O}(\\sqrt{kn})$-approximation algorithm and matches the best-known upper bound for Min-Load $k$-Clustering. We then design an approximation algorithm for $(f,g)$-clustering that interpolates, up to polylog factors, between the best known bounds for $k$-Center, $k$-Median, Min-Sum of Radii, Min-Load $k$-Clustering, (Top, $L_{1}$)-clustering, and $(L_{\\infty},g)$-clustering based on a newly defined parameter of $f$ and $g$.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 118,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06227",
        "abs_url": "https://arxiv.org/abs/2512.06227",
        "pdf_url": "https://arxiv.org/pdf/2512.06227",
        "title": "Automated Data Enrichment using Confidence-Aware Fine-Grained Debate among Open-Source LLMs for Mental Health and Online Safety",
        "authors": [
            "Junyu Mao",
            "Anthony Hills",
            "Talia Tseriotou",
            "Maria Liakata",
            "Aya Shamir",
            "Dan Sayda",
            "Dana Atzil-Slonim",
            "Natalie Djohari",
            "Arpan Mandal",
            "Silke Roth",
            "Pamela Ugwudike",
            "Mahesan Niranjan",
            "Stuart E. Middleton"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Real-world indicators are important for improving natural language processing (NLP) tasks such as life events for mental health analysis and risky behaviour for online safety, yet labelling such information in NLP training datasets is often costly and/or difficult given the dynamic nature of such events. This paper compares several LLM-based data enrichment methods and introduces a novel Confidence-Aware Fine-Grained Debate (CFD) framework in which multiple LLM agents simulate human annotators and exchange fine-grained evidence to reach consensus. We describe two new expert-annotated datasets, a mental health Reddit wellbeing dataset and an online safety Facebook sharenting risk dataset. Our CFD framework achieves the most robust data enrichment performance compared to a range of baselines and we show that this type of data enrichment consistently improves downstream tasks. Enriched features incorporated via debate transcripts yield the largest gains, outperforming the non-enriched baseline by 10.1% for the online safety task.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 119,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06259",
        "abs_url": "https://arxiv.org/abs/2512.06259",
        "pdf_url": "https://arxiv.org/pdf/2512.06259",
        "title": "Who Will Top the Charts? Multimodal Music Popularity Prediction via Adaptive Fusion of Modality Experts and Temporal Engagement Modeling",
        "authors": [
            "Yash Choudhary",
            "Preeti Rao",
            "Pushpak Bhattacharyya"
        ],
        "comments": "8 pages",
        "subjects": "Sound (cs.SD); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Predicting a song's commercial success prior to its release remains an open and critical research challenge for the music industry. Early prediction of music popularity informs strategic decisions, creative planning, and marketing. Existing methods suffer from four limitations:(i) temporal dynamics in audio and lyrics are averaged away; (ii) lyrics are represented as a bag of words, disregarding compositional structure and affective semantics; (iii) artist- and song-level historical performance is ignored; and (iv) multimodal fusion approaches rely on simple feature concatenation, resulting in poorly aligned shared representations. To address these limitations, we introduce GAMENet, an end-to-end multimodal deep learning architecture for music popularity prediction. GAMENet integrates modality-specific experts for audio, lyrics, and social metadata through an adaptive gating mechanism. We use audio features from Music4AllOnion processed via OnionEnsembleAENet, a network of autoencoders designed for robust feature extraction; lyric embeddings derived through a large language model pipeline; and newly introduced Career Trajectory Dynamics (CTD) features that capture multi-year artist career momentum and song-level trajectory statistics. Using the Music4All dataset (113k tracks), previously explored in MIR tasks but not popularity prediction, GAMENet achieves a 12% improvement in R^2 over direct multimodal feature concatenation. Spotify audio descriptors alone yield an R^2 of 0.13. Integrating aggregate CTD features increases this to 0.69, with an additional 7% gain from temporal CTD features. We further validate robustness using the SpotGenTrack Popularity Dataset (100k tracks), achieving a 16% improvement over the previous baseline. Extensive ablations confirm the model's effectiveness and the distinct contribution of each modality.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 120,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06270",
        "abs_url": "https://arxiv.org/abs/2512.06270",
        "pdf_url": "https://arxiv.org/pdf/2512.06270",
        "title": "Contextual Strongly Convex Simulation Optimization: Optimize then Predict with Inexact Solutions",
        "authors": [
            "Nifei Lin",
            "Heng Luo",
            "L. Jeff Hong"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "In this work, we study contextual strongly convex simulation optimization and adopt an \"optimize then predict\" (OTP) approach for real-time decision making. In the offline stage, simulation optimization is conducted across a set of covariates to approximate the optimal-solution function; in the online stage, decisions are obtained by evaluating this approximation at the observed covariate. The central theoretical challenge is to understand how the inexactness of solutions generated by simulation-optimization algorithms affects the optimality gap, which is overlooked in existing studies. To address this, we develop a unified analysis framework that explicitly accounts for both solution bias and variance. Using Polyak-Ruppert averaging SGD as an illustrative simulation-optimization algorithm, we analyze the optimality gap of OTP under four representative smoothing techniques: $k$ nearest neighbor, kernel smoothing, linear regression, and kernel ridge regression. We establish convergence rates, derive the optimal allocation of the computational budget $\\Gamma$ between the number of design covariates and the per-covariate simulation effort, and demonstrate the convergence rate can approximately achieve $\\Gamma^{-1}$ under appropriate smoothing technique and sample-allocation rule. Finally, through a numerical study, we validate the theoretical findings and demonstrate the effectiveness and practical value of the proposed approach.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 121,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06294",
        "abs_url": "https://arxiv.org/abs/2512.06294",
        "pdf_url": "https://arxiv.org/pdf/2512.06294",
        "title": "Interpretable Neural Approximation of Stochastic Reaction Dynamics with Guaranteed Reliability",
        "authors": [
            "Quentin Badolle",
            "Arthur Theuer",
            "Zhou Fang",
            "Ankit Gupta",
            "Mustafa Khammash"
        ],
        "comments": "",
        "subjects": "Molecular Networks (q-bio.MN); Machine Learning (cs.LG); Probability (math.PR); Quantitative Methods (q-bio.QM); Machine Learning (stat.ML)",
        "abstract": "Stochastic Reaction Networks (SRNs) are a fundamental modeling framework for systems ranging from chemical kinetics and epidemiology to ecological and synthetic biological processes. A central computational challenge is the estimation of expected outputs across initial conditions and times, a task that is rarely solvable analytically and becomes computationally prohibitive with current methods such as Finite State Projection or the Stochastic Simulation Algorithm. Existing deep learning approaches offer empirical scalability, but provide neither interpretability nor reliability guarantees, limiting their use in scientific analysis and in applications where model outputs inform real-world decisions. Here we introduce DeepSKA, a neural framework that jointly achieves interpretability, guaranteed reliability, and substantial computational gains. DeepSKA yields mathematically transparent representations that generalise across states, times, and output functions, and it integrates this structure with a small number of stochastic simulations to produce unbiased, provably convergent, and dramatically lower-variance estimates than classical Monte Carlo. We demonstrate these capabilities across nine SRNs, including nonlinear and non-mass-action models with up to ten species, where DeepSKA delivers accurate predictions and orders-of-magnitude efficiency improvements. This interpretable and reliable neural framework offers a principled foundation for developing analogous methods for other Markovian systems, including stochastic differential equations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 122,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06348",
        "abs_url": "https://arxiv.org/abs/2512.06348",
        "pdf_url": "https://arxiv.org/pdf/2512.06348",
        "title": "Modeling Spatio-temporal Extremes via Conditional Variational Autoencoders",
        "authors": [
            "Xiaoyu Ma",
            "Likun Zhang",
            "Christopher K. Wikle"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Methodology (stat.ME)",
        "abstract": "Extreme weather events are widely studied in fields such as agriculture, ecology, and meteorology. The spatio-temporal co-occurrence of extreme events can strengthen or weaken under changing climate conditions. In this paper, we propose a novel approach to model spatio-temporal extremes by integrating climate indices via a conditional variational autoencoder (cXVAE). A convolutional neural network (CNN) is embedded in the decoder to convolve climatological indices with the spatial dependence within the latent space, thereby allowing the decoder to be dependent on the climate variables. There are three main contributions here. First, we demonstrate through extensive simulations that the proposed conditional XVAE accurately emulates spatial fields and recovers spatially and temporally varying extremal dependence with very low computational cost post training. Second, we provide a simple, scalable approach to detecting condition-driven shifts and whether the dependence structure is invariant to the conditioning variable. Third, when dependence is found to be condition-sensitive, the conditional XVAE supports counterfactual experiments allowing intervention on the climate covariate and propagating the associated change through the learned decoder to quantify differences in joint tail risk, co-occurrence ranges, and return metrics. To demonstrate the practical utility and performance of the model in real-world scenarios, we apply our method to analyze the monthly maximum Fire Weather Index (FWI) over eastern Australia from 2014 to 2024 conditioned on the El Niño/Southern Oscillation (ENSO) index.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 123,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06393",
        "abs_url": "https://arxiv.org/abs/2512.06393",
        "pdf_url": "https://arxiv.org/pdf/2512.06393",
        "title": "Less Is More for Multi-Step Logical Reasoning of LLM Generalisation Under Rule Removal, Paraphrasing, and Compression",
        "authors": [
            "Qiming Bao",
            "Xiaoxuan Fu"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG); Logic in Computer Science (cs.LO)",
        "abstract": "Large language models (LLMs) excel across many natural language tasks, yet their generalisation to structural perturbations in logical contexts remains poorly understood. We introduce a controlled evaluation framework that probes reasoning reliability through four targeted stress tests: (1) rule deletion, removing either redundant or essential rules from a multi-step inference chain; (2) contradictory evidence injection; (3) logic-preserving rewrites generated through several families of equivalence laws (contrapositive, double negation, implication, De Morgan, identity, and commutativity); and (4) multi-law equivalence stacking that introduces 2-5 simultaneous logical transformations. Across three representative model families: BERT, Qwen2, and LLaMA-like models. Our experiments reveal a strikingly consistent pattern: all models achieve perfect accuracy on the base tasks and remain fully generalise to redundant rule deletion and all equivalence-based rewrites (single or multi-law), but fail sharply under essential rule deletion (dropping to 25% accuracy) and collapse completely in the presence of explicit contradictions (0% accuracy). These results demonstrate that LLMs possess stable invariance to semantic-preserving logical transformations, yet remain fundamentally brittle to missing or conflicting evidence. Our framework provides a clean diagnostic tool for isolating such reasoning failure modes and highlights persistent gaps in the logical generalisation abilities of current LLMs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 124,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06421",
        "abs_url": "https://arxiv.org/abs/2512.06421",
        "pdf_url": "https://arxiv.org/pdf/2512.06421",
        "title": "Rethinking Training Dynamics in Scale-wise Autoregressive Generation",
        "authors": [
            "Gengze Zhou",
            "Chongjian Ge",
            "Hao Tan",
            "Feng Liu",
            "Yicong Hong"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) train-test mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through a comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces a Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning train-test patterns, and a complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields a 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as a reliable post-training method for visual autoregressive generation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 125,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06435",
        "abs_url": "https://arxiv.org/abs/2512.06435",
        "pdf_url": "https://arxiv.org/pdf/2512.06435",
        "title": "Canonical Tail Dependence for Soft Extremal Clustering of Multichannel Brain Signals",
        "authors": [
            "Mara Sherlin Talento",
            "Jordan Richards",
            "Raphael Huser",
            "Hernando Ombao"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "We develop a novel characterization of extremal dependence between two cortical regions of the brain when its signals display extremely large amplitudes. We show that connectivity in the tails of the distribution reveals unique features of extreme events (e.g., seizures) that can help to identify their occurrence. Numerous studies have established that connectivity-based features are effective for discriminating brain states. Here, we demonstrate the advantage of the proposed approach: that tail connectivity provides additional discriminatory power, enabling more accurate identification of extreme-related events and improved seizure risk management. Common approaches in tail dependence modeling use pairwise summary measures or parametric models. However, these approaches do not identify channels that drive the maximal tail dependence between two groups of signals -- an information that is useful when analyzing electroencephalography of epileptic patients where specific channels are responsible for seizure occurrences. A familiar approach in traditional signal processing is canonical correlation, which we extend to the tails to develop a visualization of extremal channel-contributions. Through the tail pairwise dependence matrix (TPDM), we develop a computationally-efficient estimator for our canonical tail dependence measure. Our method is then used for accurate frequency-based soft clustering of neonates, distinguishing those with seizures from those without.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 126,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06530",
        "abs_url": "https://arxiv.org/abs/2512.06530",
        "pdf_url": "https://arxiv.org/pdf/2512.06530",
        "title": "On The Role of K-Space Acquisition in MRI Reconstruction Domain-Generalization",
        "authors": [
            "Mohammed Wattad",
            "Tamir Shor",
            "Alex Bronstein"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Recent work has established learned k-space acquisition patterns as a promising direction for improving reconstruction quality in accelerated Magnetic Resonance Imaging (MRI). Despite encouraging results, most existing research focuses on acquisition patterns optimized for a single dataset or modality, with limited consideration of their transferability across imaging domains. In this work, we demonstrate that the benefits of learned k-space sampling can extend beyond the training domain, enabling superior reconstruction performance under domain shifts. Our study presents two main contributions. First, through systematic evaluation across datasets and acquisition paradigms, we show that models trained with learned sampling patterns exhibitimproved generalization under cross-domain settings. Second, we propose a novel method that enhances domain robustness by introducing acquisition uncertainty during training-stochastically perturbing k-space trajectories to simulate variability across scanners and imaging conditions. Our results highlight the importance of treating kspace trajectory design not merely as an acceleration mechanism, but as an active degree of freedom for improving domain generalization in MRI reconstruction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 127,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06537",
        "abs_url": "https://arxiv.org/abs/2512.06537",
        "pdf_url": "https://arxiv.org/pdf/2512.06537",
        "title": "Approximate Multiplier Induced Error Propagation in Deep Neural Networks",
        "authors": [
            "A. M. H. H. Alahakoon",
            "Hassaan Saadat",
            "Darshana Jayasinghe",
            "Sri Parameswaran"
        ],
        "comments": "7 pages, Submitted to Design and Automation Conference (DAC 2026)",
        "subjects": "Hardware Architecture (cs.AR); Machine Learning (cs.LG)",
        "abstract": "Deep Neural Networks (DNNs) rely heavily on dense arithmetic operations, motivating the use of Approximate Multipliers (AxMs) to reduce energy consumption in hardware accelerators. However, a rigorous mathematical characterization of how AxMs error distributions influence DNN accuracy remains underdeveloped. This work presents an analytical framework that connects the statistical error moments of an AxM to the induced distortion in General Matrix Multiplication (GEMM). Using the Frobenius norm of the resulting error matrix, we derive a closed form expression for practical DNN dimensions that demonstrates the distortion is predominantly governed by the multiplier mean error (bias). To evaluate this model in realistic settings, we incorporate controlled error injection into GEMM and convolution layers and examine its effect on ImageNet scale networks. The predicted distortion correlates strongly with the observed accuracy degradation, and an error configurable AxM case study implemented on an FPGA further confirms the analytical trends. By providing a lightweight alternative to behavioral or hardware level simulations, this framework enables rapid estimation of AxM impact on DNN inference quality.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 128,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06553",
        "abs_url": "https://arxiv.org/abs/2512.06553",
        "pdf_url": "https://arxiv.org/pdf/2512.06553",
        "title": "A Latent Variable Framework for Scaling Laws in Large Language Models",
        "authors": [
            "Peiyao Cai",
            "Chengyu Cui",
            "Felipe Maia Polo",
            "Seamus Somerstep",
            "Leshem Choshen",
            "Mikhail Yurochkin",
            "Moulinath Banerjee",
            "Yuekai Sun",
            "Kean Ming Tan",
            "Gongjun Xu"
        ],
        "comments": "",
        "subjects": "Applications (stat.AP); Machine Learning (cs.LG)",
        "abstract": "We propose a statistical framework built on latent variable modeling for scaling laws of large language models (LLMs). Our work is motivated by the rapid emergence of numerous new LLM families with distinct architectures and training strategies, evaluated on an increasing number of benchmarks. This heterogeneity makes a single global scaling curve inadequate for capturing how performance varies across families and benchmarks. To address this, we propose a latent variable modeling framework in which each LLM family is associated with a latent variable that captures the common underlying features in that family. An LLM's performance on different benchmarks is then driven by its latent skills, which are jointly determined by the latent variable and the model's own observable features. We develop an estimation procedure for this latent variable model and establish its statistical properties. We also design efficient numerical algorithms that support estimation and various downstream tasks. Empirically, we evaluate the approach on 12 widely used benchmarks from the Open LLM Leaderboard (v1/v2).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 129,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06615",
        "abs_url": "https://arxiv.org/abs/2512.06615",
        "pdf_url": "https://arxiv.org/pdf/2512.06615",
        "title": "Latent Nonlinear Denoising Score Matching for Enhanced Learning of Structured Distributions",
        "authors": [
            "Kaichen Shen",
            "Wei Zhu"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "We present latent nonlinear denoising score matching (LNDSM), a novel training objective for score-based generative models that integrates nonlinear forward dynamics with the VAE-based latent SGM framework. This combination is achieved by reformulating the cross-entropy term using the approximate Gaussian transition induced by the Euler-Maruyama scheme. To ensure numerical stability, we identify and remove two zero-mean but variance exploding terms arising from small time steps. Experiments on variants of the MNIST dataset demonstrate that the proposed method achieves faster synthesis and enhanced learning of inherently structured distributions. Compared to benchmark structure-agnostic latent SGMs, LNDSM consistently attains superior sample quality and variability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 130,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06639",
        "abs_url": "https://arxiv.org/abs/2512.06639",
        "pdf_url": "https://arxiv.org/pdf/2512.06639",
        "title": "Learning to Hedge Swaptions",
        "authors": [
            "Zaniar Ahmadi",
            "Frédéric Godin"
        ],
        "comments": "",
        "subjects": "Risk Management (q-fin.RM); Machine Learning (cs.LG)",
        "abstract": "This paper investigates the deep hedging framework, based on reinforcement learning (RL), for the dynamic hedging of swaptions, contrasting its performance with traditional sensitivity-based rho-hedging. We design agents under three distinct objective functions (mean squared error, downside risk, and Conditional Value-at-Risk) to capture alternative risk preferences and evaluate how these objectives shape hedging styles. Relying on a three-factor arbitrage-free dynamic Nelson-Siegel model for our simulation experiments, our findings show that near-optimal hedging effectiveness is achieved when using two swaps as hedging instruments. Deep hedging strategies dynamically adapt the hedging portfolio's exposure to risk factors across states of the market. In our experiments, their out-performance over rho-hedging strategies persists even in the presence some of model misspecification. These results highlight RL's potential to deliver more efficient and resilient swaption hedging strategies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 131,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06676",
        "abs_url": "https://arxiv.org/abs/2512.06676",
        "pdf_url": "https://arxiv.org/pdf/2512.06676",
        "title": "FedDSR: Federated Deep Supervision and Regularization Towards Autonomous Driving",
        "authors": [
            "Wei-Bin Kou",
            "Guangxu Zhu",
            "Bingyang Cheng",
            "Chen Zhang",
            "Yik-Chung Wu",
            "Jianping Wang"
        ],
        "comments": "9 pages",
        "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
        "abstract": "Federated Learning (FL) enables collaborative training of autonomous driving (AD) models across distributed vehicles while preserving data privacy. However, FL encounters critical challenges such as poor generalization and slow convergence due to non-independent and identically distributed (non-IID) data from diverse driving environments. To overcome these obstacles, we introduce Federated Deep Supervision and Regularization (FedDSR), a paradigm that incorporates multi-access intermediate layer supervision and regularization within federated AD system. Specifically, FedDSR comprises following integral strategies: (I) to select multiple intermediate layers based on predefined architecture-agnostic standards. (II) to compute mutual information (MI) and negative entropy (NE) on those selected layers to serve as intermediate loss and regularizer. These terms are integrated into the output-layer loss to form a unified optimization objective, enabling comprehensive optimization across the network hierarchy. (III) to aggregate models from vehicles trained based on aforementioned rules of (I) and (II) to generate the global model on central server. By guiding and penalizing the learning of feature representations at intermediate stages, FedDSR enhances the model generalization and accelerates model convergence for federated AD. We then take the semantic segmentation task as an example to assess FedDSR and apply FedDSR to multiple model architectures and FL algorithms. Extensive experiments demonstrate that FedDSR achieves up to 8.93% improvement in mIoU and 28.57% reduction in training rounds, compared to other FL baselines, making it highly suitable for practical deployment in federated AD ecosystems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 132,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06751",
        "abs_url": "https://arxiv.org/abs/2512.06751",
        "pdf_url": "https://arxiv.org/pdf/2512.06751",
        "title": "Becoming Experienced Judges: Selective Test-Time Learning for Evaluators",
        "authors": [
            "Seungyeon Jwa",
            "Daechul Ahn",
            "Reokyoung Kim",
            "Dongyeop Kang",
            "Jonghyun Choi"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Automatic evaluation with large language models, commonly known as LLM-as-a-judge, is now standard across reasoning and alignment tasks. Despite evaluating many samples in deployment, these evaluators typically (i) treat each case independently, missing the opportunity to accumulate experience, and (ii) rely on a single fixed prompt for all cases, neglecting the need for sample-specific evaluation criteria. We introduce Learning While Evaluating (LWE), a framework that allows evaluators to improve sequentially at inference time without requiring training or validation sets. LWE maintains an evolving meta-prompt that (i) produces sample-specific evaluation instructions and (ii) refines itself through self-generated feedback. Furthermore, we propose Selective LWE, which updates the meta-prompt only on self-inconsistent cases, focusing computation where it matters most. This selective approach retains the benefits of sequential learning while being far more cost-effective. Across two pairwise comparison benchmarks, Selective LWE outperforms strong baselines, empirically demonstrating that evaluators can improve during sequential testing with a simple selective update, learning most from the cases they struggle with.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 133,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06795",
        "abs_url": "https://arxiv.org/abs/2512.06795",
        "pdf_url": "https://arxiv.org/pdf/2512.06795",
        "title": "ADAM Optimization with Adaptive Batch Selection",
        "authors": [
            "Gyu Yeol Kim",
            "Min-hwan Oh"
        ],
        "comments": "Published at ICLR 2025",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Adam is a widely used optimizer in neural network training due to its adaptive learning rate. However, because different data samples influence model updates to varying degrees, treating them equally can lead to inefficient convergence. To address this, a prior work proposed adapting the sampling distribution using a bandit framework to select samples adaptively. While promising, the bandit-based variant of Adam suffers from limited theoretical guarantees. In this paper, we introduce Adam with Combinatorial Bandit Sampling (AdamCB), which integrates combinatorial bandit techniques into Adam to resolve these issues. AdamCB is able to fully utilize feedback from multiple samples at once, enhancing both theoretical guarantees and practical performance. Our regret analysis shows that AdamCB achieves faster convergence than Adam-based methods including the previous bandit-based variant. Numerical experiments demonstrate that AdamCB consistently outperforms existing methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 134,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06809",
        "abs_url": "https://arxiv.org/abs/2512.06809",
        "pdf_url": "https://arxiv.org/pdf/2512.06809",
        "title": "A Physics-Aware Attention LSTM Autoencoder for Early Fault Diagnosis of Battery Systems",
        "authors": [
            "Jiong Yang"
        ],
        "comments": "5 pages, 7 figures",
        "subjects": "Systems and Control (eess.SY); Machine Learning (cs.LG)",
        "abstract": "Battery safety is paramount for electric vehicles. Early fault diagnosis remains a challenge due to the subtle nature of anomalies and the interference of dynamic operating noise. Existing data-driven methods often suffer from \"physical blindness\" leading to missed detections or false alarms. To address this, we propose a Physics-Aware Attention LSTM Autoencoder (PA-ALSTM-AE). This novel framework explicitly integrates battery aging laws (mileage) into the deep learning pipeline through a multi-stage fusion mechanism. Specifically, an adaptive physical feature construction module selects mileage-sensitive features, and a physics-guided latent fusion module dynamically calibrates the memory cells of the LSTM based on the aging state. Extensive experiments on the large-scale Vloong real-world dataset demonstrate that the proposed method significantly outperforms state-of-the-art baselines. Notably, it improves the recall rate of early faults by over 3 times while maintaining high precision, offering a robust solution for industrial battery management systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 135,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06849",
        "abs_url": "https://arxiv.org/abs/2512.06849",
        "pdf_url": "https://arxiv.org/pdf/2512.06849",
        "title": "Hide-and-Seek Attribution: Weakly Supervised Segmentation of Vertebral Metastases in CT",
        "authors": [
            "Matan Atad",
            "Alexander W. Marka",
            "Lisa Steinhelfer",
            "Anna Curto-Vilalta",
            "Yannik Leonhardt",
            "Sarah C. Foreman",
            "Anna-Sophia Walburga Dietrich",
            "Robert Graf",
            "Alexandra S. Gersing",
            "Bjoern Menze",
            "Daniel Rueckert",
            "Jan S. Kirschke",
            "Hendrik Möller"
        ],
        "comments": "In submission",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Accurate segmentation of vertebral metastasis in CT is clinically important yet difficult to scale, as voxel-level annotations are scarce and both lytic and blastic lesions often resemble benign degenerative changes. We introduce a weakly supervised method trained solely on vertebra-level healthy/malignant labels, without any lesion masks. The method combines a Diffusion Autoencoder (DAE) that produces a classifier-guided healthy edit of each vertebra with pixel-wise difference maps that propose candidate lesion regions. To determine which regions truly reflect malignancy, we introduce Hide-and-Seek Attribution: each candidate is revealed in turn while all others are hidden, the edited image is projected back to the data manifold by the DAE, and a latent-space classifier quantifies the isolated malignant contribution of that component. High-scoring regions form the final lytic or blastic segmentation. On held-out radiologist annotations, we achieve strong blastic/lytic performance despite no mask supervision (F1: 0.91/0.85; Dice: 0.87/0.78), exceeding baselines (F1: 0.79/0.67; Dice: 0.74/0.55). These results show that vertebra-level labels can be transformed into reliable lesion masks, demonstrating that generative editing combined with selective occlusion supports accurate weakly supervised segmentation in CT.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 136,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06906",
        "abs_url": "https://arxiv.org/abs/2512.06906",
        "pdf_url": "https://arxiv.org/pdf/2512.06906",
        "title": "MINES: Explainable Anomaly Detection through Web API Invariant Inference",
        "authors": [
            "Wenjie Zhang",
            "Yun Lin",
            "Chun Fung Amos Kwok",
            "Xiwen Teoh",
            "Xiaofei Xie",
            "Frank Liauw",
            "Hongyu Zhang",
            "Jin Song Dong"
        ],
        "comments": "",
        "subjects": "Software Engineering (cs.SE); Cryptography and Security (cs.CR); Databases (cs.DB); Machine Learning (cs.LG)",
        "abstract": "Detecting the anomalies of web applications, important infrastructures for running modern companies and governments, is crucial for providing reliable web services. Many modern web applications operate on web APIs (e.g., RESTful, SOAP, and WebSockets), their exposure invites intended attacks or unintended illegal visits, causing abnormal system behaviors. However, such anomalies can share very similar logs with normal logs, missing crucial information (which could be in database) for log discrimination. Further, log instances can be also noisy, which can further mislead the state-of-the-art log learning solutions to learn spurious correlation, resulting superficial models and rules for anomaly detection. In this work, we propose MINES which infers explainable API invariants for anomaly detection from the schema level instead of detailed raw log instances, which can (1) significantly discriminate noise in logs to identify precise normalities and (2) detect abnormal behaviors beyond the instrumented logs. Technically, MINES (1) converts API signatures into table schema to enhance the original database shema; and (2) infers the potential database constraints on the enhanced database schema to capture the potential relationships between APIs and database tables. MINES uses LLM for extracting potential relationship based on two given table structures; and use normal log instances to reject and accept LLM-generated invariants. Finally, MINES translates the inferred constraints into invariants to generate Python code for verifying the runtime logs. We extensively evaluate MINES on web-tamper attacks on the benchmarks of TrainTicket, NiceFish, Gitea, Mastodon, and NextCloud against baselines such as LogRobust, LogFormer, and WebNorm. The results show that MINES achieves high recall for the anomalies while introducing almost zero false positives, indicating a new state-of-the-art.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 137,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06912",
        "abs_url": "https://arxiv.org/abs/2512.06912",
        "pdf_url": "https://arxiv.org/pdf/2512.06912",
        "title": "Energy-Efficient Navigation for Surface Vehicles in Vortical Flow Fields",
        "authors": [
            "Rushiraj Gadhvi",
            "Sandeep Manjanna"
        ],
        "comments": "Under Review for International Conference on Robotics and Automation (ICRA 2026)",
        "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
        "abstract": "For centuries, khalasi have skillfully harnessed ocean currents to navigate vast waters with minimal effort. Emulating this intuition in autonomous systems remains a significant challenge, particularly for Autonomous Surface Vehicles tasked with long duration missions under strict energy budgets. In this work, we present a learning-based approach for energy-efficient surface vehicle navigation in vortical flow fields, where partial observability often undermines traditional path-planning methods. We present an end to end reinforcement learning framework based on Soft Actor Critic that learns flow-aware navigation policies using only local velocity measurements. Through extensive evaluation across diverse and dynamically rich scenarios, our method demonstrates substantial energy savings and robust generalization to previously unseen flow conditions, offering a promising path toward long term autonomy in ocean environments. The navigation paths generated by our proposed approach show an improvement in energy conservation 30 to 50 percent compared to the existing state of the art techniques.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 138,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06945",
        "abs_url": "https://arxiv.org/abs/2512.06945",
        "pdf_url": "https://arxiv.org/pdf/2512.06945",
        "title": "Symmetric Aggregation of Conformity Scores for Efficient Uncertainty Sets",
        "authors": [
            "Nabil Alami",
            "Jad Zakharia",
            "Souhaib Ben Taieb"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Access to multiple predictive models trained for the same task, whether in regression or classification, is increasingly common in many applications. Aggregating their predictive uncertainties to produce reliable and efficient uncertainty quantification is therefore a critical but still underexplored challenge, especially within the framework of conformal prediction (CP). While CP methods can generate individual prediction sets from each model, combining them into a single, more informative set remains a challenging problem. To address this, we propose SACP (Symmetric Aggregated Conformal Prediction), a novel method that aggregates nonconformity scores from multiple predictors. SACP transforms these scores into e-values and combines them using any symmetric aggregation function. This flexible design enables a robust, data-driven framework for selecting aggregation strategies that yield sharper prediction sets. We also provide theoretical insights that help justify the validity and performance of the SACP approach. Extensive experiments on diverse datasets show that SACP consistently improves efficiency and often outperforms state-of-the-art model aggregation baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 139,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06950",
        "abs_url": "https://arxiv.org/abs/2512.06950",
        "pdf_url": "https://arxiv.org/pdf/2512.06950",
        "title": "PARIS: Pruning Algorithm via the Representer theorem for Imbalanced Scenarios",
        "authors": [
            "Enrico Camporeale"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Space Physics (physics.space-ph)",
        "abstract": "The challenge of \\textbf{imbalanced regression} arises when standard Empirical Risk Minimization (ERM) biases models toward high-frequency regions of the data distribution, causing severe degradation on rare but high-impact ``tail'' events. Existing strategies uch as loss re-weighting or synthetic over-sampling often introduce noise, distort the underlying distribution, or add substantial algorithmic complexity. We introduce \\textbf{PARIS} (Pruning Algorithm via the Representer theorem for Imbalanced Scenarios), a principled framework that mitigates imbalance by \\emph{optimizing the training set itself}. PARIS leverages the representer theorem for neural networks to compute a \\textbf{closed-form representer deletion residual}, which quantifies the exact change in validation loss caused by removing a single training point \\emph{without retraining}. Combined with an efficient Cholesky rank-one downdating scheme, PARIS performs fast, iterative pruning that eliminates uninformative or performance-degrading samples. We use a real-world space weather example, where PARIS reduces the training set by up to 75\\% while preserving or improving overall RMSE, outperforming re-weighting, synthetic oversampling, and boosting baselines. Our results demonstrate that representer-guided dataset pruning is a powerful, interpretable, and computationally efficient approach to rare-event regression.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 140,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06956",
        "abs_url": "https://arxiv.org/abs/2512.06956",
        "pdf_url": "https://arxiv.org/pdf/2512.06956",
        "title": "Statistical analysis of Inverse Entropy-regularized Reinforcement Learning",
        "authors": [
            "Denis Belomestny",
            "Alexey Naumov",
            "Sergey Samsonov"
        ],
        "comments": "27 pages",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Statistics Theory (math.ST)",
        "abstract": "Inverse reinforcement learning aims to infer the reward function that explains expert behavior observed through trajectories of state--action pairs. A long-standing difficulty in classical IRL is the non-uniqueness of the recovered reward: many reward functions can induce the same optimal policy, rendering the inverse problem ill-posed. In this paper, we develop a statistical framework for Inverse Entropy-regularized Reinforcement Learning that resolves this ambiguity by combining entropy regularization with a least-squares reconstruction of the reward from the soft Bellman residual. This combination yields a unique and well-defined so-called least-squares reward consistent with the expert policy. We model the expert demonstrations as a Markov chain with the invariant distribution defined by an unknown expert policy $\\pi^\\star$ and estimate the policy by a penalized maximum-likelihood procedure over a class of conditional distributions on the action space. We establish high-probability bounds for the excess Kullback--Leibler divergence between the estimated policy and the expert policy, accounting for statistical complexity through covering numbers of the policy class. These results lead to non-asymptotic minimax optimal convergence rates for the least-squares reward function, revealing the interplay between smoothing (entropy regularization), model complexity, and sample size. Our analysis bridges the gap between behavior cloning, inverse reinforcement learning, and modern statistical learning theory.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 141,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06960",
        "abs_url": "https://arxiv.org/abs/2512.06960",
        "pdf_url": "https://arxiv.org/pdf/2512.06960",
        "title": "Learning Conditional Independence Differential Graphs From Time-Dependent Data",
        "authors": [
            "Jitendra K Tugnait"
        ],
        "comments": "20 pages, 4 figures, 2 tables. To be published in IEEE Access, 2025",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Signal Processing (eess.SP)",
        "abstract": "Estimation of differences in conditional independence graphs (CIGs) of two time series Gaussian graphical models (TSGGMs) is investigated where the two TSGGMs are known to have similar structure. The TSGGM structure is encoded in the inverse power spectral density (IPSD) of the time series. In several existing works, one is interested in estimating the difference in two precision matrices to characterize underlying changes in conditional dependencies of two sets of data consisting of independent and identically distributed (i.i.d.) observations. In this paper we consider estimation of the difference in two IPSDs to characterize the underlying changes in conditional dependencies of two sets of time-dependent data. Our approach accounts for data time dependencies unlike past work. We analyze a penalized D-trace loss function approach in the frequency domain for differential graph learning, using Wirtinger calculus. We consider both convex (group lasso) and non-convex (log-sum and SCAD group penalties) penalty/regularization functions. An alternating direction method of multipliers (ADMM) algorithm is presented to optimize the objective function. We establish sufficient conditions in a high-dimensional setting for consistency (convergence of the inverse power spectral density to true value in the Frobenius norm) and graph recovery. Both synthetic and real data examples are presented in support of the proposed approaches. In synthetic data examples, our log-sum-penalized differential time-series graph estimator significantly outperformed our lasso based differential time-series graph estimator which, in turn, significantly outperformed an existing lasso-penalized i.i.d. modeling approach, with $F_1$ score as the performance metric.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 142,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06973",
        "abs_url": "https://arxiv.org/abs/2512.06973",
        "pdf_url": "https://arxiv.org/pdf/2512.06973",
        "title": "Joint Learning of Feasibility-Aware Signal Temporal Logic and BarrierNet for Robust and Correct Control",
        "authors": [
            "Shuo Liu",
            "Wenliang Liu",
            "Wei Xiao",
            "Calin A. Belta"
        ],
        "comments": "16 pages, 11 figures",
        "subjects": "Systems and Control (eess.SY); Machine Learning (cs.LG)",
        "abstract": "Control Barrier Functions (CBFs) have emerged as a powerful tool for enforcing safety in optimization-based controllers, and their integration with Signal Temporal Logic (STL) has enabled the specification-driven synthesis of complex robotic behaviors. However, existing CBF-STL approaches typically rely on fixed hyperparameters and myopic, per-time step optimization, which can lead to overly conservative behavior, infeasibility near tight input limits, and difficulty satisfying long-horizon STL tasks. To address these limitations, we propose a feasibility-aware learning framework that embeds trainable, time-varying High Order Control Barrier Functions (HOCBFs) into a differentiable Quadratic Program (dQP). Our approach provides a systematic procedure for constructing time-varying HOCBF constraints for a broad fragment of STL and introduces a unified robustness measure that jointly captures STL satisfaction, QP feasibility, and control-bound compliance. Three neural networks-InitNet, RefNet, and an extended BarrierNet-collaborate to generate reference inputs and adapt constraint-related hyperparameters automatically over time and across initial conditions, reducing conservativeness while maximizing robustness. The resulting controller achieves STL satisfaction with strictly feasible dQPs and requires no manual tuning. Simulation results demonstrate that the proposed framework maintains high STL robustness under tight input bounds and significantly outperforms fixed-parameter and non-adaptive baselines in complex environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 143,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06977",
        "abs_url": "https://arxiv.org/abs/2512.06977",
        "pdf_url": "https://arxiv.org/pdf/2512.06977",
        "title": "Physics-Guided Diffusion Priors for Multi-Slice Reconstruction in Scientific Imaging",
        "authors": [
            "Laurentius Valdy",
            "Richard D. Paul",
            "Alessio Quercia",
            "Zhuo Cao",
            "Xuan Zhao",
            "Hanno Scharr",
            "Arya Bangun"
        ],
        "comments": "8 pages, 5 figures, AAAI AI2ASE 2026",
        "subjects": "Image and Video Processing (eess.IV); Machine Learning (cs.LG)",
        "abstract": "Accurate multi-slice reconstruction from limited measurement data is crucial to speed up the acquisition process in medical and scientific imaging. However, it remains challenging due to the ill-posed nature of the problem and the high computational and memory demands. We propose a framework that addresses these challenges by integrating partitioned diffusion priors with physics-based constraints. By doing so, we substantially reduce memory usage per GPU while preserving high reconstruction quality, outperforming both physics-only and full multi-slice reconstruction baselines for different modalities, namely Magnetic Resonance Imaging (MRI) and four-dimensional Scanning Transmission Electron Microscopy (4D-STEM). Additionally, we show that the proposed method improves in-distribution accuracy as well as strong generalization to out-of-distribution datasets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 144,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.06983",
        "abs_url": "https://arxiv.org/abs/2512.06983",
        "pdf_url": "https://arxiv.org/pdf/2512.06983",
        "title": "On Memory: A comparison of memory mechanisms in world models",
        "authors": [
            "Eli J. Laird",
            "Corey Clark"
        ],
        "comments": "10 pages, 1 figure",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "World models enable agents to plan within imagined environments by predicting future states conditioned on past observations and actions. However, their ability to plan over long horizons is limited by the effective memory span of the backbone architecture. This limitation leads to perceptual drift in long rollouts, hindering the model's capacity to perform loop closures within imagined trajectories. In this work, we investigate the effective memory span of transformer-based world models through an analysis of several memory augmentation mechanisms. We introduce a taxonomy that distinguishes between memory encoding and memory injection mechanisms, motivating their roles in extending the world model's memory through the lens of residual stream dynamics. Using a state recall evaluation task, we measure the memory recall of each mechanism and analyze its respective trade-offs. Our findings show that memory mechanisms improve the effective memory span in vision transformers and provide a path to completing loop closures within a world model's imagination.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 145,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07030",
        "abs_url": "https://arxiv.org/abs/2512.07030",
        "pdf_url": "https://arxiv.org/pdf/2512.07030",
        "title": "A Comprehensive Study of Supervised Machine Learning Models for Zero-Day Attack Detection: Analyzing Performance on Imbalanced Data",
        "authors": [
            "Zahra Lotfi",
            "Mostafa Lotfi"
        ],
        "comments": "13 pages, 5 figures",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Among the various types of cyberattacks, identifying zero-day attacks is problematic because they are unknown to security systems as their pattern and characteristics do not match known blacklisted attacks. There are many Machine Learning (ML) models designed to analyze and detect network attacks, especially using supervised models. However, these models are designed to classify samples (normal and attacks) based on the patterns they learn during the training phase, so they perform inefficiently on unseen attacks. This research addresses this issue by evaluating five different supervised models to assess their performance and execution time in predicting zero-day attacks and find out which model performs accurately and quickly. The goal is to improve the performance of these supervised models by not only proposing a framework that applies grid search, dimensionality reduction and oversampling methods to overcome the imbalance problem, but also comparing the effectiveness of oversampling on ml model metrics, in particular the accuracy. To emulate attack detection in real life, this research applies a highly imbalanced data set and only exposes the classifiers to zero-day attacks during the testing phase, so the models are not trained to flag the zero-day attacks. Our results show that Random Forest (RF) performs best under both oversampling and non-oversampling conditions, this increased effectiveness comes at the cost of longer processing times. Therefore, we selected XG Boost (XGB) as the top model due to its fast and highly accurate performance in detecting zero-day attacks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 146,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07038",
        "abs_url": "https://arxiv.org/abs/2512.07038",
        "pdf_url": "https://arxiv.org/pdf/2512.07038",
        "title": "Ideal Attribution and Faithful Watermarks for Language Models",
        "authors": [
            "Min Jae Song",
            "Kameron Shahabi"
        ],
        "comments": "30 pages",
        "subjects": "Cryptography and Security (cs.CR); Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "We introduce ideal attribution mechanisms, a formal abstraction for reasoning about attribution decisions over strings. At the core of this abstraction lies the ledger, an append-only log of the prompt-response interaction history between a model and its user. Each mechanism produces deterministic decisions based on the ledger and an explicit selection criterion, making it well-suited to serve as a ground truth for attribution. We frame the design goal of watermarking schemes as faithful representation of ideal attribution mechanisms. This novel perspective brings conceptual clarity, replacing piecemeal probabilistic statements with a unified language for stating the guarantees of each scheme. It also enables precise reasoning about desiderata for future watermarking schemes, even when no current construction achieves them, since the ideal functionalities are specified first. In this way, the framework provides a roadmap that clarifies which guarantees are attainable in an idealized setting and worth pursuing in practice.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 147,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07078",
        "abs_url": "https://arxiv.org/abs/2512.07078",
        "pdf_url": "https://arxiv.org/pdf/2512.07078",
        "title": "DFIR-DETR: Frequency Domain Enhancement and Dynamic Feature Aggregation for Cross-Scene Small Object Detection",
        "authors": [
            "Bo Gao",
            "Jingcheng Tong",
            "Xingsheng Chen",
            "Han Yu",
            "Zichen Li"
        ],
        "comments": "16 pages",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Detecting small objects in UAV remote sensing images and identifying surface defects in industrial inspection remain difficult tasks. These applications face common obstacles: features are sparse and weak, backgrounds are cluttered, and object scales vary dramatically. Current transformer-based detectors, while powerful, struggle with three critical issues. First, features degrade severely as networks downsample progressively. Second, spatial convolutions cannot capture long-range dependencies effectively. Third, standard upsampling methods inflate feature maps unnecessarily. We introduce DFIR-DETR to tackle these problems through dynamic feature aggregation combined with frequency-domain processing. Our architecture builds on three novel components. The DCFA module uses dynamic K-sparse attention, cutting complexity from O(N2) down to O(NK), and employs spatial gated linear units for better nonlinear modeling. The DFPN module applies amplitude-normalized upsampling to prevent feature inflation and uses dual-path shuffle convolution to retain spatial details across scales. The FIRC3 module operates in the frequency domain, achieving global receptive fields without sacrificing efficiency. We tested our method extensively on NEU-DET and VisDrone datasets. Results show mAP50 scores of 92.9% and 51.6% respectively-both state-of-the-art. The model stays lightweight with just 11.7M parameters and 41.2 GFLOPs. Strong performance across two very different domains confirms that DFIR-DETR generalizes well and works effectively in resource-limited settings for cross-scene small object detection.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 148,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07109",
        "abs_url": "https://arxiv.org/abs/2512.07109",
        "pdf_url": "https://arxiv.org/pdf/2512.07109",
        "title": "A Neural Affinity Framework for Abstract Reasoning: Diagnosing the Compositional Gap in Transformer Architectures via Procedural Task Taxonomy",
        "authors": [
            "Miguel Ingram",
            "Arthur Joseph Merritt III"
        ],
        "comments": "62 pages, 10 figures",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Responding to Hodel et al.'s (2024) call for a formal definition of task relatedness in re-arc, we present the first 9-category taxonomy of all 400 tasks, validated at 97.5% accuracy via rule-based code analysis. We prove the taxonomy's visual coherence by training a CNN on raw grid pixels (95.24% accuracy on S3, 36.25% overall, 3.3x chance), then apply the taxonomy diagnostically to the original ARC-AGI-2 test set. Our curriculum analysis reveals 35.3% of tasks exhibit low neural affinity for Transformers--a distributional bias mirroring ARC-AGI-2. To probe this misalignment, we fine-tuned a 1.7M-parameter Transformer across 302 tasks, revealing a profound Compositional Gap: 210 of 302 tasks (69.5%) achieve >80% cell accuracy (local patterns) but <10% grid accuracy (global synthesis). This provides direct evidence for a Neural Affinity Ceiling Effect, where performance is bounded by architectural suitability, not curriculum. Applying our framework to Li et al.'s independent ViTARC study (400 specialists, 1M examples each) confirms its predictive power: Very Low affinity tasks achieve 51.9% versus 77.7% for High affinity (p<0.001), with a task at 0% despite massive data. The taxonomy enables precise diagnosis: low-affinity tasks (A2) hit hard ceilings, while high-affinity tasks (C1) reach 99.8%. These findings indicate that progress requires hybrid architectures with affinity-aligned modules. We release our validated taxonomy,",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 149,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07120",
        "abs_url": "https://arxiv.org/abs/2512.07120",
        "pdf_url": "https://arxiv.org/pdf/2512.07120",
        "title": "Chromatic Feature Vectors for 2-Trees: Exact Formulas for Partition Enumeration with Network Applications",
        "authors": [
            "J. Allagan",
            "G. Morgan",
            "S. Langley",
            "R. Lopez-Bonilla",
            "V. Deriglazov"
        ],
        "comments": "18 pages",
        "subjects": "Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG)",
        "abstract": "We establish closed-form enumeration formulas for chromatic feature vectors of 2-trees under the bichromatic triangle constraint. These efficiently computable structural features derive from constrained graph colorings where each triangle uses exactly two colors, forbidding monochromatic and rainbow triangles, a constraint arising in distributed systems where components avoid complete concentration or isolation. For theta graphs Theta_n, we prove r_k(Theta_n) = S(n-2, k-1) for k >= 3 (Stirling numbers of the second kind) and r_2(Theta_n) = 2^(n-2) + 1, computable in O(n) time. For fan graphs Phi_n, we establish r_2(Phi_n) = F_{n+1} (Fibonacci numbers) and derive explicit formulas r_k(Phi_n) = sum_{t=k-1}^{n-1} a_{n-1,t} * S(t, k-1) with efficiently computable binomial coefficients, achieving O(n^2) computation per component. Unlike classical chromatic polynomials, which assign identical features to all n-vertex 2-trees, bichromatic constraints provide informative structural features. While not complete graph invariants, these features capture meaningful structural properties through connections to Fibonacci polynomials, Bell numbers, and independent set enumeration. Applications include Byzantine fault tolerance in hierarchical networks, VM allocation in cloud computing, and secret-sharing protocols in distributed cryptography.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 150,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07162",
        "abs_url": "https://arxiv.org/abs/2512.07162",
        "pdf_url": "https://arxiv.org/pdf/2512.07162",
        "title": "DeepSVM: Learning Stochastic Volatility Models with Physics-Informed Deep Operator Networks",
        "authors": [
            "Kieran A. Malandain",
            "Selim Kalici",
            "Hakob Chakhoyan"
        ],
        "comments": "",
        "subjects": "Computational Finance (q-fin.CP); Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Real-time calibration of stochastic volatility models (SVMs) is computationally bottlenecked by the need to repeatedly solve coupled partial differential equations (PDEs). In this work, we propose DeepSVM, a physics-informed Deep Operator Network (PI-DeepONet) designed to learn the solution operator of the Heston model across its entire parameter space. Unlike standard data-driven deep learning (DL) approaches, DeepSVM requires no labelled training data. Rather, we employ a hard-constrained ansatz that enforces terminal payoffs and static no-arbitrage conditions by design. Furthermore, we use Residual-based Adaptive Refinement (RAR) to stabilize training in difficult regions subject to high gradients. Overall, DeepSVM achieves a final training loss of $10^{-5}$ and predicts highly accurate option prices across a range of typical market dynamics. While pricing accuracy is high, we find that the model's derivatives (Greeks) exhibit noise in the at-the-money (ATM) regime, highlighting the specific need for higher-order regularization in physics-informed operator learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 151,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07168",
        "abs_url": "https://arxiv.org/abs/2512.07168",
        "pdf_url": "https://arxiv.org/pdf/2512.07168",
        "title": "JEPA as a Neural Tokenizer: Learning Robust Speech Representations with Density Adaptive Attention",
        "authors": [
            "Georgios Ioannides",
            "Christos Constantinou",
            "Aman Chadha",
            "Aaron Elkins",
            "Linsey Pang",
            "Ravid Shwartz-Ziv",
            "Yann LeCun"
        ],
        "comments": "UniReps: Unifying Representations in Neural Models (NeurIPS 2025 Workshop)",
        "subjects": "Sound (cs.SD); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)",
        "abstract": "We introduce a two-stage self-supervised framework that combines the Joint-Embedding Predictive Architecture (JEPA) with a Density Adaptive Attention Mechanism (DAAM) for learning robust speech representations. Stage~1 uses JEPA with DAAM to learn semantic audio features via masked prediction in latent space, fully decoupled from waveform reconstruction. Stage~2 leverages these representations for efficient tokenization using Finite Scalar Quantization (FSQ) and a mixed-radix packing scheme, followed by high-fidelity waveform reconstruction with a HiFi-GAN decoder. By integrating Gaussian mixture-based density-adaptive gating into the JEPA encoder, the model performs adaptive temporal feature selection and discovers hierarchical speech structure at a low frame rate of 2.5~Hz. The resulting tokens (47.5 tokens/sec) provide a reversible, highly compressed, and language-model-friendly representation that is competitive with, and often more efficient than, existing neural audio codecs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 152,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07201",
        "abs_url": "https://arxiv.org/abs/2512.07201",
        "pdf_url": "https://arxiv.org/pdf/2512.07201",
        "title": "Understanding Diffusion Models via Code Execution",
        "authors": [
            "Cheng Yu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Diffusion models have achieved remarkable performance in generative modeling, yet their theoretical foundations are often intricate, and the gap between mathematical formulations in papers and practical open-source implementations can be difficult to bridge. Existing tutorials primarily focus on deriving equations, offering limited guidance on how diffusion models actually operate in code. To address this, we present a concise implementation of approximately 300 lines that explains diffusion models from a code-execution perspective. Our minimal example preserves the essential components -- including forward diffusion, reverse sampling, the noise-prediction network, and the training loop -- while removing unnecessary engineering details. This technical report aims to provide researchers with a clear, implementation-first understanding of how diffusion models work in practice and how code and theory correspond. Our code and pre-trained models are available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 153,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07209",
        "abs_url": "https://arxiv.org/abs/2512.07209",
        "pdf_url": "https://arxiv.org/pdf/2512.07209",
        "title": "Coherent Audio-Visual Editing via Conditional Audio Generation Following Video Edits",
        "authors": [
            "Masato Ishii",
            "Akio Hayakawa",
            "Takashi Shibuya",
            "Yuki Mitsufuji"
        ],
        "comments": "",
        "subjects": "Multimedia (cs.MM); Machine Learning (cs.LG); Sound (cs.SD)",
        "abstract": "We introduce a novel pipeline for joint audio-visual editing that enhances the coherence between edited video and its accompanying audio. Our approach first applies state-of-the-art video editing techniques to produce the target video, then performs audio editing to align with the visual changes. To achieve this, we present a new video-to-audio generation model that conditions on the source audio, target video, and a text prompt. We extend the model architecture to incorporate conditional audio input and propose a data augmentation strategy that improves training efficiency. Furthermore, our model dynamically adjusts the influence of the source audio based on the complexity of the edits, preserving the original audio structure where possible. Experimental results demonstrate that our method outperforms existing approaches in maintaining audio-visual alignment and content integrity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 154,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07216",
        "abs_url": "https://arxiv.org/abs/2512.07216",
        "pdf_url": "https://arxiv.org/pdf/2512.07216",
        "title": "MUSE: A Simple Yet Effective Multimodal Search-Based Framework for Lifelong User Interest Modeling",
        "authors": [
            "Bin Wu",
            "Feifan Yang",
            "Zhangming Chan",
            "Yu-Ran Gu",
            "Jiawei Feng",
            "Chao Yi",
            "Xiang-Rong Sheng",
            "Han Zhu",
            "Jian Xu",
            "Mang Ye",
            "Bo Zheng"
        ],
        "comments": "",
        "subjects": "Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "Lifelong user interest modeling is crucial for industrial recommender systems, yet existing approaches rely predominantly on ID-based features, suffering from poor generalization on long-tail items and limited semantic expressiveness. While recent work explores multimodal representations for behavior retrieval in the General Search Unit (GSU), they often neglect multimodal integration in the fine-grained modeling stage -- the Exact Search Unit (ESU). In this work, we present a systematic analysis of how to effectively leverage multimodal signals across both stages of the two-stage lifelong modeling framework. Our key insight is that simplicity suffices in the GSU: lightweight cosine similarity with high-quality multimodal embeddings outperforms complex retrieval mechanisms. In contrast, the ESU demands richer multimodal sequence modeling and effective ID-multimodal fusion to unlock its full potential. Guided by these principles, we propose MUSE, a simple yet effective multimodal search-based framework. MUSE has been deployed in Taobao display advertising system, enabling 100K-length user behavior sequence modeling and delivering significant gains in top-line metrics with negligible online latency overhead. To foster community research, we share industrial deployment practices and open-source the first large-scale dataset featuring ultra-long behavior sequences paired with high-quality multimodal embeddings. Our code and data is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 155,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07218",
        "abs_url": "https://arxiv.org/abs/2512.07218",
        "pdf_url": "https://arxiv.org/pdf/2512.07218",
        "title": "NeSTR: A Neuro-Symbolic Abductive Framework for Temporal Reasoning in Large Language Models",
        "authors": [
            "Feng Liang",
            "Weixin Zeng",
            "Runhao Zhao",
            "Xiang Zhao"
        ],
        "comments": "Accepted by AAAI 2026",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, temporal reasoning, particularly under complex temporal constraints, remains a major challenge. To this end, existing approaches have explored symbolic methods, which encode temporal structure explicitly, and reflective mechanisms, which revise reasoning errors through multi-step inference. Nonetheless, symbolic approaches often underutilize the reasoning capabilities of LLMs, while reflective methods typically lack structured temporal representations, which can result in inconsistent or hallucinated reasoning. As a result, even when the correct temporal context is available, LLMs may still misinterpret or misapply time-related information, leading to incomplete or inaccurate answers. To address these limitations, in this work, we propose Neuro-Symbolic Temporal Reasoning (NeSTR), a novel framework that integrates structured symbolic representations with hybrid reflective reasoning to enhance the temporal sensitivity of LLM inference. NeSTR preserves explicit temporal relations through symbolic encoding, enforces logical consistency via verification, and corrects flawed inferences using abductive reflection. Extensive experiments on diverse temporal question answering benchmarks demonstrate that NeSTR achieves superior zero-shot performance and consistently improves temporal reasoning without any fine-tuning, showcasing the advantage of neuro-symbolic integration in enhancing temporal understanding in large language models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 156,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07228",
        "abs_url": "https://arxiv.org/abs/2512.07228",
        "pdf_url": "https://arxiv.org/pdf/2512.07228",
        "title": "Towards Robust Protective Perturbation against DeepFake Face Swapping",
        "authors": [
            "Hengyang Yao",
            "Lin Li",
            "Ke Sun",
            "Jianing Qiu",
            "Huiping Chen"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
        "abstract": "DeepFake face swapping enables highly realistic identity forgeries, posing serious privacy and security risks. A common defence embeds invisible perturbations into images, but these are fragile and often destroyed by basic transformations such as compression or resizing. In this paper, we first conduct a systematic analysis of 30 transformations across six categories and show that protection robustness is highly sensitive to the choice of training transformations, making the standard Expectation over Transformation (EOT) with uniform sampling fundamentally suboptimal. Motivated by this, we propose Expectation Over Learned distribution of Transformation (EOLT), the framework to treat transformation distribution as a learnable component rather than a fixed design choice. Specifically, EOLT employs a policy network that learns to automatically prioritize critical transformations and adaptively generate instance-specific perturbations via reinforcement learning, enabling explicit modeling of defensive bottlenecks while maintaining broad transferability. Extensive experiments demonstrate that our method achieves substantial improvements over state-of-the-art approaches, with 26% higher average robustness and up to 30% gains on challenging transformation categories.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 157,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07267",
        "abs_url": "https://arxiv.org/abs/2512.07267",
        "pdf_url": "https://arxiv.org/pdf/2512.07267",
        "title": "Non-negative DAG Learning from Time-Series Data",
        "authors": [
            "Samuel Rey",
            "Gonzalo Mateos"
        ],
        "comments": "",
        "subjects": "Signal Processing (eess.SP); Machine Learning (cs.LG)",
        "abstract": "This work aims to learn the directed acyclic graph (DAG) that captures the instantaneous dependencies underlying a multivariate time series. The observed data follow a linear structural vector autoregressive model (SVARM) with both instantaneous and time-lagged dependencies, where the instantaneous structure is modeled by a DAG to reflect potential causal relationships. While recent continuous relaxation approaches impose acyclicity through smooth constraint functions involving powers of the adjacency matrix, they lead to non-convex optimization problems that are challenging to solve. In contrast, we assume that the underlying DAG has only non-negative edge weights, and leverage this additional structure to impose acyclicity via a convex constraint. This enables us to cast the problem of non-negative DAG recovery from multivariate time-series data as a convex optimization problem in abstract form, which we solve using the method of multipliers. Crucially, the convex formulation guarantees global optimality of the solution. Finally, we assess the performance of the proposed method on synthetic time-series data, where it outperforms existing alternatives.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 158,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07279",
        "abs_url": "https://arxiv.org/abs/2512.07279",
        "pdf_url": "https://arxiv.org/pdf/2512.07279",
        "title": "Verifiable Deep Quantitative Group Testing",
        "authors": [
            "Shreyas Jayant Grampurohit",
            "Satish Mulleti",
            "Ajit Rajwade"
        ],
        "comments": "11 pages, 2 figures, 3 tables",
        "subjects": "Signal Processing (eess.SP); Machine Learning (cs.LG)",
        "abstract": "We present a neural network-based framework for solving the quantitative group testing (QGT) problem that achieves both high decoding accuracy and structural verifiability. In QGT, the objective is to identify a small subset of defective items among $N$ candidates using only $M \\ll N$ pooled tests, each reporting the number of defectives in the tested subset. We train a multi-layer perceptron to map noisy measurement vectors to binary defect indicators, achieving accurate and robust recovery even under sparse, bounded perturbations. Beyond accuracy, we show that the trained network implicitly learns the underlying pooling structure that links items to tests, allowing this structure to be recovered directly from the network's Jacobian. This indicates that the model does not merely memorize training patterns but internalizes the true combinatorial relationships governing QGT. Our findings reveal that standard feedforward architectures can learn verifiable inverse mappings in structured combinatorial recovery problems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 159,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07289",
        "abs_url": "https://arxiv.org/abs/2512.07289",
        "pdf_url": "https://arxiv.org/pdf/2512.07289",
        "title": "Equivariant Diffusion for Crystal Structure Prediction",
        "authors": [
            "Peijia Lin",
            "Pin Chen",
            "Rui Jiao",
            "Qing Mo",
            "Jianhuan Cen",
            "Wenbing Huang",
            "Yang Liu",
            "Dan Huang",
            "Yutong Lu"
        ],
        "comments": "ICML 2024",
        "subjects": "Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG)",
        "abstract": "In addressing the challenge of Crystal Structure Prediction (CSP), symmetry-aware deep learning models, particularly diffusion models, have been extensively studied, which treat CSP as a conditional generation task. However, ensuring permutation, rotation, and periodic translation equivariance during diffusion process remains incompletely addressed. In this work, we propose EquiCSP, a novel equivariant diffusion-based generative model. We not only address the overlooked issue of lattice permutation equivariance in existing models, but also develop a unique noising algorithm that rigorously maintains periodic translation equivariance throughout both training and inference processes. Our experiments indicate that EquiCSP significantly surpasses existing models in terms of generating accurate structures and demonstrates faster convergence during the training process.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 160,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07306",
        "abs_url": "https://arxiv.org/abs/2512.07306",
        "pdf_url": "https://arxiv.org/pdf/2512.07306",
        "title": "Exact Synthetic Populations for Scalable Societal and Market Modeling",
        "authors": [
            "Thierry Petit",
            "Arnault Pachot"
        ],
        "comments": "Submitted for peer review on December 7, 2025",
        "subjects": "Machine Learning (stat.ML); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "We introduce a constraint-programming framework for generating synthetic populations that reproduce target statistics with high precision while enforcing full individual consistency. Unlike data-driven approaches that infer distributions from samples, our method directly encodes aggregated statistics and structural relations, enabling exact control of demographic profiles without requiring any microdata. We validate the approach on official demographic sources and study the impact of distributional deviations on downstream analyses. This work is conducted within the Pollitics project developed by Emotia, where synthetic populations can be queried through large language models to model societal behaviors, explore market and policy scenarios, and provide reproducible decision-grade insights without personal data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 161,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07329",
        "abs_url": "https://arxiv.org/abs/2512.07329",
        "pdf_url": "https://arxiv.org/pdf/2512.07329",
        "title": "Two-dimensional RMSD projections for reaction path visualization and validation",
        "authors": [
            "Rohit Goswami"
        ],
        "comments": "4 pages, 1 figure",
        "subjects": "Chemical Physics (physics.chem-ph); Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG); Computational Physics (physics.comp-ph)",
        "abstract": "Transition state or minimum energy path finding methods constitute a routine component of the computational chemistry toolkit. Standard analysis involves trajectories conventionally plotted in terms of the relative energy to the initial state against a cumulative displacement variable, or the image number. These dimensional reductions obscure structural rearrangements in high dimensions and may often be trajectory dependent. This precludes the ability to compare optimization trajectories of different methods beyond the number of calculations, time taken, and final saddle geometry. We present a method mapping trajectories onto a two-dimension surface defined by a permutation corrected root mean square deviation from the reactant and product configurations. Energy is represented as an interpolated color-mapped surface constructed from all optimization steps using radial basis functions. This representation highlights optimization trajectories, identifies endpoint basins, and diagnoses convergence concerns invisible in one-dimensional profiles. We validate the framework on a cycloaddition reaction, showing that a machine-learned potential saddle and density functional theory reference lie on comparable energy contours despite geometric displacements.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 162,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07335",
        "abs_url": "https://arxiv.org/abs/2512.07335",
        "pdf_url": "https://arxiv.org/pdf/2512.07335",
        "title": "Machine learning in an expectation-maximisation framework for nowcasting",
        "authors": [
            "Paul Wilsens",
            "Katrien Antonio",
            "Gerda Claeskens"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Decision making often occurs in the presence of incomplete information, leading to the under- or overestimation of risk. Leveraging the observable information to learn the complete information is called nowcasting. In practice, incomplete information is often a consequence of reporting or observation delays. In this paper, we propose an expectation-maximisation (EM) framework for nowcasting that uses machine learning techniques to model both the occurrence as well as the reporting process of events. We allow for the inclusion of covariate information specific to the occurrence and reporting periods as well as characteristics related to the entity for which events occurred. We demonstrate how the maximisation step and the information flow between EM iterations can be tailored to leverage the predictive power of neural networks and (extreme) gradient boosting machines (XGBoost). With simulation experiments, we show that we can effectively model both the occurrence and reporting of events when dealing with high-dimensional covariate information. In the presence of non-linear effects, we show that our methodology outperforms existing EM-based nowcasting frameworks that use generalised linear models in the maximisation step. Finally, we apply the framework to the reporting of Argentinian Covid-19 cases, where the XGBoost-based approach again is most performant.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 163,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07342",
        "abs_url": "https://arxiv.org/abs/2512.07342",
        "pdf_url": "https://arxiv.org/pdf/2512.07342",
        "title": "PrivORL: Differentially Private Synthetic Dataset for Offline Reinforcement Learning",
        "authors": [
            "Chen Gong",
            "Zheng Liu",
            "Kecen Li",
            "Tianhao Wang"
        ],
        "comments": "Accepted at NDSS 2026; code available at this https URL",
        "subjects": "Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
        "abstract": "Recently, offline reinforcement learning (RL) has become a popular RL paradigm. In offline RL, data providers share pre-collected datasets -- either as individual transitions or sequences of transitions forming trajectories -- to enable the training of RL models (also called agents) without direct interaction with the environments. Offline RL saves interactions with environments compared to traditional RL, and has been effective in critical areas, such as navigation tasks. Meanwhile, concerns about privacy leakage from offline RL datasets have emerged. To safeguard private information in offline RL datasets, we propose the first differential privacy (DP) offline dataset synthesis method, PrivORL, which leverages a diffusion model and diffusion transformer to synthesize transitions and trajectories, respectively, under DP. The synthetic dataset can then be securely released for downstream analysis and research. PrivORL adopts the popular approach of pre-training a synthesizer on public datasets, and then fine-tuning on sensitive datasets using DP Stochastic Gradient Descent (DP-SGD). Additionally, PrivORL introduces curiosity-driven pre-training, which uses feedback from the curiosity module to diversify the synthetic dataset and thus can generate diverse synthetic transitions and trajectories that closely resemble the sensitive dataset. Extensive experiments on five sensitive offline RL datasets show that our method achieves better utility and fidelity in both DP transition and trajectory synthesis compared to baselines. The replication package is available at the GitHub repository.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 164,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07404",
        "abs_url": "https://arxiv.org/abs/2512.07404",
        "pdf_url": "https://arxiv.org/pdf/2512.07404",
        "title": "Do LLMs Trust the Code They Write?",
        "authors": [
            "Francisco Ribeiro",
            "Claudio Spiess",
            "Prem Devanbu",
            "Sarah Nadi"
        ],
        "comments": "",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Despite the effectiveness of large language models (LLMs) for code generation, they often output incorrect code. One reason is that model output probabilities are often not well-correlated with correctness, and reflect only the final output of the generation process. Inspired by findings that LLMs internally encode concepts like truthfulness, this paper explores if LLMs similarly represent code correctness. Specifically, we identify a correctness representation inside LLMs by contrasting the hidden states between pairs of correct and incorrect code for the same programming tasks. By experimenting on four LLMs, we show that exploiting this extracted correctness representation outperforms standard log-likelihood ranking, as well as verbalized model confidence. Furthermore, we explore how this internal correctness signal can be used to select higher-quality code samples, without requiring test execution. Ultimately, this work demonstrates how leveraging internal representations can enhance code generation systems and make LLMs more reliable, thus improving confidence in automatically generated code.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 165,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07425",
        "abs_url": "https://arxiv.org/abs/2512.07425",
        "pdf_url": "https://arxiv.org/pdf/2512.07425",
        "title": "Microseismic event classification with a lightweight Fourier Neural Operator model",
        "authors": [
            "Ayrat Abdullin",
            "Umair bin Waheed",
            "Leo Eisner",
            "Abdullatif Al-Shuhail"
        ],
        "comments": "Submitted to Nature Scientific Reports",
        "subjects": "Geophysics (physics.geo-ph); Machine Learning (cs.LG)",
        "abstract": "Real-time monitoring of induced seismicity is crucial for mitigating operational hazards, relying on the rapid and accurate classification of microseismic events from continuous data streams. However, while many deep learning models excel at this task, their high computational requirements often limit their practical application in real-time monitoring systems. To address this limitation, a lightweight model based on the Fourier Neural Operator (FNO) is proposed for microseismic event classification, leveraging its inherent resolution-invariance and computational efficiency for waveform processing. In the STanford EArthquake Dataset (STEAD), a global and large-scale database of seismic waveforms, the FNO-based model demonstrates high effectiveness for trigger classification, with an F1 score of 95% even in the scenario of data sparsity in training. The new FNO model greatly decreases the computer power needed relative to current deep learning models without sacrificing the classification success rate measured by the F1 score. A test on a real microseismic dataset shows a classification success rate with an F1 score of 98%, outperforming many traditional deep-learning techniques. A combination of high success rate and low computational power indicates that the FNO model can serve as a methodology of choice for real-time monitoring of microseismicity for induced seismicity. The method saves computational resources and facilitates both post-processing and real-time seismic processing suitable for the implementation of traffic light systems to prevent undesired induced seismicity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 166,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07458",
        "abs_url": "https://arxiv.org/abs/2512.07458",
        "pdf_url": "https://arxiv.org/pdf/2512.07458",
        "title": "Optimized Machine Learning Methods for Studying the Thermodynamic Behavior of Complex Spin Systems",
        "authors": [
            "Dmitrii Kapitan",
            "Pavel Ovchinnikov",
            "Konstantin Soldatov",
            "Petr Andriushchenko",
            "Vitalii Kapitan"
        ],
        "comments": "16 pages, in Russian language, 8 figures, 2 tables",
        "subjects": "Computational Physics (physics.comp-ph); Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning (cs.LG)",
        "abstract": "This paper presents a systematic study of the application of convolutional neural networks (CNNs) as an efficient and versatile tool for the analysis of critical and low-temperature phase states in spin system models. The problem of calculating the dependence of the average energy on the spatial distribution of exchange integrals for the Edwards-Anderson model on a square lattice with frustrated interactions is considered. We further construct a single convolutional classifier of phase states of the ferromagnetic Ising model on square, triangular, honeycomb, and kagome lattices, trained on configurations generated by the Swendsen-Wang cluster algorithm. Computed temperature profiles of the averaged posterior probability of the high-temperature phase form clear S-shaped curves that intersect in the vicinity of the theoretical critical temperatures and allow one to determine the critical temperature for the kagome lattice without additional retraining. It is shown that convolutional models substantially reduce the root-mean-square error (RMSE) compared with fully connected architectures and efficiently capture complex correlations between thermodynamic characteristics and the structure of magnetic correlated systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 167,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07472",
        "abs_url": "https://arxiv.org/abs/2512.07472",
        "pdf_url": "https://arxiv.org/pdf/2512.07472",
        "title": "Affordance Field Intervention: Enabling VLAs to Escape Memory Traps in Robotic Manipulation",
        "authors": [
            "Siyu Xu",
            "Zijian Wang",
            "Yunke Wang",
            "Chenghao Xia",
            "Tao Huang",
            "Chang Xu"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
        "abstract": "Vision-Language-Action (VLA) models have shown great performance in robotic manipulation by mapping visual observations and language instructions directly to actions. However, they remain brittle under distribution shifts: when test scenarios change, VLAs often reproduce memorized trajectories instead of adapting to the updated scene, which is a failure mode we refer to as the \"Memory Trap\". This limitation stems from the end-to-end design, which lacks explicit 3D spatial reasoning and prevents reliable identification of actionable regions in unfamiliar environments. To compensate for this missing spatial understanding, 3D Spatial Affordance Fields (SAFs) can provide a geometric representation that highlights where interactions are physically feasible, offering explicit cues about regions the robot should approach or avoid. We therefore introduce Affordance Field Intervention (AFI), a lightweight hybrid framework that uses SAFs as an on-demand plug-in to guide VLA behavior. Our system detects memory traps through proprioception, repositions the robot to recent high-affordance regions, and proposes affordance-driven waypoints that anchor VLA-generated actions. A SAF-based scorer then selects trajectories with the highest cumulative affordance. Extensive experiments demonstrate that our method achieves an average improvement of 23.5% across different VLA backbones ($\\pi_{0}$ and $\\pi_{0.5}$) under out-of-distribution scenarios on real-world robotic platforms, and 20.2% on the LIBERO-Pro benchmark, validating its effectiveness in enhancing VLA robustness to distribution shifts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 168,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07541",
        "abs_url": "https://arxiv.org/abs/2512.07541",
        "pdf_url": "https://arxiv.org/pdf/2512.07541",
        "title": "High-Dimensional Change Point Detection using Graph Spanning Ratio",
        "authors": [
            "Youngwen Sun",
            "Katerina Papagiannouli",
            "Vladimir Spokoiny"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Inspired by graph-based methodologies, we introduce a novel graph-spanning algorithm designed to identify changes in both offline and online data across low to high dimensions. This versatile approach is applicable to Euclidean and graph-structured data with unknown distributions, while maintaining control over error probabilities. Theoretically, we demonstrate that the algorithm achieves high detection power when the magnitude of the change surpasses the lower bound of the minimax separation rate, which scales on the order of $\\sqrt{nd}$. Our method outperforms other techniques in terms of accuracy for both Gaussian and non-Gaussian data. Notably, it maintains strong detection power even with small observation windows, making it particularly effective for online environments where timely and precise change detection is critical.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 169,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07557",
        "abs_url": "https://arxiv.org/abs/2512.07557",
        "pdf_url": "https://arxiv.org/pdf/2512.07557",
        "title": "On Conditional Independence Graph Learning From Multi-Attribute Gaussian Dependent Time Series",
        "authors": [
            "Jitendra K. Tugnait"
        ],
        "comments": "16 pages, 3 figures, 4 tables",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Signal Processing (eess.SP)",
        "abstract": "Estimation of the conditional independence graph (CIG) of high-dimensional multivariate Gaussian time series from multi-attribute data is considered. Existing methods for graph estimation for such data are based on single-attribute models where one associates a scalar time series with each node. In multi-attribute graphical models, each node represents a random vector or vector time series. In this paper we provide a unified theoretical analysis of multi-attribute graph learning for dependent time series using a penalized log-likelihood objective function formulated in the frequency domain using the discrete Fourier transform of the time-domain data. We consider both convex (sparse-group lasso) and non-convex (log-sum and SCAD group penalties) penalty/regularization functions. We establish sufficient conditions in a high-dimensional setting for consistency (convergence of the inverse power spectral density to true value in the Frobenius norm), local convexity when using non-convex penalties, and graph recovery. We do not impose any incoherence or irrepresentability condition for our convergence results. We also empirically investigate selection of the tuning parameters based on the Bayesian information criterion, and illustrate our approach using numerical examples utilizing both synthetic and real data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 170,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07578",
        "abs_url": "https://arxiv.org/abs/2512.07578",
        "pdf_url": "https://arxiv.org/pdf/2512.07578",
        "title": "$ϕ$-test: Global Feature Selection and Inference for Shapley Additive Explanations",
        "authors": [
            "Dongseok Kim",
            "Hyoungsun Choi",
            "Mohamed Jismy Aashik Rasool",
            "Gisung Oh"
        ],
        "comments": "15 pages",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Methodology (stat.ME)",
        "abstract": "We propose $\\phi$-test, a global feature-selection and significance procedure for black-box predictors that combines Shapley attributions with selective inference. Given a trained model and an evaluation dataset, $\\phi$-test performs SHAP-guided screening and fits a linear surrogate on the screened features via a selection rule with a tractable selective-inference form. For each retained feature, it outputs a Shapley-based global score, a surrogate coefficient, and post-selection $p$-values and confidence intervals in a global feature-importance table. Experiments on real tabular regression tasks with tree-based and neural backbones suggest that $\\phi$-test can retain much of the predictive ability of the original model while using only a few features and producing feature sets that remain fairly stable across resamples and backbone classes. In these settings, $\\phi$-test acts as a practical global explanation layer linking Shapley-based importance summaries with classical statistical inference.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 171,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07612",
        "abs_url": "https://arxiv.org/abs/2512.07612",
        "pdf_url": "https://arxiv.org/pdf/2512.07612",
        "title": "PCMind-2.1-Kaiyuan-2B Technical Report",
        "authors": [
            "Kairong Luo",
            "Zhenbo Sun",
            "Xinyu Shi",
            "Shengqi Chen",
            "Bowen Yu",
            "Yunyi Chen",
            "Chenyi Dang",
            "Hengtao Tao",
            "Hui Wang",
            "Fangming Liu",
            "Kaifeng Lyu",
            "Wenguang Chen"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "The rapid advancement of Large Language Models (LLMs) has resulted in a significant knowledge gap between the open-source community and industry, primarily because the latter relies on closed-source, high-quality data and training recipes. To address this, we introduce PCMind-2.1-Kaiyuan-2B, a fully open-source 2-billion-parameter model focused on improving training efficiency and effectiveness under resource constraints. Our methodology includes three key innovations: a Quantile Data Benchmarking method for systematically comparing heterogeneous open-source datasets and providing insights on data mixing strategies; a Strategic Selective Repetition scheme within a multi-phase paradigm to effectively leverage sparse, high-quality data; and a Multi-Domain Curriculum Training policy that orders samples by quality. Supported by a highly optimized data preprocessing pipeline and architectural modifications for FP16 stability, Kaiyuan-2B achieves performance competitive with state-of-the-art fully open-source models, demonstrating practical and scalable solutions for resource-limited pretraining. We release all assets (including model weights, data, and code) under Apache 2.0 license at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 172,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07650",
        "abs_url": "https://arxiv.org/abs/2512.07650",
        "pdf_url": "https://arxiv.org/pdf/2512.07650",
        "title": "Exploring Test-time Scaling via Prediction Merging on Large-Scale Recommendation",
        "authors": [
            "Fuyuan Lyu",
            "Zhentai Chen",
            "Jingyan Jiang",
            "Lingjie Li",
            "Xing Tang",
            "Xiuqiang He",
            "Xue Liu"
        ],
        "comments": "",
        "subjects": "Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "Inspired by the success of language models (LM), scaling up deep learning recommendation systems (DLRS) has become a recent trend in the community. All previous methods tend to scale up the model parameters during training time. However, how to efficiently utilize and scale up computational resources during test time remains underexplored, which can prove to be a scaling-efficient approach and bring orthogonal improvements in LM domains. The key point in applying test-time scaling to DLRS lies in effectively generating diverse yet meaningful outputs for the same instance. We propose two ways: One is to explore the heterogeneity of different model architectures. The other is to utilize the randomness of model initialization under a homogeneous architecture. The evaluation is conducted across eight models, including both classic and SOTA models, on three benchmarks. Sufficient evidence proves the effectiveness of both solutions. We further prove that under the same inference budget, test-time scaling can outperform parameter scaling. Our test-time scaling can also be seamlessly accelerated with the increase in parallel servers when deployed online, without affecting the inference time on the user side. Code is available.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 173,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07697",
        "abs_url": "https://arxiv.org/abs/2512.07697",
        "pdf_url": "https://arxiv.org/pdf/2512.07697",
        "title": "Delay-Aware Diffusion Policy: Bridging the Observation-Execution Gap in Dynamic Tasks",
        "authors": [
            "Aileen Liao",
            "Dong-Ki Kim",
            "Max Olan Smith",
            "Ali-akbar Agha-mohammadi",
            "Shayegan Omidshafiei"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
        "abstract": "As a robot senses and selects actions, the world keeps changing. This inference delay creates a gap of tens to hundreds of milliseconds between the observed state and the state at execution. In this work, we take the natural generalization from zero delay to measured delay during training and inference. We introduce Delay-Aware Diffusion Policy (DA-DP), a framework for explicitly incorporating inference delays into policy learning. DA-DP corrects zero-delay trajectories to their delay-compensated counterparts, and augments the policy with delay conditioning. We empirically validate DA-DP on a variety of tasks, robots, and delays and find its success rate more robust to delay than delay-unaware methods. DA-DP is architecture agnostic and transfers beyond diffusion policies, offering a general pattern for delay-aware imitation learning. More broadly, DA-DP encourages evaluation protocols that report performance as a function of measured latency, not just task difficulty.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 174,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07710",
        "abs_url": "https://arxiv.org/abs/2512.07710",
        "pdf_url": "https://arxiv.org/pdf/2512.07710",
        "title": "Each Prompt Matters: Scaling Reinforcement Learning Without Wasting Rollouts on Hundred-Billion-Scale MoE",
        "authors": [
            "Anxiang Zeng",
            "Haibo Zhang",
            "Hailing Zhang",
            "Kaixiang Mo",
            "Liang Yao",
            "Ling Hu",
            "Long Zhang",
            "Shuman Liu",
            "Shuyi Xie",
            "Yanshi Li",
            "Yizhang Chen",
            "Yuepeng Sheng",
            "Yuwei Huang",
            "Zhaochen Xu",
            "Zhiqiang Zhou",
            "Ziqin Liew"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "We present CompassMax-V3-Thinking, a hundred-billion-scale MoE reasoning model trained with a new RL framework built on one principle: each prompt must matter. Scaling RL to this size exposes critical inefficiencies-zero-variance prompts that waste rollouts, unstable importance sampling over long horizons, advantage inversion from standard reward models, and systemic bottlenecks in rollout processing. To overcome these challenges, we introduce several unified innovations: (1) Multi-Stage Zero-Variance Elimination, which filters out non-informative prompts and stabilizes group-based policy optimization (e.g. GRPO) by removing wasted rollouts; (2) ESPO, an entropy-adaptive optimization method that balances token-level and sequence-level importance sampling to maintain stable learning dynamics; (3) a Router Replay strategy that aligns training-time MoE router decisions with inference-time behavior to mitigate train-infer discrepancies, coupled with a reward model adjustment to prevent advantage inversion; (4) a high-throughput RL system with FP8-precision rollouts, overlapped reward computation, and length-aware scheduling to eliminate performance bottlenecks. Together, these contributions form a cohesive pipeline that makes RL on hundred-billion-scale MoE models stable and efficient. The resulting model delivers strong performance across both internal and public evaluations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 175,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07737",
        "abs_url": "https://arxiv.org/abs/2512.07737",
        "pdf_url": "https://arxiv.org/pdf/2512.07737",
        "title": "A scalable and real-time neural decoder for topological quantum codes",
        "authors": [
            "Andrew W. Senior",
            "Thomas Edlich",
            "Francisco J.H. Heras",
            "Lei M. Zhang",
            "Oscar Higgott",
            "James S. Spencer",
            "Taylor Applebaum",
            "Sam Blackwell",
            "Justin Ledford",
            "Akvilė Žemgulytė",
            "Augustin Žídek",
            "Noah Shutty",
            "Andrew Cowie",
            "Yin Li",
            "George Holland",
            "Peter Brooks",
            "Charlie Beattie",
            "Michael Newman",
            "Alex Davies",
            "Cody Jones",
            "Sergio Boixo",
            "Hartmut Neven",
            "Pushmeet Kohli",
            "Johannes Bausch"
        ],
        "comments": "",
        "subjects": "Quantum Physics (quant-ph); Machine Learning (cs.LG)",
        "abstract": "Fault-tolerant quantum computing will require error rates far below those achievable with physical qubits. Quantum error correction (QEC) bridges this gap, but depends on decoders being simultaneously fast, accurate, and scalable. This combination of requirements has not yet been met by a machine-learning decoder, nor by any decoder for promising resource-efficient codes such as the colour code. Here we introduce AlphaQubit 2, a neural-network decoder that achieves near-optimal logical error rates for both surface and colour codes at large scales under realistic noise. For the colour code, it is orders of magnitude faster than other high-accuracy decoders. For the surface code, we demonstrate real-time decoding faster than 1 microsecond per cycle up to distance 11 on current commercial accelerators with better accuracy than leading real-time decoders. These results support the practical application of a wider class of promising QEC codes, and establish a credible path towards high-accuracy, real-time neural decoding at the scales required for fault-tolerant quantum computation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 176,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07755",
        "abs_url": "https://arxiv.org/abs/2512.07755",
        "pdf_url": "https://arxiv.org/pdf/2512.07755",
        "title": "Physics-Informed Neural Networks for Source Inversion and Parameters Estimation in Atmospheric Dispersion",
        "authors": [
            "Brenda Anague",
            "Bamdad Hosseini",
            "Issa Karambal",
            "Jean Medard Ngnotchouye"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Recent studies have shown the success of deep learning in solving forward and inverse problems in engineering and scientific computing domains, such as physics-informed neural networks (PINNs). In the fields of atmospheric science and environmental monitoring, estimating emission source locations is a central task that further relies on multiple model parameters that dictate velocity profiles and diffusion parameters. Estimating these parameters at the same time as emission sources from scarce data is a difficult task. In this work, we achieve this by leveraging the flexibility and generality of PINNs. We use a weighted adaptive method based on the neural tangent kernels to solve a source inversion problem with parameter estimation on the 2D and 3D advection-diffusion equations with unknown velocity and diffusion coefficients that may vary in space and time. Our proposed weighted adaptive method is presented as an extension of PINNs for forward PDE problems to a highly ill-posed source inversion and parameter estimation problem. The key idea behind our methodology is to attempt the joint recovery of the solution, the sources along with the unknown parameters, thereby using the underlying partial differential equation as a constraint that couples multiple unknown functional parameters, leading to more efficient use of the limited information in the measurements. We present various numerical experiments, using different types of measurements that model practical engineering systems, to show that our proposed method is indeed successful and robust to additional noise in the measurements.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 177,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07770",
        "abs_url": "https://arxiv.org/abs/2512.07770",
        "pdf_url": "https://arxiv.org/pdf/2512.07770",
        "title": "Distribution-informed Online Conformal Prediction",
        "authors": [
            "Dongjian Hu",
            "Junxi Wu",
            "Shu-Tao Xia",
            "Changliang Zou"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Conformal prediction provides a pivotal and flexible technique for uncertainty quantification by constructing prediction sets with a predefined coverage rate. Many online conformal prediction methods have been developed to address data distribution shifts in fully adversarial environments, resulting in overly conservative prediction sets. We propose Conformal Optimistic Prediction (COP), an online conformal prediction algorithm incorporating underlying data pattern into the update rule. Through estimated cumulative distribution function of non-conformity scores, COP produces tighter prediction sets when predictable pattern exists, while retaining valid coverage guarantees even when estimates are inaccurate. We establish a joint bound on coverage and regret, which further confirms the validity of our approach. We also prove that COP achieves distribution-free, finite-sample coverage under arbitrary learning rates and can converge when scores are $i.i.d.$. The experimental results also show that COP can achieve valid coverage and construct shorter prediction intervals than other baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 178,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07801",
        "abs_url": "https://arxiv.org/abs/2512.07801",
        "pdf_url": "https://arxiv.org/pdf/2512.07801",
        "title": "Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support",
        "authors": [
            "Raunak Jain",
            "Mudita Khurana"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)",
        "abstract": "LLM-based agents are rapidly being plugged into expert decision-support, yet in messy, high-stakes settings they rarely make the team smarter: human-AI teams often underperform the best individual, experts oscillate between verification loops and over-reliance, and the promised complementarity does not materialise. We argue this is not just a matter of accuracy, but a fundamental gap in how we conceive AI assistance: expert decisions are made through collaborative cognitive processes where mental models, goals, and constraints are continually co-constructed, tested, and revised between human and AI. We propose Collaborative Causal Sensemaking (CCS) as a research agenda and organizing framework for decision-support agents: systems designed as partners in cognitive work, maintaining evolving models of how particular experts reason, helping articulate and revise goals, co-constructing and stress-testing causal hypotheses, and learning from the outcomes of joint decisions so that both human and agent improve over time. We sketch challenges around training ecologies that make collaborative thinking instrumentally valuable, representations and interaction protocols for co-authored models, and evaluation centred on trust and complementarity. These directions can reframe MAS research around agents that participate in collaborative sensemaking and act as AI teammates that think with their human partners.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 179,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07808",
        "abs_url": "https://arxiv.org/abs/2512.07808",
        "pdf_url": "https://arxiv.org/pdf/2512.07808",
        "title": "LUNA: LUT-Based Neural Architecture for Fast and Low-Cost Qubit Readout",
        "authors": [
            "M. A. Farooq",
            "G. Di Guglielmo",
            "A. Rajagopala",
            "N. Tran",
            "V. A. Chhabria",
            "A. Arora"
        ],
        "comments": "",
        "subjects": "Quantum Physics (quant-ph); Machine Learning (cs.LG)",
        "abstract": "Qubit readout is a critical operation in quantum computing systems, which maps the analog response of qubits into discrete classical states. Deep neural networks (DNNs) have recently emerged as a promising solution to improve readout accuracy . Prior hardware implementations of DNN-based readout are resource-intensive and suffer from high inference latency, limiting their practical use in low-latency decoding and quantum error correction (QEC) loops. This paper proposes LUNA, a fast and efficient superconducting qubit readout accelerator that combines low-cost integrator-based preprocessing with Look-Up Table (LUT) based neural networks for classification. The architecture uses simple integrators for dimensionality reduction with minimal hardware overhead, and employs LogicNets (DNNs synthesized into LUT logic) to drastically reduce resource usage while enabling ultra-low-latency inference. We integrate this with a differential evolution based exploration and optimization framework to identify high-quality design points. Our results show up to a 10.95x reduction in area and 30% lower latency with little to no loss in fidelity compared to the state-of-the-art. LUNA enables scalable, low-footprint, and high-speed qubit readout, supporting the development of larger and more reliable quantum computing systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 180,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07820",
        "abs_url": "https://arxiv.org/abs/2512.07820",
        "pdf_url": "https://arxiv.org/pdf/2512.07820",
        "title": "Graph-Based Learning of Spectro-Topographical EEG Representations with Gradient Alignment for Brain-Computer Interfaces",
        "authors": [
            "Prithila Angkan",
            "Amin Jalali",
            "Paul Hungler",
            "Ali Etemad"
        ],
        "comments": "",
        "subjects": "Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)",
        "abstract": "We present a novel graph-based learning of EEG representations with gradient alignment (GEEGA) that leverages multi-domain information to learn EEG representations for brain-computer interfaces. Our model leverages graph convolutional networks to fuse embeddings from frequency-based topographical maps and time-frequency spectrograms, capturing inter-domain relationships. GEEGA addresses the challenge of achieving high inter-class separability, which arises from the temporally dynamic and subject-sensitive nature of EEG signals by incorporating the center loss and pairwise difference loss. Additionally, GEEGA incorporates a gradient alignment strategy to resolve conflicts between gradients from different domains and the fused embeddings, ensuring that discrepancies, where gradients point in conflicting directions, are aligned toward a unified optimization direction. We validate the efficacy of our method through extensive experiments on three publicly available EEG datasets: BCI-2a, CL-Drive and CLARE. Comprehensive ablation studies further highlight the impact of various components of our model.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 181,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07827",
        "abs_url": "https://arxiv.org/abs/2512.07827",
        "pdf_url": "https://arxiv.org/pdf/2512.07827",
        "title": "An Adaptive Multi-Layered Honeynet Architecture for Threat Behavior Analysis via Deep Learning",
        "authors": [
            "Lukas Johannes Möller"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)",
        "abstract": "The escalating sophistication and variety of cyber threats have rendered static honeypots inadequate, necessitating adaptive, intelligence-driven deception. In this work, ADLAH is introduced: an Adaptive Deep Learning Anomaly Detection Honeynet designed to maximize high-fidelity threat intelligence while minimizing cost through autonomous orchestration of infrastructure. The principal contribution is offered as an end-to-end architectural blueprint and vision for an AI-driven deception platform. Feasibility is evidenced by a functional prototype of the central decision mechanism, in which a reinforcement learning (RL) agent determines, in real time, when sessions should be escalated from low-interaction sensor nodes to dynamically provisioned, high-interaction honeypots. Because sufficient live data were unavailable, field-scale validation is not claimed; instead, design trade-offs and limitations are detailed, and a rigorous roadmap toward empirical evaluation at scale is provided. Beyond selective escalation and anomaly detection, the architecture pursues automated extraction, clustering, and versioning of bot attack chains, a core capability motivated by the empirical observation that exposed services are dominated by automated traffic. Together, these elements delineate a practical path toward cost-efficient capture of high-value adversary behavior, systematic bot versioning, and the production of actionable threat intelligence.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 182,
        "date": "2025-12-09",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-12-09?abs=True",
        "arxiv_id": "2512.07832",
        "abs_url": "https://arxiv.org/abs/2512.07832",
        "pdf_url": "https://arxiv.org/pdf/2512.07832",
        "title": "Do Generalisation Results Generalise?",
        "authors": [
            "Matteo Boglioni",
            "Andrea Sgobbi",
            "Gabriel Tavernini",
            "Francesco Rita",
            "Marius Mosbach",
            "Tiago Pimentel"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "A large language model's (LLM's) out-of-distribution (OOD) generalisation ability is crucial to its deployment. Previous work assessing LLMs' generalisation performance, however, typically focuses on a single out-of-distribution dataset. This approach may fail to precisely evaluate the capabilities of the model, as the data shifts encountered once a model is deployed are much more diverse. In this work, we investigate whether OOD generalisation results generalise. More specifically, we evaluate a model's performance across multiple OOD testsets throughout a finetuning run; we then evaluate the partial correlation of performances across these testsets, regressing out in-domain performance. This allows us to assess how correlated are generalisation performances once in-domain performance is controlled for. Analysing OLMo2 and OPT, we observe no overarching trend in generalisation results: the existence of a positive or negative correlation between any two OOD testsets depends strongly on the specific choice of model analysed.",
        "gemini2.5flash": "",
        "overall_idea": ""
    }
]