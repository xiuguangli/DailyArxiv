[
    {
        "order": 1,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12507",
        "abs_url": "https://arxiv.org/abs/2507.12507",
        "pdf_url": "https://arxiv.org/pdf/2507.12507",
        "title": "Scaling Up RL: Unlocking Diverse Reasoning in LLMs via Prolonged Training",
        "authors": [
            "Mingjie Liu",
            "Shizhe Diao",
            "Jian Hu",
            "Ximing Lu",
            "Xin Dong",
            "Hao Zhang",
            "Alexander Bukharin",
            "Shaokun Zhang",
            "Jiaqi Zeng",
            "Makesh Narsimhan Sreedhar",
            "Gerald Shen",
            "David Mosallanezhad",
            "Di Zhang",
            "Jonas Yang",
            "June Yang",
            "Oleksii Kuchaiev",
            "Guilin Liu",
            "Zhiding Yu",
            "Pavlo Molchanov",
            "Yejin Choi",
            "Jan Kautz",
            "Yi Dong"
        ],
        "comments": "14 pages, 7 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Recent advancements in reasoning-focused language models such as OpenAI's O1 and DeepSeek-R1 have shown that scaling test-time computation-through chain-of-thought reasoning and iterative exploration-can yield substantial improvements on complex tasks like mathematics and code generation. These breakthroughs have been driven by large-scale reinforcement learning (RL), particularly when combined with verifiable reward signals that provide objective and grounded supervision. In this report, we investigate the effects of prolonged reinforcement learning on a small language model across a diverse set of reasoning domains. Our work identifies several key ingredients for effective training, including the use of verifiable reward tasks, enhancements to Group Relative Policy Optimization (GRPO), and practical techniques to improve training stability and generalization. We introduce controlled KL regularization, clipping ratio, and periodic reference policy resets as critical components for unlocking long-term performance gains. Our model achieves significant improvements over strong baselines, including +14.7% on math, +13.9% on coding, and +54.8% on logic puzzle tasks. To facilitate continued research, we release our model publicly.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 2,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12549",
        "abs_url": "https://arxiv.org/abs/2507.12549",
        "pdf_url": "https://arxiv.org/pdf/2507.12549",
        "title": "The Serial Scaling Hypothesis",
        "authors": [
            "Yuxi Liu",
            "Konpat Preechakul",
            "Kananart Kuwaranancharoen",
            "Yutong Bai"
        ],
        "comments": "28 pages (13 pages main text + appendices & references), 8 figures, equal-contribution first authors",
        "subjects": "Machine Learning (cs.LG); Computational Complexity (cs.CC); Machine Learning (stat.ML)",
        "abstract": "While machine learning has advanced through massive parallelization, we identify a critical blind spot: some problems are fundamentally sequential. These \"inherently serial\" problems-from mathematical reasoning to physical simulations to sequential decision-making-require dependent computational steps that cannot be parallelized. Drawing from complexity theory, we formalize this distinction and demonstrate that current parallel-centric architectures face fundamental limitations on such tasks. We argue that recognizing the serial nature of computation holds profound implications on machine learning, model design, hardware development. As AI tackles increasingly complex reasoning, deliberately scaling serial computation-not just parallel computation-is essential for continued progress.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 3,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12555",
        "abs_url": "https://arxiv.org/abs/2507.12555",
        "pdf_url": "https://arxiv.org/pdf/2507.12555",
        "title": "Can Mental Imagery Improve the Thinking Capabilities of AI Systems?",
        "authors": [
            "Slimane Larabi"
        ],
        "comments": "15 pages, 8 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Although existing models can interact with humans and provide satisfactory responses, they lack the ability to act autonomously or engage in independent reasoning. Furthermore, input data in these models is typically provided as explicit queries, even when some sensory data is already acquired. In addition, AI agents, which are computational entities designed to perform tasks and make decisions autonomously based on their programming, data inputs, and learned knowledge, have shown significant progress. However, they struggle with integrating knowledge across multiple domains, unlike humans. Mental imagery plays a fundamental role in the brain's thinking process, which involves performing tasks based on internal multisensory data, planned actions, needs, and reasoning capabilities. In this paper, we investigate how to integrate mental imagery into a machine thinking framework and how this could be beneficial in initiating the thinking process. Our proposed machine thinking framework integrates a Cognitive thinking unit supported by three auxiliary units: the Input Data Unit, the Needs Unit, and the Mental Imagery Unit. Within this framework, data is represented as natural language sentences or drawn sketches, serving both informative and decision-making purposes. We conducted validation tests for this framework, and the results are presented and discussed.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 4,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12573",
        "abs_url": "https://arxiv.org/abs/2507.12573",
        "pdf_url": "https://arxiv.org/pdf/2507.12573",
        "title": "IncA-DES: An incremental and adaptive dynamic ensemble selection approach using online K-d tree neighborhood search for data streams with concept drift",
        "authors": [
            "Eduardo V. L. Barboza",
            "Paulo R. Lisboa de Almeida",
            "Alceu de Souza Britto Jr.",
            "Robert Sabourin",
            "Rafael M. O. Cruz"
        ],
        "comments": "Preprint of article published to Information Fusion",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Data streams pose challenges not usually encountered in batch-based ML. One of them is concept drift, which is characterized by the change in data distribution over time. Among many approaches explored in literature, the fusion of classifiers has been showing good results and is getting growing attention. DS methods, due to the ensemble being instance-based, seem to be an efficient choice under drifting scenarios. However, some attention must be paid to adapting such methods for concept drift. The training must be done in order to create local experts, and the commonly used neighborhood-search DS may become prohibitive with the continuous arrival of data. In this work, we propose IncA-DES, which employs a training strategy that promotes the generation of local experts with the assumption that different regions of the feature space become available with time. Additionally, the fusion of a concept drift detector supports the maintenance of information and adaptation to a new concept. An overlap-based classification filter is also employed in order to avoid using the DS method when there is a consensus in the neighborhood, a strategy that we argue every DS method should employ, as it was shown to make them more applicable and quicker. Moreover, aiming to reduce the processing time of the kNN, we propose an Online K-d tree algorithm, which can quickly remove instances without becoming inconsistent and deals with unbalancing concerns that may occur in data streams. Experimental results showed that the proposed framework got the best average accuracy compared to seven state-of-the-art methods considering different levels of label availability and presented the smaller processing time between the most accurate methods. Additionally, the fusion with the Online K-d tree has improved processing time with a negligible loss in accuracy. We have made our framework available in an online repository.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 5,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12574",
        "abs_url": "https://arxiv.org/abs/2507.12574",
        "pdf_url": "https://arxiv.org/pdf/2507.12574",
        "title": "Assay2Mol: large language model-based drug design using BioAssay context",
        "authors": [
            "Yifan Deng",
            "Spencer S. Ericksen",
            "Anthony Gitter"
        ],
        "comments": "23 pages, 10 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Quantitative Methods (q-bio.QM)",
        "abstract": "Scientific databases aggregate vast amounts of quantitative data alongside descriptive text. In biochemistry, molecule screening assays evaluate the functional responses of candidate molecules against disease targets. Unstructured text that describes the biological mechanisms through which these targets operate, experimental screening protocols, and other attributes of assays offer rich information for new drug discovery campaigns but has been untapped because of that unstructured format. We present Assay2Mol, a large language model-based workflow that can capitalize on the vast existing biochemical screening assays for early-stage drug discovery. Assay2Mol retrieves existing assay records involving targets similar to the new target and generates candidate molecules using in-context learning with the retrieved assay screening data. Assay2Mol outperforms recent machine learning approaches that generate candidate ligand molecules for target protein structures, while also promoting more synthesizable molecule generation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 6,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12583",
        "abs_url": "https://arxiv.org/abs/2507.12583",
        "pdf_url": "https://arxiv.org/pdf/2507.12583",
        "title": "Ranking Vectors Clustering: Theory and Applications",
        "authors": [
            "Ali Fattahi",
            "Ali Eshragh",
            "Babak Aslani",
            "Meysam Rabiee"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computational Complexity (cs.CC); Applications (stat.AP); Methodology (stat.ME)",
        "abstract": "We study the problem of clustering ranking vectors, where each vector represents preferences as an ordered list of distinct integers. Specifically, we focus on the k-centroids ranking vectors clustering problem (KRC), which aims to partition a set of ranking vectors into k clusters and identify the centroid of each cluster. Unlike classical k-means clustering (KMC), KRC constrains both the observations and centroids to be ranking vectors. We establish the NP-hardness of KRC and characterize its feasible set. For the single-cluster case, we derive a closed-form analytical solution for the optimal centroid, which can be computed in linear time. To address the computational challenges of KRC, we develop an efficient approximation algorithm, KRCA, which iteratively refines initial solutions from KMC, referred to as the baseline solution. Additionally, we introduce a branch-and-bound (BnB) algorithm for efficient cluster reconstruction within KRCA, leveraging a decision tree framework to reduce computational time while incorporating a controlling parameter to balance solution quality and efficiency. We establish theoretical error bounds for KRCA and BnB. Through extensive numerical experiments on synthetic and real-world datasets, we demonstrate that KRCA consistently outperforms baseline solutions, delivering significant improvements in solution quality with fast computational times. This work highlights the practical significance of KRC for personalization and large-scale decision making, offering methodological advancements and insights that can be built upon in future studies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 7,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12584",
        "abs_url": "https://arxiv.org/abs/2507.12584",
        "pdf_url": "https://arxiv.org/pdf/2507.12584",
        "title": "Second-Order Bounds for [0,1]-Valued Regression via Betting Loss",
        "authors": [
            "Yinan Li",
            "Kwang-Sung Jun"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We consider the $[0,1]$-valued regression problem in the i.i.d. setting. In a related problem called cost-sensitive classification, \\citet{foster21efficient} have shown that the log loss minimizer achieves an improved generalization bound compared to that of the squared loss minimizer in the sense that the bound scales with the cost of the best classifier, which can be arbitrarily small depending on the problem at hand. Such a result is often called a first-order bound. For $[0,1]$-valued regression, we first show that the log loss minimizer leads to a similar first-order bound. We then ask if there exists a loss function that achieves a variance-dependent bound (also known as a second order bound), which is a strict improvement upon first-order bounds. We answer this question in the affirmative by proposing a novel loss function called the betting loss. Our result is ``variance-adaptive'' in the sense that the bound is attained \\textit{without any knowledge about the variance}, which is in contrast to modeling label (or reward) variance or the label distribution itself explicitly as part of the function class such as distributional reinforcement learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 8,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12604",
        "abs_url": "https://arxiv.org/abs/2507.12604",
        "pdf_url": "https://arxiv.org/pdf/2507.12604",
        "title": "Are encoders able to learn landmarkers for warm-starting of Hyperparameter Optimization?",
        "authors": [
            "Antoni Zajko",
            "Katarzyna Woźnica"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Effectively representing heterogeneous tabular datasets for meta-learning purposes is still an open problem. Previous approaches rely on representations that are intended to be universal. This paper proposes two novel methods for tabular representation learning tailored to a specific meta-task - warm-starting Bayesian Hyperparameter Optimization. Both follow the specific requirement formulated by ourselves that enforces representations to capture the properties of landmarkers. The first approach involves deep metric learning, while the second one is based on landmarkers reconstruction. We evaluate the proposed encoders in two ways. Next to the gain in the target meta-task, we also use the degree of fulfillment of the proposed requirement as the evaluation metric. Experiments demonstrate that while the proposed encoders can effectively learn representations aligned with landmarkers, they may not directly translate to significant performance gains in the meta-task of HPO warm-starting.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 9,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12612",
        "abs_url": "https://arxiv.org/abs/2507.12612",
        "pdf_url": "https://arxiv.org/pdf/2507.12612",
        "title": "Learning What Matters: Probabilistic Task Selection via Mutual Information for Model Finetuning",
        "authors": [
            "Prateek Chanda",
            "Saral Sureka",
            "Parth Pratim Chatterjee",
            "Krishnateja Killamsetty",
            "Nikhil Shivakumar Nayak",
            "Ganesh Ramakrishnan"
        ],
        "comments": "9, 8 tables, 7 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "The performance of finetuned large language models (LLMs) hinges critically on the composition of the training mixture. However, selecting an optimal blend of task datasets remains a largely manual, heuristic driven process, with practitioners often relying on uniform or size based sampling strategies. We introduce TASKPGM, a principled and scalable framework for mixture optimization that selects continuous task proportions by minimizing an energy function over a Markov Random Field (MRF). Task relationships are modeled using behavioral divergences such as Jensen Shannon Divergence and Pointwise Mutual Information computed from the predictive distributions of single task finetuned models. Our method yields a closed form solution under simplex constraints and provably balances representativeness and diversity among tasks. We provide theoretical guarantees, including weak submodularity for budgeted variants, and demonstrate consistent empirical improvements on Llama 2 and Mistral across evaluation suites such as MMLU and BIGBench. Beyond performance, TASKPGM offers interpretable insights into task influence and mixture composition, making it a powerful tool for efficient and robust LLM finetuning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 10,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12619",
        "abs_url": "https://arxiv.org/abs/2507.12619",
        "pdf_url": "https://arxiv.org/pdf/2507.12619",
        "title": "BootSeer: Analyzing and Mitigating Initialization Bottlenecks in Large-Scale LLM Training",
        "authors": [
            "Rui Li",
            "Xiaoyun Zhi",
            "Jinxin Chi",
            "Menghan Yu",
            "Lixin Huang",
            "Jia Zhu",
            "Weilun Zhang",
            "Xing Ma",
            "Wenjia Liu",
            "Zhicheng Zhu",
            "Daowen Luo",
            "Zuquan Song",
            "Xin Yin",
            "Chao Xiang",
            "Shuguang Wang",
            "Wencong Xiao",
            "Gene Cooperman"
        ],
        "comments": "18 pages, 14 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)",
        "abstract": "Large Language Models (LLMs) have become a cornerstone of modern AI, driving breakthroughs in natural language processing and expanding into multimodal jobs involving images, audio, and video. As with most computational software, it is important to distinguish between ordinary runtime performance and startup overhead. Prior research has focused on runtime performance: improving training efficiency and stability. This work focuses instead on the increasingly critical issue of startup overhead in training: the delay before training jobs begin execution. Startup overhead is particularly important in large, industrial-scale LLMs, where failures occur more frequently and multiple teams operate in iterative update-debug cycles. In one of our training clusters, more than 3.5% of GPU time is wasted due to startup overhead alone. In this work, we present the first in-depth characterization of LLM training startup overhead based on real production data. We analyze the components of startup cost, quantify its direct impact, and examine how it scales with job size. These insights motivate the design of Bootseer, a system-level optimization framework that addresses three primary startup bottlenecks: (a) container image loading, (b) runtime dependency installation, and (c) model checkpoint resumption. To mitigate these bottlenecks, Bootseer introduces three techniques: (a) hot block record-and-prefetch, (b) dependency snapshotting, and (c) striped HDFS-FUSE. Bootseer has been deployed in a production environment and evaluated on real LLM training workloads, demonstrating a 50% reduction in startup overhead.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 11,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12638",
        "abs_url": "https://arxiv.org/abs/2507.12638",
        "pdf_url": "https://arxiv.org/pdf/2507.12638",
        "title": "Reasoning-Finetuning Repurposes Latent Representations in Base Models",
        "authors": [
            "Jake Ward",
            "Chuqiao Lin",
            "Constantin Venhoff",
            "Neel Nanda"
        ],
        "comments": "6 pages, 6 figures. ICML 2025 Workshop on Actionable Interpretability",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Backtracking, an emergent behavior elicited by reasoning fine-tuning, has been shown to be a key mechanism in reasoning models' enhanced capabilities. Prior work has succeeded in manipulating this behavior via steering vectors, but the underlying mechanism remains poorly understood. In this work, we show that the emergence of backtracking in DeepSeek-R1-Distill-Llama-8B is in part driven by a repurposed direction already present in base model activations. Specifically, we identify a direction in base Llama-3.1-8B's residual stream which systematically induces backtracking when used to steer the distilled reasoning model, and find that the effects of steering with this direction cannot be trivially explained by token-level attributes. We further find that this direction does not induce backtracking in the base model, suggesting that the reasoning finetuning process repurposes pre-existing representations to form new behavioral circuits. Additionally, we hypothesize that this direction is one of several which may work together to mediate backtracking. Our findings offer a compelling picture that reasoning-finetuned models repurpose pre-existing base model representations, rather than learn new capabilities from scratch.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 12,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12652",
        "abs_url": "https://arxiv.org/abs/2507.12652",
        "pdf_url": "https://arxiv.org/pdf/2507.12652",
        "title": "Federated Learning in Open- and Closed-Loop EMG Decoding: A Privacy and Performance Perspective",
        "authors": [
            "Kai Malcolm",
            "César Uribe",
            "Momona Yamagami"
        ],
        "comments": "23 pages, 7 figures",
        "subjects": "Machine Learning (cs.LG); Cryptography and Security (cs.CR); Human-Computer Interaction (cs.HC)",
        "abstract": "Invasive and non-invasive neural interfaces hold promise as high-bandwidth input devices for next-generation technologies. However, neural signals inherently encode sensitive information about an individual's identity and health, making data sharing for decoder training a critical privacy challenge. Federated learning (FL), a distributed, privacy-preserving learning framework, presents a promising solution, but it remains unexplored in closed-loop adaptive neural interfaces. Here, we introduce FL-based neural decoding and systematically evaluate its performance and privacy using high-dimensional electromyography signals in both open- and closed-loop scenarios. In open-loop simulations, FL significantly outperformed local learning baselines, demonstrating its potential for high-performance, privacy-conscious neural decoding. In contrast, closed-loop user studies required adapting FL methods to accommodate single-user, real-time interactions, a scenario not supported by standard FL. This modification resulted in local learning decoders surpassing the adapted FL approach in closed-loop performance, yet local learning still carried higher privacy risks. Our findings highlight a critical performance-privacy tradeoff in real-time adaptive applications and indicate the need for FL methods specifically designed for co-adaptive, single-user applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 13,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12659",
        "abs_url": "https://arxiv.org/abs/2507.12659",
        "pdf_url": "https://arxiv.org/pdf/2507.12659",
        "title": "Improving physics-informed neural network extrapolation via transfer learning and adaptive activation functions",
        "authors": [
            "Athanasios Papastathopoulos-Katsaros",
            "Alexandra Stavrianidi",
            "Zhandong Liu"
        ],
        "comments": "18 pages, 16 figures, 7 tables Accepted to ICANN 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Dynamical Systems (math.DS); Numerical Analysis (math.NA); Machine Learning (stat.ML)",
        "abstract": "Physics-Informed Neural Networks (PINNs) are deep learning models that incorporate the governing physical laws of a system into the learning process, making them well-suited for solving complex scientific and engineering problems. Recently, PINNs have gained widespread attention as a powerful framework for combining physical principles with data-driven modeling to improve prediction accuracy. Despite their successes, however, PINNs often exhibit poor extrapolation performance outside the training domain and are highly sensitive to the choice of activation functions (AFs). In this paper, we introduce a transfer learning (TL) method to improve the extrapolation capability of PINNs. Our approach applies transfer learning (TL) within an extended training domain, using only a small number of carefully selected collocation points. Additionally, we propose an adaptive AF that takes the form of a linear combination of standard AFs, which improves both the robustness and accuracy of the model. Through a series of experiments, we demonstrate that our method achieves an average of 40% reduction in relative L2 error and an average of 50% reduction in mean absolute error in the extrapolation domain, all without a significant increase in computational cost. The code is available at this https URL .",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 14,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12677",
        "abs_url": "https://arxiv.org/abs/2507.12677",
        "pdf_url": "https://arxiv.org/pdf/2507.12677",
        "title": "Data Transformation Strategies to Remove Heterogeneity",
        "authors": [
            "Sangbong Yoo",
            "Jaeyoung Lee",
            "Chanyoung Yoon",
            "Geonyeong Son",
            "Hyein Hong",
            "Seongbum Seo",
            "Soobin Yim",
            "Chanyoung Jung",
            "Jungsoo Park",
            "Misuk Kim",
            "Yun Jang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Data heterogeneity is a prevalent issue, stemming from various conflicting factors, making its utilization complex. This uncertainty, particularly resulting from disparities in data formats, frequently necessitates the involvement of experts to find resolutions. Current methodologies primarily address conflicts related to data structures and schemas, often overlooking the pivotal role played by data transformation. As the utilization of artificial intelligence (AI) continues to expand, there is a growing demand for a more streamlined data preparation process, and data transformation becomes paramount. It customizes training data to enhance AI learning efficiency and adapts input formats to suit diverse AI models. Selecting an appropriate transformation technique is paramount in preserving crucial data details. Despite the widespread integration of AI across various industries, comprehensive reviews concerning contemporary data transformation approaches are scarce. This survey explores the intricacies of data heterogeneity and its underlying sources. It systematically categorizes and presents strategies to address heterogeneity stemming from differences in data formats, shedding light on the inherent challenges associated with each strategy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 15,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12704",
        "abs_url": "https://arxiv.org/abs/2507.12704",
        "pdf_url": "https://arxiv.org/pdf/2507.12704",
        "title": "PinFM: Foundation Model for User Activity Sequences at a Billion-scale Visual Discovery Platform",
        "authors": [
            "Xiangyi Chen",
            "Kousik Rajesh",
            "Matthew Lawhon",
            "Zelun Wang",
            "Hanyu Li",
            "Haomiao Li",
            "Saurabh Vishwas Joshi",
            "Pong Eksombatchai",
            "Jaewon Yang",
            "Yi-Ping Hsu",
            "Jiajing Xu",
            "Charles Rosenberg"
        ],
        "comments": "RecSys 2025",
        "subjects": "Machine Learning (cs.LG); Information Retrieval (cs.IR)",
        "abstract": "User activity sequences have emerged as one of the most important signals in recommender systems. We present a foundational model, PinFM, for understanding user activity sequences across multiple applications at a billion-scale visual discovery platform. We pretrain a transformer model with 20B+ parameters using extensive user activity data, then fine-tune it for specific applications, efficiently coupling it with existing models. While this pretraining-and-fine-tuning approach has been popular in other domains, such as Vision and NLP, its application in industrial recommender systems presents numerous challenges. The foundational model must be scalable enough to score millions of items every second while meeting tight cost and latency constraints imposed by these systems. Additionally, it should capture the interactions between user activities and other features and handle new items that were not present during the pretraining stage. We developed innovative techniques to address these challenges. Our infrastructure and algorithmic optimizations, such as the Deduplicated Cross-Attention Transformer (DCAT), improved our throughput by 600% on Pinterest internal data. We demonstrate that PinFM can learn interactions between user sequences and candidate items by altering input sequences, leading to a 20% increase in engagement with new items. PinFM is now deployed to help improve the experience of more than a half billion users across various applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 16,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12709",
        "abs_url": "https://arxiv.org/abs/2507.12709",
        "pdf_url": "https://arxiv.org/pdf/2507.12709",
        "title": "From SGD to Spectra: A Theory of Neural Network Weight Dynamics",
        "authors": [
            "Brian Richard Olsen",
            "Sam Fatehmanesh",
            "Frank Xiao",
            "Adarsh Kumarappan",
            "Anirudh Gajula"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Deep neural networks have revolutionized machine learning, yet their training dynamics remain theoretically unclear-we develop a continuous-time, matrix-valued stochastic differential equation (SDE) framework that rigorously connects the microscopic dynamics of SGD to the macroscopic evolution of singular-value spectra in weight matrices. We derive exact SDEs showing that squared singular values follow Dyson Brownian motion with eigenvalue repulsion, and characterize stationary distributions as gamma-type densities with power-law tails, providing the first theoretical explanation for the empirically observed 'bulk+tail' spectral structure in trained networks. Through controlled experiments on transformer and MLP architectures, we validate our theoretical predictions and demonstrate quantitative agreement between SDE-based forecasts and observed spectral evolution, providing a rigorous foundation for understanding why deep learning works.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 17,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12750",
        "abs_url": "https://arxiv.org/abs/2507.12750",
        "pdf_url": "https://arxiv.org/pdf/2507.12750",
        "title": "Multimodal-Guided Dynamic Dataset Pruning for Robust and Efficient Data-Centric Learning",
        "authors": [
            "Suorong Yang",
            "Peijia Li",
            "Yujie Liu",
            "Zhiming Xu",
            "Peng Ye",
            "Wanli Ouyang",
            "Furao Shen",
            "Dongzhan Zhou"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Modern deep models are trained on large real-world datasets, where data quality varies and redundancy is common. Data-centric approaches such as dataset pruning have shown promise in improving training efficiency and model performance. However, most existing methods rely on static heuristics or task-specific metrics, limiting their robustness and generalizability across domains. In this work, we introduce a dynamic dataset pruning framework that adaptively selects training samples based on both task-driven difficulty and cross-modality semantic consistency. By incorporating supervision from pretrained multimodal foundation models, our approach captures training dynamics while effectively filtering out uninformative samples. Our work highlights the potential of integrating cross-modality alignment for robust sample selection, advancing data-centric learning toward more efficient and robust practices across application domains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 18,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12766",
        "abs_url": "https://arxiv.org/abs/2507.12766",
        "pdf_url": "https://arxiv.org/pdf/2507.12766",
        "title": "Layer Separation Deep Learning Model with Auxiliary Variables for Partial Differential Equations",
        "authors": [
            "Yaru Liu",
            "Yiqi Gu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In this paper, we propose a new optimization framework, the layer separation (LySep) model, to improve the deep learning-based methods in solving partial differential equations. Due to the highly non-convex nature of the loss function in deep learning, existing optimization algorithms often converge to suboptimal local minima or suffer from gradient explosion or vanishing, resulting in poor performance. To address these issues, we introduce auxiliary variables to separate the layers of deep neural networks. Specifically, the output and its derivatives of each layer are represented by auxiliary variables, effectively decomposing the deep architecture into a series of shallow architectures. New loss functions with auxiliary variables are established, in which only variables from two neighboring layers are coupled. Corresponding algorithms based on alternating directions are developed, where many variables can be updated optimally in closed forms. Moreover, we provide theoretical analyses demonstrating the consistency between the LySep model and the original deep model. High-dimensional numerical results validate our theory and demonstrate the advantages of LySep in minimizing loss and reducing solution error.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 19,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12774",
        "abs_url": "https://arxiv.org/abs/2507.12774",
        "pdf_url": "https://arxiv.org/pdf/2507.12774",
        "title": "A Comprehensive Survey of Electronic Health Record Modeling: From Deep Learning Approaches to Large Language Models",
        "authors": [
            "Weijieying Ren",
            "Jingxi Zhu",
            "Zehao Liu",
            "Tianxiang Zhao",
            "Vasant Honavar"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Artificial intelligence (AI) has demonstrated significant potential in transforming healthcare through the analysis and modeling of electronic health records (EHRs). However, the inherent heterogeneity, temporal irregularity, and domain-specific nature of EHR data present unique challenges that differ fundamentally from those in vision and natural language tasks. This survey offers a comprehensive overview of recent advancements at the intersection of deep learning, large language models (LLMs), and EHR modeling. We introduce a unified taxonomy that spans five key design dimensions: data-centric approaches, neural architecture design, learning-focused strategies, multimodal learning, and LLM-based modeling systems. Within each dimension, we review representative methods addressing data quality enhancement, structural and temporal representation, self-supervised learning, and integration with clinical knowledge. We further highlight emerging trends such as foundation models, LLM-driven clinical agents, and EHR-to-text translation for downstream reasoning. Finally, we discuss open challenges in benchmarking, explainability, clinical alignment, and generalization across diverse clinical settings. This survey aims to provide a structured roadmap for advancing AI-driven EHR modeling and clinical decision support. For a comprehensive list of EHR-related methods, kindly refer to this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 20,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12787",
        "abs_url": "https://arxiv.org/abs/2507.12787",
        "pdf_url": "https://arxiv.org/pdf/2507.12787",
        "title": "Multi-Channel Graph Neural Network for Financial Risk Prediction of NEEQ Enterprises",
        "authors": [
            "Jianyu Zhu"
        ],
        "comments": "10 pages, 4 figures. Submitted for conference review",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "With the continuous evolution of China's multi-level capital market, the National Equities Exchange and Quotations (NEEQ), also known as the \"New Third Board,\" has become a critical financing platform for small and medium-sized enterprises (SMEs). However, due to their limited scale and financial resilience, many NEEQ-listed companies face elevated risks of financial distress. To address this issue, we propose a multi-channel deep learning framework that integrates structured financial indicators, textual disclosures, and enterprise relationship data for comprehensive financial risk prediction. Specifically, we design a Triple-Channel Graph Isomorphism Network (GIN) that processes numeric, textual, and graph-based inputs separately. These modality-specific representations are fused using an attention-based mechanism followed by a gating unit to enhance robustness and prediction accuracy. Experimental results on data from 7,731 real-world NEEQ companies demonstrate that our model significantly outperforms traditional machine learning methods and single-modality baselines in terms of AUC, Precision, Recall, and F1 Score. This work provides theoretical and practical insights into risk modeling for SMEs and offers a data-driven tool to support financial regulators and investors.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 21,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12803",
        "abs_url": "https://arxiv.org/abs/2507.12803",
        "pdf_url": "https://arxiv.org/pdf/2507.12803",
        "title": "FLDmamba: Integrating Fourier and Laplace Transform Decomposition with Mamba for Enhanced Time Series Prediction",
        "authors": [
            "Qianru Zhang",
            "Chenglei Yu",
            "Haixin Wang",
            "Yudong Yan",
            "Yuansheng Cao",
            "Siu-Ming Yiu",
            "Tailin Wu",
            "Hongzhi Yin"
        ],
        "comments": "12 pages",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Time series prediction, a crucial task across various domains, faces significant challenges due to the inherent complexities of time series data, including non-stationarity, multi-scale periodicity, and transient dynamics, particularly when tackling long-term predictions. While Transformer-based architectures have shown promise, their quadratic complexity with sequence length hinders their efficiency for long-term predictions. Recent advancements in State-Space Models, such as Mamba, offer a more efficient alternative for long-term modeling, but they cannot capture multi-scale periodicity and transient dynamics effectively. Meanwhile, they are susceptible to data noise issues in time series. This paper proposes a novel framework, FLDmamba (Fourier and Laplace Transform Decomposition Mamba), addressing these limitations. FLDmamba leverages the strengths of both Fourier and Laplace transforms to effectively capture both multi-scale periodicity, transient dynamics within time series data, and improve the robustness of the model to the data noise issue. Our extensive experiments demonstrate that FLDmamba achieves superior performance on time series prediction benchmarks, outperforming both Transformer-based and other Mamba-based architectures. To promote the reproducibility of our method, we have made both the code and data accessible via the following URL:{\\href{this https URL}{this https URL\\model}.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 22,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12805",
        "abs_url": "https://arxiv.org/abs/2507.12805",
        "pdf_url": "https://arxiv.org/pdf/2507.12805",
        "title": "PMKLC: Parallel Multi-Knowledge Learning-based Lossless Compression for Large-Scale Genomics Database",
        "authors": [
            "Hui Sun",
            "Yanfeng Ding",
            "Liping Yi",
            "Huidong Ma",
            "Gang Wang",
            "Xiaoguang Liu",
            "Cheng Zhong",
            "Wentong Cai"
        ],
        "comments": "Accepted via KDD-25",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Databases (cs.DB)",
        "abstract": "Learning-based lossless compressors play a crucial role in large-scale genomic database backup, storage, transmission, and management. However, their 1) inadequate compression ratio, 2) low compression \\& decompression throughput, and 3) poor compression robustness limit their widespread adoption and application in both industry and academia. To solve those challenges, we propose a novel \\underline{P}arallel \\underline{M}ulti-\\underline{K}nowledge \\underline{L}earning-based \\underline{C}ompressor (PMKLC) with four crucial designs: 1) We propose an automated multi-knowledge learning-based compression framework as compressors' backbone to enhance compression ratio and robustness; 2) we design a GPU-accelerated ($s$,$k$)-mer encoder to optimize compression throughput and computing resource usage; 3) we introduce data block partitioning and Step-wise Model Passing (SMP) mechanisms for parallel acceleration; 4) We design two compression modes PMKLC-S and PMKLC-M to meet the complex application scenarios, where the former runs on a resource-constrained single GPU and the latter is multi-GPU accelerated. We benchmark PMKLC-S/M and 14 baselines (7 traditional and 7 leaning-based) on 15 real-world datasets with different species and data sizes. Compared to baselines on the testing datasets, PMKLC-S/M achieve the average compression ratio improvement up to 73.609\\% and 73.480\\%, the average throughput improvement up to 3.036$\\times$ and 10.710$\\times$, respectively. Besides, PMKLC-S/M also achieve the best robustness and competitive memory cost, indicating its greater stability against datasets with different probability distribution perturbations, and its strong ability to run on memory-constrained devices.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 23,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12814",
        "abs_url": "https://arxiv.org/abs/2507.12814",
        "pdf_url": "https://arxiv.org/pdf/2507.12814",
        "title": "RONOM: Reduced-Order Neural Operator Modeling",
        "authors": [
            "Sven Dummer",
            "Dongwei Ye",
            "Christoph Brune"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computational Engineering, Finance, and Science (cs.CE); Numerical Analysis (math.NA)",
        "abstract": "Time-dependent partial differential equations are ubiquitous in physics-based modeling, but they remain computationally intensive in many-query scenarios, such as real-time forecasting, optimal control, and uncertainty quantification. Reduced-order modeling (ROM) addresses these challenges by constructing a low-dimensional surrogate model but relies on a fixed discretization, which limits flexibility across varying meshes during evaluation. Operator learning approaches, such as neural operators, offer an alternative by parameterizing mappings between infinite-dimensional function spaces, enabling adaptation to data across different resolutions. Whereas ROM provides rigorous numerical error estimates, neural operator learning largely focuses on discretization convergence and invariance without quantifying the error between the infinite-dimensional and the discretized operators. This work introduces the reduced-order neural operator modeling (RONOM) framework, which bridges concepts from ROM and operator learning. We establish a discretization error bound analogous to those in ROM, and get insights into RONOM's discretization convergence and discretization robustness. Moreover, two numerical examples are presented that compare RONOM to existing neural operators for solving partial differential equations. The results demonstrate that RONOM using standard vector-to-vector neural networks achieves comparable performance in input generalization and superior performance in both spatial super-resolution and discretization robustness, while also offering novel insights into temporal super-resolution scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 24,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12815",
        "abs_url": "https://arxiv.org/abs/2507.12815",
        "pdf_url": "https://arxiv.org/pdf/2507.12815",
        "title": "From Novelty to Imitation: Self-Distilled Rewards for Offline Reinforcement Learning",
        "authors": [
            "Gaurav Chaudhary",
            "Laxmidhar Behera"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Offline Reinforcement Learning (RL) aims to learn effective policies from a static dataset without requiring further agent-environment interactions. However, its practical adoption is often hindered by the need for explicit reward annotations, which can be costly to engineer or difficult to obtain retrospectively. To address this, we propose ReLOAD (Reinforcement Learning with Offline Reward Annotation via Distillation), a novel reward annotation framework for offline RL. Unlike existing methods that depend on complex alignment procedures, our approach adapts Random Network Distillation (RND) to generate intrinsic rewards from expert demonstrations using a simple yet effective embedding discrepancy measure. First, we train a predictor network to mimic a fixed target network's embeddings based on expert state transitions. Later, the prediction error between these networks serves as a reward signal for each transition in the static dataset. This mechanism provides a structured reward signal without requiring handcrafted reward annotations. We provide a formal theoretical construct that offers insights into how RND prediction errors effectively serve as intrinsic rewards by distinguishing expert-like transitions. Experiments on the D4RL benchmark demonstrate that ReLOAD enables robust offline policy learning and achieves performance competitive with traditional reward-annotated methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 25,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12837",
        "abs_url": "https://arxiv.org/abs/2507.12837",
        "pdf_url": "https://arxiv.org/pdf/2507.12837",
        "title": "Understanding the Evolution of the Neural Tangent Kernel at the Edge of Stability",
        "authors": [
            "Kaiqi Jiang",
            "Jeremy Cohen",
            "Yuanzhi Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The study of Neural Tangent Kernels (NTKs) in deep learning has drawn increasing attention in recent years. NTKs typically actively change during training and are related to feature learning. In parallel, recent work on Gradient Descent (GD) has found a phenomenon called Edge of Stability (EoS), in which the largest eigenvalue of the NTK oscillates around a value inversely proportional to the step size. However, although follow-up works have explored the underlying mechanism of such eigenvalue behavior in depth, the understanding of the behavior of the NTK eigenvectors during EoS is still missing. This paper examines the dynamics of NTK eigenvectors during EoS in detail. Across different architectures, we observe that larger learning rates cause the leading eigenvectors of the final NTK, as well as the full NTK matrix, to have greater alignment with the training target. We then study the underlying mechanism of this phenomenon and provide a theoretical analysis for a two-layer linear network. Our study enhances the understanding of GD training dynamics in deep learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 26,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12843",
        "abs_url": "https://arxiv.org/abs/2507.12843",
        "pdf_url": "https://arxiv.org/pdf/2507.12843",
        "title": "A Kernel Distribution Closeness Testing",
        "authors": [
            "Zhijian Zhou",
            "Liuhua Peng",
            "Xunye Tian",
            "Feng Liu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "The distribution closeness testing (DCT) assesses whether the distance between a distribution pair is at least $\\epsilon$-far. Existing DCT methods mainly measure discrepancies between a distribution pair defined on discrete one-dimensional spaces (e.g., using total variation), which limits their applications to complex data (e.g., images). To extend DCT to more types of data, a natural idea is to introduce maximum mean discrepancy (MMD), a powerful measurement of the distributional discrepancy between two complex distributions, into DCT scenarios. However, we find that MMD's value can be the same for many pairs of distributions that have different norms in the same reproducing kernel Hilbert space (RKHS), making MMD less informative when assessing the closeness levels for multiple distribution pairs. To mitigate the issue, we design a new measurement of distributional discrepancy, norm-adaptive MMD (NAMMD), which scales MMD's value using the RKHS norms of distributions. Based on the asymptotic distribution of NAMMD, we finally propose the NAMMD-based DCT to assess the closeness levels of a distribution pair. Theoretically, we prove that NAMMD-based DCT has higher test power compared to MMD-based DCT, with bounded type-I error, which is also validated by extensive experiments on many types of data (e.g., synthetic noise, real images). Furthermore, we also apply the proposed NAMMD for addressing the two-sample testing problem and find NAMMD-based two-sample test has higher test power than the MMD-based two-sample test in both theory and experiments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 27,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12854",
        "abs_url": "https://arxiv.org/abs/2507.12854",
        "pdf_url": "https://arxiv.org/pdf/2507.12854",
        "title": "Transformer-Based Person Identification via Wi-Fi CSI Amplitude and Phase Perturbations",
        "authors": [
            "Danilo Avola",
            "Andrea Bernardini",
            "Francesco Danese",
            "Mario Lezoche",
            "Maurizio Mancini",
            "Daniele Pannone",
            "Amedeo Ranaldi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Wi-Fi sensing is gaining momentum as a non-intrusive and privacy-preserving alternative to vision-based systems for human identification. However, person identification through wireless signals, particularly without user motion, remains largely unexplored. Most prior wireless-based approaches rely on movement patterns, such as walking gait, to extract biometric cues. In contrast, we propose a transformer-based method that identifies individuals from Channel State Information (CSI) recorded while the subject remains stationary. CSI captures fine-grained amplitude and phase distortions induced by the unique interaction between the human body and the radio signal. To support evaluation, we introduce a dataset acquired with ESP32 devices in a controlled indoor environment, featuring six participants observed across multiple orientations. A tailored preprocessing pipeline, including outlier removal, smoothing, and phase calibration, enhances signal quality. Our dual-branch transformer architecture processes amplitude and phase modalities separately and achieves 99.82\\% classification accuracy, outperforming convolutional and multilayer perceptron baselines. These results demonstrate the discriminative potential of CSI perturbations, highlighting their capacity to encode biometric traits in a consistent manner. They further confirm the viability of passive, device-free person identification using low-cost commodity Wi-Fi hardware in real-world settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 28,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12856",
        "abs_url": "https://arxiv.org/abs/2507.12856",
        "pdf_url": "https://arxiv.org/pdf/2507.12856",
        "title": "Supervised Fine Tuning on Curated Data is Reinforcement Learning (and can be improved)",
        "authors": [
            "Chongli Qin",
            "Jost Tobias Springenberg"
        ],
        "comments": "See project website for details and code at: this https URL",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Behavior Cloning (BC) on curated (or filtered) data is the predominant paradigm for supervised fine-tuning (SFT) of large language models; as well as for imitation learning of control policies. Here, we draw on a connection between this successful strategy and the theory and practice of finding optimal policies via Reinforcement Learning (RL). Building on existing literature, we clarify that SFT can be understood as maximizing a lower bound on the RL objective in a sparse reward setting. Giving support to its often observed good performance. From this viewpoint, we realize that a small modification to SFT leads to an importance weighted variant that behaves closer to training with RL as it: i) optimizes a tighter bound to the RL objective and, ii) can improve performance compared to SFT on curated data. We refer to this variant as importance weighted supervised fine-tuning (iw-SFT). We show that it is easy to implement and can be further generalized to training with quality scored data. The resulting SFT variants are competitive with more advanced RL algorithms for large language models and for training policies in continuous control tasks. For example achieving 66.7% on the AIME 2024 dataset.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 29,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12873",
        "abs_url": "https://arxiv.org/abs/2507.12873",
        "pdf_url": "https://arxiv.org/pdf/2507.12873",
        "title": "An Investigation of Ear-EEG Signals for a Novel Biometric Authentication System",
        "authors": [
            "Danilo Avola",
            "Giancarlo Crocetti",
            "Gian Luca Foresti",
            "Daniele Pannone",
            "Claudio Piciarelli",
            "Amedeo Ranaldi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "This work explores the feasibility of biometric authentication using EEG signals acquired through in-ear devices, commonly referred to as ear-EEG. Traditional EEG-based biometric systems, while secure, often suffer from low usability due to cumbersome scalp-based electrode setups. In this study, we propose a novel and practical framework leveraging ear-EEG signals as a user-friendly alternative for everyday biometric authentication. The system extracts an original combination of temporal and spectral features from ear-EEG signals and feeds them into a fully connected deep neural network for subject identification. Experimental results on the only currently available ear-EEG dataset suitable for different purposes, including biometric authentication, demonstrate promising performance, with an average accuracy of 82\\% in a subject identification scenario. These findings confirm the potential of ear-EEG as a viable and deployable direction for next-generation real-world biometric systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 30,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12874",
        "abs_url": "https://arxiv.org/abs/2507.12874",
        "pdf_url": "https://arxiv.org/pdf/2507.12874",
        "title": "Topology-Aware Activation Functions in Neural Networks",
        "authors": [
            "Pavel Snopov",
            "Oleg R. Musin"
        ],
        "comments": "Accepted to ESANN 2025. Published in the ESANN 2025 proceedings",
        "subjects": "Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)",
        "abstract": "This study explores novel activation functions that enhance the ability of neural networks to manipulate data topology during training. Building on the limitations of traditional activation functions like $\\mathrm{ReLU}$, we propose $\\mathrm{SmoothSplit}$ and $\\mathrm{ParametricSplit}$, which introduce topology \"cutting\" capabilities. These functions enable networks to transform complex data manifolds effectively, improving performance in scenarios with low-dimensional layers. Through experiments on synthetic and real-world datasets, we demonstrate that $\\mathrm{ParametricSplit}$ outperforms traditional activations in low-dimensional settings while maintaining competitive performance in higher-dimensional ones. Our findings highlight the potential of topology-aware activation functions in advancing neural network architectures. The code is available via this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 31,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12898",
        "abs_url": "https://arxiv.org/abs/2507.12898",
        "pdf_url": "https://arxiv.org/pdf/2507.12898",
        "title": "Generalist Bimanual Manipulation via Foundation Video Diffusion Models",
        "authors": [
            "Yao Feng",
            "Hengkai Tan",
            "Xinyi Mao",
            "Guodong Liu",
            "Shuhe Huang",
            "Chendong Xiang",
            "Hang Su",
            "Jun Zhu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Robotics (cs.RO)",
        "abstract": "Bimanual robotic manipulation, which involves the coordinated control of two robotic arms, is foundational for solving challenging tasks. Despite recent progress in general-purpose manipulation, data scarcity and embodiment heterogeneity remain serious obstacles to further scaling up in bimanual settings. In this paper, we introduce VIdeo Diffusion for Action Reasoning (VIDAR), a two-stage framework that leverages large-scale, diffusion-based video pre-training and a novel masked inverse dynamics model for action prediction. We pre-train the video diffusion model on 750K multi-view videos from three real-world bimanual robot platforms, utilizing a unified observation space that encodes robot, camera, task, and scene contexts. Our masked inverse dynamics model learns masks to extract action-relevant information from generated trajectories without requiring pixel-level labels, and the masks can effectively generalize to unseen backgrounds. Our experiments demonstrate that with only 20 minutes of human demonstrations on an unseen robot platform (only 1% of typical data requirements), VIDAR generalizes to unseen tasks and backgrounds with strong semantic understanding, surpassing state-of-the-art methods. Our findings highlight the potential of video foundation models, coupled with masked action prediction, to enable scalable and generalizable robotic manipulation in diverse real-world settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 32,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12900",
        "abs_url": "https://arxiv.org/abs/2507.12900",
        "pdf_url": "https://arxiv.org/pdf/2507.12900",
        "title": "Learning to Reject Low-Quality Explanations via User Feedback",
        "authors": [
            "Luca Stradiotti",
            "Dario Pesenti",
            "Stefano Teso",
            "Jesse Davis"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Machine Learning predictors are increasingly being employed in high-stakes applications such as credit scoring. Explanations help users unpack the reasons behind their predictions, but are not always \"high quality''. That is, end-users may have difficulty interpreting or believing them, which can complicate trust assessment and downstream decision-making. We argue that classifiers should have the option to refuse handling inputs whose predictions cannot be explained properly and introduce a framework for learning to reject low-quality explanations (LtX) in which predictors are equipped with a rejector that evaluates the quality of explanations. In this problem setting, the key challenges are how to properly define and assess explanation quality and how to design a suitable rejector. Focusing on popular attribution techniques, we introduce ULER (User-centric Low-quality Explanation Rejector), which learns a simple rejector from human ratings and per-feature relevance judgments to mirror human judgments of explanation quality. Our experiments show that ULER outperforms both state-of-the-art and explanation-aware learning to reject strategies at LtX on eight classification and regression benchmarks and on a new human-annotated dataset, which we will publicly release to support future research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 33,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12908",
        "abs_url": "https://arxiv.org/abs/2507.12908",
        "pdf_url": "https://arxiv.org/pdf/2507.12908",
        "title": "Fremer: Lightweight and Effective Frequency Transformer for Workload Forecasting in Cloud Services",
        "authors": [
            "Jiadong Chen",
            "Hengyu Ye",
            "Fuxin Jiang",
            "Xiao He",
            "Tieying Zhang",
            "Jianjun Chen",
            "Xiaofeng Gao"
        ],
        "comments": "12 pages, 11 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Workload forecasting is pivotal in cloud service applications, such as auto-scaling and scheduling, with profound implications for operational efficiency. Although Transformer-based forecasting models have demonstrated remarkable success in general tasks, their computational efficiency often falls short of the stringent requirements in large-scale cloud environments. Given that most workload series exhibit complicated periodic patterns, addressing these challenges in the frequency domain offers substantial advantages. To this end, we propose Fremer, an efficient and effective deep forecasting model. Fremer fulfills three critical requirements: it demonstrates superior efficiency, outperforming most Transformer-based forecasting models; it achieves exceptional accuracy, surpassing all state-of-the-art (SOTA) models in workload forecasting; and it exhibits robust performance for multi-period series. Furthermore, we collect and open-source four high-quality, open-source workload datasets derived from ByteDance's cloud services, encompassing workload data from thousands of computing instances. Extensive experiments on both our proprietary datasets and public benchmarks demonstrate that Fremer consistently outperforms baseline models, achieving average improvements of 5.5% in MSE, 4.7% in MAE, and 8.6% in SMAPE over SOTA models, while simultaneously reducing parameter scale and computational costs. Additionally, in a proactive auto-scaling test based on Kubernetes, Fremer improves average latency by 18.78% and reduces resource consumption by 2.35%, underscoring its practical efficacy in real-world applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 34,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12913",
        "abs_url": "https://arxiv.org/abs/2507.12913",
        "pdf_url": "https://arxiv.org/pdf/2507.12913",
        "title": "Robust Explanations Through Uncertainty Decomposition: A Path to Trustworthier AI",
        "authors": [
            "Chenrui Zhu",
            "Louenas Bounia",
            "Vu Linh Nguyen",
            "Sébastien Destercke",
            "Arthur Hoarau"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Recent advancements in machine learning have emphasized the need for transparency in model predictions, particularly as interpretability diminishes when using increasingly complex architectures. In this paper, we propose leveraging prediction uncertainty as a complementary approach to classical explainability methods. Specifically, we distinguish between aleatoric (data-related) and epistemic (model-related) uncertainty to guide the selection of appropriate explanations. Epistemic uncertainty serves as a rejection criterion for unreliable explanations and, in itself, provides insight into insufficient training (a new form of explanation). Aleatoric uncertainty informs the choice between feature-importance explanations and counterfactual explanations. This leverages a framework of explainability methods driven by uncertainty quantification and disentanglement. Our experiments demonstrate the impact of this uncertainty-aware approach on the robustness and attainability of explanations in both traditional machine learning and deep learning scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 35,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12927",
        "abs_url": "https://arxiv.org/abs/2507.12927",
        "pdf_url": "https://arxiv.org/pdf/2507.12927",
        "title": "Trace Reconstruction with Language Models",
        "authors": [
            "Franziska Weindel",
            "Michael Girsch",
            "Reinhard Heckel"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Information Theory (cs.IT)",
        "abstract": "The general trace reconstruction problem seeks to recover an original sequence from its noisy copies independently corrupted by deletions, insertions, and substitutions. This problem arises in applications such as DNA data storage, a promising storage medium due to its high information density and longevity. However, errors introduced during DNA synthesis, storage, and sequencing require correction through algorithms and codes, with trace reconstruction often used as part of the data retrieval process. In this work, we propose TReconLM, which leverages language models trained on next-token prediction for trace reconstruction. We pretrain language models on synthetic data and fine-tune on real-world data to adapt to technology-specific error patterns. TReconLM outperforms state-of-the-art trace reconstruction algorithms, including prior deep learning approaches, recovering a substantially higher fraction of sequences without error.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 36,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12931",
        "abs_url": "https://arxiv.org/abs/2507.12931",
        "pdf_url": "https://arxiv.org/pdf/2507.12931",
        "title": "From a Mixed-Policy Perspective: Improving Differentiable Automatic Post-editing Optimization",
        "authors": [
            "Hongze Tan"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Optimization and Control (math.OC)",
        "abstract": "This paper introduces two novel modifications to the Differentiable Automatic Post-editing Optimization (DAPO) algorithm, approached from a mixed-policy perspective. Standard policy gradient methods can suffer from instability and sample inefficiency, particularly in sparse reward settings. To address this, we first propose a method that incorporates a pre-trained, stable guiding policy ($\\piphi$) to provide off-policy experience, thereby regularizing the training of the target policy ($\\pion$). This approach improves training stability and convergence speed by adaptively adjusting the learning step size. Secondly, we extend this idea to re-utilize zero-reward samples, which are often discarded by dynamic sampling strategies like DAPO's. By treating these samples as a distinct batch guided by the expert policy, we further enhance sample efficiency. We provide a theoretical analysis for both methods, demonstrating that their objective functions converge to the optimal solution within the established theoretical framework of reinforcement learning. The proposed mixed-policy framework effectively balances exploration and exploitation, promising more stable and efficient policy optimization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 37,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12935",
        "abs_url": "https://arxiv.org/abs/2507.12935",
        "pdf_url": "https://arxiv.org/pdf/2507.12935",
        "title": "MC$^2$A: Enabling Algorithm-Hardware Co-Design for Efficient Markov Chain Monte Carlo Acceleration",
        "authors": [
            "Shirui Zhao",
            "Jun Yin",
            "Lingyun Yao",
            "Martin Andraud",
            "Wannes Meert",
            "Marian Verhelst"
        ],
        "comments": "14 pages, 15 figures, IEEE journal paper",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Hardware Architecture (cs.AR)",
        "abstract": "An increasing number of applications are exploiting sampling-based algorithms for planning, optimization, and inference. The Markov Chain Monte Carlo (MCMC) algorithms form the computational backbone of this emerging branch of machine learning. Unfortunately, the high computational cost limits their feasibility for large-scale problems and real-world applications, and the existing MCMC acceleration solutions are either limited in hardware flexibility or fail to maintain efficiency at the system level across a variety of end-to-end applications. This paper introduces \\textbf{MC$^2$A}, an algorithm-hardware co-design framework, enabling efficient and flexible optimization for MCMC acceleration. Firstly, \\textbf{MC$^2$A} analyzes the MCMC workload diversity through an extension of the processor performance roofline model with a 3rd dimension to derive the optimal balance between the compute, sampling and memory parameters. Secondly, \\textbf{MC$^2$A} proposes a parametrized hardware accelerator architecture with flexible and efficient support of MCMC kernels with a pipeline of ISA-programmable tree-structured processing units, reconfigurable samplers and a crossbar interconnect to support irregular access. Thirdly, the core of \\textbf{MC$^2$A} is powered by a novel Gumbel sampler that eliminates exponential and normalization operations. In the end-to-end case study, \\textbf{MC$^2$A} achieves an overall {$307.6\\times$, $1.4\\times$, $2.0\\times$, $84.2\\times$} speedup compared to the CPU, GPU, TPU and state-of-the-art MCMC accelerator. Evaluated on various representative MCMC workloads, this work demonstrates and exploits the feasibility of general hardware acceleration to popularize MCMC-based solutions in diverse application domains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 38,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12948",
        "abs_url": "https://arxiv.org/abs/2507.12948",
        "pdf_url": "https://arxiv.org/pdf/2507.12948",
        "title": "Probabilistic Soundness Guarantees in LLM Reasoning Chains",
        "authors": [
            "Weiqiu You",
            "Anton Xue",
            "Shreya Havaldar",
            "Delip Rao",
            "Helen Jin",
            "Chris Callison-Burch",
            "Eric Wong"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
        "abstract": "In reasoning chains generated by large language models (LLMs), initial errors often propagate and undermine the reliability of the final conclusion. Current LLM-based error detection methods often fail to detect propagated errors because they do not properly account for how earlier errors might corrupt judgments of downstream reasoning. To better detect such propagated errors, we introduce Autoregressive Reasoning Entailment Stability (ARES), a novel probabilistic framework that prevents error propagation by judging each claim based only on previously-assessed sound premises. This inductive method yields a nuanced score for each step and provides certified statistical guarantees of its soundness, rather than a brittle binary label. ARES achieves state-of-the-art performance across four benchmarks (72.1% Macro-F1, +8.2 points) and demonstrates superior robustness on very long synthetic reasoning chains, where it excels at detecting propagated errors (90.3% F1, +27.6 points).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 39,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12950",
        "abs_url": "https://arxiv.org/abs/2507.12950",
        "pdf_url": "https://arxiv.org/pdf/2507.12950",
        "title": "Insights into a radiology-specialised multimodal large language model with sparse autoencoders",
        "authors": [
            "Kenza Bouzid",
            "Shruthi Bannur",
            "Daniel Coelho de Castro",
            "Anton Schwaighofer",
            "Javier Alvarez-Valle",
            "Stephanie L. Hyland"
        ],
        "comments": "Actionable Interpretability Workshop at ICML 2025. 24 pages, 7 figures, 5 tables",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Interpretability can improve the safety, transparency and trust of AI models, which is especially important in healthcare applications where decisions often carry significant consequences. Mechanistic interpretability, particularly through the use of sparse autoencoders (SAEs), offers a promising approach for uncovering human-interpretable features within large transformer-based models. In this study, we apply Matryoshka-SAE to the radiology-specialised multimodal large language model, MAIRA-2, to interpret its internal representations. Using large-scale automated interpretability of the SAE features, we identify a range of clinically relevant concepts - including medical devices (e.g., line and tube placements, pacemaker presence), pathologies such as pleural effusion and cardiomegaly, longitudinal changes and textual features. We further examine the influence of these features on model behaviour through steering, demonstrating directional control over generations with mixed success. Our results reveal practical and methodological challenges, yet they offer initial insights into the internal concepts learned by MAIRA-2 - marking a step toward deeper mechanistic understanding and interpretability of a radiology-adapted multimodal large language model, and paving the way for improved model transparency. We release the trained SAEs and interpretations: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 40,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12963",
        "abs_url": "https://arxiv.org/abs/2507.12963",
        "pdf_url": "https://arxiv.org/pdf/2507.12963",
        "title": "A Spectral Interpretation of Redundancy in a Graph Reservoir",
        "authors": [
            "Anna Bison",
            "Alessandro Sperduti"
        ],
        "comments": "This paper has been accepted for presentation at the 3rd International Workshop on Reservoir Computing (RC 2025) at ICANN 2025",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Reservoir computing has been successfully applied to graphs as a preprocessing method to improve the training efficiency of Graph Neural Networks (GNNs). However, a common issue that arises when repeatedly applying layer operators on graphs is over-smoothing, which consists in the convergence of graph signals toward low-frequency components of the graph Laplacian. This work revisits the definition of the reservoir in the Multiresolution Reservoir Graph Neural Network (MRGNN), a spectral reservoir model, and proposes a variant based on a Fairing algorithm originally introduced in the field of surface design in computer graphics. This algorithm provides a pass-band spectral filter that allows smoothing without shrinkage, and it can be adapted to the graph setting through the Laplacian operator. Given its spectral formulation, this method naturally connects to GNN architectures for tasks where smoothing, when properly controlled, can be beneficial,such as graph classification. The core contribution of the paper lies in the theoretical analysis of the algorithm from a random walks perspective. In particular, it shows how tuning the spectral coefficients can be interpreted as modulating the contribution of redundant random walks. Exploratory experiments based on the MRGNN architecture illustrate the potential of this approach and suggest promising directions for future research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 41,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12969",
        "abs_url": "https://arxiv.org/abs/2507.12969",
        "pdf_url": "https://arxiv.org/pdf/2507.12969",
        "title": "WaveletInception Networks for Drive-by Vibration-Based Infrastructure Health Monitoring",
        "authors": [
            "Reza Riahi Samani",
            "Alfredo Nunez",
            "Bart De Schutter"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "This paper presents a novel deep learning-based framework for infrastructure health monitoring using drive-by vibration response signals. Recognizing the importance of spectral and temporal information, we introduce the WaveletInception-BiLSTM network. The WaveletInception feature extractor utilizes a Learnable Wavelet Packet Transform (LWPT) as the stem for extracting vibration signal features, incorporating spectral information in the early network layers. This is followed by 1D Inception networks that extract multi-scale, high-level features at deeper layers. The extracted vibration signal features are then integrated with operational conditions via a Long Short-term Memory (LSTM) layer. The resulting feature extraction network effectively analyzes drive-by vibration signals across various measurement speeds without preprocessing and uses LSTM to capture interrelated temporal dependencies among different modes of information and to create feature vectors for health condition estimation. The estimator head is designed with a sequential modeling architecture using bidirectional LSTM (BiLSTM) networks, capturing bi-directional temporal relationships from drive-by measurements. This architecture allows for a high-resolution, beam-level assessment of infrastructure health conditions. A case study focusing on railway track stiffness estimation with simulated drive-by vibration signals shows that the model significantly outperforms state-of-the-art methods in estimating railway ballast and railpad stiffness parameters. Results underscore the potential of this approach for accurate, localized, and fully automated drive-by infrastructure health monitoring.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 42,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12979",
        "abs_url": "https://arxiv.org/abs/2507.12979",
        "pdf_url": "https://arxiv.org/pdf/2507.12979",
        "title": "A Distributed Generative AI Approach for Heterogeneous Multi-Domain Environments under Data Sharing constraints",
        "authors": [
            "Youssef Tawfilis",
            "Hossam Amer",
            "Minar El-Aasser",
            "Tallal Elshabrawy"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Federated Learning has gained increasing attention for its ability to enable multiple nodes to collaboratively train machine learning models without sharing their raw data. At the same time, Generative AI -- particularly Generative Adversarial Networks (GANs) -- have achieved remarkable success across a wide range of domains, such as healthcare, security, and Image Generation. However, training generative models typically requires large datasets and significant computational resources, which are often unavailable in real-world settings. Acquiring such resources can be costly and inefficient, especially when many underutilized devices -- such as IoT devices and edge devices -- with varying capabilities remain idle. Moreover, obtaining large datasets is challenging due to privacy concerns and copyright restrictions, as most devices are unwilling to share their data. To address these challenges, we propose a novel approach for decentralized GAN training that enables the utilization of distributed data and underutilized, low-capability devices while not sharing data in its raw form. Our approach is designed to tackle key challenges in decentralized environments, combining KLD-weighted Clustered Federated Learning to address the issues of data heterogeneity and multi-domain datasets, with Heterogeneous U-Shaped split learning to tackle the challenge of device heterogeneity under strict data sharing constraints -- ensuring that no labels or raw data, whether real or synthetic, are ever shared between nodes. Experimental results shows that our approach demonstrates consistent and significant improvements across key performance metrics, where it achieves 1.1x -- 2.2x higher image generation scores, an average 10% boost in classification metrics (up to 50% in multi-domain non-IID settings), in much lower latency compared to several benchmarks. Find our code at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 43,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12983",
        "abs_url": "https://arxiv.org/abs/2507.12983",
        "pdf_url": "https://arxiv.org/pdf/2507.12983",
        "title": "FedGA: A Fair Federated Learning Framework Based on the Gini Coefficient",
        "authors": [
            "ShanBin Liu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Distributed, Parallel, and Cluster Computing (cs.DC)",
        "abstract": "Fairness has emerged as one of the key challenges in federated learning. In horizontal federated settings, data heterogeneity often leads to substantial performance disparities across clients, raising concerns about equitable model behavior. To address this issue, we propose FedGA, a fairness-aware federated learning algorithm. We first employ the Gini coefficient to measure the performance disparity among clients. Based on this, we establish a relationship between the Gini coefficient $G$ and the update scale of the global model ${U_s}$, and use this relationship to adaptively determine the timing of fairness intervention. Subsequently, we dynamically adjust the aggregation weights according to the system's real-time fairness status, enabling the global model to better incorporate information from clients with relatively poor this http URL conduct extensive experiments on the Office-Caltech-10, CIFAR-10, and Synthetic datasets. The results show that FedGA effectively improves fairness metrics such as variance and the Gini coefficient, while maintaining strong overall performance, demonstrating the effectiveness of our approach.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 44,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12990",
        "abs_url": "https://arxiv.org/abs/2507.12990",
        "pdf_url": "https://arxiv.org/pdf/2507.12990",
        "title": "Teach Old SAEs New Domain Tricks with Boosting",
        "authors": [
            "Nikita Koriagin",
            "Yaroslav Aksenov",
            "Daniil Laptev",
            "Gleb Gerasimov",
            "Nikita Balagansky",
            "Daniil Gavrilov"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Sparse Autoencoders have emerged as powerful tools for interpreting the internal representations of Large Language Models, yet they often fail to capture domain-specific features not prevalent in their training corpora. This paper introduces a residual learning approach that addresses this feature blindness without requiring complete retraining. We propose training a secondary SAE specifically to model the reconstruction error of a pretrained SAE on domain-specific texts, effectively capturing features missed by the primary model. By summing the outputs of both models during inference, we demonstrate significant improvements in both LLM cross-entropy and explained variance metrics across multiple specialized domains. Our experiments show that this method efficiently incorporates new domain knowledge into existing SAEs while maintaining their performance on general tasks. This approach enables researchers to selectively enhance SAE interpretability for specific domains of interest, opening new possibilities for targeted mechanistic interpretability of LLMs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 45,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13001",
        "abs_url": "https://arxiv.org/abs/2507.13001",
        "pdf_url": "https://arxiv.org/pdf/2507.13001",
        "title": "SMART: Relation-Aware Learning of Geometric Representations for Knowledge Graphs",
        "authors": [
            "Kossi Amouzouvi",
            "Bowen Song",
            "Andrea Coletta",
            "Luigi Bellomarini",
            "Jens Lehmann",
            "Sahar Vahdati"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Knowledge graph representation learning approaches provide a mapping between symbolic knowledge in the form of triples in a knowledge graph (KG) and their feature vectors. Knowledge graph embedding (KGE) models often represent relations in a KG as geometric transformations. Most state-of-the-art (SOTA) KGE models are derived from elementary geometric transformations (EGTs), such as translation, scaling, rotation, and reflection, or their combinations. These geometric transformations enable the models to effectively preserve specific structural and relational patterns of the KG. However, the current use of EGTs by KGEs remains insufficient without considering relation-specific transformations. Although recent models attempted to address this problem by ensembling SOTA baseline models in different ways, only a single or composite version of geometric transformations are used by such baselines to represent all the relations. In this paper, we propose a framework that evaluates how well each relation fits with different geometric transformations. Based on this ranking, the model can: (1) assign the best-matching transformation to each relation, or (2) use majority voting to choose one transformation type to apply across all relations. That is, the model learns a single relation-specific EGT in low dimensional vector space through an attention mechanism. Furthermore, we use the correlation between relations and EGTs, which are learned in a low dimension, for relation embeddings in a high dimensional vector space. The effectiveness of our models is demonstrated through comprehensive evaluations on three benchmark KGs as well as a real-world financial KG, witnessing a performance comparable to leading models",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 46,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13022",
        "abs_url": "https://arxiv.org/abs/2507.13022",
        "pdf_url": "https://arxiv.org/pdf/2507.13022",
        "title": "Fault detection and diagnosis for the engine electrical system of a space launcher based on a temporal convolutional autoencoder and calibrated classifiers",
        "authors": [
            "Luis Basora",
            "Louison Bocquet-Nouaille",
            "Elinirina Robinson",
            "Serge Le Gonidec"
        ],
        "comments": "53 pages, 16 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In the context of the health monitoring for the next generation of reusable space launchers, we outline a first step toward developing an onboard fault detection and diagnostic capability for the electrical system that controls the engine valves. Unlike existing approaches in the literature, our solution is designed to meet a broader range of key requirements. This includes estimating confidence levels for predictions, detecting out-of-distribution (OOD) cases, and controlling false alarms. The proposed solution is based on a temporal convolutional autoencoder to automatically extract low-dimensional features from raw sensor data. Fault detection and diagnosis are respectively carried out using a binary and a multiclass classifier trained on the autoencoder latent and residual spaces. The classifiers are histogram-based gradient boosting models calibrated to output probabilities that can be interpreted as confidence levels. A relatively simple technique, based on inductive conformal anomaly detection, is used to identify OOD data. We leverage other simple yet effective techniques, such as cumulative sum control chart (CUSUM) to limit the false alarms, and threshold moving to address class imbalance in fault detection. The proposed framework is highly configurable and has been evaluated on simulated data, covering both nominal and anomalous operational scenarios. The results indicate that our solution is a promising first step, though testing with real data will be necessary to ensure that it achieves the required maturity level for operational use.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 47,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13034",
        "abs_url": "https://arxiv.org/abs/2507.13034",
        "pdf_url": "https://arxiv.org/pdf/2507.13034",
        "title": "Confidence-Filtered Relevance (CFR): An Interpretable and Uncertainty-Aware Machine Learning Framework for Naturalness Assessment in Satellite Imagery",
        "authors": [
            "Ahmed Emam",
            "Ribana Roscher"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Protected natural areas play a vital role in ecological balance and ecosystem services. Monitoring these regions at scale using satellite imagery and machine learning is promising, but current methods often lack interpretability and uncertainty-awareness, and do not address how uncertainty affects naturalness assessment. In contrast, we propose Confidence-Filtered Relevance (CFR), a data-centric framework that combines LRP Attention Rollout with Deep Deterministic Uncertainty (DDU) estimation to analyze how model uncertainty influences the interpretability of relevance heatmaps. CFR partitions the dataset into subsets based on uncertainty thresholds, enabling systematic analysis of how uncertainty shapes the explanations of naturalness in satellite imagery. Applied to the AnthroProtect dataset, CFR assigned higher relevance to shrublands, forests, and wetlands, aligning with other research on naturalness assessment. Moreover, our analysis shows that as uncertainty increases, the interpretability of these relevance heatmaps declines and their entropy grows, indicating less selective and more ambiguous attributions. CFR provides a data-centric approach to assess the relevance of patterns to naturalness in satellite imagery based on their associated certainty.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 48,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13043",
        "abs_url": "https://arxiv.org/abs/2507.13043",
        "pdf_url": "https://arxiv.org/pdf/2507.13043",
        "title": "The Power of Architecture: Deep Dive into Transformer Architectures for Long-Term Time Series Forecasting",
        "authors": [
            "Lefei Shen",
            "Mouxiang Chen",
            "Han Fu",
            "Xiaoxue Ren",
            "Xiaoyun Joy Wang",
            "Jianling Sun",
            "Zhuo Li",
            "Chenghao Liu"
        ],
        "comments": "15 pages, 6 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Transformer-based models have recently become dominant in Long-term Time Series Forecasting (LTSF), yet the variations in their architecture, such as encoder-only, encoder-decoder, and decoder-only designs, raise a crucial question: What Transformer architecture works best for LTSF tasks? However, existing models are often tightly coupled with various time-series-specific designs, making it difficult to isolate the impact of the architecture itself. To address this, we propose a novel taxonomy that disentangles these designs, enabling clearer and more unified comparisons of Transformer architectures. Our taxonomy considers key aspects such as attention mechanisms, forecasting aggregations, forecasting paradigms, and normalization layers. Through extensive experiments, we uncover several key insights: bi-directional attention with joint-attention is most effective; more complete forecasting aggregation improves performance; and the direct-mapping paradigm outperforms autoregressive approaches. Furthermore, our combined model, utilizing optimal architectural choices, consistently outperforms several existing models, reinforcing the validity of our conclusions. We hope these findings offer valuable guidance for future research on Transformer architectural designs in LTSF. Our code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 49,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13054",
        "abs_url": "https://arxiv.org/abs/2507.13054",
        "pdf_url": "https://arxiv.org/pdf/2507.13054",
        "title": "On statistical learning of graphs",
        "authors": [
            "Vittorio Cipriani",
            "Valentino Delle Rose",
            "Luca San Mauro",
            "Giovanni Solda"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Logic (math.LO)",
        "abstract": "We study PAC and online learnability of hypothesis classes formed by copies of a countably infinite graph G, where each copy is induced by permuting G's vertices. This corresponds to learning a graph's labeling, knowing its structure and label set. We consider classes where permutations move only finitely many vertices. Our main result shows that PAC learnability of all such finite-support copies implies online learnability of the full isomorphism type of G, and is equivalent to the condition of automorphic triviality. We also characterize graphs where copies induced by swapping two vertices are not learnable, using a relaxation of the extension property of the infinite random graph. Finally, we show that, for all G and k>2, learnability for k-vertex permutations is equivalent to that for 2-vertex permutations, yielding a four-class partition of infinite graphs, whose complexity we also determine using tools coming from both descriptive set theory and computability theory.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 50,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13079",
        "abs_url": "https://arxiv.org/abs/2507.13079",
        "pdf_url": "https://arxiv.org/pdf/2507.13079",
        "title": "DASViT: Differentiable Architecture Search for Vision Transformer",
        "authors": [
            "Pengjin Wu",
            "Ferrante Neri",
            "Zhenhua Feng"
        ],
        "comments": "Accepted to the International Joint Conference on Neural Networks (IJCNN) 2025",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Designing effective neural networks is a cornerstone of deep learning, and Neural Architecture Search (NAS) has emerged as a powerful tool for automating this process. Among the existing NAS approaches, Differentiable Architecture Search (DARTS) has gained prominence for its efficiency and ease of use, inspiring numerous advancements. Since the rise of Vision Transformers (ViT), researchers have applied NAS to explore ViT architectures, often focusing on macro-level search spaces and relying on discrete methods like evolutionary algorithms. While these methods ensure reliability, they face challenges in discovering innovative architectural designs, demand extensive computational resources, and are time-intensive. To address these limitations, we introduce Differentiable Architecture Search for Vision Transformer (DASViT), which bridges the gap in differentiable search for ViTs and uncovers novel designs. Experiments show that DASViT delivers architectures that break traditional Transformer encoder designs, outperform ViT-B/16 on multiple datasets, and achieve superior efficiency with fewer parameters and FLOPs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 51,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13090",
        "abs_url": "https://arxiv.org/abs/2507.13090",
        "pdf_url": "https://arxiv.org/pdf/2507.13090",
        "title": "MUPAX: Multidimensional Problem Agnostic eXplainable AI",
        "authors": [
            "Vincenzo Dentamaro",
            "Felice Franchini",
            "Giuseppe Pirlo",
            "Irina Voiculescu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Robust XAI techniques should ideally be simultaneously deterministic, model agnostic, and guaranteed to converge. We propose MULTIDIMENSIONAL PROBLEM AGNOSTIC EXPLAINABLE AI (MUPAX), a deterministic, model agnostic explainability technique, with guaranteed convergency. MUPAX measure theoretic formulation gives principled feature importance attribution through structured perturbation analysis that discovers inherent input patterns and eliminates spurious relationships. We evaluate MUPAX on an extensive range of data modalities and tasks: audio classification (1D), image classification (2D), volumetric medical image analysis (3D), and anatomical landmark detection, demonstrating dimension agnostic effectiveness. The rigorous convergence guarantees extend to any loss function and arbitrary dimensions, making MUPAX applicable to virtually any problem context for AI. By contrast with other XAI methods that typically decrease performance when masking, MUPAX not only preserves but actually enhances model accuracy by capturing only the most important patterns of the original data. Extensive benchmarking against the state of the XAI art demonstrates MUPAX ability to generate precise, consistent and understandable explanations, a crucial step towards explainable and trustworthy AI systems. The source code will be released upon publication.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 52,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13092",
        "abs_url": "https://arxiv.org/abs/2507.13092",
        "pdf_url": "https://arxiv.org/pdf/2507.13092",
        "title": "Uncertainty-Aware Cross-Modal Knowledge Distillation with Prototype Learning for Multimodal Brain-Computer Interfaces",
        "authors": [
            "Hyo-Jeong Jang",
            "Hye-Bin Shin",
            "Seong-Whan Lee"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Human-Computer Interaction (cs.HC)",
        "abstract": "Electroencephalography (EEG) is a fundamental modality for cognitive state monitoring in brain-computer interfaces (BCIs). However, it is highly susceptible to intrinsic signal errors and human-induced labeling errors, which lead to label noise and ultimately degrade model performance. To enhance EEG learning, multimodal knowledge distillation (KD) has been explored to transfer knowledge from visual models with rich representations to EEG-based models. Nevertheless, KD faces two key challenges: modality gap and soft label misalignment. The former arises from the heterogeneous nature of EEG and visual feature spaces, while the latter stems from label inconsistencies that create discrepancies between ground truth labels and distillation targets. This paper addresses semantic uncertainty caused by ambiguous features and weakly defined labels. We propose a novel cross-modal knowledge distillation framework that mitigates both modality and label inconsistencies. It aligns feature semantics through a prototype-based similarity module and introduces a task-specific distillation head to resolve label-induced inconsistency in supervision. Experimental results demonstrate that our approach improves EEG-based emotion regression and classification performance, outperforming both unimodal and multimodal baselines on a public multimodal dataset. These findings highlight the potential of our framework for BCI applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 53,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13133",
        "abs_url": "https://arxiv.org/abs/2507.13133",
        "pdf_url": "https://arxiv.org/pdf/2507.13133",
        "title": "NGTM: Substructure-based Neural Graph Topic Model for Interpretable Graph Generation",
        "authors": [
            "Yuanxin Zhuang",
            "Dazhong Shen",
            "Ying Sun"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Graph generation plays a pivotal role across numerous domains, including molecular design and knowledge graph construction. Although existing methods achieve considerable success in generating realistic graphs, their interpretability remains limited, often obscuring the rationale behind structural decisions. To address this challenge, we propose the Neural Graph Topic Model (NGTM), a novel generative framework inspired by topic modeling in natural language processing. NGTM represents graphs as mixtures of latent topics, each defining a distribution over semantically meaningful substructures, which facilitates explicit interpretability at both local and global scales. The generation process transparently integrates these topic distributions with a global structural variable, enabling clear semantic tracing of each generated graph. Experiments demonstrate that NGTM achieves competitive generation quality while uniquely enabling fine-grained control and interpretability, allowing users to tune structural features or induce biological properties through topic-level adjustments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 54,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13155",
        "abs_url": "https://arxiv.org/abs/2507.13155",
        "pdf_url": "https://arxiv.org/pdf/2507.13155",
        "title": "NonverbalTTS: A Public English Corpus of Text-Aligned Nonverbal Vocalizations with Emotion Annotations for Text-to-Speech",
        "authors": [
            "Maksim Borisov",
            "Egor Spirin",
            "Daria Diatlova"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Sound (cs.SD)",
        "abstract": "Current expressive speech synthesis models are constrained by the limited availability of open-source datasets containing diverse nonverbal vocalizations (NVs). In this work, we introduce NonverbalTTS (NVTTS), a 17-hour open-access dataset annotated with 10 types of NVs (e.g., laughter, coughs) and 8 emotional categories. The dataset is derived from popular sources, VoxCeleb and Expresso, using automated detection followed by human validation. We propose a comprehensive pipeline that integrates automatic speech recognition (ASR), NV tagging, emotion classification, and a fusion algorithm to merge transcriptions from multiple annotators. Fine-tuning open-source text-to-speech (TTS) models on the NVTTS dataset achieves parity with closed-source systems such as CosyVoice2, as measured by both human evaluation and automatic metrics, including speaker similarity and NV fidelity. By releasing NVTTS and its accompanying annotation guidelines, we address a key bottleneck in expressive TTS research. The dataset is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 55,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13158",
        "abs_url": "https://arxiv.org/abs/2507.13158",
        "pdf_url": "https://arxiv.org/pdf/2507.13158",
        "title": "Inverse Reinforcement Learning Meets Large Language Model Post-Training: Basics, Advances, and Opportunities",
        "authors": [
            "Hao Sun",
            "Mihaela van der Schaar"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "In the era of Large Language Models (LLMs), alignment has emerged as a fundamental yet challenging problem in the pursuit of more reliable, controllable, and capable machine intelligence. The recent success of reasoning models and conversational AI systems has underscored the critical role of reinforcement learning (RL) in enhancing these systems, driving increased research interest at the intersection of RL and LLM alignment. This paper provides a comprehensive review of recent advances in LLM alignment through the lens of inverse reinforcement learning (IRL), emphasizing the distinctions between RL techniques employed in LLM alignment and those in conventional RL tasks. In particular, we highlight the necessity of constructing neural reward models from human data and discuss the formal and practical implications of this paradigm shift. We begin by introducing fundamental concepts in RL to provide a foundation for readers unfamiliar with the field. We then examine recent advances in this research agenda, discussing key challenges and opportunities in conducting IRL for LLM alignment. Beyond methodological considerations, we explore practical aspects, including datasets, benchmarks, evaluation metrics, infrastructure, and computationally efficient training and inference techniques. Finally, we draw insights from the literature on sparse-reward RL to identify open questions and potential research directions. By synthesizing findings from diverse studies, we aim to provide a structured and critical overview of the field, highlight unresolved challenges, and outline promising future directions for improving LLM alignment through RL and IRL techniques.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 56,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13181",
        "abs_url": "https://arxiv.org/abs/2507.13181",
        "pdf_url": "https://arxiv.org/pdf/2507.13181",
        "title": "Spectral Bellman Method: Unifying Representation and Exploration in RL",
        "authors": [
            "Ofir Nabati",
            "Bo Dai",
            "Shie Mannor",
            "Guy Tennenholtz"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The effect of representation has been demonstrated in reinforcement learning, from both theoretical and empirical successes. However, the existing representation learning mainly induced from model learning aspects, misaligning with our RL tasks. This work introduces Spectral Bellman Representation, a novel framework derived from the Inherent Bellman Error (IBE) condition, which aligns with the fundamental structure of Bellman updates across a space of possible value functions, therefore, directly towards value-based RL. Our key insight is the discovery of a fundamental spectral relationship: under the zero-IBE condition, the transformation of a distribution of value functions by the Bellman operator is intrinsically linked to the feature covariance structure. This spectral connection yields a new, theoretically-grounded objective for learning state-action features that inherently capture this Bellman-aligned covariance. Our method requires a simple modification to existing algorithms. We demonstrate that our learned representations enable structured exploration, by aligning feature covariance with Bellman dynamics, and improve overall performance, particularly in challenging hard-exploration and long-horizon credit assignment tasks. Our framework naturally extends to powerful multi-step Bellman operators, further broadening its impact. Spectral Bellman Representation offers a principled and effective path toward learning more powerful and structurally sound representations for value-based reinforcement learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 57,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13191",
        "abs_url": "https://arxiv.org/abs/2507.13191",
        "pdf_url": "https://arxiv.org/pdf/2507.13191",
        "title": "GradNetOT: Learning Optimal Transport Maps with GradNets",
        "authors": [
            "Shreyas Chaudhari",
            "Srinivasa Pranav",
            "José M. F. Moura"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Monotone gradient functions play a central role in solving the Monge formulation of the optimal transport problem, which arises in modern applications ranging from fluid dynamics to robot swarm control. When the transport cost is the squared Euclidean distance, Brenier's theorem guarantees that the unique optimal map is the gradient of a convex function, namely a monotone gradient map, and it satisfies a Monge-Ampère equation. In [arXiv:2301.10862] [arXiv:2404.07361], we proposed Monotone Gradient Networks (mGradNets), neural networks that directly parameterize the space of monotone gradient maps. In this work, we leverage mGradNets to directly learn the optimal transport mapping by minimizing a training loss function defined using the Monge-Ampère equation. We empirically show that the structural bias of mGradNets facilitates the learning of optimal transport maps and employ our method for a robot swarm control problem.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 58,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13207",
        "abs_url": "https://arxiv.org/abs/2507.13207",
        "pdf_url": "https://arxiv.org/pdf/2507.13207",
        "title": "MoTM: Towards a Foundation Model for Time Series Imputation based on Continuous Modeling",
        "authors": [
            "Etienne Le Naour",
            "Tahar Nabil",
            "Ghislain Agoua"
        ],
        "comments": "10th Workshop on Advanced Analytics and Learning on Temporal Data (AALTD), ECML 2025",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Recent years have witnessed a growing interest for time series foundation models, with a strong emphasis on the forecasting task. Yet, the crucial task of out-of-domain imputation of missing values remains largely underexplored. We propose a first step to fill this gap by leveraging implicit neural representations (INRs). INRs model time series as continuous functions and naturally handle various missing data scenarios and sampling rates. While they have shown strong performance within specific distributions, they struggle under distribution shifts. To address this, we introduce MoTM (Mixture of Timeflow Models), a step toward a foundation model for time series imputation. Building on the idea that a new time series is a mixture of previously seen patterns, MoTM combines a basis of INRs, each trained independently on a distinct family of time series, with a ridge regressor that adapts to the observed context at inference. We demonstrate robust in-domain and out-of-domain generalization across diverse imputation scenarios (e.g., block and pointwise missingness, variable sampling rates), paving the way for adaptable foundation imputation models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 59,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13250",
        "abs_url": "https://arxiv.org/abs/2507.13250",
        "pdf_url": "https://arxiv.org/pdf/2507.13250",
        "title": "Leveraging Asynchronous Cross-border Market Data for Improved Day-Ahead Electricity Price Forecasting in European Markets",
        "authors": [
            "Maria Margarida Mascarenhas",
            "Jilles De Blauwe",
            "Mikael Amelin",
            "Hussain Kazmi"
        ],
        "comments": "Both Maria Margarida Mascarenhas and Jilles De Blauwe contributed equally to the paper",
        "subjects": "Machine Learning (cs.LG); Systems and Control (eess.SY)",
        "abstract": "Accurate short-term electricity price forecasting is crucial for strategically scheduling demand and generation bids in day-ahead markets. While data-driven techniques have shown considerable prowess in achieving high forecast accuracy in recent years, they rely heavily on the quality of input covariates. In this paper, we investigate whether asynchronously published prices as a result of differing gate closure times (GCTs) in some bidding zones can improve forecasting accuracy in other markets with later GCTs. Using a state-of-the-art ensemble of models, we show significant improvements of 22% and 9% in forecast accuracy in the Belgian (BE) and Swedish bidding zones (SE3) respectively, when including price data from interconnected markets with earlier GCT (Germany-Luxembourg, Austria, and Switzerland). This improvement holds for both general as well as extreme market conditions. Our analysis also yields further important insights: frequent model recalibration is necessary for maximum accuracy but comes at substantial additional computational costs, and using data from more markets does not always lead to better performance - a fact we delve deeper into with interpretability analysis of the forecast models. Overall, these findings provide valuable guidance for market participants and decision-makers aiming to optimize bidding strategies within increasingly interconnected and volatile European energy markets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 60,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13263",
        "abs_url": "https://arxiv.org/abs/2507.13263",
        "pdf_url": "https://arxiv.org/pdf/2507.13263",
        "title": "Merge Kernel for Bayesian Optimization on Permutation Space",
        "authors": [
            "Zikai Xie",
            "Linjiang Chen"
        ],
        "comments": "8 pages, submitted to AAAI-26",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Bayesian Optimization (BO) algorithm is a standard tool for black-box optimization problems. The current state-of-the-art BO approach for permutation spaces relies on the Mallows kernel-an $\\Omega(n^2)$ representation that explicitly enumerates every pairwise comparison. Inspired by the close relationship between the Mallows kernel and pairwise comparison, we propose a novel framework for generating kernel functions on permutation space based on sorting algorithms. Within this framework, the Mallows kernel can be viewed as a special instance derived from bubble sort. Further, we introduce the \\textbf{Merge Kernel} constructed from merge sort, which replaces the quadratic complexity with $\\Theta(n\\log n)$ to achieve the lowest possible complexity. The resulting feature vector is significantly shorter, can be computed in linearithmic time, yet still efficiently captures meaningful permutation distances. To boost robustness and right-invariance without sacrificing compactness, we further incorporate three lightweight, task-agnostic descriptors: (1) a shift histogram, which aggregates absolute element displacements and supplies a global misplacement signal; (2) a split-pair line, which encodes selected long-range comparisons by aligning elements across the two halves of the whole permutation; and (3) sliding-window motifs, which summarize local order patterns that influence near-neighbor objectives. Our empirical evaluation demonstrates that the proposed kernel consistently outperforms the state-of-the-art Mallows kernel across various permutation optimization benchmarks. Results confirm that the Merge Kernel provides a more compact yet more effective solution for Bayesian optimization in permutation space.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 61,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13305",
        "abs_url": "https://arxiv.org/abs/2507.13305",
        "pdf_url": "https://arxiv.org/pdf/2507.13305",
        "title": "Boosting Team Modeling through Tempo-Relational Representation Learning",
        "authors": [
            "Vincenzo Marco De Luca",
            "Giovanna Varni",
            "Andrea Passerini"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Team modeling remains a fundamental challenge at the intersection of Artificial Intelligence and the Social Sciences. Social Science research emphasizes the need to jointly model dynamics and relations, while practical applications demand unified models capable of inferring multiple team constructs simultaneously, providing interpretable insights and actionable recommendations to enhance team performance. However, existing works do not meet these practical demands. To bridge this gap, we present TRENN, a novel tempo-relational architecture that integrates: (i) an automatic temporal graph extractor, (ii) a tempo-relational encoder, (iii) a decoder for team construct prediction, and (iv) two complementary explainability modules. TRENN jointly captures relational and temporal team dynamics, providing a solid foundation for MT-TRENN, which extends TReNN by replacing the decoder with a multi-task head, enabling the model to learn shared Social Embeddings and simultaneously predict multiple team constructs, including Emergent Leadership, Leadership Style, and Teamwork components. Experimental results demonstrate that our approach significantly outperforms approaches that rely exclusively on temporal or relational information. Additionally, experimental evaluation has shown that the explainability modules integrated in MT-TRENN yield interpretable insights and actionable suggestions to support team improvement. These capabilities make our approach particularly well-suited for Human-Centered AI applications, such as intelligent decision-support systems in high-stakes collaborative environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 62,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13323",
        "abs_url": "https://arxiv.org/abs/2507.13323",
        "pdf_url": "https://arxiv.org/pdf/2507.13323",
        "title": "GeoReg: Weight-Constrained Few-Shot Regression for Socio-Economic Estimation using LLM",
        "authors": [
            "Kyeongjin Ahn",
            "Sungwon Han",
            "Seungeon Lee",
            "Donghyun Ahn",
            "Hyoshin Kim",
            "Jungwon Kim",
            "Jihee Kim",
            "Sangyoon Park",
            "Meeyoung Cha"
        ],
        "comments": "15 pages, 13 figures, 7 tables",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Socio-economic indicators like regional GDP, population, and education levels, are crucial to shaping policy decisions and fostering sustainable development. This research introduces GeoReg a regression model that integrates diverse data sources, including satellite imagery and web-based geospatial information, to estimate these indicators even for data-scarce regions such as developing countries. Our approach leverages the prior knowledge of large language model (LLM) to address the scarcity of labeled data, with the LLM functioning as a data engineer by extracting informative features to enable effective estimation in few-shot settings. Specifically, our model obtains contextual relationships between data features and the target indicator, categorizing their correlations as positive, negative, mixed, or irrelevant. These features are then fed into the linear estimator with tailored weight constraints for each category. To capture nonlinear patterns, the model also identifies meaningful feature interactions and integrates them, along with nonlinear transformations. Experiments across three countries at different stages of development demonstrate that our model outperforms baselines in estimating socio-economic indicators, even for low-income countries with limited data availability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 63,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13338",
        "abs_url": "https://arxiv.org/abs/2507.13338",
        "pdf_url": "https://arxiv.org/pdf/2507.13338",
        "title": "Training Transformers with Enforced Lipschitz Constants",
        "authors": [
            "Laker Newhouse",
            "R. Preston Hess",
            "Franz Cesista",
            "Andrii Zahorodnii",
            "Jeremy Bernstein",
            "Phillip Isola"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Neural networks are often highly sensitive to input and weight perturbations. This sensitivity has been linked to pathologies such as vulnerability to adversarial examples, divergent training, and overfitting. To combat these problems, past research has looked at building neural networks entirely from Lipschitz components. However, these techniques have not matured to the point where researchers have trained a modern architecture such as a transformer with a Lipschitz certificate enforced beyond initialization. To explore this gap, we begin by developing and benchmarking novel, computationally-efficient tools for maintaining norm-constrained weight matrices. Applying these tools, we are able to train transformer models with Lipschitz bounds enforced throughout training. We find that optimizer dynamics matter: switching from AdamW to Muon improves standard methods -- weight decay and spectral normalization -- allowing models to reach equal performance with a lower Lipschitz bound. Inspired by Muon's update having a fixed spectral norm, we co-design a weight constraint method that improves the Lipschitz vs. performance tradeoff on MLPs and 2M parameter transformers. Our 2-Lipschitz transformer on Shakespeare text reaches validation accuracy 60%. Scaling to 145M parameters, our 10-Lipschitz transformer reaches 21% accuracy on internet text. However, to match the NanoGPT baseline validation accuracy of 39.4%, our Lipschitz upper bound increases to 10^264. Nonetheless, our Lipschitz transformers train without stability measures such as layer norm, QK norm, and logit tanh softcapping.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 64,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2407.02740",
        "abs_url": "https://arxiv.org/abs/2407.02740",
        "pdf_url": "https://arxiv.org/pdf/2407.02740",
        "title": "Implementation and Analysis of GPU Algorithms for Vecchia Approximation",
        "authors": [
            "Zachary James",
            "Joseph Guinness"
        ],
        "comments": "",
        "subjects": "Computation (stat.CO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Gaussian Processes have become an indispensable part of the spatial statistician's toolbox but are unsuitable for analyzing large dataset because of the significant time and memory needed to fit the associated model exactly. Vecchia Approximation is widely used to reduce the computational complexity and can be calculated with embarrassingly parallel algorithms. While multi-core software has been developed for Vecchia Approximation, such as the GpGp R package, software designed to run on graphics processing units (GPU) is lacking, despite the tremendous success GPUs have had in statistics and machine learning. We compare three different ways to implement Vecchia Approximation on a GPU: two of which are similar to methods used for other Gaussian Process approximations and one that is new. The impact of memory type on performance is investigated and the final method is optimized accordingly. We show that our new method outperforms the other two and then present it in the GpGpU R package. We compare GpGpU to existing multi-core and GPU-accelerated software by fitting Gaussian Process models on various datasets, including a large spatial-temporal dataset of $n>10^6$ points collected from an earth-observing satellite. Our results show that GpGpU achieves faster runtimes and better predictive accuracy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 65,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2407.18798",
        "abs_url": "https://arxiv.org/abs/2407.18798",
        "pdf_url": "https://arxiv.org/pdf/2407.18798",
        "title": "Predicting 3D Rigid Body Dynamics with Deep Residual Network",
        "authors": [
            "Abiodun Finbarrs Oketunji"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR); Machine Learning (cs.LG); Software Engineering (cs.SE)",
        "abstract": "This study investigates the application of deep residual networks for predicting the dynamics of interacting three-dimensional rigid bodies. We present a framework combining a 3D physics simulator implemented in C++ with a deep learning model constructed using PyTorch. The simulator generates training data encompassing linear and angular motion, elastic collisions, fluid friction, gravitational effects, and damping. Our deep residual network, consisting of an input layer, multiple residual blocks, and an output layer, is designed to handle the complexities of 3D dynamics. We evaluate the network's performance using a datasetof 10,000 simulated scenarios, each involving 3-5 interacting rigid bodies. The model achieves a mean squared error of 0.015 for position predictions and 0.022 for orientation predictions, representing a 25% improvement over baseline methods. Our results demonstrate the network's ability to capture intricate physical interactions, with particular success in predicting elastic collisions and rotational dynamics. This work significantly contributes to physics-informed machine learning by showcasing the immense potential of deep residual networks in modeling complex 3D physical systems. We discuss our approach's limitations and propose future directions for improving generalization to more diverse object shapes and materials.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 66,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2501.02707",
        "abs_url": "https://arxiv.org/abs/2501.02707",
        "pdf_url": "https://arxiv.org/pdf/2501.02707",
        "title": "Refining Coarse-Grained Molecular Topologies: A Bayesian Optimization Approach",
        "authors": [
            "Pranoy Ray",
            "Adam P. Generale",
            "Nikhith Vankireddy",
            "Yuichiro Asoma",
            "Masataka Nakauchi",
            "Haein Lee",
            "Katsuhisa Yoshida",
            "Yoshishige Okuno",
            "Surya R. Kalidindi"
        ],
        "comments": "",
        "subjects": "Chemical Physics (physics.chem-ph); Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG)",
        "abstract": "Molecular Dynamics (MD) simulations are essential for accurately predicting the physical and chemical properties of large molecular systems across various pressure and temperature ensembles. However, the high computational costs associated with All-Atom (AA) MD simulations have led to the development of Coarse-Grained Molecular Dynamics (CGMD), providing a lower-dimensional compression of the AA structure into representative CG beads, offering reduced computational expense at the cost of predictive accuracy. Existing CGMD methods, such as CG-Martini (calibrated against experimental data), aim to generate an embedding of a topology that sufficiently generalizes across a range of structures. Detrimentally, in attempting to specify parameterization with applicability across molecular classes, it is unable to specialize to domain-specific applications, where sufficient accuracy and computational speed are critical. This work presents a novel approach to optimize derived results from CGMD simulations by refining the general-purpose Martini3 topologies specifically the bonded interaction parameters within a given coarse-grained mapping - for domain-specific applications using Bayesian Optimization methodologies. We have developed and validated a CG potential applicable to any degree of polymerization, representing a significant advancement in the field. Our optimized CG potential, based on the Martini3 framework, aims to achieve accuracy comparable to AAMD while maintaining the computational efficiency of CGMD. This approach bridges the gap between efficiency and accuracy in multiscale molecular simulations, potentially enabling more rapid and cost-effective molecular discovery across various scientific and technological domains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 67,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12469",
        "abs_url": "https://arxiv.org/abs/2507.12469",
        "pdf_url": "https://arxiv.org/pdf/2507.12469",
        "title": "Perfect diffusion is $\\mathsf{TC}^0$ -- Bad diffusion is Turing-complete",
        "authors": [
            "Yuxi Liu"
        ],
        "comments": "7 pages",
        "subjects": "Computational Complexity (cs.CC); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "This paper explores the computational complexity of diffusion-based language modeling. We prove a dichotomy based on the quality of the score-matching network in a diffusion model. In one direction, a network that exactly computes the score function of some initial distribution can only perform language modeling within the $\\mathsf{TC}^0$ complexity class, reflecting limitations tied to rapid convergence. In the other direction, we show that if there is no requirement for the network to match any score function, then diffusion modeling can simulate any Turing machine in a certain sense. This dichotomy provides a theoretical lens on the capabilities and limitations of diffusion models, particularly concerning tasks requiring sequential computation. We conjecture extensions of our theoretical results, including for the case where the diffusion model is not perfect, but merely good. We also discuss the wider context and practical implications, and hypothesize that a machine learning architecture that can interpolate between sequential and parallel modes of operation would be superior to both Transformers and diffusion models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 68,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12473",
        "abs_url": "https://arxiv.org/abs/2507.12473",
        "pdf_url": "https://arxiv.org/pdf/2507.12473",
        "title": "The Generalist Brain Module: Module Repetition in Neural Networks in Light of the Minicolumn Hypothesis",
        "authors": [
            "Mia-Katrin Kvalsund",
            "Mikkel Elle Lepperød"
        ],
        "comments": "",
        "subjects": "Neurons and Cognition (q-bio.NC); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)",
        "abstract": "While modern AI continues to advance, the biological brain remains the pinnacle of neural networks in its robustness, adaptability, and efficiency. This review explores an AI architectural path inspired by the brain's structure, particularly the minicolumn hypothesis, which views the neocortex as a distributed system of repeated modules - a structure we connect to collective intelligence (CI). Despite existing work, there is a lack of comprehensive reviews connecting the cortical column to the architectures of repeated neural modules. This review aims to fill that gap by synthesizing historical, theoretical, and methodological perspectives on neural module repetition. We distinguish between architectural repetition - reusing structure - and parameter-shared module repetition, where the same functional unit is repeated across a network. The latter exhibits key CI properties such as robustness, adaptability, and generalization. Evidence suggests that the repeated module tends to converge toward a generalist module: simple, flexible problem solvers capable of handling many roles in the ensemble. This generalist tendency may offer solutions to longstanding challenges in modern AI: improved energy efficiency during training through simplicity and scalability, and robust embodied control via generalization. While empirical results suggest such systems can generalize to out-of-distribution problems, theoretical results are still lacking. Overall, architectures featuring module repetition remain an emerging and unexplored architectural strategy, with significant untapped potential for both efficiency, robustness, and adaptiveness. We believe that a system that adopts the benefits of CI, while adhering to architectural and functional principles of the minicolumns, could challenge the modern AI problems of scalability, energy consumption, and democratization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 69,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12482",
        "abs_url": "https://arxiv.org/abs/2507.12482",
        "pdf_url": "https://arxiv.org/pdf/2507.12482",
        "title": "Kodezi Chronos: A Debugging-First Language Model for Repository-Scale, Memory-Driven Code Understanding",
        "authors": [
            "Ishraq Khan",
            "Assad Chowdary",
            "Sharoz Haseeb",
            "Urvish Patel"
        ],
        "comments": "10 pages, 10 figures, 7 tables, IEEE Conference format, Q4 2025 model release, Q1 2026 Kodezi OS deployment",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)",
        "abstract": "Large Language Models (LLMs) have advanced code generation and software automation, but are fundamentally constrained by limited inference-time context and lack of explicit code structure reasoning. We introduce Kodezi Chronos, a next-generation architecture for autonomous code understanding, debugging, and maintenance, designed to operate across ultra-long contexts comprising entire codebases, histories, and documentation, all without fixed window limits. Kodezi Chronos leverages a multi-level embedding memory engine, combining vector and graph-based indexing with continuous code-aware retrieval. This enables efficient and accurate reasoning over millions of lines of code, supporting repository-scale comprehension, multi-file refactoring, and real-time self-healing actions. Our evaluation introduces a novel Multi Random Retrieval benchmark, specifically tailored to the software engineering domain. Unlike classical retrieval benchmarks, this method requires the model to resolve arbitrarily distant and obfuscated associations across code artifacts, simulating realistic tasks such as variable tracing, dependency migration, and semantic bug localization. Chronos outperforms prior LLMs and code models, demonstrating a 23% improvement in real-world bug detection and reducing debugging cycles by up to 40% compared to traditional sequence-based approaches. By natively interfacing with IDEs and CI/CD workflows, Chronos enables seamless, autonomous software maintenance, elevating code reliability and productivity while reducing manual effort. These results mark a critical advance toward self-sustaining, continuously optimized software ecosystems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 70,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12485",
        "abs_url": "https://arxiv.org/abs/2507.12485",
        "pdf_url": "https://arxiv.org/pdf/2507.12485",
        "title": "Quantum Transfer Learning to Boost Dementia Detection",
        "authors": [
            "Sounak Bhowmik",
            "Talita Perciano",
            "Himanshu Thapliyal"
        ],
        "comments": "",
        "subjects": "Quantum Physics (quant-ph); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Dementia is a devastating condition with profound implications for individuals, families, and healthcare systems. Early and accurate detection of dementia is critical for timely intervention and improved patient outcomes. While classical machine learning and deep learning approaches have been explored extensively for dementia prediction, these solutions often struggle with high-dimensional biomedical data and large-scale datasets, quickly reaching computational and performance limitations. To address this challenge, quantum machine learning (QML) has emerged as a promising paradigm, offering faster training and advanced pattern recognition capabilities. This work aims to demonstrate the potential of quantum transfer learning (QTL) to enhance the performance of a weak classical deep learning model applied to a binary classification task for dementia detection. Besides, we show the effect of noise on the QTL-based approach, investigating the reliability and robustness of this method. Using the OASIS 2 dataset, we show how quantum techniques can transform a suboptimal classical model into a more effective solution for biomedical image classification, highlighting their potential impact on advancing healthcare technology.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 71,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12490",
        "abs_url": "https://arxiv.org/abs/2507.12490",
        "pdf_url": "https://arxiv.org/pdf/2507.12490",
        "title": "Spatially Grounded Explanations in Vision Language Models for Document Visual Question Answering",
        "authors": [
            "Maximiliano Hormazábal Lagos",
            "Héctor Cerezo-Costas",
            "Dimosthenis Karatzas"
        ],
        "comments": "This work has been accepted for presentation at the 16th Conference and Labs of the Evaluation Forum (CLEF 2025) and will be published in the proceedings by Springer in the Lecture Notes in Computer Science (LNCS) series. Please cite the published version when available",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "We introduce EaGERS, a fully training-free and model-agnostic pipeline that (1) generates natural language rationales via a vision language model, (2) grounds these rationales to spatial sub-regions by computing multimodal embedding similarities over a configurable grid with majority voting, and (3) restricts the generation of responses only from the relevant regions selected in the masked image. Experiments on the DocVQA dataset demonstrate that our best configuration not only outperforms the base model on exact match accuracy and Average Normalized Levenshtein Similarity metrics but also enhances transparency and reproducibility in DocVQA without additional model fine-tuning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 72,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12492",
        "abs_url": "https://arxiv.org/abs/2507.12492",
        "pdf_url": "https://arxiv.org/pdf/2507.12492",
        "title": "Sporadic Federated Learning Approach in Quantum Environment to Tackle Quantum Noise",
        "authors": [
            "Ratun Rahman",
            "Atit Pokharel",
            "Dinh C. Nguyen"
        ],
        "comments": "",
        "subjects": "Quantum Physics (quant-ph); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Quantum Federated Learning (QFL) is an emerging paradigm that combines quantum computing and federated learning (FL) to enable decentralized model training while maintaining data privacy over quantum networks. However, quantum noise remains a significant barrier in QFL, since modern quantum devices experience heterogeneous noise levels due to variances in hardware quality and sensitivity to quantum decoherence, resulting in inadequate training performance. To address this issue, we propose SpoQFL, a novel QFL framework that leverages sporadic learning to mitigate quantum noise heterogeneity in distributed quantum systems. SpoQFL dynamically adjusts training strategies based on noise fluctuations, enhancing model robustness, convergence stability, and overall learning efficiency. Extensive experiments on real-world datasets demonstrate that SpoQFL significantly outperforms conventional QFL approaches, achieving superior training performance and more stable convergence.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 73,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12496",
        "abs_url": "https://arxiv.org/abs/2507.12496",
        "pdf_url": "https://arxiv.org/pdf/2507.12496",
        "title": "FOUNDER: Grounding Foundation Models in World Models for Open-Ended Embodied Decision Making",
        "authors": [
            "Yucen Wang",
            "Rui Yu",
            "Shenghua Wan",
            "Le Gan",
            "De-Chuan Zhan"
        ],
        "comments": "Accepted by Forty-Second International Conference on Machine Learning (ICML 2025)",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Foundation Models (FMs) and World Models (WMs) offer complementary strengths in task generalization at different levels. In this work, we propose FOUNDER, a framework that integrates the generalizable knowledge embedded in FMs with the dynamic modeling capabilities of WMs to enable open-ended task solving in embodied environments in a reward-free manner. We learn a mapping function that grounds FM representations in the WM state space, effectively inferring the agent's physical states in the world simulator from external observations. This mapping enables the learning of a goal-conditioned policy through imagination during behavior learning, with the mapped task serving as the goal state. Our method leverages the predicted temporal distance to the goal state as an informative reward signal. FOUNDER demonstrates superior performance on various multi-task offline visual control benchmarks, excelling in capturing the deep-level semantics of tasks specified by text or videos, particularly in scenarios involving complex observations or domain gaps where prior methods struggle. The consistency of our learned reward function with the ground-truth reward is also empirically validated. Our project website is this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 74,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12497",
        "abs_url": "https://arxiv.org/abs/2507.12497",
        "pdf_url": "https://arxiv.org/pdf/2507.12497",
        "title": "Differentially Private Conformal Prediction via Quantile Binary Search",
        "authors": [
            "Ogonnaya M. Romanus",
            "Roberto Molinari"
        ],
        "comments": "",
        "subjects": "Methodology (stat.ME); Machine Learning (cs.LG); Applications (stat.AP); Computation (stat.CO); Machine Learning (stat.ML)",
        "abstract": "Most Differentially Private (DP) approaches focus on limiting privacy leakage from learners based on the data that they are trained on, there are fewer approaches that consider leakage when procedures involve a calibration dataset which is common in uncertainty quantification methods such as Conformal Prediction (CP). Since there is a limited amount of approaches in this direction, in this work we deliver a general DP approach for CP that we call Private Conformity via Quantile Search (P-COQS). The proposed approach adapts an existing randomized binary search algorithm for computing DP quantiles in the calibration phase of CP thereby guaranteeing privacy of the consequent prediction sets. This however comes at a price of slightly under-covering with respect to the desired $(1 - \\alpha)$-level when using finite-sample calibration sets (although broad empirical results show that the P-COQS generally targets the required level in the considered cases). Confirming properties of the adapted algorithm and quantifying the approximate coverage guarantees of the consequent CP, we conduct extensive experiments to examine the effects of privacy noise, sample size and significance level on the performance of our approach compared to existing alternatives. In addition, we empirically evaluate our approach on several benchmark datasets, including CIFAR-10, ImageNet and CoronaHack. Our results suggest that the proposed method is robust to privacy noise and performs favorably with respect to the current DP alternative in terms of empirical coverage, efficiency, and informativeness. Specifically, the results indicate that P-COQS produces smaller conformal prediction sets while simultaneously targeting the desired coverage and privacy guarantees in all these experimental settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 75,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12499",
        "abs_url": "https://arxiv.org/abs/2507.12499",
        "pdf_url": "https://arxiv.org/pdf/2507.12499",
        "title": "ReAL-AD: Towards Human-Like Reasoning in End-to-End Autonomous Driving",
        "authors": [
            "Yuhang Lu",
            "Jiadong Tu",
            "Yuexin Ma",
            "Xinge Zhu"
        ],
        "comments": "Accepted by ICCV2025",
        "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
        "abstract": "End-to-end autonomous driving has emerged as a promising approach to unify perception, prediction, and planning within a single framework, reducing information loss and improving adaptability. However, existing methods often rely on fixed and sparse trajectory supervision, limiting their ability to capture the hierarchical reasoning process that human drivers naturally employ. To bridge this gap, we propose ReAL-AD, a Reasoning-Augmented Learning framework that structures decision-making in autonomous driving based on the three-tier human cognitive model: Driving Strategy, Driving Decision, and Driving Operation, where Vision-Language Models (VLMs) are incorporated to enhance situational awareness and structured reasoning across these levels. Specifically, we introduce: (1) the Strategic Reasoning Injector, which formulates high-level driving strategies by interpreting complex traffic contexts from VLM-generated insights; (2) the Tactical Reasoning Integrator, which refines strategic intent into interpretable tactical choices such as lane changes, overtaking, and speed adjustments; and (3) the Hierarchical Trajectory Decoder, which progressively translates tactical decisions into precise control actions for smooth and human-like trajectory execution. Extensive evaluations show that integrating our framework improves planning accuracy and safety by over 30%, making end-to-end autonomous driving more interpretable and aligned with human-like hierarchical reasoning. The project page can be found at: \\href{this https URL}{\\texttt{this http URL\\_page/realad}}",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 76,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12503",
        "abs_url": "https://arxiv.org/abs/2507.12503",
        "pdf_url": "https://arxiv.org/pdf/2507.12503",
        "title": "Complex non-backtracking matrix for directed graphs",
        "authors": [
            "Keishi Sando",
            "Hideitsu Hino"
        ],
        "comments": "",
        "subjects": "Combinatorics (math.CO); Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Graph representation matrices are essential tools in graph data analysis. Recently, Hermitian adjacency matrices have been proposed to investigate directed graph structures. Previous studies have demonstrated that these matrices can extract valuable information for clustering. In this paper, we propose the complex non-backtracking matrix that integrates the properties of the Hermitian adjacency matrix and the non-backtracking matrix. The proposed matrix has similar properties with the non-backtracking matrix of undirected graphs. We reveal relationships between the complex non-backtracking matrix and the Hermitian adjacency matrix. Also, we provide intriguing insights that this matrix representation holds cluster information, particularly for sparse directed graphs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 77,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12562",
        "abs_url": "https://arxiv.org/abs/2507.12562",
        "pdf_url": "https://arxiv.org/pdf/2507.12562",
        "title": "Rel-HNN: Split Parallel Hypergraph Neural Network for Learning on Relational Databases",
        "authors": [
            "Md. Tanvir Alam",
            "Md. Ahasanul Alam",
            "Md Mahmudur Rahman",
            "Md. Mosaddek Khan"
        ],
        "comments": "",
        "subjects": "Databases (cs.DB); Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)",
        "abstract": "Relational databases (RDBs) are ubiquitous in enterprise and real-world applications. Flattening the database poses challenges for deep learning models that rely on fixed-size input representations to capture relational semantics from the structured nature of relational data. Graph neural networks (GNNs) have been proposed to address this, but they often oversimplify relational structures by modeling all the tuples as monolithic nodes and ignoring intra-tuple associations. In this work, we propose a novel hypergraph-based framework, that we call rel-HNN, which models each unique attribute-value pair as a node and each tuple as a hyperedge, enabling the capture of fine-grained intra-tuple relationships. Our approach learns explicit multi-level representations across attribute-value, tuple, and table levels. To address the scalability challenges posed by large RDBs, we further introduce a split-parallel training algorithm that leverages multi-GPU execution for efficient hypergraph learning. Extensive experiments on real-world and benchmark datasets demonstrate that rel-HNN significantly outperforms existing methods in both classification and regression tasks. Moreover, our split-parallel training achieves substantial speedups -- up to 3.18x for learning on relational data and up to 2.94x for hypergraph learning -- compared to conventional single-GPU execution.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 78,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12563",
        "abs_url": "https://arxiv.org/abs/2507.12563",
        "pdf_url": "https://arxiv.org/pdf/2507.12563",
        "title": "Evaluation of Neural Surrogates for Physical Modelling Synthesis of Nonlinear Elastic Plates",
        "authors": [
            "Carlos De La Vega Martin",
            "Rodrigo Diaz Fernandez",
            "Mark Sandler"
        ],
        "comments": "",
        "subjects": "Sound (cs.SD); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)",
        "abstract": "Physical modelling synthesis aims to generate audio from physical simulations of vibrating structures. Thin elastic plates are a common model for drum membranes. Traditional numerical methods like finite differences and finite elements offer high accuracy but are computationally demanding, limiting their use in real-time audio applications. This paper presents a comparative analysis of neural network-based approaches for solving the vibration of nonlinear elastic plates. We evaluate several state-of-the-art models, trained on short sequences, for prediction of long sequences in an autoregressive fashion. We show some of the limitations of these models, and why is not enough to look at the prediction error in the time domain. We discuss the implications for real-time audio synthesis and propose future directions for improving neural approaches to model nonlinear vibration.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 79,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12578",
        "abs_url": "https://arxiv.org/abs/2507.12578",
        "pdf_url": "https://arxiv.org/pdf/2507.12578",
        "title": "Deep Bilinear Koopman Model for Real-Time Vehicle Control in Frenet Frame",
        "authors": [
            "Mohammad Abtahi",
            "Farhang Motallebi Araghi",
            "Navid Mojahed",
            "Shima Nazari"
        ],
        "comments": "14 pages, 8 figures. This manuscript is under review with IEEE Transactions on Intelligent Vehicles",
        "subjects": "Systems and Control (eess.SY); Machine Learning (cs.LG); Robotics (cs.RO)",
        "abstract": "Accurate modeling and control of autonomous vehicles remain a fundamental challenge due to the nonlinear and coupled nature of vehicle dynamics. While Koopman operator theory offers a framework for deploying powerful linear control techniques, learning a finite-dimensional invariant subspace for high-fidelity modeling continues to be an open problem. This paper presents a deep Koopman approach for modeling and control of vehicle dynamics within the curvilinear Frenet frame. The proposed framework uses a deep neural network architecture to simultaneously learn the Koopman operator and its associated invariant subspace from the data. Input-state bilinear interactions are captured by the algorithm while preserving convexity, which makes it suitable for real-time model predictive control (MPC) application. A multi-step prediction loss is utilized during training to ensure long-horizon prediction capability. To further enhance real-time trajectory tracking performance, the model is integrated with a cumulative error regulator (CER) module, which compensates for model mismatch by mitigating accumulated prediction errors. Closed-loop performance is evaluated through hardware-in-the-loop (HIL) experiments using a CarSim RT model as the target plant, with real-time validation conducted on a dSPACE SCALEXIO system. The proposed controller achieved significant reductions in tracking error relative to baseline controllers, confirming its suitability for real-time implementation in embedded autonomous vehicle systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 80,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12590",
        "abs_url": "https://arxiv.org/abs/2507.12590",
        "pdf_url": "https://arxiv.org/pdf/2507.12590",
        "title": "Best Practices for Large-Scale, Pixel-Wise Crop Mapping and Transfer Learning Workflows",
        "authors": [
            "Judy Long",
            "Tao Liu",
            "Sean Alexander Woznicki",
            "Miljana Marković",
            "Oskar Marko",
            "Molly Sears"
        ],
        "comments": "A review article. 41 pages, 22 figures. Preprint",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Crop mapping involves identifying and classifying crop types using spatial data, primarily derived from remote sensing imagery. This study presents the first comprehensive review of large-scale, pixel-wise crop mapping workflows, encompassing both conventional supervised methods and emerging transfer learning approaches. To identify the optimal supervised crop mapping workflows, we conducted systematic experiments, comparing six widely adopted satellite image-based preprocessing methods, alongside eleven supervised pixel-wise classification models. Additionally, we assessed the synergistic impact of varied training sample sizes and variable combinations. Moreover, we identified optimal transfer learning techniques for different magnitudes of domain shift. The evaluation of best methods was conducted across five diverse agricultural sites. Landsat 8 served as the primary satellite data source. Labels come from CDL trusted pixels and field surveys. Our findings reveal three key insights. First, fine-scale interval preprocessing paired with Transformer models consistently delivered optimal performance for both supervised and transferable workflows. RF offered rapid training and competitive performance in conventional supervised learning and direct transfer to similar domains. Second, transfer learning techniques enhanced workflow adaptability, with UDA being effective for homogeneous crop classes while fine-tuning remains robust across diverse scenarios. Finally, workflow choice depends heavily on the availability of labeled samples. With a sufficient sample size, supervised training typically delivers more accurate and generalizable results. Below a certain threshold, transfer learning that matches the level of domain shift is a viable alternative to achieve crop mapping. Repository: Best-Practices-for-Large-Scale-Pixel-Wise-Crop-Mapping-and-Transfer-Learning-Workflows",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 81,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12599",
        "abs_url": "https://arxiv.org/abs/2507.12599",
        "pdf_url": "https://arxiv.org/pdf/2507.12599",
        "title": "A Survey of Explainable Reinforcement Learning: Targets, Methods and Needs",
        "authors": [
            "Léo Saulières"
        ],
        "comments": "69 pages, 19 figures",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "The success of recent Artificial Intelligence (AI) models has been accompanied by the opacity of their internal mechanisms, due notably to the use of deep neural networks. In order to understand these internal mechanisms and explain the output of these AI models, a set of methods have been proposed, grouped under the domain of eXplainable AI (XAI). This paper focuses on a sub-domain of XAI, called eXplainable Reinforcement Learning (XRL), which aims to explain the actions of an agent that has learned by reinforcement learning. We propose an intuitive taxonomy based on two questions \"What\" and \"How\". The first question focuses on the target that the method explains, while the second relates to the way the explanation is provided. We use this taxonomy to provide a state-of-the-art review of over 250 papers. In addition, we present a set of domains close to XRL, which we believe should get attention from the community. Finally, we identify some needs for the field of XRL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 82,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12644",
        "abs_url": "https://arxiv.org/abs/2507.12644",
        "pdf_url": "https://arxiv.org/pdf/2507.12644",
        "title": "VLMgineer: Vision Language Models as Robotic Toolsmiths",
        "authors": [
            "George Jiayuan Gao",
            "Tianyu Li",
            "Junyao Shi",
            "Yihan Li",
            "Zizhe Zhang",
            "Nadia Figueroa",
            "Dinesh Jayaraman"
        ],
        "comments": "Project Website: this https URL",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Tool design and use reflect the ability to understand and manipulate the physical world through creativity, planning, and foresight. As such, these capabilities are often regarded as measurable indicators of intelligence across biological species. While much of today's research on robotic intelligence focuses on generating better controllers, inventing smarter tools offers a complementary form of physical intelligence: shifting the onus of problem-solving onto the tool's design. Given the vast and impressive common-sense, reasoning, and creative capabilities of today's foundation models, we investigate whether these models can provide useful priors to automatically design and effectively wield such tools? We present VLMgineer, a framework that harnesses the code generation abilities of vision language models (VLMs) together with evolutionary search to iteratively co-design physical tools and the action plans that operate them to perform a task. We evaluate VLMgineer on a diverse new benchmark of everyday manipulation scenarios that demand creative tool design and use. Across this suite, VLMgineer consistently discovers tools and policies that solve tasks more effectively and innovatively, transforming challenging robotics problems into straightforward executions. It also outperforms VLM-generated designs from human specifications and existing human-crafted tools for everyday tasks. To facilitate future research on automated tool invention, we will release our benchmark and code.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 83,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12645",
        "abs_url": "https://arxiv.org/abs/2507.12645",
        "pdf_url": "https://arxiv.org/pdf/2507.12645",
        "title": "A Novel Data Augmentation Strategy for Robust Deep Learning Classification of Biomedical Time-Series Data: Application to ECG and EEG Analysis",
        "authors": [
            "Mohammed Guhdar",
            "Ramadhan J. Mstafa",
            "Abdulhakeem O. Mohammed"
        ],
        "comments": "",
        "subjects": "Signal Processing (eess.SP); Machine Learning (cs.LG)",
        "abstract": "The increasing need for accurate and unified analysis of diverse biological signals, such as ECG and EEG, is paramount for comprehensive patient assessment, especially in synchronous monitoring. Despite advances in multi-sensor fusion, a critical gap remains in developing unified architectures that effectively process and extract features from fundamentally different physiological signals. Another challenge is the inherent class imbalance in many biomedical datasets, often causing biased performance in traditional methods. This study addresses these issues by proposing a novel and unified deep learning framework that achieves state-of-the-art performance across different signal types. Our method integrates a ResNet-based CNN with an attention mechanism, enhanced by a novel data augmentation strategy: time-domain concatenation of multiple augmented variants of each signal to generate richer representations. Unlike prior work, we scientifically increase signal complexity to achieve future-reaching capabilities, which resulted in the best predictions compared to the state of the art. Preprocessing steps included wavelet denoising, baseline removal, and standardization. Class imbalance was effectively managed through the combined use of this advanced data augmentation and the Focal Loss function. Regularization techniques were applied during training to ensure generalization. We rigorously evaluated the proposed architecture on three benchmark datasets: UCI Seizure EEG, MIT-BIH Arrhythmia, and PTB Diagnostic ECG. It achieved accuracies of 99.96%, 99.78%, and 100%, respectively, demonstrating robustness across diverse signal types and clinical contexts. Finally, the architecture requires ~130 MB of memory and processes each sample in ~10 ms, suggesting suitability for deployment on low-end or wearable devices.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 84,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12657",
        "abs_url": "https://arxiv.org/abs/2507.12657",
        "pdf_url": "https://arxiv.org/pdf/2507.12657",
        "title": "Distributional Reinforcement Learning on Path-dependent Options",
        "authors": [
            "Ahmet Umur Özsoy"
        ],
        "comments": "",
        "subjects": "Mathematical Finance (q-fin.MF); Machine Learning (cs.LG)",
        "abstract": "We reinterpret and propose a framework for pricing path-dependent financial derivatives by estimating the full distribution of payoffs using Distributional Reinforcement Learning (DistRL). Unlike traditional methods that focus on expected option value, our approach models the entire conditional distribution of payoffs, allowing for risk-aware pricing, tail-risk estimation, and enhanced uncertainty quantification. We demonstrate the efficacy of this method on Asian options, using quantile-based value function approximators.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 85,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12661",
        "abs_url": "https://arxiv.org/abs/2507.12661",
        "pdf_url": "https://arxiv.org/pdf/2507.12661",
        "title": "Physics constrained learning of stochastic characteristics",
        "authors": [
            "Pardha Sai Krishna Ala",
            "Ameya Salvi",
            "Venkat Krovi",
            "Matthias Schmid"
        ],
        "comments": "6 pages, 6 figures",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Systems and Control (eess.SY)",
        "abstract": "Accurate state estimation requires careful consideration of uncertainty surrounding the process and measurement models; these characteristics are usually not well-known and need an experienced designer to select the covariance matrices. An error in the selection of covariance matrices could impact the accuracy of the estimation algorithm and may sometimes cause the filter to diverge. Identifying noise characteristics has long been a challenging problem due to uncertainty surrounding noise sources and difficulties in systematic noise modeling. Most existing approaches try identifying unknown covariance matrices through an optimization algorithm involving innovation sequences. In recent years, learning approaches have been utilized to determine the stochastic characteristics of process and measurement models. We present a learning-based methodology with different loss functions to identify noise characteristics and test these approaches' performance for real-time vehicle state estimation",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 86,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12666",
        "abs_url": "https://arxiv.org/abs/2507.12666",
        "pdf_url": "https://arxiv.org/pdf/2507.12666",
        "title": "Fly, Fail, Fix: Iterative Game Repair with Reinforcement Learning and Large Multimodal Models",
        "authors": [
            "Alex Zook",
            "Josef Spjut",
            "Jonathan Tremblay"
        ],
        "comments": "Published at Reinforcement Learning and Video Games workshop this https URL",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Game design hinges on understanding how static rules and content translate into dynamic player behavior - something modern generative systems that inspect only a game's code or assets struggle to capture. We present an automated design iteration framework that closes this gap by pairing a reinforcement learning (RL) agent, which playtests the game, with a large multimodal model (LMM), which revises the game based on what the agent does. In each loop the RL player completes several episodes, producing (i) numerical play metrics and/or (ii) a compact image strip summarising recent video frames. The LMM designer receives a gameplay goal and the current game configuration, analyses the play traces, and edits the configuration to steer future behaviour toward the goal. We demonstrate results that LMMs can reason over behavioral traces supplied by RL agents to iteratively refine game mechanics, pointing toward practical, scalable tools for AI-assisted game design.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 87,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12686",
        "abs_url": "https://arxiv.org/abs/2507.12686",
        "pdf_url": "https://arxiv.org/pdf/2507.12686",
        "title": "Finite-Dimensional Gaussian Approximation for Deep Neural Networks: Universality in Random Weights",
        "authors": [
            "Krishnakumar Balasubramanian",
            "Nathan Ross"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Probability (math.PR); Statistics Theory (math.ST)",
        "abstract": "We study the Finite-Dimensional Distributions (FDDs) of deep neural networks with randomly initialized weights that have finite-order moments. Specifically, we establish Gaussian approximation bounds in the Wasserstein-$1$ norm between the FDDs and their Gaussian limit assuming a Lipschitz activation function and allowing the layer widths to grow to infinity at arbitrary relative rates. In the special case where all widths are proportional to a common scale parameter $n$ and there are $L-1$ hidden layers, we obtain convergence rates of order $n^{-({1}/{6})^{L-1} + \\epsilon}$, for any $\\epsilon > 0$.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 88,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12691",
        "abs_url": "https://arxiv.org/abs/2507.12691",
        "pdf_url": "https://arxiv.org/pdf/2507.12691",
        "title": "Benchmarking Deception Probes via Black-to-White Performance Boosts",
        "authors": [
            "Avi Parrack",
            "Carlo Leonardo Attubato",
            "Stefan Heimersheim"
        ],
        "comments": "Preprint. 37 pages, 10 figures, 7 tables",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "AI assistants will occasionally respond deceptively to user queries. Recently, linear classifiers (called \"deception probes\") have been trained to distinguish the internal activations of a language model during deceptive versus honest responses. However, it's unclear how effective these probes are at detecting deception in practice, nor whether such probes are resistant to simple counter strategies from a deceptive assistant who wishes to evade detection. In this paper, we compare white-box monitoring (where the monitor has access to token-level probe activations) to black-box monitoring (without such access). We benchmark deception probes by the extent to which the white box monitor outperforms the black-box monitor, i.e. the black-to-white performance boost. We find weak but encouraging black-to-white performance boosts from existing deception probes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 89,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12755",
        "abs_url": "https://arxiv.org/abs/2507.12755",
        "pdf_url": "https://arxiv.org/pdf/2507.12755",
        "title": "Domain-Enhanced Dual-Branch Model for Efficient and Interpretable Accident Anticipation",
        "authors": [
            "Yanchen Guan",
            "Haicheng Liao",
            "Chengyue Wang",
            "Bonan Wang",
            "Jiaxun Zhang",
            "Jia Hu",
            "Zhenning Li"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Developing precise and computationally efficient traffic accident anticipation system is crucial for contemporary autonomous driving technologies, enabling timely intervention and loss prevention. In this paper, we propose an accident anticipation framework employing a dual-branch architecture that effectively integrates visual information from dashcam videos with structured textual data derived from accident reports. Furthermore, we introduce a feature aggregation method that facilitates seamless integration of multimodal inputs through large models (GPT-4o, Long-CLIP), complemented by targeted prompt engineering strategies to produce actionable feedback and standardized accident archives. Comprehensive evaluations conducted on benchmark datasets (DAD, CCD, and A3D) validate the superior predictive accuracy, enhanced responsiveness, reduced computational overhead, and improved interpretability of our approach, thus establishing a new benchmark for state-of-the-art performance in traffic accident anticipation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 90,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12762",
        "abs_url": "https://arxiv.org/abs/2507.12762",
        "pdf_url": "https://arxiv.org/pdf/2507.12762",
        "title": "World Model-Based End-to-End Scene Generation for Accident Anticipation in Autonomous Driving",
        "authors": [
            "Yanchen Guan",
            "Haicheng Liao",
            "Chengyue Wang",
            "Xingcheng Liu",
            "Jiaxun Zhang",
            "Zhenning Li"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Reliable anticipation of traffic accidents is essential for advancing autonomous driving systems. However, this objective is limited by two fundamental challenges: the scarcity of diverse, high-quality training data and the frequent absence of crucial object-level cues due to environmental disruptions or sensor deficiencies. To tackle these issues, we propose a comprehensive framework combining generative scene augmentation with adaptive temporal reasoning. Specifically, we develop a video generation pipeline that utilizes a world model guided by domain-informed prompts to create high-resolution, statistically consistent driving scenarios, particularly enriching the coverage of edge cases and complex interactions. In parallel, we construct a dynamic prediction model that encodes spatio-temporal relationships through strengthened graph convolutions and dilated temporal operators, effectively addressing data incompleteness and transient visual noise. Furthermore, we release a new benchmark dataset designed to better capture diverse real-world driving risks. Extensive experiments on public and newly released datasets confirm that our framework enhances both the accuracy and lead time of accident anticipation, offering a robust solution to current data and modeling limitations in safety-critical autonomous driving applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 91,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12768",
        "abs_url": "https://arxiv.org/abs/2507.12768",
        "pdf_url": "https://arxiv.org/pdf/2507.12768",
        "title": "AnyPos: Automated Task-Agnostic Actions for Bimanual Manipulation",
        "authors": [
            "Hengkai Tan",
            "Yao Feng",
            "Xinyi Mao",
            "Shuhe Huang",
            "Guodong Liu",
            "Zhongkai Hao",
            "Hang Su",
            "Jun Zhu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Robotics (cs.RO)",
        "abstract": "Vision-language-action (VLA) models have shown promise on task-conditioned control in complex settings such as bimanual manipulation. However, the heavy reliance on task-specific human demonstrations limits their generalization and incurs high data acquisition costs. In this work, we present a new notion of task-agnostic action paradigm that decouples action execution from task-specific conditioning, enhancing scalability, efficiency, and cost-effectiveness. To address the data collection challenges posed by this paradigm -- such as low coverage density, behavioral redundancy, and safety risks -- we introduce ATARA (Automated Task-Agnostic Random Actions), a scalable self-supervised framework that accelerates collection by over $ 30\\times $ compared to human teleoperation. To further enable effective learning from task-agnostic data, which often suffers from distribution mismatch and irrelevant trajectories, we propose AnyPos, an inverse dynamics model equipped with Arm-Decoupled Estimation and a Direction-Aware Decoder (DAD). We additionally integrate a video-conditioned action validation module to verify the feasibility of learned policies across diverse manipulation tasks. Extensive experiments show that the AnyPos-ATARA pipeline yields a 51% improvement in test accuracy and achieves 30-40% higher success rates in downstream tasks such as lifting, pick-and-place, and clicking, using replay-based video validation. Project Page: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 92,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12773",
        "abs_url": "https://arxiv.org/abs/2507.12773",
        "pdf_url": "https://arxiv.org/pdf/2507.12773",
        "title": "Sample-Constrained Black Box Optimization for Audio Personalization",
        "authors": [
            "Rajalaxmi Rajagopalan",
            "Yu-Lin Wei",
            "Romit Roy Choudhury"
        ],
        "comments": "Published in AAAI 2024",
        "subjects": "Sound (cs.SD); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)",
        "abstract": "We consider the problem of personalizing audio to maximize user experience. Briefly, we aim to find a filter $h^*$, which applied to any music or speech, will maximize the user's satisfaction. This is a black-box optimization problem since the user's satisfaction function is unknown. Substantive work has been done on this topic where the key idea is to play audio samples to the user, each shaped by a different filter $h_i$, and query the user for their satisfaction scores $f(h_i)$. A family of ``surrogate\" functions is then designed to fit these scores and the optimization method gradually refines these functions to arrive at the filter $\\hat{h}^*$ that maximizes satisfaction. In certain applications, we observe that a second type of querying is possible where users can tell us the individual elements $h^*[j]$ of the optimal filter $h^*$. Consider an analogy from cooking where the goal is to cook a recipe that maximizes user satisfaction. A user can be asked to score various cooked recipes (e.g., tofu fried rice) or to score individual ingredients (say, salt, sugar, rice, chicken, etc.). Given a budget of $B$ queries, where a query can be of either type, our goal is to find the recipe that will maximize this user's satisfaction. Our proposal builds on Sparse Gaussian Process Regression (GPR) and shows how a hybrid approach can outperform any one type of querying. Our results are validated through simulations and real world experiments, where volunteers gave feedback on music/speech audio and were able to achieve high satisfaction levels. We believe this idea of hybrid querying opens new problems in black-box optimization and solutions can benefit other applications beyond audio personalization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 93,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12780",
        "abs_url": "https://arxiv.org/abs/2507.12780",
        "pdf_url": "https://arxiv.org/pdf/2507.12780",
        "title": "Compact Vision Transformer by Reduction of Kernel Complexity",
        "authors": [
            "Yancheng Wang",
            "Yingzhen Yang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Self-attention and transformer architectures have become foundational components in modern deep learning. Recent efforts have integrated transformer blocks into compact neural architectures for computer vision, giving rise to various efficient vision transformers. In this work, we introduce Transformer with Kernel Complexity Reduction, or KCR-Transformer, a compact transformer block equipped with differentiable channel selection, guided by a novel and sharp theoretical generalization bound. KCR-Transformer performs input/output channel selection in the MLP layers of transformer blocks to reduce the computational cost. Furthermore, we provide a rigorous theoretical analysis establishing a tight generalization bound for networks equipped with KCR-Transformer blocks. Leveraging such strong theoretical results, the channel pruning by KCR-Transformer is conducted in a generalization-aware manner, ensuring that the resulting network retains a provably small generalization error. Our KCR-Transformer is compatible with many popular and compact transformer networks, such as ViT and Swin, and it reduces the FLOPs of the vision transformers while maintaining or even improving the prediction accuracy. In the experiments, we replace all the transformer blocks in the vision transformers with KCR-Transformer blocks, leading to KCR-Transformer networks with different backbones. The resulting TCR-Transformers achieve superior performance on various computer vision tasks, achieving even better performance than the original models with even less FLOPs and parameters.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 94,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12808",
        "abs_url": "https://arxiv.org/abs/2507.12808",
        "pdf_url": "https://arxiv.org/pdf/2507.12808",
        "title": "Large Language Models' Internal Perception of Symbolic Music",
        "authors": [
            "Andrew Shin",
            "Kunitake Kaneko"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS)",
        "abstract": "Large language models (LLMs) excel at modeling relationships between strings in natural language and have shown promise in extending to other symbolic domains like coding or mathematics. However, the extent to which they implicitly model symbolic music remains underexplored. This paper investigates how LLMs represent musical concepts by generating symbolic music data from textual prompts describing combinations of genres and styles, and evaluating their utility through recognition and generation tasks. We produce a dataset of LLM-generated MIDI files without relying on explicit musical training. We then train neural networks entirely on this LLM-generated MIDI dataset and perform genre and style classification as well as melody completion, benchmarking their performance against established models. Our results demonstrate that LLMs can infer rudimentary musical structures and temporal relationships from text, highlighting both their potential to implicitly encode musical patterns and their limitations due to a lack of explicit musical context, shedding light on their generative capabilities for symbolic music.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 95,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12818",
        "abs_url": "https://arxiv.org/abs/2507.12818",
        "pdf_url": "https://arxiv.org/pdf/2507.12818",
        "title": "Self Balancing Neural Network: A Novel Method to Estimate Average Treatment Effect",
        "authors": [
            "Atomsa Gemechu Abdisa",
            "Yingchun Zhou",
            "Yuqi Qiu"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "In observational studies, confounding variables affect both treatment and outcome. Moreover, instrumental variables also influence the treatment assignment mechanism. This situation sets the study apart from a standard randomized controlled trial, where the treatment assignment is random. Due to this situation, the estimated average treatment effect becomes biased. To address this issue, a standard approach is to incorporate the estimated propensity score when estimating the average treatment effect. However, these methods incur the risk of misspecification in propensity score models. To solve this issue, a novel method called the \"Self balancing neural network\" (Sbnet), which lets the model itself obtain its pseudo propensity score from the balancing net, is proposed in this study. The proposed method estimates the average treatment effect by using the balancing net as a key part of the feedforward neural network. This formulation resolves the estimation of the average treatment effect in one step. Moreover, the multi-pseudo propensity score framework, which is estimated from the diversified balancing net and used for the estimation of the average treatment effect, is presented. Finally, the proposed methods are compared with state-of-the-art methods on three simulation setups and real-world datasets. It has been shown that the proposed self-balancing neural network shows better performance than state-of-the-art methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 96,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12821",
        "abs_url": "https://arxiv.org/abs/2507.12821",
        "pdf_url": "https://arxiv.org/pdf/2507.12821",
        "title": "Assessing adaptive world models in machines with novel games",
        "authors": [
            "Lance Ying",
            "Katherine M. Collins",
            "Prafull Sharma",
            "Cedric Colas",
            "Kaiya Ivy Zhao",
            "Adrian Weller",
            "Zenna Tavares",
            "Phillip Isola",
            "Samuel J. Gershman",
            "Jacob D. Andreas",
            "Thomas L. Griffiths",
            "Francois Chollet",
            "Kelsey R. Allen",
            "Joshua B. Tenenbaum"
        ],
        "comments": "17 pages, 4 figures",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Human intelligence exhibits a remarkable capacity for rapid adaptation and effective problem-solving in novel and unfamiliar contexts. We argue that this profound adaptability is fundamentally linked to the efficient construction and refinement of internal representations of the environment, commonly referred to as world models, and we refer to this adaptation mechanism as world model induction. However, current understanding and evaluation of world models in artificial intelligence (AI) remains narrow, often focusing on static representations learned from training on a massive corpora of data, instead of the efficiency and efficacy of models in learning these representations through interaction and exploration within a novel environment. In this Perspective, we provide a view of world model induction drawing on decades of research in cognitive science on how humans learn and adapt so efficiently; we then call for a new evaluation framework for assessing adaptive world models in AI. Concretely, we propose a new benchmarking paradigm based on suites of carefully designed games with genuine, deep and continually refreshing novelty in the underlying game structures -- we refer to this kind of games as novel games. We detail key desiderata for constructing these games and propose appropriate metrics to explicitly challenge and evaluate the agent's ability for rapid world model induction. We hope that this new evaluation framework will inspire future evaluation efforts on world models in AI and provide a crucial step towards developing AI systems capable of the human-like rapid adaptation and robust generalization -- a critical component of artificial general intelligence.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 97,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12825",
        "abs_url": "https://arxiv.org/abs/2507.12825",
        "pdf_url": "https://arxiv.org/pdf/2507.12825",
        "title": "Autoregressive Speech Enhancement via Acoustic Tokens",
        "authors": [
            "Luca Della Libera",
            "Cem Subakan",
            "Mirco Ravanelli"
        ],
        "comments": "5 pages, 2 figures",
        "subjects": "Sound (cs.SD); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)",
        "abstract": "In speech processing pipelines, improving the quality and intelligibility of real-world recordings is crucial. While supervised regression is the primary method for speech enhancement, audio tokenization is emerging as a promising alternative for a smooth integration with other modalities. However, research on speech enhancement using discrete representations is still limited. Previous work has mainly focused on semantic tokens, which tend to discard key acoustic details such as speaker identity. Additionally, these studies typically employ non-autoregressive models, assuming conditional independence of outputs and overlooking the potential improvements offered by autoregressive modeling. To address these gaps we: 1) conduct a comprehensive study of the performance of acoustic tokens for speech enhancement, including the effect of bitrate and noise strength; 2) introduce a novel transducer-based autoregressive architecture specifically designed for this task. Experiments on VoiceBank and Libri1Mix datasets show that acoustic tokens outperform semantic tokens in terms of preserving speaker identity, and that our autoregressive approach can further improve performance. Nevertheless, we observe that discrete representations still fall short compared to continuous ones, highlighting the need for further research in this area.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 98,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12832",
        "abs_url": "https://arxiv.org/abs/2507.12832",
        "pdf_url": "https://arxiv.org/pdf/2507.12832",
        "title": "MVA 2025 Small Multi-Object Tracking for Spotting Birds Challenge: Dataset, Methods, and Results",
        "authors": [
            "Yuki Kondo",
            "Norimichi Ukita",
            "Riku Kanayama",
            "Yuki Yoshida",
            "Takayuki Yamaguchi",
            "Xiang Yu",
            "Guang Liang",
            "Xinyao Liu",
            "Guan-Zhang Wang",
            "Wei-Ta Chu",
            "Bing-Cheng Chuang",
            "Jia-Hua Lee",
            "Pin-Tseng Kuo",
            "I-Hsuan Chu",
            "Yi-Shein Hsiao",
            "Cheng-Han Wu",
            "Po-Yi Wu",
            "Jui-Chien Tsou",
            "Hsuan-Chi Liu",
            "Chun-Yi Lee",
            "Yuan-Fu Yang",
            "Kosuke Shigematsu",
            "Asuka Shin",
            "Ba Tran"
        ],
        "comments": "This paper is the official challenge report for SMOT4SB and is published in the proceedings of MVA 2025 (19th International Conference on Machine Vision and Applications). Official challenge page: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Small Multi-Object Tracking (SMOT) is particularly challenging when targets occupy only a few dozen pixels, rendering detection and appearance-based association unreliable. Building on the success of the MVA2023 SOD4SB challenge, this paper introduces the SMOT4SB challenge, which leverages temporal information to address limitations of single-frame detection. Our three main contributions are: (1) the SMOT4SB dataset, consisting of 211 UAV video sequences with 108,192 annotated frames under diverse real-world conditions, designed to capture motion entanglement where both camera and targets move freely in 3D; (2) SO-HOTA, a novel metric combining Dot Distance with HOTA to mitigate the sensitivity of IoU-based metrics to small displacements; and (3) a competitive MVA2025 challenge with 78 participants and 308 submissions, where the winning method achieved a 5.1x improvement over the baseline. This work lays a foundation for advancing SMOT in UAV scenarios with applications in bird strike avoidance, agriculture, fisheries, and ecological monitoring.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 99,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12840",
        "abs_url": "https://arxiv.org/abs/2507.12840",
        "pdf_url": "https://arxiv.org/pdf/2507.12840",
        "title": "Bridging the Gap: Leveraging Retrieval-Augmented Generation to Better Understand Public Concerns about Vaccines",
        "authors": [
            "Muhammad Javed",
            "Sedigh Khademi Habibabadi",
            "Christopher Palmer",
            "Hazel Clothier",
            "Jim Buttery",
            "Gerardo Luis Dimaguila"
        ],
        "comments": "",
        "subjects": "Information Retrieval (cs.IR); Machine Learning (cs.LG); Social and Information Networks (cs.SI)",
        "abstract": "Vaccine hesitancy threatens public health, leading to delayed or rejected vaccines. Social media is a vital source for understanding public concerns, and traditional methods like topic modelling often struggle to capture nuanced opinions. Though trained for query answering, large Language Models (LLMs) often miss current events and community concerns. Additionally, hallucinations in LLMs can compromise public health communication. To address these limitations, we developed a tool (VaxPulse Query Corner) using the Retrieval Augmented Generation technique. It addresses complex queries about public vaccine concerns on various online platforms, aiding public health administrators and stakeholders in understanding public concerns and implementing targeted interventions to boost vaccine confidence. Analysing 35,103 Shingrix social media posts, it achieved answer faithfulness (0.96) and relevance (0.94).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 100,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12869",
        "abs_url": "https://arxiv.org/abs/2507.12869",
        "pdf_url": "https://arxiv.org/pdf/2507.12869",
        "title": "WhoFi: Deep Person Re-Identification via Wi-Fi Channel Signal Encoding",
        "authors": [
            "Danilo Avola",
            "Daniele Pannone",
            "Dario Montagnini",
            "Emad Emam"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Person Re-Identification is a key and challenging task in video surveillance. While traditional methods rely on visual data, issues like poor lighting, occlusion, and suboptimal angles often hinder performance. To address these challenges, we introduce WhoFi, a novel pipeline that utilizes Wi-Fi signals for person re-identification. Biometric features are extracted from Channel State Information (CSI) and processed through a modular Deep Neural Network (DNN) featuring a Transformer-based encoder. The network is trained using an in-batch negative loss function to learn robust and generalizable biometric signatures. Experiments on the NTU-Fi dataset show that our approach achieves competitive results compared to state-of-the-art methods, confirming its effectiveness in identifying individuals via Wi-Fi signals.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 101,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12878",
        "abs_url": "https://arxiv.org/abs/2507.12878",
        "pdf_url": "https://arxiv.org/pdf/2507.12878",
        "title": "Bayesian Modeling and Estimation of Linear Time-Variant Systems using Neural Networks and Gaussian Processes",
        "authors": [
            "Yaniv Shulman"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "The identification of Linear Time-Variant (LTV) systems from input-output data is a fundamental yet challenging ill-posed inverse problem. This work introduces a unified Bayesian framework that models the system's impulse response, $h(t, \\tau)$, as a stochastic process. We decompose the response into a posterior mean and a random fluctuation term, a formulation that provides a principled approach for quantifying uncertainty and naturally defines a new, useful system class we term Linear Time-Invariant in Expectation (LTIE). To perform inference, we leverage modern machine learning techniques, including Bayesian neural networks and Gaussian Processes, using scalable variational inference. We demonstrate through a series of experiments that our framework can robustly infer the properties of an LTI system from a single noisy observation, show superior data efficiency compared to classical methods in a simulated ambient noise tomography problem, and successfully track a continuously varying LTV impulse response by using a structured Gaussian Process prior. This work provides a flexible and robust methodology for uncertainty-aware system identification in dynamic environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 102,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12879",
        "abs_url": "https://arxiv.org/abs/2507.12879",
        "pdf_url": "https://arxiv.org/pdf/2507.12879",
        "title": "Autonomous Resource Management in Microservice Systems via Reinforcement Learning",
        "authors": [
            "Yujun Zou",
            "Nia Qi",
            "Yingnan Deng",
            "Zhihao Xue",
            "Ming Gong",
            "Wuyang Zhang"
        ],
        "comments": "",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)",
        "abstract": "This paper proposes a reinforcement learning-based method for microservice resource scheduling and optimization, aiming to address issues such as uneven resource allocation, high latency, and insufficient throughput in traditional microservice architectures. In microservice systems, as the number of services and the load increase, efficiently scheduling and allocating resources such as computing power, memory, and storage becomes a critical research challenge. To address this, the paper employs an intelligent scheduling algorithm based on reinforcement learning. Through the interaction between the agent and the environment, the resource allocation strategy is continuously optimized. In the experiments, the paper considers different resource conditions and load scenarios, evaluating the proposed method across multiple dimensions, including response time, throughput, resource utilization, and cost efficiency. The experimental results show that the reinforcement learning-based scheduling method significantly improves system response speed and throughput under low load and high concurrency conditions, while also optimizing resource utilization and reducing energy consumption. Under multi-dimensional resource conditions, the proposed method can consider multiple objectives and achieve optimized resource scheduling. Compared to traditional static resource allocation methods, the reinforcement learning model demonstrates stronger adaptability and optimization capability. It can adjust resource allocation strategies in real time, thereby maintaining good system performance in dynamically changing load and resource environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 103,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12885",
        "abs_url": "https://arxiv.org/abs/2507.12885",
        "pdf_url": "https://arxiv.org/pdf/2507.12885",
        "title": "VAR-MATH: Probing True Mathematical Reasoning in Large Language Models via Symbolic Multi-Instance Benchmarks",
        "authors": [
            "Jian Yao",
            "Ran Cheng",
            "Kay Chen Tan"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Recent advances in reinforcement learning (RL) have led to substantial improvements in the mathematical reasoning abilities of large language models (LLMs), as measured by standard benchmarks. However, these gains often persist even when models are trained with flawed signals, such as random or inverted rewards, raising a fundamental question: do such improvements reflect true reasoning, or are they merely artifacts of overfitting to benchmark-specific patterns? To address this question, we take an evaluation-centric perspective and identify two critical shortcomings in existing protocols. First, \\emph{benchmark contamination} arises from the public availability of test problems, increasing the risk of data leakage. Second, \\emph{evaluation fragility} stems from the reliance on single-instance assessments, which are highly sensitive to stochastic outputs and fail to capture reasoning consistency. To overcome these limitations, we introduce {VAR-MATH}, a symbolic evaluation framework designed to probe genuine reasoning ability. By converting fixed numerical problems into symbolic templates and requiring models to solve multiple instantiations of each, VAR-MATH enforces consistent reasoning across structurally equivalent variants, thereby mitigating contamination and improving evaluation robustness. We apply VAR-MATH to transform two popular benchmarks, AMC23 and AIME24, into their symbolic counterparts, VAR-AMC23 and VAR-AIME24. Experimental results reveal substantial performance drops for RL-trained models on the variabilized versions, especially for smaller models, with average declines of 48.0\\% on AMC23 and 58.3\\% on AIME24. These findings suggest that many existing RL methods rely on superficial heuristics and fail to generalize beyond specific numerical forms. Overall, VAR-MATH offers a principled, contamination-resistant evaluation paradigm for mathematical reasoning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 104,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12911",
        "abs_url": "https://arxiv.org/abs/2507.12911",
        "pdf_url": "https://arxiv.org/pdf/2507.12911",
        "title": "LaViPlan : Language-Guided Visual Path Planning with RLVR",
        "authors": [
            "Hayeon Oh"
        ],
        "comments": "11 pages, 6 figures",
        "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
        "abstract": "Out-of-distribution (OOD) scenarios in autonomous driving refer to situations that deviate from the training domain, often leading to unexpected and potentially hazardous behavior from planners that lack prior exposure to such cases. Recently, Vision-Language Models (VLMs) have been introduced into autonomous driving research for their promising generalization capabilities in OOD settings. Early studies demonstrated that VLMs could recognize OOD scenarios and generate user-level decisions such as \"go straight\" or \"turn right.\" However, a new challenge has emerged due to the misalignment between the VLM's high-level decisions or visual reasoning expressed in language, and the low-level predicted trajectories interpreted as actions. In this paper, we propose LaViPlan, a framework that leverages Reinforcement Learning with Verifiable Rewards (RLVR) to optimize VLMs using planning-oriented metrics. This approach addresses the vision-language-action misalignment observed in existing VLMs fine-tuned via supervised learning, which can recognize driving scenarios but often produce context-unaware decisions. Experimental results demonstrate that our method improves situational awareness and decision-making under OOD conditions, highlighting its potential to mitigate the misalignment issue. This work introduces a promising post-training paradigm for VLM agents in the context of autonomous driving.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 105,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12933",
        "abs_url": "https://arxiv.org/abs/2507.12933",
        "pdf_url": "https://arxiv.org/pdf/2507.12933",
        "title": "DMQ: Dissecting Outliers of Diffusion Models for Post-Training Quantization",
        "authors": [
            "Dongyeun Lee",
            "Jiwan Hur",
            "Hyounguk Shon",
            "Jae Young Lee",
            "Junmo Kim"
        ],
        "comments": "Accepted by ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Diffusion models have achieved remarkable success in image generation but come with significant computational costs, posing challenges for deployment in resource-constrained environments. Recent post-training quantization (PTQ) methods have attempted to mitigate this issue by focusing on the iterative nature of diffusion models. However, these approaches often overlook outliers, leading to degraded performance at low bit-widths. In this paper, we propose a DMQ which combines Learned Equivalent Scaling (LES) and channel-wise Power-of-Two Scaling (PTS) to effectively address these challenges. Learned Equivalent Scaling optimizes channel-wise scaling factors to redistribute quantization difficulty between weights and activations, reducing overall quantization error. Recognizing that early denoising steps, despite having small quantization errors, crucially impact the final output due to error accumulation, we incorporate an adaptive timestep weighting scheme to prioritize these critical steps during learning. Furthermore, identifying that layers such as skip connections exhibit high inter-channel variance, we introduce channel-wise Power-of-Two Scaling for activations. To ensure robust selection of PTS factors even with small calibration set, we introduce a voting algorithm that enhances reliability. Extensive experiments demonstrate that our method significantly outperforms existing works, especially at low bit-widths such as W4A6 (4-bit weight, 6-bit activation) and W4A8, maintaining high image generation quality and model stability. The code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 106,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12953",
        "abs_url": "https://arxiv.org/abs/2507.12953",
        "pdf_url": "https://arxiv.org/pdf/2507.12953",
        "title": "cIDIR: Conditioned Implicit Neural Representation for Regularized Deformable Image Registration",
        "authors": [
            "Sidaty El Hadramy",
            "Oumeymah Cherkaoui",
            "Philippe C. Cattin"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Regularization is essential in deformable image registration (DIR) to ensure that the estimated Deformation Vector Field (DVF) remains smooth, physically plausible, and anatomically consistent. However, fine-tuning regularization parameters in learning-based DIR frameworks is computationally expensive, often requiring multiple training iterations. To address this, we propose cIDI, a novel DIR framework based on Implicit Neural Representations (INRs) that conditions the registration process on regularization hyperparameters. Unlike conventional methods that require retraining for each regularization hyperparameter setting, cIDIR is trained over a prior distribution of these hyperparameters, then optimized over the regularization hyperparameters by using the segmentations masks as an observation. Additionally, cIDIR models a continuous and differentiable DVF, enabling seamless integration of advanced regularization techniques via automatic differentiation. Evaluated on the DIR-LAB dataset, $\\operatorname{cIDIR}$ achieves high accuracy and robustness across the dataset.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 107,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12964",
        "abs_url": "https://arxiv.org/abs/2507.12964",
        "pdf_url": "https://arxiv.org/pdf/2507.12964",
        "title": "Demographic-aware fine-grained classification of pediatric wrist fractures",
        "authors": [
            "Ammar Ahmed",
            "Ali Shariq Imran",
            "Zenun Kastrati",
            "Sher Muhammad Daudpota"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Wrist pathologies are frequently observed, particularly among children who constitute the majority of fracture cases. However, diagnosing these conditions is time-consuming and requires specialized expertise. Computer vision presents a promising avenue, contingent upon the availability of extensive datasets, a notable challenge in medical imaging. Therefore, reliance solely on one modality, such as images, proves inadequate, especially in an era of diverse and plentiful data types. In this study, we employ a multifaceted approach to address the challenge of recognizing wrist pathologies using an extremely limited dataset. Initially, we approach the problem as a fine-grained recognition task, aiming to identify subtle X-ray pathologies that conventional CNNs overlook. Secondly, we enhance network performance by fusing patient metadata with X-ray images. Thirdly, rather than pre-training on a coarse-grained dataset like ImageNet, we utilize weights trained on a fine-grained dataset. While metadata integration has been used in other medical domains, this is a novel application for wrist pathologies. Our results show that a fine-grained strategy and metadata integration improve diagnostic accuracy by 2% with a limited dataset and by over 10% with a larger fracture-focused dataset.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 108,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12966",
        "abs_url": "https://arxiv.org/abs/2507.12966",
        "pdf_url": "https://arxiv.org/pdf/2507.12966",
        "title": "Investigating Forecasting Models for Pandemic Infections Using Heterogeneous Data Sources: A 2-year Study with COVID-19",
        "authors": [
            "Zacharias Komodromos",
            "Kleanthis Malialis",
            "Panayiotis Kolios"
        ],
        "comments": "Keywords: epidemiology, pandemic forecasting, COVID-19, infections, machine learning Accepted: IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB) 2025",
        "subjects": "Populations and Evolution (q-bio.PE); Machine Learning (cs.LG)",
        "abstract": "Emerging in December 2019, the COVID-19 pandemic caused widespread health, economic, and social disruptions. Rapid global transmission overwhelmed healthcare systems, resulting in high infection rates, hospitalisations, and fatalities. To minimise the spread, governments implemented several non-pharmaceutical interventions like lockdowns and travel restrictions. While effective in controlling transmission, these measures also posed significant economic and societal challenges. Although the WHO declared COVID-19 no longer a global health emergency in May 2023, its impact persists, shaping public health strategies. The vast amount of data collected during the pandemic offers valuable insights into disease dynamics, transmission, and intervention effectiveness. Leveraging these insights can improve forecasting models, enhancing preparedness and response to future outbreaks while mitigating their social and economic impact. This paper presents a large-scale case study on COVID-19 forecasting in Cyprus, utilising a two-year dataset that integrates epidemiological data, vaccination records, policy measures, and weather conditions. We analyse infection trends, assess forecasting performance, and examine the influence of external factors on disease dynamics. The insights gained contribute to improved pandemic preparedness and response strategies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 109,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12988",
        "abs_url": "https://arxiv.org/abs/2507.12988",
        "pdf_url": "https://arxiv.org/pdf/2507.12988",
        "title": "Variance-Based Pruning for Accelerating and Compressing Trained Networks",
        "authors": [
            "Uranik Berisha",
            "Jens Mehnert",
            "Alexandru Paul Condurache"
        ],
        "comments": "Accepted at IEEE/CVF International Conference on Computer Vision (ICCV) 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Increasingly expensive training of ever larger models such as Vision Transfomers motivate reusing the vast library of already trained state-of-the-art networks. However, their latency, high computational costs and memory demands pose significant challenges for deployment, especially on resource-constrained hardware. While structured pruning methods can reduce these factors, they often require costly retraining, sometimes for up to hundreds of epochs, or even training from scratch to recover the lost accuracy resulting from the structural modifications. Maintaining the provided performance of trained models after structured pruning and thereby avoiding extensive retraining remains a challenge. To solve this, we introduce Variance-Based Pruning, a simple and structured one-shot pruning technique for efficiently compressing networks, with minimal finetuning. Our approach first gathers activation statistics, which are used to select neurons for pruning. Simultaneously the mean activations are integrated back into the model to preserve a high degree of performance. On ImageNet-1k recognition tasks, we demonstrate that directly after pruning DeiT-Base retains over 70% of its original performance and requires only 10 epochs of fine-tuning to regain 99% of the original accuracy while simultaneously reducing MACs by 35% and model size by 36%, thus speeding up the model by 1.44x.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 110,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.12998",
        "abs_url": "https://arxiv.org/abs/2507.12998",
        "pdf_url": "https://arxiv.org/pdf/2507.12998",
        "title": "Differential-informed Sample Selection Accelerates Multimodal Contrastive Learning",
        "authors": [
            "Zihua Zhao",
            "Feng Hong",
            "Mengxi Chen",
            "Pengyi Chen",
            "Benyuan Liu",
            "Jiangchao Yao",
            "Ya Zhang",
            "Yanfeng Wang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "The remarkable success of contrastive-learning-based multimodal models has been greatly driven by training on ever-larger datasets with expensive compute consumption. Sample selection as an alternative efficient paradigm plays an important direction to accelerate the training process. However, recent advances on sample selection either mostly rely on an oracle model to offline select a high-quality coreset, which is limited in the cold-start scenarios, or focus on online selection based on real-time model predictions, which has not sufficiently or efficiently considered the noisy correspondence. To address this dilemma, we propose a novel Differential-Informed Sample Selection (DISSect) method, which accurately and efficiently discriminates the noisy correspondence for training acceleration. Specifically, we rethink the impact of noisy correspondence on contrastive learning and propose that the differential between the predicted correlation of the current model and that of a historical model is more informative to characterize sample quality. Based on this, we construct a robust differential-based sample selection and analyze its theoretical insights. Extensive experiments on three benchmark datasets and various downstream tasks demonstrate the consistent superiority of DISSect over current state-of-the-art methods. Source code is available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 111,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13024",
        "abs_url": "https://arxiv.org/abs/2507.13024",
        "pdf_url": "https://arxiv.org/pdf/2507.13024",
        "title": "When Pattern-by-Pattern Works: Theoretical and Empirical Insights for Logistic Models with Missing Values",
        "authors": [
            "Christophe Muller",
            "Erwan Scornet",
            "Julie Josse"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Predicting a response with partially missing inputs remains a challenging task even in parametric models, since parameter estimation in itself is not sufficient to predict on partially observed inputs. Several works study prediction in linear models. In this paper, we focus on logistic models, which present their own difficulties. From a theoretical perspective, we prove that a Pattern-by-Pattern strategy (PbP), which learns one logistic model per missingness pattern, accurately approximates Bayes probabilities in various missing data scenarios (MCAR, MAR and MNAR). Empirically, we thoroughly compare various methods (constant and iterative imputations, complete case analysis, PbP, and an EM algorithm) across classification, probability estimation, calibration, and parameter inference. Our analysis provides a comprehensive view on the logistic regression with missing values. It reveals that mean imputation can be used as baseline for low sample sizes, and improved performance is obtained via nonlinear multiple iterative imputation techniques with the labels (MICE.RF.Y). For large sample sizes, PbP is the best method for Gaussian mixtures, and we recommend MICE.RF.Y in presence of nonlinear features.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 112,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13033",
        "abs_url": "https://arxiv.org/abs/2507.13033",
        "pdf_url": "https://arxiv.org/pdf/2507.13033",
        "title": "(Exhaustive) Symbolic Regression and model selection by minimum description length",
        "authors": [
            "Harry Desmond"
        ],
        "comments": "15 pages, 4 figures; Invited review for the Royal Society Philosophical Transactions A special issue \"Symbolic regression in the physical sciences\"",
        "subjects": "Instrumentation and Methods for Astrophysics (astro-ph.IM); Cosmology and Nongalactic Astrophysics (astro-ph.CO); Astrophysics of Galaxies (astro-ph.GA); Machine Learning (cs.LG)",
        "abstract": "Symbolic regression is the machine learning method for learning functions from data. After a brief overview of the symbolic regression landscape, I will describe the two main challenges that traditional algorithms face: they have an unknown (and likely significant) probability of failing to find any given good function, and they suffer from ambiguity and poorly-justified assumptions in their function-selection procedure. To address these I propose an exhaustive search and model selection by the minimum description length principle, which allows accuracy and complexity to be directly traded off by measuring each in units of information. I showcase the resulting publicly available Exhaustive Symbolic Regression algorithm on three open problems in astrophysics: the expansion history of the universe, the effective behaviour of gravity in galaxies and the potential of the inflaton field. In each case the algorithm identifies many functions superior to the literature standards. This general purpose methodology should find widespread utility in science and beyond.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 113,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13094",
        "abs_url": "https://arxiv.org/abs/2507.13094",
        "pdf_url": "https://arxiv.org/pdf/2507.13094",
        "title": "Unsupervised Ground Metric Learning",
        "authors": [
            "Janis Auffenberg",
            "Jonas Bresch",
            "Oleh Melnyk",
            "Gabriele Steidl"
        ],
        "comments": "10 figures, 1 table",
        "subjects": "Optimization and Control (math.OC); Machine Learning (cs.LG); Numerical Analysis (math.NA)",
        "abstract": "Data classification without access to labeled samples remains a challenging problem. It usually depends on an appropriately chosen distance between features, a topic addressed in metric learning. Recently, Huizing, Cantini and Peyré proposed to simultaneously learn optimal transport (OT) cost matrices between samples and features of the dataset. This leads to the task of finding positive eigenvectors of a certain nonlinear function that maps cost matrices to OT distances. Having this basic idea in mind, we consider both the algorithmic and the modeling part of unsupervised metric learning. First, we examine appropriate algorithms and their convergence. In particular, we propose to use the stochastic random function iteration algorithm and prove that it converges linearly for our setting, although our operators are not paracontractive as it was required for convergence so far. Second, we ask the natural question if the OT distance can be replaced by other distances. We show how Mahalanobis-like distances fit into our considerations. Further, we examine an approach via graph Laplacians. In contrast to the previous settings, we have just to deal with linear functions in the wanted matrices here, so that simple algorithms from linear algebra can be applied.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 114,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13105",
        "abs_url": "https://arxiv.org/abs/2507.13105",
        "pdf_url": "https://arxiv.org/pdf/2507.13105",
        "title": "SemCSE: Semantic Contrastive Sentence Embeddings Using LLM-Generated Summaries For Scientific Abstracts",
        "authors": [
            "Marc Brinner",
            "Sina Zarriess"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "We introduce SemCSE, an unsupervised method for learning semantic embeddings of scientific texts. Building on recent advances in contrastive learning for text embeddings, our approach leverages LLM-generated summaries of scientific abstracts to train a model that positions semantically related summaries closer together in the embedding space. This resulting objective ensures that the model captures the true semantic content of a text, in contrast to traditional citation-based approaches that do not necessarily reflect semantic similarity. To validate this, we propose a novel benchmark designed to assess a model's ability to understand and encode the semantic content of scientific texts, demonstrating that our method enforces a stronger semantic separation within the embedding space. Additionally, we evaluate SemCSE on the comprehensive SciRepEval benchmark for scientific text embeddings, where it achieves state-of-the-art performance among models of its size, thus highlighting the benefits of a semantically focused training approach.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 115,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13106",
        "abs_url": "https://arxiv.org/abs/2507.13106",
        "pdf_url": "https://arxiv.org/pdf/2507.13106",
        "title": "Deep Learning-Based Fetal Lung Segmentation from Diffusion-weighted MRI Images and Lung Maturity Evaluation for Fetal Growth Restriction",
        "authors": [
            "Zhennan Xiao",
            "Katharine Brudkiewicz",
            "Zhen Yuan",
            "Rosalind Aughwane",
            "Magdalena Sokolska",
            "Joanna Chappell",
            "Trevor Gaunt",
            "Anna L. David",
            "Andrew P. King",
            "Andrew Melbourne"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Fetal lung maturity is a critical indicator for predicting neonatal outcomes and the need for post-natal intervention, especially for pregnancies affected by fetal growth restriction. Intra-voxel incoherent motion analysis has shown promising results for non-invasive assessment of fetal lung development, but its reliance on manual segmentation is time-consuming, thus limiting its clinical applicability. In this work, we present an automated lung maturity evaluation pipeline for diffusion-weighted magnetic resonance images that consists of a deep learning-based fetal lung segmentation model and a model-fitting lung maturity assessment. A 3D nnU-Net model was trained on manually segmented images selected from the baseline frames of 4D diffusion-weighted MRI scans. The segmentation model demonstrated robust performance, yielding a mean Dice coefficient of 82.14%. Next, voxel-wise model fitting was performed based on both the nnU-Net-predicted and manual lung segmentations to quantify IVIM parameters reflecting tissue microstructure and perfusion. The results suggested no differences between the two. Our work shows that a fully automated pipeline is possible for supporting fetal lung maturity assessment and clinical decision-making.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 116,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13120",
        "abs_url": "https://arxiv.org/abs/2507.13120",
        "pdf_url": "https://arxiv.org/pdf/2507.13120",
        "title": "RS-TinyNet: Stage-wise Feature Fusion Network for Detecting Tiny Objects in Remote Sensing Images",
        "authors": [
            "Xiaozheng Jiang",
            "Wei Zhang",
            "Xuerui Mao"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Detecting tiny objects in remote sensing (RS) imagery has been a long-standing challenge due to their extremely limited spatial information, weak feature representations, and dense distributions across complex backgrounds. Despite numerous efforts devoted, mainstream detectors still underperform in such scenarios. To bridge this gap, we introduce RS-TinyNet, a multi-stage feature fusion and enhancement model explicitly tailored for RS tiny object detection in various RS scenarios. RS-TinyNet comes with two novel designs: tiny object saliency modeling and feature integrity reconstruction. Guided by these principles, we design three step-wise feature enhancement modules. Among them, the multi-dimensional collaborative attention (MDCA) module employs multi-dimensional attention to enhance the saliency of tiny objects. Additionally, the auxiliary reversible branch (ARB) and a progressive fusion detection head (PFDH) module are introduced to preserve information flow and fuse multi-level features to bridge semantic gaps and retain structural detail. Comprehensive experiments on public RS dataset AI-TOD show that our RS-TinyNet surpasses existing state-of-the-art (SOTA) detectors by 4.0% AP and 6.5% AP75. Evaluations on DIOR benchmark dataset further validate its superior detection performance in diverse RS scenarios. These results demonstrate that the proposed multi-stage feature fusion strategy offers an effective and practical solution for tiny object detection in complex RS environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 117,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13122",
        "abs_url": "https://arxiv.org/abs/2507.13122",
        "pdf_url": "https://arxiv.org/pdf/2507.13122",
        "title": "Search for Z/2 eigenfunctions on the sphere using machine learning",
        "authors": [
            "Andriy Haydys",
            "Willem Adriaan Salm"
        ],
        "comments": "14 pages, 12 pictures",
        "subjects": "Differential Geometry (math.DG); Machine Learning (cs.LG); Numerical Analysis (math.NA)",
        "abstract": "We use machine learning to search for examples of Z/2 eigenfunctions on the 2-sphere. For this we created a multivalued version of a feedforward deep neural network, and we implemented it using the JAX library. We found Z/2 eigenfunctions for three cases: In the first two cases we fixed the branch points at the vertices of a tetrahedron and at a cube respectively. In a third case, we allowed the AI to move the branch points around and, in the end, it positioned the branch points at the vertices of a squashed tetrahedron.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 118,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13162",
        "abs_url": "https://arxiv.org/abs/2507.13162",
        "pdf_url": "https://arxiv.org/pdf/2507.13162",
        "title": "Orbis: Overcoming Challenges of Long-Horizon Prediction in Driving World Models",
        "authors": [
            "Arian Mousakhan",
            "Sudhanshu Mittal",
            "Silvio Galesso",
            "Karim Farid",
            "Thomas Brox"
        ],
        "comments": "Project page: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Existing world models for autonomous driving struggle with long-horizon generation and generalization to challenging scenarios. In this work, we develop a model using simple design choices, and without additional supervision or sensors, such as maps, depth, or multiple cameras. We show that our model yields state-of-the-art performance, despite having only 469M parameters and being trained on 280h of video data. It particularly stands out in difficult scenarios like turning maneuvers and urban traffic. We test whether discrete token models possibly have advantages over continuous models based on flow matching. To this end, we set up a hybrid tokenizer that is compatible with both approaches and allows for a side-by-side comparison. Our study concludes in favor of the continuous autoregressive model, which is less brittle on individual design choices and more powerful than the model built on discrete tokens. Code, models and qualitative results are publicly available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 119,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13170",
        "abs_url": "https://arxiv.org/abs/2507.13170",
        "pdf_url": "https://arxiv.org/pdf/2507.13170",
        "title": "SHIELD: A Secure and Highly Enhanced Integrated Learning for Robust Deepfake Detection against Adversarial Attacks",
        "authors": [
            "Kutub Uddin",
            "Awais Khan",
            "Muhammad Umar Farooq",
            "Khalid Malik"
        ],
        "comments": "",
        "subjects": "Sound (cs.SD); Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)",
        "abstract": "Audio plays a crucial role in applications like speaker verification, voice-enabled smart devices, and audio conferencing. However, audio manipulations, such as deepfakes, pose significant risks by enabling the spread of misinformation. Our empirical analysis reveals that existing methods for detecting deepfake audio are often vulnerable to anti-forensic (AF) attacks, particularly those attacked using generative adversarial networks. In this article, we propose a novel collaborative learning method called SHIELD to defend against generative AF attacks. To expose AF signatures, we integrate an auxiliary generative model, called the defense (DF) generative model, which facilitates collaborative learning by combining input and output. Furthermore, we design a triplet model to capture correlations for real and AF attacked audios with real-generated and attacked-generated audios using auxiliary generative models. The proposed SHIELD strengthens the defense against generative AF attacks and achieves robust performance across various generative models. The proposed AF significantly reduces the average detection accuracy from 95.49% to 59.77% for ASVspoof2019, from 99.44% to 38.45% for In-the-Wild, and from 98.41% to 51.18% for HalfTruth for three different generative models. The proposed SHIELD mechanism is robust against AF attacks and achieves an average accuracy of 98.13%, 98.58%, and 99.57% in match, and 98.78%, 98.62%, and 98.85% in mismatch settings for the ASVspoof2019, In-the-Wild, and HalfTruth datasets, respectively.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 120,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13194",
        "abs_url": "https://arxiv.org/abs/2507.13194",
        "pdf_url": "https://arxiv.org/pdf/2507.13194",
        "title": "Relation-Aware Slicing in Cross-Domain Alignment",
        "authors": [
            "Dhruv Sarkar",
            "Aprameyo Chakrabartty",
            "Anish Chakrabarty",
            "Swagatam Das"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "The Sliced Gromov-Wasserstein (SGW) distance, aiming to relieve the computational cost of solving a non-convex quadratic program that is the Gromov-Wasserstein distance, utilizes projecting directions sampled uniformly from unit hyperspheres. This slicing mechanism incurs unnecessary computational costs due to uninformative directions, which also affects the representative power of the distance. However, finding a more appropriate distribution over the projecting directions (slicing distribution) is often an optimization problem in itself that comes with its own computational cost. In addition, with more intricate distributions, the sampling itself may be expensive. As a remedy, we propose an optimization-free slicing distribution that provides fast sampling for the Monte Carlo approximation. We do so by introducing the Relation-Aware Projecting Direction (RAPD), effectively capturing the pairwise association of each of two pairs of random vectors, each following their ambient law. This enables us to derive the Relation-Aware Slicing Distribution (RASD), a location-scale law corresponding to sampled RAPDs. Finally, we introduce the RASGW distance and its variants, e.g., IWRASGW (Importance Weighted RASGW), which overcome the shortcomings experienced by SGW. We theoretically analyze its properties and substantiate its empirical prowess using extensive experiments on various alignment tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 121,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13222",
        "abs_url": "https://arxiv.org/abs/2507.13222",
        "pdf_url": "https://arxiv.org/pdf/2507.13222",
        "title": "Computational-Statistical Tradeoffs from NP-hardness",
        "authors": [
            "Guy Blanc",
            "Caleb Koch",
            "Carmen Strassle",
            "Li-Yang Tan"
        ],
        "comments": "To appear at FOCS 2025",
        "subjects": "Computational Complexity (cs.CC); Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG)",
        "abstract": "A central question in computer science and statistics is whether efficient algorithms can achieve the information-theoretic limits of statistical problems. Many computational-statistical tradeoffs have been shown under average-case assumptions, but since statistical problems are average-case in nature, it has been a challenge to base them on standard worst-case assumptions. In PAC learning where such tradeoffs were first studied, the question is whether computational efficiency can come at the cost of using more samples than information-theoretically necessary. We base such tradeoffs on $\\mathsf{NP}$-hardness and obtain: $\\circ$ Sharp computational-statistical tradeoffs assuming $\\mathsf{NP}$ requires exponential time: For every polynomial $p(n)$, there is an $n$-variate class $C$ with VC dimension $1$ such that the sample complexity of time-efficiently learning $C$ is $\\Theta(p(n))$. $\\circ$ A characterization of $\\mathsf{RP}$ vs. $\\mathsf{NP}$ in terms of learning: $\\mathsf{RP} = \\mathsf{NP}$ iff every $\\mathsf{NP}$-enumerable class is learnable with $O(\\mathrm{VCdim}(C))$ samples in polynomial time. The forward implication has been known since (Pitt and Valiant, 1988); we prove the reverse implication. Notably, all our lower bounds hold against improper learners. These are the first $\\mathsf{NP}$-hardness results for improperly learning a subclass of polynomial-size circuits, circumventing formal barriers of Applebaum, Barak, and Xiao (2008).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 122,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13246",
        "abs_url": "https://arxiv.org/abs/2507.13246",
        "pdf_url": "https://arxiv.org/pdf/2507.13246",
        "title": "The carbon cost of materials discovery: Can machine learning really accelerate the discovery of new photovoltaics?",
        "authors": [
            "Matthew Walker",
            "Keith T. Butler"
        ],
        "comments": "",
        "subjects": "Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG)",
        "abstract": "Computational screening has become a powerful complement to experimental efforts in the discovery of high-performance photovoltaic (PV) materials. Most workflows rely on density functional theory (DFT) to estimate electronic and optical properties relevant to solar energy conversion. Although more efficient than laboratory-based methods, DFT calculations still entail substantial computational and environmental costs. Machine learning (ML) models have recently gained attention as surrogates for DFT, offering drastic reductions in resource use with competitive predictive performance. In this study, we reproduce a canonical DFT-based workflow to estimate the maximum efficiency limit and progressively replace its components with ML surrogates. By quantifying the CO$_2$ emissions associated with each computational strategy, we evaluate the trade-offs between predictive efficacy and environmental cost. Our results reveal multiple hybrid ML/DFT strategies that optimize different points along the accuracy--emissions front. We find that direct prediction of scalar quantities, such as maximum efficiency, is significantly more tractable than using predicted absorption spectra as an intermediate step. Interestingly, ML models trained on DFT data can outperform DFT workflows using alternative exchange--correlation functionals in screening applications, highlighting the consistency and utility of data-driven approaches. We also assess strategies to improve ML-driven screening through expanded datasets and improved model architectures tailored to PV-relevant features. This work provides a quantitative framework for building low-emission, high-throughput discovery pipelines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 123,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13255",
        "abs_url": "https://arxiv.org/abs/2507.13255",
        "pdf_url": "https://arxiv.org/pdf/2507.13255",
        "title": "Automating Steering for Safe Multimodal Large Language Models",
        "authors": [
            "Lyucheng Wu",
            "Mengru Wang",
            "Ziwen Xu",
            "Tri Cao",
            "Nay Oo",
            "Bryan Hooi",
            "Shumin Deng"
        ],
        "comments": "Working in progress. 22 pages (8+ for main); 25 figures; 1 table",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG); Multimedia (cs.MM)",
        "abstract": "Recent progress in Multimodal Large Language Models (MLLMs) has unlocked powerful cross-modal reasoning abilities, but also raised new safety concerns, particularly when faced with adversarial multimodal inputs. To improve the safety of MLLMs during inference, we introduce a modular and adaptive inference-time intervention technology, AutoSteer, without requiring any fine-tuning of the underlying model. AutoSteer incorporates three core components: (1) a novel Safety Awareness Score (SAS) that automatically identifies the most safety-relevant distinctions among the model's internal layers; (2) an adaptive safety prober trained to estimate the likelihood of toxic outputs from intermediate representations; and (3) a lightweight Refusal Head that selectively intervenes to modulate generation when safety risks are detected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical benchmarks demonstrate that AutoSteer significantly reduces the Attack Success Rate (ASR) for textual, visual, and cross-modal threats, while maintaining general abilities. These findings position AutoSteer as a practical, interpretable, and effective framework for safer deployment of multimodal AI systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 124,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13277",
        "abs_url": "https://arxiv.org/abs/2507.13277",
        "pdf_url": "https://arxiv.org/pdf/2507.13277",
        "title": "Evaluating Reinforcement Learning Algorithms for Navigation in Simulated Robotic Quadrupeds: A Comparative Study Inspired by Guide Dog Behaviour",
        "authors": [
            "Emma M. A. Harrison"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Robots are increasingly integrated across industries, particularly in healthcare. However, many valuable applications for quadrupedal robots remain overlooked. This research explores the effectiveness of three reinforcement learning algorithms in training a simulated quadruped robot for autonomous navigation and obstacle avoidance. The goal is to develop a robotic guide dog simulation capable of path following and obstacle avoidance, with long-term potential for real-world assistance to guide dogs and visually impaired individuals. It also seeks to expand research into medical 'pets', including robotic guide and alert dogs. A comparative analysis of thirteen related research papers shaped key evaluation criteria, including collision detection, pathfinding algorithms, sensor usage, robot type, and simulation platforms. The study focuses on sensor inputs, collision frequency, reward signals, and learning progression to determine which algorithm best supports robotic navigation in complex environments. Custom-made environments were used to ensure fair evaluation of all three algorithms under controlled conditions, allowing consistent data collection. Results show that Proximal Policy Optimization (PPO) outperformed Deep Q-Network (DQN) and Q-learning across all metrics, particularly in average and median steps to goal per episode. By analysing these results, this study contributes to robotic navigation, AI and medical robotics, offering insights into the feasibility of AI-driven quadruped mobility and its role in assistive robotics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 125,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13283",
        "abs_url": "https://arxiv.org/abs/2507.13283",
        "pdf_url": "https://arxiv.org/pdf/2507.13283",
        "title": "Stochastic Weakly Convex Optimization Under Heavy-Tailed Noises",
        "authors": [
            "Tianxi Zhu",
            "Yi Xu",
            "Xiangyang Ji"
        ],
        "comments": "",
        "subjects": "Optimization and Control (math.OC); Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "An increasing number of studies have focused on stochastic first-order methods (SFOMs) under heavy-tailed gradient noises, which have been observed in the training of practical deep learning models. In this paper, we focus on two types of gradient noises: one is sub-Weibull noise, and the other is noise under the assumption that it has a bounded $p$-th central moment ($p$-BCM) with $p\\in (1, 2]$. The latter is more challenging due to the occurrence of infinite variance when $p\\in (1, 2)$. Under these two gradient noise assumptions, the in-expectation and high-probability convergence of SFOMs have been extensively studied in the contexts of convex optimization and standard smooth optimization. However, for weakly convex objectives-a class that includes all Lipschitz-continuous convex objectives and smooth objectives-our understanding of the in-expectation and high-probability convergence of SFOMs under these two types of noises remains incomplete. We investigate the high-probability convergence of the vanilla stochastic subgradient descent (SsGD) method under sub-Weibull noises, as well as the high-probability and in-expectation convergence of clipped SsGD under the $p$-BCM noises. Both analyses are conducted in the context of weakly convex optimization. For weakly convex objectives that may be non-convex and non-smooth, our results demonstrate that the theoretical dependence of vanilla SsGD on the failure probability and number of iterations under sub-Weibull noises does not degrade compared to the case of smooth objectives. Under $p$-BCM noises, our findings indicate that the non-smoothness and non-convexity of weakly convex objectives do not impact the theoretical dependence of clipped SGD on the failure probability relative to the smooth case; however, the sample complexity we derived is worse than a well-known lower bound for smooth optimization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 126,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13287",
        "abs_url": "https://arxiv.org/abs/2507.13287",
        "pdf_url": "https://arxiv.org/pdf/2507.13287",
        "title": "Optimal Empirical Risk Minimization under Temporal Distribution Shifts",
        "authors": [
            "Yujin Jeong",
            "Ramesh Johari",
            "Dominik Rothenhäusler",
            "Emily Fox"
        ],
        "comments": "",
        "subjects": "Methodology (stat.ME); Machine Learning (cs.LG)",
        "abstract": "Temporal distribution shifts pose a key challenge for machine learning models trained and deployed in dynamically evolving environments. This paper introduces RIDER (RIsk minimization under Dynamically Evolving Regimes) which derives optimally-weighted empirical risk minimization procedures under temporal distribution shifts. Our approach is theoretically grounded in the random distribution shift model, where random shifts arise as a superposition of numerous unpredictable changes in the data-generating process. We show that common weighting schemes, such as pooling all data, exponentially weighting data, and using only the most recent data, emerge naturally as special cases in our framework. We demonstrate that RIDER consistently improves out-of-sample predictive performance when applied as a fine-tuning step on the Yearbook dataset, across a range of benchmark methods in Wild-Time. Moreover, we show that RIDER outperforms standard weighting strategies in two other real-world tasks: predicting stock market volatility and forecasting ride durations in NYC taxi data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 127,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13340",
        "abs_url": "https://arxiv.org/abs/2507.13340",
        "pdf_url": "https://arxiv.org/pdf/2507.13340",
        "title": "Latent Policy Steering with Embodiment-Agnostic Pretrained World Models",
        "authors": [
            "Yiqi Wang",
            "Mrinal Verghese",
            "Jeff Schneider"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Learning visuomotor policies via imitation has proven effective across a wide range of robotic domains. However, the performance of these policies is heavily dependent on the number of training demonstrations, which requires expensive data collection in the real world. In this work, we aim to reduce data collection efforts when learning visuomotor robot policies by leveraging existing or cost-effective data from a wide range of embodiments, such as public robot datasets and the datasets of humans playing with objects (human data from play). Our approach leverages two key insights. First, we use optic flow as an embodiment-agnostic action representation to train a World Model (WM) across multi-embodiment datasets, and finetune it on a small amount of robot data from the target embodiment. Second, we develop a method, Latent Policy Steering (LPS), to improve the output of a behavior-cloned policy by searching in the latent space of the WM for better action sequences. In real world experiments, we observe significant improvements in the performance of policies trained with a small amount of data (over 50% relative improvement with 30 demonstrations and over 20% relative improvement with 50 demonstrations) by combining the policy with a WM pretrained on two thousand episodes sampled from the existing Open X-embodiment dataset across different robots or a cost-effective human dataset from play.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 128,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13348",
        "abs_url": "https://arxiv.org/abs/2507.13348",
        "pdf_url": "https://arxiv.org/pdf/2507.13348",
        "title": "VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning",
        "authors": [
            "Senqiao Yang",
            "Junyi Li",
            "Xin Lai",
            "Bei Yu",
            "Hengshuang Zhao",
            "Jiaya Jia"
        ],
        "comments": "Code and models are available at this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Recent advancements in vision-language models (VLMs) have improved performance by increasing the number of visual tokens, which are often significantly longer than text tokens. However, we observe that most real-world scenarios do not require such an extensive number of visual tokens. While the performance drops significantly in a small subset of OCR-related tasks, models still perform accurately in most other general VQA tasks with only 1/4 resolution. Therefore, we propose to dynamically process distinct samples with different resolutions, and present a new paradigm for visual token compression, namely, VisionThink. It starts with a downsampled image and smartly decides whether it is sufficient for problem solving. Otherwise, the model could output a special token to request the higher-resolution image. Compared to existing Efficient VLM methods that compress tokens using fixed pruning ratios or thresholds, VisionThink autonomously decides whether to compress tokens case by case. As a result, it demonstrates strong fine-grained visual understanding capability on OCR-related tasks, and meanwhile saves substantial visual tokens on simpler tasks. We adopt reinforcement learning and propose the LLM-as-Judge strategy to successfully apply RL to general VQA tasks. Moreover, we carefully design a reward function and penalty mechanism to achieve a stable and reasonable image resize call ratio. Extensive experiments demonstrate the superiority, efficiency, and effectiveness of our method. Our code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 129,
        "date": "2025-07-18",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-07-18?abs=True",
        "arxiv_id": "2507.13350",
        "abs_url": "https://arxiv.org/abs/2507.13350",
        "pdf_url": "https://arxiv.org/pdf/2507.13350",
        "title": "Hierarchical Rectified Flow Matching with Mini-Batch Couplings",
        "authors": [
            "Yichi Zhang",
            "Yici Yan",
            "Alex Schwing",
            "Zhizhen Zhao"
        ],
        "comments": "Project Page: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Flow matching has emerged as a compelling generative modeling approach that is widely used across domains. To generate data via a flow matching model, an ordinary differential equation (ODE) is numerically solved via forward integration of the modeled velocity field. To better capture the multi-modality that is inherent in typical velocity fields, hierarchical flow matching was recently introduced. It uses a hierarchy of ODEs that are numerically integrated when generating data. This hierarchy of ODEs captures the multi-modal velocity distribution just like vanilla flow matching is capable of modeling a multi-modal data distribution. While this hierarchy enables to model multi-modal velocity distributions, the complexity of the modeled distribution remains identical across levels of the hierarchy. In this paper, we study how to gradually adjust the complexity of the distributions across different levels of the hierarchy via mini-batch couplings. We show the benefits of mini-batch couplings in hierarchical rectified flow matching via compelling results on synthetic and imaging data. Code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    }
]