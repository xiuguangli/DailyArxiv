[
    {
        "order": 1,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08806",
        "abs_url": "https://arxiv.org/abs/2507.08806",
        "pdf_url": "https://arxiv.org/pdf/2507.08806",
        "title": "Think Clearly: Improving Reasoning via Redundant Token Pruning",
        "authors": [
            "Daewon Choi",
            "Jimin Lee",
            "Jihoon Tack",
            "Woomin Song",
            "Saket Dingliwal",
            "Sai Muralidhar Jayanthi",
            "Bhavana Ganesh",
            "Jinwoo Shin",
            "Aram Galstyan",
            "Sravan Babu Bodapati"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Recent large language models have shown promising capabilities in long-form reasoning, following structured chains of thought before arriving at a final answer. However, we observe that these reasoning paths tend to include substantial redundancy; analyzing attention patterns reveals that attention scores are widely scattered, particularly incorrect answers exhibit greater attention sparsity. In this paper, we demonstrate that deliberately removing this redundancy in the reasoning process significantly improves performance through clear thinking, i.e., removing distraction. Specifically, we systematically identify reasoning redundancy by measuring token-level attention scores to a special end-of-thinking token, which is appended to an explicit instruction inserted to conclude each intermediate reasoning step. Furthermore, we propose structure-aware pruning that prioritizes removing tokens in low-contributing reasoning chunks over individual tokens. After evicting redundant tokens, we remove the injected end-of-thinking instruction, then resume the reasoning generation. We demonstrate that our method significantly improves overall accuracy across reasoning-intensive benchmarks without any training involved. In particular, our method shows strong performance on challenging mathematical competition benchmarks such as AIME and AMC, where reasoning redundancy is more prevalent.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 2,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08875",
        "abs_url": "https://arxiv.org/abs/2507.08875",
        "pdf_url": "https://arxiv.org/pdf/2507.08875",
        "title": "A New Approach for Multicriteria Assessment in the Ranking of Alternatives Using Cardinal and Ordinal Data",
        "authors": [
            "Fuh-Hwa Franklin Liu",
            "Su-Chuan Shih"
        ],
        "comments": "38 pages, 6 figures, 5 table. A practice applicable method for multi-criteria assessments using cardinal and ordinal data",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Modern methods for multi-criteria assessment (MCA), such as Data Envelopment Analysis (DEA), Stochastic Frontier Analysis (SFA), and Multiple Criteria Decision-Making (MCDM), are utilized to appraise a collection of Decision-Making Units (DMUs), also known as alternatives, based on several criteria. These methodologies inherently rely on assumptions and can be influenced by subjective judgment to effectively tackle the complex evaluation challenges in various fields. In real-world scenarios, it is essential to incorporate both quantitative and qualitative criteria as they consist of cardinal and ordinal data. Despite the inherent variability in the criterion values of different alternatives, the homogeneity assumption is often employed, significantly affecting evaluations. To tackle these challenges and determine the most appropriate alternative, we propose a novel MCA approach that combines two Virtual Gap Analysis (VGA) models. The VGA framework, rooted in linear programming, is pivotal in the MCA methodology. This approach improves efficiency and fairness, ensuring that evaluations are both comprehensive and dependable, thus offering a strong and adaptive solution. Two comprehensive numerical examples demonstrate the accuracy and transparency of our proposed method. The goal is to encourage continued advancement and stimulate progress in automated decision systems and decision support systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 3,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08892",
        "abs_url": "https://arxiv.org/abs/2507.08892",
        "pdf_url": "https://arxiv.org/pdf/2507.08892",
        "title": "Multi-Actor Generative Artificial Intelligence as a Game Engine",
        "authors": [
            "Alexander Sasha Vezhnevets",
            "Jayd Matyas",
            "Logan Cross",
            "Davide Paglieri",
            "Minsuk Chang",
            "William A. Cunningham",
            "Simon Osindero",
            "William S. Isaac",
            "Joel Z. Leibo"
        ],
        "comments": "13 pages",
        "subjects": "Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)",
        "abstract": "Generative AI can be used in multi-actor environments with purposes ranging from social science modeling to interactive narrative and AI evaluation. Supporting this diversity of use cases -- which we classify as Simulationist, Dramatist, and Evaluationist -- demands a flexible scenario definition framework. We argue here that a good approach is to take inspiration from tabletop role-playing games (TTRPGs), where a Game Master (GM) is responsible for the environment and generates all parts of the story not directly determined by the voluntary actions of player characters. We argue that the Entity-Component architectural pattern is useful here. In such a system, the GM is not a hardcoded computer game but is itself a configurable entity, composed of components just like any other actor. By design, the approach allows for a separation between the underlying implementation details handled by an engineer, the creation of reusable components, and their composition and configuration managed by a designer who constructs entities from the components. This separation of concerns is instrumental for achieving rapid iteration, maintaining modularity, and ultimately to ensure scalability. We describe the ongoing evolution of the Concordia library in terms of this philosophy, demonstrating how it allows users to effectively configure scenarios that align with their specific goals.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 4,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09080",
        "abs_url": "https://arxiv.org/abs/2507.09080",
        "pdf_url": "https://arxiv.org/pdf/2507.09080",
        "title": "BioAnalyst: A Foundation Model for Biodiversity",
        "authors": [
            "Athanasios Trantas",
            "Martino Mensio",
            "Stylianos Stasinos",
            "Sebastian Gribincea",
            "Taimur Khan",
            "Damian Podareanu",
            "Aliene van der Veen"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "The accelerating loss of biodiversity presents critical challenges for ecological research and conservation strategies. The preservation of biodiversity is paramount for maintaining ecological balance and ensuring the sustainability of ecosystems. However, biodiversity faces numerous threats, including habitat loss, climate change, and the proliferation of invasive species. Addressing these and other ecology-related challenges, both at local and global scales, requires comprehensive monitoring, predictive and conservation planning capabilities. Artificial Intelligence (AI) Foundation Models (FMs) have gained significant momentum in numerous scientific domains by leveraging vast datasets to learn general-purpose representations adaptable to various downstream tasks. This paradigm holds immense promise for biodiversity conservation. In response, we introduce BioAnalyst, the first Foundation Model tailored for biodiversity analysis and conservation planning. BioAnalyst employs a transformer-based architecture, pre-trained on extensive multi-modal datasets encompassing species occurrence records, remote sensing indicators, climate and environmental variables. BioAnalyst is designed for adaptability, allowing for fine-tuning of a range of downstream tasks, such as species distribution modelling, habitat suitability assessments, invasive species detection, and population trend forecasting. We evaluate the model's performance on two downstream use cases, demonstrating its generalisability compared to existing methods, particularly in data-scarce scenarios for two distinct use-cases, establishing a new accuracy baseline for ecological forecasting. By openly releasing BioAnalyst and its fine-tuning workflows to the scientific community, we aim to foster collaborative efforts in biodiversity modelling and advance AI-driven solutions to pressing ecological challenges.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 5,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09089",
        "abs_url": "https://arxiv.org/abs/2507.09089",
        "pdf_url": "https://arxiv.org/pdf/2507.09089",
        "title": "Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity",
        "authors": [
            "Joel Becker",
            "Nate Rush",
            "Elizabeth Barnes",
            "David Rein"
        ],
        "comments": "50 pages, 8 tables, 22 figures",
        "subjects": "Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Software Engineering (cs.SE)",
        "abstract": "Despite widespread adoption, the impact of AI tools on software development in the wild remains understudied. We conduct a randomized controlled trial (RCT) to understand how AI tools at the February-June 2025 frontier affect the productivity of experienced open-source developers. 16 developers with moderate AI experience complete 246 tasks in mature projects on which they have an average of 5 years of prior experience. Each task is randomly assigned to allow or disallow usage of early 2025 AI tools. When AI tools are allowed, developers primarily use Cursor Pro, a popular code editor, and Claude 3.5/3.7 Sonnet. Before starting tasks, developers forecast that allowing AI will reduce completion time by 24%. After completing the study, developers estimate that allowing AI reduced completion time by 20%. Surprisingly, we find that allowing AI actually increases completion time by 19%--AI tooling slowed developers down. This slowdown also contradicts predictions from experts in economics (39% shorter) and ML (38% shorter). To understand this result, we collect and evaluate evidence for 20 properties of our setting that a priori could contribute to the observed slowdown effect--for example, the size and quality standards of projects, or prior developer experience with AI tooling. Although the influence of experimental artifacts cannot be entirely ruled out, the robustness of the slowdown effect across our analyses suggests it is unlikely to primarily be a function of our experimental design.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 6,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09179",
        "abs_url": "https://arxiv.org/abs/2507.09179",
        "pdf_url": "https://arxiv.org/pdf/2507.09179",
        "title": "Hide-and-Shill: A Reinforcement Learning Framework for Market Manipulation Detection in Symphony-a Decentralized Multi-Agent System",
        "authors": [
            "Ronghua Shi",
            "Yiou Liu",
            "Xinyu Ying",
            "Yang Tan",
            "Yuchun Feng",
            "Lynn Ai",
            "Bill Shi",
            "Xuhui Wang",
            "Zhuang Liu"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Decentralized finance (DeFi) has introduced a new era of permissionless financial innovation but also led to unprecedented market manipulation. Without centralized oversight, malicious actors coordinate shilling campaigns and pump-and-dump schemes across various platforms. We propose a Multi-Agent Reinforcement Learning (MARL) framework for decentralized manipulation detection, modeling the interaction between manipulators and detectors as a dynamic adversarial game. This framework identifies suspicious patterns using delayed token price reactions as financial this http URL method introduces three innovations: (1) Group Relative Policy Optimization (GRPO) to enhance learning stability in sparse-reward and partially observable settings; (2) a theory-based reward function inspired by rational expectations and information asymmetry, differentiating price discovery from manipulation noise; and (3) a multi-modal agent pipeline that integrates LLM-based semantic features, social graph signals, and on-chain market data for informed this http URL framework is integrated within the Symphony system, a decentralized multi-agent architecture enabling peer-to-peer agent execution and trust-aware learning through distributed logs, supporting chain-verifiable evaluation. Symphony promotes adversarial co-evolution among strategic actors and maintains robust manipulation detection without centralized oracles, enabling real-time surveillance across global DeFi this http URL on 100,000 real-world discourse episodes and validated in adversarial simulations, Hide-and-Shill achieves top performance in detection accuracy and causal attribution. This work bridges multi-agent systems with financial surveillance, advancing a new paradigm for decentralized market intelligence. All resources are available at the Hide-and-Shill GitHub repository to promote open research and reproducibility.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 7,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09329",
        "abs_url": "https://arxiv.org/abs/2507.09329",
        "pdf_url": "https://arxiv.org/pdf/2507.09329",
        "title": "When Developer Aid Becomes Security Debt: A Systematic Analysis of Insecure Behaviors in LLM Coding Agents",
        "authors": [
            "Matous Kozak",
            "Roshanak Zilouchian Moghaddam",
            "Siva Sivaraman"
        ],
        "comments": "15 pages",
        "subjects": "Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)",
        "abstract": "LLM-based coding agents are rapidly being deployed in software development, yet their security implications remain poorly understood. These agents, while capable of accelerating software development, may inadvertently introduce insecure practices. We conducted the first systematic security evaluation of autonomous coding agents, analyzing over 12,000 actions across five state-of-the-art models (GPT-4o, GPT-4.1, Claude variants) on 93 real-world software setup tasks. Our findings reveal significant security concerns: 21% of agent trajectories contained insecure actions, with models showing substantial variation in security behavior. We developed a high-precision detection system that identified four major vulnerability categories, with information exposure (CWE-200) being the most prevalent one. We also evaluated mitigation strategies including feedback mechanisms and security reminders with various effectiveness between models. GPT-4.1 demonstrated exceptional security awareness with 96.8% mitigation success. Our work provides the first comprehensive framework for evaluating coding agent security and highlights the need for security-aware design of next generation LLM-based coding agents.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 8,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09369",
        "abs_url": "https://arxiv.org/abs/2507.09369",
        "pdf_url": "https://arxiv.org/pdf/2507.09369",
        "title": "A Taxonomy of Omnicidal Futures Involving Artificial Intelligence",
        "authors": [
            "Andrew Critch",
            "Jacob Tsimerman"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "This report presents a taxonomy and examples of potential omnicidal events resulting from AI: scenarios where all or almost all humans are killed. These events are not presented as inevitable, but as possibilities that we can work to avoid. Insofar as large institutions require a degree of public support in order to take certain actions, we hope that by presenting these possibilities in public, we can help to support preventive measures against catastrophic risks from AI.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 9,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09374",
        "abs_url": "https://arxiv.org/abs/2507.09374",
        "pdf_url": "https://arxiv.org/pdf/2507.09374",
        "title": "EduFlow: Advancing MLLMs' Problem-Solving Proficiency through Multi-Stage, Multi-Perspective Critique",
        "authors": [
            "Chenglin Zhu",
            "Tao Zhang",
            "Chong Li",
            "Mingan Lin",
            "Zenan Zhou",
            "Jian Xie"
        ],
        "comments": "14 pages,4 figures",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Multimodal large language models (MLLMs) still perform poorly on scientific tasks, particularly those requiring multi-step and interpretable reasoning. Their limitations include insufficient scientific reasoning patterns, lack of global coherence in multi-step inference, and the absence of reflective self-correction, making them unreliable in structured scientific contexts. We introduce EduFlow, the first end-to-end framework that covers the full pipeline of educational scientific reasoning, including data selection, MCTS-based trajectory construction, model training, and output optimization. At its core is EduPRM, a process-aware reward model that critiques reasoning steps with tags and justifications. EduPRM is trained via curriculum learning on three complementary supervision sources: MCTS-guided trajectories, error-injected critiques, and teacher-student dialogues, enabling dynamic adaptation to multi-stage problem solving and iterative refinement during inference. We further propose EduMCTS, a domain-adapted search framework that introduces bootstrapping actions specifically designed for educational reasoning, such as a self-reflection mechanism that promotes reflective error correction. It further leverages EduPRM's fine-grained feedback to guide the search toward higher-quality reasoning trajectories. By applying self-consistency and rejection sampling, we constructed EduMCTS-160K, a large-scale dataset of educational reasoning trajectories. Extensive experiments demonstrate that EduFlow enhances reasoning consistency and coherence. Code, data, and models will be released.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 10,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09389",
        "abs_url": "https://arxiv.org/abs/2507.09389",
        "pdf_url": "https://arxiv.org/pdf/2507.09389",
        "title": "Knowledge Conceptualization Impacts RAG Efficacy",
        "authors": [
            "Chris Davis Jaldi",
            "Anmol Saini",
            "Elham Ghiasi",
            "O. Divine Eziolise",
            "Cogan Shimizu"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Information Retrieval (cs.IR)",
        "abstract": "Explainability and interpretability are cornerstones of frontier and next-generation artificial intelligence (AI) systems. This is especially true in recent systems, such as large language models (LLMs), and more broadly, generative AI. On the other hand, adaptability to new domains, contexts, or scenarios is also an important aspect for a successful system. As such, we are particularly interested in how we can merge these two efforts, that is, investigating the design of transferable and interpretable neurosymbolic AI systems. Specifically, we focus on a class of systems referred to as ''Agentic Retrieval-Augmented Generation'' systems, which actively select, interpret, and query knowledge sources in response to natural language prompts. In this paper, we systematically evaluate how different conceptualizations and representations of knowledge, particularly the structure and complexity, impact an AI agent (in this case, an LLM) in effectively querying a triplestore. We report our results, which show that there are impacts from both approaches, and we discuss their impact and implications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 11,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09407",
        "abs_url": "https://arxiv.org/abs/2507.09407",
        "pdf_url": "https://arxiv.org/pdf/2507.09407",
        "title": "LLM-Stackelberg Games: Conjectural Reasoning Equilibria and Their Applications to Spearphishing",
        "authors": [
            "Quanyan Zhu"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Computer Science and Game Theory (cs.GT)",
        "abstract": "We introduce the framework of LLM-Stackelberg games, a class of sequential decision-making models that integrate large language models (LLMs) into strategic interactions between a leader and a follower. Departing from classical Stackelberg assumptions of complete information and rational agents, our formulation allows each agent to reason through structured prompts, generate probabilistic behaviors via LLMs, and adapt their strategies through internal cognition and belief updates. We define two equilibrium concepts: reasoning and behavioral equilibrium, which aligns an agent's internal prompt-based reasoning with observable behavior, and conjectural reasoning equilibrium, which accounts for epistemic uncertainty through parameterized models over an opponent's response. These layered constructs capture bounded rationality, asymmetric information, and meta-cognitive adaptation. We illustrate the framework through a spearphishing case study, where a sender and a recipient engage in a deception game using structured reasoning prompts. This example highlights the cognitive richness and adversarial potential of LLM-mediated interactions. Our results show that LLM-Stackelberg games provide a powerful paradigm for modeling decision-making in domains such as cybersecurity, misinformation, and recommendation systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 12,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09495",
        "abs_url": "https://arxiv.org/abs/2507.09495",
        "pdf_url": "https://arxiv.org/pdf/2507.09495",
        "title": "GenAI-based Multi-Agent Reinforcement Learning towards Distributed Agent Intelligence: A Generative-RL Agent Perspective",
        "authors": [
            "Hang Wang",
            "Junshan Zhang"
        ],
        "comments": "Position paper",
        "subjects": "Artificial Intelligence (cs.AI); Emerging Technologies (cs.ET); Human-Computer Interaction (cs.HC); Robotics (cs.RO); Systems and Control (eess.SY)",
        "abstract": "Multi-agent reinforcement learning faces fundamental challenges that conventional approaches have failed to overcome: exponentially growing joint action spaces, non-stationary environments where simultaneous learning creates moving targets, and partial observability that constrains coordination. Current methods remain reactive, employing stimulus-response mechanisms that fail when facing novel scenarios. We argue for a transformative paradigm shift from reactive to proactive multi-agent intelligence through generative AI-based reinforcement learning. This position advocates reconceptualizing agents not as isolated policy optimizers, but as sophisticated generative models capable of synthesizing complex multi-agent dynamics and making anticipatory decisions based on predictive understanding of future interactions. Rather than responding to immediate observations, generative-RL agents can model environment evolution, predict other agents' behaviors, generate coordinated action sequences, and engage in strategic reasoning accounting for long-term dynamics. This approach leverages pattern recognition and generation capabilities of generative AI to enable proactive decision-making, seamless coordination through enhanced communication, and dynamic adaptation to evolving scenarios. We envision this paradigm shift will unlock unprecedented possibilities for distributed intelligence, moving beyond individual optimization toward emergent collective behaviors representing genuine collaborative intelligence. The implications extend across autonomous systems, robotics, and human-AI collaboration, promising solutions to coordination challenges intractable under traditional reactive frameworks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 13,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09534",
        "abs_url": "https://arxiv.org/abs/2507.09534",
        "pdf_url": "https://arxiv.org/pdf/2507.09534",
        "title": "Consistency Trajectory Planning: High-Quality and Efficient Trajectory Optimization for Offline Model-Based Reinforcement Learning",
        "authors": [
            "Guanquan Wang",
            "Takuya Hiraoka",
            "Yoshimasa Tsuruoka"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)",
        "abstract": "This paper introduces Consistency Trajectory Planning (CTP), a novel offline model-based reinforcement learning method that leverages the recently proposed Consistency Trajectory Model (CTM) for efficient trajectory optimization. While prior work applying diffusion models to planning has demonstrated strong performance, it often suffers from high computational costs due to iterative sampling procedures. CTP supports fast, single-step trajectory generation without significant degradation in policy quality. We evaluate CTP on the D4RL benchmark and show that it consistently outperforms existing diffusion-based planning methods in long-horizon, goal-conditioned tasks. Notably, CTP achieves higher normalized returns while using significantly fewer denoising steps. In particular, CTP achieves comparable performance with over $120\\times$ speedup in inference time, demonstrating its practicality and effectiveness for high-performance, low-latency offline planning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 14,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09540",
        "abs_url": "https://arxiv.org/abs/2507.09540",
        "pdf_url": "https://arxiv.org/pdf/2507.09540",
        "title": "Learning to Control Dynamical Agents via Spiking Neural Networks and Metropolis-Hastings Sampling",
        "authors": [
            "Ali Safa",
            "Farida Mohsen",
            "Ali Al-Zawqari"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Robotics (cs.RO)",
        "abstract": "Spiking Neural Networks (SNNs) offer biologically inspired, energy-efficient alternatives to traditional Deep Neural Networks (DNNs) for real-time control systems. However, their training presents several challenges, particularly for reinforcement learning (RL) tasks, due to the non-differentiable nature of spike-based communication. In this work, we introduce what is, to our knowledge, the first framework that employs Metropolis-Hastings (MH) sampling, a Bayesian inference technique, to train SNNs for dynamical agent control in RL environments without relying on gradient-based methods. Our approach iteratively proposes and probabilistically accepts network parameter updates based on accumulated reward signals, effectively circumventing the limitations of backpropagation while enabling direct optimization on neuromorphic platforms. We evaluated this framework on two standard control benchmarks: AcroBot and CartPole. The results demonstrate that our MH-based approach outperforms conventional Deep Q-Learning (DQL) baselines and prior SNN-based RL approaches in terms of maximizing the accumulated reward while minimizing network resources and training episodes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 15,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09588",
        "abs_url": "https://arxiv.org/abs/2507.09588",
        "pdf_url": "https://arxiv.org/pdf/2507.09588",
        "title": "eSapiens: A Platform for Secure and Auditable Retrieval-Augmented Generation",
        "authors": [
            "Isaac Shi",
            "Zeyuan Li",
            "Fan Liu",
            "Wenli Wang",
            "Lewei He",
            "Yang Yang",
            "Tianyu Shi"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "We present eSapiens, an AI-as-a-Service (AIaaS) platform engineered around a business-oriented trifecta: proprietary data, operational workflows, and any major agnostic Large Language Model (LLM). eSapiens gives businesses full control over their AI assets, keeping everything in-house for AI knowledge retention and data security. eSapiens AI Agents (Sapiens) empower your team by providing valuable insights and automating repetitive tasks, enabling them to focus on high-impact work and drive better business outcomes. The system integrates structured document ingestion, hybrid vector retrieval, and no-code orchestration via LangChain, and supports top LLMs including OpenAI, Claude, Gemini, and DeepSeek. A key component is the THOR Agent, which handles structured SQL-style queries and generates actionable insights over enterprise databases. To evaluate the system, we conduct two experiments. First, a retrieval benchmark on legal corpora reveals that a chunk size of 512 tokens yields the highest retrieval precision (Top-3 accuracy: 91.3%). Second, a generation quality test using TRACe metrics across five LLMs shows that eSapiens delivers more context-consistent outputs with up to a 23% improvement in factual alignment. These results demonstrate the effectiveness of eSapiens in enabling trustworthy, auditable AI workflows for high-stakes domains like legal and finance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 16,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09611",
        "abs_url": "https://arxiv.org/abs/2507.09611",
        "pdf_url": "https://arxiv.org/pdf/2507.09611",
        "title": "The Hidden Costs of AI: A Review of Energy, E-Waste, and Inequality in Model Development",
        "authors": [
            "Jenis Winsta"
        ],
        "comments": "5 pages, 3 figures",
        "subjects": "Artificial Intelligence (cs.AI); Computers and Society (cs.CY)",
        "abstract": "Artificial intelligence (AI) has made remarkable progress in recent years, yet its rapid expansion brings overlooked environmental and ethical challenges. This review explores four critical areas where AI's impact extends beyond performance: energy consumption, electronic waste (e-waste), inequality in compute access, and the hidden energy burden of cybersecurity systems. Drawing from recent studies and institutional reports, the paper highlights systemic issues such as high emissions from model training, rising hardware turnover, global infrastructure disparities, and the energy demands of securing AI. By connecting these concerns, the review contributes to Responsible AI discourse by identifying key research gaps and advocating for sustainable, transparent, and equitable development practices. Ultimately, it argues that AI's progress must align with ethical responsibility and environmental stewardship to ensure a more inclusive and sustainable technological future.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 17,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09617",
        "abs_url": "https://arxiv.org/abs/2507.09617",
        "pdf_url": "https://arxiv.org/pdf/2507.09617",
        "title": "Bridging Bots: from Perception to Action via Multimodal-LMs and Knowledge Graphs",
        "authors": [
            "Margherita Martorana",
            "Francesca Urgese",
            "Mark Adamik",
            "Ilaria Tiddi"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Robotics (cs.RO)",
        "abstract": "Personal service robots are deployed to support daily living in domestic environments, particularly for elderly and individuals requiring assistance. These robots must perceive complex and dynamic surroundings, understand tasks, and execute context-appropriate actions. However, current systems rely on proprietary, hard-coded solutions tied to specific hardware and software, resulting in siloed implementations that are difficult to adapt and scale across platforms. Ontologies and Knowledge Graphs (KGs) offer a solution to enable interoperability across systems, through structured and standardized representations of knowledge and reasoning. However, symbolic systems such as KGs and ontologies struggle with raw and noisy sensory input. In contrast, multimodal language models are well suited for interpreting input such as images and natural language, but often lack transparency, consistency, and knowledge grounding. In this work, we propose a neurosymbolic framework that combines the perceptual strengths of multimodal language models with the structured representations provided by KGs and ontologies, with the aim of supporting interoperability in robotic applications. Our approach generates ontology-compliant KGs that can inform robot behavior in a platform-independent manner. We evaluated this framework by integrating robot perception data, ontologies, and five multimodal models (three LLaMA and two GPT models), using different modes of neural-symbolic interaction. We assess the consistency and effectiveness of the generated KGs across multiple runs and configurations, and perform statistical analyzes to evaluate performance. Results show that GPT-o1 and LLaMA 4 Maverick consistently outperform other models. However, our findings also indicate that newer models do not guarantee better results, highlighting the critical role of the integration strategy in generating ontology-compliant KGs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 18,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09626",
        "abs_url": "https://arxiv.org/abs/2507.09626",
        "pdf_url": "https://arxiv.org/pdf/2507.09626",
        "title": "humancompatible.interconnect: Testing Properties of Repeated Uses of Interconnections of AI Systems",
        "authors": [
            "Rodion Nazarov",
            "Anthony Quinn",
            "Robert Shorten",
            "Jakub Marecek"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Systems and Control (eess.SY)",
        "abstract": "Artificial intelligence (AI) systems often interact with multiple agents. The regulation of such AI systems often requires that {\\em a priori\\/} guarantees of fairness and robustness be satisfied. With stochastic models of agents' responses to the outputs of AI systems, such {\\em a priori\\/} guarantees require non-trivial reasoning about the corresponding stochastic systems. Here, we present an open-source PyTorch-based toolkit for the use of stochastic control techniques in modelling interconnections of AI systems and properties of their repeated uses. It models robustness and fairness desiderata in a closed-loop fashion, and provides {\\em a priori\\/} guarantees for these interconnections. The PyTorch-based toolkit removes much of the complexity associated with the provision of fairness guarantees for closed-loop models of multi-agent systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 19,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09662",
        "abs_url": "https://arxiv.org/abs/2507.09662",
        "pdf_url": "https://arxiv.org/pdf/2507.09662",
        "title": "Towards Concise and Adaptive Thinking in Large Reasoning Models: A Survey",
        "authors": [
            "Jason Zhu",
            "Hongyu Li"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Large reasoning models (LRMs) like OpenAI o1 and DeepSeek R1 have demonstrated impressive performance on complex reasoning tasks like mathematics and programming with long Chain-of-Thought (CoT) reasoning sequences (slow-thinking), compared with traditional large language models (fast-thinking). However, these reasoning models also face a huge challenge that generating unnecessarily lengthy and redundant reasoning chains even for trivial questions. This phenomenon leads to a significant waste of inference resources, increases the response time for simple queries, and hinders the practical application of LRMs in real-world products. To this end, it is crucial to shorten lengthy reasoning chains and learn adaptive reasoning between fast and slow thinking based on input difficulty. In this survey, we provide a comprehensive overview of recent progress in concise and adaptive thinking for efficient reasoning of LRMs, including methodologies, benchmarks, and challenges for future exploration. We hope this survey can help researchers quickly understand the landscape of this field and inspire novel adaptive thinking ideas to facilitate better usage of LRMs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 20,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09742",
        "abs_url": "https://arxiv.org/abs/2507.09742",
        "pdf_url": "https://arxiv.org/pdf/2507.09742",
        "title": "Causality-informed Anomaly Detection in Partially Observable Sensor Networks: Moving beyond Correlations",
        "authors": [
            "Xiaofeng Xiao",
            "Bo Shen",
            "Xubo Yue"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Nowadays, as AI-driven manufacturing becomes increasingly popular, the volume of data streams requiring real-time monitoring continues to grow. However, due to limited resources, it is impractical to place sensors at every location to detect unexpected shifts. Therefore, it is necessary to develop an optimal sensor placement strategy that enables partial observability of the system while detecting anomalies as quickly as possible. Numerous approaches have been proposed to address this challenge; however, most existing methods consider only variable correlations and neglect a crucial factor: Causality. Moreover, although a few techniques incorporate causal analysis, they rely on interventions-artificially creating anomalies-to identify causal effects, which is impractical and might lead to catastrophic losses. In this paper, we introduce a causality-informed deep Q-network (Causal DQ) approach for partially observable sensor placement in anomaly detection. By integrating causal information at each stage of Q-network training, our method achieves faster convergence and tighter theoretical error bounds. Furthermore, the trained causal-informed Q-network significantly reduces the detection time for anomalies under various settings, demonstrating its effectiveness for sensor placement in large-scale, real-world data streams. Beyond the current implementation, our technique's fundamental insights can be applied to various reinforcement learning problems, opening up new possibilities for real-world causality-informed machine learning methods in engineering applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 21,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09751",
        "abs_url": "https://arxiv.org/abs/2507.09751",
        "pdf_url": "https://arxiv.org/pdf/2507.09751",
        "title": "Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations",
        "authors": [
            "Bradley P. Allen",
            "Prateek Chhikara",
            "Thomas Macaulay Ferguson",
            "Filip Ilievski",
            "Paul Groth"
        ],
        "comments": "29 pages, 9 tables, 3 figures. Accepted to the 19th Conference on Neurosymbolic Learning and Reasoning (NeSy 2025)",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Logic in Computer Science (cs.LO)",
        "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, but they exhibit problems with logical consistency in the output they generate. How can we harness LLMs' broad-coverage parametric knowledge in formal reasoning despite their inconsistency? We present a method for directly integrating an LLM into the interpretation function of the formal semantics for a paraconsistent logic. We provide experimental evidence for the feasibility of the method by evaluating the function using datasets created from several short-form factuality benchmarks. Unlike prior work, our method offers a theoretical framework for neuro-symbolic reasoning that leverages an LLM's knowledge while preserving the underlying logic's soundness and completeness properties.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 22,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09801",
        "abs_url": "https://arxiv.org/abs/2507.09801",
        "pdf_url": "https://arxiv.org/pdf/2507.09801",
        "title": "Technical Requirements for Halting Dangerous AI Activities",
        "authors": [
            "Peter Barnett",
            "Aaron Scher",
            "David Abecassis"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computers and Society (cs.CY)",
        "abstract": "The rapid development of AI systems poses unprecedented risks, including loss of control, misuse, geopolitical instability, and concentration of power. To navigate these risks and avoid worst-case outcomes, governments may proactively establish the capability for a coordinated halt on dangerous AI development and deployment. In this paper, we outline key technical interventions that could allow for a coordinated halt on dangerous AI activities. We discuss how these interventions may contribute to restricting various dangerous AI activities, and show how these interventions can form the technical foundation for potential AI governance plans.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 23,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09850",
        "abs_url": "https://arxiv.org/abs/2507.09850",
        "pdf_url": "https://arxiv.org/pdf/2507.09850",
        "title": "The Challenge of Teaching Reasoning to LLMs Without RL or Distillation",
        "authors": [
            "Wei Du",
            "Branislav Kisacanin",
            "George Armstrong",
            "Shubham Toshniwal",
            "Ivan Moshkov",
            "Alexan Ayrapetyan",
            "Sadegh Mahdavi",
            "Dan Zhao",
            "Shizhe Diao",
            "Dragan Masulovic",
            "Marius Stanean",
            "Advaith Avadhanam",
            "Max Wang",
            "Ashmit Dutta",
            "Shitij Govil",
            "Sri Yanamandara",
            "Mihir Tandon",
            "Sriram Ananthakrishnan",
            "Vedant Rathi",
            "David Zhang",
            "Joonseok Kang",
            "Leon Luo",
            "Titu Andreescu",
            "Boris Ginsburg",
            "Igor Gitman"
        ],
        "comments": "Accepted at the Second AI for Math Workshop at the 42nd International Conference on Machine Learning (ICML 2025)",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Reasoning-capable language models achieve state-of-the-art performance in diverse complex tasks by generating long, explicit Chain-of-Thought (CoT) traces. While recent works show that base models can acquire such reasoning traces via reinforcement learning or distillation from stronger models like DeepSeek-R1, previous works demonstrate that even short CoT prompting without fine-tuning is able to improve reasoning. We ask whether long CoT can be induced in a base model using only prompting or minimal tuning. Using just 20 long CoT examples from the reasoning model \\texttt{QwQ-32B-Preview}, we lightly fine-tune the base model \\texttt{Qwen2.5-32B}. The resulting model outperforms the much larger \\texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of high-quality examples can unlock strong reasoning capabilities. We further explore using CoT data from non-reasoning models and human annotators, enhanced with prompt engineering, multi-pass editing, and structural guidance. However, neither matches the performance of reasoning model traces, suggesting that certain latent qualities of expert CoT are difficult to replicate. We analyze key properties of reasoning data, such as problem difficulty, diversity, and answer length, that influence reasoning distillation. While challenges remain, we are optimistic that carefully curated human-written CoT, even in small quantities, can activate reasoning behaviors in base models. We release our human-authored dataset across refinement stages and invite further investigation into what makes small-scale reasoning supervision so effective.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 24,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09854",
        "abs_url": "https://arxiv.org/abs/2507.09854",
        "pdf_url": "https://arxiv.org/pdf/2507.09854",
        "title": "Model-Grounded Symbolic Artificial Intelligence Systems Learning and Reasoning with Model-Grounded Symbolic Artificial Intelligence Systems",
        "authors": [
            "Aniruddha Chattopadhyay",
            "Raj Dandekar",
            "Kaushik Roy"
        ],
        "comments": "Accepted as paper in 19th International Conference on Neurosymbolic Learning and Reasoning,NeSy 2025",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Neurosymbolic artificial intelligence (AI) systems combine neural network and classical symbolic AI mechanisms to exploit the complementary strengths of large scale, generalizable learning and robust, verifiable reasoning. Numerous classifications of neurosymbolic AI illustrate how these two components can be integrated in distinctly different ways. In this work, we propose reinterpreting instruction tuned large language models as model grounded symbolic AI systems where natural language serves as the symbolic layer and grounding is achieved through the models internal representation space. Within this framework, we investigate and develop novel learning and reasoning approaches that preserve structural similarities to traditional learning and reasoning paradigms. Preliminary evaluations across axiomatic deductive reasoning procedures of varying complexity provide insights into the effectiveness of our approach in improving learning efficiency and reasoning reliability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 25,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09884",
        "abs_url": "https://arxiv.org/abs/2507.09884",
        "pdf_url": "https://arxiv.org/pdf/2507.09884",
        "title": "VerifyBench: A Systematic Benchmark for Evaluating Reasoning Verifiers Across Domains",
        "authors": [
            "Xuzhao Li",
            "Xuchen Li",
            "Shiyu Hu",
            "Yongzhen Guo",
            "Wentao Zhang"
        ],
        "comments": "Preprint, Under review",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Large language models (LLMs) increasingly rely on reinforcement learning (RL) to enhance their reasoning capabilities through feedback. A critical challenge is verifying the consistency of model-generated responses and reference answers, since these responses are often lengthy, diverse, and nuanced. Rule-based verifiers struggle with complexity, prompting the use of model-based verifiers. However, specialized verifiers lack flexibility, while general LLM judges can be inconsistent. Existing research primarily focuses on building better verifiers, yet a systematic evaluation of different types of verifiers' performance across domains remains lacking, severely constraining the reliable development of Reinforcement Learning with Verifiable Reward (RLVR). To address this, we propose VerifyBench--a cross-domain comprehensive benchmark for systematically evaluating verifiers. We construct 4,000 expert-level questions covering mathematics, physics, chemistry, and biology. Each question is equipped with reference answers and diverse responses. The reliability of the evaluation is ensured through a rigorous annotation process conducted by a multidisciplinary expert team. We design a four-dimensional experimental framework to comprehensively compare the performance boundaries of specialized verifiers and general LLMs under combined conditions of extracted answers vs. complete responses, and short vs. long outputs. Our evaluation uncovers fundamental trade-offs in verifiers: while specialized verifiers achieve leading accuracy, they exhibit deficiencies in recall; general models show stronger inclusivity but unstable precision. More importantly, we discover verifiers' high sensitivity to input structure and inherent limitations in cross-domain generalization, providing critical insights into the bottlenecks of current verifier technology.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 26,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09955",
        "abs_url": "https://arxiv.org/abs/2507.09955",
        "pdf_url": "https://arxiv.org/pdf/2507.09955",
        "title": "DeepSeek: Paradigm Shifts and Technical Evolution in Large AI Models",
        "authors": [
            "Luolin Xiong",
            "Haofen Wang",
            "Xi Chen",
            "Lu Sheng",
            "Yun Xiong",
            "Jingping Liu",
            "Yanghua Xiao",
            "Huajun Chen",
            "Qing-Long Han",
            "Yang Tang"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "DeepSeek, a Chinese Artificial Intelligence (AI) startup, has released their V3 and R1 series models, which attracted global attention due to their low cost, high performance, and open-source advantages. This paper begins by reviewing the evolution of large AI models focusing on paradigm shifts, the mainstream Large Language Model (LLM) paradigm, and the DeepSeek paradigm. Subsequently, the paper highlights novel algorithms introduced by DeepSeek, including Multi-head Latent Attention (MLA), Mixture-of-Experts (MoE), Multi-Token Prediction (MTP), and Group Relative Policy Optimization (GRPO). The paper then explores DeepSeek engineering breakthroughs in LLM scaling, training, inference, and system-level optimization architecture. Moreover, the impact of DeepSeek models on the competitive AI landscape is analyzed, comparing them to mainstream LLMs across various fields. Finally, the paper reflects on the insights gained from DeepSeek innovations and discusses future trends in the technical and engineering development of large AI models, particularly in data, training, and reasoning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 27,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09989",
        "abs_url": "https://arxiv.org/abs/2507.09989",
        "pdf_url": "https://arxiv.org/pdf/2507.09989",
        "title": "Improving monotonic optimization in heterogeneous multi-agent reinforcement learning with optimal marginal deterministic policy gradient",
        "authors": [
            "Xiaoyang Yu",
            "Youfang Lin",
            "Shuo Wang",
            "Sheng Han"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)",
        "abstract": "In heterogeneous multi-agent reinforcement learning (MARL), achieving monotonic improvement plays a pivotal role in enhancing performance. The HAPPO algorithm proposes a feasible solution by introducing a sequential update scheme, which requires independent learning with No Parameter-sharing (NoPS). However, heterogeneous MARL generally requires Partial Parameter-sharing (ParPS) based on agent grouping to achieve high cooperative performance. Our experiments prove that directly combining ParPS with the sequential update scheme leads to the policy updating baseline drift problem, thereby failing to achieve improvement. To solve the conflict between monotonic improvement and ParPS, we propose the Optimal Marginal Deterministic Policy Gradient (OMDPG) algorithm. First, we replace the sequentially computed $Q_{\\psi}^s(s,a_{1:i})$ with the Optimal Marginal Q (OMQ) function $\\phi_{\\psi}^*(s,a_{1:i})$ derived from Q-functions. This maintains MAAD's monotonic improvement while eliminating the conflict through optimal joint action sequences instead of sequential policy ratio calculations. Second, we introduce the Generalized Q Critic (GQC) as the critic function, employing pessimistic uncertainty-constrained loss to optimize different Q-value estimations. This provides the required Q-values for OMQ computation and stable baselines for actor updates. Finally, we implement a Centralized Critic Grouped Actor (CCGA) architecture that simultaneously achieves ParPS in local policy networks and accurate global Q-function computation. Experimental results in SMAC and MAMuJoCo environments demonstrate that OMDPG outperforms various state-of-the-art MARL baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 28,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10000",
        "abs_url": "https://arxiv.org/abs/2507.10000",
        "pdf_url": "https://arxiv.org/pdf/2507.10000",
        "title": "On The Role of Intentionality in Knowledge Representation: Analyzing Scene Context for Cognitive Agents with a Tiny Language Model",
        "authors": [
            "Mark Burgess"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Since Searle's work deconstructing intent and intentionality in the realm of philosophy, the practical meaning of intent has received little attention in science and technology. Intentionality and context are both central to the scope of Promise Theory's model of Semantic Spacetime, used as an effective Tiny Language Model. One can identify themes and concepts from a text, on a low level (without knowledge of the specific language) by using process coherence as a guide. Any agent process can assess superficially a degree of latent `intentionality' in data by looking for anomalous multi-scale anomalies and assessing the work done to form them. Scale separation can be used to sort parts into `intended' content and `ambient context', using the spacetime coherence as a measure. This offers an elementary but pragmatic interpretation of latent intentionality for very low computational cost, and without reference to extensive training or reasoning capabilities. The process is well within the reach of basic organisms as it does not require large scale artificial probabilistic batch processing. The level of concept formation depends, however, on the memory capacity of the agent.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 29,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10007",
        "abs_url": "https://arxiv.org/abs/2507.10007",
        "pdf_url": "https://arxiv.org/pdf/2507.10007",
        "title": "Deep Hidden Cognition Facilitates Reliable Chain-of-Thought Reasoning",
        "authors": [
            "Zijun Chen",
            "Wenbo Hu",
            "Richang Hong"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Chain of Thought (CoT) reasoning has demonstrated remarkable deep reasoning capabilities in both large language models (LLMs) and multimodal large language models (MLLMs). However, its reliability is often undermined by the accumulation of errors in intermediate steps. This paper introduces an novel approach to calibrate the CoT reasoning accuracy by leveraging the model's intrinsic veracity encoding. We discover that specific attention head activations reliably reflect the truthfulness of reasoning steps in CoT. Based on this insight, we train a confidence predictor to evaluate the correctness of each reasoning step using these truthfulness-sensitive activations, dynamically selecting the most plausible reasoning path via beam search. Experimental results demonstrate that our method significantly outperforms the state-of-the-art baselines (e.g., Few-Shot CoT, Self-Consistency, and Self-Evaluation Guided Beam Search) across the mathematical, symbolic, and commonsense reasoning tasks, exhibiting superior accuracy and reliability in both unimodal and multimodal settings. We further validate the approach on large reasoning models, confirming its applicability to specialized reasoning models. Additionally, we explore the role of the model's self-correction ability in CoT reasoning. This work provides a novel reliability improvement path for CoT reasoning with broad application potential.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 30,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10045",
        "abs_url": "https://arxiv.org/abs/2507.10045",
        "pdf_url": "https://arxiv.org/pdf/2507.10045",
        "title": "Automating SPARQL Query Translations between DBpedia and Wikidata",
        "authors": [
            "Malte Christian Bartels",
            "Debayan Banerjee",
            "Ricardo Usbeck"
        ],
        "comments": "18 pages, 2 figues. Paper accepted at SEMANTiCS 2025 conference happening on September 2025",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "This paper investigates whether state-of-the-art Large Language Models (LLMs) can automatically translate SPARQL between popular Knowledge Graph (KG) schemas. We focus on translations between the DBpedia and Wikidata KG, and later on DBLP and OpenAlex KG. This study addresses a notable gap in KG interoperability research by rigorously evaluating LLM performance on SPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first align 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100 DBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic KGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and Mistral-Large-Instruct-2407 are selected based on their sizes and architectures and tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs were compared with gold answers, and resulting errors were categorized. We find that the performance varies markedly across models and prompting strategies, and that translations for Wikidata to DBpedia work far better than translations for DBpedia to Wikidata.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 31,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10076",
        "abs_url": "https://arxiv.org/abs/2507.10076",
        "pdf_url": "https://arxiv.org/pdf/2507.10076",
        "title": "On Gradual Semantics for Assumption-Based Argumentation",
        "authors": [
            "Anna Rapberger",
            "Fabrizio Russo",
            "Antonio Rago",
            "Francesca Toni"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "In computational argumentation, gradual semantics are fine-grained alternatives to extension-based and labelling-based semantics . They ascribe a dialectical strength to (components of) arguments sanctioning their degree of acceptability. Several gradual semantics have been studied for abstract, bipolar and quantitative bipolar argumentation frameworks (QBAFs), as well as, to a lesser extent, for some forms of structured argumentation. However, this has not been the case for assumption-based argumentation (ABA), despite it being a popular form of structured argumentation with several applications where gradual semantics could be useful. In this paper, we fill this gap and propose a family of novel gradual semantics for equipping assumptions, which are the core components in ABA frameworks, with dialectical strengths. To do so, we use bipolar set-based argumentation frameworks as an abstraction of (potentially non-flat) ABA frameworks and generalise state-of-the-art modular gradual semantics for QBAFs. We show that our gradual ABA semantics satisfy suitable adaptations of desirable properties of gradual QBAF semantics, such as balance and monotonicity. We also explore an argument-based approach that leverages established QBAF modular semantics directly, and use it as baseline. Finally, we conduct experiments with synthetic ABA frameworks to compare our gradual ABA semantics with its argument-based counterpart and assess convergence.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 32,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10106",
        "abs_url": "https://arxiv.org/abs/2507.10106",
        "pdf_url": "https://arxiv.org/pdf/2507.10106",
        "title": "BlueGlass: A Framework for Composite AI Safety",
        "authors": [
            "Harshal Nandigramwar",
            "Syed Qutub",
            "Kay-Ulrich Scholl"
        ],
        "comments": "Accepted at ICML 2025 [Actionable Interpretability Workshop]",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "As AI systems become increasingly capable and ubiquitous, ensuring the safety of these systems is critical. However, existing safety tools often target different aspects of model safety and cannot provide full assurance in isolation, highlighting a need for integrated and composite methodologies. This paper introduces BlueGlass, a framework designed to facilitate composite AI safety workflows by providing a unified infrastructure enabling the integration and composition of diverse safety tools that operate across model internals and outputs. Furthermore, to demonstrate the utility of this framework, we present three safety-oriented analyses on vision-language models for the task of object detection: (1) distributional evaluation, revealing performance trade-offs and potential failure modes across distributions; (2) probe-based analysis of layer dynamics highlighting shared hierarchical learning via phase transition; and (3) sparse autoencoders identifying interpretable concepts. More broadly, this work contributes foundational infrastructure and findings for building more robust and reliable AI systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 33,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10119",
        "abs_url": "https://arxiv.org/abs/2507.10119",
        "pdf_url": "https://arxiv.org/pdf/2507.10119",
        "title": "Analysis of AI Techniques for Orchestrating Edge-Cloud Application Migration",
        "authors": [
            "Sadig Gojayev",
            "Ahmad Anaqreh",
            "Carolina Fortuna"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Application migration in edge-cloud system enables high QoS and cost effective service delivery. However, automatically orchestrating such migration is typically solved with heuristic approaches. Starting from the Markov Decision Process (MDP), in this paper, we identify, analyze and compare selected state-of-the-art Artificial Intelligence (AI) planning and Reinforcement Learning (RL) approaches for solving the class of edge-cloud application migration problems that can be modeled as Towers of Hanoi (ToH) problems. We introduce a new classification based on state space definition and analyze the compared models also through this lense. The aim is to understand available techniques capable of orchestrating such application migration in emerging computing continuum environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 34,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10124",
        "abs_url": "https://arxiv.org/abs/2507.10124",
        "pdf_url": "https://arxiv.org/pdf/2507.10124",
        "title": "Could you be wrong: Debiasing LLMs using a metacognitive prompt for improving human decision making",
        "authors": [
            "Thomas T. Hills"
        ],
        "comments": "12 pages, 3 figures",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Identifying bias in LLMs is ongoing. Because they are still in development, what is true today may be false tomorrow. We therefore need general strategies for debiasing that will outlive current models. Strategies developed for debiasing human decision making offer one promising approach as they incorporate an LLM-style prompt intervention designed to bring latent knowledge into awareness during decision making. LLMs trained on vast amounts of information contain information about potential biases, counter-arguments, and contradictory evidence, but that information may only be brought to bear if prompted. Metacognitive prompts developed in the human decision making literature are designed to achieve this, and as I demonstrate here, they show promise with LLMs. The prompt I focus on here is \"could you be wrong?\" Following an LLM response, this prompt leads LLMs to produce additional information, including why they answered as they did, errors, biases, contradictory evidence, and alternatives, none of which were apparent in their initial response. Indeed, this metaknowledge often reveals that how LLMs and users interpret prompts are not aligned. Here I demonstrate this prompt using a set of questions taken from recent articles about LLM biases, including implicit discriminatory biases and failures of metacognition. \"Could you be wrong\" prompts the LLM to identify its own biases and produce cogent metacognitive reflection. I also present another example involving convincing but incomplete information, which is readily corrected by the metacognitive prompt. In sum, this work argues that human psychology offers a new avenue for prompt engineering, leveraging a long history of effective prompt-based improvements to human decision making.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 35,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10134",
        "abs_url": "https://arxiv.org/abs/2507.10134",
        "pdf_url": "https://arxiv.org/pdf/2507.10134",
        "title": "FRSICL: LLM-Enabled In-Context Learning Flight Resource Allocation for Fresh Data Collection in UAV-Assisted Wildfire Monitoring",
        "authors": [
            "Yousef Emami",
            "Hao Zhou",
            "Miguel Gutierrez Gaitan",
            "Kai Li",
            "Luis Almeida"
        ],
        "comments": "8 pages, 8 figures",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Unmanned Aerial Vehicles (UAVs) are vital for public safety, particularly in wildfire monitoring, where early detection minimizes environmental impact. In UAV-Assisted Wildfire Monitoring (UAWM) systems, joint optimization of sensor transmission scheduling and velocity is critical for minimizing Age of Information (AoI) from stale sensor data. Deep Reinforcement Learning (DRL) has been used for such optimization; however, its limitations such as low sampling efficiency, simulation-to-reality gaps, and complex training render it unsuitable for time-critical applications like wildfire monitoring. This paper introduces a new online Flight Resource Allocation scheme based on LLM-Enabled In-Context Learning (FRSICL) to jointly optimize the UAV's flight control and data collection schedule along the trajectory in real time, thereby asymptotically minimizing the average AoI across ground sensors. In contrast to DRL, FRSICL generates data collection schedules and controls velocity using natural language task descriptions and feedback from the environment, enabling dynamic decision-making without extensive retraining. Simulation results confirm the effectiveness of the proposed FRSICL compared to Proximal Policy Optimization (PPO) and Nearest-Neighbor baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 36,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10142",
        "abs_url": "https://arxiv.org/abs/2507.10142",
        "pdf_url": "https://arxiv.org/pdf/2507.10142",
        "title": "Adaptability in Multi-Agent Reinforcement Learning: A Framework and Unified Review",
        "authors": [
            "Siyi Hu",
            "Mohamad A Hady",
            "Jianglin Qiao",
            "Jimmy Cao",
            "Mahardhika Pratama",
            "Ryszard Kowalczyk"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multiagent Systems (cs.MA)",
        "abstract": "Multi-Agent Reinforcement Learning (MARL) has shown clear effectiveness in coordinating multiple agents across simulated benchmarks and constrained scenarios. However, its deployment in real-world multi-agent systems (MAS) remains limited, primarily due to the complex and dynamic nature of such environments. These challenges arise from multiple interacting sources of variability, including fluctuating agent populations, evolving task goals, and inconsistent execution conditions. Together, these factors demand that MARL algorithms remain effective under continuously changing system configurations and operational demands. To better capture and assess this capacity for adjustment, we introduce the concept of \\textit{adaptability} as a unified and practically grounded lens through which to evaluate the reliability of MARL algorithms under shifting conditions, broadly referring to any changes in the environment dynamics that may occur during learning or execution. Centred on the notion of adaptability, we propose a structured framework comprising three key dimensions: learning adaptability, policy adaptability, and scenario-driven adaptability. By adopting this adaptability perspective, we aim to support more principled assessments of MARL performance beyond narrowly defined benchmarks. Ultimately, this survey contributes to the development of algorithms that are better suited for deployment in dynamic, real-world multi-agent systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 37,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10156",
        "abs_url": "https://arxiv.org/abs/2507.10156",
        "pdf_url": "https://arxiv.org/pdf/2507.10156",
        "title": "Introducing the Swiss Food Knowledge Graph: AI for Context-Aware Nutrition Recommendation",
        "authors": [
            "Lubnaa Abdur Rahman",
            "Ioannis Papathanail",
            "Stavroula Mougiakakou"
        ],
        "comments": "10 pages, 2 Figures, 7 tables",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "AI has driven significant progress in the nutrition field, especially through multimedia-based automatic dietary assessment. However, existing automatic dietary assessment systems often overlook critical non-visual factors, such as recipe-specific ingredient substitutions that can significantly alter nutritional content, and rarely account for individual dietary needs, including allergies, restrictions, cultural practices, and personal preferences. In Switzerland, while food-related information is available, it remains fragmented, and no centralized repository currently integrates all relevant nutrition-related aspects within a Swiss context. To bridge this divide, we introduce the Swiss Food Knowledge Graph (SwissFKG), the first resource, to our best knowledge, to unite recipes, ingredients, and their substitutions with nutrient data, dietary restrictions, allergen information, and national nutrition guidelines under one graph. We establish a LLM-powered enrichment pipeline for populating the graph, whereby we further present the first benchmark of four off-the-shelf (<70 B parameter) LLMs for food knowledge augmentation. Our results demonstrate that LLMs can effectively enrich the graph with relevant nutritional information. Our SwissFKG goes beyond recipe recommendations by offering ingredient-level information such as allergen and dietary restriction information, and guidance aligned with nutritional guidelines. Moreover, we implement a Graph-RAG application to showcase how the SwissFKG's rich natural-language data structure can help LLM answer user-specific nutrition queries, and we evaluate LLM-embedding pairings by comparing user-query responses against predefined expected answers. As such, our work lays the foundation for the next generation of dietary assessment tools that blend visual, contextual, and cultural dimensions of eating.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 38,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10174",
        "abs_url": "https://arxiv.org/abs/2507.10174",
        "pdf_url": "https://arxiv.org/pdf/2507.10174",
        "title": "Should We Ever Prefer Decision Transformer for Offline Reinforcement Learning?",
        "authors": [
            "Yumi Omori",
            "Zixuan Dong",
            "Keith Ross"
        ],
        "comments": "Accepted by RLBrew: Ingredients for Developing Generalist Agents workshop (RLC 2025)",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "In recent years, extensive work has explored the application of the Transformer architecture to reinforcement learning problems. Among these, Decision Transformer (DT) has gained particular attention in the context of offline reinforcement learning due to its ability to frame return-conditioned policy learning as a sequence modeling task. Most recently, Bhargava et al. (2024) provided a systematic comparison of DT with more conventional MLP-based offline RL algorithms, including Behavior Cloning (BC) and Conservative Q-Learning (CQL), and claimed that DT exhibits superior performance in sparse-reward and low-quality data settings. In this paper, through experimentation on robotic manipulation tasks (Robomimic) and locomotion benchmarks (D4RL), we show that MLP-based Filtered Behavior Cloning (FBC) achieves competitive or superior performance compared to DT in sparse-reward environments. FBC simply filters out low-performing trajectories from the dataset and then performs ordinary behavior cloning on the filtered dataset. FBC is not only very straightforward, but it also requires less training data and is computationally more efficient. The results therefore suggest that DT is not preferable for sparse-reward environments. From prior work, arguably, DT is also not preferable for dense-reward environments. Thus, we pose the question: Is DT ever preferable?",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 39,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10208",
        "abs_url": "https://arxiv.org/abs/2507.10208",
        "pdf_url": "https://arxiv.org/pdf/2507.10208",
        "title": "Survey for Categorising Explainable AI Studies Using Data Analysis Task Frameworks",
        "authors": [
            "Hamzah Ziadeh",
            "Hendrik Knoche"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)",
        "abstract": "Research into explainable artificial intelligence (XAI) for data analysis tasks suffer from a large number of contradictions and lack of concrete design recommendations stemming from gaps in understanding the tasks that require AI assistance. In this paper, we drew on multiple fields such as visual analytics, cognition, and dashboard design to propose a method for categorising and comparing XAI studies under three dimensions: what, why, and who. We identified the main problems as: inadequate descriptions of tasks, context-free studies, and insufficient testing with target users. We propose that studies should specifically report on their users' domain, AI, and data analysis expertise to illustrate the generalisability of their findings. We also propose study guidelines for designing and reporting XAI tasks to improve the XAI community's ability to parse the rapidly growing field. We hope that our contribution can help researchers and designers better identify which studies are most relevant to their work, what gaps exist in the research, and how to handle contradictory results regarding XAI design.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 40,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10281",
        "abs_url": "https://arxiv.org/abs/2507.10281",
        "pdf_url": "https://arxiv.org/pdf/2507.10281",
        "title": "Toward Real-World Table Agents: Capabilities, Workflows, and Design Principles for LLM-based Table Intelligence",
        "authors": [
            "Jiaming Tian",
            "Liyao Li",
            "Wentao Ye",
            "Haobo Wang",
            "Lingxin Wang",
            "Lihua Yu",
            "Zujie Ren",
            "Gang Chen",
            "Junbo Zhao"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Databases (cs.DB)",
        "abstract": "Tables are fundamental in domains such as finance, healthcare, and public administration, yet real-world table tasks often involve noise, structural heterogeneity, and semantic complexity--issues underexplored in existing research that primarily targets clean academic datasets. This survey focuses on LLM-based Table Agents, which aim to automate table-centric workflows by integrating preprocessing, reasoning, and domain adaptation. We define five core competencies--C1: Table Structure Understanding, C2: Table and Query Semantic Understanding, C3: Table Retrieval and Compression, C4: Executable Reasoning with Traceability, and C5: Cross-Domain Generalization--to analyze and compare current approaches. In addition, a detailed examination of the Text-to-SQL Agent reveals a performance gap between academic benchmarks and real-world scenarios, especially for open-source models. Finally, we provide actionable insights to improve the robustness, generalization, and efficiency of LLM-based Table Agents in practical settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 41,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10397",
        "abs_url": "https://arxiv.org/abs/2507.10397",
        "pdf_url": "https://arxiv.org/pdf/2507.10397",
        "title": "Instance space analysis of the capacitated vehicle routing problem",
        "authors": [
            "Alessandra M. M. M. Gouvêa",
            "Nuno Paulos",
            "Eduardo Uchoa e Mariá C. V. Nascimento"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "This paper seeks to advance CVRP research by addressing the challenge of understanding the nuanced relationships between instance characteristics and metaheuristic (MH) performance. We present Instance Space Analysis (ISA) as a valuable tool that allows for a new perspective on the field. By combining the ISA methodology with a dataset from the DIMACS 12th Implementation Challenge on Vehicle Routing, our research enabled the identification of 23 relevant instance characteristics. Our use of the PRELIM, SIFTED, and PILOT stages, which employ dimensionality reduction and machine learning methods, allowed us to create a two-dimensional projection of the instance space to understand how the structure of instances affect the behavior of MHs. A key contribution of our work is that we provide a projection matrix, which makes it straightforward to incorporate new instances into this analysis and allows for a new method for instance analysis in the CVRP field.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 42,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10421",
        "abs_url": "https://arxiv.org/abs/2507.10421",
        "pdf_url": "https://arxiv.org/pdf/2507.10421",
        "title": "SentiDrop: A Multi Modal Machine Learning model for Predicting Dropout in Distance Learning",
        "authors": [
            "Meriem Zerkouk",
            "Miloud Mihoubi",
            "Belkacem Chikhaoui"
        ],
        "comments": "International Conference on Education and New Learning Technologies (2025)",
        "subjects": "Artificial Intelligence (cs.AI); Emerging Technologies (cs.ET); Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "School dropout is a serious problem in distance learning, where early detection is crucial for effective intervention and student perseverance. Predicting student dropout using available educational data is a widely researched topic in learning analytics. Our partner's distance learning platform highlights the importance of integrating diverse data sources, including socio-demographic data, behavioral data, and sentiment analysis, to accurately predict dropout risks. In this paper, we introduce a novel model that combines sentiment analysis of student comments using the Bidirectional Encoder Representations from Transformers (BERT) model with socio-demographic and behavioral data analyzed through Extreme Gradient Boosting (XGBoost). We fine-tuned BERT on student comments to capture nuanced sentiments, which were then merged with key features selected using feature importance techniques in XGBoost. Our model was tested on unseen data from the next academic year, achieving an accuracy of 84\\%, compared to 82\\% for the baseline model. Additionally, the model demonstrated superior performance in other metrics, such as precision and F1-score. The proposed method could be a vital tool in developing personalized strategies to reduce dropout rates and encourage student perseverance",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 43,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10446",
        "abs_url": "https://arxiv.org/abs/2507.10446",
        "pdf_url": "https://arxiv.org/pdf/2507.10446",
        "title": "Acquiring and Adapting Priors for Novel Tasks via Neural Meta-Architectures",
        "authors": [
            "Sudarshan Babu"
        ],
        "comments": "arXiv admin note: text overlap with arXiv:2310.17075",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "The ability to transfer knowledge from prior experiences to novel tasks stands as a pivotal capability of intelligent agents, including both humans and computational models. This principle forms the basis of transfer learning, where large pre-trained neural networks are fine-tuned to adapt to downstream tasks. Transfer learning has demonstrated tremendous success, both in terms of task adaptation speed and performance. However there are several domains where, due to lack of data, training such large pre-trained models or foundational models is not a possibility - computational chemistry, computational immunology, and medical imaging are examples. To address these challenges, our work focuses on designing architectures to enable efficient acquisition of priors when large amounts of data are unavailable. In particular, we demonstrate that we can use neural memory to enable adaptation on non-stationary distributions with only a few samples. Then we demonstrate that our hypernetwork designs (a network that generates another network) can acquire more generalizable priors than standard networks when trained with Model Agnostic Meta-Learning (MAML). Subsequently, we apply hypernetworks to 3D scene generation, demonstrating that they can acquire priors efficiently on just a handful of training scenes, thereby leading to faster text-to-3D generation. We then extend our hypernetwork framework to perform 3D segmentation on novel scenes with limited data by efficiently transferring priors from earlier viewed scenes. Finally, we repurpose an existing molecular generative method as a pre-training framework that facilitates improved molecular property prediction, addressing critical challenges in computational immunology.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 44,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10522",
        "abs_url": "https://arxiv.org/abs/2507.10522",
        "pdf_url": "https://arxiv.org/pdf/2507.10522",
        "title": "DeepResearch$^{\\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology",
        "authors": [
            "Jennifer D'Souza",
            "Endres Keno Sander",
            "Andrei Aioanei"
        ],
        "comments": "12 pages, 3 figures",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Multiagent Systems (cs.MA)",
        "abstract": "We introduce DeepResearch$^{\\text{Eco}}$, a novel agentic LLM-based system for automated scientific synthesis that supports recursive, depth- and breadth-controlled exploration of original research questions -- enhancing search diversity and nuance in the retrieval of relevant scientific literature. Unlike conventional retrieval-augmented generation pipelines, DeepResearch enables user-controllable synthesis with transparent reasoning and parameter-driven configurability, facilitating high-throughput integration of domain-specific evidence while maintaining analytical rigor. Applied to 49 ecological research questions, DeepResearch achieves up to a 21-fold increase in source integration and a 14.9-fold rise in sources integrated per 1,000 words. High-parameter settings yield expert-level analytical depth and contextual diversity. Source code available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 45,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2412.11407",
        "abs_url": "https://arxiv.org/abs/2412.11407",
        "pdf_url": "https://arxiv.org/pdf/2412.11407",
        "title": "An Enhanced Classification Method Based on Adaptive Multi-Scale Fusion for Long-tailed Multispectral Point Clouds",
        "authors": [
            "TianZhu Liu",
            "BangYan Hu",
            "YanFeng Gu",
            "Xian Li",
            "Aleksandra Pižurica"
        ],
        "comments": "16 pages, 9 figures, 5 tables",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Image and Video Processing (eess.IV)",
        "abstract": "Multispectral point cloud (MPC) captures 3D spatial-spectral information from the observed scene, which can be used for scene understanding and has a wide range of applications. However, most of the existing classification methods were extensively tested on indoor datasets, and when applied to outdoor datasets they still face problems including sparse labeled targets, differences in land-covers scales, and long-tailed distributions. To address the above issues, an enhanced classification method based on adaptive multi-scale fusion for MPCs with long-tailed distributions is proposed. In the training set generation stage, a grid-balanced sampling strategy is designed to reliably generate training samples from sparse labeled datasets. In the feature learning stage, a multi-scale feature fusion module is proposed to fuse shallow features of land-covers at different scales, addressing the issue of losing fine features due to scale variations in land-covers. In the classification stage, an adaptive hybrid loss module is devised to utilize multi-classification heads with adaptive weights to balance the learning ability of different classes, improving the classification performance of small classes due to various-scales and long-tailed distributions in land-covers. Experimental results on three MPC datasets demonstrate the effectiveness of the proposed method compared with the state-of-the-art methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 46,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.07855",
        "abs_url": "https://arxiv.org/abs/2507.07855",
        "pdf_url": "https://arxiv.org/pdf/2507.07855",
        "title": "Principled Foundations for Preference Optimization",
        "authors": [
            "Wenxuan Zhou",
            "Shujian Zhang",
            "Brice Magdalou",
            "John Lambert",
            "Ehsan Amid",
            "Richard Nock",
            "Andrew Hard"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "In this paper, we show that direct preference optimization (DPO) is a very specific form of a connection between two major theories in the ML context of learning from preferences: loss functions (Savage) and stochastic choice (Doignon-Falmagne and Machina). The connection is established for all of Savage's losses and at this level of generality, (i) it includes support for abstention on the choice theory side, (ii) it includes support for non-convex objectives on the ML side, and (iii) it allows to frame for free some notable extensions of the DPO setting, including margins and corrections for length. Getting to understand how DPO operates from a general principled perspective is crucial because of the huge and diverse application landscape of models, because of the current momentum around DPO, but also -- and importantly -- because many state of the art variations on DPO definitely occupy a small region of the map that we cover. It also helps to understand the pitfalls of departing from this map, and figure out workarounds.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 47,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08052",
        "abs_url": "https://arxiv.org/abs/2507.08052",
        "pdf_url": "https://arxiv.org/pdf/2507.08052",
        "title": "Lightweight Cloud Masking Models for On-Board Inference in Hyperspectral Imaging",
        "authors": [
            "Mazen Ali",
            "António Pereira",
            "Fabio Gentile",
            "Aser Cortines",
            "Sam Mugel",
            "Román Orús",
            "Stelios P. Neophytides",
            "Michalis Mavrovouniotis"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Cloud and cloud shadow masking is a crucial preprocessing step in hyperspectral satellite imaging, enabling the extraction of high-quality, analysis-ready data. This study evaluates various machine learning approaches, including gradient boosting methods such as XGBoost and LightGBM as well as convolutional neural networks (CNNs). All boosting and CNN models achieved accuracies exceeding 93%. Among the investigated models, the CNN with feature reduction emerged as the most efficient, offering a balance of high accuracy, low storage requirements, and rapid inference times on both CPUs and GPUs. Variations of this version, with only up to 597 trainable parameters, demonstrated the best trade-off in terms of deployment feasibility, accuracy, and computational efficiency. These results demonstrate the potential of lightweight artificial intelligence (AI) models for real-time hyperspectral image processing, supporting the development of on-board satellite AI systems for space-based applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 48,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08827",
        "abs_url": "https://arxiv.org/abs/2507.08827",
        "pdf_url": "https://arxiv.org/pdf/2507.08827",
        "title": "Advancing network resilience theories with symbolized reinforcement learning",
        "authors": [
            "Yu Zheng",
            "Jingtao Ding",
            "Depeng Jin",
            "Jianxi Gao",
            "Yong Li"
        ],
        "comments": "",
        "subjects": "Physics and Society (physics.soc-ph); Artificial Intelligence (cs.AI)",
        "abstract": "Many complex networks display remarkable resilience under external perturbations, internal failures and environmental changes, yet they can swiftly deteriorate into dysfunction upon the removal of a few keystone nodes. Discovering theories that measure network resilience offers the potential to prevent catastrophic collapses--from species extinctions to financial crise--with profound implications for real-world systems. Current resilience theories address the problem from a single perspective of topology, neglecting the crucial role of system dynamics, due to the intrinsic complexity of the coupling between topology and dynamics which exceeds the capabilities of human analytical methods. Here, we report an automatic method for resilience theory discovery, which learns from how AI solves a complicated network dismantling problem and symbolizes its network attack strategies into theoretical formulas. This proposed self-inductive approach discovers the first resilience theory that accounts for both topology and dynamics, highlighting how the correlation between node degree and state shapes overall network resilience, and offering insights for designing early warning signals of systematic collapses. Additionally, our approach discovers formulas that refine existing well-established resilience theories with over 37.5% improvement in accuracy, significantly advancing human understanding of complex networks with AI.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 49,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08829",
        "abs_url": "https://arxiv.org/abs/2507.08829",
        "pdf_url": "https://arxiv.org/pdf/2507.08829",
        "title": "Efficient Triple Modular Redundancy for Reliability Enhancement of DNNs Using Explainable AI",
        "authors": [
            "Kimia Soroush",
            "Nastaran Shirazi",
            "Mohsen Raji"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Deep Neural Networks (DNNs) are widely employed in safety-critical domains, where ensuring their reliability is essential. Triple Modular Redundancy (TMR) is an effective technique to enhance the reliability of DNNs in the presence of bit-flip faults. In order to handle the significant overhead of TMR, it is applied selectively on the parameters and components with the highest contribution at the model output. Hence, the accuracy of the selection criterion plays the key role on the efficiency of TMR. This paper presents an efficient TMR approach to enhance the reliability of DNNs against bit-flip faults using an Explainable Artificial Intelligence (XAI) method. Since XAI can provide valuable insights about the importance of individual neurons and weights in the performance of the network, they can be applied as the selection metric in TMR techniques. The proposed method utilizes a low-cost, gradient-based XAI technique known as Layer-wise Relevance Propagation (LRP) to calculate importance scores for DNN parameters. These scores are then used to enhance the reliability of the model, with the most critical weights being protected by TMR. The proposed approach is evaluated on two DNN models, VGG16 and AlexNet, using datasets such as MNIST and CIFAR-10. The results demonstrate that the method can protect the AlexNet model at a bit error rate of 10-4, achieving over 60% reliability improvement while maintaining the same overhead as state-of-the-art methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 50,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08833",
        "abs_url": "https://arxiv.org/abs/2507.08833",
        "pdf_url": "https://arxiv.org/pdf/2507.08833",
        "title": "LoRA Is Slower Than You Think",
        "authors": [
            "Seokmin Ko"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Low-Rank Adaptation (LoRA) is one of the most widely used techniques for fine-tuning large language models (LLMs). By introducing a small number of trainable low-rank weight matrices, LoRA substantially reduces the number of parameters that need to be updated, offering significant advantages in memory consumption and computational efficiency compared to full fine-tuning. However, we observed that LoRA does not consistently provide speed improvements across all model architectures and training setups. Motivated by this inconsistency, we conduct a comprehensive analysis of LoRA's performance and investigate the underlying factors limiting its speedup. Based on our findings, we propose several methods for more efficient fine-tuning of LLMs. We empirically evaluate these methods and compare them to LoRA, demonstrating that our approach achieves comparable or superior performance while delivering more consistent training speed improvements. Our work offers valuable insights and practical guidelines for practitioners seeking to optimize LLM fine-tuning under resource constraints.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 51,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08835",
        "abs_url": "https://arxiv.org/abs/2507.08835",
        "pdf_url": "https://arxiv.org/pdf/2507.08835",
        "title": "Representation learning with a transformer by contrastive learning for money laundering detection",
        "authors": [
            "Harold Guéneau",
            "Alain Celisse",
            "Pascal Delange"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Statistics Theory (math.ST); Risk Management (q-fin.RM); Statistical Finance (q-fin.ST)",
        "abstract": "The present work tackles the money laundering detection problem. A new procedure is introduced which exploits structured time series of both qualitative and quantitative data by means of a transformer neural network. The first step of this procedure aims at learning representations of time series through contrastive learning (without any labels). The second step leverages these representations to generate a money laundering scoring of all observations. A two-thresholds approach is then introduced, which ensures a controlled false-positive rate by means of the Benjamini-Hochberg (BH) procedure. Experiments confirm that the transformer is able to produce general representations that succeed in exploiting money laundering patterns with minimal supervision from domain experts. It also illustrates the higher ability of the new procedure for detecting nonfraudsters as well as fraudsters, while keeping the false positive rate under control. This greatly contrasts with rule-based procedures or the ones based on LSTM architectures.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 52,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08838",
        "abs_url": "https://arxiv.org/abs/2507.08838",
        "pdf_url": "https://arxiv.org/pdf/2507.08838",
        "title": "wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models",
        "authors": [
            "Xiaohang Tang",
            "Rares Dolga",
            "Sangwoong Yoon",
            "Ilija Bogunovic"
        ],
        "comments": "Preprint",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "Improving the reasoning capabilities of diffusion-based large language models (dLLMs) through reinforcement learning (RL) remains an open problem. The intractability of dLLMs likelihood function necessitates approximating the current, old, and reference policy likelihoods at each policy optimization step. This reliance introduces additional computational overhead and lead to potentially large bias -- particularly when approximation errors occur in the denominator of policy ratios used for importance sampling. To mitigate these issues, we introduce $\\mathtt{wd1}$, a novel policy optimization approach that reformulates the objective as a weighted likelihood, requiring only a single approximation for the current parametrized policy likelihood. Experiments on widely used reasoning benchmarks demonstrate that $\\mathtt{wd1}$, without supervised fine-tuning (SFT) or any supervised data, outperforms existing RL methods for dLLMs, achieving up to 16% higher accuracy. $\\mathtt{wd1}$ delivers additional computational gains, including reduced training time and fewer function evaluations (NFEs) per gradient step. These findings, combined with the simplicity of method's implementation and R1-Zero-like training (no SFT), position $\\mathtt{wd1}$ as a more effective and efficient method for applying RL to dLLMs reasoning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 53,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08839",
        "abs_url": "https://arxiv.org/abs/2507.08839",
        "pdf_url": "https://arxiv.org/pdf/2507.08839",
        "title": "Domain-Adaptive Diagnosis of Lewy Body Disease with Transferability Aware Transformer",
        "authors": [
            "Xiaowei Yu",
            "Jing Zhang",
            "Tong Chen",
            "Yan Zhuang",
            "Minheng Chen",
            "Chao Cao",
            "Yanjun Lyu",
            "Lu Zhang",
            "Li Su",
            "Tianming Liu",
            "Dajiang Zhu"
        ],
        "comments": "MICCAI 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Image and Video Processing (eess.IV)",
        "abstract": "Lewy Body Disease (LBD) is a common yet understudied form of dementia that imposes a significant burden on public health. It shares clinical similarities with Alzheimer's disease (AD), as both progress through stages of normal cognition, mild cognitive impairment, and dementia. A major obstacle in LBD diagnosis is data scarcity, which limits the effectiveness of deep learning. In contrast, AD datasets are more abundant, offering potential for knowledge transfer. However, LBD and AD data are typically collected from different sites using different machines and protocols, resulting in a distinct domain shift. To effectively leverage AD data while mitigating domain shift, we propose a Transferability Aware Transformer (TAT) that adapts knowledge from AD to enhance LBD diagnosis. Our method utilizes structural connectivity (SC) derived from structural MRI as training data. Built on the attention mechanism, TAT adaptively assigns greater weights to disease-transferable features while suppressing domain-specific ones, thereby reducing domain shift and improving diagnostic accuracy with limited LBD data. The experimental results demonstrate the effectiveness of TAT. To the best of our knowledge, this is the first study to explore domain adaptation from AD to LBD under conditions of data scarcity and domain shift, providing a promising framework for domain-adaptive diagnosis of rare diseases.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 54,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08841",
        "abs_url": "https://arxiv.org/abs/2507.08841",
        "pdf_url": "https://arxiv.org/pdf/2507.08841",
        "title": "Zero-Shot Neural Architecture Search with Weighted Response Correlation",
        "authors": [
            "Kun Jing",
            "Luoyu Chen",
            "Jungang Xu",
            "Jianwei Tai",
            "Yiyu Wang",
            "Shuaimin Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Neural architecture search (NAS) is a promising approach for automatically designing neural network architectures. However, the architecture estimation of NAS is computationally expensive and time-consuming because of training multiple architectures from scratch. Although existing zero-shot NAS methods use training-free proxies to accelerate the architecture estimation, their effectiveness, stability, and generality are still lacking. We present a novel training-free estimation proxy called weighted response correlation (WRCor). WRCor utilizes correlation coefficient matrices of responses across different input samples to calculate the proxy scores of estimated architectures, which can measure their expressivity and generalizability. Experimental results on proxy evaluation demonstrate that WRCor and its voting proxies are more efficient estimation strategies than existing proxies. We also apply them with different search strategies in architecture search. Experimental results on architecture search show that our zero-shot NAS algorithm outperforms most existing NAS algorithms in different search spaces. Our NAS algorithm can discover an architecture with a 22.1% test error on the ImageNet-1k dataset within 4 GPU hours. All codes are publicly available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 55,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08842",
        "abs_url": "https://arxiv.org/abs/2507.08842",
        "pdf_url": "https://arxiv.org/pdf/2507.08842",
        "title": "Gradients as an Action: Towards Communication-Efficient Federated Recommender Systems via Adaptive Action Sharing",
        "authors": [
            "Zhufeng Lu",
            "Chentao Jia",
            "Ming Hu",
            "Xiaofei Xie",
            "Mingsong Chen"
        ],
        "comments": "This paper has been accepted by ACM SIGKDD 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "As a promising privacy-aware collaborative model training paradigm, Federated Learning (FL) is becoming popular in the design of distributed recommender systems. However, Federated Recommender Systems (FedRecs) greatly suffer from two major problems: i) extremely high communication overhead due to massive item embeddings involved in recommendation systems, and ii) intolerably low training efficiency caused by the entanglement of both heterogeneous network environments and client devices. Although existing methods attempt to employ various compression techniques to reduce communication overhead, due to the parameter errors introduced by model compression, they inevitably suffer from model performance degradation. To simultaneously address the above problems, this paper presents a communication-efficient FedRec framework named FedRAS, which adopts an action-sharing strategy to cluster the gradients of item embedding into a specific number of model updating actions for communication rather than directly compressing the item embeddings. In this way, the cloud server can use the limited actions from clients to update all the items. Since gradient values are significantly smaller than item embeddings, constraining the directions of gradients (i.e., the action space) introduces smaller errors compared to compressing the entire item embedding matrix into a reduced space. To accommodate heterogeneous devices and network environments, FedRAS incorporates an adaptive clustering mechanism that dynamically adjusts the number of actions. Comprehensive experiments on well-known datasets demonstrate that FedRAS can reduce the size of communication payloads by up to 96.88%, while not sacrificing recommendation performance within various heterogeneous scenarios. We have open-sourced FedRAS at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 56,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08843",
        "abs_url": "https://arxiv.org/abs/2507.08843",
        "pdf_url": "https://arxiv.org/pdf/2507.08843",
        "title": "Can We Predict Your Next Move Without Breaking Your Privacy?",
        "authors": [
            "Arpita Soni",
            "Sahil Tripathi",
            "Gautam Siddharth Kashyap",
            "Manaswi Kulahara",
            "Mohammad Anas Azeez",
            "Zohaib Hasan Siddiqui",
            "Nipun Joshi",
            "Jiechao Gao"
        ],
        "comments": "Accepted in the 17th International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2025), scheduled for 25 - 28 August 2025 in Ontario, Canada",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "We propose FLLL3M--Federated Learning with Large Language Models for Mobility Modeling--a privacy-preserving framework for Next-Location Prediction (NxLP). By retaining user data locally and leveraging LLMs through an efficient outer product mechanism, FLLL3M ensures high accuracy with low resource demands. It achieves SOT results on Gowalla (Acc@1: 12.55, MRR: 0.1422), WeePlace (10.71, 0.1285), Brightkite (10.42, 0.1169), and FourSquare (8.71, 0.1023), while reducing parameters by up to 45.6% and memory usage by 52.7%.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 57,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08845",
        "abs_url": "https://arxiv.org/abs/2507.08845",
        "pdf_url": "https://arxiv.org/pdf/2507.08845",
        "title": "DAFOS: Dynamic Adaptive Fanout Optimization Sampler",
        "authors": [
            "Irfan Ullah",
            "Young-Koo Lee"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Graph Neural Networks (GNNs) are becoming an essential tool for learning from graph-structured data, however uniform neighbor sampling and static fanout settings frequently limit GNNs' scalability and efficiency. In this paper, we propose the Dynamic Adaptive Fanout Optimization Sampler (DAFOS), a novel approach that dynamically adjusts the fanout based on model performance and prioritizes important nodes during training. Our approach leverages node scoring based on node degree to focus computational resources on structurally important nodes, incrementing the fanout as the model training progresses. DAFOS also integrates an early stopping mechanism to halt training when performance gains diminish. Experiments conducted on three benchmark datasets, ogbnarxiv, Reddit, and ogbn-products, demonstrate that our approach significantly improves training speed and accuracy compared to a state-of-the-art approach. DAFOS achieves a 3.57x speedup on the ogbn-arxiv dataset and a 12.6x speedup on the Reddit dataset while improving the F1 score from 68.5% to 71.21% on ogbn-arxiv and from 73.78% to 76.88% on the ogbn-products dataset, respectively. These results highlight the potential of DAFOS as an efficient and scalable solution for large-scale GNN training.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 58,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08848",
        "abs_url": "https://arxiv.org/abs/2507.08848",
        "pdf_url": "https://arxiv.org/pdf/2507.08848",
        "title": "Assuring the Safety of Reinforcement Learning Components: AMLAS-RL",
        "authors": [
            "Calum Corrie Imrie",
            "Ioannis Stefanakos",
            "Sepeedeh Shahbeigi",
            "Richard Hawkins",
            "Simon Burton"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Robotics (cs.RO); Software Engineering (cs.SE)",
        "abstract": "The rapid advancement of machine learning (ML) has led to its increasing integration into cyber-physical systems (CPS) across diverse domains. While CPS offer powerful capabilities, incorporating ML components introduces significant safety and assurance challenges. Among ML techniques, reinforcement learning (RL) is particularly suited for CPS due to its capacity to handle complex, dynamic environments where explicit models of interaction between system and environment are unavailable or difficult to construct. However, in safety-critical applications, this learning process must not only be effective but demonstrably safe. Safe-RL methods aim to address this by incorporating safety constraints during learning, yet they fall short in providing systematic assurance across the RL lifecycle. The AMLAS methodology offers structured guidance for assuring the safety of supervised learning components, but it does not directly apply to the unique challenges posed by RL. In this paper, we adapt AMLAS to provide a framework for generating assurance arguments for an RL-enabled system through an iterative process; AMLAS-RL. We demonstrate AMLAS-RL using a running example of a wheeled vehicle tasked with reaching a target goal without collision.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 59,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08853",
        "abs_url": "https://arxiv.org/abs/2507.08853",
        "pdf_url": "https://arxiv.org/pdf/2507.08853",
        "title": "Clio-X: AWeb3 Solution for Privacy-Preserving AI Access to Digital Archives",
        "authors": [
            "Victoria L. Lemieux",
            "Rosa Gil",
            "Faith Molosiwa",
            "Qihong Zhou",
            "Binming Li",
            "Roberto Garcia",
            "Luis De La Torre Cubillo",
            "Zehua Wang"
        ],
        "comments": "28 pages, 8 figures",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Digital Libraries (cs.DL)",
        "abstract": "As archives turn to artificial intelligence to manage growing volumes of digital records, privacy risks inherent in current AI data practices raise critical concerns about data sovereignty and ethical accountability. This paper explores how privacy-enhancing technologies (PETs) and Web3 architectures can support archives to preserve control over sensitive content while still being able to make it available for access by researchers. We present Clio-X, a decentralized, privacy-first Web3 digital solution designed to embed PETs into archival workflows and support AI-enabled reference and access. Drawing on a user evaluation of a medium-fidelity prototype, the study reveals both interest in the potential of the solution and significant barriers to adoption related to trust, system opacity, economic concerns, and governance. Using Rogers' Diffusion of Innovation theory, we analyze the sociotechnical dimensions of these barriers and propose a path forward centered on participatory design and decentralized governance through a Clio-X Decentralized Autonomous Organization. By integrating technical safeguards with community-based oversight, Clio-X offers a novel model to ethically deploy AI in cultural heritage contexts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 60,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08858",
        "abs_url": "https://arxiv.org/abs/2507.08858",
        "pdf_url": "https://arxiv.org/pdf/2507.08858",
        "title": "Foundation models for time series forecasting: Application in conformal prediction",
        "authors": [
            "Sami Achour",
            "Yassine Bouher",
            "Duong Nguyen",
            "Nicolas Chesneau"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "The zero-shot capabilities of foundation models (FMs) for time series forecasting offer promising potentials in conformal prediction, as most of the available data can be allocated to calibration. This study compares the performance of Time Series Foundation Models (TSFMs) with traditional methods, including statistical models and gradient boosting, within a conformal prediction setting. Our findings highlight two key advantages of TSFMs. First, when the volume of data is limited, TSFMs provide more reliable conformalized prediction intervals than classic models, thanks to their superior predictive accuracy. Second, the calibration process is more stable because more data are used for calibration. Morever, the fewer data available, the more pronounced these benefits become, as classic models require a substantial amount of data for effective training. These results underscore the potential of foundation models in improving conformal prediction reliability in time series applications, particularly in data-constrained cases. All the code to reproduce the experiments is available.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 61,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08864",
        "abs_url": "https://arxiv.org/abs/2507.08864",
        "pdf_url": "https://arxiv.org/pdf/2507.08864",
        "title": "Privacy-Utility-Fairness: A Balanced Approach to Vehicular-Traffic Management System",
        "authors": [
            "Poushali Sengupta",
            "Sabita Maharjan",
            "frank Eliassen",
            "Yan Zhang"
        ],
        "comments": "accepted in VTC 2025 Spring, Oslo, Norway",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)",
        "abstract": "Location-based vehicular traffic management faces significant challenges in protecting sensitive geographical data while maintaining utility for traffic management and fairness across regions. Existing state-of-the-art solutions often fail to meet the required level of protection against linkage attacks and demographic biases, leading to privacy leakage and inequity in data analysis. In this paper, we propose a novel algorithm designed to address the challenges regarding the balance of privacy, utility, and fairness in location-based vehicular traffic management systems. In this context, utility means providing reliable and meaningful traffic information, while fairness ensures that all regions and individuals are treated equitably in data use and decision-making. Employing differential privacy techniques, we enhance data security by integrating query-based data access with iterative shuffling and calibrated noise injection, ensuring that sensitive geographical data remains protected. We ensure adherence to epsilon-differential privacy standards by implementing the Laplace mechanism. We implemented our algorithm on vehicular location-based data from Norway, demonstrating its ability to maintain data utility for traffic management and urban planning while ensuring fair representation of all geographical areas without being overrepresented or underrepresented. Additionally, we have created a heatmap of Norway based on our model, illustrating the privatized and fair representation of the traffic conditions across various cities. Our algorithm provides privacy in vehicular traffic",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 62,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08871",
        "abs_url": "https://arxiv.org/abs/2507.08871",
        "pdf_url": "https://arxiv.org/pdf/2507.08871",
        "title": "Next-Generation Travel Demand Modeling with a Generative Framework for Household Activity Coordination",
        "authors": [
            "Xishun Liao",
            "Haoxuan Ma",
            "Yifan Liu",
            "Yuxiang Wei",
            "Brian Yueshuai He",
            "Chris Stanford",
            "Jiaqi Ma"
        ],
        "comments": "8 pages, 7 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Travel demand models are critical tools for planning, policy, and mobility system design. Traditional activity-based models (ABMs), although grounded in behavioral theories, often rely on simplified rules and assumptions, and are costly to develop and difficult to adapt across different regions. This paper presents a learning-based travel demand modeling framework that synthesizes household-coordinated daily activity patterns based on a household's socio-demographic profiles. The whole framework integrates population synthesis, coordinated activity generation, location assignment, and large-scale microscopic traffic simulation into a unified system. It is fully generative, data-driven, scalable, and transferable to other regions. A full-pipeline implementation is conducted in Los Angeles with a 10 million population. Comprehensive validation shows that the model closely replicates real-world mobility patterns and matches the performance of legacy ABMs with significantly reduced modeling cost and greater scalability. With respect to the SCAG ABM benchmark, the origin-destination matrix achieves a cosine similarity of 0.97, and the daily vehicle miles traveled (VMT) in the network yields a 0.006 Jensen-Shannon Divergence (JSD) and a 9.8% mean absolute percentage error (MAPE). When compared to real-world observations from Caltrans PeMS, the evaluation on corridor-level traffic speed and volume reaches a 0.001 JSD and a 6.11% MAPE.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 63,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08873",
        "abs_url": "https://arxiv.org/abs/2507.08873",
        "pdf_url": "https://arxiv.org/pdf/2507.08873",
        "title": "Contrastive Language-Image Pre-Training Model based Semantic Communication Performance Optimization",
        "authors": [
            "Shaoran Yang",
            "Dongyu Wei",
            "Hanzhi Yu",
            "Zhaohui Yang",
            "Yuchen Liu",
            "Mingzhe Chen"
        ],
        "comments": "Submitted to IEEE GLOBECOM 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "In this paper, a novel contrastive language-image pre-training (CLIP) model based semantic communication framework is designed. Compared to standard neural network (e.g.,convolutional neural network) based semantic encoders and decoders that require joint training over a common dataset, our CLIP model based method does not require any training procedures thus enabling a transmitter to extract data meanings of the original data without neural network model training, and the receiver to train a neural network for follow-up task implementation without the communications with the transmitter. Next, we investigate the deployment of the CLIP model based semantic framework over a noisy wireless network. Since the semantic information generated by the CLIP model is susceptible to wireless noise and the spectrum used for semantic information transmission is limited, it is necessary to jointly optimize CLIP model architecture and spectrum resource block (RB) allocation to maximize semantic communication performance while considering wireless noise, the delay and energy used for semantic communication. To achieve this goal, we use a proximal policy optimization (PPO) based reinforcement learning (RL) algorithm to learn how wireless noise affect the semantic communication performance thus finding optimal CLIP model and RB for each user. Simulation results show that our proposed method improves the convergence rate by up to 40%, and the accumulated reward by 4x compared to soft actor-critic.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 64,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08877",
        "abs_url": "https://arxiv.org/abs/2507.08877",
        "pdf_url": "https://arxiv.org/pdf/2507.08877",
        "title": "ODIA: Oriented Distillation for Inline Acceleration of LLM-based Function Calling",
        "authors": [
            "Hanlong Zhang",
            "Jingsheng Yang",
            "Hao Li",
            "Yuhao He",
            "Franck Gong"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Function Calling is a crucial technique that enables Large Language Models (LLMs) to interact with external systems through APIs. However, the high latency associated with LLM-based Function Calling significantly impacts user experience. This paper presents a novel approach called Oriented Distillation for Inline Acceleration (ODIA) that leverages online user interaction data to accelerate Function Calling. By automatically identifying \"simple queries\" from production traffic and distilling knowledge from larger models to smaller ones, our method reduces response latency by 45% (expected) and 78% (median) while maintaining accuracy. We demonstrate the effectiveness of our approach through real-world deployment in a music application, where the smaller model successfully handles 60% of traffic with negligible accuracy loss. Our method requires minimal human intervention and continuously improves through automated data collection and model updating, making it a practical solution for production environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 65,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08878",
        "abs_url": "https://arxiv.org/abs/2507.08878",
        "pdf_url": "https://arxiv.org/pdf/2507.08878",
        "title": "Towards Privacy-Preserving and Personalized Smart Homes via Tailored Small Language Models",
        "authors": [
            "Xinyu Huang",
            "Leming Shen",
            "Zijing Ma",
            "Yuanqing Zheng"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
        "abstract": "Large Language Models (LLMs) have showcased remarkable generalizability in language comprehension and hold significant potential to revolutionize human-computer interaction in smart homes. Existing LLM-based smart home assistants typically transmit user commands, along with user profiles and home configurations, to remote servers to obtain personalized services. However, users are increasingly concerned about the potential privacy leaks to the remote servers. To address this issue, we develop HomeLLaMA, an on-device assistant for privacy-preserving and personalized smart home serving with a tailored small language model (SLM). HomeLLaMA learns from cloud LLMs to deliver satisfactory responses and enable user-friendly interactions. Once deployed, HomeLLaMA facilitates proactive interactions by continuously updating local SLMs and user profiles. To further enhance user experience while protecting their privacy, we develop PrivShield to offer an optional privacy-preserving LLM-based smart home serving for those users, who are unsatisfied with local responses and willing to send less-sensitive queries to remote servers. For evaluation, we build a comprehensive benchmark DevFinder to assess the service quality. Extensive experiments and user studies (M=100) demonstrate that HomeLLaMA can provide personalized services while significantly enhancing user privacy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 66,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08879",
        "abs_url": "https://arxiv.org/abs/2507.08879",
        "pdf_url": "https://arxiv.org/pdf/2507.08879",
        "title": "A Multi-Level Strategy for Deepfake Content Moderation under EU Regulation",
        "authors": [
            "Max-Paul Förster",
            "Luca Deck",
            "Raimund Weidlich",
            "Niklas Kühl"
        ],
        "comments": "",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI)",
        "abstract": "The growing availability and use of deepfake technologies increases risks for democratic societies, e.g., for political communication on online platforms. The EU has responded with transparency obligations for providers and deployers of Artificial Intelligence (AI) systems and online platforms. This includes marking deepfakes during generation and labeling deepfakes when they are shared. However, the lack of industry and enforcement standards poses an ongoing challenge. Through a multivocal literature review, we summarize methods for marking, detecting, and labeling deepfakes and assess their effectiveness under EU regulation. Our results indicate that individual methods fail to meet regulatory and practical requirements. Therefore, we propose a multi-level strategy combining the strengths of existing methods. To account for the masses of content on online platforms, our multi-level strategy provides scalability and practicality via a simple scoring mechanism. At the same time, it is agnostic to types of deepfake technology and allows for context-specific risk weighting.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 67,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08881",
        "abs_url": "https://arxiv.org/abs/2507.08881",
        "pdf_url": "https://arxiv.org/pdf/2507.08881",
        "title": "The Consistency-Acceptability Divergence of LLMs in Judicial Decision-Making: Task and Stakeholder Dimensions",
        "authors": [
            "Zhang MingDa",
            "Xu Qing"
        ],
        "comments": "12 pages,2 figures",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI); Social and Information Networks (cs.SI)",
        "abstract": "The integration of large language model (LLM) technology into judicial systems is fundamentally transforming legal practice worldwide. However, this global transformation has revealed an urgent paradox requiring immediate attention. This study introduces the concept of ``consistency-acceptability divergence'' for the first time, referring to the gap between technical consistency and social acceptance. While LLMs achieve high consistency at the technical level, this consistency demonstrates both positive and negative effects. Through comprehensive analysis of recent data on LLM judicial applications from 2023--2025, this study finds that addressing this challenge requires understanding both task and stakeholder dimensions. This study proposes the Dual-Track Deliberative Multi-Role LLM Judicial Governance Framework (DTDMR-LJGF), which enables intelligent task classification and meaningful interaction among diverse stakeholders. This framework offers both theoretical insights and practical guidance for building an LLM judicial ecosystem that balances technical efficiency with social legitimacy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 68,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08885",
        "abs_url": "https://arxiv.org/abs/2507.08885",
        "pdf_url": "https://arxiv.org/pdf/2507.08885",
        "title": "AirScape: An Aerial Generative World Model with Motion Controllability",
        "authors": [
            "Baining Zhao",
            "Rongze Tang",
            "Mingyuan Jia",
            "Ziyou Wang",
            "Fanghang Man",
            "Xin Zhang",
            "Yu Shang",
            "Weichen Zhang",
            "Chen Gao",
            "Wei Wu",
            "Xin Wang",
            "Xinlei Chen",
            "Yong Li"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
        "abstract": "How to enable robots to predict the outcomes of their own motion intentions in three-dimensional space has been a fundamental problem in embodied intelligence. To explore more general spatial imagination capabilities, here we present AirScape, the first world model designed for six-degree-of-freedom aerial agents. AirScape predicts future observation sequences based on current visual inputs and motion intentions. Specifically, we construct an dataset for aerial world model training and testing, which consists of 11k video-intention pairs. This dataset includes first-person-view videos capturing diverse drone actions across a wide range of scenarios, with over 1,000 hours spent annotating the corresponding motion intentions. Then we develop a two-phase training schedule to train a foundation model -- initially devoid of embodied spatial knowledge -- into a world model that is controllable by motion intentions and adheres to physical spatio-temporal constraints.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 69,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08890",
        "abs_url": "https://arxiv.org/abs/2507.08890",
        "pdf_url": "https://arxiv.org/pdf/2507.08890",
        "title": "Overview of the TREC 2023 deep learning track",
        "authors": [
            "Nick Craswell",
            "Bhaskar Mitra",
            "Emine Yilmaz",
            "Hossein A. Rahmani",
            "Daniel Campos",
            "Jimmy Lin",
            "Ellen M. Voorhees",
            "Ian Soboroff"
        ],
        "comments": "arXiv admin note: substantial text overlap with arXiv:2507.08191",
        "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "This is the fifth year of the TREC Deep Learning track. As in previous years, we leverage the MS MARCO datasets that made hundreds of thousands of human-annotated training labels available for both passage and document ranking tasks. We mostly repeated last year's design, to get another matching test set, based on the larger, cleaner, less-biased v2 passage and document set, with passage ranking as primary and document ranking as a secondary task (using labels inferred from passage). As we did last year, we sample from MS MARCO queries that were completely held out, unused in corpus construction, unlike the test queries in the first three years. This approach yields a more difficult test with more headroom for improvement. Alongside the usual MS MARCO (human) queries from MS MARCO, this year we generated synthetic queries using a fine-tuned T5 model and using a GPT-4 prompt. The new headline result this year is that runs using Large Language Model (LLM) prompting in some way outperformed runs that use the \"nnlm\" approach, which was the best approach in the previous four years. Since this is the last year of the track, future iterations of prompt-based ranking can happen in other tracks. Human relevance assessments were applied to all query types, not just human MS MARCO queries. Evaluation using synthetic queries gave similar results to human queries, with system ordering agreement of $\\tau=0.8487$. However, human effort was needed to select a subset of the synthetic queries that were usable. We did not see clear evidence of bias, where runs using GPT-4 were favored when evaluated using synthetic GPT-4 queries, or where runs using T5 were favored when evaluated on synthetic T5 queries.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 70,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08898",
        "abs_url": "https://arxiv.org/abs/2507.08898",
        "pdf_url": "https://arxiv.org/pdf/2507.08898",
        "title": "SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems",
        "authors": [
            "Wenliang Shan",
            "Michael Fu",
            "Rui Yang",
            "Chakkrit Tantithamthavorn"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Safety alignment is critical for LLM-powered systems. While recent LLM-powered guardrail approaches such as LlamaGuard achieve high detection accuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''), they struggle with multilingual unsafe inputs. This limitation leaves LLM systems vulnerable to unsafe and jailbreak prompts written in low-resource languages such as those in Southeast Asia. This paper introduces SEALGuard, a multilingual guardrail designed to improve the safety alignment across diverse languages. It aims to address the multilingual safety alignment gap of existing guardrails and ensure effective filtering of unsafe and jailbreak prompts in LLM-powered systems. We adapt a general-purpose multilingual language model into a multilingual guardrail using low-rank adaptation (LoRA). We construct SEALSBench, a large-scale multilingual safety alignment dataset containing over 260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases. We evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on this benchmark. Our findings show that multilingual unsafe and jailbreak prompts substantially degrade the performance of the state-of-the-art LlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and 18%, respectively, compared to its performance on English-only prompts. In contrast, SEALGuard outperforms existing guardrails in detecting multilingual unsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and achieving the best DSR, precision, and F1-score. Our ablation study further reveals the contributions of adaptation strategies and model size to the overall performance of SEALGuard. We release our pre-trained model and benchmark at this https URL to support further research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 71,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08902",
        "abs_url": "https://arxiv.org/abs/2507.08902",
        "pdf_url": "https://arxiv.org/pdf/2507.08902",
        "title": "Generation of structure-guided pMHC-I libraries using Diffusion Models",
        "authors": [
            "Sergio Mares",
            "Ariel Espinoza Weinberger",
            "Nilah M. Ioannidis"
        ],
        "comments": "Accepted to the The 2nd Workshop on Generative AI and Biology ICML Workshop 2025",
        "subjects": "Quantitative Methods (q-bio.QM); Artificial Intelligence (cs.AI)",
        "abstract": "Personalized vaccines and T-cell immunotherapies depend critically on identifying peptide-MHC class I (pMHC-I) interactions capable of eliciting potent immune responses. However, current benchmarks and models inherit biases present in mass-spectrometry and binding-assay datasets, limiting discovery of novel peptide ligands. To address this issue, we introduce a structure-guided benchmark of pMHC-I peptides designed using diffusion models conditioned on crystal structure interaction distances. Spanning twenty high-priority HLA alleles, this benchmark is independent of previously characterized peptides yet reproduces canonical anchor residue preferences, indicating structural generalization without experimental dataset bias. Using this resource, we demonstrate that state-of-the-art sequence-based predictors perform poorly at recognizing the binding potential of these structurally stable designs, indicating allele-specific limitations invisible in conventional evaluations. Our geometry-aware design pipeline yields peptides with high predicted structural integrity and higher residue diversity than existing datasets, representing a key resource for unbiased model training and evaluation. Our code, and data are available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 72,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08905",
        "abs_url": "https://arxiv.org/abs/2507.08905",
        "pdf_url": "https://arxiv.org/pdf/2507.08905",
        "title": "Last Layer Hamiltonian Monte Carlo",
        "authors": [
            "Koen Vellenga",
            "H. Joe Steinhauer",
            "Göran Falkman",
            "Jonas Andersson",
            "Anders Sjögren"
        ],
        "comments": "25 pages, 15 figures, 6 tables, currently under submission",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "We explore the use of Hamiltonian Monte Carlo (HMC) sampling as a probabilistic last layer approach for deep neural networks (DNNs). While HMC is widely regarded as a gold standard for uncertainty estimation, the computational demands limit its application to large-scale datasets and large DNN architectures. Although the predictions from the sampled DNN parameters can be parallelized, the computational cost still scales linearly with the number of samples (similar to an ensemble). Last layer HMC (LL--HMC) reduces the required computations by restricting the HMC sampling to the final layer of a DNN, making it applicable to more data-intensive scenarios with limited computational resources. In this paper, we compare LL-HMC against five last layer probabilistic deep learning (LL-PDL) methods across three real-world video datasets for driver action and intention. We evaluate the in-distribution classification performance, calibration, and out-of-distribution (OOD) detection. Due to the stochastic nature of the probabilistic evaluations, we performed five grid searches for different random seeds to avoid being reliant on a single initialization for the hyperparameter configurations. The results show that LL--HMC achieves competitive in-distribution classification and OOD detection performance. Additional sampled last layer parameters do not improve the classification performance, but can improve the OOD detection. Multiple chains or starting positions did not yield consistent improvements.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 73,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08912",
        "abs_url": "https://arxiv.org/abs/2507.08912",
        "pdf_url": "https://arxiv.org/pdf/2507.08912",
        "title": "Fair-FLIP: Fair Deepfake Detection with Fairness-Oriented Final Layer Input Prioritising",
        "authors": [
            "Tomasz Szandala",
            "Fatima Ezzeddine",
            "Natalia Rusin",
            "Silvia Giordano",
            "Omran Ayoub"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Artificial Intelligence-generated content has become increasingly popular, yet its malicious use, particularly the deepfakes, poses a serious threat to public trust and discourse. While deepfake detection methods achieve high predictive performance, they often exhibit biases across demographic attributes such as ethnicity and gender. In this work, we tackle the challenge of fair deepfake detection, aiming to mitigate these biases while maintaining robust detection capabilities. To this end, we propose a novel post-processing approach, referred to as Fairness-Oriented Final Layer Input Prioritising (Fair-FLIP), that reweights a trained model's final-layer inputs to reduce subgroup disparities, prioritising those with low variability while demoting highly variable ones. Experimental results comparing Fair-FLIP to both the baseline (without fairness-oriented de-biasing) and state-of-the-art approaches show that Fair-FLIP can enhance fairness metrics by up to 30% while maintaining baseline accuracy, with only a negligible reduction of 0.25%. Code is available on Github: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 74,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08920",
        "abs_url": "https://arxiv.org/abs/2507.08920",
        "pdf_url": "https://arxiv.org/pdf/2507.08920",
        "title": "AMix-1: A Pathway to Test-Time Scalable Protein Foundation Model",
        "authors": [
            "Changze Lv",
            "Jiang Zhou",
            "Siyu Long",
            "Lihao Wang",
            "Jiangtao Feng",
            "Dongyu Xue",
            "Yu Pei",
            "Hao Wang",
            "Zherui Zhang",
            "Yuchen Cai",
            "Zhiqiang Gao",
            "Ziyuan Ma",
            "Jiakai Hu",
            "Chaochen Gao",
            "Jingjing Gong",
            "Yuxuan Song",
            "Shuyi Zhang",
            "Xiaoqing Zheng",
            "Deyi Xiong",
            "Lei Bai",
            "Ya-Qin Zhang",
            "Wei-Ying Ma",
            "Bowen Zhou",
            "Hao Zhou"
        ],
        "comments": "",
        "subjects": "Biomolecules (q-bio.BM); Artificial Intelligence (cs.AI)",
        "abstract": "We introduce AMix-1, a powerful protein foundation model built on Bayesian Flow Networks and empowered by a systematic training methodology, encompassing pretraining scaling laws, emergent capability analysis, in-context learning mechanism, and test-time scaling algorithm. To guarantee robust scalability, we establish a predictive scaling law and reveal the progressive emergence of structural understanding via loss perspective, culminating in a strong 1.7-billion model. Building on this foundation, we devise a multiple sequence alignment (MSA)-based in-context learning strategy to unify protein design into a general framework, where AMix-1 recognizes deep evolutionary signals among MSAs and consistently generates structurally and functionally coherent proteins. This framework enables the successful design of a dramatically improved AmeR variant with an up to $50\\times$ activity increase over its wild type. Pushing the boundaries of protein engineering, we further empower AMix-1 with an evolutionary test-time scaling algorithm for in silico directed evolution that delivers substantial, scalable performance gains as verification budgets are intensified, laying the groundwork for next-generation lab-in-the-loop protein design.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 75,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08924",
        "abs_url": "https://arxiv.org/abs/2507.08924",
        "pdf_url": "https://arxiv.org/pdf/2507.08924",
        "title": "From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation",
        "authors": [
            "Seokhee Hong",
            "Sunkyoung Kim",
            "Guijin Son",
            "Soyeon Kim",
            "Yeonjung Hong",
            "Jinsik Lee"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "The development of Large Language Models (LLMs) requires robust benchmarks that encompass not only academic domains but also industrial fields to effectively evaluate their applicability in real-world scenarios. In this paper, we introduce two Korean expert-level benchmarks. KMMLU-Redux, reconstructed from the existing KMMLU, consists of questions from the Korean National Technical Qualification exams, with critical errors removed to enhance reliability. KMMLU-Pro is based on Korean National Professional Licensure exams to reflect professional knowledge in Korea. Our experiments demonstrate that these benchmarks comprehensively represent industrial knowledge in Korea. We release our dataset publicly available.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 76,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08944",
        "abs_url": "https://arxiv.org/abs/2507.08944",
        "pdf_url": "https://arxiv.org/pdf/2507.08944",
        "title": "Optimizing Sequential Multi-Step Tasks with Parallel LLM Agents",
        "authors": [
            "Enhao Zhang",
            "Erkang Zhu",
            "Gagan Bansal",
            "Adam Fourney",
            "Hussein Mozannar",
            "Jack Gerrits"
        ],
        "comments": "ICML 2025 Workshop on MAS",
        "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI)",
        "abstract": "Large language model (LLM)-based multi-agent systems have demonstrated remarkable promise for tackling complex tasks by breaking them down into subtasks that are iteratively planned, executed, observed, and refined. Despite their effectiveness, these systems often incur high latency because real-world problems frequently demand multiple iterative cycles of reasoning steps. To address this challenge, we propose M1-Parallel, a framework that concurrently runs multiple multi-agent teams in parallel to uncover distinct solution paths. By leveraging an event-driven communication model with asynchronous messaging, M1-Parallel efficiently capitalizes on the inherent diversity of valid plans to either reduce end-to-end latency or boost task completion rates. Our experiments on complex tasks show that M1-Parallel with early termination achieves up to $2.2\\times$ speedup while preserving accuracy, and that M1-Parallel with aggregation yields higher task completion rates. We further investigate strategies aimed at encouraging diverse execution plans but observe no additional performance gains over repeated sampling. Overall, these findings underscore the potential of parallel plan execution for optimizing multi-agent systems for real-world, high-complexity reasoning tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 77,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08945",
        "abs_url": "https://arxiv.org/abs/2507.08945",
        "pdf_url": "https://arxiv.org/pdf/2507.08945",
        "title": "GraphRunner: A Multi-Stage Framework for Efficient and Accurate Graph-Based Retrieval",
        "authors": [
            "Savini Kashmira",
            "Jayanaka L. Dantanarayana",
            "Krisztián Flautner",
            "Lingjia Tang",
            "Jason Mars"
        ],
        "comments": "",
        "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI)",
        "abstract": "Conventional Retrieval Augmented Generation (RAG) approaches are common in text-based applications. However, they struggle with structured, interconnected datasets like knowledge graphs, where understanding underlying relationships is crucial for accurate retrieval. A common direction in graph-based retrieval employs iterative, rule-based traversal guided by Large Language Models (LLMs). Such existing iterative methods typically combine reasoning with single hop traversal at each step, making them vulnerable to LLM reasoning errors and hallucinations that ultimately hinder the retrieval of relevant information. To address these limitations, we propose GraphRunner, a novel graph-based retrieval framework that operates in three distinct stages: planning, verification, and execution. This introduces high-level traversal actions that enable multi-hop exploration in a single step. It also generates a holistic traversal plan, which is verified against the graph structure and pre-defined traversal actions, reducing reasoning errors and detecting hallucinations before execution. GraphRunner significantly reduces LLM reasoning errors and detects hallucinations through validation. Our evaluation using the GRBench dataset shows that GraphRunner consistently outperforms existing approaches, achieving 10-50% performance improvements over the strongest baseline while reducing inference cost by 3.0-12.9x and response generation time by 2.5-7.1x, making it significantly more robust and efficient for graph-based retrieval tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 78,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08958",
        "abs_url": "https://arxiv.org/abs/2507.08958",
        "pdf_url": "https://arxiv.org/pdf/2507.08958",
        "title": "Bridging Literature and the Universe Via A Multi-Agent Large Language Model System",
        "authors": [
            "Xiaowen Zhang",
            "Zhenyu Bi",
            "Patrick Lachance",
            "Xuan Wang",
            "Tiziana Di Matteo",
            "Rupert A.C. Croft"
        ],
        "comments": "6 pages, 4 figures",
        "subjects": "Instrumentation and Methods for Astrophysics (astro-ph.IM); Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)",
        "abstract": "As cosmological simulations and their associated software become increasingly complex, physicists face the challenge of searching through vast amounts of literature and user manuals to extract simulation parameters from dense academic papers, each using different models and formats. Translating these parameters into executable scripts remains a time-consuming and error-prone process. To improve efficiency in physics research and accelerate the cosmological simulation process, we introduce SimAgents, a multi-agent system designed to automate both parameter configuration from the literature and preliminary analysis for cosmology research. SimAgents is powered by specialized LLM agents capable of physics reasoning, simulation software validation, and tool execution. These agents collaborate through structured communication, ensuring that extracted parameters are physically meaningful, internally consistent, and software-compliant. We also construct a cosmological parameter extraction evaluation dataset by collecting over 40 simulations in published papers from Arxiv and leading journals that cover diverse simulation types. Experiments on the dataset demonstrate a strong performance of SimAgents, highlighting its effectiveness and potential to accelerate scientific research for physicists. Our demonstration video is available at: this https URL. The complete system and dataset are publicly available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 79,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08960",
        "abs_url": "https://arxiv.org/abs/2507.08960",
        "pdf_url": "https://arxiv.org/pdf/2507.08960",
        "title": "How to Train a Leader: Hierarchical Reasoning in Multi-Agent LLMs",
        "authors": [
            "Andrew Estornell",
            "Jean-Francois Ton",
            "Muhammad Faaiz Taufiq",
            "Hang Li"
        ],
        "comments": "",
        "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Large Language Models (LLMs) have achieved strong performance on a wide range of complex reasoning tasks, yet further gains are often possible by leveraging the complementary strengths of multiple models. While multi-agent frameworks can improve solution quality by leveraging multiple LLMs, existing methods are often computationally expensive, both at training and inference time. In this work, we introduce a hierarchical multi-agent framework that addresses these challenges by training only a single leader LLM to coordinate a team of untrained peer agents. To this end, we propose Multi-agent guided Leader Policy \\textbf{O}ptimization (MLPO), a novel approach which trains the leader to evaluate and synthesize agent responses without auxiliary value networks or explicit agent feedback. Leaders trained with MLPO exhibit improved performance not only when interacting with the agent team at inference time, but also enjoy improved performance when deployed in single-agent settings without the team. Empirical results on Big-Bench Hard (BBH), MATH, and MMLU demonstrate that our framework achieves substantial performance improvements over both single-agent and multi-agent baselines. Our results highlight the effectiveness and efficiency of training a single, flexible leader for collaborative reasoning in multi-agent LLM systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 80,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08965",
        "abs_url": "https://arxiv.org/abs/2507.08965",
        "pdf_url": "https://arxiv.org/pdf/2507.08965",
        "title": "Theory-Informed Improvements to Classifier-Free Guidance for Discrete Diffusion Models",
        "authors": [
            "Kevin Rojas",
            "Ye He",
            "Chieh-Hsin Lai",
            "Yuta Takida",
            "Yuki Mitsufuji",
            "Molei Tao"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "Classifier-Free Guidance (CFG) is a widely used technique for conditional generation and improving sample quality in continuous diffusion models, and recent works have extended it to discrete diffusion. This paper theoretically analyzes CFG in the context of masked discrete diffusion, focusing on the role of guidance schedules. Our analysis shows that high guidance early in sampling (when inputs are heavily masked) harms generation quality, while late-stage guidance has a larger effect. These findings provide a theoretical explanation for empirical observations in recent studies on guidance schedules. The analysis also reveals an imperfection of the current CFG implementations. These implementations can unintentionally cause imbalanced transitions, such as unmasking too rapidly during the early stages of generation, which degrades the quality of the resulting samples. To address this, we draw insight from the analysis and propose a novel classifier-free guidance mechanism empirically applicable to any discrete diffusion. Intuitively, our method smoothens the transport between the data distribution and the initial (masked/uniform) distribution, which results in improved sample quality. Remarkably, our method is achievable via a simple one-line code change. The efficacy of our method is empirically demonstrated with experiments on ImageNet (masked discrete diffusion) and QM9 (uniform discrete diffusion).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 81,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08966",
        "abs_url": "https://arxiv.org/abs/2507.08966",
        "pdf_url": "https://arxiv.org/pdf/2507.08966",
        "title": "ToxBench: A Binding Affinity Prediction Benchmark with AB-FEP-Calculated Labels for Human Estrogen Receptor Alpha",
        "authors": [
            "Meng Liu",
            "Karl Leswing",
            "Simon K. S. Chu",
            "Farhad Ramezanghorbani",
            "Griffin Young",
            "Gabriel Marques",
            "Prerna Das",
            "Anjali Panikar",
            "Esther Jamir",
            "Mohammed Sulaiman Shamsudeen",
            "K. Shawn Watts",
            "Ananya Sen",
            "Hari Priya Devannagari",
            "Edward B. Miller",
            "Muyun Lihan",
            "Howook Hwang",
            "Janet Paulsen",
            "Xin Yu",
            "Kyle Gion",
            "Timur Rvachov",
            "Emine Kucukbenli",
            "Saee Gopal Paliwal"
        ],
        "comments": "Workshop on Generative AI for Biology at ICML 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Chemical Physics (physics.chem-ph); Biomolecules (q-bio.BM)",
        "abstract": "Protein-ligand binding affinity prediction is essential for drug discovery and toxicity assessment. While machine learning (ML) promises fast and accurate predictions, its progress is constrained by the availability of reliable data. In contrast, physics-based methods such as absolute binding free energy perturbation (AB-FEP) deliver high accuracy but are computationally prohibitive for high-throughput applications. To bridge this gap, we introduce ToxBench, the first large-scale AB-FEP dataset designed for ML development and focused on a single pharmaceutically critical target, Human Estrogen Receptor Alpha (ER$\\alpha$). ToxBench contains 8,770 ER$\\alpha$-ligand complex structures with binding free energies computed via AB-FEP with a subset validated against experimental affinities at 1.75 kcal/mol RMSE, along with non-overlapping ligand splits to assess model generalizability. Using ToxBench, we further benchmark state-of-the-art ML methods, and notably, our proposed DualBind model, which employs a dual-loss framework to effectively learn the binding energy function. The benchmark results demonstrate the superior performance of DualBind and the potential of ML to approximate AB-FEP at a fraction of the computational cost.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 82,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08972",
        "abs_url": "https://arxiv.org/abs/2507.08972",
        "pdf_url": "https://arxiv.org/pdf/2507.08972",
        "title": "Simulating Three-dimensional Turbulence with Physics-informed Neural Networks",
        "authors": [
            "Sifan Wang",
            "Shyam Sankaran",
            "Panos Stinis",
            "Paris Perdikaris"
        ],
        "comments": "25 pages, 13 figures, 3 tables",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computational Physics (physics.comp-ph); Fluid Dynamics (physics.flu-dyn)",
        "abstract": "Turbulent fluid flows are among the most computationally demanding problems in science, requiring enormous computational resources that become prohibitive at high flow speeds. Physics-informed neural networks (PINNs) represent a radically different approach that trains neural networks directly from physical equations rather than data, offering the potential for continuous, mesh-free solutions. Here we show that appropriately designed PINNs can successfully simulate fully turbulent flows in both two and three dimensions, directly learning solutions to the fundamental fluid equations without traditional computational grids or training data. Our approach combines several algorithmic innovations including adaptive network architectures, causal training, and advanced optimization methods to overcome the inherent challenges of learning chaotic dynamics. Through rigorous validation on challenging turbulence problems, we demonstrate that PINNs accurately reproduce key flow statistics including energy spectra, kinetic energy, enstrophy, and Reynolds stresses. Our results demonstrate that neural equation solvers can handle complex chaotic systems, opening new possibilities for continuous turbulence modeling that transcends traditional computational limitations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 83,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08977",
        "abs_url": "https://arxiv.org/abs/2507.08977",
        "pdf_url": "https://arxiv.org/pdf/2507.08977",
        "title": "Simulation as Supervision: Mechanistic Pretraining for Scientific Discovery",
        "authors": [
            "Carson Dudley",
            "Reiden Magdaleno",
            "Christopher Harding",
            "Marisa Eisenberg"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "Scientific modeling faces a core limitation: mechanistic models offer interpretability but collapse under real-world complexity, while machine learning models are flexible but require large labeled datasets, cannot infer unobservable quantities, and operate as black boxes. We introduce Simulation-Grounded Neural Networks (SGNNs), a general framework that uses mechanistic simulations as training data for neural networks. SGNNs are pretrained on synthetic corpora spanning diverse model structures, parameter regimes, stochasticity, and observational artifacts. We evaluated SGNNs across scientific disciplines and modeling tasks, and found that SGNNs achieved state-of-the-art results across settings: for prediction tasks, they nearly tripled COVID-19 forecasting skill versus CDC baselines, reduced chemical yield prediction error by one third, and maintained accuracy in ecological forecasting where task specific models failed. For inference tasks, SGNNs also accurately classified the source of information spread in simulated social networks and enabled supervised learning for unobservable targets, such as estimating COVID-19 transmissibility more accurately than traditional methods even in early outbreaks. Finally, SGNNs enable back-to-simulation attribution, a new form of mechanistic interpretability. Given real world input, SGNNs retrieve simulations based on what the model has learned to see as most similar, revealing which underlying dynamics the model believes are active. This provides process-level insight -- what the model thinks is happening -- not just which features mattered. SGNNs unify scientific theory with deep learning flexibility and unlock a new modeling paradigm -- transforming simulations from rigid, post hoc tools into flexible sources of supervision, enabling robust, interpretable inference even when ground truth is missing.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 84,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.08980",
        "abs_url": "https://arxiv.org/abs/2507.08980",
        "pdf_url": "https://arxiv.org/pdf/2507.08980",
        "title": "Learning Diffusion Models with Flexible Representation Guidance",
        "authors": [
            "Chenyu Wang",
            "Cai Zhou",
            "Sharut Gupta",
            "Zongyu Lin",
            "Stefanie Jegelka",
            "Stephen Bates",
            "Tommi Jaakkola"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Diffusion models can be improved with additional guidance towards more effective representations of input. Indeed, prior empirical work has already shown that aligning internal representations of the diffusion model with those of pre-trained models improves generation quality. In this paper, we present a systematic framework for incorporating representation guidance into diffusion models. We provide alternative decompositions of denoising models along with their associated training criteria, where the decompositions determine when and how the auxiliary representations are incorporated. Guided by our theoretical insights, we introduce two new strategies for enhancing representation alignment in diffusion models. First, we pair examples with target representations either derived from themselves or arisen from different synthetic modalities, and subsequently learn a joint model over the multimodal pairs. Second, we design an optimal training curriculum that balances representation learning and data generation. Our experiments across image, protein sequence, and molecule generation tasks demonstrate superior performance as well as accelerated training. In particular, on the class-conditional ImageNet $256\\times 256$ benchmark, our guidance results in $23.3$ times faster training than the original SiT-XL as well as four times speedup over the state-of-the-art method REPA. The code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 85,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09009",
        "abs_url": "https://arxiv.org/abs/2507.09009",
        "pdf_url": "https://arxiv.org/pdf/2507.09009",
        "title": "Multimodal Cardiovascular Risk Profiling Using Self-Supervised Learning of Polysomnography",
        "authors": [
            "Zhengxiao He",
            "Huayu Li",
            "Geng Yuan",
            "William D.S. Killgore",
            "Stuart F. Quan",
            "Chen X. Chen",
            "Ao Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Methods: We developed a self-supervised deep learning model that extracts meaningful patterns from multi-modal signals (Electroencephalography (EEG), Electrocardiography (ECG), and respiratory signals). The model was trained on data from 4,398 participants. Projection scores were derived by contrasting embeddings from individuals with and without CVD outcomes. External validation was conducted in an independent cohort with 1,093 participants. The source code is available on this https URL. Results: The projection scores revealed distinct and clinically meaningful patterns across modalities. ECG-derived features were predictive of both prevalent and incident cardiac conditions, particularly CVD mortality. EEG-derived features were predictive of incident hypertension and CVD mortality. Respiratory signals added complementary predictive value. Combining these projection scores with the Framingham Risk Score consistently improved predictive performance, achieving area under the curve values ranging from 0.607 to 0.965 across different outcomes. Findings were robustly replicated and validated in the external testing cohort. Conclusion: Our findings demonstrate that the proposed framework can generate individualized CVD risk scores directly from PSG data. The resulting projection scores have the potential to be integrated into clinical practice, enhancing risk assessment and supporting personalized care.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 86,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09010",
        "abs_url": "https://arxiv.org/abs/2507.09010",
        "pdf_url": "https://arxiv.org/pdf/2507.09010",
        "title": "Hybrid Systolic Array Accelerator with Optimized Dataflow for Edge Large Language Model Inference",
        "authors": [
            "Chun-Ting Chen",
            "HanGyeol Mun",
            "Jian Meng",
            "Mohamed S. Abdelfattah",
            "Jae-sun Seo"
        ],
        "comments": "Accepted as a conference paper at the 2025 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED)",
        "subjects": "Hardware Architecture (cs.AR); Artificial Intelligence (cs.AI)",
        "abstract": "Edge inference for large language models (LLM) offers secure, low-latency, and cost-effective inference solutions. We emphasize that an edge accelerator should achieve high area efficiency and minimize external memory access (EMA) during the memory-bound decode stage, while maintaining high energy efficiency during the compute intensive prefill stage. This paper proposes an edge LLM inference accelerator featuring a hybrid systolic array (HSA) architecture that optimizes inference efficiency in both stages. To further reduce EMA, we adopt MXINT4 weight quantization and propose an optimized dataflow tailored for HSA, ensuring negligible dequantization overhead and achieving 100% hardware utilization with minimal accuracy loss under edge DRAM bandwidth constraints. For non-linear operations, we incorporate optimized root mean square normalization (RMSNorm) and rotary position embedding (RoPE) units, reducing their latency, area, and memory access overhead while enabling end-to-end inference on our accelerator. Our solution achieves 247/117 (token/s/mm2) while running a 1.3B LLM on long-input/long-output scenarios, providing >2.45x/13.5x improvement over existing approaches, while maintaining superior energy efficiency in token generation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 87,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09019",
        "abs_url": "https://arxiv.org/abs/2507.09019",
        "pdf_url": "https://arxiv.org/pdf/2507.09019",
        "title": "On Evaluating Performance of LLM Inference Serving Systems",
        "authors": [
            "Amey Agrawal",
            "Nitin Kedia",
            "Anmol Agarwal",
            "Jayashree Mohan",
            "Nipun Kwatra",
            "Souvik Kundu",
            "Ramachandran Ramjee",
            "Alexey Tumanov"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)",
        "abstract": "The rapid evolution of Large Language Model (LLM) inference systems has yielded significant efficiency improvements. However, our systematic analysis reveals that current evaluation methodologies frequently exhibit fundamental flaws, often manifesting as common evaluation anti-patterns that obscure true performance characteristics and impede scientific progress. Through a comprehensive examination of recent systems, we identify recurring anti-patterns across three key dimensions: Baseline Fairness, Evaluation Setup, and Metric Design. These anti-patterns are uniquely problematic for LLM inference due to its dual-phase nature combining distinct prefill and decode operations, its handling of highly heterogeneous workloads, and its strict temporal requirements for interactive use. We demonstrate how common anti-patterns -- such as inadequate baseline comparisons that conflate engineering effort with algorithmic novelty, workload selections that fail to represent production scenarios, and metric normalizations that hide substantial performance variability like generation stalls-lead to misleading conclusions. To address these challenges, we provide a comprehensive checklist derived from our analysis, establishing a framework for recognizing and avoiding these anti-patterns in favor of robust LLM inference evaluation. To demonstrate the practical application of our framework, we present a case study analyzing speculative decoding, a technique whose bursty, non-uniform token generation is easily misinterpreted when evaluated using approaches characteristic of these anti-patterns. Our work establishes a rigorous foundation for evaluation methodology, enabling meaningful comparisons, ensuring reproducible results, and ultimately accelerating genuine progress in LLM inference systems by moving beyond common anti-patterns to align evaluation with real-world requirements.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 88,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09023",
        "abs_url": "https://arxiv.org/abs/2507.09023",
        "pdf_url": "https://arxiv.org/pdf/2507.09023",
        "title": "Accelerating Drug Discovery Through Agentic AI: A Multi-Agent Approach to Laboratory Automation in the DMTA Cycle",
        "authors": [
            "Yao Fehlis",
            "Charles Crain",
            "Aidan Jensen",
            "Michael Watson",
            "James Juhasz",
            "Paul Mandel",
            "Betty Liu",
            "Shawn Mahon",
            "Daren Wilson",
            "Nick Lynch-Jonely",
            "Ben Leedom",
            "David Fuller"
        ],
        "comments": "",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)",
        "abstract": "The pharmaceutical industry faces unprecedented challenges in drug discovery, with traditional approaches struggling to meet modern therapeutic development demands. This paper introduces a novel AI framework, Tippy, that transforms laboratory automation through specialized AI agents operating within the Design-Make-Test-Analyze (DMTA) cycle. Our multi-agent system employs five specialized agents - Supervisor, Molecule, Lab, Analysis, and Report, with Safety Guardrail oversight - each designed to excel in specific phases of the drug discovery pipeline. Tippy represents the first production-ready implementation of specialized AI agents for automating the DMTA cycle, providing a concrete example of how AI can transform laboratory workflows. By leveraging autonomous AI agents that reason, plan, and collaborate, we demonstrate how Tippy accelerates DMTA cycles while maintaining scientific rigor essential for pharmaceutical research. The system shows significant improvements in workflow efficiency, decision-making speed, and cross-disciplinary coordination, offering a new paradigm for AI-assisted drug discovery.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 89,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09028",
        "abs_url": "https://arxiv.org/abs/2507.09028",
        "pdf_url": "https://arxiv.org/pdf/2507.09028",
        "title": "From Classical Machine Learning to Emerging Foundation Models: Review on Multimodal Data Integration for Cancer Research",
        "authors": [
            "Amgad Muneer",
            "Muhammad Waqas",
            "Maliazurina B Saad",
            "Eman Showkatian",
            "Rukhmini Bandyopadhyay",
            "Hui Xu",
            "Wentao Li",
            "Joe Y Chang",
            "Zhongxing Liao",
            "Cara Haymaker",
            "Luisa Solis Soto",
            "Carol C Wu",
            "Natalie I Vokes",
            "Xiuning Le",
            "Lauren A Byers",
            "Don L Gibbons",
            "John V Heymach",
            "Jianjun Zhang",
            "Jia Wu"
        ],
        "comments": "6 figures, 3 tables",
        "subjects": "Quantitative Methods (q-bio.QM); Artificial Intelligence (cs.AI)",
        "abstract": "Cancer research is increasingly driven by the integration of diverse data modalities, spanning from genomics and proteomics to imaging and clinical factors. However, extracting actionable insights from these vast and heterogeneous datasets remains a key challenge. The rise of foundation models (FMs) -- large deep-learning models pretrained on extensive amounts of data serving as a backbone for a wide range of downstream tasks -- offers new avenues for discovering biomarkers, improving diagnosis, and personalizing treatment. This paper presents a comprehensive review of widely adopted integration strategies of multimodal data to assist advance the computational approaches for data-driven discoveries in oncology. We examine emerging trends in machine learning (ML) and deep learning (DL), including methodological frameworks, validation protocols, and open-source resources targeting cancer subtype classification, biomarker discovery, treatment guidance, and outcome prediction. This study also comprehensively covers the shift from traditional ML to FMs for multimodal integration. We present a holistic view of recent FMs advancements and challenges faced during the integration of multi-omics with advanced imaging data. We identify the state-of-the-art FMs, publicly available multi-modal repositories, and advanced tools and methods for data integration. We argue that current state-of-the-art integrative methods provide the essential groundwork for developing the next generation of large-scale, pre-trained models poised to further revolutionize oncology. To the best of our knowledge, this is the first review to systematically map the transition from conventional ML to advanced FM for multimodal data integration in oncology, while also framing these developments as foundational for the forthcoming era of large-scale AI models in cancer research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 90,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09029",
        "abs_url": "https://arxiv.org/abs/2507.09029",
        "pdf_url": "https://arxiv.org/pdf/2507.09029",
        "title": "Model Parallelism With Subnetwork Data Parallelism",
        "authors": [
            "Vaibhav Singh",
            "Zafir Khalid",
            "Edouard Oyallon",
            "Eugene Belilovsky"
        ],
        "comments": "6 pages, 1 figure",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Distributed pre-training of large models at scale often imposes heavy memory demands on individual nodes and incurs significant intra-node communication costs. We propose a novel alternative approach that reduces the memory requirements by training small, structured subnetworks of the model on separate workers. Unlike pipelining, our method avoids inter-node activation communication and maintains bandwidth requirements that are comparable to or lower than standard data parallel communication schemes based on all-reduce. We evaluate two subnetwork construction strategies guided by the principle of ensuring uniform representation of each parameter across the distributed training setup. Our results show that the stochastic block dropping technique consistently outperforms the width-wise subnetwork construction previously explored in federated learning. We empirically attribute this superior performance to stronger gradient alignment in subnetworks that retain blocks having skip connections. Preliminary experiments highlight the promise of our approach, achieving a 20-40% reduction in memory usage without any loss in performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 91,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09036",
        "abs_url": "https://arxiv.org/abs/2507.09036",
        "pdf_url": "https://arxiv.org/pdf/2507.09036",
        "title": "BrainLesion Suite: A Flexible and User-Friendly Framework for Modular Brain Lesion Image Analysis",
        "authors": [
            "Florian Kofler",
            "Marcel Rosier",
            "Mehdi Astaraki",
            "Hendrik Möller",
            "Ilhem Isra Mekki",
            "Josef A. Buchner",
            "Anton Schmick",
            "Arianna Pfiffer",
            "Eva Oswald",
            "Lucas Zimmer",
            "Ezequiel de la Rosa",
            "Sarthak Pati",
            "Julian Canisius",
            "Arianna Piffer",
            "Ujjwal Baid",
            "Mahyar Valizadeh",
            "Akis Linardos",
            "Jan C. Peeken",
            "Surprosanna Shit",
            "Felix Steinbauer",
            "Daniel Rueckert",
            "Rolf Heckemann",
            "Spyridon Bakas",
            "Jan Kirschke",
            "Constantin von See",
            "Ivan Ezhov",
            "Marie Piraud",
            "Benedikt Wiestler",
            "Bjoern Menze"
        ],
        "comments": "16p, 3f",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Software Engineering (cs.SE)",
        "abstract": "BrainLesion Suite is a versatile toolkit for building modular brain lesion image analysis pipelines in Python. Following Pythonic principles, BrainLesion Suite is designed to provide a 'brainless' development experience, minimizing cognitive effort and streamlining the creation of complex workflows for clinical and scientific practice. At its core is an adaptable preprocessing module that performs co-registration, atlas registration, and optional skull-stripping and defacing on arbitrary multi-modal input images. BrainLesion Suite leverages algorithms from the BraTS challenge to synthesize missing modalities, inpaint lesions, and generate pathology-specific tumor segmentations. BrainLesion Suite also enables quantifying segmentation model performance, with tools such as panoptica to compute lesion-wise metrics. Although BrainLesion Suite was originally developed for image analysis pipelines of brain lesions such as glioma, metastasis, and multiple sclerosis, it can be adapted for other biomedical image analysis applications. The individual BrainLesion Suite packages and tutorials are accessible on GitHub.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 92,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09037",
        "abs_url": "https://arxiv.org/abs/2507.09037",
        "pdf_url": "https://arxiv.org/pdf/2507.09037",
        "title": "ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making",
        "authors": [
            "Bharadwaj Ravichandran",
            "David Joy",
            "Paul Elliott",
            "Brian Hu",
            "Jadie Adams",
            "Christopher Funk",
            "Emily Veenhuis",
            "Anthony Hoogs",
            "Arslan Basharat"
        ],
        "comments": "10 pages total (including appendix), ICML 2025 Workshop on Reliable and Responsible Foundation Models",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Large language models (LLMs) are increasingly being used as decision aids. However, users have diverse values and preferences that can affect their decision-making, which requires novel methods for LLM alignment and personalization. Existing LLM comparison tools largely focus on benchmarking tasks, such as knowledge-based question answering. In contrast, our proposed ALIGN system focuses on dynamic personalization of LLM-based decision-makers through prompt-based alignment to a set of fine-grained attributes. Key features of our system include robust configuration management, structured output generation with reasoning, and several algorithm implementations with swappable LLM backbones, enabling different types of analyses. Our user interface enables a qualitative, side-by-side comparison of LLMs and their alignment to various attributes, with a modular backend for easy algorithm integration. Additionally, we perform a quantitative analysis comparing alignment approaches in two different domains: demographic alignment for public opinion surveys and value alignment for medical triage decision-making. The entire ALIGN framework is open source and will enable new research on reliable, responsible, and personalized LLM-based decision-makers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 93,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09063",
        "abs_url": "https://arxiv.org/abs/2507.09063",
        "pdf_url": "https://arxiv.org/pdf/2507.09063",
        "title": "SetupBench: Assessing Software Engineering Agents' Ability to Bootstrap Development Environments",
        "authors": [
            "Avi Arora",
            "Jinu Jang",
            "Roshanak Zilouchian Moghaddam"
        ],
        "comments": "",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Modern Large Language Model (LLM) agents promise end to end assistance with real-world software tasks, yet existing benchmarks evaluate LLM agents almost exclusively in pre-baked environments where every dependency is pre-installed. To fill this gap, we introduce SetupBench, a 93 instance benchmark that isolates the environment-bootstrap skill: starting from a bare Linux sandbox, an agent must install packages, resolve dependency conflicts, initialize databases, and configure background services. Our tasks span seven language ecosystems, five database engines, and multi-service orchestration scenarios, each accompanies by a natural language problem statement and a deterministic success command. Through evaluation of OpenHands, a state-of-the-art coding agent, we find low success rates across task categories, with particular challenges in repository setup (38.9-57.4%) and local database configuration (20.0-53.3%). Our analysis reveals systematic failure modes including incomplete development tooling installation, hallucinated task constraints, and non-persistent environment modifications that break agent-human collaboration workflows. We identify substantial inefficiencies in agent exploration strategies, with 38-89% of actions being unnecessary compared to optimal human behavior. These findings highlight gaps in current agents' practical environment-bootstrap capabilities. By targeting this critical yet under-evaluated capability, SetupBench provides a rigorous yard-stick for the next generation of software developer agents aiming to solve end to end real-wold tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 94,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09068",
        "abs_url": "https://arxiv.org/abs/2507.09068",
        "pdf_url": "https://arxiv.org/pdf/2507.09068",
        "title": "Infinite Video Understanding",
        "authors": [
            "Dell Zhang",
            "Xiangyu Chen",
            "Jixiang Luo",
            "Mengxi Jia",
            "Changzhi Sun",
            "Ruilong Ren",
            "Jingren Liu",
            "Hao Sun",
            "Xuelong Li"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG); Multimedia (cs.MM)",
        "abstract": "The rapid advancements in Large Language Models (LLMs) and their multimodal extensions (MLLMs) have ushered in remarkable progress in video understanding. However, a fundamental challenge persists: effectively processing and comprehending video content that extends beyond minutes or hours. While recent efforts like Video-XL-2 have demonstrated novel architectural solutions for extreme efficiency, and advancements in positional encoding such as HoPE and VideoRoPE++ aim to improve spatio-temporal understanding over extensive contexts, current state-of-the-art models still encounter significant computational and memory constraints when faced with the sheer volume of visual tokens from lengthy sequences. Furthermore, maintaining temporal coherence, tracking complex events, and preserving fine-grained details over extended periods remain formidable hurdles, despite progress in agentic reasoning systems like Deep Video Discovery. This position paper posits that a logical, albeit ambitious, next frontier for multimedia research is Infinite Video Understanding -- the capability for models to continuously process, understand, and reason about video data of arbitrary, potentially never-ending duration. We argue that framing Infinite Video Understanding as a blue-sky research objective provides a vital north star for the multimedia, and the wider AI, research communities, driving innovation in areas such as streaming architectures, persistent memory mechanisms, hierarchical and adaptive representations, event-centric reasoning, and novel evaluation paradigms. Drawing inspiration from recent work on long/ultra-long video understanding and several closely related fields, we outline the core challenges and key research directions towards achieving this transformative capability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 95,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09076",
        "abs_url": "https://arxiv.org/abs/2507.09076",
        "pdf_url": "https://arxiv.org/pdf/2507.09076",
        "title": "Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence Emotion Recognition in Conversation",
        "authors": [
            "Jialong Mai",
            "Xiaofen Xing",
            "Yawei Li",
            "Zhipeng Li",
            "Jingyuan Xing",
            "Xiangmin Xu"
        ],
        "comments": "submitted to EMNLP 2025",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Recent research has focused on applying speech large language model (SLLM) to improve speech emotion recognition (SER). However, the inherently high frame rate in speech modality severely limits the signal processing and understanding capabilities of SLLM. For example, a SLLM with a 4K context window can only process 80 seconds of audio at 50Hz feature sampling rate before reaching its capacity limit. Input token compression methods used in SLLM overlook the continuity and inertia of emotions across multiple conversation turns. This paper proposes a Dynamic Parameter Memory (DPM) mechanism with contextual semantics and sentence-level emotion encoding, enabling processing of unlimited-length audio with limited context windows in SLLM. Specifically, DPM progressively encodes sentence-level information and emotions into a temporary LoRA module during inference to effectively \"memorize\" the contextual information. We trained an emotion SLLM as a backbone and incorporated our DPM into inference for emotion recognition in conversation (ERC). Experimental results on the IEMOCAP dataset show that DPM significantly improves the emotion recognition capabilities of SLLM when processing long audio sequences, achieving state-of-the-art performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 96,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09083",
        "abs_url": "https://arxiv.org/abs/2507.09083",
        "pdf_url": "https://arxiv.org/pdf/2507.09083",
        "title": "Learning from Synthetic Labs: Language Models as Auction Participants",
        "authors": [
            "Anand Shah",
            "Kehang Zhu",
            "Yanchen Jiang",
            "Jeffrey G. Wang",
            "Arif K. Dayi",
            "John J. Horton",
            "David C. Parkes"
        ],
        "comments": "",
        "subjects": "Computer Science and Game Theory (cs.GT); Artificial Intelligence (cs.AI)",
        "abstract": "This paper investigates the behavior of simulated AI agents (large language models, or LLMs) in auctions, introducing a novel synthetic data-generating process to help facilitate the study and design of auctions. We find that LLMs -- when endowed with chain of thought reasoning capacity -- agree with the experimental literature in auctions across a variety of classic auction formats. In particular, we find that LLM bidders produce results consistent with risk-averse human bidders; that they perform closer to theoretical predictions in obviously strategy-proof auctions; and, that they succumb to the winner's curse in common value settings. On prompting, we find that LLMs are not very sensitive to naive changes in prompts (e.g., language, currency) but can improve dramatically towards theoretical predictions with the right mental model (i.e., the language of Nash deviations). We run 1,000$+$ auctions for less than $\\$$400 with GPT-4 models (three orders of magnitude cheaper than modern auction experiments) and develop a framework flexible enough to run auction experiments with any LLM model and a wide range of auction design specifications, facilitating further experimental study by decreasing costs and serving as a proof-of-concept for the use of LLM proxies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 97,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09084",
        "abs_url": "https://arxiv.org/abs/2507.09084",
        "pdf_url": "https://arxiv.org/pdf/2507.09084",
        "title": "Queue up for takeoff: a transferable deep learning framework for flight delay prediction",
        "authors": [
            "Nnamdi Daniel Aghanya",
            "Ta Duong Vu",
            "Amaëlle Diop",
            "Charlotte Deville",
            "Nour Imane Kerroumi",
            "Irene Moulitsas",
            "Jun Li",
            "Desmond Bisandu"
        ],
        "comments": "3 figures, 20 pages references and appendix included,",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Flight delays are a significant challenge in the aviation industry, causing major financial and operational disruptions. To improve passenger experience and reduce revenue loss, flight delay prediction models must be both precise and generalizable across different networks. This paper introduces a novel approach that combines Queue-Theory with a simple attention model, referred to as the Queue-Theory SimAM (QT-SimAM). To validate our model, we used data from the US Bureau of Transportation Statistics, where our proposed QT-SimAM (Bidirectional) model outperformed existing methods with an accuracy of 0.927 and an F1 score of 0.932. To assess transferability, we tested the model on the EUROCONTROL dataset. The results demonstrated strong performance, achieving an accuracy of 0.826 and an F1 score of 0.791. Ultimately, this paper outlines an effective, end-to-end methodology for predicting flight delays. The proposed model's ability to forecast delays with high accuracy across different networks can help reduce passenger anxiety and improve operational decision-making",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 98,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09087",
        "abs_url": "https://arxiv.org/abs/2507.09087",
        "pdf_url": "https://arxiv.org/pdf/2507.09087",
        "title": "Deep Reinforcement Learning with Gradient Eligibility Traces",
        "authors": [
            "Esraa Elelimy",
            "Brett Daley",
            "Andrew Patterson",
            "Marlos C. Machado",
            "Adam White",
            "Martha White"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "Achieving fast and stable off-policy learning in deep reinforcement learning (RL) is challenging. Most existing methods rely on semi-gradient temporal-difference (TD) methods for their simplicity and efficiency, but are consequently susceptible to divergence. While more principled approaches like Gradient TD (GTD) methods have strong convergence guarantees, they have rarely been used in deep RL. Recent work introduced the Generalized Projected Bellman Error ($\\GPBE$), enabling GTD methods to work efficiently with nonlinear function approximation. However, this work is only limited to one-step methods, which are slow at credit assignment and require a large number of samples. In this paper, we extend the $\\GPBE$ objective to support multistep credit assignment based on the $\\lambda$-return and derive three gradient-based methods that optimize this new objective. We provide both a forward-view formulation compatible with experience replay and a backward-view formulation compatible with streaming algorithms. Finally, we evaluate the proposed algorithms and show that they outperform both PPO and StreamQ in MuJoCo and MinAtar environments, respectively. Code available at this https URL\\_algos",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 99,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09100",
        "abs_url": "https://arxiv.org/abs/2507.09100",
        "pdf_url": "https://arxiv.org/pdf/2507.09100",
        "title": "AInsight: Augmenting Expert Decision-Making with On-the-Fly Insights Grounded in Historical Data",
        "authors": [
            "Mohammad Abolnejadian",
            "Shakiba Amirshahi",
            "Matthew Brehmer",
            "Anamaria Crisan"
        ],
        "comments": "7 pages and 4 figures. Proceedings of the 7th ACM Conference on Conversational User Interfaces (CUI '25)",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "In decision-making conversations, experts must navigate complex choices and make on-the-spot decisions while engaged in conversation. Although extensive historical data often exists, the real-time nature of these scenarios makes it infeasible for decision-makers to review and leverage relevant information. This raises an interesting question: What if experts could utilize relevant past data in real-time decision-making through insights derived from past data? To explore this, we implemented a conversational user interface, taking doctor-patient interactions as an example use case. Our system continuously listens to the conversation, identifies patient problems and doctor-suggested solutions, and retrieves related data from an embedded dataset, generating concise insights using a pipeline built around a retrieval-based Large Language Model (LLM) agent. We evaluated the prototype by embedding Health Canada datasets into a vector database and conducting simulated studies using sample doctor-patient dialogues, showing effectiveness but also challenges, setting directions for the next steps of our work.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 100,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09104",
        "abs_url": "https://arxiv.org/abs/2507.09104",
        "pdf_url": "https://arxiv.org/pdf/2507.09104",
        "title": "CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards",
        "authors": [
            "Taolin Zhang",
            "Maosong Cao",
            "Alexander Lam",
            "Songyang Zhang",
            "Kai Chen"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Recently, the role of LLM-as-judge in evaluating large language models has gained prominence. However, current judge models suffer from narrow specialization and limited robustness, undermining their capacity for comprehensive evaluations. In this work, we present CompassJudger-2, a novel generalist judge model that overcomes these limitations via a task-driven, multi-domain data curation strategy. Central to our approach is supervising judgment tasks with verifiable rewards, guiding intrinsic critical reasoning through rejection sampling to foster robust, generalizable judgment capabilities. We introduce a refined learning objective with margin policy gradient loss to enhance performance. Empirically, CompassJudger-2 achieves superior results across multiple judge and reward benchmarks, and our 7B model demonstrates competitive judgment accuracy with significantly larger models like DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, a comprehensive benchmark evaluating cross-domain judgment accuracy and rank consistency to standardize judge model evaluation. These contributions advance robust, scalable LLM judgment and establish new performance and evaluation standards.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 101,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09108",
        "abs_url": "https://arxiv.org/abs/2507.09108",
        "pdf_url": "https://arxiv.org/pdf/2507.09108",
        "title": "SPICE: An Automated SWE-Bench Labeling Pipeline for Issue Clarity, Test Coverage, and Effort Estimation",
        "authors": [
            "Aaditya Bhatia",
            "Gustavo A. Oliva",
            "Gopi Krishnan Rajbahadur",
            "Haoxiang Zhang",
            "Yihao Chen",
            "Zhilong Chen",
            "Arthur Leung",
            "Dayi Lin",
            "Boyuan Chen",
            "Ahmed E. Hassan"
        ],
        "comments": "",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)",
        "abstract": "High-quality labeled datasets are crucial for training and evaluating foundation models in software engineering, but creating them is often prohibitively expensive and labor-intensive. We introduce SPICE, a scalable, automated pipeline for labeling SWE-bench-style datasets with annotations for issue clarity, test coverage, and effort estimation. SPICE combines context-aware code navigation, rationale-driven prompting, and multi-pass consensus to produce labels that closely approximate expert annotations. SPICE's design was informed by our own experience and frustration in labeling more than 800 instances from SWE-Gym. SPICE achieves strong agreement with human-labeled SWE-bench Verified data while reducing the cost of labeling 1,000 instances from around $100,000 (manual annotation) to just $5.10. These results demonstrate SPICE's potential to enable cost-effective, large-scale dataset creation for SE-focused FMs. To support the community, we release both SPICE tool and SPICE Bench, a new dataset of 6,802 SPICE-labeled instances curated from 291 open-source projects in SWE-Gym (over 13x larger than SWE-bench Verified).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 102,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09117",
        "abs_url": "https://arxiv.org/abs/2507.09117",
        "pdf_url": "https://arxiv.org/pdf/2507.09117",
        "title": "Towards Human-level Dexterity via Robot Learning",
        "authors": [
            "Gagan Khandate"
        ],
        "comments": "PhD thesis",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
        "abstract": "Dexterous intelligence -- the ability to perform complex interactions with multi-fingered hands -- is a pinnacle of human physical intelligence and emergent higher-order cognitive skills. However, contrary to Moravec's paradox, dexterous intelligence in humans appears simple only superficially. Many million years were spent co-evolving the human brain and hands including rich tactile sensing. Achieving human-level dexterity with robotic hands has long been a fundamental goal in robotics and represents a critical milestone toward general embodied intelligence. In this pursuit, computational sensorimotor learning has made significant progress, enabling feats such as arbitrary in-hand object reorientation. However, we observe that achieving higher levels of dexterity requires overcoming very fundamental limitations of computational sensorimotor learning. I develop robot learning methods for highly dexterous multi-fingered manipulation by directly addressing these limitations at their root cause. Chiefly, through key studies, this disseration progressively builds an effective framework for reinforcement learning of dexterous multi-fingered manipulation skills. These methods adopt structured exploration, effectively overcoming the limitations of random exploration in reinforcement learning. The insights gained culminate in a highly effective reinforcement learning that incorporates sampling-based planning for direct exploration. Additionally, this thesis explores a new paradigm of using visuo-tactile human demonstrations for dexterity, introducing corresponding imitation learning techniques.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 103,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09132",
        "abs_url": "https://arxiv.org/abs/2507.09132",
        "pdf_url": "https://arxiv.org/pdf/2507.09132",
        "title": "Heterogeneous Graph Prompt Learning via Adaptive Weight Pruning",
        "authors": [
            "Chu-Yuan Wei",
            "Shun-Yao Liu",
            "Sheng-Da Zhuo",
            "Chang-Dong Wang",
            "Shu-Qiang Huang",
            "Mohsen Guizani"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Graph Neural Networks (GNNs) have achieved remarkable success in various graph-based tasks (e.g., node classification or link prediction). Despite their triumphs, GNNs still face challenges such as long training and inference times, difficulty in capturing complex relationships, and insufficient feature extraction. To tackle these issues, graph pre-training and graph prompt methods have garnered increasing attention for their ability to leverage large-scale datasets for initial learning and task-specific adaptation, offering potential improvements in GNN performance. However, previous research has overlooked the potential of graph prompts in optimizing models, as well as the impact of both positive and negative graph prompts on model stability and efficiency. To bridge this gap, we propose a novel framework combining graph prompts with weight pruning, called GPAWP, which aims to enhance the performance and efficiency of graph prompts by using fewer of them. We evaluate the importance of graph prompts using an importance assessment function to determine positive and negative weights at different granularities. Through hierarchically structured pruning, we eliminate negative prompt labels, resulting in more parameter-efficient and competitively performing prompts. Extensive experiments on three benchmark datasets demonstrate the superiority of GPAWP, leading to a significant reduction in parameters in node classification tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 104,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09137",
        "abs_url": "https://arxiv.org/abs/2507.09137",
        "pdf_url": "https://arxiv.org/pdf/2507.09137",
        "title": "POIFormer: A Transformer-Based Framework for Accurate and Scalable Point-of-Interest Attribution",
        "authors": [
            "Nripsuta Ani Saxena",
            "Shang-Ling Hsu",
            "Mehul Shetty",
            "Omar Alkhadra",
            "Cyrus Shahabi",
            "Abigail L. Horn"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Accurately attributing user visits to specific Points of Interest (POIs) is a foundational task for mobility analytics, personalized services, marketing and urban planning. However, POI attribution remains challenging due to GPS inaccuracies, typically ranging from 2 to 20 meters in real-world settings, and the high spatial density of POIs in urban environments, where multiple venues can coexist within a small radius (e.g., over 50 POIs within a 100-meter radius in dense city centers). Relying on proximity is therefore often insufficient for determining which POI was actually visited. We introduce \\textsf{POIFormer}, a novel Transformer-based framework for accurate and efficient POI attribution. Unlike prior approaches that rely on limited spatiotemporal, contextual, or behavioral features, \\textsf{POIFormer} jointly models a rich set of signals, including spatial proximity, visit timing and duration, contextual features from POI semantics, and behavioral features from user mobility and aggregated crowd behavior patterns--using the Transformer's self-attention mechanism to jointly model complex interactions across these dimensions. By leveraging the Transformer to model a user's past and future visits (with the current visit masked) and incorporating crowd-level behavioral patterns through pre-computed KDEs, \\textsf{POIFormer} enables accurate, efficient attribution in large, noisy mobility datasets. Its architecture supports generalization across diverse data sources and geographic contexts while avoiding reliance on hard-to-access or unavailable data layers, making it practical for real-world deployment. Extensive experiments on real-world mobility datasets demonstrate significant improvements over existing baselines, particularly in challenging real-world settings characterized by spatial noise and dense POI clustering.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 105,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09149",
        "abs_url": "https://arxiv.org/abs/2507.09149",
        "pdf_url": "https://arxiv.org/pdf/2507.09149",
        "title": "Advanced Health Misinformation Detection Through Hybrid CNN-LSTM Models Informed by the Elaboration Likelihood Model (ELM)",
        "authors": [
            "Mkululi Sikosana",
            "Sean Maudsley-Barton",
            "Oluwaseun Ajao"
        ],
        "comments": "11 Pages, 2 Figures, 3 Tables conference paper to appear in proceedings of International Conference on Artificial Intelligence, Computer, Data Sciences and Applications (ACDSA'25)",
        "subjects": "Social and Information Networks (cs.SI); Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG)",
        "abstract": "Health misinformation during the COVID-19 pandemic has significantly challenged public health efforts globally. This study applies the Elaboration Likelihood Model (ELM) to enhance misinformation detection on social media using a hybrid Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) model. The model aims to enhance the detection accuracy and reliability of misinformation classification by integrating ELM-based features such as text readability, sentiment polarity, and heuristic cues (e.g., punctuation frequency). The enhanced model achieved an accuracy of 97.37%, precision of 96.88%, recall of 98.50%, F1-score of 97.41%, and ROC-AUC of 99.50%. A combined model incorporating feature engineering further improved performance, achieving a precision of 98.88%, recall of 99.80%, F1-score of 99.41%, and ROC-AUC of 99.80%. These findings highlight the value of ELM features in improving detection performance, offering valuable contextual information. This study demonstrates the practical application of psychological theories in developing advanced machine learning algorithms to address health misinformation effectively.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 106,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09155",
        "abs_url": "https://arxiv.org/abs/2507.09155",
        "pdf_url": "https://arxiv.org/pdf/2507.09155",
        "title": "OPENXRD: A Comprehensive Benchmark and Enhancement Framework for LLM/MLLM XRD Question Answering",
        "authors": [
            "Ali Vosoughi",
            "Ayoub Shahnazari",
            "Yufeng Xi",
            "Zeliang Zhang",
            "Griffin Hess",
            "Chenliang Xu",
            "Niaz Abdolrahim"
        ],
        "comments": "10 pages, 6 figures, 5 tables. Code and dataset available at this https URL. Project webpage: this https URL",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "This work presents OPENXRD, an open-book pipeline designed for crystallography question answering, which integrates textual prompts with concise supporting content generated by GPT-4.5. Instead of using scanned textbooks, which may lead to copyright issues, OPENXRD generates compact, domain-specific references that help smaller models understand key concepts in X-ray diffraction (XRD). We evaluate OPENXRD on a well-defined set of 217 expert-level XRD questions by comparing different vision-language models, including GPT-4 and LLaVA-based frameworks such as Mistral, LLaMA, and QWEN, under both closed-book (without supporting material) and open-book (with supporting material) conditions. Our experimental results show significant accuracy improvements in models that use the GPT-4.5-generated summaries, particularly those with limited prior training in crystallography. OPENXRD uses knowledge from larger models to fill knowledge gaps in crystallography and shows that AI-generated texts can help smaller models reason more effectively in scientific tasks. While the current version of OPENXRD focuses on text-based inputs, we also explore future extensions such as adding real crystal diagrams or diffraction patterns to improve interpretation in specialized materials science contexts. Overall, OPENXRD shows that specialized open-book systems can be useful in materials science and provides a foundation for broader natural language processing (NLP) tools in critical scientific fields.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 107,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09158",
        "abs_url": "https://arxiv.org/abs/2507.09158",
        "pdf_url": "https://arxiv.org/pdf/2507.09158",
        "title": "Automatic Contouring of Spinal Vertebrae on X-Ray using a Novel Sandwich U-Net Architecture",
        "authors": [
            "Sunil Munthumoduku Krishna Murthy",
            "Kumar Rajamani",
            "Srividya Tirunellai Rajamani",
            "Yupei Li",
            "Qiyang Sun",
            "Bjoern W. Schuller"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "In spinal vertebral mobility disease, accurately extracting and contouring vertebrae is essential for assessing mobility impairments and monitoring variations during flexion-extension movements. Precise vertebral contouring plays a crucial role in surgical planning; however, this process is traditionally performed manually by radiologists or surgeons, making it labour-intensive, time-consuming, and prone to human error. In particular, mobility disease analysis requires the individual contouring of each vertebra, which is both tedious and susceptible to inconsistencies. Automated methods provide a more efficient alternative, enabling vertebra identification, segmentation, and contouring with greater accuracy and reduced time consumption. In this study, we propose a novel U-Net variation designed to accurately segment thoracic vertebrae from anteroposterior view on X-Ray images. Our proposed approach, incorporating a ``sandwich\" U-Net structure with dual activation functions, achieves a 4.1\\% improvement in Dice score compared to the baseline U-Net model, enhancing segmentation accuracy while ensuring reliable vertebral contour extraction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 108,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09173",
        "abs_url": "https://arxiv.org/abs/2507.09173",
        "pdf_url": "https://arxiv.org/pdf/2507.09173",
        "title": "Towards Interpretable Drug-Drug Interaction Prediction: A Graph-Based Approach with Molecular and Network-Level Explanations",
        "authors": [
            "Mengjie Chen",
            "Ming Zhang",
            "Cunquan Qu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Molecular Networks (q-bio.MN)",
        "abstract": "Drug-drug interactions (DDIs) represent a critical challenge in pharmacology, often leading to adverse drug reactions with significant implications for patient safety and healthcare outcomes. While graph-based methods have achieved strong predictive performance, most approaches treat drug pairs independently, overlooking the complex, context-dependent interactions unique to drug pairs. Additionally, these models struggle to integrate biological interaction networks and molecular-level structures to provide meaningful mechanistic insights. In this study, we propose MolecBioNet, a novel graph-based framework that integrates molecular and biomedical knowledge for robust and interpretable DDI prediction. By modeling drug pairs as unified entities, MolecBioNet captures both macro-level biological interactions and micro-level molecular influences, offering a comprehensive perspective on DDIs. The framework extracts local subgraphs from biomedical knowledge graphs and constructs hierarchical interaction graphs from molecular representations, leveraging classical graph neural network methods to learn multi-scale representations of drug pairs. To enhance accuracy and interpretability, MolecBioNet introduces two domain-specific pooling strategies: context-aware subgraph pooling (CASPool), which emphasizes biologically relevant entities, and attention-guided influence pooling (AGIPool), which prioritizes influential molecular substructures. The framework further employs mutual information minimization regularization to enhance information diversity during embedding fusion. Experimental results demonstrate that MolecBioNet outperforms state-of-the-art methods in DDI prediction, while ablation studies and embedding visualizations further validate the advantages of unified drug pair modeling and multi-scale knowledge integration.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 109,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09177",
        "abs_url": "https://arxiv.org/abs/2507.09177",
        "pdf_url": "https://arxiv.org/pdf/2507.09177",
        "title": "Continual Reinforcement Learning by Planning with Online World Models",
        "authors": [
            "Zichen Liu",
            "Guoji Fu",
            "Chao Du",
            "Wee Sun Lee",
            "Min Lin"
        ],
        "comments": "ICML 2025 Spotlight",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "Continual reinforcement learning (CRL) refers to a naturalistic setting where an agent needs to endlessly evolve, by trial and error, to solve multiple tasks that are presented sequentially. One of the largest obstacles to CRL is that the agent may forget how to solve previous tasks when learning a new task, known as catastrophic forgetting. In this paper, we propose to address this challenge by planning with online world models. Specifically, we learn a Follow-The-Leader shallow model online to capture the world dynamics, in which we plan using model predictive control to solve a set of tasks specified by any reward functions. The online world model is immune to forgetting by construction with a proven regret bound of $\\mathcal{O}(\\sqrt{K^2D\\log(T)})$ under mild assumptions. The planner searches actions solely based on the latest online model, thus forming a FTL Online Agent (OA) that updates incrementally. To assess OA, we further design Continual Bench, a dedicated environment for CRL, and compare with several strong baselines under the same model-planning algorithmic framework. The empirical results show that OA learns continuously to solve new tasks while not forgetting old skills, outperforming agents built on deep world models with various continual learning techniques.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 110,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09202",
        "abs_url": "https://arxiv.org/abs/2507.09202",
        "pdf_url": "https://arxiv.org/pdf/2507.09202",
        "title": "XiChen: An observation-scalable fully AI-driven global weather forecasting system with 4D variational knowledge",
        "authors": [
            "Wuxin Wang",
            "Weicheng Ni",
            "Lilan Huang",
            "Tao Hao",
            "Ben Fei",
            "Shuo Ma",
            "Taikang Yuan",
            "Yanlai Zhao",
            "Kefeng Deng",
            "Xiaoyong Li",
            "Boheng Duan",
            "Lei Bai",
            "Kaijun Ren"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Atmospheric and Oceanic Physics (physics.ao-ph)",
        "abstract": "Recent advancements in Artificial Intelligence (AI) demonstrate significant potential to revolutionize weather forecasting. However, most AI-driven models rely on Numerical Weather Prediction (NWP) systems for initial condition preparation, which often consumes hours on supercomputers. Here we introduce XiChen, the first observation-scalable fully AI-driven global weather forecasting system, whose entire pipeline, from Data Assimilation (DA) to medium-range forecasting, can be accomplished within only 17 seconds. XiChen is built upon a foundation model that is pre-trained for weather forecasting. Meanwhile, this model is subsequently fine-tuned to serve as both observation operators and DA models, thereby scalably assimilating conventional and raw satellite observations. Furthermore, the integration of four-dimensional variational knowledge ensures that XiChen's DA and medium-range forecasting accuracy rivals that of operational NWP systems, amazingly achieving a skillful forecasting lead time exceeding 8.25 days. These findings demonstrate that XiChen holds strong potential toward fully AI-driven weather forecasting independent of NWP systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 111,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09227",
        "abs_url": "https://arxiv.org/abs/2507.09227",
        "pdf_url": "https://arxiv.org/pdf/2507.09227",
        "title": "PanoDiff-SR: Synthesizing Dental Panoramic Radiographs using Diffusion and Super-resolution",
        "authors": [
            "Sanyam Jain",
            "Bruna Neves de Freitas",
            "Andreas Basse-OConnor",
            "Alexandros Iosifidis",
            "Ruben Pauwels"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "There has been increasing interest in the generation of high-quality, realistic synthetic medical images in recent years. Such synthetic datasets can mitigate the scarcity of public datasets for artificial intelligence research, and can also be used for educational purposes. In this paper, we propose a combination of diffusion-based generation (PanoDiff) and Super-Resolution (SR) for generating synthetic dental panoramic radiographs (PRs). The former generates a low-resolution (LR) seed of a PR (256 X 128) which is then processed by the SR model to yield a high-resolution (HR) PR of size 1024 X 512. For SR, we propose a state-of-the-art transformer that learns local-global relationships, resulting in sharper edges and textures. Experimental results demonstrate a Frechet inception distance score of 40.69 between 7243 real and synthetic images (in HR). Inception scores were 2.55, 2.30, 2.90 and 2.98 for real HR, synthetic HR, real LR and synthetic LR images, respectively. Among a diverse group of six clinical experts, all evaluating a mixture of 100 synthetic and 100 real PRs in a time-limited observation, the average accuracy in distinguishing real from synthetic images was 68.5% (with 50% corresponding to random guessing).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 112,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09248",
        "abs_url": "https://arxiv.org/abs/2507.09248",
        "pdf_url": "https://arxiv.org/pdf/2507.09248",
        "title": "AGCD-Net: Attention Guided Context Debiasing Network for Emotion Recognition",
        "authors": [
            "Varsha Devi",
            "Amine Bohi",
            "Pardeep Kumar"
        ],
        "comments": "13 Pages, 4 figures, 2 tables ICIAP 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Context-aware emotion recognition (CAER) enhances affective computing in real-world scenarios, but traditional methods often suffer from context bias-spurious correlation between background context and emotion labels (e.g. associating ``garden'' with ``happy''). In this paper, we propose \\textbf{AGCD-Net}, an Attention Guided Context Debiasing model that introduces \\textit{Hybrid ConvNeXt}, a novel convolutional encoder that extends the ConvNeXt backbone by integrating Spatial Transformer Network and Squeeze-and-Excitation layers for enhanced feature recalibration. At the core of AGCD-Net is the Attention Guided - Causal Intervention Module (AG-CIM), which applies causal theory, perturbs context features, isolates spurious correlations, and performs an attention-driven correction guided by face features to mitigate context bias. Experimental results on the CAER-S dataset demonstrate the effectiveness of AGCD-Net, achieving state-of-the-art performance and highlighting the importance of causal debiasing for robust emotion recognition in complex settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 113,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09264",
        "abs_url": "https://arxiv.org/abs/2507.09264",
        "pdf_url": "https://arxiv.org/pdf/2507.09264",
        "title": "Controllable Patching for Compute-Adaptive Surrogate Modeling of Partial Differential Equations",
        "authors": [
            "Payel Mukhopadhyay",
            "Michael McCabe",
            "Ruben Ohana",
            "Miles Cranmer"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Image and Video Processing (eess.IV)",
        "abstract": "Patch-based transformer surrogates have become increasingly effective for modeling spatiotemporal dynamics, but the fixed patch size is a major limitation for budget-conscience deployment in production. We introduce two lightweight, architecture-agnostic modules-the Convolutional Kernel Modulator (CKM) and Convolutional Stride Modulator (CSM)-that enable dynamic patch size control at inference in patch based models, without retraining or accuracy loss. Combined with a cyclic patch-size rollout, our method mitigates patch artifacts and improves long-term stability for video-like prediction tasks. Applied to a range of challenging 2D and 3D PDE benchmarks, our approach improves rollout fidelity and runtime efficiency. To our knowledge, this is the first framework to enable inference-time patch-size tunability in patch-based PDE surrogates. Its plug-and-play design makes it broadly applicable across architectures-establishing a general foundation for compute-adaptive modeling in PDE surrogate tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 114,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09269",
        "abs_url": "https://arxiv.org/abs/2507.09269",
        "pdf_url": "https://arxiv.org/pdf/2507.09269",
        "title": "Cross Knowledge Distillation between Artificial and Spiking Neural Networks",
        "authors": [
            "Shuhan Ye",
            "Yuanbin Qian",
            "Chong Wang",
            "Sunqi Lin",
            "Jiazhen Xu",
            "Jiangbo Qian",
            "Yuqi Li"
        ],
        "comments": "This paper has been accepted by ICME2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Recently, Spiking Neural Networks (SNNs) have demonstrated rich potential in computer vision domain due to their high biological plausibility, event-driven characteristic and energy-saving efficiency. Still, limited annotated event-based datasets and immature SNN architectures result in their performance inferior to that of Artificial Neural Networks (ANNs). To enhance the performance of SNNs on their optimal data format, DVS data, we explore using RGB data and well-performing ANNs to implement knowledge distillation. In this case, solving cross-modality and cross-architecture challenges is necessary. In this paper, we propose cross knowledge distillation (CKD), which not only leverages semantic similarity and sliding replacement to mitigate the cross-modality challenge, but also uses an indirect phased knowledge distillation to mitigate the cross-architecture challenge. We validated our method on main-stream neuromorphic datasets, including N-Caltech101 and CEP-DVS. The experimental results show that our method outperforms current State-of-the-Art methods. The code will be available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 115,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09279",
        "abs_url": "https://arxiv.org/abs/2507.09279",
        "pdf_url": "https://arxiv.org/pdf/2507.09279",
        "title": "Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models",
        "authors": [
            "Anita Kriz",
            "Elizabeth Laura Janes",
            "Xing Shen",
            "Tal Arbel"
        ],
        "comments": "Accepted to ICCV 2025 Workshop CVAMD",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Multimodal large language models (MLLMs) hold considerable promise for applications in healthcare. However, their deployment in safety-critical settings is hindered by two key limitations: (i) sensitivity to prompt design, and (ii) a tendency to generate incorrect responses with high confidence. As clinicians may rely on a model's stated confidence to gauge the reliability of its predictions, it is especially important that when a model expresses high confidence, it is also highly accurate. We introduce Prompt4Trust, the first reinforcement learning (RL) framework for prompt augmentation targeting confidence calibration in MLLMs. A lightweight LLM is trained to produce context-aware auxiliary prompts that guide a downstream task MLLM to generate responses in which the expressed confidence more accurately reflects predictive accuracy. Unlike conventional calibration techniques, Prompt4Trust specifically prioritizes aspects of calibration most critical for safe and trustworthy clinical decision-making. Beyond improvements driven by this clinically motivated calibration objective, our proposed method also improves task accuracy, achieving state-of-the-art medical visual question answering (VQA) performance on the PMC-VQA benchmark, which is composed of multiple-choice questions spanning diverse medical imaging modalities. Moreover, our framework trained with a small downstream task MLLM showed promising zero-shot generalization to larger MLLMs in our experiments, suggesting the potential for scalable calibration without the associated computational costs. This work demonstrates the potential of automated yet human-aligned prompt engineering for improving the the trustworthiness of MLLMs in safety critical settings. Our codebase can be found at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 116,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09299",
        "abs_url": "https://arxiv.org/abs/2507.09299",
        "pdf_url": "https://arxiv.org/pdf/2507.09299",
        "title": "ViT-ProtoNet for Few-Shot Image Classification: A Multi-Benchmark Evaluation",
        "authors": [
            "Abdulvahap Mutlu",
            "Şengül Doğan",
            "Türker Tuncer"
        ],
        "comments": "All codes are available at this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "The remarkable representational power of Vision Transformers (ViTs) remains underutilized in few-shot image classification. In this work, we introduce ViT-ProtoNet, which integrates a ViT-Small backbone into the Prototypical Network framework. By averaging class conditional token embeddings from a handful of support examples, ViT-ProtoNet constructs robust prototypes that generalize to novel categories under 5-shot settings. We conduct an extensive empirical evaluation on four standard benchmarks: Mini-ImageNet, FC100, CUB-200, and CIFAR-FS, including overlapped support variants to assess robustness. Across all splits, ViT-ProtoNet consistently outperforms CNN-based prototypical counterparts, achieving up to a 3.2\\% improvement in 5-shot accuracy and demonstrating superior feature separability in latent space. Furthermore, it outperforms or is competitive with transformer-based competitors using a more lightweight backbone. Comprehensive ablations examine the impact of transformer depth, patch size, and fine-tuning strategy. To foster reproducibility, we release code and pretrained weights. Our results establish ViT-ProtoNet as a powerful, flexible approach for few-shot classification and set a new baseline for transformer-based meta-learners.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 117,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09308",
        "abs_url": "https://arxiv.org/abs/2507.09308",
        "pdf_url": "https://arxiv.org/pdf/2507.09308",
        "title": "AlphaVAE: Unified End-to-End RGBA Image Reconstruction and Generation with Alpha-Aware Representation Learning",
        "authors": [
            "Zile Wang",
            "Hao Yu",
            "Jiabo Zhan",
            "Chun Yuan"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Recent advances in latent diffusion models have achieved remarkable results in high-fidelity RGB image synthesis by leveraging pretrained VAEs to compress and reconstruct pixel data at low computational cost. However, the generation of transparent or layered content (RGBA image) remains largely unexplored, due to the lack of large-scale benchmarks. In this work, we propose ALPHA, the first comprehensive RGBA benchmark that adapts standard RGB metrics to four-channel images via alpha blending over canonical backgrounds. We further introduce ALPHAVAE, a unified end-to-end RGBA VAE that extends a pretrained RGB VAE by incorporating a dedicated alpha channel. The model is trained with a composite objective that combines alpha-blended pixel reconstruction, patch-level fidelity, perceptual consistency, and dual KL divergence constraints to ensure latent fidelity across both RGB and alpha representations. Our RGBA VAE, trained on only 8K images in contrast to 1M used by prior methods, achieves a +4.9 dB improvement in PSNR and a +3.2% increase in SSIM over LayerDiffuse in reconstruction. It also enables superior transparent image generation when fine-tuned within a latent diffusion framework. Our code, data, and models are released on this https URL for reproducibility.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 118,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09315",
        "abs_url": "https://arxiv.org/abs/2507.09315",
        "pdf_url": "https://arxiv.org/pdf/2507.09315",
        "title": "Enhancing Interpretability in Software Change Management with Chain-of-Thought Reasoning",
        "authors": [
            "Yongqian Sun",
            "Weihua Kuang",
            "Chao Shen",
            "Xidao Wen",
            "Tinghua Zheng",
            "Heng Liu",
            "Shenglin Zhang",
            "Bo Wu",
            "Dan Pei"
        ],
        "comments": "22 pages, 19 figures",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)",
        "abstract": "In modern online services, frequent software changes introduce significant risks. To tackle this challenge, we propose SCELM (Software Change Evaluation and Lifecycle Management), an end-to-end automated framework for software change management. SCELM aims to manage software changes efficiently and precisely, significantly reducing service failures and economic losses.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 119,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09347",
        "abs_url": "https://arxiv.org/abs/2507.09347",
        "pdf_url": "https://arxiv.org/pdf/2507.09347",
        "title": "A Framework for Predictive Directional Trading Based on Volatility and Causal Inference",
        "authors": [
            "Ivan Letteri"
        ],
        "comments": "",
        "subjects": "Statistical Finance (q-fin.ST); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "Purpose: This study introduces a novel framework for identifying and exploiting predictive lead-lag relationships in financial markets. We propose an integrated approach that combines advanced statistical methodologies with machine learning models to enhance the identification and exploitation of predictive relationships between equities. Methods: We employed a Gaussian Mixture Model (GMM) to cluster nine prominent stocks based on their mid-range historical volatility profiles over a three-year period. From the resulting clusters, we constructed a multi-stage causal inference pipeline, incorporating the Granger Causality Test (GCT), a customised Peter-Clark Momentary Conditional Independence (PCMCI) test, and Effective Transfer Entropy (ETE) to identify robust, predictive linkages. Subsequently, Dynamic Time Warping (DTW) and a K-Nearest Neighbours (KNN) classifier were utilised to determine the optimal time lag for trade execution. The resulting strategy was rigorously backtested. Results: The proposed volatility-based trading strategy, tested from 8 June 2023 to 12 August 2023, demonstrated substantial efficacy. The portfolio yielded a total return of 15.38%, significantly outperforming the 10.39% return of a comparative Buy-and-Hold strategy. Key performance metrics, including a Sharpe Ratio up to 2.17 and a win rate up to 100% for certain pairs, confirmed the strategy's viability. Conclusion: This research contributes a systematic and robust methodology for identifying profitable trading opportunities derived from volatility-based causal relationships. The findings have significant implications for both academic research in financial modelling and the practical application of algorithmic trading, offering a structured approach to developing resilient, data-driven strategies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 120,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09353",
        "abs_url": "https://arxiv.org/abs/2507.09353",
        "pdf_url": "https://arxiv.org/pdf/2507.09353",
        "title": "Impute With Confidence: A Framework for Uncertainty Aware Multivariate Time Series Imputation",
        "authors": [
            "Addison Weatherhead",
            "Anna Goldenberg"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "Time series data with missing values is common across many domains. Healthcare presents special challenges due to prolonged periods of sensor disconnection. In such cases, having a confidence measure for imputed values is critical. Most existing methods either overlook model uncertainty or lack mechanisms to estimate it. To address this gap, we introduce a general framework that quantifies and leverages uncertainty for selective imputation. By focusing on values the model is most confident in, highly unreliable imputations are avoided. Our experiments on multiple EHR datasets, covering diverse types of missingness, demonstrate that selectively imputing less-uncertain values not only reduces imputation errors but also improves downstream tasks. Specifically, we show performance gains in a 24-hour mortality prediction task, underscoring the practical benefit of incorporating uncertainty into time series imputation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 121,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09378",
        "abs_url": "https://arxiv.org/abs/2507.09378",
        "pdf_url": "https://arxiv.org/pdf/2507.09378",
        "title": "Context-Aware Regularization with Markovian Integration for Attention-Based Nucleotide Analysis",
        "authors": [
            "Mohammadsaleh Refahi",
            "Mahdi Abavisani",
            "Bahrad A. Sokhansanj",
            "James R. Brown",
            "Gail Rosen"
        ],
        "comments": "",
        "subjects": "Genomics (q-bio.GN); Artificial Intelligence (cs.AI)",
        "abstract": "Transformers have revolutionized nucleotide sequence analysis, yet capturing long-range dependencies remains challenging. Recent studies show that autoregressive transformers often exhibit Markovian behavior by relying on fixed-length context windows for next-token prediction. However, standard self-attention mechanisms are computationally inefficient for long sequences due to their quadratic complexity and do not explicitly enforce global transition consistency. We introduce CARMANIA (Context-Aware Regularization with Markovian Integration for Attention-Based Nucleotide Analysis), a self-supervised pretraining framework that augments next-token (NT) prediction with a transition-matrix (TM) loss. The TM loss aligns predicted token transitions with empirically derived n-gram statistics from each input sequence, encouraging the model to capture higher-order dependencies beyond local context. This integration enables CARMANIA to learn organism-specific sequence structures that reflect both evolutionary constraints and functional organization. We evaluate CARMANIA across diverse genomic tasks, including regulatory element prediction, functional gene classification, taxonomic inference, antimicrobial resistance detection, and biosynthetic gene cluster classification. CARMANIA outperforms the previous best long-context model by at least 7 percent, matches state-of-the-art on shorter sequences (exceeding prior results on 20 out of 40 tasks while running approximately 2.5 times faster), and shows particularly strong improvements on enhancer and housekeeping gene classification tasks, including up to a 34 percent absolute gain in Matthews correlation coefficient (MCC) for enhancer prediction. The TM loss boosts accuracy in 33 of 40 tasks, especially where local motifs or regulatory patterns drive prediction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 122,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09382",
        "abs_url": "https://arxiv.org/abs/2507.09382",
        "pdf_url": "https://arxiv.org/pdf/2507.09382",
        "title": "Fair CCA for Fair Representation Learning: An ADNI Study",
        "authors": [
            "Bojian Hou",
            "Zhanliang Wang",
            "Zhuoping Zhou",
            "Boning Tong",
            "Zexuan Wang",
            "Jingxuan Bao",
            "Duy Duong-Tran",
            "Qi Long",
            "Li Shen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computers and Society (cs.CY)",
        "abstract": "Canonical correlation analysis (CCA) is a technique for finding correlations between different data modalities and learning low-dimensional representations. As fairness becomes crucial in machine learning, fair CCA has gained attention. However, previous approaches often overlook the impact on downstream classification tasks, limiting applicability. We propose a novel fair CCA method for fair representation learning, ensuring the projected features are independent of sensitive attributes, thus enhancing fairness without compromising accuracy. We validate our method on synthetic data and real-world data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), demonstrating its ability to maintain high correlation analysis performance while improving fairness in classification tasks. Our work enables fair machine learning in neuroimaging studies where unbiased analysis is essential.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 123,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09406",
        "abs_url": "https://arxiv.org/abs/2507.09406",
        "pdf_url": "https://arxiv.org/pdf/2507.09406",
        "title": "Adversarial Activation Patching: A Framework for Detecting and Mitigating Emergent Deception in Safety-Aligned Transformers",
        "authors": [
            "Santhosh Kumar Ravindran"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Large language models (LLMs) aligned for safety through techniques like reinforcement learning from human feedback (RLHF) often exhibit emergent deceptive behaviors, where outputs appear compliant but subtly mislead or omit critical information. This paper introduces adversarial activation patching, a novel mechanistic interpretability framework that leverages activation patching as an adversarial tool to induce, detect, and mitigate such deception in transformer-based models. By sourcing activations from \"deceptive\" prompts and patching them into safe forward passes at specific layers, we simulate vulnerabilities and quantify deception rates. Through toy neural network simulations across multiple scenarios (e.g., 1000 trials per setup), we demonstrate that adversarial patching increases deceptive outputs to 23.9% from a 0% baseline, with layer-specific variations supporting our hypotheses. We propose six hypotheses, including transferability across models, exacerbation in multimodal settings, and scaling effects. An expanded literature review synthesizes over 20 key works in interpretability, deception, and adversarial attacks. Mitigation strategies, such as activation anomaly detection and robust fine-tuning, are detailed, alongside ethical considerations and future research directions. This work advances AI safety by highlighting patching's dual-use potential and provides a roadmap for empirical studies on large-scale models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 124,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09420",
        "abs_url": "https://arxiv.org/abs/2507.09420",
        "pdf_url": "https://arxiv.org/pdf/2507.09420",
        "title": "Domain Adaptation and Multi-view Attention for Learnable Landmark Tracking with Sparse Data",
        "authors": [
            "Timothy Chase Jr",
            "Karthik Dantu"
        ],
        "comments": "Presented at the RSS Space Robotics Workshop 2025. Poster available online at this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)",
        "abstract": "The detection and tracking of celestial surface terrain features are crucial for autonomous spaceflight applications, including Terrain Relative Navigation (TRN), Entry, Descent, and Landing (EDL), hazard analysis, and scientific data collection. Traditional photoclinometry-based pipelines often rely on extensive a priori imaging and offline processing, constrained by the computational limitations of radiation-hardened systems. While historically effective, these approaches typically increase mission costs and duration, operate at low processing rates, and have limited generalization. Recently, learning-based computer vision has gained popularity to enhance spacecraft autonomy and overcome these limitations. While promising, emerging techniques frequently impose computational demands exceeding the capabilities of typical spacecraft hardware for real-time operation and are further challenged by the scarcity of labeled training data for diverse extraterrestrial environments. In this work, we present novel formulations for in-situ landmark tracking via detection and description. We utilize lightweight, computationally efficient neural network architectures designed for real-time execution on current-generation spacecraft flight processors. For landmark detection, we propose improved domain adaptation methods that enable the identification of celestial terrain features with distinct, cheaply acquired training data. Concurrently, for landmark description, we introduce a novel attention alignment formulation that learns robust feature representations that maintain correspondence despite significant landmark viewpoint variations. Together, these contributions form a unified system for landmark tracking that demonstrates superior performance compared to existing state-of-the-art techniques.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 125,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09439",
        "abs_url": "https://arxiv.org/abs/2507.09439",
        "pdf_url": "https://arxiv.org/pdf/2507.09439",
        "title": "Dynamic Sparse Causal-Attention Temporal Networks for Interpretable Causality Discovery in Multivariate Time Series",
        "authors": [
            "Meriem Zerkouk",
            "Miloud Mihoubi",
            "Belkacem Chikhaoui"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)",
        "abstract": "Understanding causal relationships in multivariate time series (MTS) is essential for effective decision-making in fields such as finance and marketing, where complex dependencies and lagged effects challenge conventional analytical approaches. We introduce Dynamic Sparse Causal-Attention Temporal Networks for Interpretable Causality Discovery in MTS (DyCAST-Net), a novel architecture designed to enhance causal discovery by integrating dilated temporal convolutions and dynamic sparse attention mechanisms. DyCAST-Net effectively captures multiscale temporal dependencies through dilated convolutions while leveraging an adaptive thresholding strategy in its attention mechanism to eliminate spurious connections, ensuring both accuracy and interpretability. A statistical shuffle test validation further strengthens robustness by filtering false positives and improving causal inference reliability. Extensive evaluations on financial and marketing datasets demonstrate that DyCAST-Net consistently outperforms existing models such as TCDF, GCFormer, and CausalFormer. The model provides a more precise estimation of causal delays and significantly reduces false discoveries, particularly in noisy environments. Moreover, attention heatmaps offer interpretable insights, uncovering hidden causal patterns such as the mediated effects of advertising on consumer behavior and the influence of macroeconomic indicators on financial markets. Case studies illustrate DyCAST-Net's ability to detect latent mediators and lagged causal factors, making it particularly effective in high-dimensional, dynamic settings. The model's architecture enhanced by RMSNorm stabilization and causal masking ensures scalability and adaptability across diverse application domains",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 126,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09440",
        "abs_url": "https://arxiv.org/abs/2507.09440",
        "pdf_url": "https://arxiv.org/pdf/2507.09440",
        "title": "Transformers Don't In-Context Learn Least Squares Regression",
        "authors": [
            "Joshua Hill",
            "Benjamin Eyre",
            "Elliot Creager"
        ],
        "comments": "21 pages, 16 figures, ICML 2025 Workshop on Reliable and Responsible Foundation Models",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "In-context learning (ICL) has emerged as a powerful capability of large pretrained transformers, enabling them to solve new tasks implicit in example input-output pairs without any gradient updates. Despite its practical success, the mechanisms underlying ICL remain largely mysterious. In this work we study synthetic linear regression to probe how transformers implement learning at inference time. Previous works have demonstrated that transformers match the performance of learning rules such as Ordinary Least Squares (OLS) regression or gradient descent and have suggested ICL is facilitated in transformers through the learned implementation of one of these techniques. In this work, we demonstrate through a suite of out-of-distribution generalization experiments that transformers trained for ICL fail to generalize after shifts in the prompt distribution, a behaviour that is inconsistent with the notion of transformers implementing algorithms such as OLS. Finally, we highlight the role of the pretraining corpus in shaping ICL behaviour through a spectral analysis of the learned representations in the residual stream. Inputs from the same distribution as the training data produce representations with a unique spectral signature: inputs from this distribution tend to have the same top two singular vectors. This spectral signature is not shared by out-of-distribution inputs, and a metric characterizing the presence of this signature is highly correlated with low loss.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 127,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09445",
        "abs_url": "https://arxiv.org/abs/2507.09445",
        "pdf_url": "https://arxiv.org/pdf/2507.09445",
        "title": "Fourier Basis Mapping: A Time-Frequency Learning Framework for Time Series Forecasting",
        "authors": [
            "Runze Yang",
            "Longbing Cao",
            "Xin You",
            "Kun Fang",
            "Jianxun Li",
            "Jie Yang"
        ],
        "comments": "18 pages, 6 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "The integration of Fourier transform and deep learning opens new avenues for time series forecasting. We reconsider the Fourier transform from a basis functions perspective. Specifically, the real and imaginary parts of the frequency components can be regarded as the coefficients of cosine and sine basis functions at tiered frequency levels, respectively. We find that existing Fourier-based methods face inconsistent starting cycles and inconsistent series length issues. They fail to interpret frequency components precisely and overlook temporal information. Accordingly, the novel Fourier Basis Mapping (FBM) method addresses these issues by integrating time-frequency features through Fourier basis expansion and mapping in the time-frequency space. Our approach extracts explicit frequency features while preserving temporal characteristics. FBM supports plug-and-play integration with various types of neural networks by only adjusting the first initial projection layer for better performance. First, we propose FBM-L, FBM-NL, and FBM-NP to enhance linear, MLP-based, and Transformer-based models, respectively, demonstrating the effectiveness of time-frequency features. Next, we propose a synergetic model architecture, termed FBM-S, which decomposes the seasonal, trend, and interaction effects into three separate blocks, each designed to model time-frequency features in a specialized manner. Finally, we introduce several techniques tailored for time-frequency features, including interaction masking, centralization, patching, rolling window projection, and multi-scale down-sampling. The results are validated on diverse real-world datasets for both long-term and short-term forecasting tasks with SOTA performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 128,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09460",
        "abs_url": "https://arxiv.org/abs/2507.09460",
        "pdf_url": "https://arxiv.org/pdf/2507.09460",
        "title": "Enhancing ALS Progression Tracking with Semi-Supervised ALSFRS-R Scores Estimated from Ambient Home Health Monitoring",
        "authors": [
            "Noah Marchal",
            "William E. Janes",
            "Mihail Popescu",
            "Xing Song"
        ],
        "comments": "31 pages, 8 Figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Clinical monitoring of functional decline in ALS relies on periodic assessments that may miss critical changes occurring between visits. To address this gap, semi-supervised regression models were developed to estimate rates of decline in a case series cohort by targeting ALSFRS- R scale trajectories with continuous in-home sensor monitoring data. Our analysis compared three model paradigms (individual batch learning and cohort-level batch versus incremental fine-tuned transfer learning) across linear slope, cubic polynomial, and ensembled self-attention pseudo-label interpolations. Results revealed cohort homogeneity across functional domains responding to learning methods, with transfer learning improving prediction error for ALSFRS-R subscales in 28 of 32 contrasts (mean RMSE=0.20(0.04)), and individual batch learning for predicting the composite scale (mean RMSE=3.15(1.25)) in 2 of 3. Self-attention interpolation achieved the lowest prediction error for subscale-level models (mean RMSE=0.19(0.06)), capturing complex nonlinear progression patterns, outperforming linear and cubic interpolations in 20 of 32 contrasts, though linear interpolation proved more stable in all ALSFRS-R composite scale models (mean RMSE=0.23(0.10)). We identified distinct homogeneity-heterogeneity profiles across functional domains with respiratory and speech exhibiting patient-specific patterns benefiting from personalized incremental adaptation, while swallowing and dressing functions followed cohort-level trajectories suitable for transfer models. These findings suggest that matching learning and pseudo-labeling techniques to functional domain-specific homogeneity-heterogeneity profiles enhances predictive accuracy in ALS progression tracking. Integrating adaptive model selection within sensor monitoring platforms could enable timely interventions and scalable deployment in future multi-center studies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 129,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09470",
        "abs_url": "https://arxiv.org/abs/2507.09470",
        "pdf_url": "https://arxiv.org/pdf/2507.09470",
        "title": "Enhancing Clinical Text Classification via Fine-Tuned DRAGON Longformer Models",
        "authors": [
            "Mingchuan Yang",
            "Ziyuan Huang"
        ],
        "comments": "29 pages, 5 tables",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "This study explores the optimization of the DRAGON Longformer base model for clinical text classification, specifically targeting the binary classification of medical case descriptions. A dataset of 500 clinical cases containing structured medical observations was used, with 400 cases for training and 100 for validation. Enhancements to the pre-trained joeranbosma/dragon-longformer-base-mixed-domain model included hyperparameter tuning, domain-specific preprocessing, and architectural adjustments. Key modifications involved increasing sequence length from 512 to 1024 tokens, adjusting learning rates from 1e-05 to 5e-06, extending training epochs from 5 to 8, and incorporating specialized medical terminology. The optimized model achieved notable performance gains: accuracy improved from 72.0% to 85.2%, precision from 68.0% to 84.1%, recall from 75.0% to 86.3%, and F1-score from 71.0% to 85.2%. Statistical analysis confirmed the significance of these improvements (p < .001). The model demonstrated enhanced capability in interpreting medical terminology, anatomical measurements, and clinical observations. These findings contribute to domain-specific language model research and offer practical implications for clinical natural language processing applications. The optimized model's strong performance across diverse medical conditions underscores its potential for broad use in healthcare settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 130,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09477",
        "abs_url": "https://arxiv.org/abs/2507.09477",
        "pdf_url": "https://arxiv.org/pdf/2507.09477",
        "title": "Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs",
        "authors": [
            "Yangning Li",
            "Weizhi Zhang",
            "Yuyao Yang",
            "Wei-Chieh Huang",
            "Yaozu Wu",
            "Junyu Luo",
            "Yuanchen Bei",
            "Henry Peng Zou",
            "Xiao Luo",
            "Yusheng Zhao",
            "Chunkit Chan",
            "Yankai Chen",
            "Zhongfen Deng",
            "Yinghui Li",
            "Hai-Tao Zheng",
            "Dongyuan Li",
            "Renhe Jiang",
            "Ming Zhang",
            "Yangqiu Song",
            "Philip S. Yu"
        ],
        "comments": "submitted to ARR May",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language Models (LLMs) by injecting external knowledge, yet it falls short on problems that demand multi-step inference; conversely, purely reasoning-oriented approaches often hallucinate or mis-ground facts. This survey synthesizes both strands under a unified reasoning-retrieval perspective. We first map how advanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then, we show how retrieved knowledge of different type supply missing premises and expand context for complex inference (RAG-Enhanced Reasoning). Finally, we spotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs iteratively interleave search and reasoning to achieve state-of-the-art performance across knowledge-intensive benchmarks. We categorize methods, datasets, and open challenges, and outline research avenues toward deeper RAG-Reasoning systems that are more effective, multimodally-adaptive, trustworthy, and human-centric. The collection is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 131,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09481",
        "abs_url": "https://arxiv.org/abs/2507.09481",
        "pdf_url": "https://arxiv.org/pdf/2507.09481",
        "title": "Evaluating LLMs on Sequential API Call Through Automated Test Generation",
        "authors": [
            "Yuheng Huang",
            "Da Song",
            "Zhenlan Ji",
            "Shuai Wang",
            "Lei Ma"
        ],
        "comments": "",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "By integrating tools from external APIs, Large Language Models (LLMs) have expanded their promising capabilities in a diverse spectrum of complex real-world tasks. However, testing, evaluation, and analysis of LLM tool use remain in their early stages. Most existing benchmarks rely on manually collected test cases, many of which cannot be automatically checked for semantic correctness and instead depend on static methods such as string matching. Additionally, these benchmarks often overlook the complex interactions that occur between sequential API calls, which are common in real-world applications. To fill the gap, in this paper, we introduce StateGen, an automated framework designed to generate diverse coding tasks involving sequential API interactions. StateGen combines state-machine-based API constraint solving and validation, energy-based sampling, and control-flow injection to generate executable programs. These programs are then translated into human-like natural language task descriptions through a collaboration of two LLM agents. Utilizing StateGen, we construct StateEval, a benchmark encompassing 120 verified test cases spanning across three representative scenarios: Session Service, Tensor Operation, and ElevenLabs MCP. Experimental results confirm that StateGen can effectively generate challenging and realistic API-oriented tasks, highlighting areas for improvement in current LLMs incorporating APIs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 132,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09482",
        "abs_url": "https://arxiv.org/abs/2507.09482",
        "pdf_url": "https://arxiv.org/pdf/2507.09482",
        "title": "ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning",
        "authors": [
            "Changli Wang",
            "Rui Wu",
            "Fang Yin"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)",
        "abstract": "Human emotions are complex, with sarcasm being a subtle and distinctive form. Despite progress in sarcasm research, sarcasm generation remains underexplored, primarily due to the overreliance on textual modalities and the neglect of visual cues, as well as the mismatch between image content and sarcastic intent in existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm generation dataset with 4,970 samples, each containing an image, a sarcastic text, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation framework that integrates Proximal Policy Optimization (PPO) and contrastive learning. PPO utilizes reward scores from DIP to steer the generation of sarcastic texts, while contrastive learning encourages the model to favor outputs with higher reward scores. These strategies improve overall generation quality and produce texts with more pronounced sarcastic intent. We evaluate ViSP across five metric sets and find it surpasses all baselines, including large language models, underscoring their limitations in sarcasm generation. Furthermore, we analyze the distributions of Sarcasm Scores and Factual Incongruity for both M2SaG and the texts generated by ViSP. The generated texts exhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity (0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic content than the original dataset. % The dataset and code will be publicly available. Our dataset and code will be released at \\textit{this https URL}.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 133,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09487",
        "abs_url": "https://arxiv.org/abs/2507.09487",
        "pdf_url": "https://arxiv.org/pdf/2507.09487",
        "title": "HMID-Net: An Exploration of Masked Image Modeling and Knowledge Distillation in Hyperbolic Space",
        "authors": [
            "Changli Wang",
            "Fang Yin",
            "Jiafeng Liu",
            "Rui Wu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Visual and semantic concepts are often structured in a hierarchical manner. For instance, textual concept `cat' entails all images of cats. A recent study, MERU, successfully adapts multimodal learning techniques from Euclidean space to hyperbolic space, effectively capturing the visual-semantic hierarchy. However, a critical question remains: how can we more efficiently train a model to capture and leverage this hierarchy? In this paper, we propose the \\textit{Hyperbolic Masked Image and Distillation Network} (HMID-Net), a novel and efficient method that integrates Masked Image Modeling (MIM) and knowledge distillation techniques within hyperbolic space. To the best of our knowledge, this is the first approach to leverage MIM and knowledge distillation in hyperbolic space to train highly efficient models. In addition, we introduce a distillation loss function specifically designed to facilitate effective knowledge transfer in hyperbolic space. Our experiments demonstrate that MIM and knowledge distillation techniques in hyperbolic space can achieve the same remarkable success as in Euclidean space. Extensive evaluations show that our method excels across a wide range of downstream tasks, significantly outperforming existing models like MERU and CLIP in both image classification and retrieval.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 134,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09492",
        "abs_url": "https://arxiv.org/abs/2507.09492",
        "pdf_url": "https://arxiv.org/pdf/2507.09492",
        "title": "SDTN and TRN: Adaptive Spectral-Spatial Feature Extraction for Hyperspectral Image Classification",
        "authors": [
            "Fuyin Ye",
            "Erwen Yao",
            "Jianyong Chen",
            "Fengmei He",
            "Junxiang Zhang",
            "Lihao Ni"
        ],
        "comments": "4 pages, 2 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Hyperspectral image classification plays a pivotal role in precision agriculture, providing accurate insights into crop health monitoring, disease detection, and soil analysis. However, traditional methods struggle with high-dimensional data, spectral-spatial redundancy, and the scarcity of labeled samples, often leading to suboptimal performance. To address these challenges, we propose the Self-Adaptive Tensor- Regularized Network (SDTN), which combines tensor decomposition with regularization mechanisms to dynamically adjust tensor ranks, ensuring optimal feature representation tailored to the complexity of the data. Building upon SDTN, we propose the Tensor-Regularized Network (TRN), which integrates the features extracted by SDTN into a lightweight network capable of capturing spectral-spatial features at multiple scales. This approach not only maintains high classification accuracy but also significantly reduces computational complexity, making the framework highly suitable for real-time deployment in resource-constrained environments. Experiments on PaviaU datasets demonstrate significant improvements in accuracy and reduced model parameters compared to state-of-the-art methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 135,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09508",
        "abs_url": "https://arxiv.org/abs/2507.09508",
        "pdf_url": "https://arxiv.org/pdf/2507.09508",
        "title": "A Mixture of Linear Corrections Generates Secure Code",
        "authors": [
            "Weichen Yu",
            "Ravi Mangal",
            "Terry Zhuo",
            "Matt Fredrikson",
            "Corina S. Pasareanu"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
        "abstract": "Large language models (LLMs) have become proficient at sophisticated code-generation tasks, yet remain ineffective at reliably detecting or avoiding code vulnerabilities. Does this deficiency stem from insufficient learning about code vulnerabilities, or is it merely a result of ineffective prompting? Using representation engineering techniques, we investigate whether LLMs internally encode the concepts necessary to identify code vulnerabilities. We find that current LLMs encode precise internal representations that distinguish vulnerable from secure code--achieving greater accuracy than standard prompting approaches. Leveraging these vulnerability-sensitive representations, we develop an inference-time steering technique that subtly modulates the model's token-generation probabilities through a mixture of corrections (MoC). Our method effectively guides LLMs to produce less vulnerable code without compromising functionality, demonstrating a practical approach to controlled vulnerability management in generated code. Notably, MoC enhances the security ratio of Qwen2.5-Coder-7B by 8.9\\%, while simultaneously improving functionality on HumanEval pass@1 by 2.1\\%.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 136,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09514",
        "abs_url": "https://arxiv.org/abs/2507.09514",
        "pdf_url": "https://arxiv.org/pdf/2507.09514",
        "title": "QuarterMap: Efficient Post-Training Token Pruning for Visual State Space Models",
        "authors": [
            "Tien-Yu Chi",
            "Hung-Yueh Chiang",
            "Diana Marculescu",
            "Kai-Chiang Wu"
        ],
        "comments": "Accepted by Efficient Systems for Foundation Models Workshop at the International Conference on Machine Learning (ICML) 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "State space models (SSMs) reduce the quadratic complexity of transformers by leveraging linear recurrence. Recently, VMamba has emerged as a strong SSM-based vision backbone, yet remains bottlenecked by spatial redundancy in its four-directional scan. We propose QuarterMap, a post-training activation pruning method that removes redundant spatial activations before scanning and restores dimensions via nearest-neighbor upsampling. Our method improves throughput without retraining. On ImageNet-1K, QuarterMap achieves up to 11% speedup on VMamba with less than 0.9% accuracy drop, and yields similar gains on ADE20K segmentation. Beyond VMamba, we validate QuarterMap on MedMamba, a domain-specific model that shares the same four-directional scanning structure, where it consistently improves throughput while preserving accuracy across multiple medical imaging tasks. Compared to token merging methods like ToMe, QuarterMap is tailored for SSMs and avoids costly merge-unmerge operations. Our method offers a plug-and-play tool for deployment-time efficiency without compromising transferability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 137,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09523",
        "abs_url": "https://arxiv.org/abs/2507.09523",
        "pdf_url": "https://arxiv.org/pdf/2507.09523",
        "title": "An Analysis of Action-Value Temporal-Difference Methods That Learn State Values",
        "authors": [
            "Brett Daley",
            "Prabhat Nagarajan",
            "Martha White",
            "Marlos C. Machado"
        ],
        "comments": "Published at RLC/RLJ 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "The hallmark feature of temporal-difference (TD) learning is bootstrapping: using value predictions to generate new value predictions. The vast majority of TD methods for control learn a policy by bootstrapping from a single action-value function (e.g., Q-learning and Sarsa). Significantly less attention has been given to methods that bootstrap from two asymmetric value functions: i.e., methods that learn state values as an intermediate step in learning action values. Existing algorithms in this vein can be categorized as either QV-learning or AV-learning. Though these algorithms have been investigated to some degree in prior work, it remains unclear if and when it is advantageous to learn two value functions instead of just one -- and whether such approaches are theoretically sound in general. In this paper, we analyze these algorithmic families in terms of convergence and sample efficiency. We find that while both families are more efficient than Expected Sarsa in the prediction setting, only AV-learning methods offer any major benefit over Q-learning in the control setting. Finally, we introduce a new AV-learning algorithm called Regularized Dueling Q-learning (RDQ), which significantly outperforms Dueling DQN in the MinAtar benchmark.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 138,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09531",
        "abs_url": "https://arxiv.org/abs/2507.09531",
        "pdf_url": "https://arxiv.org/pdf/2507.09531",
        "title": "VDInstruct: Zero-Shot Key Information Extraction via Content-Aware Vision Tokenization",
        "authors": [
            "Son Nguyen",
            "Giang Nguyen",
            "Hung Dao",
            "Thao Do",
            "Daeyoung Kim"
        ],
        "comments": "Under Review",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Key Information Extraction (KIE) underpins the understanding of visual documents (e.g., receipts and contracts) by extracting precise semantic content and accurately capturing spatial structure. Yet existing multimodal large language models (MLLMs) often perform poorly on dense documents and rely on vision tokenization approaches that scale with image size, leading to redundant computation and memory inefficiency. To address these challenges, we introduce VDInstruct, an MLLM that separates spatial region detection from semantic feature extraction. Central to our model is a content-aware tokenization strategy: rather than fragmenting the entire image uniformly, it generates tokens in proportion to document complexity, preserving critical structure while eliminating wasted tokens. Leveraging a three-stage training paradigm, our model achieves state-of-the-art (SOTA) results on KIE benchmarks, matching or exceeding the accuracy of leading approaches while reducing the number of image tokens by roughly 3.6x. In zero-shot evaluations, VDInstruct surpasses strong baselines-such as DocOwl 1.5-by +5.5 F1 points, highlighting its robustness to unseen documents. These findings show that content-aware tokenization combined with explicit layout modeling offers a promising direction forward for document understanding. Data, source code, and model weights will be made publicly available.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 139,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09538",
        "abs_url": "https://arxiv.org/abs/2507.09538",
        "pdf_url": "https://arxiv.org/pdf/2507.09538",
        "title": "On the Importance of Neural Membrane Potential Leakage for LIDAR-based Robot Obstacle Avoidance using Spiking Neural Networks",
        "authors": [
            "Zainab Ali",
            "Lujayn Al-Amir",
            "Ali Safa"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
        "abstract": "Using neuromorphic computing for robotics applications has gained much attention in recent year due to the remarkable ability of Spiking Neural Networks (SNNs) for high-precision yet low memory and compute complexity inference when implemented in neuromorphic hardware. This ability makes SNNs well-suited for autonomous robot applications (such as in drones and rovers) where battery resources and payload are typically limited. Within this context, this paper studies the use of SNNs for performing direct robot navigation and obstacle avoidance from LIDAR data. A custom robot platform equipped with a LIDAR is set up for collecting a labeled dataset of LIDAR sensing data together with the human-operated robot control commands used for obstacle avoidance. Crucially, this paper provides what is, to the best of our knowledge, a first focused study about the importance of neuron membrane leakage on the SNN precision when processing LIDAR data for obstacle avoidance. It is shown that by carefully tuning the membrane potential leakage constant of the spiking Leaky Integrate-and-Fire (LIF) neurons used within our SNN, it is possible to achieve on-par robot control precision compared to the use of a non-spiking Convolutional Neural Network (CNN). Finally, the LIDAR dataset collected during this work is released as open-source with the hope of benefiting future research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 140,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09562",
        "abs_url": "https://arxiv.org/abs/2507.09562",
        "pdf_url": "https://arxiv.org/pdf/2507.09562",
        "title": "Prompt Engineering in Segment Anything Model: Methodologies, Applications, and Emerging Challenges",
        "authors": [
            "Yidong Jiang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "The Segment Anything Model (SAM) has revolutionized image segmentation through its innovative prompt-based approach, yet the critical role of prompt engineering in its success remains underexplored. This paper presents the first comprehensive survey focusing specifically on prompt engineering techniques for SAM and its variants. We systematically organize and analyze the rapidly growing body of work in this emerging field, covering fundamental methodologies, practical applications, and key challenges. Our review reveals how prompt engineering has evolved from simple geometric inputs to sophisticated multimodal approaches, enabling SAM's adaptation across diverse domains including medical imaging and remote sensing. We identify unique challenges in prompt optimization and discuss promising research directions. This survey fills an important gap in the literature by providing a structured framework for understanding and advancing prompt engineering in foundation models for segmentation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 141,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09566",
        "abs_url": "https://arxiv.org/abs/2507.09566",
        "pdf_url": "https://arxiv.org/pdf/2507.09566",
        "title": "Identifying Offline Metrics that Predict Online Impact: A Pragmatic Strategy for Real-World Recommender Systems",
        "authors": [
            "Timo Wilm",
            "Philipp Normann"
        ],
        "comments": "This work was accepted for publication in the 19th ACM Conference on Recommender Systems (RecSys 2025). The final published version will be available at the ACM Digital Library",
        "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "A critical challenge in recommender systems is to establish reliable relationships between offline and online metrics that predict real-world performance. Motivated by recent advances in Pareto front approximation, we introduce a pragmatic strategy for identifying offline metrics that align with online impact. A key advantage of this approach is its ability to simultaneously serve multiple test groups, each with distinct offline performance metrics, in an online experiment controlled by a single model. The method is model-agnostic for systems with a neural network backbone, enabling broad applicability across architectures and domains. We validate the strategy through a large-scale online experiment in the field of session-based recommender systems on the OTTO e-commerce platform. The online experiment identifies significant alignments between offline metrics and real-word click-through rate, post-click conversion rate and units sold. Our strategy provides industry practitioners with a valuable tool for understanding offline-to-online metric relationships and making informed, data-driven decisions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 142,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09574",
        "abs_url": "https://arxiv.org/abs/2507.09574",
        "pdf_url": "https://arxiv.org/pdf/2507.09574",
        "title": "MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive Vision Generation Models",
        "authors": [
            "Haozhe Zhao",
            "Zefan Cai",
            "Shuzheng Si",
            "Liang Chen",
            "Jiuxiang Gu",
            "Wen Xiao",
            "Junjie Hu"
        ],
        "comments": "24 pages,12 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Recent text-to-image models produce high-quality results but still struggle with precise visual control, balancing multimodal inputs, and requiring extensive training for complex multimodal image generation. To address these limitations, we propose MENTOR, a novel autoregressive (AR) framework for efficient Multimodal-conditioned Tuning for Autoregressive multimodal image generation. MENTOR combines an AR image generator with a two-stage training paradigm, enabling fine-grained, token-level alignment between multimodal inputs and image outputs without relying on auxiliary adapters or cross-attention modules. The two-stage training consists of: (1) a multimodal alignment stage that establishes robust pixel- and semantic-level alignment, followed by (2) a multimodal instruction tuning stage that balances the integration of multimodal inputs and enhances generation controllability. Despite modest model size, suboptimal base components, and limited training resources, MENTOR achieves strong performance on the DreamBench++ benchmark, outperforming competitive baselines in concept preservation and prompt following. Additionally, our method delivers superior image reconstruction fidelity, broad task adaptability, and improved training efficiency compared to diffusion-based methods. Dataset, code, and models are available at: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 143,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09583",
        "abs_url": "https://arxiv.org/abs/2507.09583",
        "pdf_url": "https://arxiv.org/pdf/2507.09583",
        "title": "A Serverless Architecture for Real-Time Stock Analysis using Large Language Models: An Iterative Development and Debugging Case Study",
        "authors": [
            "Taniv Ashraf"
        ],
        "comments": "6 pages. The live application can be viewed at this https URL and the source code is available at this https URL",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)",
        "abstract": "The advent of powerful, accessible Large Language Models (LLMs) like Google's Gemini presents new opportunities for democratizing financial data analysis. This paper documents the design, implementation, and iterative debugging of a novel, serverless system for real-time stock analysis. The system leverages the Gemini API for qualitative assessment, automates data ingestion and processing via GitHub Actions, and presents the findings through a decoupled, static frontend. We detail the architectural evolution of the system, from initial concepts to a robust, event-driven pipeline, highlighting the practical challenges encountered during deployment. A significant portion of this paper is dedicated to a case study on the debugging process, covering common software errors, platform-specific permission issues, and rare, environment-level platform bugs. The final architecture operates at a near-zero cost, demonstrating a viable model for individuals to build sophisticated AI-powered financial tools. The operational application is publicly accessible, and the complete source code is available for review. We conclude by discussing the role of LLMs in financial analysis, the importance of robust debugging methodologies, and the emerging paradigm of human-AI collaboration in software development.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 144,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09592",
        "abs_url": "https://arxiv.org/abs/2507.09592",
        "pdf_url": "https://arxiv.org/pdf/2507.09592",
        "title": "THOR: Transformer Heuristics for On-Demand Retrieval",
        "authors": [
            "Isaac Shi",
            "Zeyuan Li",
            "Fan Liu",
            "Wenli Wang",
            "Lewei He",
            "Yang Yang",
            "Tianyu Shi"
        ],
        "comments": "",
        "subjects": "Databases (cs.DB); Artificial Intelligence (cs.AI)",
        "abstract": "We introduce the THOR (Transformer Heuristics for On-Demand Retrieval) Module, designed and implemented by eSapiens, a secure, scalable engine that transforms natural-language questions into verified, read-only SQL analytics for enterprise databases. The Text-to-SQL module follows a decoupled orchestration/execution architecture: a Supervisor Agent routes queries, Schema Retrieval dynamically injects table and column metadata, and a SQL Generation Agent emits single-statement SELECT queries protected by a read-only guardrail. An integrated Self-Correction & Rating loop captures empty results, execution errors, or low-quality outputs and triggers up to five LLM-driven regeneration attempts. Finally, a Result Interpretation Agent produces concise, human-readable insights and hands raw rows to the Insight & Intelligence engine for visualization or forecasting. Smoke tests across finance, sales, and operations scenarios demonstrate reliable ad-hoc querying and automated periodic reporting. By embedding schema awareness, fault-tolerant execution, and compliance guardrails, the THOR Module empowers non-technical users to access live data with zero-SQL simplicity and enterprise-grade safety.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 145,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09601",
        "abs_url": "https://arxiv.org/abs/2507.09601",
        "pdf_url": "https://arxiv.org/pdf/2507.09601",
        "title": "NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance",
        "authors": [
            "Hanwool Lee",
            "Sara Yu",
            "Yewon Hwang",
            "Jonghyun Choi",
            "Heejae Ahn",
            "Sungbum Jung",
            "Youngjae Yu"
        ],
        "comments": "Under Review",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computational Finance (q-fin.CP)",
        "abstract": "General-purpose sentence embedding models often struggle to capture specialized financial semantics, especially in low-resource languages like Korean, due to domain-specific jargon, temporal meaning shifts, and misaligned bilingual vocabularies. To address these gaps, we introduce NMIXX (Neural eMbeddings for Cross-lingual eXploration of Finance), a suite of cross-lingual embedding models fine-tuned with 18.8K high-confidence triplets that pair in-domain paraphrases, hard negatives derived from a semantic-shift typology, and exact Korean-English translations. Concurrently, we release KorFinSTS, a 1,921-pair Korean financial STS benchmark spanning news, disclosures, research reports, and regulations, designed to expose nuances that general benchmarks miss. When evaluated against seven open-license baselines, NMIXX's multilingual bge-m3 variant achieves Spearman's rho gains of +0.10 on English FinSTS and +0.22 on KorFinSTS, outperforming its pre-adaptation checkpoint and surpassing other models by the largest margin, while revealing a modest trade-off in general STS performance. Our analysis further shows that models with richer Korean token coverage adapt more effectively, underscoring the importance of tokenizer design in low-resource, cross-lingual settings. By making both models and the benchmark publicly available, we provide the community with robust tools for domain-adapted, multilingual representation learning in finance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 146,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09602",
        "abs_url": "https://arxiv.org/abs/2507.09602",
        "pdf_url": "https://arxiv.org/pdf/2507.09602",
        "title": "DRAGD: A Federated Unlearning Data Reconstruction Attack Based on Gradient Differences",
        "authors": [
            "Bocheng Ju",
            "Junchao Fan",
            "Jiaqi Liu",
            "Xiaolin Chang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Federated learning enables collaborative machine learning while preserving data privacy. However, the rise of federated unlearning, designed to allow clients to erase their data from the global model, introduces new privacy concerns. Specifically, the gradient exchanges during the unlearning process can leak sensitive information about deleted data. In this paper, we introduce DRAGD, a novel attack that exploits gradient discrepancies before and after unlearning to reconstruct forgotten data. We also present DRAGDP, an enhanced version of DRAGD that leverages publicly available prior data to improve reconstruction accuracy, particularly for complex datasets like facial images. Extensive experiments across multiple datasets demonstrate that DRAGD and DRAGDP significantly outperform existing methods in data this http URL work highlights a critical privacy vulnerability in federated unlearning and offers a practical solution, advancing the security of federated unlearning systems in real-world applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 147,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09630",
        "abs_url": "https://arxiv.org/abs/2507.09630",
        "pdf_url": "https://arxiv.org/pdf/2507.09630",
        "title": "Brain Stroke Detection and Classification Using CT Imaging with Transformer Models and Explainable AI",
        "authors": [
            "Shomukh Qari",
            "Maha A. Thafar"
        ],
        "comments": "5 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Stroke is one of the leading causes of death globally, making early and accurate diagnosis essential for improving patient outcomes, particularly in emergency settings where timely intervention is critical. CT scans are the key imaging modality because of their speed, accessibility, and cost-effectiveness. This study proposed an artificial intelligence framework for multiclass stroke classification (ischemic, hemorrhagic, and no stroke) using CT scan images from a dataset provided by the Republic of Turkey's Ministry of Health. The proposed method adopted MaxViT, a state-of-the-art Vision Transformer, as the primary deep learning model for image-based stroke classification, with additional transformer variants (vision transformer, transformer-in-transformer, and ConvNext). To enhance model generalization and address class imbalance, we applied data augmentation techniques, including synthetic image generation. The MaxViT model trained with augmentation achieved the best performance, reaching an accuracy and F1-score of 98.00%, outperforming all other evaluated models and the baseline methods. The primary goal of this study was to distinguish between stroke types with high accuracy while addressing crucial issues of transparency and trust in artificial intelligence models. To achieve this, Explainable Artificial Intelligence (XAI) was integrated into the framework, particularly Grad-CAM++. It provides visual explanations of the model's decisions by highlighting relevant stroke regions in the CT scans and establishing an accurate, interpretable, and clinically applicable solution for early stroke detection. This research contributed to the development of a trustworthy AI-assisted diagnostic tool for stroke, facilitating its integration into clinical practice and enhancing access to timely and optimal stroke diagnosis in emergency departments, thereby saving more lives.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 148,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09647",
        "abs_url": "https://arxiv.org/abs/2507.09647",
        "pdf_url": "https://arxiv.org/pdf/2507.09647",
        "title": "KEN: Knowledge Augmentation and Emotion Guidance Network for Multimodal Fake News Detection",
        "authors": [
            "Peican Zhu",
            "Yubo Jing",
            "Le Cheng",
            "Keke Tang",
            "Yangming Guo"
        ],
        "comments": "Accepted by ACM MM 2025",
        "subjects": "Multimedia (cs.MM); Artificial Intelligence (cs.AI)",
        "abstract": "In recent years, the rampant spread of misinformation on social media has made accurate detection of multimodal fake news a critical research focus. However, previous research has not adequately understood the semantics of images, and models struggle to discern news authenticity with limited textual information. Meanwhile, treating all emotional types of news uniformly without tailored approaches further leads to performance degradation. Therefore, we propose a novel Knowledge Augmentation and Emotion Guidance Network (KEN). On the one hand, we effectively leverage LVLM's powerful semantic understanding and extensive world knowledge. For images, the generated captions provide a comprehensive understanding of image content and scenes, while for text, the retrieved evidence helps break the information silos caused by the closed and limited text and context. On the other hand, we consider inter-class differences between different emotional types of news through balanced learning, achieving fine-grained modeling of the relationship between emotional types and authenticity. Extensive experiments on two real-world datasets demonstrate the superiority of our KEN.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 149,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09664",
        "abs_url": "https://arxiv.org/abs/2507.09664",
        "pdf_url": "https://arxiv.org/pdf/2507.09664",
        "title": "SimStep: Chain-of-Abstractions for Incremental Specification and Debugging of AI-Generated Interactive Simulations",
        "authors": [
            "Zoe Kaputa",
            "Anika Rajaram",
            "Vryan Almanon Feliciano",
            "Zhuoyue Lyu",
            "Maneesh Agrawala",
            "Hari Subramonyam"
        ],
        "comments": "",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)",
        "abstract": "Programming-by-prompting with generative AI offers a new paradigm for end-user programming, shifting the focus from syntactic fluency to semantic intent. This shift holds particular promise for non-programmers such as educators, who can describe instructional goals in natural language to generate interactive learning content. Yet in bypassing direct code authoring, many of programming's core affordances - such as traceability, stepwise refinement, and behavioral testing - are lost. We propose the Chain-of-Abstractions (CoA) framework as a way to recover these affordances while preserving the expressive flexibility of natural language. CoA decomposes the synthesis process into a sequence of cognitively meaningful, task-aligned representations that function as checkpoints for specification, inspection, and refinement. We instantiate this approach in SimStep, an authoring environment for teachers that scaffolds simulation creation through four intermediate abstractions: Concept Graph, Scenario Graph, Learning Goal Graph, and UI Interaction Graph. To address ambiguities and misalignments, SimStep includes an inverse correction process that surfaces in-filled model assumptions and enables targeted revision without requiring users to manipulate code. Evaluations with educators show that CoA enables greater authoring control and interpretability in programming-by-prompting workflows.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 150,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09678",
        "abs_url": "https://arxiv.org/abs/2507.09678",
        "pdf_url": "https://arxiv.org/pdf/2507.09678",
        "title": "Conformal Prediction for Privacy-Preserving Machine Learning",
        "authors": [
            "Alexander David Balinsky",
            "Dominik Krzeminski",
            "Alexander Balinsky"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Statistics Theory (math.ST)",
        "abstract": "We investigate the integration of Conformal Prediction (CP) with supervised learning on deterministically encrypted data, aiming to bridge the gap between rigorous uncertainty quantification and privacy-preserving machine learning. Using AES-encrypted variants of the MNIST dataset, we demonstrate that CP methods remain effective even when applied directly in the encrypted domain, owing to the preservation of data exchangeability under fixed-key encryption. We test traditional $p$-value-based against $e$-value-based conformal predictors. Our empirical evaluation reveals that models trained on deterministically encrypted data retain the ability to extract meaningful structure, achieving 36.88\\% test accuracy -- significantly above random guessing (9.56\\%) observed with per-instance encryption. Moreover, $e$-value-based CP achieves predictive set coverage of over 60\\% with 4.3 loss-threshold calibration, correctly capturing the true label in 4888 out of 5000 test cases. In contrast, the $p$-value-based CP yields smaller predictive sets but with reduced coverage accuracy. These findings highlight both the promise and limitations of CP in encrypted data settings and underscore critical trade-offs between prediction set compactness and reliability. %Our work sets a foundation for principled uncertainty quantification in secure, privacy-aware learning systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 151,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09682",
        "abs_url": "https://arxiv.org/abs/2507.09682",
        "pdf_url": "https://arxiv.org/pdf/2507.09682",
        "title": "OrQstrator: An AI-Powered Framework for Advanced Quantum Circuit Optimization",
        "authors": [
            "Laura Baird",
            "Armin Moin"
        ],
        "comments": "IEEE International Conference on Quantum Computing and Engineering (QCE) 2025 - Extended Abstract",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI); Emerging Technologies (cs.ET)",
        "abstract": "We propose a novel approach, OrQstrator, which is a modular framework for conducting quantum circuit optimization in the Noisy Intermediate-Scale Quantum (NISQ) era. Our framework is powered by Deep Reinforcement Learning (DRL). Our orchestration engine intelligently selects among three complementary circuit optimizers: A DRL-based circuit rewriter trained to reduce depth and gate count via learned rewrite sequences; a domain-specific optimizer that performs efficient local gate resynthesis and numeric optimization; a parameterized circuit instantiator that improves compilation by optimizing template circuits during gate set translation. These modules are coordinated by a central orchestration engine that learns coordination policies based on circuit structure, hardware constraints, and backend-aware performance features such as gate count, depth, and expected fidelity. The system outputs an optimized circuit for hardware-aware transpilation and execution, leveraging techniques from an existing state-of-the-art approach, called the NISQ Analyzer, to adapt to backend constraints.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 152,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09687",
        "abs_url": "https://arxiv.org/abs/2507.09687",
        "pdf_url": "https://arxiv.org/pdf/2507.09687",
        "title": "Post-Training Quantization of Generative and Discriminative LSTM Text Classifiers: A Study of Calibration, Class Balance, and Robustness",
        "authors": [
            "Md Mushfiqur Rahaman",
            "Elliot Chang",
            "Tasmiah Haque",
            "Srinjoy Das"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Text classification plays a pivotal role in edge computing applications like industrial monitoring, health diagnostics, and smart assistants, where low latency and high accuracy are both key requirements. Generative classifiers, in particular, have been shown to exhibit robustness to out-of-distribution and noisy data, which is an extremely critical consideration for deployment in such real-time edge environments. However, deploying such models on edge devices faces computational and memory constraints. Post Training Quantization (PTQ) reduces model size and compute costs without retraining, making it ideal for edge deployment. In this work, we present a comprehensive comparative study of generative and discriminative Long Short Term Memory (LSTM)-based text classification models with PTQ using the Brevitas quantization library. We evaluate both types of classifier models across multiple bitwidths and assess their robustness under regular and noisy input conditions. We find that while discriminative classifiers remain robust, generative ones are more sensitive to bitwidth, calibration data used during PTQ, and input noise during quantized inference. We study the influence of class imbalance in calibration data for both types of classifiers, comparing scenarios with evenly and unevenly distributed class samples including their effect on weight adjustments and activation profiles during PTQ. Using test statistics derived from nonparametric hypothesis testing, we identify that using class imbalanced data during calibration introduces insufficient weight adaptation at lower bitwidths for generative LSTM classifiers, thereby leading to degraded performance. This study underscores the role of calibration data in PTQ and when generative classifiers succeed or fail under noise, aiding deployment in edge environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 153,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09694",
        "abs_url": "https://arxiv.org/abs/2507.09694",
        "pdf_url": "https://arxiv.org/pdf/2507.09694",
        "title": "Frequency-aware Surrogate Modeling With SMT Kernels For Advanced Data Forecasting",
        "authors": [
            "Nicolas Gonel",
            "Paul Saves",
            "Joseph Morlier"
        ],
        "comments": "AeroBest 2025, Instituto Superior Tecnico of the University of Lisbon, Portugal",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Optimization and Control (math.OC); Machine Learning (stat.ML)",
        "abstract": "This paper introduces a comprehensive open-source framework for developing correlation kernels, with a particular focus on user-defined and composition of kernels for surrogate modeling. By advancing kernel-based modeling techniques, we incorporate frequency-aware elements that effectively capture complex mechanical behaviors and timefrequency dynamics intrinsic to aircraft systems. Traditional kernel functions, often limited to exponential-based methods, are extended to include a wider range of kernels such as exponential squared sine and rational quadratic kernels, along with their respective firstand second-order derivatives. The proposed methodologies are first validated on a sinus cardinal test case and then applied to forecasting Mauna-Loa Carbon Dioxide (CO 2 ) concentrations and airline passenger traffic. All these advancements are integrated into the open-source Surrogate Modeling Toolbox (SMT 2.0), providing a versatile platform for both standard and customizable kernel configurations. Furthermore, the framework enables the combination of various kernels to leverage their unique strengths into composite models tailored to specific problems. The resulting framework offers a flexible toolset for engineers and researchers, paving the way for numerous future applications in metamodeling for complex, frequency-sensitive domains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 154,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09703",
        "abs_url": "https://arxiv.org/abs/2507.09703",
        "pdf_url": "https://arxiv.org/pdf/2507.09703",
        "title": "EPT-2 Technical Report",
        "authors": [
            "Roberto Molinaro",
            "Niall Siegenheim",
            "Niels Poulsen",
            "Jordan Dane Daubinet",
            "Henry Martin",
            "Mark Frey",
            "Kevin Thiart",
            "Alexander Jakob Dautel",
            "Andreas Schlueter",
            "Alex Grigoryev",
            "Bogdan Danciu",
            "Nikoo Ekhtiari",
            "Bas Steunebrink",
            "Leonie Wagner",
            "Marvin Vincent Gabler"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "We present EPT-2, the latest iteration in our Earth Physics Transformer (EPT) family of foundation AI models for Earth system forecasting. EPT-2 delivers substantial improvements over its predecessor, EPT-1.5, and sets a new state of the art in predicting energy-relevant variables-including 10m and 100m wind speed, 2m temperature, and surface solar radiation-across the full 0-240h forecast horizon. It consistently outperforms leading AI weather models such as Microsoft Aurora, as well as the operational numerical forecast system IFS HRES from the European Centre for Medium-Range Weather Forecasts (ECMWF). In parallel, we introduce a perturbation-based ensemble model of EPT-2 for probabilistic forecasting, called EPT-2e. Remarkably, EPT-2e significantly surpasses the ECMWF ENS mean-long considered the gold standard for medium- to longrange forecasting-while operating at a fraction of the computational cost. EPT models, as well as third-party forecasts, are accessible via the this http URL platform.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 155,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09725",
        "abs_url": "https://arxiv.org/abs/2507.09725",
        "pdf_url": "https://arxiv.org/pdf/2507.09725",
        "title": "Visual Homing in Outdoor Robots Using Mushroom Body Circuits and Learning Walks",
        "authors": [
            "Gabriel G. Gattaux",
            "Julien R. Serres",
            "Franck Ruffier",
            "Antoine Wystrach"
        ],
        "comments": "Published by Springer Nature with the 14th bioinspired and biohybrid systems conference in Sheffield, and presented at the conference in July 2025",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Ants achieve robust visual homing with minimal sensory input and only a few learning walks, inspiring biomimetic solutions for autonomous navigation. While Mushroom Body (MB) models have been used in robotic route following, they have not yet been applied to visual homing. We present the first real-world implementation of a lateralized MB architecture for visual homing onboard a compact autonomous car-like robot. We test whether the sign of the angular path integration (PI) signal can categorize panoramic views, acquired during learning walks and encoded in the MB, into \"goal on the left\" and \"goal on the right\" memory banks, enabling robust homing in natural outdoor settings. We validate this approach through four incremental experiments: (1) simulation showing attractor-like nest dynamics; (2) real-world homing after decoupled learning walks, producing nest search behavior; (3) homing after random walks using noisy PI emulated with GPS-RTK; and (4) precise stopping-at-the-goal behavior enabled by a fifth MB Output Neuron (MBON) encoding goal-views to control velocity. This mimics the accurate homing behavior of ants and functionally resembles waypoint-based position control in robotics, despite relying solely on visual input. Operating at 8 Hz on a Raspberry Pi 4 with 32x32 pixel views and a memory footprint under 9 kB, our system offers a biologically grounded, resource-efficient solution for autonomous visual homing.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 156,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09733",
        "abs_url": "https://arxiv.org/abs/2507.09733",
        "pdf_url": "https://arxiv.org/pdf/2507.09733",
        "title": "Universal Physics Simulation: A Foundational Diffusion Approach",
        "authors": [
            "Bradley Camburn"
        ],
        "comments": "10 pages, 3 figures. Foundational AI model for universal physics simulation using sketch-guided diffusion transformers. Achieves SSIM > 0.8 on electromagnetic field generation without requiring a priori physics encoding",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "We present the first foundational AI model for universal physics simulation that learns physical laws directly from boundary-condition data without requiring a priori equation encoding. Traditional physics-informed neural networks (PINNs) and finite-difference methods necessitate explicit mathematical formulation of governing equations, fundamentally limiting their generalizability and discovery potential. Our sketch-guided diffusion transformer approach reimagines computational physics by treating simulation as a conditional generation problem, where spatial boundary conditions guide the synthesis of physically accurate steady-state solutions. By leveraging enhanced diffusion transformer architectures with novel spatial relationship encoding, our model achieves direct boundary-to-equilibrium mapping and is generalizable to diverse physics domains. Unlike sequential time-stepping methods that accumulate errors over iterations, our approach bypasses temporal integration entirely, directly generating steady-state solutions with SSIM > 0.8 while maintaining sub-pixel boundary accuracy. Our data-informed approach enables physics discovery through learned representations analyzable via Layer-wise Relevance Propagation (LRP), revealing emergent physical relationships without predetermined mathematical constraints. This work represents a paradigm shift from AI-accelerated physics to AI-discovered physics, establishing the first truly universal physics simulation framework.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 157,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09759",
        "abs_url": "https://arxiv.org/abs/2507.09759",
        "pdf_url": "https://arxiv.org/pdf/2507.09759",
        "title": "AI-Enhanced Pediatric Pneumonia Detection: A CNN-Based Approach Using Data Augmentation and Generative Adversarial Networks (GANs)",
        "authors": [
            "Abdul Manaf",
            "Nimra Mughal"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Pneumonia is a leading cause of mortality in children under five, requiring accurate chest X-ray diagnosis. This study presents a machine learning-based Pediatric Chest Pneumonia Classification System to assist healthcare professionals in diagnosing pneumonia from chest X-ray images. The CNN-based model was trained on 5,863 labeled chest X-ray images from children aged 0-5 years from the Guangzhou Women and Children's Medical Center. To address limited data, we applied augmentation techniques (rotation, zooming, shear, horizontal flipping) and employed GANs to generate synthetic images, addressing class imbalance. The system achieved optimal performance using combined original, augmented, and GAN-generated data, evaluated through accuracy and F1 score metrics. The final model was deployed via a Flask web application, enabling real-time classification with probability estimates. Results demonstrate the potential of deep learning and GANs in improving diagnostic accuracy and efficiency for pediatric pneumonia classification, particularly valuable in resource-limited clinical settings this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 158,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09762",
        "abs_url": "https://arxiv.org/abs/2507.09762",
        "pdf_url": "https://arxiv.org/pdf/2507.09762",
        "title": "EventHunter: Dynamic Clustering and Ranking of Security Events from Hacker Forum Discussions",
        "authors": [
            "Yasir Ech-Chammakhy",
            "Anas Motii",
            "Anass Rabii",
            "Jaafar Chbili"
        ],
        "comments": "Accepted for publication at the 28th International Symposium on Research in Attacks, Intrusions, and Defenses (RAID 2025)",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Hacker forums provide critical early warning signals for emerging cybersecurity threats, but extracting actionable intelligence from their unstructured and noisy content remains a significant challenge. This paper presents an unsupervised framework that automatically detects, clusters, and prioritizes security events discussed across hacker forum posts. Our approach leverages Transformer-based embeddings fine-tuned with contrastive learning to group related discussions into distinct security event clusters, identifying incidents like zero-day disclosures or malware releases without relying on predefined keywords. The framework incorporates a daily ranking mechanism that prioritizes identified events using quantifiable metrics reflecting timeliness, source credibility, information completeness, and relevance. Experimental evaluation on real-world hacker forum data demonstrates that our method effectively reduces noise and surfaces high-priority threats, enabling security analysts to mount proactive responses. By transforming disparate hacker forum discussions into structured, actionable intelligence, our work addresses fundamental challenges in automated threat detection and analysis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 159,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09766",
        "abs_url": "https://arxiv.org/abs/2507.09766",
        "pdf_url": "https://arxiv.org/pdf/2507.09766",
        "title": "Toward accurate RUL and SOH estimation using reinforced graph-based PINNs enhanced with dynamic weights",
        "authors": [
            "Mohamadreza Akbari Pour",
            "Ali Ghasemzadeh",
            "MohamadAli Bijarchi",
            "Mohammad Behshad Shafii"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Accurate estimation of Remaining Useful Life (RUL) and State of Health (SOH) is essential for Prognostics and Health Management (PHM) across a wide range of industrial applications. We propose a novel framework -- Reinforced Graph-Based Physics-Informed Neural Networks Enhanced with Dynamic Weights (RGPD) -- that combines physics-based supervision with advanced spatio-temporal learning. Graph Convolutional Recurrent Networks (GCRNs) embed graph-convolutional filters within recurrent units to capture how node representations evolve over time. Graph Attention Convolution (GATConv) leverages a self-attention mechanism to compute learnable, edge-wise attention coefficients, dynamically weighting neighbor contributions for adaptive spatial aggregation. A Soft Actor-Critic (SAC) module is positioned between the Temporal Attention Unit (TAU) and GCRN to further improve the spatio-temporal learning. This module improves attention and prediction accuracy by dynamically scaling hidden representations to minimize noise and highlight informative features. To identify the most relevant physical constraints in each area, Q-learning agents dynamically assign weights to physics-informed loss terms, improving generalization across real-time industrial systems and reducing the need for manual tuning. In both RUL and SOH estimation tasks, the proposed method consistently outperforms state-of-the-art models, demonstrating strong robustness and predictive accuracy across varied degradation patterns across three diverse industrial benchmark datasets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 160,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09780",
        "abs_url": "https://arxiv.org/abs/2507.09780",
        "pdf_url": "https://arxiv.org/pdf/2507.09780",
        "title": "BitParticle: Partializing Sparse Dual-Factors to Build Quasi-Synchronizing MAC Arrays for Energy-efficient DNNs",
        "authors": [
            "Feilong Qiaoyuan",
            "Jihe Wang",
            "Zhiyu Sun",
            "Linying Wu",
            "Yuanhua Xiao",
            "Danghui Wang"
        ],
        "comments": "9 pages, 13 figures, 3 Tables",
        "subjects": "Hardware Architecture (cs.AR); Artificial Intelligence (cs.AI)",
        "abstract": "Bit-level sparsity in quantized deep neural networks (DNNs) offers significant potential for optimizing Multiply-Accumulate (MAC) operations. However, two key challenges still limit its practical exploitation. First, conventional bit-serial approaches cannot simultaneously leverage the sparsity of both factors, leading to a complete waste of one factor' s sparsity. Methods designed to exploit dual-factor sparsity are still in the early stages of exploration, facing the challenge of partial product explosion. Second, the fluctuation of bit-level sparsity leads to variable cycle counts for MAC operations. Existing synchronous scheduling schemes that are suitable for dual-factor sparsity exhibit poor flexibility and still result in significant underutilization of MAC units. To address the first challenge, this study proposes a MAC unit that leverages dual-factor sparsity through the emerging particlization-based approach. The proposed design addresses the issue of partial product explosion through simple control logic, resulting in a more area- and energy-efficient MAC unit. In addition, by discarding less significant intermediate results, the design allows for further hardware simplification at the cost of minor accuracy loss. To address the second challenge, a quasi-synchronous scheme is introduced that adds cycle-level elasticity to the MAC array, reducing pipeline stalls and thereby improving MAC unit utilization. Evaluation results show that the exact version of the proposed MAC array architecture achieves a 29.2% improvement in area efficiency compared to the state-of-the-art bit-sparsity-driven architecture, while maintaining comparable energy efficiency. The approximate variant further improves energy efficiency by 7.5%, compared to the exact version. Index-Terms: DNN acceleration, Bit-level sparsity, MAC unit",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 161,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09788",
        "abs_url": "https://arxiv.org/abs/2507.09788",
        "pdf_url": "https://arxiv.org/pdf/2507.09788",
        "title": "TinyTroupe: An LLM-powered Multiagent Persona Simulation Toolkit",
        "authors": [
            "Paulo Salem",
            "Robert Sim",
            "Christopher Olsen",
            "Prerit Saxena",
            "Rafael Barcelos",
            "Yi Ding"
        ],
        "comments": "9 pages. Preprint to be submitted to peer-review",
        "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Human-Computer Interaction (cs.HC)",
        "abstract": "Recent advances in Large Language Models (LLM) have led to a new class of autonomous agents, renewing and expanding interest in the area. LLM-powered Multiagent Systems (MAS) have thus emerged, both for assistive and simulation purposes, yet tools for realistic human behavior simulation -- with its distinctive challenges and opportunities -- remain underdeveloped. Existing MAS libraries and tools lack fine-grained persona specifications, population sampling facilities, experimentation support, and integrated validation, among other key capabilities, limiting their utility for behavioral studies, social simulation, and related applications. To address these deficiencies, in this work we introduce TinyTroupe, a simulation toolkit enabling detailed persona definitions (e.g., nationality, age, occupation, personality, beliefs, behaviors) and programmatic control via numerous LLM-driven mechanisms. This allows for the concise formulation of behavioral problems of practical interest, either at the individual or group level, and provides effective means for their solution. TinyTroupe's components are presented using representative working examples, such as brainstorming and market research sessions, thereby simultaneously clarifying their purpose and demonstrating their usefulness. Quantitative and qualitative evaluations of selected aspects are also provided, highlighting possibilities, limitations, and trade-offs. The approach, though realized as a specific Python implementation, is meant as a novel conceptual contribution, which can be partially or fully incorporated in other contexts. The library is available as open source at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 162,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09790",
        "abs_url": "https://arxiv.org/abs/2507.09790",
        "pdf_url": "https://arxiv.org/pdf/2507.09790",
        "title": "Prompting for Performance: Exploring LLMs for Configuring Software",
        "authors": [
            "Helge Spieker",
            "Théo Matricon",
            "Nassim Belmecheri",
            "Jørn Eirik Betten",
            "Gauthier Le Bartz Lyan",
            "Heraldo Borges",
            "Quentin Mazouni",
            "Dennis Gross",
            "Arnaud Gotlieb",
            "Mathieu Acher"
        ],
        "comments": "",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI); Performance (cs.PF)",
        "abstract": "Software systems usually provide numerous configuration options that can affect performance metrics such as execution time, memory usage, binary size, or bitrate. On the one hand, making informed decisions is challenging and requires domain expertise in options and their combinations. On the other hand, machine learning techniques can search vast configuration spaces, but with a high computational cost, since concrete executions of numerous configurations are required. In this exploratory study, we investigate whether large language models (LLMs) can assist in performance-oriented software configuration through prompts. We evaluate several LLMs on tasks including identifying relevant options, ranking configurations, and recommending performant configurations across various configurable systems, such as compilers, video encoders, and SAT solvers. Our preliminary results reveal both positive abilities and notable limitations: depending on the task and systems, LLMs can well align with expert knowledge, whereas hallucinations or superficial reasoning can emerge in other cases. These findings represent a first step toward systematic evaluations and the design of LLM-based solutions to assist with software configuration.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 163,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09792",
        "abs_url": "https://arxiv.org/abs/2507.09792",
        "pdf_url": "https://arxiv.org/pdf/2507.09792",
        "title": "CADmium: Fine-Tuning Code Language Models for Text-Driven Sequential CAD Design",
        "authors": [
            "Prashant Govindarajan",
            "Davide Baldelli",
            "Jay Pathak",
            "Quentin Fournier",
            "Sarath Chandar"
        ],
        "comments": "",
        "subjects": "Graphics (cs.GR); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Computer-aided design (CAD) is the digital construction of 2D and 3D objects, and is central to a wide range of engineering and manufacturing applications like automobile and aviation. Despite its importance, CAD modeling remains largely a time-intensive, manual task. Recent works have attempted to automate this process with small transformer-based models and handcrafted CAD sequence representations. However, there has been little effort to leverage the potential of large language models (LLMs) for sequential CAD design. In this work, we introduce a new large-scale dataset of more than 170k CAD models annotated with high-quality, human-like descriptions generated with our pipeline based on GPT-4.1. Using this dataset, we fine-tune powerful code-LLMs to generate CAD sequences represented in a JSON-based format from natural language descriptions, demonstrating the viability and effectiveness of this approach for text-conditioned CAD generation. Because simple metrics often fail to reflect the quality of generated objects, we introduce geometric and topological metrics based on sphericity, mean curvature, and Euler characteristic to provide richer structural insights. Our experiments and ablation studies on both synthetic and human-annotated data demonstrate that CADmium is able to automate CAD design, drastically speeding up the design of new objects. The dataset, code, and fine-tuned models are available online.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 164,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09805",
        "abs_url": "https://arxiv.org/abs/2507.09805",
        "pdf_url": "https://arxiv.org/pdf/2507.09805",
        "title": "Federated Learning with Graph-Based Aggregation for Traffic Forecasting",
        "authors": [
            "Audri Banik",
            "Glaucio Haroldo Silva de Carvalho",
            "Renata Dividino"
        ],
        "comments": "Accepted at FedKDD 2025: International Joint Workshop on Federated Learning for Data Mining and Graph Analytics. 6 pages, 1 figure",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "In traffic prediction, the goal is to estimate traffic speed or flow in specific regions or road segments using historical data collected by devices deployed in each area. Each region or road segment can be viewed as an individual client that measures local traffic flow, making Federated Learning (FL) a suitable approach for collaboratively training models without sharing raw data. In centralized FL, a central server collects and aggregates model updates from multiple clients to build a shared model while preserving each client's data privacy. Standard FL methods, such as Federated Averaging (FedAvg), assume that clients are independent, which can limit performance in traffic prediction tasks where spatial relationships between clients are important. Federated Graph Learning methods can capture these dependencies during server-side aggregation, but they often introduce significant computational overhead. In this paper, we propose a lightweight graph-aware FL approach that blends the simplicity of FedAvg with key ideas from graph learning. Rather than training full models, our method applies basic neighbourhood aggregation principles to guide parameter updates, weighting client models based on graph connectivity. This approach captures spatial relationships effectively while remaining computationally efficient. We evaluate our method on two benchmark traffic datasets, METR-LA and PEMS-BAY, and show that it achieves competitive performance compared to standard baselines and recent graph-based federated learning techniques.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 165,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09816",
        "abs_url": "https://arxiv.org/abs/2507.09816",
        "pdf_url": "https://arxiv.org/pdf/2507.09816",
        "title": "Compressed Computation: Dense Circuits in a Toy Model of the Universal-AND Problem",
        "authors": [
            "Adam Newgas"
        ],
        "comments": "9 pages, 9 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Neural networks are capable of superposition -- representing more features than there are dimensions. Recent work considers the analogous concept for computation instead of storage, proposing theoretical constructions. But there has been little investigation into whether these circuits can be learned in practice. In this work, we investigate a toy model for the Universal-AND problem which computes the AND of all $m\\choose 2$ pairs of $m$ sparse inputs. The hidden dimension that determines the number of non-linear activations is restricted to pressure the model to find a compute-efficient circuit, called compressed computation. We find that the training process finds a simple solution that does not correspond to theoretical constructions. It is fully dense -- every neuron contributes to every output. The solution circuit naturally scales with dimension, trading off error rates for neuron efficiency. It is similarly robust to changes in sparsity and other key parameters, and extends naturally to other boolean operations and boolean circuits. We explain the found solution in detail and compute why it is more efficient than the theoretical constructions at low sparsity. Our findings shed light on the types of circuits that models like to form and the flexibility of the superposition representation. This contributes to a broader understanding of network circuitry and interpretability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 166,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09826",
        "abs_url": "https://arxiv.org/abs/2507.09826",
        "pdf_url": "https://arxiv.org/pdf/2507.09826",
        "title": "Bridging Neural Networks and Dynamic Time Warping for Adaptive Time Series Classification",
        "authors": [
            "Jintao Qu",
            "Zichong Wang",
            "Chenhao Wu",
            "Wenbin Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Neural networks have achieved remarkable success in time series classification, but their reliance on large amounts of labeled data for training limits their applicability in cold-start scenarios. Moreover, they lack interpretability, reducing transparency in decision-making. In contrast, dynamic time warping (DTW) combined with a nearest neighbor classifier is widely used for its effectiveness in limited-data settings and its inherent interpretability. However, as a non-parametric method, it is not trainable and cannot leverage large amounts of labeled data, making it less effective than neural networks in rich-resource scenarios. In this work, we aim to develop a versatile model that adapts to cold-start conditions and becomes trainable with labeled data, while maintaining interpretability. We propose a dynamic length-shortening algorithm that transforms time series into prototypes while preserving key structural patterns, thereby enabling the reformulation of the DTW recurrence relation into an equivalent recurrent neural network. Based on this, we construct a trainable model that mimics DTW's alignment behavior. As a neural network, it becomes trainable when sufficient labeled data is available, while still retaining DTW's inherent interpretability. We apply the model to several benchmark time series classification tasks and observe that it significantly outperforms previous approaches in low-resource settings and remains competitive in rich-resource settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 167,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09831",
        "abs_url": "https://arxiv.org/abs/2507.09831",
        "pdf_url": "https://arxiv.org/pdf/2507.09831",
        "title": "Generative Cognitive Diagnosis",
        "authors": [
            "Jiatong Li",
            "Qi Liu",
            "Mengxiao Zhu"
        ],
        "comments": "Preprint; 15 pages, 12 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)",
        "abstract": "Cognitive diagnosis (CD) models latent cognitive states of human learners by analyzing their response patterns on diagnostic tests, serving as a crucial machine learning technique for educational assessment and evaluation. Traditional cognitive diagnosis models typically follow a transductive prediction paradigm that optimizes parameters to fit response scores and extract learner abilities. These approaches face significant limitations as they cannot perform instant diagnosis for new learners without computationally expensive retraining and produce diagnostic outputs with limited reliability. In this study, we introduces a novel generative diagnosis paradigm that fundamentally shifts CD from predictive to generative modeling, enabling inductive inference of cognitive states without parameter re-optimization. We propose two simple yet effective instantiations of this paradigm: Generative Item Response Theory (G-IRT) and Generative Neural Cognitive Diagnosis Model (G-NCDM), which achieve excellent performance improvements over traditional methods. The generative approach disentangles cognitive state inference from response prediction through a well-designed generation process that incorporates identifiability and monotonicity conditions. Extensive experiments on real-world datasets demonstrate the effectiveness of our methodology in addressing scalability and reliability challenges, especially $\\times 100$ speedup for the diagnosis of new learners. Our framework opens new avenues for cognitive diagnosis applications in artificial intelligence, particularly for intelligent model evaluation and intelligent education systems. The code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 168,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09836",
        "abs_url": "https://arxiv.org/abs/2507.09836",
        "pdf_url": "https://arxiv.org/pdf/2507.09836",
        "title": "Multi-residual Mixture of Experts Learning for Cooperative Control in Multi-vehicle Systems",
        "authors": [
            "Vindula Jayawardana",
            "Sirui Li",
            "Yashar Farid",
            "Cathy Wu"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multiagent Systems (cs.MA); Systems and Control (eess.SY)",
        "abstract": "Autonomous vehicles (AVs) are becoming increasingly popular, with their applications now extending beyond just a mode of transportation to serving as mobile actuators of a traffic flow to control flow dynamics. This contrasts with traditional fixed-location actuators, such as traffic signals, and is referred to as Lagrangian traffic control. However, designing effective Lagrangian traffic control policies for AVs that generalize across traffic scenarios introduces a major challenge. Real-world traffic environments are highly diverse, and developing policies that perform robustly across such diverse traffic scenarios is challenging. It is further compounded by the joint complexity of the multi-agent nature of traffic systems, mixed motives among participants, and conflicting optimization objectives subject to strict physical and external constraints. To address these challenges, we introduce Multi-Residual Mixture of Expert Learning (MRMEL), a novel framework for Lagrangian traffic control that augments a given suboptimal nominal policy with a learned residual while explicitly accounting for the structure of the traffic scenario space. In particular, taking inspiration from residual reinforcement learning, MRMEL augments a suboptimal nominal AV control policy by learning a residual correction, but at the same time dynamically selects the most suitable nominal policy from a pool of nominal policies conditioned on the traffic scenarios and modeled as a mixture of experts. We validate MRMEL using a case study in cooperative eco-driving at signalized intersections in Atlanta, Dallas Fort Worth, and Salt Lake City, with real-world data-driven traffic scenarios. The results show that MRMEL consistently yields superior performance-achieving an additional 4%-9% reduction in aggregate vehicle emissions relative to the strongest baseline in each setting.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 169,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09837",
        "abs_url": "https://arxiv.org/abs/2507.09837",
        "pdf_url": "https://arxiv.org/pdf/2507.09837",
        "title": "A Pre-training Framework for Relational Data with Information-theoretic Principles",
        "authors": [
            "Quang Truong",
            "Zhikai Chen",
            "Mingxuan Ju",
            "Tong Zhao",
            "Neil Shah",
            "Jiliang Tang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Relational databases underpin critical infrastructure across a wide range of domains, yet the design of generalizable pre-training strategies for learning from relational databases remains an open challenge due to task heterogeneity. Specifically, there exist infinitely many possible downstream tasks, as tasks are defined based on relational schema graphs, temporal dependencies, and SQL-defined label logics. An effective pre-training framework is desired to take these factors into account in order to obtain task-aware representations. By incorporating knowledge of the underlying distribution that drives label generation, downstream tasks can benefit from relevant side-channel information. To bridge this gap, we introduce Task Vector Estimation (TVE), a novel pre-training framework that constructs predictive supervisory signals via set-based aggregation over schema traversal graphs, explicitly modeling next-window relational dynamics. We formalize our approach through an information-theoretic lens, demonstrating that task-informed representations retain more relevant signals than those obtained without task priors. Extensive experiments on the RelBench benchmark show that TVE consistently outperforms traditional pre-training baselines. Our findings advocate for pre-training objectives that encode task heterogeneity and temporal structure as design principles for predictive modeling on relational databases.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 170,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09846",
        "abs_url": "https://arxiv.org/abs/2507.09846",
        "pdf_url": "https://arxiv.org/pdf/2507.09846",
        "title": "Through the River: Understanding the Benefit of Schedule-Free Methods for Language Model Training",
        "authors": [
            "Minhak Song",
            "Beomhan Baek",
            "Kwangjun Ahn",
            "Chulhee Yun"
        ],
        "comments": "Comments would be appreciated!",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Optimization and Control (math.OC); Machine Learning (stat.ML)",
        "abstract": "As both model and dataset sizes continue to scale rapidly, conventional pretraining strategies with fixed compute budgets-such as cosine learning rate schedules-are increasingly inadequate for large-scale training. Recent alternatives, including warmup-stable-decay (WSD) schedules and weight averaging, offer greater flexibility. However, WSD relies on explicit decay phases to track progress, while weight averaging addresses this limitation at the cost of additional memory. In search of a more principled and scalable alternative, we revisit the Schedule-Free (SF) method [Defazio et al., 2024], which has shown strong empirical performance across diverse settings. We show that SF-AdamW effectively navigates the \"river\" structure of the loss landscape without decay phases or auxiliary averaging, making it particularly suitable for continuously scaling training workloads. To understand this behavior, we conduct a theoretical and empirical analysis of SF dynamics, revealing that it implicitly performs weight averaging without memory overhead. Guided by this analysis, we propose a refined variant of SF that improves robustness to momentum and performs better under large batch sizes, addressing key limitations of the original method. Together, these results establish SF as a practical, scalable, and theoretically grounded approach for language model training.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 171,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09860",
        "abs_url": "https://arxiv.org/abs/2507.09860",
        "pdf_url": "https://arxiv.org/pdf/2507.09860",
        "title": "Secure and Efficient UAV-Based Face Detection via Homomorphic Encryption and Edge Computing",
        "authors": [
            "Nguyen Van Duc",
            "Bui Duc Manh",
            "Quang-Trung Luu",
            "Dinh Thai Hoang",
            "Van-Linh Nguyen",
            "Diep N. Nguyen"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
        "abstract": "This paper aims to propose a novel machine learning (ML) approach incorporating Homomorphic Encryption (HE) to address privacy limitations in Unmanned Aerial Vehicles (UAV)-based face detection. Due to challenges related to distance, altitude, and face orientation, high-resolution imagery and sophisticated neural networks enable accurate face recognition in dynamic environments. However, privacy concerns arise from the extensive surveillance capabilities of UAVs. To resolve this issue, we propose a novel framework that integrates HE with advanced neural networks to secure facial data throughout the inference phase. This method ensures that facial data remains secure with minimal impact on detection accuracy. Specifically, the proposed system leverages the Cheon-Kim-Kim-Song (CKKS) scheme to perform computations directly on encrypted data, optimizing computational efficiency and security. Furthermore, we develop an effective data encoding method specifically designed to preprocess the raw facial data into CKKS form in a Single-Instruction-Multiple-Data (SIMD) manner. Building on this, we design a secure inference algorithm to compute on ciphertext without needing decryption. This approach not only protects data privacy during the processing of facial data but also enhances the efficiency of UAV-based face detection systems. Experimental results demonstrate that our method effectively balances privacy protection and detection performance, making it a viable solution for UAV-based secure face detection. Significantly, our approach (while maintaining data confidentially with HE encryption) can still achieve an accuracy of less than 1% compared to the benchmark without using encryption.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 172,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09861",
        "abs_url": "https://arxiv.org/abs/2507.09861",
        "pdf_url": "https://arxiv.org/pdf/2507.09861",
        "title": "A Survey on MLLM-based Visually Rich Document Understanding: Methods, Challenges, and Emerging Trends",
        "authors": [
            "Yihao Ding",
            "Siwen Luo",
            "Yue Dai",
            "Yanbei Jiang",
            "Zechuan Li",
            "Geoffrey Martin",
            "Yifan Peng"
        ],
        "comments": "Work in progress",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Visually-Rich Document Understanding (VRDU) has emerged as a critical field, driven by the need to automatically process documents containing complex visual, textual, and layout information. Recently, Multimodal Large Language Models (MLLMs) have shown remarkable potential in this domain, leveraging both Optical Character Recognition (OCR)-dependent and OCR-free frameworks to extract and interpret information in document images. This survey reviews recent advancements in MLLM-based VRDU, highlighting three core components: (1) methods for encoding and fusing textual, visual, and layout features; (2) training paradigms, including pretraining strategies, instruction-response tuning, and the trainability of different model modules; and (3) datasets utilized for pretraining, instruction-tuning, and supervised fine-tuning. Finally, we discuss the challenges and opportunities in this evolving field and propose future directions to advance the efficiency, generalizability, and robustness of VRDU systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 173,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09864",
        "abs_url": "https://arxiv.org/abs/2507.09864",
        "pdf_url": "https://arxiv.org/pdf/2507.09864",
        "title": "Intersection of Reinforcement Learning and Bayesian Optimization for Intelligent Control of Industrial Processes: A Safe MPC-based DPG using Multi-Objective BO",
        "authors": [
            "Hossein Nejatbakhsh Esfahani",
            "Javad Mohammadpour Velni"
        ],
        "comments": "",
        "subjects": "Systems and Control (eess.SY); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Optimization and Control (math.OC)",
        "abstract": "Model Predictive Control (MPC)-based Reinforcement Learning (RL) offers a structured and interpretable alternative to Deep Neural Network (DNN)-based RL methods, with lower computational complexity and greater transparency. However, standard MPC-RL approaches often suffer from slow convergence, suboptimal policy learning due to limited parameterization, and safety issues during online adaptation. To address these challenges, we propose a novel framework that integrates MPC-RL with Multi-Objective Bayesian Optimization (MOBO). The proposed MPC-RL-MOBO utilizes noisy evaluations of the RL stage cost and its gradient, estimated via a Compatible Deterministic Policy Gradient (CDPG) approach, and incorporates them into a MOBO algorithm using the Expected Hypervolume Improvement (EHVI) acquisition function. This fusion enables efficient and safe tuning of the MPC parameters to achieve improved closed-loop performance, even under model imperfections. A numerical example demonstrates the effectiveness of the proposed approach in achieving sample-efficient, stable, and high-performance learning for control systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 174,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09866",
        "abs_url": "https://arxiv.org/abs/2507.09866",
        "pdf_url": "https://arxiv.org/pdf/2507.09866",
        "title": "Turning the Tide: Repository-based Code Reflection",
        "authors": [
            "Wei Zhang",
            "Jian Yang",
            "Jiaxi Yang",
            "Ya Wang",
            "Zhoujun Li",
            "Zeyu Cui",
            "Binyuan Hui",
            "Junyang Lin"
        ],
        "comments": "",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)",
        "abstract": "Code large language models (LLMs) enhance programming by understanding and generating code across languages, offering intelligent feedback, bug detection, and code updates through reflection, improving development efficiency and accessibility. While benchmarks (e.g. HumanEval/LiveCodeBench) evaluate code generation and real-world relevance, previous works ignore the scenario of modifying code in repositories. Considering challenges remaining in improving reflection capabilities and avoiding data contamination in dynamic benchmarks, we introduce LiveRepoReflection, a challenging benchmark for evaluating code understanding and generation in multi-file repository contexts, featuring 1,888 rigorously filtered test cases across $6$ programming languages to ensure diversity, correctness, and high difficulty. Further, we create RepoReflection-Instruct, a large-scale, quality-filtered instruction-tuning dataset derived from diverse sources, used to train RepoReflectionCoder through a two-turn dialogue process involving code generation and error-driven repair. The leaderboard evaluates over 40 LLMs to reflect the model performance of repository-based code reflection.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 175,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09871",
        "abs_url": "https://arxiv.org/abs/2507.09871",
        "pdf_url": "https://arxiv.org/pdf/2507.09871",
        "title": "Task Priors: Enhancing Model Evaluation by Considering the Entire Space of Downstream Tasks",
        "authors": [
            "Niket Patel",
            "Randall Balestriero"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "The grand goal of AI research, and particularly Self Supervised Learning (SSL), is to produce systems that can successfully solve any possible task. In contrast, current evaluation methods available to AI researchers typically rely on a fixed collection of hand-picked downstream benchmarks. Hence, a large amount of effort is put into designing and searching for large collection of evaluation tasks that can serve as a proxy of our grand goal. We argue that such a rigid evaluation protocol creates a silent bottleneck in AI research. To remedy that, we define a probabilistic space of downstream tasks obtained by adopting a distribution of tasks and by defining Task Priors. Under this view, one can evaluate a model's performance over the set of all possible downstream tasks. Our framework is the first to provide answers to key questions such as (i) what is the average performance of my model over all possible downstream tasks weighted by the probability to encounter each task? or (ii) what is the variance of my model's performance across all downstream tasks under the defined Task Priors? Beyond establishing a new standard for evaluation, we believe that Task Priors will accelerate the pace of research in SSL - where downstream task evaluation is the sole qualitative signal that researchers have access to.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 176,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09875",
        "abs_url": "https://arxiv.org/abs/2507.09875",
        "pdf_url": "https://arxiv.org/pdf/2507.09875",
        "title": "Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition",
        "authors": [
            "Qinyuan Ye",
            "Robin Jia",
            "Xiang Ren"
        ],
        "comments": "Code: this https URL",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Large language models demonstrate the intriguing ability to perform unseen tasks via in-context learning. However, it remains unclear what mechanisms inside the model drive such task-level generalization. In this work, we approach this question through the lens of off-by-one addition (i.e., 1+1=3, 2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function as a second step. Leveraging circuit-style interpretability techniques such as path patching, we analyze the models' internal computations behind their notable performance and present three key findings. First, we uncover a function induction mechanism that explains the model's generalization from standard addition to off-by-one addition. This mechanism resembles the structure of the induction head mechanism found in prior work and elevates it to a higher level of abstraction. Second, we show that the induction of the +1 function is governed by multiple attention heads in parallel, each of which emits a distinct piece of the +1 function. Finally, we find that this function induction mechanism is reused in a broader range of tasks, including synthetic tasks such as shifted multiple-choice QA and algorithmic tasks such as base-8 addition. Overall, our findings offer deeper insights into how reusable and composable structures within language models enable task-level generalization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 177,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09876",
        "abs_url": "https://arxiv.org/abs/2507.09876",
        "pdf_url": "https://arxiv.org/pdf/2507.09876",
        "title": "ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models",
        "authors": [
            "Yongheng Zhang",
            "Xu Liu",
            "Ruihan Tao",
            "Qiguang Chen",
            "Hao Fei",
            "Wanxiang Che",
            "Libo Qin"
        ],
        "comments": "Accepted by ACM MM 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Video understanding plays a vital role in bridging low-level visual signals with high-level cognitive reasoning, and is fundamental to applications such as autonomous driving, embodied AI, and the broader pursuit of AGI. The rapid development of large language models (LLMs), particularly those utilizing Chain-of-Thought (CoT) technology, has significantly advanced video reasoning capabilities. However, current approaches primarily depend on textual information for reasoning, overlooking the visual modality in the actual video reasoning process. In contrast, humans naturally re-examine visual content while reasoning. Motivated by this, we introduce a novel video reasoning paradigm: Video-Text Interleaved CoT (ViTCoT), which facilitates more intuitive and cognitively aligned reasoning. To the end, first, we construct the Video-Text Interleaved Benchmark (ViTIB), which is created using MLLMs for key-video selection and manually verified. Furthermore, we extensively explore the potential of the ViTCoT paradigm in the video understanding field. Extensive experiments demonstrate that ViTCoT significantly enhances performance compared to the traditional text-only CoT paradigm and effectively activates more neuron values in MLLMs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 178,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09879",
        "abs_url": "https://arxiv.org/abs/2507.09879",
        "pdf_url": "https://arxiv.org/pdf/2507.09879",
        "title": "Covering a Few Submodular Constraints and Applications",
        "authors": [
            "Tanvi Bajpai",
            "Chandra Chekuri",
            "Pooja Kulkarni"
        ],
        "comments": "34 pages. Accepted to APPROX 2025",
        "subjects": "Data Structures and Algorithms (cs.DS); Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT)",
        "abstract": "We consider the problem of covering multiple submodular constraints. Given a finite ground set $N$, a cost function $c: N \\rightarrow \\mathbb{R}_+$, $r$ monotone submodular functions $f_1,f_2,\\ldots,f_r$ over $N$ and requirements $b_1,b_2,\\ldots,b_r$ the goal is to find a minimum cost subset $S \\subseteq N$ such that $f_i(S) \\ge b_i$ for $1 \\le i \\le r$. When $r=1$ this is the well-known Submodular Set Cover problem. Previous work \\cite{chekuri2022covering} considered the setting when $r$ is large and developed bi-criteria approximation algorithms, and approximation algorithms for the important special case when each $f_i$ is a weighted coverage function. These are fairly general models and capture several concrete and interesting problems as special cases. The approximation ratios for these problem are at least $\\Omega(\\log r)$ which is unavoidable when $r$ is part of the input. In this paper, motivated by some recent applications, we consider the problem when $r$ is a \\emph{fixed constant} and obtain two main results. For covering multiple submodular constraints we obtain a randomized bi-criteria approximation algorithm that for any given integer $\\alpha \\ge 1$ outputs a set $S$ such that $f_i(S) \\ge$ $(1-1/e^\\alpha -\\epsilon)b_i$ for each $i \\in [r]$ and $\\mathbb{E}[c(S)] \\le (1+\\epsilon)\\alpha \\cdot \\sf{OPT}$. Second, when the $f_i$ are weighted coverage functions from a deletion-closed set system we obtain a $(1+\\epsilon)$ $(\\frac{e}{e-1})$ $(1+\\beta)$-approximation where $\\beta$ is the approximation ratio for the underlying set cover instances via the natural LP. These results show that one can obtain nearly as good an approximation for any fixed $r$ as what one would achieve for $r=1$. We mention some applications that follow easily from these general results and anticipate more in the future.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 179,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09887",
        "abs_url": "https://arxiv.org/abs/2507.09887",
        "pdf_url": "https://arxiv.org/pdf/2507.09887",
        "title": "TolerantECG: A Foundation Model for Imperfect Electrocardiogram",
        "authors": [
            "Huynh Nguyen Dang",
            "Thang Pham",
            "Ngan Le",
            "Van Nguyen"
        ],
        "comments": "10 pages, 6 figures. Accepted to ACM Multimedia 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Signal Processing (eess.SP)",
        "abstract": "The electrocardiogram (ECG) is an essential and effective tool for diagnosing heart diseases. However, its effectiveness can be compromised by noise or unavailability of one or more leads of the standard 12-lead recordings, resulting in diagnostic errors or uncertainty. To address these challenges, we propose TolerantECG, a foundation model for ECG signals that is robust to noise and capable of functioning with arbitrary subsets of the standard 12-lead ECG. TolerantECG training combines contrastive and self-supervised learning frameworks to jointly learn ECG signal representations alongside their corresponding knowledge-retrieval-based text report descriptions and corrupted or lead-missing signals. Comprehensive benchmarking results demonstrate that TolerantECG consistently ranks as the best or second-best performer across various ECG signal conditions and class levels in the PTB-XL dataset, and achieves the highest performance on the MIT-BIH Arrhythmia Database.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 180,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09888",
        "abs_url": "https://arxiv.org/abs/2507.09888",
        "pdf_url": "https://arxiv.org/pdf/2507.09888",
        "title": "NeuTSFlow: Modeling Continuous Functions Behind Time Series Forecasting",
        "authors": [
            "Huibo Xu",
            "Likang Wu",
            "Xianquan Wang",
            "Haoning Dang",
            "Chun-Wun Cheng",
            "Angelica I Aviles-Rivero",
            "Qi Liu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "Time series forecasting is a fundamental task with broad applications, yet conventional methods often treat data as discrete sequences, overlooking their origin as noisy samples of continuous processes. Crucially, discrete noisy observations cannot uniquely determine a continuous function; instead, they correspond to a family of plausible functions. Mathematically, time series can be viewed as noisy observations of a continuous function family governed by a shared probability measure. Thus, the forecasting task can be framed as learning the transition from the historical function family to the future function family. This reframing introduces two key challenges: (1) How can we leverage discrete historical and future observations to learn the relationships between their underlying continuous functions? (2) How can we model the transition path in function space from the historical function family to the future function family? To address these challenges, we propose NeuTSFlow, a novel framework that leverages Neural Operators to facilitate flow matching for learning path of measure between historical and future function families. By parameterizing the velocity field of the flow in infinite-dimensional function spaces, NeuTSFlow moves beyond traditional methods that focus on dependencies at discrete points, directly modeling function-level features instead. Experiments on diverse forecasting tasks demonstrate NeuTSFlow's superior accuracy and robustness, validating the effectiveness of the function-family perspective.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 181,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09890",
        "abs_url": "https://arxiv.org/abs/2507.09890",
        "pdf_url": "https://arxiv.org/pdf/2507.09890",
        "title": "Soft Graph Clustering for single-cell RNA Sequencing Data",
        "authors": [
            "Ping Xu",
            "Pengfei Wang",
            "Zhiyuan Ning",
            "Meng Xiao",
            "Min Wu",
            "Yuanchun Zhou"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Genomics (q-bio.GN)",
        "abstract": "Clustering analysis is fundamental in single-cell RNA sequencing (scRNA-seq) data analysis for elucidating cellular heterogeneity and diversity. Recent graph-based scRNA-seq clustering methods, particularly graph neural networks (GNNs), have significantly improved in tackling the challenges of high-dimension, high-sparsity, and frequent dropout events that lead to ambiguous cell population boundaries. However, their reliance on hard graph constructions derived from thresholded similarity matrices presents challenges:(i) The simplification of intercellular relationships into binary edges (0 or 1) by applying thresholds, which restricts the capture of continuous similarity features among cells and leads to significant information loss.(ii) The presence of significant inter-cluster connections within hard graphs, which can confuse GNN methods that rely heavily on graph structures, potentially causing erroneous message propagation and biased clustering outcomes. To tackle these challenges, we introduce scSGC, a Soft Graph Clustering for single-cell RNA sequencing data, which aims to more accurately characterize continuous similarities among cells through non-binary edge weights, thereby mitigating the limitations of rigid data structures. The scSGC framework comprises three core components: (i) a zero-inflated negative binomial (ZINB)-based feature autoencoder; (ii) a dual-channel cut-informed soft graph embedding module; and (iii) an optimal transport-based clustering optimization module. Extensive experiments across ten datasets demonstrate that scSGC outperforms 13 state-of-the-art clustering models in clustering accuracy, cell type annotation, and computational efficiency. These results highlight its substantial potential to advance scRNA-seq data analysis and deepen our understanding of cellular heterogeneity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 182,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09891",
        "abs_url": "https://arxiv.org/abs/2507.09891",
        "pdf_url": "https://arxiv.org/pdf/2507.09891",
        "title": "Sequence-Model-Guided Measurement Selection for Quantum State Learning",
        "authors": [
            "Jiaxin Huang",
            "Yan Zhu",
            "Giulio Chiribella",
            "Ya-Dong Wu"
        ],
        "comments": "",
        "subjects": "Quantum Physics (quant-ph); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Characterization of quantum systems from experimental data is a central problem in quantum science and technology. But which measurements should be used to gather data in the first place? While optimal measurement choices can be worked out for small quantum systems, the optimization becomes intractable as the system size grows large. To address this problem, we introduce a deep neural network with a sequence model architecture that searches for efficient measurement choices in a data-driven, adaptive manner. The model can be applied to a variety of tasks, including the prediction of linear and nonlinear properties of quantum states, as well as state clustering and state tomography tasks. In all these tasks, we find that the measurement choices identified by our neural network consistently outperform the uniformly random choice. Intriguingly, for topological quantum systems, our model tends to recommend measurements at the system's boundaries, even when the task is to predict bulk properties. This behavior suggests that the neural network may have independently discovered a connection between boundaries and bulk, without having been provided any built-in knowledge of quantum physics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 183,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09898",
        "abs_url": "https://arxiv.org/abs/2507.09898",
        "pdf_url": "https://arxiv.org/pdf/2507.09898",
        "title": "Advanced U-Net Architectures with CNN Backbones for Automated Lung Cancer Detection and Segmentation in Chest CT Images",
        "authors": [
            "Alireza Golkarieha",
            "Kiana Kiashemshakib",
            "Sajjad Rezvani Boroujenic",
            "Nasibeh Asadi Isakand"
        ],
        "comments": "This manuscript has 20 pages and 10 figures. It is submitted to the Journal 'Scientific Reports'",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "This study investigates the effectiveness of U-Net architectures integrated with various convolutional neural network (CNN) backbones for automated lung cancer detection and segmentation in chest CT images, addressing the critical need for accurate diagnostic tools in clinical settings. A balanced dataset of 832 chest CT images (416 cancerous and 416 non-cancerous) was preprocessed using Contrast Limited Adaptive Histogram Equalization (CLAHE) and resized to 128x128 pixels. U-Net models were developed with three CNN backbones: ResNet50, VGG16, and Xception, to segment lung regions. After segmentation, CNN-based classifiers and hybrid models combining CNN feature extraction with traditional machine learning classifiers (Support Vector Machine, Random Forest, and Gradient Boosting) were evaluated using 5-fold cross-validation. Metrics included accuracy, precision, recall, F1-score, Dice coefficient, and ROC-AUC. U-Net with ResNet50 achieved the best performance for cancerous lungs (Dice: 0.9495, Accuracy: 0.9735), while U-Net with VGG16 performed best for non-cancerous segmentation (Dice: 0.9532, Accuracy: 0.9513). For classification, the CNN model using U-Net with Xception achieved 99.1 percent accuracy, 99.74 percent recall, and 99.42 percent F1-score. The hybrid CNN-SVM-Xception model achieved 96.7 percent accuracy and 97.88 percent F1-score. Compared to prior methods, our framework consistently outperformed existing models. In conclusion, combining U-Net with advanced CNN backbones provides a powerful method for both segmentation and classification of lung cancer in CT scans, supporting early diagnosis and clinical decision-making.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 184,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09901",
        "abs_url": "https://arxiv.org/abs/2507.09901",
        "pdf_url": "https://arxiv.org/pdf/2507.09901",
        "title": "Large Population Models",
        "authors": [
            "Ayush Chopra"
        ],
        "comments": "Aggregation of Several Papers from MIT PhD Research. this http URL",
        "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI)",
        "abstract": "Many of society's most pressing challenges, from pandemic response to supply chain disruptions to climate adaptation, emerge from the collective behavior of millions of autonomous agents making decisions over time. Large Population Models (LPMs) offer an approach to understand these complex systems by simulating entire populations with realistic behaviors and interactions at unprecedented scale. LPMs extend traditional modeling approaches through three key innovations: computational methods that efficiently simulate millions of agents simultaneously, mathematical frameworks that learn from diverse real-world data streams, and privacy-preserving communication protocols that bridge virtual and physical environments. This allows researchers to observe how agent behavior aggregates into system-level outcomes and test interventions before real-world implementation. While current AI advances primarily focus on creating \"digital humans\" with sophisticated individual capabilities, LPMs develop \"digital societies\" where the richness of interactions reveals emergent phenomena. By bridging individual agent behavior and population-scale dynamics, LPMs offer a complementary path in AI research illuminating collective intelligence and providing testing grounds for policies and social innovations before real-world deployment. We discuss the technical foundations and some open problems here. LPMs are implemented by the AgentTorch framework (this http URL)",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 185,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09924",
        "abs_url": "https://arxiv.org/abs/2507.09924",
        "pdf_url": "https://arxiv.org/pdf/2507.09924",
        "title": "MixLoRA-DSI: Dynamically Expandable Mixture-of-LoRA Experts for Rehearsal-Free Generative Retrieval over Dynamic Corpora",
        "authors": [
            "Tuan-Luc Huynh",
            "Thuy-Trang Vu",
            "Weiqing Wang",
            "Trung Le",
            "Dragan Gašević",
            "Yuan-Fang Li",
            "Thanh-Toan Do"
        ],
        "comments": "",
        "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Continually updating model-based indexes in generative retrieval with new documents remains challenging, as full retraining is computationally expensive and impractical under resource constraints. We propose MixLoRA-DSI, a novel framework that combines an expandable mixture of Low-Rank Adaptation experts with a layer-wise out-of-distribution (OOD)-driven expansion strategy. Instead of allocating new experts for each new corpus, our proposed expansion strategy enables sublinear parameter growth by selectively introducing new experts only when significant number of OOD documents are detected. Experiments on NQ320k and MS MARCO Passage demonstrate that MixLoRA-DSI outperforms full-model update baselines, with minimal parameter overhead and substantially lower training costs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 186,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09929",
        "abs_url": "https://arxiv.org/abs/2507.09929",
        "pdf_url": "https://arxiv.org/pdf/2507.09929",
        "title": "Aligning Generative Speech Enhancement with Human Preferences via Direct Preference Optimization",
        "authors": [
            "Haoyang Li",
            "Nana Hou",
            "Yuchen Hu",
            "Jixun Yao",
            "Sabato Marco Siniscalchi",
            "Eng Siong Chng"
        ],
        "comments": "",
        "subjects": "Audio and Speech Processing (eess.AS); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "This work investigates speech enhancement (SE) from the perspective of language models (LMs). We propose a novel method that leverages Direct Preference Optimization (DPO) to improve the perceptual quality of enhanced speech. Using UTMOS, a neural MOS prediction model, as a proxy for human ratings, our approach guides optimization toward perceptually preferred outputs. This differs from existing LM-based SE methods that focus on maximizing the likelihood of clean speech tokens, which may misalign with human perception and degrade quality despite low prediction error. Experiments on the 2020 Deep Noise Suppression Challenge test sets demonstrate that applying DPO to a pretrained LM-based SE model yields consistent improvements across various speech quality metrics, with relative gains of up to 56%. To our knowledge, this is the first application of DPO to SE and the first to incorporate proxy perceptual feedback into LM-based SE training, pointing to a promising direction for perceptually aligned SE.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 187,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09931",
        "abs_url": "https://arxiv.org/abs/2507.09931",
        "pdf_url": "https://arxiv.org/pdf/2507.09931",
        "title": "Mechanistic Interpretability of LoRA-Adapted Language Models for Nuclear Reactor Safety Applications",
        "authors": [
            "Yoon Pyo Lee"
        ],
        "comments": "Submitted to Nuclear Technology. 22 pages, 2 tables, 4 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "The integration of Large Language Models (LLMs) into safety-critical domains, such as nuclear engineering, necessitates a deep understanding of their internal reasoning processes. This paper presents a novel methodology for interpreting how an LLM encodes and utilizes domain-specific knowledge, using a Boiling Water Reactor system as a case study. We adapted a general-purpose LLM (Gemma-3-1b-it) to the nuclear domain using a parameter-efficient fine-tuning technique known as Low-Rank Adaptation. By comparing the neuron activation patterns of the base model to those of the fine-tuned model, we identified a sparse set of neurons whose behavior was significantly altered during the adaptation process. To probe the causal role of these specialized neurons, we employed a neuron silencing technique. Our results demonstrate that while silencing most of these specialized neurons individually did not produce a statistically significant effect, deactivating the entire group collectively led to a statistically significant degradation in task performance. Qualitative analysis further revealed that silencing these neurons impaired the model's ability to generate detailed, contextually accurate technical information. This paper provides a concrete methodology for enhancing the transparency of an opaque black-box model, allowing domain expertise to be traced to verifiable neural circuits. This offers a pathway towards achieving nuclear-grade artificial intelligence (AI) assurance, addressing the verification and validation challenges mandated by nuclear regulatory frameworks (e.g., 10 CFR 50 Appendix B), which have limited AI deployment in safety-critical nuclear operations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 188,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09935",
        "abs_url": "https://arxiv.org/abs/2507.09935",
        "pdf_url": "https://arxiv.org/pdf/2507.09935",
        "title": "Enhancing Retrieval Augmented Generation with Hierarchical Text Segmentation Chunking",
        "authors": [
            "Hai Toan Nguyen",
            "Tien Dat Nguyen",
            "Viet Ha Nguyen"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Retrieval-Augmented Generation (RAG) systems commonly use chunking strategies for retrieval, which enhance large language models (LLMs) by enabling them to access external knowledge, ensuring that the retrieved information is up-to-date and domain-specific. However, traditional methods often fail to create chunks that capture sufficient semantic meaning, as they do not account for the underlying textual structure. This paper proposes a novel framework that enhances RAG by integrating hierarchical text segmentation and clustering to generate more meaningful and semantically coherent chunks. During inference, the framework retrieves information by leveraging both segment-level and cluster-level vector representations, thereby increasing the likelihood of retrieving more precise and contextually relevant information. Evaluations on the NarrativeQA, QuALITY, and QASPER datasets indicate that the proposed method achieved improved results compared to traditional chunking techniques.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 189,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09937",
        "abs_url": "https://arxiv.org/abs/2507.09937",
        "pdf_url": "https://arxiv.org/pdf/2507.09937",
        "title": "Memorization Sinks: Isolating Memorization during LLM Training",
        "authors": [
            "Gaurav R. Ghosal",
            "Pratyush Maini",
            "Aditi Raghunathan"
        ],
        "comments": "Accepted at the 2025 International Conference of Machine Learning",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Large language models are susceptible to memorizing repeated sequences, posing privacy and copyright concerns. A popular mitigation strategy is to remove memorized information from specific neurons post-hoc. However, such approaches have shown limited success so far. In a controlled setting, we show that the memorization of natural sequences (those that resemble linguistically plausible text) become mechanistically entangled with general language abilities, thereby becoming challenging to remove post-hoc. In this work, we put forward a new paradigm of MemSinks that promotes isolation of memorization by design. We leverage a sequence identifier that activates a unique set of memorization neurons for each sequence across repetitions. By analyzing the dynamics of learning and forgetting, we argue that MemSinks facilitates isolation of memorized content, making it easier to remove without compromising general language capabilities. We implement MemSinks at the billion-parameter and billion-token scale, and observe both effective isolation and strong generalization. To our knowledge, this is the first proof-of-concept on real data demonstrating that simultaneous generalization and isolation is achievable. We open-source our code at this http URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 190,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09950",
        "abs_url": "https://arxiv.org/abs/2507.09950",
        "pdf_url": "https://arxiv.org/pdf/2507.09950",
        "title": "Can GPT-4o mini and Gemini 2.0 Flash Predict Fine-Grained Fashion Product Attributes? A Zero-Shot Analysis",
        "authors": [
            "Shubham Shukla",
            "Kunal Sonalkar"
        ],
        "comments": "11 pages, 2 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "The fashion retail business is centered around the capacity to comprehend products. Product attribution helps in comprehending products depending on the business process. Quality attribution improves the customer experience as they navigate through millions of products offered by a retail website. It leads to well-organized product catalogs. In the end, product attribution directly impacts the 'discovery experience' of the customer. Although large language models (LLMs) have shown remarkable capabilities in understanding multimodal data, their performance on fine-grained fashion attribute recognition remains under-explored. This paper presents a zero-shot evaluation of state-of-the-art LLMs that balance performance with speed and cost efficiency, mainly GPT-4o-mini and Gemini 2.0 Flash. We have used the dataset DeepFashion-MultiModal (this https URL) to evaluate these models in the attribution tasks of fashion products. Our study evaluates these models across 18 categories of fashion attributes, offering insight into where these models excel. We only use images as the sole input for product information to create a constrained environment. Our analysis shows that Gemini 2.0 Flash demonstrates the strongest overall performance with a macro F1 score of 56.79% across all attributes, while GPT-4o-mini scored a macro F1 score of 43.28%. Through detailed error analysis, our findings provide practical insights for deploying these LLMs in production e-commerce product attribution-related tasks and highlight the need for domain-specific fine-tuning approaches. This work also lays the groundwork for future research in fashion AI and multimodal attribute extraction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 191,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09966",
        "abs_url": "https://arxiv.org/abs/2507.09966",
        "pdf_url": "https://arxiv.org/pdf/2507.09966",
        "title": "A Brain Tumor Segmentation Method Based on CLIP and 3D U-Net with Cross-Modal Semantic Guidance and Multi-Level Feature Fusion",
        "authors": [
            "Mingda Zhang"
        ],
        "comments": "13 pages,6 figures",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Precise segmentation of brain tumors from magnetic resonance imaging (MRI) is essential for neuro-oncology diagnosis and treatment planning. Despite advances in deep learning methods, automatic segmentation remains challenging due to tumor morphological heterogeneity and complex three-dimensional spatial relationships. Current techniques primarily rely on visual features extracted from MRI sequences while underutilizing semantic knowledge embedded in medical reports. This research presents a multi-level fusion architecture that integrates pixel-level, feature-level, and semantic-level information, facilitating comprehensive processing from low-level data to high-level concepts. The semantic-level fusion pathway combines the semantic understanding capabilities of Contrastive Language-Image Pre-training (CLIP) models with the spatial feature extraction advantages of 3D U-Net through three mechanisms: 3D-2D semantic bridging, cross-modal semantic guidance, and semantic-based attention mechanisms. Experimental validation on the BraTS 2020 dataset demonstrates that the proposed model achieves an overall Dice coefficient of 0.8567, representing a 4.8% improvement compared to traditional 3D U-Net, with a 7.3% Dice coefficient increase in the clinically important enhancing tumor (ET) region.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 192,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09973",
        "abs_url": "https://arxiv.org/abs/2507.09973",
        "pdf_url": "https://arxiv.org/pdf/2507.09973",
        "title": "Tiny Reward Models",
        "authors": [
            "Sarah Pan"
        ],
        "comments": "2025 ICML Efficient Systems for Foundation Models Workshop",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Large decoder-based language models have become the dominant architecture for reward modeling in reinforcement learning from human feedback (RLHF). However, as reward models are increasingly deployed in test-time strategies, their inference costs become a growing concern. We present TinyRM, a family of small, bidirectional masked language models (MLMs) with as few as 400 million parameters, that rival the capabilities of models over 175 times larger on reasoning and safety preference modeling tasks. TinyRM combines FLAN-style prompting, Directional Low-Rank Adaptation (DoRA), and layer freezing to achieve strong performance on RewardBench, despite using significantly fewer resources. Our experiments suggest that small models benefit from domain-specific tuning strategies, particularly in reasoning, where lightweight finetuning methods are especially effective. While challenges remain in building generalist models and conversational preference modeling, our preliminary results highlight the promise of lightweight bidirectional architectures as efficient, scalable alternatives for preference modeling.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 193,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09985",
        "abs_url": "https://arxiv.org/abs/2507.09985",
        "pdf_url": "https://arxiv.org/pdf/2507.09985",
        "title": "Demonstrating the Octopi-1.5 Visual-Tactile-Language Model",
        "authors": [
            "Samson Yu",
            "Kelvin Lin",
            "Harold Soh"
        ],
        "comments": "Published at R:SS 2025",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
        "abstract": "Touch is recognized as a vital sense for humans and an equally important modality for robots, especially for dexterous manipulation, material identification, and scenarios involving visual occlusion. Building upon very recent work in touch foundation models, this demonstration will feature Octopi-1.5, our latest visual-tactile-language model. Compared to its predecessor, Octopi-1.5 introduces the ability to process tactile signals from multiple object parts and employs a simple retrieval-augmented generation (RAG) module to improve performance on tasks and potentially learn new objects on-the-fly. The system can be experienced live through a new handheld tactile-enabled interface, the TMI, equipped with GelSight and TAC-02 tactile sensors. This convenient and accessible setup allows users to interact with Octopi-1.5 without requiring a robot. During the demonstration, we will showcase Octopi-1.5 solving tactile inference tasks by leveraging tactile inputs and commonsense knowledge. For example, in a Guessing Game, Octopi-1.5 will identify objects being grasped and respond to follow-up queries about how to handle it (e.g., recommending careful handling for soft fruits). We also plan to demonstrate Octopi-1.5's RAG capabilities by teaching it new items. With live interactions, this demonstration aims to highlight both the progress and limitations of VTLMs such as Octopi-1.5 and to foster further interest in this exciting field. Code for Octopi-1.5 and design files for the TMI gripper are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 194,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09990",
        "abs_url": "https://arxiv.org/abs/2507.09990",
        "pdf_url": "https://arxiv.org/pdf/2507.09990",
        "title": "Differentially Private Federated Low Rank Adaptation Beyond Fixed-Matrix",
        "authors": [
            "Ming Wen",
            "Jiaqi Zhu",
            "Yuedong Xu",
            "Yipeng Zhou",
            "Dingding Han"
        ],
        "comments": "23 pages, NeurIPS 2025 under review",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
        "abstract": "Large language models (LLMs) typically require fine-tuning for domain-specific tasks, and LoRA offers a computationally efficient approach by training low-rank adapters. LoRA is also communication-efficient for federated LLMs when multiple users collaboratively fine-tune a global LLM model without sharing their proprietary raw data. However, even the transmission of local adapters between a server and clients risks serious privacy leakage. Applying differential privacy (DP) to federated LoRA encounters a dilemma: adding noise to both adapters amplifies synthetic noise on the model, while fixing one adapter impairs the learnability of fine-tuning. In this paper, we propose FedASK (Differentially Private Federated Low Rank Adaptation with Double Sketching) , a novel federated LoRA framework to enable effective updating of both low-rank adapters with robust differential privacy. Inspired by randomized SVD, our key idea is a two-stage sketching pipeline. This pipeline first aggregates carefully sketched, privacy-preserving local updates, and then reconstructs the global matrices on the server to facilitate effective updating of both adapters. We theoretically prove FedASK's differential privacy guarantee and its exact aggregation property. Comprehensive experiments demonstrate that FedASK consistently outperforms baseline methods across a variety of privacy settings and data distributions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 195,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.09992",
        "abs_url": "https://arxiv.org/abs/2507.09992",
        "pdf_url": "https://arxiv.org/pdf/2507.09992",
        "title": "Evolution of Fear and Social Rewards in Prey-Predator Relationship",
        "authors": [
            "Yuji Kanagawa",
            "Kenji Doya"
        ],
        "comments": "Preprint. Under review",
        "subjects": "Populations and Evolution (q-bio.PE); Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)",
        "abstract": "Fear is a critical brain function for detecting danger and learning to avoid specific stimuli that can lead to danger. While fear is believed to have evolved under pressure from predators, experimentally reproducing the evolution is challenging. To investigate the relationship between environmental conditions, the evolution of fear, and the evolution of other rewards, such as food reward and social reward, we developed a distributed evolutionary simulation. In our simulation, prey and predator agents co-evolve their innate reward functions, including a possibly fear-like term for observing predators, and learn behaviors via reinforcement learning. Surprisingly, our simulation revealed that social reward for observing the same species is more important for prey to survive, and fear-like negative reward for observing predators evolves only after acquiring social reward. We also found that the predator with increased hunting ability (larger mouth) amplified fear emergence, but also that fear evolution is more stable with non-evolving predators that are bad at chasing prey. Additionally, unlike for predators, we found that positive rewards evolve in opposition to fear for stationary threats, as areas with abundant leftover food develop around them. These findings suggest that fear and social reward have had a complex interplay with each other through evolution, along with the nature of predators and threats.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 196,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10015",
        "abs_url": "https://arxiv.org/abs/2507.10015",
        "pdf_url": "https://arxiv.org/pdf/2507.10015",
        "title": "(Almost) Free Modality Stitching of Foundation Models",
        "authors": [
            "Jaisidh Singh",
            "Diganta Misra",
            "Boris Knyazev",
            "Antonio Orvieto"
        ],
        "comments": "Pre-print",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Foundation multi-modal models are often designed by stitching of multiple existing pretrained uni-modal models: for example, an image classifier with an text model. This stitching process is performed by training a connector module that aims to align the representation spaces of these uni-modal models towards a multi-modal objective. However, given the complexity of training such connectors on large scale web-based datasets coupled with the ever-increasing number of available pretrained uni-modal models, the task of uni-modal models selection and subsequent connector module training becomes computationally demanding. To address this under-studied critical problem, we propose Hypernetwork Model Alignment (Hyma), a novel all-in-one solution for optimal uni-modal model selection and connector training by leveraging hypernetworks. Specifically, our framework utilizes the parameter prediction capability of a hypernetwork to obtain jointly trained connector modules for $N \\times M$ combinations of uni-modal models. In our experiments, Hyma reduces the cost of searching for the best performing uni-modal model pair by $10\\times$, while matching the ranking and trained connector performance obtained via grid search across a suite of diverse multi-modal benchmarks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 197,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10056",
        "abs_url": "https://arxiv.org/abs/2507.10056",
        "pdf_url": "https://arxiv.org/pdf/2507.10056",
        "title": "Lightweight Model for Poultry Disease Detection from Fecal Images Using Multi-Color Space Feature Optimization and Machine Learning",
        "authors": [
            "A. K. M. Shoriful Islam",
            "Md. Rakib Hassan",
            "Macbah Uddin",
            "Md. Shahidur Rahman"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Poultry farming is a vital component of the global food supply chain, yet it remains highly vulnerable to infectious diseases such as coccidiosis, salmonellosis, and Newcastle disease. This study proposes a lightweight machine learning-based approach to detect these diseases by analyzing poultry fecal images. We utilize multi-color space feature extraction (RGB, HSV, LAB) and explore a wide range of color, texture, and shape-based descriptors, including color histograms, local binary patterns (LBP), wavelet transforms, and edge detectors. Through a systematic ablation study and dimensionality reduction using PCA and XGBoost feature selection, we identify a compact global feature set that balances accuracy and computational efficiency. An artificial neural network (ANN) classifier trained on these features achieved 95.85% accuracy while requiring no GPU and only 638 seconds of execution time in Google Colab. Compared to deep learning models such as Xception and MobileNetV3, our proposed model offers comparable accuracy with drastically lower resource usage. This work demonstrates a cost-effective, interpretable, and scalable alternative to deep learning for real-time poultry disease detection in low-resource agricultural settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 198,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10057",
        "abs_url": "https://arxiv.org/abs/2507.10057",
        "pdf_url": "https://arxiv.org/pdf/2507.10057",
        "title": "PRISM: Fine-Grained Paper-to-Paper Retrieval with Multi-Aspect-Aware Query Optimization",
        "authors": [
            "Sangwoo Park",
            "Jinheon Baek",
            "Soyeong Jeong",
            "Sung Ju Hwang"
        ],
        "comments": "",
        "subjects": "Information Retrieval (cs.IR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Scientific paper retrieval, particularly framed as document-to-document retrieval, aims to identify relevant papers in response to a long-form query paper, rather than a short query string. Previous approaches to this task have focused on abstracts, embedding them into dense vectors as surrogates for full documents and calculating similarity across them, although abstracts provide only sparse and high-level summaries. To address this, we propose PRISM, a novel document-to-document retrieval method that introduces multiple, fine-grained representations for both the query and candidate papers. In particular, each query paper is decomposed into multiple aspect-specific views and individually embedded, which are then matched against candidate papers similarity segmented to consider their multifaceted dimensions. Moreover, we present SciFullBench, a novel benchmark in which the complete and segmented context of full papers for both queries and candidates is available. Then, experimental results show that PRISM improves performance by an average of 4.3% over existing retrieval baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 199,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10073",
        "abs_url": "https://arxiv.org/abs/2507.10073",
        "pdf_url": "https://arxiv.org/pdf/2507.10073",
        "title": "Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires",
        "authors": [
            "Simon Münker"
        ],
        "comments": "15pages, 1 figure, 2 tables",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Are AI systems truly representing human values, or merely averaging across them? Our study suggests a concerning reality: Large Language Models (LLMs) fail to represent diverse cultural moral frameworks despite their linguistic capabilities. We expose significant gaps between AI-generated and human moral intuitions by applying the Moral Foundations Questionnaire across 19 cultural contexts. Comparing multiple state-of-the-art LLMs' origins against human baseline data, we find these models systematically homogenize moral diversity. Surprisingly, increased model size doesn't consistently improve cultural representation fidelity. Our findings challenge the growing use of LLMs as synthetic populations in social science research and highlight a fundamental limitation in current AI alignment approaches. Without data-driven alignment beyond prompting, these systems cannot capture the nuanced, culturally-specific moral intuitions. Our results call for more grounded alignment objectives and evaluation metrics to ensure AI systems represent diverse human values rather than flattening the moral landscape.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 200,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10075",
        "abs_url": "https://arxiv.org/abs/2507.10075",
        "pdf_url": "https://arxiv.org/pdf/2507.10075",
        "title": "TGLD: A Trust-Aware Game-Theoretic Lane-Changing Decision Framework for Automated Vehicles in Heterogeneous Traffic",
        "authors": [
            "Jie Pan",
            "Tianyi Wang",
            "Yangyang Wang",
            "Junfeng Jiao",
            "Christian Claudel"
        ],
        "comments": "6 pages, 7 figures, accepted for IEEE International Conference on Intelligent Transportation Systems (ITSC) 2025",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Systems and Control (eess.SY)",
        "abstract": "Automated vehicles (AVs) face a critical need to adopt socially compatible behaviors and cooperate effectively with human-driven vehicles (HVs) in heterogeneous traffic environment. However, most existing lane-changing frameworks overlook HVs' dynamic trust levels, limiting their ability to accurately predict human driver behaviors. To address this gap, this study proposes a trust-aware game-theoretic lane-changing decision (TGLD) framework. First, we formulate a multi-vehicle coalition game, incorporating fully cooperative interactions among AVs and partially cooperative behaviors from HVs informed by real-time trust evaluations. Second, we develop an online trust evaluation method to dynamically estimate HVs' trust levels during lane-changing interactions, guiding AVs to select context-appropriate cooperative maneuvers. Lastly, social compatibility objectives are considered by minimizing disruption to surrounding vehicles and enhancing the predictability of AV behaviors, thereby ensuring human-friendly and context-adaptive lane-changing strategies. A human-in-the-loop experiment conducted in a highway on-ramp merging scenario validates our TGLD approach. Results show that AVs can effectively adjust strategies according to different HVs' trust levels and driving styles. Moreover, incorporating a trust mechanism significantly improves lane-changing efficiency, maintains safety, and contributes to transparent and adaptive AV-HV interactions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 201,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10085",
        "abs_url": "https://arxiv.org/abs/2507.10085",
        "pdf_url": "https://arxiv.org/pdf/2507.10085",
        "title": "Enhancing Chain-of-Thought Reasoning with Critical Representation Fine-tuning",
        "authors": [
            "Chenxi Huang",
            "Shaotian Yan",
            "Liang Xie",
            "Binbin Lin",
            "Sinan Fan",
            "Yue Xin",
            "Deng Cai",
            "Chen Shen",
            "Jieping Ye"
        ],
        "comments": "Accepted by ACL 2025",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Representation Fine-tuning (ReFT), a recently proposed Parameter-Efficient Fine-Tuning (PEFT) method, has attracted widespread attention for significantly improving parameter efficiency by editing representation space alone. In this work, we investigate applying ReFT to complex reasoning tasks. However, directly using the native ReFT method, which modifies fixed representations at the beginning and end of each layer, yields suboptimal performance, as these fixed-position representations have uncertain impact on the outputs. We observe that, in complex reasoning tasks, there often exist certain critical representations. These representations either integrate significant information from preceding layers or regulate subsequent layer representations. Through layer-by-layer propagation, they exert a substantial influence on the final output. Naturally, fine-tuning these critical representations has the potential to greatly enhance reasoning performance. Building upon these insights, we propose Critical Representation Fine-Tuning (CRFT), a novel method that identifies and optimizes these critical representations through information flow analysis. CRFT operates within a supervised learning framework, dynamically optimizing critical representations in a low-rank linear subspace while freezing the base model. The effectiveness and efficiency of our method are validated across eight benchmarks for arithmetic and commonsense reasoning, using LLaMA and Mistral model families. Furthermore, our method also adapts effectively to few-shot settings, boosting one-shot accuracy by 16.4%. Our work highlights the untapped potential of representation-level optimization for CoT reasoning, offering a lightweight yet powerful alternative to traditional PEFT methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 202,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10120",
        "abs_url": "https://arxiv.org/abs/2507.10120",
        "pdf_url": "https://arxiv.org/pdf/2507.10120",
        "title": "A Variance-Reduced Cubic-Regularized Newton for Policy Optimization",
        "authors": [
            "Cheng Sun",
            "Zhen Zhang",
            "Shaofu Yang"
        ],
        "comments": "13 pages, 1 figure",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "In this paper, we study a second-order approach to policy optimization in reinforcement learning. Existing second-order methods often suffer from suboptimal sample complexity or rely on unrealistic assumptions about importance sampling. To overcome these limitations, we propose VR-CR-PN, a variance-reduced cubic-regularized policy Newton algorithm. To the best of our knowledge, this is the first algorithm that integrates Hessian-aided variance reduction with second-order policy optimization, effectively addressing the distribution shift problem and achieving best-known sample complexity under general nonconvex conditions but without the need for importance sampling. We theoretically establish that VR-CR-PN achieves a sample complexity of $\\tilde{\\mathcal{O}}(\\epsilon^{-3})$ to reach an $\\epsilon$-second-order stationary point, significantly improving upon the previous best result of $\\tilde{\\mathcal{O}}(\\epsilon^{-3.5})$ under comparable assumptions. As an additional contribution, we introduce a novel Hessian estimator for the expected return function, which admits a uniform upper bound independent of the horizon length $H$, allowing the algorithm to achieve horizon-independent sample complexity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 203,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10127",
        "abs_url": "https://arxiv.org/abs/2507.10127",
        "pdf_url": "https://arxiv.org/pdf/2507.10127",
        "title": "Taming Modern Point Tracking for Speckle Tracking Echocardiography via Impartial Motion",
        "authors": [
            "Md Abulkalam Azad",
            "John Nyberg",
            "Håvard Dalen",
            "Bjørnar Grenne",
            "Lasse Lovstakken",
            "Andreas Østvik"
        ],
        "comments": "Accepted to CVAMD workshop at ICCV 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Accurate motion estimation for tracking deformable tissues in echocardiography is essential for precise cardiac function measurements. While traditional methods like block matching or optical flow struggle with intricate cardiac motion, modern point tracking approaches remain largely underexplored in this domain. This work investigates the potential of state-of-the-art (SOTA) point tracking methods for ultrasound, with a focus on echocardiography. Although these novel approaches demonstrate strong performance in general videos, their effectiveness and generalizability in echocardiography remain limited. By analyzing cardiac motion throughout the heart cycle in real B-mode ultrasound videos, we identify that a directional motion bias across different views is affecting the existing training strategies. To mitigate this, we refine the training procedure and incorporate a set of tailored augmentations to reduce the bias and enhance tracking robustness and generalization through impartial cardiac motion. We also propose a lightweight network leveraging multi-scale cost volumes from spatial context alone to challenge the advanced spatiotemporal point tracking models. Experiments demonstrate that fine-tuning with our strategies significantly improves models' performances over their baselines, even for out-of-distribution (OOD) cases. For instance, EchoTracker boosts overall position accuracy by 60.7% and reduces median trajectory error by 61.5% across heart cycle phases. Interestingly, several point tracking models fail to outperform our proposed simple model in terms of tracking accuracy and generalization, reflecting their limitations when applied to echocardiography. Nevertheless, clinical evaluation reveals that these methods improve GLS measurements, aligning more closely with expert-validated, semi-automated tools and thus demonstrating better reproducibility in real-world applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 204,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10132",
        "abs_url": "https://arxiv.org/abs/2507.10132",
        "pdf_url": "https://arxiv.org/pdf/2507.10132",
        "title": "Wavelet-Enhanced Neural ODE and Graph Attention for Interpretable Energy Forecasting",
        "authors": [
            "Usman Gani Joy"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "Accurate forecasting of energy demand and supply is critical for optimizing sustainable energy systems, yet it is challenged by the variability of renewable sources and dynamic consumption patterns. This paper introduces a neural framework that integrates continuous-time Neural Ordinary Differential Equations (Neural ODEs), graph attention, multi-resolution wavelet transformations, and adaptive learning of frequencies to address the issues of time series prediction. The model employs a robust ODE solver, using the Runge-Kutta method, paired with graph-based attention and residual connections to better understand both structural and temporal patterns. Through wavelet-based feature extraction and adaptive frequency modulation, it adeptly captures and models diverse, multi-scale temporal dynamics. When evaluated across seven diverse datasets: ETTh1, ETTh2, ETTm1, ETTm2 (electricity transformer temperature), and Waste, Solar, and Hydro (renewable energy), this architecture consistently outperforms state-of-the-art baselines in various forecasting metrics, proving its robustness in capturing complex temporal dependencies. Furthermore, the model enhances interpretability through SHAP analysis, making it suitable for sustainable energy applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 205,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10133",
        "abs_url": "https://arxiv.org/abs/2507.10133",
        "pdf_url": "https://arxiv.org/pdf/2507.10133",
        "title": "Extending Defeasibility for Propositional Standpoint Logics",
        "authors": [
            "Nicholas Leisegang",
            "Thomas Meyer",
            "Ivan Varzinczak"
        ],
        "comments": "",
        "subjects": "Logic in Computer Science (cs.LO); Artificial Intelligence (cs.AI)",
        "abstract": "In this paper, we introduce a new defeasible version of propositional standpoint logic by integrating Kraus et al.'s defeasible conditionals, Britz and Varzinczak's notions of defeasible necessity and distinct possibility, along with Leisegang et al.'s approach to defeasibility into the standpoint logics of Gómez Álvarez and Rudolph. The resulting logical framework allows for the expression of defeasibility on the level of implications, standpoint modal operators, and standpoint-sharpening statements. We provide a preferential semantics for this extended language and propose a tableaux calculus, which is shown to be sound and complete with respect to preferential entailment. We also establish the computational complexity of the tableaux procedure to be in PSpace.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 206,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10136",
        "abs_url": "https://arxiv.org/abs/2507.10136",
        "pdf_url": "https://arxiv.org/pdf/2507.10136",
        "title": "A PBN-RL-XAI Framework for Discovering a \"Hit-and-Run\" Therapeutic Strategy in Melanoma",
        "authors": [
            "Zhonglin Liu"
        ],
        "comments": "9 pages, 5 figures. Submitted to the IEEE International Conference on Bioinformatics and Biomedicine (BIBM) 2025. Code is available at this https URL",
        "subjects": "Quantitative Methods (q-bio.QM); Artificial Intelligence (cs.AI)",
        "abstract": "Innate resistance to anti-PD-1 immunotherapy remains a major clinical challenge in metastatic melanoma, with the underlying molecular networks being poorly understood. To address this, we constructed a dynamic Probabilistic Boolean Network model using transcriptomic data from patient tumor biopsies to elucidate the regulatory logic governing therapy response. We then employed a reinforcement learning agent to systematically discover optimal, multi-step therapeutic interventions and used explainable artificial intelligence to mechanistically interpret the agent's control policy. The analysis revealed that a precisely timed, 4-step temporary inhibition of the lysyl oxidase like 2 protein (LOXL2) was the most effective strategy. Our explainable analysis showed that this ''hit-and-run\" intervention is sufficient to erase the molecular signature driving resistance, allowing the network to self-correct without requiring sustained intervention. This study presents a novel, time-dependent therapeutic hypothesis for overcoming immunotherapy resistance and provides a powerful computational framework for identifying non-obvious intervention protocols in complex biological systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 207,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10172",
        "abs_url": "https://arxiv.org/abs/2507.10172",
        "pdf_url": "https://arxiv.org/pdf/2507.10172",
        "title": "Play Style Identification Using Low-Level Representations of Play Traces in MicroRTS",
        "authors": [
            "Ruizhe Yu Xia",
            "Jeremy Gow",
            "Simon Lucas"
        ],
        "comments": "Accepted as Short Paper for IEEE CoG",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Play style identification can provide valuable game design insights and enable adaptive experiences, with the potential to improve game playing agents. Previous work relies on domain knowledge to construct play trace representations using handcrafted features. More recent approaches incorporate the sequential structure of play traces but still require some level of domain abstraction. In this study, we explore the use of unsupervised CNN-LSTM autoencoder models to obtain latent representations directly from low-level play trace data in MicroRTS. We demonstrate that this approach yields a meaningful separation of different game playing agents in the latent space, reducing reliance on domain expertise and its associated biases. This latent space is then used to guide the exploration of diverse play styles within studied AI players.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 208,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10177",
        "abs_url": "https://arxiv.org/abs/2507.10177",
        "pdf_url": "https://arxiv.org/pdf/2507.10177",
        "title": "Abusive text transformation using LLMs",
        "authors": [
            "Rohitash Chandra",
            "Jiyong Choi"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Although Large Language Models (LLMs) have demonstrated significant advancements in natural language processing tasks, their effectiveness in the classification and transformation of abusive text into non-abusive versions remains an area for exploration. In this study, we aim to use LLMs to transform abusive text (tweets and reviews) featuring hate speech and swear words into non-abusive text, while retaining the intent of the text. We evaluate the performance of two state-of-the-art LLMs, such as Gemini, GPT-4o, DeekSeek and Groq, on their ability to identify abusive text. We them to transform and obtain a text that is clean from abusive and inappropriate content but maintains a similar level of sentiment and semantics, i.e. the transformed text needs to maintain its message. Afterwards, we evaluate the raw and transformed datasets with sentiment analysis and semantic analysis. Our results show Groq provides vastly different results when compared with other LLMs. We have identified similarities between GPT-4o and DeepSeek-V3.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 209,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10179",
        "abs_url": "https://arxiv.org/abs/2507.10179",
        "pdf_url": "https://arxiv.org/pdf/2507.10179",
        "title": "The Second Machine Turn: From Checking Proofs to Creating Concepts",
        "authors": [
            "Asvin G"
        ],
        "comments": "",
        "subjects": "History and Overview (math.HO); Artificial Intelligence (cs.AI)",
        "abstract": "We identify a second machine turn in the process of mathematical discovery: after automating proof-checking, AI is now poised to automate the *creation* of mathematical concepts themselves. We discuss the current state of the art, obstacles and potential solutions as well as a preliminary attempt at mathematizing the creation of concepts itself. The paper ends with an assessment of how these capabilities could reshape mathematics and human-machine collaboration, and a few different futures we might find ourselves in.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 210,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10182",
        "abs_url": "https://arxiv.org/abs/2507.10182",
        "pdf_url": "https://arxiv.org/pdf/2507.10182",
        "title": "Breaking the Myth: Can Small Models Infer Postconditions Too?",
        "authors": [
            "Gehao Zhang",
            "Zhenting Wang",
            "Juan Zhai"
        ],
        "comments": "",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)",
        "abstract": "Formal specifications are essential for ensuring software correctness, yet manually writing them is tedious and error-prone. Large Language Models (LLMs) have shown promise in generating such specifications from natural language intents, but the giant model size and high computational demands raise a fundamental question: Do we really need large models for this task? In this paper, we show that a small, fine-tuned language model can achieve high-quality postcondition generation with much lower computational costs. We construct a specialized dataset of prompts, reasoning logs, and postconditions, then supervise the fine-tuning of a $7$B-parameter code model. Our approach tackles real-world repository dependencies and preserves pre-state information, allowing for expressive and accurate specifications. We evaluate the model on a benchmark of real-world Java bugs (Defects4J) and compare against both proprietary giants (e.g., GPT-4o) and open-source large models. Empirical results demonstrate that our compact model matches or outperforms significantly larger counterparts in syntax correctness, semantic correctness, and bug-distinguishing capability. These findings highlight that targeted fine-tuning on a modest dataset can enable small models to achieve results formerly seen only in massive, resource-heavy LLMs, offering a practical and efficient path for the real-world adoption of automated specification generation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 211,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10194",
        "abs_url": "https://arxiv.org/abs/2507.10194",
        "pdf_url": "https://arxiv.org/pdf/2507.10194",
        "title": "Learning Private Representations through Entropy-based Adversarial Training",
        "authors": [
            "Tassilo Klein",
            "Moin Nabi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "How can we learn a representation with high predictive power while preserving user privacy? We present an adversarial representation learning method for sanitizing sensitive content from the learned representation. Specifically, we introduce a variant of entropy - focal entropy, which mitigates the potential information leakage of the existing entropy-based approaches. We showcase feasibility on multiple benchmarks. The results suggest high target utility at moderate privacy leakage.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 212,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10200",
        "abs_url": "https://arxiv.org/abs/2507.10200",
        "pdf_url": "https://arxiv.org/pdf/2507.10200",
        "title": "Natural Language-based Assessment of L2 Oral Proficiency using LLMs",
        "authors": [
            "Stefano Bannò",
            "Rao Ma",
            "Mengjie Qian",
            "Siyuan Tang",
            "Kate Knill",
            "Mark Gales"
        ],
        "comments": "Accepted for the 10th Workshop on Speech and Language Technology in Education (SLaTE 2025)",
        "subjects": "Audio and Speech Processing (eess.AS); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Natural language-based assessment (NLA) is an approach to second language assessment that uses instructions - expressed in the form of can-do descriptors - originally intended for human examiners, aiming to determine whether large language models (LLMs) can interpret and apply them in ways comparable to human assessment. In this work, we explore the use of such descriptors with an open-source LLM, Qwen 2.5 72B, to assess responses from the publicly available S&I Corpus in a zero-shot setting. Our results show that this approach - relying solely on textual information - achieves competitive performance: while it does not outperform state-of-the-art speech LLMs fine-tuned for the task, it surpasses a BERT-based model trained specifically for this purpose. NLA proves particularly effective in mismatched task settings, is generalisable to other data types and languages, and offers greater interpretability, as it is grounded in clearly explainable, widely applicable language descriptors.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 213,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10202",
        "abs_url": "https://arxiv.org/abs/2507.10202",
        "pdf_url": "https://arxiv.org/pdf/2507.10202",
        "title": "A Training-Free, Task-Agnostic Framework for Enhancing MLLM Performance on High-Resolution Images",
        "authors": [
            "Jaeseong Lee",
            "Yeeun Choi",
            "Heechan Choi",
            "Hanjung Kim",
            "Seonjoo Kim"
        ],
        "comments": "Accepted at CVPR 2025 Workshop on Emergent Visual Abilities and Limits of Foundation Models",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in vision-language understanding, reasoning, and generation. However, they struggle with tasks requiring fine-grained localization and reasoning in high-resolution images. This constraint stems from the fact that MLLMs are fine-tuned with fixed image resolution to align with the pre-trained image encoder used in MLLM. Consequently, feeding high-resolution images directly into MLLMs leads to poor generalization due to a train-test resolution discrepancy, while downsampling these images-although ensuring consistency-compromises fine-grained visual details and ultimately degrades performance. To address this challenge, we propose Extract Candidate then Predict (ECP), a novel training-free, task-agnostic two-stage framework designed to enhance MLLM performance on high-resolution images. The key intuition behind ECP is that while MLLMs struggle with high-resolution images, their predictions on downsampled images still contain implicit localization cues. By first identifying candidate region using the coarse prediction and then predicting the final output based on candidate region, ECP effectively preserves fine-grained details while mitigating the challenges posed by high-resolution data. We validate our framework on 4K GUI grounding and 4K, 8K MLLM perception, achieving +21.3%, +5.8%, +5.2% absolute improvement compared to baseline respectively, demonstrating its effectiveness. Code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 214,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10216",
        "abs_url": "https://arxiv.org/abs/2507.10216",
        "pdf_url": "https://arxiv.org/pdf/2507.10216",
        "title": "Absher: A Benchmark for Evaluating Large Language Models Understanding of Saudi Dialects",
        "authors": [
            "Renad Al-Monef",
            "Hassan Alhuzali",
            "Nora Alturayeif",
            "Ashwag Alasmari"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "As large language models (LLMs) become increasingly central to Arabic NLP applications, evaluating their understanding of regional dialects and cultural nuances is essential, particularly in linguistically diverse settings like Saudi Arabia. This paper introduces \\texttt{Absher}, a comprehensive benchmark specifically designed to assess LLMs performance across major Saudi dialects. \\texttt{Absher} comprises over 18,000 multiple-choice questions spanning six distinct categories: Meaning, True/False, Fill-in-the-Blank, Contextual Usage, Cultural Interpretation, and Location Recognition. These questions are derived from a curated dataset of dialectal words, phrases, and proverbs sourced from various regions of Saudi Arabia. We evaluate several state-of-the-art LLMs, including multilingual and Arabic-specific models. We also provide detailed insights into their capabilities and limitations. Our results reveal notable performance gaps, particularly in tasks requiring cultural inference or contextual understanding. Our findings highlight the urgent need for dialect-aware training and culturally aligned evaluation methodologies to improve LLMs performance in real-world Arabic applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 215,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10223",
        "abs_url": "https://arxiv.org/abs/2507.10223",
        "pdf_url": "https://arxiv.org/pdf/2507.10223",
        "title": "ProGait: A Multi-Purpose Video Dataset and Benchmark for Transfemoral Prosthesis Users",
        "authors": [
            "Xiangyu Yin",
            "Boyuan Yang",
            "Weichen Liu",
            "Qiyao Xue",
            "Abrar Alamri",
            "Goeran Fiedler",
            "Wei Gao"
        ],
        "comments": "Accepted by ICCV'25",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Prosthetic legs play a pivotal role in clinical rehabilitation, allowing individuals with lower-limb amputations the ability to regain mobility and improve their quality of life. Gait analysis is fundamental for optimizing prosthesis design and alignment, directly impacting the mobility and life quality of individuals with lower-limb amputations. Vision-based machine learning (ML) methods offer a scalable and non-invasive solution to gait analysis, but face challenges in correctly detecting and analyzing prosthesis, due to their unique appearances and new movement patterns. In this paper, we aim to bridge this gap by introducing a multi-purpose dataset, namely ProGait, to support multiple vision tasks including Video Object Segmentation, 2D Human Pose Estimation, and Gait Analysis (GA). ProGait provides 412 video clips from four above-knee amputees when testing multiple newly-fitted prosthetic legs through walking trials, and depicts the presence, contours, poses, and gait patterns of human subjects with transfemoral prosthetic legs. Alongside the dataset itself, we also present benchmark tasks and fine-tuned baseline models to illustrate the practical application and performance of the ProGait dataset. We compared our baseline models against pre-trained vision models, demonstrating improved generalizability when applying the ProGait dataset for prosthesis-specific tasks. Our code is available at this https URL and dataset at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 216,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10240",
        "abs_url": "https://arxiv.org/abs/2507.10240",
        "pdf_url": "https://arxiv.org/pdf/2507.10240",
        "title": "Visual Analytics for Explainable and Trustworthy Artificial Intelligence",
        "authors": [
            "Angelos Chatzimparmpas"
        ],
        "comments": "",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Our society increasingly depends on intelligent systems to solve complex problems, ranging from recommender systems suggesting the next movie to watch to AI models assisting in medical diagnoses for hospitalized patients. With the iterative improvement of diagnostic accuracy and efficiency, AI holds significant potential to mitigate medical misdiagnoses by preventing numerous deaths and reducing an economic burden of approximately 450 EUR billion annually. However, a key obstacle to AI adoption lies in the lack of transparency: many automated systems function as \"black boxes,\" providing predictions without revealing the underlying processes. This opacity can hinder experts' ability to trust and rely on AI systems. Visual analytics (VA) provides a compelling solution by combining AI models with interactive visualizations. These specialized charts and graphs empower users to incorporate their domain expertise to refine and improve the models, bridging the gap between AI and human understanding. In this work, we define, categorize, and explore how VA solutions can foster trust across the stages of a typical AI pipeline. We propose a design space for innovative visualizations and present an overview of our previously developed VA dashboards, which support critical tasks within the various pipeline stages, including data processing, feature engineering, hyperparameter tuning, understanding, debugging, refining, and comparing models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 217,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10250",
        "abs_url": "https://arxiv.org/abs/2507.10250",
        "pdf_url": "https://arxiv.org/pdf/2507.10250",
        "title": "DepViT-CAD: Deployable Vision Transformer-Based Cancer Diagnosis in Histopathology",
        "authors": [
            "Ashkan Shakarami",
            "Lorenzo Nicole",
            "Rocco Cappellesso",
            "Angelo Paolo Dei Tos",
            "Stefano Ghidoni"
        ],
        "comments": "25 pages, 15 figures",
        "subjects": "Image and Video Processing (eess.IV); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Accurate and timely cancer diagnosis from histopathological slides is vital for effective clinical decision-making. This paper introduces DepViT-CAD, a deployable AI system for multi-class cancer diagnosis in histopathology. At its core is MAViT, a novel Multi-Attention Vision Transformer designed to capture fine-grained morphological patterns across diverse tumor types. MAViT was trained on expert-annotated patches from 1008 whole-slide images, covering 11 diagnostic categories, including 10 major cancers and non-tumor tissue. DepViT-CAD was validated on two independent cohorts: 275 WSIs from The Cancer Genome Atlas and 50 routine clinical cases from pathology labs, achieving diagnostic sensitivities of 94.11% and 92%, respectively. By combining state-of-the-art transformer architecture with large-scale real-world validation, DepViT-CAD offers a robust and scalable approach for AI-assisted cancer diagnostics. To support transparency and reproducibility, software and code will be made publicly available at GitHub.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 218,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10300",
        "abs_url": "https://arxiv.org/abs/2507.10300",
        "pdf_url": "https://arxiv.org/pdf/2507.10300",
        "title": "FaceLLM: A Multimodal Large Language Model for Face Understanding",
        "authors": [
            "Hatef Otroshi Shahreza",
            "Sébastien Marcel"
        ],
        "comments": "Accepted in ICCV 2025 workshops",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Multimodal large language models (MLLMs) have shown remarkable performance in vision-language tasks. However, existing MLLMs are primarily trained on generic datasets, limiting their ability to reason on domain-specific visual cues such as those in facial images. In particular, tasks that require detailed understanding of facial structure, expression, emotion, and demographic features remain underexplored by MLLMs due to the lack of large-scale annotated face image-text datasets. In this work, we introduce FaceLLM, a multimodal large language model trained specifically for facial image understanding. To construct the training data, we propose a novel weakly supervised pipeline that uses ChatGPT with attribute-aware prompts to generate high-quality question-answer pairs based on images from the FairFace dataset. The resulting corpus, called FairFaceGPT, covers a diverse set of attributes including expression, pose, skin texture, and forensic information. Our experiments demonstrate that FaceLLM improves the performance of MLLMs on various face-centric tasks and achieves state-of-the-art performance. This work highlights the potential of synthetic supervision via language models for building domain-specialized MLLMs, and sets a precedent for trustworthy, human-centric multimodal AI systems. FairFaceGPT dataset and pretrained FaceLLM models are publicly available in the project page.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 219,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10311",
        "abs_url": "https://arxiv.org/abs/2507.10311",
        "pdf_url": "https://arxiv.org/pdf/2507.10311",
        "title": "Recognizing Dementia from Neuropsychological Tests with State Space Models",
        "authors": [
            "Liming Wang",
            "Saurabhchand Bhati",
            "Cody Karjadi",
            "Rhoda Au",
            "James Glass"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Early detection of dementia is critical for timely medical intervention and improved patient outcomes. Neuropsychological tests are widely used for cognitive assessment but have traditionally relied on manual scoring. Automatic dementia classification (ADC) systems aim to infer cognitive decline directly from speech recordings of such tests. We propose Demenba, a novel ADC framework based on state space models, which scale linearly in memory and computation with sequence length. Trained on over 1,000 hours of cognitive assessments administered to Framingham Heart Study participants, some of whom were diagnosed with dementia through adjudicated review, our method outperforms prior approaches in fine-grained dementia classification by 21\\%, while using fewer parameters. We further analyze its scaling behavior and demonstrate that our model gains additional improvement when fused with large language models, paving the way for more transparent and scalable dementia assessment tools. Code: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 220,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10324",
        "abs_url": "https://arxiv.org/abs/2507.10324",
        "pdf_url": "https://arxiv.org/pdf/2507.10324",
        "title": "Toolsuite for Implementing Multiagent Systems Based on Communication Protocols",
        "authors": [
            "Amit K. Chopra",
            "Samuel H. Christie V",
            "Munindar P. Singh"
        ],
        "comments": "",
        "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI); Programming Languages (cs.PL); Software Engineering (cs.SE)",
        "abstract": "Interaction-Oriented Programming (IOP) is an approach to building a multiagent system by modeling the interactions between its roles via a flexible interaction protocol and implementing agents to realize the interactions of the roles they play in the protocol. In recent years, we have developed an extensive suite of software that enables multiagent system developers to apply IOP. These include tools for efficiently verifying protocols for properties such as liveness and safety and middleware that simplifies the implementation of agents. This paper presents some of that software suite.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 221,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10348",
        "abs_url": "https://arxiv.org/abs/2507.10348",
        "pdf_url": "https://arxiv.org/pdf/2507.10348",
        "title": "Feature Distillation is the Better Choice for Model-Heterogeneous Federated Learning",
        "authors": [
            "Yichen Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Model-Heterogeneous Federated Learning (Hetero-FL) has attracted growing attention for its ability to aggregate knowledge from heterogeneous models while keeping private data locally. To better aggregate knowledge from clients, ensemble distillation, as a widely used and effective technique, is often employed after global aggregation to enhance the performance of the global model. However, simply combining Hetero-FL and ensemble distillation does not always yield promising results and can make the training process unstable. The reason is that existing methods primarily focus on logit distillation, which, while being model-agnostic with softmax predictions, fails to compensate for the knowledge bias arising from heterogeneous models. To tackle this challenge, we propose a stable and efficient Feature Distillation for model-heterogeneous Federated learning, dubbed FedFD, that can incorporate aligned feature information via orthogonal projection to integrate knowledge from heterogeneous models better. Specifically, a new feature-based ensemble federated knowledge distillation paradigm is proposed. The global model on the server needs to maintain a projection layer for each client-side model architecture to align the features separately. Orthogonal techniques are employed to re-parameterize the projection layer to mitigate knowledge bias from heterogeneous models and thus maximize the distilled knowledge. Extensive experiments show that FedFD achieves superior performance compared to state-of-the-art methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 222,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10349",
        "abs_url": "https://arxiv.org/abs/2507.10349",
        "pdf_url": "https://arxiv.org/pdf/2507.10349",
        "title": "TAT: Temporal-Aligned Transformer for Multi-Horizon Peak Demand Forecasting",
        "authors": [
            "Zhiyuan Zhao",
            "Sitan Yang",
            "Kin G. Olivares",
            "Boris N. Oreshkin",
            "Stan Vitebsky",
            "Michael W. Mahoney",
            "B. Aditya Prakash",
            "Dmitry Efimov"
        ],
        "comments": "9 pages, 4 figures, 7 tables, published at KDD 2025 workshop on AI for Supply Chain: Today and Future",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Multi-horizon time series forecasting has many practical applications such as demand forecasting. Accurate demand prediction is critical to help make buying and inventory decisions for supply chain management of e-commerce and physical retailers, and such predictions are typically required for future horizons extending tens of weeks. This is especially challenging during high-stake sales events when demand peaks are particularly difficult to predict accurately. However, these events are important not only for managing supply chain operations but also for ensuring a seamless shopping experience for customers. To address this challenge, we propose Temporal-Aligned Transformer (TAT), a multi-horizon forecaster leveraging apriori-known context variables such as holiday and promotion events information for improving predictive performance. Our model consists of an encoder and decoder, both embedded with a novel Temporal Alignment Attention (TAA), designed to learn context-dependent alignment for peak demand forecasting. We conduct extensive empirical analysis on two large-scale proprietary datasets from a large e-commerce retailer. We demonstrate that TAT brings up to 30% accuracy improvement on peak demand forecasting while maintaining competitive overall performance compared to other state-of-the-art methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 223,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10398",
        "abs_url": "https://arxiv.org/abs/2507.10398",
        "pdf_url": "https://arxiv.org/pdf/2507.10398",
        "title": "Devanagari Handwritten Character Recognition using Convolutional Neural Network",
        "authors": [
            "Diksha Mehta",
            "Prateek Mehta"
        ],
        "comments": "9 pages, 6 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Handwritten character recognition is getting popular among researchers because of its possible applications in facilitating technological search engines, social media, recommender systems, etc. The Devanagari script is one of the oldest language scripts in India that does not have proper digitization tools. With the advancement of computing and technology, the task of this research is to extract handwritten Hindi characters from an image of Devanagari script with an automated approach to save time and obsolete data. In this paper, we present a technique to recognize handwritten Devanagari characters using two deep convolutional neural network layers. This work employs a methodology that is useful to enhance the recognition rate and configures a convolutional neural network for effective Devanagari handwritten text recognition (DHTR). This approach uses the Devanagari handwritten character dataset (DHCD), an open dataset with 36 classes of Devanagari characters. Each of these classes has 1700 images for training and testing purposes. This approach obtains promising results in terms of accuracy by achieving 96.36% accuracy in testing and 99.55% in training time.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 224,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10409",
        "abs_url": "https://arxiv.org/abs/2507.10409",
        "pdf_url": "https://arxiv.org/pdf/2507.10409",
        "title": "Energy Efficiency in AI for 5G and Beyond: A DeepRx Case Study",
        "authors": [
            "Amine Lbath",
            "Ibtissam Labriji"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "This study addresses the challenge of balancing energy efficiency with performance in AI/ML models, focusing on DeepRX, a deep learning receiver based on a fully convolutional ResNet architecture. We evaluate the energy consumption of DeepRX, considering factors including FLOPs/Watt and FLOPs/clock, and find consistency between estimated and actual energy usage, influenced by memory access patterns. The research extends to comparing energy dynamics during training and inference phases. A key contribution is the application of knowledge distillation (KD) to train a compact DeepRX student model that emulates the performance of the teacher model but with reduced energy consumption. We experiment with different student model sizes, optimal teacher sizes, and KD hyperparameters. Performance is measured by comparing the Bit Error Rate (BER) performance versus Signal-to-Interference & Noise Ratio (SINR) values of the distilled model and a model trained from scratch. The distilled models demonstrate a lower error floor across SINR levels, highlighting the effectiveness of KD in achieving energy-efficient AI solutions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 225,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10419",
        "abs_url": "https://arxiv.org/abs/2507.10419",
        "pdf_url": "https://arxiv.org/pdf/2507.10419",
        "title": "Multiple Choice Learning of Low Rank Adapters for Language Modeling",
        "authors": [
            "Victor Letzelter",
            "Hugo Malard",
            "Mathieu Fontaine",
            "Gaël Richard",
            "Slim Essid",
            "Andrei Bursuc",
            "Patrick Pérez"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (stat.ML)",
        "abstract": "We propose LoRA-MCL, a training scheme that extends next-token prediction in language models with a method designed to decode diverse, plausible sentence continuations at inference time. Traditional language modeling is an intrinsically ill-posed problem: given a context, multiple futures may be equally plausible. Our approach leverages Multiple Choice Learning (MCL) and the Winner-Takes-All (WTA) loss to efficiently handle ambiguity through Low-Rank Adaptation (LoRA). We provide a theoretical interpretation of applying Multiple Choice Learning to Language Modeling, assuming the data is generated from a mixture of distributions. To illustrate the proposed approach, we use data sampled from mixtures of Markov chains. We then demonstrate with extensive experiments on real-world visual and audio captioning tasks that our method achieves high diversity and relevance in generated outputs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 226,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10430",
        "abs_url": "https://arxiv.org/abs/2507.10430",
        "pdf_url": "https://arxiv.org/pdf/2507.10430",
        "title": "Efficient Federated Learning with Heterogeneous Data and Adaptive Dropout",
        "authors": [
            "Ji Liu",
            "Beichen Ma",
            "Qiaolin Yu",
            "Ruoming Jin",
            "Jingbo Zhou",
            "Yang Zhou",
            "Huaiyu Dai",
            "Haixun Wang",
            "Dejing Dou",
            "Patrick Valduriez"
        ],
        "comments": "29 pages, to appear in ACM Transactions on Knowledge Discovery from Data (TKDD)",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Federated Learning (FL) is a promising distributed machine learning approach that enables collaborative training of a global model using multiple edge devices. The data distributed among the edge devices is highly heterogeneous. Thus, FL faces the challenge of data distribution and heterogeneity, where non-Independent and Identically Distributed (non-IID) data across edge devices may yield in significant accuracy drop. Furthermore, the limited computation and communication capabilities of edge devices increase the likelihood of stragglers, thus leading to slow model convergence. In this paper, we propose the FedDHAD FL framework, which comes with two novel methods: Dynamic Heterogeneous model aggregation (FedDH) and Adaptive Dropout (FedAD). FedDH dynamically adjusts the weights of each local model within the model aggregation process based on the non-IID degree of heterogeneous data to deal with the statistical data heterogeneity. FedAD performs neuron-adaptive operations in response to heterogeneous devices to improve accuracy while achieving superb efficiency. The combination of these two methods makes FedDHAD significantly outperform state-of-the-art solutions in terms of accuracy (up to 6.7% higher), efficiency (up to 2.02 times faster), and computation cost (up to 15.0% smaller).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 227,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10435",
        "abs_url": "https://arxiv.org/abs/2507.10435",
        "pdf_url": "https://arxiv.org/pdf/2507.10435",
        "title": "From Sequence to Structure: Uncovering Substructure Reasoning in Transformers",
        "authors": [
            "Xinnan Dai",
            "Kai Yang",
            "Jay Revolinsky",
            "Kai Guo",
            "Aoran Wang",
            "Bohang Zhang",
            "Jiliang Tang"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Recent studies suggest that large language models (LLMs) possess the capability to solve graph reasoning tasks. Notably, even when graph structures are embedded within textual descriptions, LLMs can still effectively answer related questions. This raises a fundamental question: How can a decoder-only Transformer architecture understand underlying graph structures? To address this, we start with the substructure extraction task, interpreting the inner mechanisms inside the transformers and analyzing the impact of the input queries. Specifically, through both empirical results and theoretical analysis, we present Induced Substructure Filtration (ISF), a perspective that captures the substructure identification in the multi-layer transformers. We further validate the ISF process in LLMs, revealing consistent internal dynamics across layers. Building on these insights, we explore the broader capabilities of Transformers in handling diverse graph types. Specifically, we introduce the concept of thinking in substructures to efficiently extract complex composite patterns, and demonstrate that decoder-only Transformers can successfully extract substructures from attributed graphs, such as molecular graphs. Together, our findings offer a new insight on how sequence-based Transformers perform the substructure extraction task over graph data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 228,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10442",
        "abs_url": "https://arxiv.org/abs/2507.10442",
        "pdf_url": "https://arxiv.org/pdf/2507.10442",
        "title": "Response Wide Shut? Surprising Observations in Basic Vision Language Model Capabilities",
        "authors": [
            "Shivam Chandhok",
            "Wan-Cyuan Fan",
            "Vered Shwartz",
            "Vineeth N Balasubramanian",
            "Leonid Sigal"
        ],
        "comments": "Accepted at ACL 2025 (Main Conference)",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Vision-language Models (VLMs) have emerged as general-purpose tools for addressing a variety of complex computer vision problems. Such models have been shown to be highly capable, but, at the same time, lacking some basic visual understanding skills. In this paper, we set out to understand the limitations of SoTA VLMs on fundamental visual tasks by constructing a series of tests that probe which components of design, specifically, may be lacking. Importantly, we go significantly beyond the current benchmarks, which simply measure the final performance of VLM response, by also comparing and contrasting it to the performance of probes trained directly on features obtained from the visual encoder, intermediate vision-language projection and LLM-decoder output. In doing so, we uncover shortcomings in VLMs and make a number of important observations about their capabilities, robustness and how they process visual information. We hope our insights will guide progress in further improving VLMs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 229,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10445",
        "abs_url": "https://arxiv.org/abs/2507.10445",
        "pdf_url": "https://arxiv.org/pdf/2507.10445",
        "title": "Referential ambiguity and clarification requests: comparing human and LLM behaviour",
        "authors": [
            "Chris Madge",
            "Matthew Purver",
            "Massimo Poesio"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "In this work we examine LLMs' ability to ask clarification questions in task-oriented dialogues that follow the asynchronous instruction-giver/instruction-follower format. We present a new corpus that combines two existing annotations of the Minecraft Dialogue Corpus -- one for reference and ambiguity in reference, and one for SDRT including clarifications -- into a single common format providing the necessary information to experiment with clarifications and their relation to ambiguity. With this corpus we compare LLM actions with original human-generated clarification questions, examining how both humans and LLMs act in the case of ambiguity. We find that there is only a weak link between ambiguity and humans producing clarification questions in these dialogues, and low correlation between humans and LLMs. Humans hardly ever produce clarification questions for referential ambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce more clarification questions for referential ambiguity, but less so for task uncertainty. We question if LLMs' ability to ask clarification questions is predicated on their recent ability to simulate reasoning, and test this with different reasoning approaches, finding that reasoning does appear to increase question frequency and relevancy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 230,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10447",
        "abs_url": "https://arxiv.org/abs/2507.10447",
        "pdf_url": "https://arxiv.org/pdf/2507.10447",
        "title": "Evaluating Fake Music Detection Performance Under Audio Augmentations",
        "authors": [
            "Tomasz Sroka",
            "Tomasz Wężowicz",
            "Dominik Sidorczuk",
            "Mateusz Modrzejewski"
        ],
        "comments": "ISMIR 2025 LBD, 2 pages + bibliography, 1 figure",
        "subjects": "Sound (cs.SD); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)",
        "abstract": "With the rapid advancement of generative audio models, distinguishing between human-composed and generated music is becoming increasingly challenging. As a response, models for detecting fake music have been proposed. In this work, we explore the robustness of such systems under audio augmentations. To evaluate model generalization, we constructed a dataset consisting of both real and synthetic music generated using several systems. We then apply a range of audio transformations and analyze how they affect classification accuracy. We test the performance of a recent state-of-the-art musical deepfake detection model in the presence of audio augmentations. The performance of the model decreases significantly even with the introduction of light augmentations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 231,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10449",
        "abs_url": "https://arxiv.org/abs/2507.10449",
        "pdf_url": "https://arxiv.org/pdf/2507.10449",
        "title": "CoralVQA: A Large-Scale Visual Question Answering Dataset for Coral Reef Image Understanding",
        "authors": [
            "Hongyong Han",
            "Wei Wang",
            "Gaowei Zhang",
            "Mingjie Li",
            "Yi Wang"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Coral reefs are vital yet vulnerable ecosystems that require continuous monitoring to support conservation. While coral reef images provide essential information in coral monitoring, interpreting such images remains challenging due to the need for domain expertise. Visual Question Answering (VQA), powered by Large Vision-Language Models (LVLMs), has great potential in user-friendly interaction with coral reef images. However, applying VQA to coral imagery demands a dedicated dataset that addresses two key challenges: domain-specific annotations and multidimensional questions. In this work, we introduce CoralVQA, the first large-scale VQA dataset for coral reef analysis. It contains 12,805 real-world coral images from 67 coral genera collected from 3 oceans, along with 277,653 question-answer pairs that comprehensively assess ecological and health-related conditions. To construct this dataset, we develop a semi-automatic data construction pipeline in collaboration with marine biologists to ensure both scalability and professional-grade data quality. CoralVQA presents novel challenges and provides a comprehensive benchmark for studying vision-language reasoning in the context of coral reef images. By evaluating several state-of-the-art LVLMs, we reveal key limitations and opportunities. These insights form a foundation for future LVLM development, with a particular emphasis on supporting coral conservation efforts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 232,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10457",
        "abs_url": "https://arxiv.org/abs/2507.10457",
        "pdf_url": "https://arxiv.org/pdf/2507.10457",
        "title": "Logic layer Prompt Control Injection (LPCI): A Novel Security Vulnerability Class in Agentic Systems",
        "authors": [
            "Hammad Atta",
            "Ken Huang",
            "Manish Bhatt",
            "Kamal Ahmed",
            "Muhammad Aziz Ul Haq",
            "Yasir Mehmood"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "The integration of large language models (LLMs) into enterprise systems has created a new class of covert security vulnerabilities, particularly within logic-execution layers and persistent-memory contexts. In this paper, we introduce Logic-Layer Prompt Control Injection (LPCI), a novel attack category in which encoded, delayed, and conditionally triggered payloads are embedded in memory, vector stores, or tool outputs. These payloads can bypass conventional input filters and trigger unauthorised behaviour across sessions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 233,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10461",
        "abs_url": "https://arxiv.org/abs/2507.10461",
        "pdf_url": "https://arxiv.org/pdf/2507.10461",
        "title": "RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for Pansharpening",
        "authors": [
            "Tao Tang",
            "Chengxu Yang"
        ],
        "comments": "To appear in the proceedings of the 6th International Conference on Artificial Intelligence and Electromechanical Automation (AIEA 2025). 5 pages, 6 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multimedia (cs.MM); Image and Video Processing (eess.IV)",
        "abstract": "Pansharpening refers to the process of integrating a high resolution panchromatic (PAN) image with a lower resolution multispectral (MS) image to generate a fused product, which is pivotal in remote sensing. Despite the effectiveness of CNNs in addressing this challenge, they are inherently constrained by the uniform application of convolutional kernels across all spatial positions, overlooking local content variations. To overcome this issue, we introduce RAPNet, a new architecture that leverages content-adaptive convolution. At its core, RAPNet employs the Receptive-field Adaptive Pansharpening Convolution (RAPConv), designed to produce spatially adaptive kernels responsive to local feature context, thereby enhancing the precision of spatial detail extraction. Additionally, the network integrates the Pansharpening Dynamic Feature Fusion (PAN-DFF) module, which incorporates an attention mechanism to achieve an optimal balance between spatial detail enhancement and spectral fidelity. Comprehensive evaluations on publicly available datasets confirm that RAPNet delivers superior performance compared to existing approaches, as demonstrated by both quantitative metrics and qualitative assessments. Ablation analyses further substantiate the effectiveness of the proposed adaptive components.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 234,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10464",
        "abs_url": "https://arxiv.org/abs/2507.10464",
        "pdf_url": "https://arxiv.org/pdf/2507.10464",
        "title": "AudioMAE++: learning better masked audio representations with SwiGLU FFNs",
        "authors": [
            "Sarthak Yadav",
            "Sergios Theodoridis",
            "Zheng-Hua Tan"
        ],
        "comments": "TO APPEAR AT IEEE MLSP 2025",
        "subjects": "Sound (cs.SD); Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)",
        "abstract": "Masked Autoencoders (MAEs) trained on audio spectrogram patches have emerged as a prominent approach for learning self-supervised audio representations. While several recent papers have evaluated key aspects of training MAEs on audio data, the majority of these approaches still leverage vanilla transformer building blocks, whereas the transformer community has seen steady integration of newer architectural advancements. In this work, we propose AudioMAE++, a revamped audio masked autoencoder with two such enhancements, namely macaron-style transformer blocks with gated linear units. When pretrained on the AudioSet dataset, the proposed AudioMAE++ models outperform existing MAE based approaches on 10 diverse downstream tasks, demonstrating excellent performance on audio classification and speech-based benchmarks. The proposed AudioMAE++ models also demonstrate excellent scaling characteristics, outperforming directly comparable standard MAE baselines with up to 4x more parameters.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 235,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10469",
        "abs_url": "https://arxiv.org/abs/2507.10469",
        "pdf_url": "https://arxiv.org/pdf/2507.10469",
        "title": "An Empirical Evaluation of AI-Powered Non-Player Characters' Perceived Realism and Performance in Virtual Reality Environments",
        "authors": [
            "Mikko Korkiakoski",
            "Saeid Sheikhi",
            "Jesper Nyman",
            "Jussi Saariniemi",
            "Kalle Tapio",
            "Panos Kostakos"
        ],
        "comments": "",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI); Multimedia (cs.MM)",
        "abstract": "Advancements in artificial intelligence (AI) have significantly enhanced the realism and interactivity of non-player characters (NPCs) in virtual reality (VR), creating more engaging and believable user experiences. This paper evaluates AI-driven NPCs within a VR interrogation simulator, focusing on their perceived realism, usability, and system performance. The simulator features two AI-powered NPCs, a suspect, and a partner, using GPT-4 Turbo to engage participants in a scenario to determine the suspect's guilt or innocence. A user study with 18 participants assessed the system using the System Usability Scale (SUS), Game Experience Questionnaire (GEQ), and a Virtual Agent Believability Questionnaire, alongside latency measurements for speech-to-text (STT), text-to-speech (TTS), OpenAI GPT-4 Turbo, and overall (cycle) latency. Results showed an average cycle latency of 7 seconds, influenced by the increasing conversational context. Believability scored 6.67 out of 10, with high ratings in behavior, social relationships, and intelligence but moderate scores in emotion and personality. The system achieved a SUS score of 79.44, indicating good usability. These findings demonstrate the potential of large language models to improve NPC realism and interaction in VR while highlighting challenges in reducing system latency and enhancing emotional depth. This research contributes to the development of more sophisticated AI-driven NPCs, revealing the need for performance optimization to achieve increasingly immersive virtual experiences.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 236,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10474",
        "abs_url": "https://arxiv.org/abs/2507.10474",
        "pdf_url": "https://arxiv.org/pdf/2507.10474",
        "title": "Privacy-Preserving Multi-Stage Fall Detection Framework with Semi-supervised Federated Learning and Robotic Vision Confirmation",
        "authors": [
            "Seyed Alireza Rahimi Azghadi",
            "Truong-Thanh-Hung Nguyen",
            "Helene Fournier",
            "Monica Wachowicz",
            "Rene Richard",
            "Francis Palma",
            "Hung Cao"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Robotics (cs.RO)",
        "abstract": "The aging population is growing rapidly, and so is the danger of falls in older adults. A major cause of injury is falling, and detection in time can greatly save medical expenses and recovery time. However, to provide timely intervention and avoid unnecessary alarms, detection systems must be effective and reliable while addressing privacy concerns regarding the user. In this work, we propose a framework for detecting falls using several complementary systems: a semi-supervised federated learning-based fall detection system (SF2D), an indoor localization and navigation system, and a vision-based human fall recognition system. A wearable device and an edge device identify a fall scenario in the first system. On top of that, the second system uses an indoor localization technique first to localize the fall location and then navigate a robot to inspect the scenario. A vision-based detection system running on an edge device with a mounted camera on a robot is used to recognize fallen people. Each of the systems of this proposed framework achieves different accuracy rates. Specifically, the SF2D has a 0.81% failure rate equivalent to 99.19% accuracy, while the vision-based fallen people detection achieves 96.3% accuracy. However, when we combine the accuracy of these two systems with the accuracy of the navigation system (95% success rate), our proposed framework creates a highly reliable performance for fall detection, with an overall accuracy of 99.99%. Not only is the proposed framework safe for older adults, but it is also a privacy-preserving solution for detecting falls.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 237,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10475",
        "abs_url": "https://arxiv.org/abs/2507.10475",
        "pdf_url": "https://arxiv.org/pdf/2507.10475",
        "title": "Can You Detect the Difference?",
        "authors": [
            "İsmail Tarım",
            "Aytuğ Onan"
        ],
        "comments": "11 pages, 3 figures, 2 tables. Code and data: this https URL. Cross-list requested to cs.AI for AI-safety relevance",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "The rapid advancement of large language models (LLMs) has raised concerns about reliably detecting AI-generated text. Stylometric metrics work well on autoregressive (AR) outputs, but their effectiveness on diffusion-based models is unknown. We present the first systematic comparison of diffusion-generated text (LLaDA) and AR-generated text (LLaMA) using 2 000 samples. Perplexity, burstiness, lexical diversity, readability, and BLEU/ROUGE scores show that LLaDA closely mimics human text in perplexity and burstiness, yielding high false-negative rates for AR-oriented detectors. LLaMA shows much lower perplexity but reduced lexical fidelity. Relying on any single metric fails to separate diffusion outputs from human writing. We highlight the need for diffusion-aware detectors and outline directions such as hybrid models, diffusion-specific stylometric signatures, and robust watermarking.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 238,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10492",
        "abs_url": "https://arxiv.org/abs/2507.10492",
        "pdf_url": "https://arxiv.org/pdf/2507.10492",
        "title": "BenchReAD: A systematic benchmark for retinal anomaly detection",
        "authors": [
            "Chenyu Lian",
            "Hong-Yu Zhou",
            "Zhanli Hu",
            "Jing Qin"
        ],
        "comments": "MICCAI 2025",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Retinal anomaly detection plays a pivotal role in screening ocular and systemic diseases. Despite its significance, progress in the field has been hindered by the absence of a comprehensive and publicly available benchmark, which is essential for the fair evaluation and advancement of methodologies. Due to this limitation, previous anomaly detection work related to retinal images has been constrained by (1) a limited and overly simplistic set of anomaly types, (2) test sets that are nearly saturated, and (3) a lack of generalization evaluation, resulting in less convincing experimental setups. Furthermore, existing benchmarks in medical anomaly detection predominantly focus on one-class supervised approaches (training only with negative samples), overlooking the vast amounts of labeled abnormal data and unlabeled data that are commonly available in clinical practice. To bridge these gaps, we introduce a benchmark for retinal anomaly detection, which is comprehensive and systematic in terms of data and algorithm. Through categorizing and benchmarking previous methods, we find that a fully supervised approach leveraging disentangled representations of abnormalities (DRA) achieves the best performance but suffers from significant drops in performance when encountering certain unseen anomalies. Inspired by the memory bank mechanisms in one-class supervised learning, we propose NFM-DRA, which integrates DRA with a Normal Feature Memory to mitigate the performance degradation, establishing a new SOTA. The benchmark is publicly available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 239,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10496",
        "abs_url": "https://arxiv.org/abs/2507.10496",
        "pdf_url": "https://arxiv.org/pdf/2507.10496",
        "title": "Cameras as Relative Positional Encoding",
        "authors": [
            "Ruilong Li",
            "Brent Yi",
            "Junchen Liu",
            "Hang Gao",
            "Yi Ma",
            "Angjoo Kanazawa"
        ],
        "comments": "Project Page: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Transformers are increasingly prevalent for multi-view computer vision tasks, where geometric relationships between viewpoints are critical for 3D perception. To leverage these relationships, multi-view transformers must use camera geometry to ground visual tokens in 3D space. In this work, we compare techniques for conditioning transformers on cameras: token-level raymap encodings, attention-level relative pose encodings, and a new relative encoding we propose -- Projective Positional Encoding (PRoPE) -- that captures complete camera frustums, both intrinsics and extrinsics, as a relative positional encoding. Our experiments begin by showing how relative camera conditioning improves performance in feedforward novel view synthesis, with further gains from PRoPE. This holds across settings: scenes with both shared and varying intrinsics, when combining token- and attention-level conditioning, and for generalization to inputs with out-of-distribution sequence lengths and camera intrinsics. We then verify that these benefits persist for different tasks, stereo depth estimation and discriminative spatial cognition, as well as larger model sizes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 240,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10500",
        "abs_url": "https://arxiv.org/abs/2507.10500",
        "pdf_url": "https://arxiv.org/pdf/2507.10500",
        "title": "Scene-Aware Conversational ADAS with Generative AI for Real-Time Driver Assistance",
        "authors": [
            "Kyungtae Han",
            "Yitao Chen",
            "Rohit Gupta",
            "Onur Altintas"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC)",
        "abstract": "While autonomous driving technologies continue to advance, current Advanced Driver Assistance Systems (ADAS) remain limited in their ability to interpret scene context or engage with drivers through natural language. These systems typically rely on predefined logic and lack support for dialogue-based interaction, making them inflexible in dynamic environments or when adapting to driver intent. This paper presents Scene-Aware Conversational ADAS (SC-ADAS), a modular framework that integrates Generative AI components including large language models, vision-to-text interpretation, and structured function calling to enable real-time, interpretable, and adaptive driver assistance. SC-ADAS supports multi-turn dialogue grounded in visual and sensor context, allowing natural language recommendations and driver-confirmed ADAS control. Implemented in the CARLA simulator with cloud-based Generative AI, the system executes confirmed user intents as structured ADAS commands without requiring model fine-tuning. We evaluate SC-ADAS across scene-aware, conversational, and revisited multi-turn interactions, highlighting trade-offs such as increased latency from vision-based context retrieval and token growth from accumulated dialogue history. These results demonstrate the feasibility of combining conversational reasoning, scene perception, and modular ADAS control to support the next generation of intelligent driver assistance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 241,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10502",
        "abs_url": "https://arxiv.org/abs/2507.10502",
        "pdf_url": "https://arxiv.org/pdf/2507.10502",
        "title": "Benchmarking and Evaluation of AI Models in Biology: Outcomes and Recommendations from the CZI Virtual Cells Workshop",
        "authors": [
            "Elizabeth Fahsbender",
            "Alma Andersson",
            "Jeremy Ash",
            "Polina Binder",
            "Daniel Burkhardt",
            "Benjamin Chang",
            "Georg K. Gerber",
            "Anthony Gitter",
            "Patrick Godau",
            "Ankit Gupta",
            "Genevieve Haliburton",
            "Siyu He",
            "Trey Ideker",
            "Ivana Jelic",
            "Aly Khan",
            "Yang-Joon Kim",
            "Aditi Krishnapriyan",
            "Jon M. Laurent",
            "Tianyu Liu",
            "Emma Lundberg",
            "Shalin B. Mehta",
            "Rob Moccia",
            "Angela Oliveira Pisco",
            "Katherine S. Pollard",
            "Suresh Ramani",
            "Julio Saez-Rodriguez",
            "Yasin Senbabaoglu",
            "Elana Simon",
            "Srinivasan Sivanandan",
            "Gustavo Stolovitzky",
            "Marc Valer",
            "Bo Wang",
            "Xikun Zhang",
            "James Zou",
            "Katrina Kalantar"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Artificial intelligence holds immense promise for transforming biology, yet a lack of standardized, cross domain, benchmarks undermines our ability to build robust, trustworthy models. Here, we present insights from a recent workshop that convened machine learning and computational biology experts across imaging, transcriptomics, proteomics, and genomics to tackle this gap. We identify major technical and systemic bottlenecks such as data heterogeneity and noise, reproducibility challenges, biases, and the fragmented ecosystem of publicly available resources and propose a set of recommendations for building benchmarking frameworks that can efficiently compare ML models of biological systems across tasks and data modalities. By promoting high quality data curation, standardized tooling, comprehensive evaluation metrics, and open, collaborative platforms, we aim to accelerate the development of robust benchmarks for AI driven Virtual Cells. These benchmarks are crucial for ensuring rigor, reproducibility, and biological relevance, and will ultimately advance the field toward integrated models that drive new discoveries, therapeutic insights, and a deeper understanding of cellular systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 242,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10510",
        "abs_url": "https://arxiv.org/abs/2507.10510",
        "pdf_url": "https://arxiv.org/pdf/2507.10510",
        "title": "Chat with AI: The Surprising Turn of Real-time Video Communication from Human to AI",
        "authors": [
            "Jiangkai Wu",
            "Zhiyuan Ren",
            "Liming Liu",
            "Xinggong Zhang"
        ],
        "comments": "",
        "subjects": "Networking and Internet Architecture (cs.NI); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Multimedia (cs.MM)",
        "abstract": "AI Video Chat emerges as a new paradigm for Real-time Communication (RTC), where one peer is not a human, but a Multimodal Large Language Model (MLLM). This makes interaction between humans and AI more intuitive, as if chatting face-to-face with a real person. However, this poses significant challenges to latency, because the MLLM inference takes up most of the response time, leaving very little time for video streaming. Due to network uncertainty and instability, transmission latency becomes a critical bottleneck preventing AI from being like a real person. To address this, we propose Artic, an AI-oriented Real-time Communication framework, exploring the network requirement shift from \"humans watching video\" to \"AI understanding video\". To reduce bitrate dramatically while maintaining MLLM accuracy, we propose Context-Aware Video Streaming that recognizes the importance of each video region for chat and allocates bitrate almost exclusively to chat-important regions. To avoid packet retransmission, we propose Loss-Resilient Adaptive Frame Rate that leverages previous frames to substitute for lost/delayed frames while avoiding bitrate waste. To evaluate the impact of video streaming quality on MLLM accuracy, we build the first benchmark, named Degraded Video Understanding Benchmark (DeViBench). Finally, we discuss some open questions and ongoing solutions for AI Video Chat.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 243,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10530",
        "abs_url": "https://arxiv.org/abs/2507.10530",
        "pdf_url": "https://arxiv.org/pdf/2507.10530",
        "title": "Accurate generation of chemical reaction transition states by conditional flow matching",
        "authors": [
            "Ping Tuo",
            "Jiale Chen",
            "Ju Li"
        ],
        "comments": "",
        "subjects": "Chemical Physics (physics.chem-ph); Artificial Intelligence (cs.AI)",
        "abstract": "Transition state (TS) structures define the critical geometries and energy barriers underlying chemical reactivity, yet their fleeting nature renders them experimentally elusive and drives the reliance on costly, high-throughput density functional theory (DFT) calculations. Here, we introduce TS-GEN, a conditional flow-matching generative model that maps samples from a simple Gaussian prior directly to transition-state saddle-point geometries in a single, deterministic pass. By embedding both reactant and product conformations as conditioning information, TS-GEN learns to transport latent noise to true TS structures via an optimal-transport path, effectively replacing the iterative optimization common in nudged-elastic band or string-method algorithms. TS-GEN delivers unprecedented accuracy, achieving a root-mean-square deviation of $0.004\\ \\rm{\\mathring{A}}$ (vs. $0.103\\ \\rm{\\mathring{A}}$ for prior state-of-the-art) and a mean barrier-height error of $1.019\\ {\\rm kcal/mol}$ (vs. $2.864\\ {\\rm kcal/mol}$), while requiring only $0.06\\ {\\rm s}$ GPU time per inference. Over 87% of generated TSs meet chemical-accuracy criteria ($<1.58\\ {\\rm kcal/mol}$ error), substantially outpacing existing methods. TS-GEN also exhibits strong transferability to out-of-distribution reactions from a larger database. By uniting sub-angstrom precision, sub-second speed, and broad applicability, TS-GEN will be highly useful for high-throughput exploration of complex reaction networks, paving the way to the exploration of novel chemical reaction mechanisms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 244,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10532",
        "abs_url": "https://arxiv.org/abs/2507.10532",
        "pdf_url": "https://arxiv.org/pdf/2507.10532",
        "title": "Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination",
        "authors": [
            "Mingqi Wu",
            "Zhihao Zhang",
            "Qiaole Dong",
            "Zhiheng Xi",
            "Jun Zhao",
            "Senjie Jin",
            "Xiaoran Fan",
            "Yuhao Zhou",
            "Yanwei Fu",
            "Qin Liu",
            "Songyang Zhang",
            "Qi Zhang"
        ],
        "comments": "26 pages",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "The reasoning capabilities of large language models (LLMs) have been a longstanding focus of research. Recent works have further enhanced these capabilities using reinforcement learning (RL), with many new methods claiming significant improvements with minimal or no external supervision. Surprisingly, some studies even suggest that random or incorrect reward signals can enhance reasoning performance. However, these breakthroughs are mostly reported on the Qwen2.5 model family and evaluated on well-known benchmarks such as MATH-500, AMC, and AIME, while failing to achieve similar gains on other models like Llama, which warrants further investigation. Our analysis shows that although Qwen2.5 achieves strong mathematical reasoning performance, its pretraining on large-scale web corpora makes it vulnerable to data contamination in popular benchmarks. As a result, results derived from these benchmarks may be unreliable. To address this, we introduce a generator that produces fully synthetic arithmetic problems of arbitrary length and difficulty, yielding a clean dataset we call RandomCalculation. Using these leakage-free datasets, we show that only accurate reward signals consistently improve performance, while noisy or incorrect signals do not. We advocate for evaluating RL methods on uncontaminated benchmarks and across diverse model families to ensure trustworthy conclusions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 245,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10534",
        "abs_url": "https://arxiv.org/abs/2507.10534",
        "pdf_url": "https://arxiv.org/pdf/2507.10534",
        "title": "WildFX: A DAW-Powered Pipeline for In-the-Wild Audio FX Graph Modeling",
        "authors": [
            "Qihui Yang",
            "Taylor Berg-Kirkpatrick",
            "Julian McAuley",
            "Zachary Novack"
        ],
        "comments": "",
        "subjects": "Sound (cs.SD); Artificial Intelligence (cs.AI)",
        "abstract": "Despite rapid progress in end-to-end AI music generation, AI-driven modeling of professional Digital Signal Processing (DSP) workflows remains challenging. In particular, while there is growing interest in neural black-box modeling of audio effect graphs (e.g. reverb, compression, equalization), AI-based approaches struggle to replicate the nuanced signal flow and parameter interactions used in professional workflows. Existing differentiable plugin approaches often diverge from real-world tools, exhibiting inferior performance relative to simplified neural controllers under equivalent computational constraints. We introduce WildFX, a pipeline containerized with Docker for generating multi-track audio mixing datasets with rich effect graphs, powered by a professional Digital Audio Workstation (DAW) backend. WildFX supports seamless integration of cross-platform commercial plugins or any plugins in the wild, in VST/VST3/LV2/CLAP formats, enabling structural complexity (e.g., sidechains, crossovers) and achieving efficient parallelized processing. A minimalist metadata interface simplifies project/plugin configuration. Experiments demonstrate the pipeline's validity through blind estimation of mixing graphs, plugin/gain parameters, and its ability to bridge AI research with practical DSP demands. The code is available on: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 246,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10535",
        "abs_url": "https://arxiv.org/abs/2507.10535",
        "pdf_url": "https://arxiv.org/pdf/2507.10535",
        "title": "CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks",
        "authors": [
            "Hongchao Jiang",
            "Yiming Chen",
            "Yushi Cao",
            "Hung-yi Lee",
            "Robby T. Tan"
        ],
        "comments": "Dataset is available at this https URL",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Software Engineering (cs.SE)",
        "abstract": "Large Language Models (LLMs) have significantly advanced the state-of-the-art in various coding tasks. Beyond directly answering user queries, LLMs can also serve as judges, assessing and comparing the quality of responses generated by other models. Such an evaluation capability is crucial both for benchmarking different LLMs and for improving response quality through response ranking. However, despite the growing adoption of the LLM-as-a-Judge paradigm, its effectiveness in coding scenarios remains underexplored due to the absence of dedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a benchmark explicitly designed to evaluate the performance of LLM-as-a-Judge models across three critical coding tasks: code generation, code repair, and unit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge models, we find that recent thinking models significantly outperform non-thinking models on our carefully designed code judging tasks. Notably, even relatively small thinking models, such as Qwen3-8B, can outperform specially trained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still exhibit significant randomness in their judgment of coding tasks. For pairwise judging tasks, simply changing the order in which responses are presented can substantially impact accuracy. In addition, when judging code and unit tests written by different LLMs, LLM-as-a-Judge models also show variance in performance. This sensitivity raises concerns about the reliability and consistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal prompting strategies for LLM-as-a-Judge. We find that using pair-wise comparison outperforms scalar point-wise judging. Furthermore, retaining comments and reasoning in the full, unprocessed LLM response leads to improved judge performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 247,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10542",
        "abs_url": "https://arxiv.org/abs/2507.10542",
        "pdf_url": "https://arxiv.org/pdf/2507.10542",
        "title": "ScaffoldAvatar: High-Fidelity Gaussian Avatars with Patch Expressions",
        "authors": [
            "Shivangi Aneja",
            "Sebastian Weiss",
            "Irene Baeza",
            "Prashanth Chandran",
            "Gaspard Zoss",
            "Matthias Nießner",
            "Derek Bradley"
        ],
        "comments": "(SIGGRAPH 2025) Paper Video: this https URL Project Page: this https URL",
        "subjects": "Graphics (cs.GR); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Generating high-fidelity real-time animated sequences of photorealistic 3D head avatars is important for many graphics applications, including immersive telepresence and movies. This is a challenging problem particularly when rendering digital avatar close-ups for showing character's facial microfeatures and expressions. To capture the expressive, detailed nature of human heads, including skin furrowing and finer-scale facial movements, we propose to couple locally-defined facial expressions with 3D Gaussian splatting to enable creating ultra-high fidelity, expressive and photorealistic 3D head avatars. In contrast to previous works that operate on a global expression space, we condition our avatar's dynamics on patch-based local expression features and synthesize 3D Gaussians at a patch level. In particular, we leverage a patch-based geometric 3D face model to extract patch expressions and learn how to translate these into local dynamic skin appearance and motion by coupling the patches with anchor points of Scaffold-GS, a recent hierarchical scene representation. These anchors are then used to synthesize 3D Gaussians on-the-fly, conditioned by patch-expressions and viewing direction. We employ color-based densification and progressive training to obtain high-quality results and faster convergence for high resolution 3K training images. By leveraging patch-level expressions, ScaffoldAvatar consistently achieves state-of-the-art performance with visually natural motion, while encompassing diverse facial expressions and styles in real time.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 248,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10546",
        "abs_url": "https://arxiv.org/abs/2507.10546",
        "pdf_url": "https://arxiv.org/pdf/2507.10546",
        "title": "Disentangling Neural Disjunctive Normal Form Models",
        "authors": [
            "Kexin Gu Baugh",
            "Vincent Perreault",
            "Matthew Baugh",
            "Luke Dickens",
            "Katsumi Inoue",
            "Alessandra Russo"
        ],
        "comments": "Accepted at NeSy 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Neural Disjunctive Normal Form (DNF) based models are powerful and interpretable approaches to neuro-symbolic learning and have shown promising results in classification and reinforcement learning settings without prior knowledge of the tasks. However, their performance is degraded by the thresholding of the post-training symbolic translation process. We show here that part of the performance degradation during translation is due to its failure to disentangle the learned knowledge represented in the form of the networks' weights. We address this issue by proposing a new disentanglement method; by splitting nodes that encode nested rules into smaller independent nodes, we are able to better preserve the models' performance. Through experiments on binary, multiclass, and multilabel classification tasks (including those requiring predicate invention), we demonstrate that our disentanglement method provides compact and interpretable logical representations for the neural DNF-based models, with performance closer to that of their pre-translation counterparts. Our code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 249,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10548",
        "abs_url": "https://arxiv.org/abs/2507.10548",
        "pdf_url": "https://arxiv.org/pdf/2507.10548",
        "title": "EmbRACE-3K: Embodied Reasoning and Action in Complex Environments",
        "authors": [
            "Mingxian Lin",
            "Wei Huang",
            "Yitang Li",
            "Chengjie Jiang",
            "Kui Wu",
            "Fangwei Zhong",
            "Shengju Qian",
            "Xin Wang",
            "Xiaojuan Qi"
        ],
        "comments": "Project page: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Recent advanced vision-language models(VLMs) have demonstrated strong performance on passive, offline image and video understanding tasks. However, their effectiveness in embodied settings, which require online interaction and active scene understanding remains limited. In such scenarios, an agent perceives the environment from a first-person perspective, with each action dynamically shaping subsequent observations. Even state-of-the-art models such as GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment interactions, exhibiting clear limitations in spatial reasoning and long-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset of over 3,000 language-guided tasks situated in diverse, photorealistic environments constructed using Unreal Engine and the UnrealCV-Zoo framework. The tasks encompass a wide range of embodied challenges, including navigation, object manipulation, and multi-stage goal execution. Each task unfolds as a multi-step trajectory, pairing first-person visual observations with high-level instructions, grounded actions, and natural language rationales that express the agent's intent at every step. Using EmRACE-3K, we establish a benchmark to evaluate the embodied reasoning capabilities of VLMs across three key dimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage Goal Execution. In zero-shot settings, all models achieve success rates below 20%, underscoring the challenge posed by our benchmark and the current limitations of VLMs in interactive environments. To demonstrate the utility of EmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning followed by reinforcement learning. This approach yields substantial improvements across all three challenge categories, highlighting the dataset's effectiveness in enabling the development of embodied reasoning capabilities.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 250,
        "date": "2025-07-15",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-07-15?abs=True",
        "arxiv_id": "2507.10552",
        "abs_url": "https://arxiv.org/abs/2507.10552",
        "pdf_url": "https://arxiv.org/pdf/2507.10552",
        "title": "Self-supervised Learning on Camera Trap Footage Yields a Strong Universal Face Embedder",
        "authors": [
            "Vladimir Iashin",
            "Horace Lee",
            "Dan Schofield",
            "Andrew Zisserman"
        ],
        "comments": "Accepted for publication. Project page, code and weights: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Camera traps are revolutionising wildlife monitoring by capturing vast amounts of visual data; however, the manual identification of individual animals remains a significant bottleneck. This study introduces a fully self-supervised approach to learning robust chimpanzee face embeddings from unlabeled camera-trap footage. Leveraging the DINOv2 framework, we train Vision Transformers on automatically mined face crops, eliminating the need for identity labels. Our method demonstrates strong open-set re-identification performance, surpassing supervised baselines on challenging benchmarks such as Bossou, despite utilising no labelled data during training. This work underscores the potential of self-supervised learning in biodiversity monitoring and paves the way for scalable, non-invasive population studies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    }
]