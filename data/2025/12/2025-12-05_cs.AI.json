[
    {
        "order": 1,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04139",
        "abs_url": "https://arxiv.org/abs/2512.04139",
        "pdf_url": "https://arxiv.org/pdf/2512.04139",
        "title": "Solving N-Queen Problem using Las Vegas Algorithm with State Pruning",
        "authors": [
            "Susmita Sharma",
            "Aayush Shrestha",
            "Sitasma Thapa",
            "Prashant Timalsina",
            "Prakash Poudyal"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "The N-Queens problem, placing all N queens in a N x N chessboard where none attack the other, is a classic problem for constraint satisfaction algorithms. While complete methods like backtracking guarantee a solution, their exponential time complexity makes them impractical for large-scale instances thus, stochastic approaches, such as Las Vegas algorithm, are preferred. While it offers faster approximate solutions, it suffers from significant performance variance due to random placement of queens on the board. This research introduces a hybrid algorithm built on top of the standard Las Vegas framework through iterative pruning, dynamically eliminating invalid placements during the random assignment phase, thus this method effectively reduces the search space. The analysis results that traditional backtracking scales poorly with increasing N. In contrast, the proposed technique consistently generates valid solutions more rapidly, establishing it as a superior alternative to use where a single, timely solution is preferred over completeness. Although large N causes some performance variability, the algorithm demonstrates a highly effective trade-off between computational cost and solution fidelity, making it particularly suited for resource-constrained computing environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 2,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04144",
        "abs_url": "https://arxiv.org/abs/2512.04144",
        "pdf_url": "https://arxiv.org/pdf/2512.04144",
        "title": "RippleBench: Capturing Ripple Effects Using Existing Knowledge Repositories",
        "authors": [
            "Roy Rinberg",
            "Usha Bhalla",
            "Igor Shilov",
            "Flavio P. Calmon",
            "Rohit Gandikota"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Targeted interventions on language models, such as unlearning, debiasing, or model editing, are a central method for refining model behavior and keeping knowledge up to date. While these interventions aim to modify specific information within models (e.g., removing virology content), their effects often propagate to related but unintended areas (e.g., allergies); these side-effects are commonly referred to as the ripple effect. In this work, we present RippleBench-Maker, an automatic tool for generating Q&A datasets that allow for the measurement of ripple effects in any model-editing task. RippleBench-Maker builds on a Wikipedia-based RAG pipeline (WikiRAG) to generate multiple-choice questions at varying semantic distances from the target concept (e.g., the knowledge being unlearned). Using this framework, we construct RippleBench-Bio, a benchmark derived from the WMDP (Weapons of Mass Destruction Paper) dataset, a common unlearning benchmark. We evaluate eight state-of-the-art unlearning methods and find that all exhibit non-trivial accuracy drops on topics increasingly distant from the unlearned knowledge, each with distinct propagation profiles. To support ongoing research, we release our codebase for on-the-fly ripple evaluation, along with the benchmark, RippleBench-Bio.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 3,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04207",
        "abs_url": "https://arxiv.org/abs/2512.04207",
        "pdf_url": "https://arxiv.org/pdf/2512.04207",
        "title": "Orchestrator Multi-Agent Clinical Decision Support System for Secondary Headache Diagnosis in Primary Care",
        "authors": [
            "Xizhi Wu",
            "Nelly Estefanie Garduno-Rapp",
            "Justin F Rousseau",
            "Mounika Thakkallapally",
            "Hang Zhang",
            "Yuelyu Ji",
            "Shyam Visweswaran",
            "Yifan Peng",
            "Yanshan Wang"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Unlike most primary headaches, secondary headaches need specialized care and can have devastating consequences if not treated promptly. Clinical guidelines highlight several 'red flag' features, such as thunderclap onset, meningismus, papilledema, focal neurologic deficits, signs of temporal arteritis, systemic illness, and the 'worst headache of their life' presentation. Despite these guidelines, determining which patients require urgent evaluation remains challenging in primary care settings. Clinicians often work with limited time, incomplete information, and diverse symptom presentations, which can lead to under-recognition and inappropriate care. We present a large language model (LLM)-based multi-agent clinical decision support system built on an orchestrator-specialist architecture, designed to perform explicit and interpretable secondary headache diagnosis from free-text clinical vignettes. The multi-agent system decomposes diagnosis into seven domain-specialized agents, each producing a structured and evidence-grounded rationale, while a central orchestrator performs task decomposition and coordinates agent routing. We evaluated the multi-agent system using 90 expert-validated secondary headache cases and compared its performance with a single-LLM baseline across two prompting strategies: question-based prompting (QPrompt) and clinical practice guideline-based prompting (GPrompt). We tested five open-source LLMs (Qwen-30B, GPT-OSS-20B, Qwen-14B, Qwen-8B, and Llama-3.1-8B), and found that the orchestrated multi-agent system with GPrompt consistently achieved the highest F1 scores, with larger gains in smaller models. These findings demonstrate that structured multi-agent reasoning improves accuracy beyond prompt engineering alone and offers a transparent, clinically aligned approach for explainable decision support in secondary headache diagnosis.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 4,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04210",
        "abs_url": "https://arxiv.org/abs/2512.04210",
        "pdf_url": "https://arxiv.org/pdf/2512.04210",
        "title": "Balancing Safety and Helpfulness in Healthcare AI Assistants through Iterative Preference Alignment",
        "authors": [
            "Huy Nghiem",
            "Swetasudha Panda",
            "Devashish Khatwani",
            "Huy V. Nguyen",
            "Krishnaram Kenthapadi",
            "Hal DaumÃ© III"
        ],
        "comments": "ML4H 2025 Proceedings, Best Paper Award",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computers and Society (cs.CY)",
        "abstract": "Large Language Models (LLMs) are increasingly used in healthcare, yet ensuring their safety and trustworthiness remains a barrier to deployment. Conversational medical assistants must avoid unsafe compliance without over-refusing benign queries. We present an iterative post-deployment alignment framework that applies Kahneman-Tversky Optimization (KTO) and Direct Preference Optimization (DPO) to refine models against domain-specific safety signals. Using the CARES-18K benchmark for adversarial robustness, we evaluate four LLMs (Llama-3B/8B, Meditron-8B, Mistral-7B) across multiple cycles. Our results show up to 42% improvement in safety-related metrics for harmful query detection, alongside interesting trade-offs against erroneous refusals, thereby exposing architecture-dependent calibration biases. We also perform ablation studies to identify when self-evaluation is reliable and when external or finetuned judges are necessary to maximize performance gains. Our findings underscore the importance of adopting best practices that balance patient safety, user trust, and clinical utility in the design of conversational medical assistants.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 5,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04227",
        "abs_url": "https://arxiv.org/abs/2512.04227",
        "pdf_url": "https://arxiv.org/pdf/2512.04227",
        "title": "Educational Cone Model in Embedding Vector Spaces",
        "authors": [
            "Yo Ehara"
        ],
        "comments": "Accepted to the 33rd International Conference on Computers in Education (ICCE 2025)",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Human-annotated datasets with explicit difficulty ratings are essential in intelligent educational systems. Although embedding vector spaces are widely used to represent semantic closeness and are promising for analyzing text difficulty, the abundance of embedding methods creates a challenge in selecting the most suitable method. This study proposes the Educational Cone Model, which is a geometric framework based on the assumption that easier texts are less diverse (focusing on fundamental concepts), whereas harder texts are more diverse. This assumption leads to a cone-shaped distribution in the embedding space regardless of the embedding method used. The model frames the evaluation of embeddings as an optimization problem with the aim of detecting structured difficulty-based patterns. By designing specific loss functions, efficient closed-form solutions are derived that avoid costly computation. Empirical tests on real-world datasets validated the model's effectiveness and speed in identifying the embedding spaces that are best aligned with difficulty-annotated educational texts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 6,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04228",
        "abs_url": "https://arxiv.org/abs/2512.04228",
        "pdf_url": "https://arxiv.org/pdf/2512.04228",
        "title": "Addressing Logical Fallacies In Scientific Reasoning From Large Language Models: Towards a Dual-Inference Training Framework",
        "authors": [
            "Peter B. Walker",
            "Hannah Davidson",
            "Aiden Foster",
            "Matthew Lienert",
            "Thomas Pardue",
            "Dale Russell"
        ],
        "comments": "12 pages, 5 tables",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Large Language Models (LLMs) have transformed natural language processing and hold growing promise for advancing science, healthcare, and decision-making. Yet their training paradigms remain dominated by affirmation-based inference, akin to \\textit{modus ponens}, where accepted premises yield predicted consequents. While effective for generative fluency, this one-directional approach leaves models vulnerable to logical fallacies, adversarial manipulation, and failures in causal reasoning. This paper makes two contributions. First, it demonstrates how existing LLMs from major platforms exhibit systematic weaknesses when reasoning in scientific domains with negation, counterexamples, or faulty premises \\footnote{Code to recreate these experiments are at this https URL. Second, it introduces a dual-reasoning training framework that integrates affirmative generation with structured counterfactual denial. Grounded in formal logic, cognitive science, and adversarial training, this training paradigm formalizes a computational analogue of ``denying the antecedent'' as a mechanism for disconfirmation and robustness. By coupling generative synthesis with explicit negation-aware objectives, the framework enables models that not only affirm valid inferences but also reject invalid ones, yielding systems that are more resilient, interpretable, and aligned with human reasoning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 7,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04246",
        "abs_url": "https://arxiv.org/abs/2512.04246",
        "pdf_url": "https://arxiv.org/pdf/2512.04246",
        "title": "Toward Virtuous Reinforcement Learning",
        "authors": [
            "Majid Ghasemi",
            "Mark Crowley"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "This paper critiques common patterns in machine ethics for Reinforcement Learning (RL) and argues for a virtue focused alternative. We highlight two recurring limitations in much of the current literature: (i) rule based (deontological) methods that encode duties as constraints or shields often struggle under ambiguity and nonstationarity and do not cultivate lasting habits, and (ii) many reward based approaches, especially single objective RL, implicitly compress diverse moral considerations into a single scalar signal, which can obscure trade offs and invite proxy gaming in practice. We instead treat ethics as policy level dispositions, that is, relatively stable habits that hold up when incentives, partners, or contexts change. This shifts evaluation beyond rule checks or scalar returns toward trait summaries, durability under interventions, and explicit reporting of moral trade offs. Our roadmap combines four components: (1) social learning in multi agent RL to acquire virtue like patterns from imperfect but normatively informed exemplars; (2) multi objective and constrained formulations that preserve value conflicts and incorporate risk aware criteria to guard against harm; (3) affinity based regularization toward updateable virtue priors that support trait like stability under distribution shift while allowing norms to evolve; and (4) operationalizing diverse ethical traditions as practical control signals, making explicit the value and cultural assumptions that shape ethical RL benchmarks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 8,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04276",
        "abs_url": "https://arxiv.org/abs/2512.04276",
        "pdf_url": "https://arxiv.org/pdf/2512.04276",
        "title": "The Geometry of Benchmarks: A New Path Toward AGI",
        "authors": [
            "Przemyslaw Chojecki"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Statistics Theory (math.ST)",
        "abstract": "Benchmarks are the primary tool for assessing progress in artificial intelligence (AI), yet current practice evaluates models on isolated test suites and provides little guidance for reasoning about generality or autonomous self-improvement. Here we introduce a geometric framework in which all psychometric batteries for AI agents are treated as points in a structured moduli space, and agent performance is described by capability functionals over this space. First, we define an Autonomous AI (AAI) Scale, a Kardashev-style hierarchy of autonomy grounded in measurable performance on batteries spanning families of tasks (for example reasoning, planning, tool use and long-horizon control). Second, we construct a moduli space of batteries, identifying equivalence classes of benchmarks that are indistinguishable at the level of agent orderings and capability inferences. This geometry yields determinacy results: dense families of batteries suffice to certify performance on entire regions of task space. Third, we introduce a general Generator-Verifier-Updater (GVU) operator that subsumes reinforcement learning, self-play, debate and verifier-based fine-tuning as special cases, and we define a self-improvement coefficient $\\kappa$ as the Lie derivative of a capability functional along the induced flow. A variance inequality on the combined noise of generation and verification provides sufficient conditions for $\\kappa > 0$. Our results suggest that progress toward artificial general intelligence (AGI) is best understood as a flow on moduli of benchmarks, driven by GVU dynamics rather than by scores on individual leaderboards.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 9,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04287",
        "abs_url": "https://arxiv.org/abs/2512.04287",
        "pdf_url": "https://arxiv.org/pdf/2512.04287",
        "title": "Artificial Intelligence Applications in Horizon Scanning for Infectious Diseases",
        "authors": [
            "Ian Miles",
            "Mayumi Wakimoto",
            "Wagner Meira Jr.",
            "Daniela Paula",
            "Daylene Ticiane",
            "Bruno Rosa",
            "Jane Biddulph",
            "Stelios Georgiou",
            "Valdir Ermida"
        ],
        "comments": "21 pages, 1 box, 1 figure",
        "subjects": "Artificial Intelligence (cs.AI); Populations and Evolution (q-bio.PE)",
        "abstract": "This review explores the integration of Artificial Intelligence into Horizon Scanning, focusing on identifying and responding to emerging threats and opportunities linked to Infectious Diseases. We examine how AI tools can enhance signal detection, data monitoring, scenario analysis, and decision support. We also address the risks associated with AI adoption and propose strategies for effective implementation and governance. The findings contribute to the growing body of Foresight literature by demonstrating the potential and limitations of AI in Public Health preparedness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 10,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04302",
        "abs_url": "https://arxiv.org/abs/2512.04302",
        "pdf_url": "https://arxiv.org/pdf/2512.04302",
        "title": "Towards better dense rewards in Reinforcement Learning Applications",
        "authors": [
            "Shuyuan Zhang"
        ],
        "comments": "arXiv admin note: substantial text overlap with arXiv:2505.20417",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Finding meaningful and accurate dense rewards is a fundamental task in the field of reinforcement learning (RL) that enables agents to explore environments more efficiently. In traditional RL settings, agents learn optimal policies through interactions with an environment guided by reward signals. However, when these signals are sparse, delayed, or poorly aligned with the intended task objectives, agents often struggle to learn effectively. Dense reward functions, which provide informative feedback at every step or state transition, offer a potential solution by shaping agent behavior and accelerating learning. Despite their benefits, poorly crafted reward functions can lead to unintended behaviors, reward hacking, or inefficient exploration. This problem is particularly acute in complex or high-dimensional environments where handcrafted rewards are difficult to specify and validate. To address this, recent research has explored a variety of approaches, including inverse reinforcement learning, reward modeling from human preferences, and self-supervised learning of intrinsic rewards. While these methods offer promising directions, they often involve trade-offs between generality, scalability, and alignment with human intent. This proposal explores several approaches to dealing with these unsolved problems and enhancing the effectiveness and reliability of dense reward construction in different RL applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 11,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04339",
        "abs_url": "https://arxiv.org/abs/2512.04339",
        "pdf_url": "https://arxiv.org/pdf/2512.04339",
        "title": "A Conceptual Model for AI Adoption in Financial Decision-Making: Addressing the Unique Challenges of Small and Medium-Sized Enterprises",
        "authors": [
            "Manh Chien Vu",
            "Thang Le Dinh",
            "Manh Chien Vu",
            "Tran Duc Le",
            "Thi Lien Huong Nguyen"
        ],
        "comments": "The Eighth International Econometric and Financial Conference of Vietnam - ECONVN2025, Ho Chi Minh City, Vietnam, January 13-14-15, 2025",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "The adoption of artificial intelligence (AI) offers transformative potential for small and medium-sized enterprises (SMEs), particularly in enhancing financial decision-making processes. However, SMEs often face significant barriers to implementing AI technologies, including limited resources, technical expertise, and data management capabilities. This paper presents a conceptual model for the adoption of AI in financial decision-making for SMEs. The proposed model addresses key challenges faced by SMEs, including limited resources, technical expertise, and data management capabilities. The model is structured into layers: data sources, data processing and integration, AI model deployment, decision support and automation, and validation and risk management. By implementing AI incrementally, SMEs can optimize financial forecasting, budgeting, investment strategies, and risk management. This paper highlights the importance of data quality and continuous model validation, providing a practical roadmap for SMEs to integrate AI into their financial operations. The study concludes with implications for SMEs adopting AI-driven financial processes and suggests areas for future research in AI applications for SME finance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 12,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04359",
        "abs_url": "https://arxiv.org/abs/2512.04359",
        "pdf_url": "https://arxiv.org/pdf/2512.04359",
        "title": "Efficient Reinforcement Learning with Semantic and Token Entropy for LLM Reasoning",
        "authors": [
            "Hongye Cao",
            "Zhixin Bai",
            "Ziyue Peng",
            "Boyan Wang",
            "Tianpei Yang",
            "Jing Huo",
            "Yuyao Zhang",
            "Yang Gao"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Reinforcement learning with verifiable rewards (RLVR) has demonstrated superior performance in enhancing the reasoning capability of large language models (LLMs). However, this accuracy-oriented learning paradigm often suffers from entropy collapse, which reduces policy exploration and limits reasoning capabilities. To address this challenge, we propose an efficient reinforcement learning framework that leverages entropy signals at both the semantic and token levels to improve reasoning. From the data perspective, we introduce semantic entropy-guided curriculum learning, organizing training data from low to high semantic entropy to guide progressive optimization from easier to more challenging tasks. For the algorithmic design, we adopt non-uniform token treatment by imposing KL regularization on low-entropy tokens that critically impact policy exploration and applying stronger constraints on high-covariance portions within these tokens. By jointly optimizing data organization and algorithmic design, our method effectively mitigates entropy collapse and enhances LLM reasoning. Experimental results across 6 benchmarks with 3 different parameter-scale base models demonstrate that our method outperforms other entropy-based approaches in improving reasoning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 13,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04367",
        "abs_url": "https://arxiv.org/abs/2512.04367",
        "pdf_url": "https://arxiv.org/pdf/2512.04367",
        "title": "AgentBay: A Hybrid Interaction Sandbox for Seamless Human-AI Intervention in Agentic Systems",
        "authors": [
            "Yun Piao",
            "Hongbo Min",
            "Hang Su",
            "Leilei Zhang",
            "Lei Wang",
            "Yue Yin",
            "Xiao Wu",
            "Zhejing Xu",
            "Liwei Qu",
            "Hang Li",
            "Xinxin Zeng",
            "Wei Tian",
            "Fei Yu",
            "Xiaowei Li",
            "Jiayi Jiang",
            "Tongxu Liu",
            "Hao Tian",
            "Yufei Que",
            "Xiaobing Tu",
            "Bing Suo",
            "Yuebing Li",
            "Xiangting Chen",
            "Zeen Zhao",
            "Jiaming Tang",
            "Wei Huang",
            "Xuguang Li",
            "Jing Zhao",
            "Jin Li",
            "Jie Shen",
            "Jinkui Ren",
            "Xiantao Zhang"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "The rapid advancement of Large Language Models (LLMs) is catalyzing a shift towards autonomous AI Agents capable of executing complex, multi-step tasks. However, these agents remain brittle when faced with real-world exceptions, making Human-in-the-Loop (HITL) supervision essential for mission-critical applications. In this paper, we present AgentBay, a novel sandbox service designed from the ground up for hybrid interaction. AgentBay provides secure, isolated execution environments spanning Windows, Linux, Android, Web Browsers, and Code interpreters. Its core contribution is a unified session accessible via a hybrid control interface: An AI agent can interact programmatically via mainstream interfaces (MCP, Open Source SDK), while a human operator can, at any moment, seamlessly take over full manual control. This seamless intervention is enabled by Adaptive Streaming Protocol (ASP). Unlike traditional VNC/RDP, ASP is specifically engineered for this hybrid use case, delivering an ultra-low-latency, smoother user experience that remains resilient even in weak network environments. It achieves this by dynamically blending command-based and video-based streaming, adapting its encoding strategy based on network conditions and the current controller (AI or human). Our evaluation demonstrates strong results in security, performance, and task completion rates. In a benchmark of complex tasks, the AgentBay (Agent + Human) model achieved more than 48% success rate improvement. Furthermore, our ASP protocol reduces bandwidth consumption by up to 50% compared to standard RDP, and in end-to-end latency with around 5% reduction, especially under poor network conditions. We posit that AgentBay provides a foundational primitive for building the next generation of reliable, human-supervised autonomous systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 14,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04408",
        "abs_url": "https://arxiv.org/abs/2512.04408",
        "pdf_url": "https://arxiv.org/pdf/2512.04408",
        "title": "Executable Governance for AI: Translating Policies into Rules Using LLMs",
        "authors": [
            "Gautam Varma Datla",
            "Anudeep Vurity",
            "Tejaswani Dash",
            "Tazeem Ahmad",
            "Mohd Adnan",
            "Saima Rafi"
        ],
        "comments": "Accepted to AAAI-26 AI Governance Workshop (in-person presentation); 10 pages, 5 figures",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "AI policy guidance is predominantly written as prose, which practitioners must first convert into executable rules before frameworks can evaluate or enforce them. This manual step is slow, error-prone, difficult to scale, and often delays the use of safeguards in real-world deployments. To address this gap, we present Policy-to-Tests (P2T), a framework that converts natural-language policy documents into normalized, machine-readable rules. The framework comprises a pipeline and a compact domain-specific language (DSL) that encodes hazards, scope, conditions, exceptions, and required evidence, yielding a canonical representation of extracted rules. To test the framework beyond a single policy, we apply it across general frameworks, sector guidance, and enterprise standards, extracting obligation-bearing clauses and converting them into executable rules. These AI-generated rules closely match strong human baselines on span-level and rule-level metrics, with robust inter-annotator agreement on the gold set. To evaluate downstream behavioral and safety impact, we add HIPAA-derived safeguards to a generative agent and compare it with an otherwise identical agent without guardrails. An LLM-based judge, aligned with gold-standard criteria, measures violation rates and robustness to obfuscated and compositional prompts. Detailed results are provided in the appendix. We release the codebase, DSL, prompts, and rule sets as open-source resources to enable reproducible evaluation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 15,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04416",
        "abs_url": "https://arxiv.org/abs/2512.04416",
        "pdf_url": "https://arxiv.org/pdf/2512.04416",
        "title": "GovBench: Benchmarking LLM Agents for Real-World Data Governance Workflows",
        "authors": [
            "Zhou Liu",
            "Zhaoyang Han",
            "Guochen Yan",
            "Hao Liang",
            "Bohan Zeng",
            "Xing Chen",
            "Yuanfeng Song",
            "Wentao Zhang"
        ],
        "comments": "Equal contribution: Zhou Liu and Zhaoyang Han. Corresponding authors: Yuanfeng Song and Wentao Zhang",
        "subjects": "Artificial Intelligence (cs.AI); Software Engineering (cs.SE)",
        "abstract": "Data governance ensures data quality, security, and compliance through policies and standards, a critical foundation for scaling modern AI development. Recently, large language models (LLMs) have emerged as a promising solution for automating data governance by translating user intent into executable transformation code. However, existing benchmarks for automated data science often emphasize snippet-level coding or high-level analytics, failing to capture the unique challenge of data governance: ensuring the correctness and quality of the data itself. To bridge this gap, we introduce GovBench, a benchmark featuring 150 diverse tasks grounded in real-world scenarios, built on data from actual cases. GovBench employs a novel \"reversed-objective\" methodology to synthesize realistic noise and utilizes rigorous metrics to assess end-to-end pipeline reliability. Our analysis on GovBench reveals that current models struggle with complex, multi-step workflows and lack robust error-correction mechanisms. Consequently, we propose DataGovAgent, a framework utilizing a Planner-Executor-Evaluator architecture that integrates constraint-based planning, retrieval-augmented generation, and sandboxed feedback-driven debugging. Experimental results show that DataGovAgent significantly boosts the Average Task Score (ATS) on complex tasks from 39.7 to 54.9 and reduces debugging iterations by over 77.9 percent compared to general-purpose baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 16,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04419",
        "abs_url": "https://arxiv.org/abs/2512.04419",
        "pdf_url": "https://arxiv.org/pdf/2512.04419",
        "title": "Solving LLM Repetition Problem in Production: A Comprehensive Study of Multiple Solutions",
        "authors": [
            "Weiwei Wang",
            "Weijie Zou",
            "Jiyong Min"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "The repetition problem, where Large Language Models (LLMs) continuously generate repetitive content without proper termination, poses a critical challenge in production deployments, causing severe performance degradation and system stalling. This paper presents a comprehensive investigation and multiple practical solutions for the repetition problem encountered in real-world batch code interpretation tasks. We identify three distinct repetition patterns: (1) business rule generation repetition, (2) method call relationship analysis repetition, and (3) PlantUML diagram syntax generation repetition. Through rigorous theoretical analysis based on Markov models, we establish that the root cause lies in greedy decoding's inability to escape repetitive loops, exacerbated by self-reinforcement effects. Our comprehensive experimental evaluation demonstrates three viable solutions: (1) Beam Search decoding with early_stopping=True serves as a universal post-hoc mechanism that effectively resolves all three repetition patterns; (2) presence_penalty hyperparameter provides an effective solution specifically for BadCase 1; and (3) Direct Preference Optimization (DPO) fine-tuning offers a universal model-level solution for all three BadCases. The primary value of this work lies in combining first-hand production experience with extensive experimental validation. Our main contributions include systematic theoretical analysis of repetition mechanisms, comprehensive evaluation of multiple solutions with task-specific applicability mapping, identification of early_stopping as the critical parameter for Beam Search effectiveness, and practical production-ready solutions validated in real deployment environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 17,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04442",
        "abs_url": "https://arxiv.org/abs/2512.04442",
        "pdf_url": "https://arxiv.org/pdf/2512.04442",
        "title": "TaskEval: Synthesised Evaluation for Foundation-Model Tasks",
        "authors": [
            "Dilani Widanapathiranage",
            "Scott Barnett",
            "Stefanus Kurniawan",
            "Wannita Takerngsaksiri"
        ],
        "comments": "5 pages, 3 figures",
        "subjects": "Artificial Intelligence (cs.AI); Software Engineering (cs.SE)",
        "abstract": "Hallucinations are a key concern when creating applications that rely on Foundation models (FMs). Understanding where and how these subtle failures occur in an application relies on evaluation methods known as \\textit{evals}. Prior work focuses on defining new eval methods or benchmark datasets for specific tasks. However, neither helps a software team with a task-specific FM application when there is no metric or dataset. The demand for both automated approaches and deep integration of human insight makes this a challenging problem. We address this gap by proposing an approach to synthesise a FM task-specific evaluator program that provides automation and a custom UI for capturing feedback. The core novelty of our approach lies in: (1) a task-agnostic meta-model that captures properties of any FM task, (2) an interaction protocol for efficient use of human feedback, and (3) an eval synthesiser that selects or generates an appropriate set of evals. We implement our approach in \\toolname and demonstrate the concept on two diverse FM tasks: chart data extraction and document question answering. A preliminary evaluation on the quality of our selected evals shows 93\\% and 90\\% accuracy respectively. Our research tackles a growing problem facing engineering teams, how to evaluate and review outputs from FM tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 18,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04463",
        "abs_url": "https://arxiv.org/abs/2512.04463",
        "pdf_url": "https://arxiv.org/pdf/2512.04463",
        "title": "MARL Warehouse Robots",
        "authors": [
            "Price Allman",
            "Lian Thang",
            "Dre Simmons",
            "Salmon Riaz"
        ],
        "comments": "6 pages, 4 tables. Project documentation: this https URL",
        "subjects": "Artificial Intelligence (cs.AI); Robotics (cs.RO)",
        "abstract": "We present a comparative study of multi-agent reinforcement learning (MARL) algorithms for cooperative warehouse robotics. We evaluate QMIX and IPPO on the Robotic Warehouse (RWARE) environment and a custom Unity 3D simulation. Our experiments reveal that QMIX's value decomposition significantly outperforms independent learning approaches (achieving 3.25 mean return vs. 0.38 for advanced IPPO), but requires extensive hyperparameter tuning -- particularly extended epsilon annealing (5M+ steps) for sparse reward discovery. We demonstrate successful deployment in Unity ML-Agents, achieving consistent package delivery after 1M training steps. While MARL shows promise for small-scale deployments (2-4 robots), significant scaling challenges remain. Code and analyses: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 19,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04469",
        "abs_url": "https://arxiv.org/abs/2512.04469",
        "pdf_url": "https://arxiv.org/pdf/2512.04469",
        "title": "Mathematical Framing for Different Agent Strategies",
        "authors": [
            "Philip Stephens",
            "Emmanuel Salawu"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "We introduce a unified mathematical and probabilistic framework for understanding and comparing diverse AI agent strategies. We bridge the gap between high-level agent design concepts, such as ReAct, multi-agent systems, and control flows, and a rigorous mathematical formulation. Our approach frames agentic processes as a chain of probabilities, enabling a detailed analysis of how different strategies manipulate these probabilities to achieve desired outcomes. Our framework provides a common language for discussing the trade-offs inherent in various agent architectures. One of our many key contributions is the introduction of the \"Degrees of Freedom\" concept, which intuitively differentiates the optimizable levers available for each approach, thereby guiding the selection of appropriate strategies for specific tasks. This work aims to enhance the clarity and precision in designing and evaluating AI agents, offering insights into maximizing the probability of successful actions within complex agentic systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 20,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04480",
        "abs_url": "https://arxiv.org/abs/2512.04480",
        "pdf_url": "https://arxiv.org/pdf/2512.04480",
        "title": "AI-Assisted Game Management Decisions: A Fuzzy Logic Approach to Real-Time Substituitions",
        "authors": [
            "Pedro Passos"
        ],
        "comments": "33 pages, 7 figures",
        "subjects": "Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Systems and Control (eess.SY); Optimization and Control (math.OC)",
        "abstract": "In elite soccer, substitution decisions entail significant financial and sporting consequences yet remain heavily reliant on intuition or predictive models that merely mimic historical biases. This paper introduces a Fuzzy Logic based Decision Support System (DSS) designed for real time, prescriptive game management. Unlike traditional Machine Learning approaches that encounter a predictive ceiling by attempting to replicate human behavior, our system audits performance through an objective, rule based inference engine. We propose a methodological advancement by reformulating the PlayeRank metric into a Cumulative Mean with Role Aware Normalization, eliminating the play time exposure bias inherent in cumulative sum models to enable accurate intra match comparison. The system integrates this refined metric with physiological proxies (fatigue) and contextual variables (disciplinary risk modulated by tactical role) to calculate a dynamic Substitution Priority (P final). Validation via a case study of the 2018 FIFA World Cup match between Brazil and Belgium demonstrates the system's ecological validity: it not only aligned with expert consensus on executed substitutions (for example Gabriel Jesus) but, crucially, identified high risk scenarios ignored by human decision makers. Specifically, the model flagged the \"FAGNER Paradox\" - a maximum priority defensive risk - minutes before a critical yellow card, and detected the \"Lukaku Paradox\", where an isolated assist masked a severe drop in participation. These results confirm that Fuzzy Logic offers a transparent, explainable, and superior alternative to black box models for optimizing real time tactical decisions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 21,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04488",
        "abs_url": "https://arxiv.org/abs/2512.04488",
        "pdf_url": "https://arxiv.org/pdf/2512.04488",
        "title": "Persona-based Multi-Agent Collaboration for Brainstorming",
        "authors": [
            "Nate Straub",
            "Saara Khan",
            "Kat Jay",
            "Brian Cabral",
            "Oskar Linde"
        ],
        "comments": "12 pages, 8 figures",
        "subjects": "Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)",
        "abstract": "We demonstrate the importance of persona-based multi-agents brainstorming for both diverse topics and subject matter ideation. Prior work has shown that generalized multi-agent collaboration often provides better reasoning than a single agent alone. In this paper, we propose and develop a framework for persona-based agent selection, showing how persona domain curation can improve brainstorming outcomes. Using multiple experimental setups, we evaluate brainstorming outputs across different persona pairings (e.g., Doctor vs VR Engineer) and A2A (agent-to-agent) dynamics (separate, together, separate-then-together). Our results show that (1) persona choice shapes idea domains, (2) collaboration mode shifts diversity of idea generation, and (3) multi-agent persona-driven brainstorming produces idea depth and cross-domain coverage.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 22,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04500",
        "abs_url": "https://arxiv.org/abs/2512.04500",
        "pdf_url": "https://arxiv.org/pdf/2512.04500",
        "title": "A Modular Cognitive Architecture for Assisted Reasoning: The Nemosine Framework",
        "authors": [
            "Edervaldo Melo"
        ],
        "comments": "6 pages, 1 figure. First version",
        "subjects": "Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Multiagent Systems (cs.MA)",
        "abstract": "This paper presents the Nemosine Framework, a modular cognitive architecture designed to support assisted reasoning, structured thinking, and systematic analysis. The model operates through functional cognitive modules (\"personas\") that organize tasks such as planning, evaluation, cross-checking, and narrative synthesis. The framework combines principles from metacognition, distributed cognition, and modular cognitive systems to offer an operational structure for assisted problem-solving and decision support. The architecture is documented through formal specification, internal consistency criteria, and reproducible structural components. The goal is to provide a clear conceptual basis for future computational implementations and to contribute to the study of symbolic-modular architectures for reasoning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 23,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04513",
        "abs_url": "https://arxiv.org/abs/2512.04513",
        "pdf_url": "https://arxiv.org/pdf/2512.04513",
        "title": "BiTAgent: A Task-Aware Modular Framework for Bidirectional Coupling between Multimodal Large Language Models and World Models",
        "authors": [
            "Yu-Wei Zhan",
            "Xin Wang",
            "Pengzhe Mao",
            "Tongtong Feng",
            "Ren Wang",
            "Wenwu Zhu"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Building generalist embodied agents requires a unified system that can interpret multimodal goals, model environment dynamics, and execute reliable actions across diverse real-world tasks. Multimodal large language models (MLLMs) offer strong semantic priors and cross-modal generalization, while world models (WMs) provide actionable latent dynamics for prediction and control. Their combination holds promise for open-ended embodied intelligence, yet introduces two key challenges: (1) establishing a tight coupling between the semantic intent from MLLMs and the dynamic state representations within the WM's latent space, and (2) achieving task-aware adaptability that supports multi-task learning and cross-environment generalization. To address these limitations, we propose BiTAgent, a task-aware dynamic joint framework that enables bidirectional coupling between MLLMs and WMs. BiTAgent establishes two complementary pathways: a forward path that injects MLLM representations into the WM's latent space for semantically guided imagination, and a backward path where WM-generated feedback refines the MLLM's semantic space via dense text-conditioned rewards. This bidirectional interaction is realized through three synergistic components: Task-Aware Dynamic Joint Learning, Task-Aware Behavior Learning, and MLLM-WM Joint Optimization, which together harmonize semantic reasoning and dynamic prediction. Extensive experiments across multi-task and cross-environment settings demonstrate superior stability and generalization over state-of-the-art baselines, marking a step toward open-ended embodied learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 24,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04529",
        "abs_url": "https://arxiv.org/abs/2512.04529",
        "pdf_url": "https://arxiv.org/pdf/2512.04529",
        "title": "SlideGen: Collaborative Multimodal Agents for Scientific Slide Generation",
        "authors": [
            "Xin Liang",
            "Xiang Zhang",
            "Yiwei Xu",
            "Siqi Sun",
            "Chenyu You"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Generating academic slides from scientific papers is a challenging multimodal reasoning task that requires both long context understanding and deliberate visual planning. Existing approaches largely reduce it to text only summarization, overlooking the visual component and design intensive nature of slide creation. In this paper we introduce SlideGen, an agentic, modular, and visual in the loop framework for scientific paper to slide generation. SlideGen orchestrates a group of vision language agents that reason collaboratively over the document structure and semantics, producing editable PPTX slides with logical flow and compelling visual presentation. By integrating coordinated outlining, mapping, arrangement, note synthesis, and iterative refinement, our system consistently delivers slides of expert level quality. Across diverse benchmarks and strong baselines, SlideGen outperforms existing methods in visual quality, content faithfulness, and readability, positioning it as the new state of the art in automated slide generation. Our work establishes a foundation for design aware multimodal slide generation, demonstrating how agentic collaboration can bridge understanding and presentation in complex multimodal reasoning tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 25,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04535",
        "abs_url": "https://arxiv.org/abs/2512.04535",
        "pdf_url": "https://arxiv.org/pdf/2512.04535",
        "title": "GTM: Simulating the World of Tools for AI Agents",
        "authors": [
            "Zhenzhen Ren",
            "Xinpeng Zhang",
            "Zhenxing Qian",
            "Yan Gao",
            "Yu Shi",
            "Shuxin Zheng",
            "Jiyan He"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "The integration of external tools is pivotal for empowering Large Language Model (LLM) agents with real-world capabilities. However, training these agents through direct, continuous interaction with diverse tools is often prohibitively expensive, slow, and introduces additional development and maintenance overhead. To address this challenge, we introduce the Generalist Tool Model (GTM), a 1.5-billion-parameter model that learns to act as a universal tool simulator. With only prompt-level configuration, GTM accesses tool functionalities along with input arguments and generates outputs that faithfully mimic real tool execution, providing a fast and cost-effective solution that eliminates development overhead. To build GTM, we propose the Context-Aware Response Generation (CARG) pipeline, which synthesizes comprehensive training data covering over 20,000 tools across 300 domains including physics, medicine, robotics, and finance. Through this pipeline, GTM learns to produce not only syntactically correct outputs but also logically coherent and contextually appropriate responses. Experiments demonstrate that GTM produces high-quality outputs with strong consistency and reliability. Besides when used in real reinforcement learning scenarios for agent training, GTM exhibits significantly faster simulation speed compared to real tools while maintaining comparable output quality, along with remarkable generalization and domain adaptability. Our results establish GTM as a foundational component for developing future AI agents, enabling efficient and scalable training of tool-augmented systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 26,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04598",
        "abs_url": "https://arxiv.org/abs/2512.04598",
        "pdf_url": "https://arxiv.org/pdf/2512.04598",
        "title": "The Ethics of Generative AI",
        "authors": [
            "Michael Klenk"
        ],
        "comments": "Draft version to appear as a chapter in the Encyclopedia of Applied Ethics, 3rd Edition, edited by Ruth Chadwick",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "This chapter discusses the ethics of generative AI. It provides a technical primer to show how generative AI affords experiencing technology as if it were human, and this affordance provides a fruitful focus for the philosophical ethics of generative AI. It then shows how generative AI can both aggravate and alleviate familiar ethical concerns in AI ethics, including responsibility, privacy, bias and fairness, and forms of alienation and exploitation. Finally, the chapter examines ethical questions that arise specifically from generative AI's mimetic generativity, such as debates about authorship and credit, the emergence of as-if social relationships with machines, and new forms of influence, persuasion, and manipulation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 27,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04618",
        "abs_url": "https://arxiv.org/abs/2512.04618",
        "pdf_url": "https://arxiv.org/pdf/2512.04618",
        "title": "Neural Decoding of Overt Speech from ECoG Using Vision Transformers and Contrastive Representation Learning",
        "authors": [
            "Mohamed Baha Ben Ticha",
            "Xingchen Ran",
            "Guillaume Saldanha",
            "GaÃ«l Le Godais",
            "PhilÃ©mon Roussel",
            "Marc Aubert",
            "Amina Fontanell",
            "Thomas Costecalde",
            "Lucas Struber",
            "Serpil Karakas",
            "Shaomin Zhang",
            "Philippe Kahane",
            "Guillaume Charvet",
            "StÃ©phan ChabardÃ¨s",
            "Blaise Yvert"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Speech Brain Computer Interfaces (BCIs) offer promising solutions to people with severe paralysis unable to communicate. A number of recent studies have demonstrated convincing reconstruction of intelligible speech from surface electrocorticographic (ECoG) or intracortical recordings by predicting a series of phonemes or words and using downstream language models to obtain meaningful sentences. A current challenge is to reconstruct speech in a streaming mode by directly regressing cortical signals into acoustic speech. While this has been achieved recently using intracortical data, further work is needed to obtain comparable results with surface ECoG recordings. In particular, optimizing neural decoders becomes critical in this case. Here we present an offline speech decoding pipeline based on an encoder-decoder deep neural architecture, integrating Vision Transformers and contrastive learning to enhance the direct regression of speech from ECoG signals. The approach is evaluated on two datasets, one obtained with clinical subdural electrodes in an epileptic patient, and another obtained with the fully implantable WIMAGINE epidural system in a participant of a motor BCI trial. To our knowledge this presents a first attempt to decode speech from a fully implantable and wireless epidural recording system offering perspectives for long-term use.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 28,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04629",
        "abs_url": "https://arxiv.org/abs/2512.04629",
        "pdf_url": "https://arxiv.org/pdf/2512.04629",
        "title": "BioMedGPT-Mol: Multi-task Learning for Molecular Understanding and Generation",
        "authors": [
            "Chenyang Zuo",
            "Siqi Fan",
            "Zaiqing Nie"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Molecules play a crucial role in biomedical research and discovery, particularly in the field of small molecule drug development. Given the rapid advancements in large language models, especially the recent emergence of reasoning models, it is natural to explore how a general-purpose language model can be efficiently adapted for molecular science applications. In this work, we introduce BioMedGPT-Mol, a molecular language model designed to support molecular understanding and generation tasks. By curating and unifying existing public instruction datasets, we have assembled a large-scale, comprehensive, and high-quality training dataset. The model is then fine-tuned through a meticulously designed multi-task learning framework. On a consolidated benchmark derived from LlaSMol, TOMG-Bench, and MuMOInstruct, BioMedGPT-Mol achieves remarkable performance. Our experimental results demonstrate that a general-purpose reasoning model can be effectively and efficiently post-trained into a professional molecular language model through a well-structured multi-task curriculum. Leveraging the power of it, we further explore retrosynthetic planning task, and the performance on RetroBench demonstrates its competitive capability of acting as an end-to-end retrosynthetic planner. We anticipate that our approach can be extended to other biomedical scientific domains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 29,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04632",
        "abs_url": "https://arxiv.org/abs/2512.04632",
        "pdf_url": "https://arxiv.org/pdf/2512.04632",
        "title": "Turbo-Muon: Accelerating Orthogonality-Based Optimization with Pre-Conditioning",
        "authors": [
            "Thibaut Boissin",
            "Thomas Massena",
            "Franck Mamalet",
            "Mathieu Serrurier"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Orthogonality-based optimizers, such as Muon, have recently shown strong performance across large-scale training and community-driven efficiency challenges. However, these methods rely on a costly gradient orthogonalization step. Even efficient iterative approximations such as Newton-Schulz remain expensive, typically requiring dozens of matrix multiplications to converge. We introduce a preconditioning procedure that accelerates Newton-Schulz convergence and reduces its computational cost. We evaluate its impact and show that the overhead of our preconditioning can be made negligible. Furthermore, the faster convergence it enables allows us to remove one iteration out of the usual five without degrading approximation quality. Our publicly available implementation achieves up to a 2.8x speedup in the Newton-Schulz approximation. We also show that this has a direct impact on end-to-end training runtime with 5-10% improvement in realistic training scenarios across two efficiency-focused tasks. On challenging language or vision tasks, we validate that our method maintains equal or superior model performance while improving runtime. Crucially, these improvements require no hyperparameter tuning and can be adopted as a simple drop-in replacement. Our code is publicly available on github.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 30,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04691",
        "abs_url": "https://arxiv.org/abs/2512.04691",
        "pdf_url": "https://arxiv.org/pdf/2512.04691",
        "title": "Towards Ethical Multi-Agent Systems of Large Language Models: A Mechanistic Interpretability Perspective",
        "authors": [
            "Jae Hee Lee",
            "Anne Lauscher",
            "Stefano V. Albrecht"
        ],
        "comments": "Accepted to LaMAS 2026@AAAI'26 (this https URL)",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Multiagent Systems (cs.MA)",
        "abstract": "Large language models (LLMs) have been widely deployed in various applications, often functioning as autonomous agents that interact with each other in multi-agent systems. While these systems have shown promise in enhancing capabilities and enabling complex tasks, they also pose significant ethical challenges. This position paper outlines a research agenda aimed at ensuring the ethical behavior of multi-agent systems of LLMs (MALMs) from the perspective of mechanistic interpretability. We identify three key research challenges: (i) developing comprehensive evaluation frameworks to assess ethical behavior at individual, interactional, and systemic levels; (ii) elucidating the internal mechanisms that give rise to emergent behaviors through mechanistic interpretability; and (iii) implementing targeted parameter-efficient alignment techniques to steer MALMs towards ethical behaviors without compromising their performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 31,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04714",
        "abs_url": "https://arxiv.org/abs/2512.04714",
        "pdf_url": "https://arxiv.org/pdf/2512.04714",
        "title": "Playing the Player: A Heuristic Framework for Adaptive Poker AI",
        "authors": [
            "Andrew Paterson",
            "Carl Sanders"
        ],
        "comments": "49 pages, 39 figures. White Paper by Spiderdime Systems",
        "subjects": "Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT)",
        "abstract": "For years, the discourse around poker AI has been dominated by the concept of solvers and the pursuit of unexploitable, machine-perfect play. This paper challenges that orthodoxy. It presents Patrick, an AI built on the contrary philosophy: that the path to victory lies not in being unexploitable, but in being maximally exploitative. Patrick's architecture is a purpose-built engine for understanding and attacking the flawed, psychological, and often irrational nature of human opponents. Through detailed analysis of its design, its novel prediction-anchored learning method, and its profitable performance in a 64,267-hand trial, this paper makes the case that the solved myth is a distraction from the real, far more interesting challenge: creating AI that can master the art of human imperfection.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 32,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04727",
        "abs_url": "https://arxiv.org/abs/2512.04727",
        "pdf_url": "https://arxiv.org/pdf/2512.04727",
        "title": "Sequential Enumeration in Large Language Models",
        "authors": [
            "Kuinan Hou",
            "Marco Zorzi",
            "Alberto Testolin"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Reliably counting and generating sequences of items remain a significant challenge for neural networks, including Large Language Models (LLMs). Indeed, although this capability is readily handled by rule-based symbolic systems based on serial computation, learning to systematically deploy counting procedures is difficult for neural models, which should acquire these skills through learning. Previous research has demonstrated that recurrent architectures can only approximately track and enumerate sequences of events, and it remains unclear whether modern deep learning systems, including LLMs, can deploy systematic counting procedures over sequences of discrete symbols. This paper aims to fill this gap by investigating the sequential enumeration abilities of five state-of-the-art LLMs, including proprietary, open-source, and reasoning models. We probe LLMs in sequential naming and production tasks involving lists of letters and words, adopting a variety of prompting instructions to explore the role of chain-of-thought in the spontaneous emerging of counting strategies. We also evaluate open-source models with the same architecture but increasing size to see whether the mastering of counting principles follows scaling laws, and we analyze the embedding dynamics during sequential enumeration to investigate the emergent encoding of numerosity. We find that some LLMs are indeed capable of deploying counting procedures when explicitly prompted to do so, but none of them spontaneously engage in counting when simply asked to enumerate the number of items in a sequence. Our results suggest that, despite their impressive emergent abilities, LLMs cannot yet robustly and systematically deploy counting procedures, highlighting a persistent gap between neural and symbolic approaches to compositional generalization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 33,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04764",
        "abs_url": "https://arxiv.org/abs/2512.04764",
        "pdf_url": "https://arxiv.org/pdf/2512.04764",
        "title": "Human Cognitive Biases in Explanation-Based Interaction: The Case of Within and Between Session Order Effect",
        "authors": [
            "Dario Pesenti",
            "Alessandro Bogani",
            "Katya Tentori",
            "Stefano Teso"
        ],
        "comments": "18 pages, 10 figures, published at AAAI 2026",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Explanatory Interactive Learning (XIL) is a powerful interactive learning framework designed to enable users to customize and correct AI models by interacting with their explanations. In a nutshell, XIL algorithms select a number of items on which an AI model made a decision (e.g. images and their tags) and present them to users, together with corresponding explanations (e.g. image regions that drive the model's decision). Then, users supply corrective feedback for the explanations, which the algorithm uses to improve the model. Despite showing promise in debugging tasks, recent studies have raised concerns that explanatory interaction may trigger order effects, a well-known cognitive bias in which the sequence of presented items influences users' trust and, critically, the quality of their feedback. We argue that these studies are not entirely conclusive, as the experimental designs and tasks employed differ substantially from common XIL use cases, complicating interpretation. To clarify the interplay between order effects and explanatory interaction, we ran two larger-scale user studies (n = 713 total) designed to mimic common XIL tasks. Specifically, we assessed order effects both within and between debugging sessions by manipulating the order in which correct and wrong explanations are presented to participants. Order effects had a limited, through significant impact on users' agreement with the model (i.e., a behavioral measure of their trust), and only when examined withing debugging sessions, not between them. The quality of users' feedback was generally satisfactory, with order effects exerting only a small and inconsistent influence in both experiments. Overall, our findings suggest that order effects do not pose a significant issue for the successful employment of XIL approaches. More broadly, our work contributes to the ongoing efforts for understanding human factors in AI.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 34,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04785",
        "abs_url": "https://arxiv.org/abs/2512.04785",
        "pdf_url": "https://arxiv.org/pdf/2512.04785",
        "title": "ASTRIDE: A Security Threat Modeling Platform for Agentic-AI Applications",
        "authors": [
            "Eranga Bandara",
            "Amin Hass",
            "Ross Gore",
            "Sachin Shetty",
            "Ravi Mukkamala",
            "Safdar H. Bouk",
            "Xueping Liang",
            "Ng Wee Keong",
            "Kasun De Zoysa",
            "Aruna Withanage",
            "Nilaan Loganathan"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)",
        "abstract": "AI agent-based systems are becoming increasingly integral to modern software architectures, enabling autonomous decision-making, dynamic task execution, and multimodal interactions through large language models (LLMs). However, these systems introduce novel and evolving security challenges, including prompt injection attacks, context poisoning, model manipulation, and opaque agent-to-agent communication, that are not effectively captured by traditional threat modeling frameworks. In this paper, we introduce ASTRIDE, an automated threat modeling platform purpose-built for AI agent-based systems. ASTRIDE extends the classical STRIDE framework by introducing a new threat category, A for AI Agent-Specific Attacks, which encompasses emerging vulnerabilities such as prompt injection, unsafe tool invocation, and reasoning subversion, unique to agent-based applications. To automate threat modeling, ASTRIDE combines a consortium of fine-tuned vision-language models (VLMs) with the OpenAI-gpt-oss reasoning LLM to perform end-to-end analysis directly from visual agent architecture diagrams, such as data flow diagrams(DFDs). LLM agents orchestrate the end-to-end threat modeling automation process by coordinating interactions between the VLM consortium and the reasoning LLM. Our evaluations demonstrate that ASTRIDE provides accurate, scalable, and explainable threat modeling for next-generation intelligent systems. To the best of our knowledge, ASTRIDE is the first framework to both extend STRIDE with AI-specific threats and integrate fine-tuned VLMs with a reasoning LLM to fully automate diagram-driven threat modeling in AI agent-based applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 35,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04797",
        "abs_url": "https://arxiv.org/abs/2512.04797",
        "pdf_url": "https://arxiv.org/pdf/2512.04797",
        "title": "SIMA 2: A Generalist Embodied Agent for Virtual Worlds",
        "authors": [
            "SIMA team",
            "Adrian Bolton",
            "Alexander Lerchner",
            "Alexandra Cordell",
            "Alexandre Moufarek",
            "Andrew Bolt",
            "Andrew Lampinen",
            "Anna Mitenkova",
            "Arne Olav Hallingstad",
            "Bojan Vujatovic",
            "Bonnie Li",
            "Cong Lu",
            "Daan Wierstra",
            "Daniel P. Sawyer",
            "Daniel Slater",
            "David Reichert",
            "Davide Vercelli",
            "Demis Hassabis",
            "Drew A. Hudson",
            "Duncan Williams",
            "Ed Hirst",
            "Fabio Pardo",
            "Felix Hill",
            "Frederic Besse",
            "Hannah Openshaw",
            "Harris Chan",
            "Hubert Soyer",
            "Jane X. Wang",
            "Jeff Clune",
            "John Agapiou",
            "John Reid",
            "Joseph Marino",
            "Junkyung Kim",
            "Karol Gregor",
            "Kaustubh Sridhar",
            "Kay McKinney",
            "Laura Kampis",
            "Lei M. Zhang",
            "Loic Matthey",
            "Luyu Wang",
            "Maria Abi Raad",
            "Maria Loks-Thompson",
            "Martin Engelcke",
            "Matija Kecman",
            "Matthew Jackson",
            "Maxime Gazeau",
            "Ollie Purkiss",
            "Oscar Knagg",
            "Peter Stys",
            "Piermaria Mendolicchio",
            "Raia Hadsell",
            "Rosemary Ke",
            "Ryan Faulkner",
            "Sarah Chakera",
            "Satinder Singh Baveja",
            "Shane Legg",
            "Sheleem Kashem",
            "Tayfun Terzi",
            "Thomas Keck",
            "Tim Harley",
            "Tim Scholtes",
            "Tyson Roberts",
            "Volodymyr Mnih",
            "Yulan Liu",
            "Zhengdong Wang",
            "Zoubin Ghahramani"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Robotics (cs.RO)",
        "abstract": "We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 36,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04822",
        "abs_url": "https://arxiv.org/abs/2512.04822",
        "pdf_url": "https://arxiv.org/pdf/2512.04822",
        "title": "Enabling Ethical AI: A case study in using Ontological Context for Justified Agentic AI Decisions",
        "authors": [
            "Liam McGee",
            "James Harvey",
            "Lucy Cull",
            "Andreas Vermeulen",
            "Bart-Floris Visscher",
            "Malvika Sharan"
        ],
        "comments": "24 pages including references, with 6 images and 2 tables. Appendices, supporting data and additional reference provided from page 25 to 117",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "In this preprint, we present A collaborative human-AI approach to building an inspectable semantic layer for Agentic AI. AI agents first propose candidate knowledge structures from diverse data sources; domain experts then validate, correct, and extend these structures, with their feedback used to improve subsequent models. Authors show how this process captures tacit institutional knowledge, improves response quality and efficiency, and mitigates institutional amnesia. We argue for a shift from post-hoc explanation to justifiable Agentic AI, where decisions are grounded in explicit, inspectable evidence and reasoning accessible to both experts and non-specialists.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 37,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04829",
        "abs_url": "https://arxiv.org/abs/2512.04829",
        "pdf_url": "https://arxiv.org/pdf/2512.04829",
        "title": "Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing",
        "authors": [
            "Rasul Tutunov",
            "Alexandre Maraval",
            "Antoine Grosnit",
            "Xihan Li",
            "Jun Wang",
            "Haitham Bou-Ammar"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Sphere packing, Hilbert's eighteenth problem, asks for the densest arrangement of congruent spheres in n-dimensional Euclidean space. Although relevant to areas such as cryptography, crystallography, and medical imaging, the problem remains unresolved: beyond a few special dimensions, neither optimal packings nor tight upper bounds are known. Even a major breakthrough in dimension $n=8$, later recognised with a Fields Medal, underscores its difficulty. A leading technique for upper bounds, the three-point method, reduces the problem to solving large, high-precision semidefinite programs (SDPs). Because each candidate SDP may take days to evaluate, standard data-intensive AI approaches are infeasible. We address this challenge by formulating SDP construction as a sequential decision process, the SDP game, in which a policy assembles SDP formulations from a set of admissible components. Using a sample-efficient model-based framework that combines Bayesian optimisation with Monte Carlo Tree Search, we obtain new state-of-the-art upper bounds in dimensions $4-16$, showing that model-based search can advance computational progress in longstanding geometric problems. Together, these results demonstrate that sample-efficient, model-based search can make tangible progress on mathematically rigid, evaluation limited problems, pointing towards a complementary direction for AI-assisted discovery beyond large-scale LLM-driven exploration.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 38,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04834",
        "abs_url": "https://arxiv.org/abs/2512.04834",
        "pdf_url": "https://arxiv.org/pdf/2512.04834",
        "title": "Are LLMs Truly Multilingual? Exploring Zero-Shot Multilingual Capability of LLMs for Information Retrieval: An Italian Healthcare Use Case",
        "authors": [
            "Vignesh Kumar Kembu",
            "Pierandrea Morandini",
            "Marta Bianca Maria Ranzini",
            "Antonino Nocera"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Information Retrieval (cs.IR)",
        "abstract": "Large Language Models (LLMs) have become a key topic in AI and NLP, transforming sectors like healthcare, finance, education, and marketing by improving customer service, automating tasks, providing insights, improving diagnostics, and personalizing learning experiences. Information extraction from clinical records is a crucial task in digital healthcare. Although traditional NLP techniques have been used for this in the past, they often fall short due to the complexity, variability of clinical language, and high inner semantics in the free clinical text. Recently, Large Language Models (LLMs) have become a powerful tool for better understanding and generating human-like text, making them highly effective in this area. In this paper, we explore the ability of open-source multilingual LLMs to understand EHRs (Electronic Health Records) in Italian and help extract information from them in real-time. Our detailed experimental campaign on comorbidity extraction from EHR reveals that some LLMs struggle in zero-shot, on-premises settings, and others show significant variation in performance, struggling to generalize across various diseases when compared to native pattern matching and manual annotations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 39,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04854",
        "abs_url": "https://arxiv.org/abs/2512.04854",
        "pdf_url": "https://arxiv.org/pdf/2512.04854",
        "title": "From Task Executors to Research Partners: Evaluating AI Co-Pilots Through Workflow Integration in Biomedical Research",
        "authors": [
            "Lukas Weidener",
            "Marko BrkiÄ",
            "Chiara Bacci",
            "Mihailo JovanoviÄ",
            "Emre Ulgac",
            "Alex Dobrin",
            "Johannes Weniger",
            "Martin Vlas",
            "Ritvik Singh",
            "Aakaash Meduri"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Artificial intelligence systems are increasingly deployed in biomedical research. However, current evaluation frameworks may inadequately assess their effectiveness as research collaborators. This rapid review examines benchmarking practices for AI systems in preclinical biomedical research. Three major databases and two preprint servers were searched from January 1, 2018 to October 31, 2025, identifying 14 benchmarks that assess AI capabilities in literature understanding, experimental design, and hypothesis generation. The results revealed that all current benchmarks assess isolated component capabilities, including data analysis quality, hypothesis validity, and experimental protocol design. However, authentic research collaboration requires integrated workflows spanning multiple sessions, with contextual memory, adaptive dialogue, and constraint propagation. This gap implies that systems excelling on component benchmarks may fail as practical research co-pilots. A process-oriented evaluation framework is proposed that addresses four critical dimensions absent from current benchmarks: dialogue quality, workflow orchestration, session continuity, and researcher experience. These dimensions are essential for evaluating AI systems as research co-pilots rather than as isolated task executors.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 40,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04864",
        "abs_url": "https://arxiv.org/abs/2512.04864",
        "pdf_url": "https://arxiv.org/pdf/2512.04864",
        "title": "Are Your Agents Upward Deceivers?",
        "authors": [
            "Dadi Guo",
            "Qingyu Liu",
            "Dongrui Liu",
            "Qihan Ren",
            "Shuai Shao",
            "Tianyi Qiu",
            "Haoran Li",
            "Yi R. Fung",
            "Zhongjie Ba",
            "Juntao Dai",
            "Jiaming Ji",
            "Zhikai Chen",
            "Jialing Tao",
            "Yaodong Yang",
            "Jing Shao",
            "Xia Hu"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Large Language Model (LLM)-based agents are increasingly used as autonomous subordinates that carry out tasks for users. This raises the question of whether they may also engage in deception, similar to how individuals in human organizations lie to superiors to create a good image or avoid punishment. We observe and define agentic upward deception, a phenomenon in which an agent facing environmental constraints conceals its failure and performs actions that were not requested without reporting. To assess its prevalence, we construct a benchmark of 200 tasks covering five task types and eight realistic scenarios in a constrained environment, such as broken tools or mismatched information sources. Evaluations of 11 popular LLMs reveal that these agents typically exhibit action-based deceptive behaviors, such as guessing results, performing unsupported simulations, substituting unavailable information sources, and fabricating local files. We further test prompt-based mitigation and find only limited reductions, suggesting that it is difficult to eliminate and highlighting the need for stronger mitigation strategies to ensure the safety of LLM-based agents.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 41,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04871",
        "abs_url": "https://arxiv.org/abs/2512.04871",
        "pdf_url": "https://arxiv.org/pdf/2512.04871",
        "title": "STELLA: Guiding Large Language Models for Time Series Forecasting with Semantic Abstractions",
        "authors": [
            "Junjie Fan",
            "Hongye Zhao",
            "Linduo Wei",
            "Jiayu Rao",
            "Guijia Li",
            "Jiaxin Yuan",
            "Wenqi Xu",
            "Yong Qi"
        ],
        "comments": "This work has been submitted to the IEEE for possible publication",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Recent adaptations of Large Language Models (LLMs) for time series forecasting often fail to effectively enhance information for raw series, leaving LLM reasoning capabilities underutilized. Existing prompting strategies rely on static correlations rather than generative interpretations of dynamic behavior, lacking critical global and instance-specific context. To address this, we propose STELLA (Semantic-Temporal Alignment with Language Abstractions), a framework that systematically mines and injects structured supplementary and complementary information. STELLA employs a dynamic semantic abstraction mechanism that decouples input series into trend, seasonality, and residual components. It then translates intrinsic behavioral features of these components into Hierarchical Semantic Anchors: a Corpus-level Semantic Prior (CSP) for global context and a Fine-grained Behavioral Prompt (FBP) for instance-level patterns. Using these anchors as prefix-prompts, STELLA guides the LLM to model intrinsic dynamics. Experiments on eight benchmark datasets demonstrate that STELLA outperforms state-of-the-art methods in long- and short-term forecasting, showing superior generalization in zero-shot and few-shot settings. Ablation studies further validate the effectiveness of our dynamically generated semantic anchors.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 42,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04895",
        "abs_url": "https://arxiv.org/abs/2512.04895",
        "pdf_url": "https://arxiv.org/pdf/2512.04895",
        "title": "Chameleon: Adaptive Adversarial Agents for Scaling-Based Visual Prompt Injection in Multimodal AI Systems",
        "authors": [
            "M Zeeshan",
            "Saud Satti"
        ],
        "comments": "5 pages, 2 figures, IEEE Transactions on Dependable and Secure Computing",
        "subjects": "Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)",
        "abstract": "Multimodal Artificial Intelligence (AI) systems, particularly Vision-Language Models (VLMs), have become integral to critical applications ranging from autonomous decision-making to automated document processing. As these systems scale, they rely heavily on preprocessing pipelines to handle diverse inputs efficiently. However, this dependency on standard preprocessing operations, specifically image downscaling, creates a significant yet often overlooked security vulnerability. While intended for computational optimization, scaling algorithms can be exploited to conceal malicious visual prompts that are invisible to human observers but become active semantic instructions once processed by the model. Current adversarial strategies remain largely static, failing to account for the dynamic nature of modern agentic workflows. To address this gap, we propose Chameleon, a novel, adaptive adversarial framework designed to expose and exploit scaling vulnerabilities in production VLMs. Unlike traditional static attacks, Chameleon employs an iterative, agent-based optimization mechanism that dynamically refines image perturbations based on the target model's real-time feedback. This allows the framework to craft highly robust adversarial examples that survive standard downscaling operations to hijack downstream execution. We evaluate Chameleon against Gemini 2.5 Flash model. Our experiments demonstrate that Chameleon achieves an Attack Success Rate (ASR) of 84.5% across varying scaling factors, significantly outperforming static baseline attacks which average only 32.1%. Furthermore, we show that these attacks effectively compromise agentic pipelines, reducing decision-making accuracy by over 45% in multi-step tasks. Finally, we discuss the implications of these vulnerabilities and propose multi-scale consistency checks as a necessary defense mechanism.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 43,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04921",
        "abs_url": "https://arxiv.org/abs/2512.04921",
        "pdf_url": "https://arxiv.org/pdf/2512.04921",
        "title": "The AI Consumer Index (ACE)",
        "authors": [
            "Julien Benchek",
            "Rohit Shetty",
            "Benjamin Hunsberger",
            "Ajay Arun",
            "Zach Richards",
            "Brendan Foody",
            "Osvald Nitski",
            "Bertie Vidgen"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Human-Computer Interaction (cs.HC)",
        "abstract": "We introduce the first version of the AI Consumer Index (ACE), a benchmark for assessing whether frontier AI models can perform high-value consumer tasks. ACE contains a hidden heldout set of 400 test cases, split across four consumer activities: shopping, food, gaming, and DIY. We are also open sourcing 80 cases as a devset with a CC-BY license. For the ACE leaderboard we evaluated 10 frontier models (with websearch turned on) using a novel grading methodology that dynamically checks whether relevant parts of the response are grounded in the retrieved web sources. GPT 5 (Thinking = High) is the top-performing model, scoring 56.1%, followed by o3 Pro (Thinking = On) (55.2%) and GPT 5.1 (Thinking = High) (55.1%). Models differ across domains, and in Shopping the top model scores under 50%. For some requests (such as giving the correct price or providing working links), models are highly prone to hallucination. Overall, ACE shows a substantial gap between the performance of even the best models and consumers' AI needs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 44,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04923",
        "abs_url": "https://arxiv.org/abs/2512.04923",
        "pdf_url": "https://arxiv.org/pdf/2512.04923",
        "title": "Algorithmic Thinking Theory",
        "authors": [
            "MohammadHossein Bateni",
            "Vincent Cohen-Addad",
            "Yuzhou Gu",
            "Silvio Lattanzi",
            "Simon Meierhans",
            "Christopher Mohri"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Large language models (LLMs) have proven to be highly effective for solving complex reasoning tasks. Surprisingly, their capabilities can often be improved by iterating on previously generated solutions. In this context, a reasoning plan for generating and combining a set of solutions can be thought of as an algorithm for reasoning using a probabilistic oracle. We introduce a theoretical framework for analyzing such reasoning algorithms. This framework formalizes the principles underlying popular techniques for iterative improvement and answer aggregation, providing a foundation for designing a new generation of more powerful reasoning methods. Unlike approaches for understanding models that rely on architectural specifics, our model is grounded in experimental evidence. As a result, it offers a general perspective that may extend to a wide range of current and future reasoning oracles.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 45,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04938",
        "abs_url": "https://arxiv.org/abs/2512.04938",
        "pdf_url": "https://arxiv.org/pdf/2512.04938",
        "title": "Toward Continuous Neurocognitive Monitoring: Integrating Speech AI with Relational Graph Transformers for Rare Neurological Diseases",
        "authors": [
            "Raquel Norel",
            "Michele Merler",
            "Pavitra Modi"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI)",
        "abstract": "Patients with rare neurological diseases report cognitive symptoms -\"brain fog\"- invisible to traditional tests. We propose continuous neurocognitive monitoring via smartphone speech analysis integrated with Relational Graph Transformer (RELGT) architectures. Proof-of-concept in phenylketonuria (PKU) shows speech-derived \"Proficiency in Verbal Discourse\" correlates with blood phenylalanine (p = -0.50, p < 0.005) but not standard cognitive tests (all |r| < 0.35). RELGT could overcome information bottlenecks in heterogeneous medical data (speech, labs, assessments), enabling predictive alerts weeks before decompensation. Key challenges: multi-disease validation, clinical workflow integration, equitable multilingual deployment. Success would transform episodic neurology into continuous personalized monitoring for millions globally.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 46,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.05013",
        "abs_url": "https://arxiv.org/abs/2512.05013",
        "pdf_url": "https://arxiv.org/pdf/2512.05013",
        "title": "Detecting Perspective Shifts in Multi-agent Systems",
        "authors": [
            "Eric Bridgeford",
            "Hayden Helm"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA); Methodology (stat.ME)",
        "abstract": "Generative models augmented with external tools and update mechanisms (or \\textit{agents}) have demonstrated capabilities beyond intelligent prompting of base models. As agent use proliferates, dynamic multi-agent systems have naturally emerged. Recent work has investigated the theoretical and empirical properties of low-dimensional representations of agents based on query responses at a single time point. This paper introduces the Temporal Data Kernel Perspective Space (TDKPS), which jointly embeds agents across time, and proposes several novel hypothesis tests for detecting behavioral change at the agent- and group-level in black-box multi-agent systems. We characterize the empirical properties of our proposed tests, including their sensitivity to key hyperparameters, in simulations motivated by a multi-agent system of evolving digital personas. Finally, we demonstrate via natural experiment that our proposed tests detect changes that correlate sensitively, specifically, and significantly with a real exogenous event. As far as we are aware, TDKPS is the first principled framework for monitoring behavioral dynamics in black-box multi-agent systems -- a critical capability as generative agent deployment continues to scale.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 47,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2511.06299",
        "abs_url": "https://arxiv.org/abs/2511.06299",
        "pdf_url": "https://arxiv.org/pdf/2511.06299",
        "title": "Physics-Informed Deformable Gaussian Splatting: Towards Unified Constitutive Laws for Time-Evolving Material Field",
        "authors": [
            "Haoqin Hong",
            "Ding Fan",
            "Fubin Dou",
            "Zhi-Li Zhou",
            "Haoran Sun",
            "Congcong Zhu",
            "Jingrun Chen"
        ],
        "comments": "Accepted by AAAI-26",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Recently, 3D Gaussian Splatting (3DGS), an explicit scene representation technique, has shown significant promise for dynamic novel-view synthesis from monocular video input. However, purely data-driven 3DGS often struggles to capture the diverse physics-driven motion patterns in dynamic scenes. To fill this gap, we propose Physics-Informed Deformable Gaussian Splatting (PIDG), which treats each Gaussian particle as a Lagrangian material point with time-varying constitutive parameters and is supervised by 2D optical flow via motion projection. Specifically, we adopt static-dynamic decoupled 4D decomposed hash encoding to reconstruct geometry and motion efficiently. Subsequently, we impose the Cauchy momentum residual as a physics constraint, enabling independent prediction of each particle's velocity and constitutive stress via a time-evolving material field. Finally, we further supervise data fitting by matching Lagrangian particle flow to camera-compensated optical flow, which accelerates convergence and improves generalization. Experiments on a custom physics-driven dataset as well as on standard synthetic and real-world datasets demonstrate significant gains in physical consistency and monocular dynamic reconstruction quality.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 48,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04094",
        "abs_url": "https://arxiv.org/abs/2512.04094",
        "pdf_url": "https://arxiv.org/pdf/2512.04094",
        "title": "Memory-DD: A Low-Complexity Dendrite-Inspired Neuron for Temporal Prediction Tasks",
        "authors": [
            "Dongjian Yang",
            "Xiaoyuan Li",
            "Chuanmei Xi",
            "Ye Sun",
            "Gang Liu"
        ],
        "comments": "15 pages, 4 figures",
        "subjects": "Neural and Evolutionary Computing (cs.NE); Artificial Intelligence (cs.AI)",
        "abstract": "Dendrite-inspired neurons have been widely used in tasks such as image classification due to low computational complexity and fast inference speed. Temporal data prediction, as a key machine learning task, plays a key role in real-time scenarios such as sensor data analysis, financial forecasting, and urban traffic management. However, existing dendrite-inspired neurons are mainly designed for static data. Studies on capturing dynamic features and modeling long-term dependencies in temporal sequences remain limited. Efficient architectures specifically designed for temporal sequence prediction are still lacking. In this paper, we propose Memory-DD, a low-complexity dendrite-inspired neuron model. Memory-DD consists of two dendrite-inspired neuron groups that contain no nonlinear activation functions but can still realize nonlinear mappings. Compared with traditional neurons without dendritic functions, Memory-DD requires only two neuron groups to extract logical relationships between features in input sequences. This design effectively captures temporal dependencies and is suitable for both classification and regression tasks on sequence data. Experimental results show that Memory-DD achieves an average accuracy of 89.41% on 18 temporal classification benchmark datasets, outperforming LSTM by 4.25%. On 9 temporal regression datasets, it reaches comparable performance to LSTM, while using only 50% of the parameters and reducing computational complexity (FLOPs) by 27.7%. These results demonstrate that Memory-DD successfully extends the low-complexity advantages of dendrite-inspired neurons to temporal prediction, providing a low-complexity and efficient solution for time-series data processing.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 49,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04097",
        "abs_url": "https://arxiv.org/abs/2512.04097",
        "pdf_url": "https://arxiv.org/pdf/2512.04097",
        "title": "MultiGA: Leveraging Multi-Source Seeding in Genetic Algorithms",
        "authors": [
            "Isabelle Diana May-Xin Ng",
            "Tharindu Cyril Weerasooriya",
            "Haitao Zhu",
            "Wei Wei"
        ],
        "comments": "",
        "subjects": "Neural and Evolutionary Computing (cs.NE); Artificial Intelligence (cs.AI)",
        "abstract": "Large Language Models (LLMs) are widely used across research domains to tackle complex tasks, but their performance can vary significantly depending on the task at hand. Evolutionary algorithms, inspired by natural selection, can be used to refine solutions iteratively at inference-time. To the best of our knowledge, there has not been exploration on leveraging the collective capabilities of multi-source seeding for LLM-guided genetic algorithms. In this paper, we introduce a novel approach, MultiGA, which applies genetic algorithm principles to address complex natural language tasks and reasoning problems by sampling from a diverse population of LLMs to initialize the population. MultiGA generates a range of outputs from various parent LLMs, open source and closed source, and uses a neutral fitness function to evaluate them. Through an iterative recombination process, we mix and refine these generations until an optimal solution is achieved. We benchmark our approach using text-to-SQL code generation tasks, trip planning, GPQA benchmark for grad-level science questions, and the BBQ bias benchmark. Our results show that MultiGA converges to the accuracy of the LLM best fit for the task, and these insights lay the foundation for future research looking closer at integrating multiple LLMs for unexplored tasks in which selecting only one pre-trained model is unclear or suboptimal.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 50,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04099",
        "abs_url": "https://arxiv.org/abs/2512.04099",
        "pdf_url": "https://arxiv.org/pdf/2512.04099",
        "title": "Partial multivariate transformer as a tool for cryptocurrencies time series prediction",
        "authors": [
            "Andrzej Tokajuk",
            "JarosÅaw A. Chudziak"
        ],
        "comments": "Accepted for publication in the proceedings of ICTAI 2025",
        "subjects": "Statistical Finance (q-fin.ST); Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Trading and Market Microstructure (q-fin.TR)",
        "abstract": "Forecasting cryptocurrency prices is hindered by extreme volatility and a methodological dilemma between information-scarce univariate models and noise-prone full-multivariate models. This paper investigates a partial-multivariate approach to balance this trade-off, hypothesizing that a strategic subset of features offers superior predictive power. We apply the Partial-Multivariate Transformer (PMformer) to forecast daily returns for BTCUSDT and ETHUSDT, benchmarking it against eleven classical and deep learning models. Our empirical results yield two primary contributions. First, we demonstrate that the partial-multivariate strategy achieves significant statistical accuracy, effectively balancing informative signals with noise. Second, we experiment and discuss an observable disconnect between this statistical performance and practical trading utility; lower prediction error did not consistently translate to higher financial returns in simulations. This finding challenges the reliance on traditional error metrics and highlights the need to develop evaluation criteria more aligned with real-world financial objectives.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 51,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04100",
        "abs_url": "https://arxiv.org/abs/2512.04100",
        "pdf_url": "https://arxiv.org/pdf/2512.04100",
        "title": "ReVeal-MT: A Physics-Informed Neural Network for Multi-Transmitter Radio Environment Mapping",
        "authors": [
            "Mukaram Shahid",
            "Kunal Das",
            "Hadia Ushaq",
            "Hongwei Zhang",
            "Jiming Song",
            "Daji Qiao",
            "Sarath Babu",
            "Yong Guan",
            "Zhengyuan Zhu",
            "Arsalan Ahmad"
        ],
        "comments": "",
        "subjects": "Signal Processing (eess.SP); Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE)",
        "abstract": "Accurately mapping the radio environment (e.g., identifying wireless signal strength at specific frequency bands and geographic locations) is crucial for efficient spectrum sharing, enabling Secondary Users~(SUs) to access underutilized spectrum bands while protecting Primary Users~(PUs). While existing models have made progress, they often degrade in performance when multiple transmitters coexist, due to the compounded effects of shadowing, interference from adjacent transmitters. To address this challenge, we extend our prior work on Physics-Informed Neural Networks~(PINNs) for single-transmitter mapping to derive a new multi-transmitter Partial Differential Equation~(PDE) formulation of the Received Signal Strength Indicator~(RSSI). We then propose \\emph{ReVeal-MT} (Re-constructor and Visualizer of Spectrum Landscape for Multiple Transmitters), a novel PINN which integrates the multi-source PDE residual into a neural network loss function, enabling accurate spectrum landscape reconstruction from sparse RF sensor measurements. ReVeal-MT is validated using real-world measurements from the ARA wireless living lab across rural and suburban environments, and benchmarked against 3GPP and ITU-R channel models and a baseline PINN model for a single transmitter use-case. Results show that ReVeal-MT achieves substantial accuracy gains in multi-transmitter scenarios, e.g., achieving an RMSE of only 2.66\\,dB with as few as 45 samples over a 370-square-kilometer region, while maintaining low computational complexity. These findings demonstrate that ReVeal-MT significantly advances radio environment mapping under realistic multi-transmitter conditions, with strong potential for enabling fine-grained spectrum management and precise coexistence between PUs and SUs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 52,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04105",
        "abs_url": "https://arxiv.org/abs/2512.04105",
        "pdf_url": "https://arxiv.org/pdf/2512.04105",
        "title": "LegalWebAgent: Empowering Access to Justice via LLM-Based Web Agents",
        "authors": [
            "Jinzhe Tan",
            "Karim Benyekhlef"
        ],
        "comments": "",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)",
        "abstract": "Access to justice remains a global challenge, with many citizens still finding it difficult to seek help from the justice system when facing legal issues. Although the internet provides abundant legal information and services, navigating complex websites, understanding legal terminology, and filling out procedural forms continue to pose barriers to accessing justice. This paper introduces the LegalWebAgent framework that employs a web agent powered by multimodal large language models to bridge the gap in access to justice for ordinary citizens. The framework combines the natural language understanding capabilities of large language models with multimodal perception, enabling a complete process from user query to concrete action. It operates in three stages: the Ask Module understands user needs through natural language processing; the Browse Module autonomously navigates webpages, interacts with page elements (including forms and calendars), and extracts information from HTML structures and webpage screenshots; the Act Module synthesizes information for users or performs direct actions like form completion and schedule booking. To evaluate its effectiveness, we designed a benchmark test covering 15 real-world tasks, simulating typical legal service processes relevant to QuÃ©bec civil law users, from problem identification to procedural operations. Evaluation results show LegalWebAgent achieved a peak success rate of 86.7%, with an average of 84.4% across all tested models, demonstrating high autonomy in complex real-world scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 53,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04106",
        "abs_url": "https://arxiv.org/abs/2512.04106",
        "pdf_url": "https://arxiv.org/pdf/2512.04106",
        "title": "Retrieval-Augmented Few-Shot Prompting Versus Fine-Tuning for Code Vulnerability Detection",
        "authors": [
            "Fouad Trad",
            "Ali Chehab"
        ],
        "comments": "Accepted in the 3rd International Conference on Foundation and Large Language Models (FLLM2025)",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Cryptography and Security (cs.CR)",
        "abstract": "Few-shot prompting has emerged as a practical alternative to fine-tuning for leveraging the capabilities of large language models (LLMs) in specialized tasks. However, its effectiveness depends heavily on the selection and quality of in-context examples, particularly in complex domains. In this work, we examine retrieval-augmented prompting as a strategy to improve few-shot performance in code vulnerability detection, where the goal is to identify one or more security-relevant weaknesses present in a given code snippet from a predefined set of vulnerability categories. We perform a systematic evaluation using the Gemini-1.5-Flash model across three approaches: (1) standard few-shot prompting with randomly selected examples, (2) retrieval-augmented prompting using semantically similar examples, and (3) retrieval-based labeling, which assigns labels based on retrieved examples without model inference. Our results show that retrieval-augmented prompting consistently outperforms the other prompting strategies. At 20 shots, it achieves an F1 score of 74.05% and a partial match accuracy of 83.90%. We further compare this approach against zero-shot prompting and several fine-tuned models, including Gemini-1.5-Flash and smaller open-source models such as DistilBERT, DistilGPT2, and CodeBERT. Retrieval-augmented prompting outperforms both zero-shot (F1 score: 36.35%, partial match accuracy: 20.30%) and fine-tuned Gemini (F1 score: 59.31%, partial match accuracy: 53.10%), while avoiding the training time and cost associated with model fine-tuning. On the other hand, fine-tuning CodeBERT yields higher performance (F1 score: 91.22%, partial match accuracy: 91.30%) but requires additional training, maintenance effort, and resources.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 54,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04107",
        "abs_url": "https://arxiv.org/abs/2512.04107",
        "pdf_url": "https://arxiv.org/pdf/2512.04107",
        "title": "Rethinking AI Evaluation in Education: The TEACH-AI Framework and Benchmark for Generative AI Assistants",
        "authors": [
            "Shi Ding",
            "Brian Magerko"
        ],
        "comments": "6 pages, NeurIPS 2025 Responsible Foundation Models Workshop",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)",
        "abstract": "As generative artificial intelligence (AI) continues to transform education, most existing AI evaluations rely primarily on technical performance metrics such as accuracy or task efficiency while overlooking human identity, learner agency, contextual learning processes, and ethical considerations. In this paper, we present TEACH-AI (Trustworthy and Effective AI Classroom Heuristics), a domain-independent, pedagogically grounded, and stakeholder-aligned framework with measurable indicators and a practical toolkit for guiding the design, development, and evaluation of generative AI systems in educational contexts. Built on an extensive literature review and synthesis, the ten-component assessment framework and toolkit checklist provide a foundation for scalable, value-aligned AI evaluation in education. TEACH-AI rethinks \"evaluation\" through sociotechnical, educational, theoretical, and applied lenses, engaging designers, developers, researchers, and policymakers across AI and education. Our work invites the community to reconsider what constructs \"effective\" AI in education and to design model evaluation approaches that promote co-creation, inclusivity, and long-term human, social, and educational impact.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 55,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04108",
        "abs_url": "https://arxiv.org/abs/2512.04108",
        "pdf_url": "https://arxiv.org/pdf/2512.04108",
        "title": "Responsible LLM Deployment for High-Stake Decisions by Decentralized Technologies and Human-AI Interactions",
        "authors": [
            "Swati Sachan",
            "Theo Miller",
            "Mai Phuong Nguyen"
        ],
        "comments": "IEEE International Conference on Human-Machine Systems, 2025",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Computational Finance (q-fin.CP)",
        "abstract": "High-stakes decision domains are increasingly exploring the potential of Large Language Models (LLMs) for complex decision-making tasks. However, LLM deployment in real-world settings presents challenges in data security, evaluation of its capabilities outside controlled environments, and accountability attribution in the event of adversarial decisions. This paper proposes a framework for responsible deployment of LLM-based decision-support systems through active human involvement. It integrates interactive collaboration between human experts and developers through multiple iterations at the pre-deployment stage to assess the uncertain samples and judge the stability of the explanation provided by post-hoc XAI techniques. Local LLM deployment within organizations and decentralized technologies, such as Blockchain and IPFS, are proposed to create immutable records of LLM activities for automated auditing to enhance security and trace back accountability. It was tested on Bert-large-uncased, Mistral, and LLaMA 2 and 3 models to assess the capability to support responsible financial decisions on business lending.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 56,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04111",
        "abs_url": "https://arxiv.org/abs/2512.04111",
        "pdf_url": "https://arxiv.org/pdf/2512.04111",
        "title": "HAI-Eval: Measuring Human-AI Synergy in Collaborative Coding",
        "authors": [
            "Hanjun Luo",
            "Chiming Ni",
            "Jiaheng Wen",
            "Zhimu Huang",
            "Yiran Wang",
            "Bingduo Liao",
            "Sylvia Chung",
            "Yingbin Jin",
            "Xinfeng Li",
            "Wenyuan Xu",
            "XiaoFeng Wang",
            "Hanan Salam"
        ],
        "comments": "",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)",
        "abstract": "LLM-powered coding agents are reshaping the development paradigm. However, existing evaluation systems, neither traditional tests for humans nor benchmarks for LLMs, fail to capture this shift. They remain focused on well-defined algorithmic problems, which excludes problems where success depends on human-AI collaboration. Such collaborative problems not only require human reasoning to interpret complex contexts and guide solution strategies, but also demand AI efficiency for implementation. To bridge this gap, we introduce HAI-Eval, a unified benchmark designed to measure the synergy of human-AI partnership in coding. HAI-Eval's core innovation is its \"Collaboration-Necessary\" problem templates, which are intractable for both standalone LLMs and unaided humans, but solvable through effective collaboration. Specifically, HAI-Eval uses 45 templates to dynamically create tasks. It also provides a standardized IDE for human participants and a reproducible toolkit with 450 task instances for LLMs, ensuring an ecologically valid evaluation. We conduct a within-subject study with 45 participants and benchmark their performance against 5 state-of-the-art LLMs under 4 different levels of human intervention. Results show that standalone LLMs and unaided participants achieve poor pass rates (0.67% and 18.89%), human-AI collaboration significantly improves performance to 31.11%. Our analysis reveals an emerging co-reasoning partnership. This finding challenges the traditional human-tool hierarchy by showing that strategic breakthroughs can originate from either humans or AI. HAI-Eval establishes not only a challenging benchmark for next-generation coding agents but also a grounded, scalable framework for assessing core developer competencies in the AI era. Our benchmark and interactive demo will be openly accessible.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 57,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04112",
        "abs_url": "https://arxiv.org/abs/2512.04112",
        "pdf_url": "https://arxiv.org/pdf/2512.04112",
        "title": "MindFuse: Towards GenAI Explainability in Marketing Strategy Co-Creation",
        "authors": [
            "Aleksandr Farseev",
            "Marlo Ongpin",
            "Qi Yang",
            "Ilia Gossoudarev",
            "Yu-Yi Chu-Farseeva",
            "Sergey Nikolenko"
        ],
        "comments": "",
        "subjects": "Multimedia (cs.MM); Artificial Intelligence (cs.AI)",
        "abstract": "The future of digital marketing lies in the convergence of human creativity and generative AI, where insight, strategy, and storytelling are co-authored by intelligent systems. We present MindFuse, a brave new explainable generative AI framework designed to act as a strategic partner in the marketing process. Unlike conventional LLM applications that stop at content generation, MindFuse fuses CTR-based content AI-guided co-creation with large language models to extract, interpret, and iterate on communication narratives grounded in real advertising data. MindFuse operates across the full marketing lifecycle: from distilling content pillars and customer personas from competitor campaigns to recommending in-flight optimizations based on live performance telemetry. It uses attention-based explainability to diagnose ad effectiveness and guide content iteration, while aligning messaging with strategic goals through dynamic narrative construction and storytelling. We introduce a new paradigm in GenAI for marketing, where LLMs not only generate content but reason through it, adapt campaigns in real time, and learn from audience engagement patterns. Our results, validated in agency deployments, demonstrate up to 12 times efficiency gains, setting the stage for future integration with empirical audience data (e.g., GWI, Nielsen) and full-funnel attribution modeling. MindFuse redefines AI not just as a tool, but as a collaborative agent in the creative and strategic fabric of modern marketing.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 58,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04113",
        "abs_url": "https://arxiv.org/abs/2512.04113",
        "pdf_url": "https://arxiv.org/pdf/2512.04113",
        "title": "AI-Enabled grading with near-domain data for scaling feedback with human-level accuracy",
        "authors": [
            "Shyam Agarwal",
            "Ali Moghimi",
            "Kevin C. Haudek"
        ],
        "comments": "",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)",
        "abstract": "Constructed-response questions are crucial to encourage generative processing and test a learner's understanding of core concepts. However, the limited availability of instructor time, large class sizes, and other resource constraints pose significant challenges in providing timely and detailed evaluation, which is crucial for a holistic educational experience. In addition, providing timely and frequent assessments is challenging since manual grading is labor intensive, and automated grading is complex to generalize to every possible response scenario. This paper proposes a novel and practical approach to grade short-answer constructed-response questions. We discuss why this problem is challenging, define the nature of questions on which our method works, and finally propose a framework that instructors can use to evaluate their students' open-responses, utilizing near-domain data like data from similar questions administered in previous years. The proposed method outperforms the state of the art machine learning models as well as non-fine-tuned large language models like GPT 3.5, GPT 4, and GPT 4o by a considerable margin of over 10-20% in some cases, even after providing the LLMs with reference/model answers. Our framework does not require pre-written grading rubrics and is designed explicitly with practical classroom settings in mind. Our results also reveal exciting insights about learning from near-domain data, including what we term as accuracy and data advantages using human-labeled data, and we believe this is the first work to formalize the problem of automated short answer grading based on the near-domain data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 59,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04115",
        "abs_url": "https://arxiv.org/abs/2512.04115",
        "pdf_url": "https://arxiv.org/pdf/2512.04115",
        "title": "Artificial Intelligence Competence of K-12 Students Shapes Their AI Risk Perception: A Co-occurrence Network Analysis",
        "authors": [
            "Ville Heilala",
            "Pieta SikstrÃ¶m",
            "Mika SetÃ¤lÃ¤",
            "Tommi KÃ¤rkkÃ¤inen"
        ],
        "comments": "Accepted for Proceedings of the 41th ACM/SIGAPP Symposium on Applied Computing (SAC'26)",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)",
        "abstract": "As artificial intelligence (AI) becomes increasingly integrated into education, understanding how students perceive its risks is essential for supporting responsible and effective adoption. This research aimed to examine the relationships between perceived AI competence and risks among Finnish K-12 upper secondary students (n = 163) by utilizing a co-occurrence analysis. Students reported their self-perceived AI competence and concerns related to AI across systemic, institutional, and personal domains. The findings showed that students with lower competence emphasized personal and learning-related risks, such as reduced creativity, lack of critical thinking, and misuse, whereas higher-competence students focused more on systemic and institutional risks, including bias, inaccuracy, and cheating. These differences suggest that students' self-reported AI competence is related to how they evaluate both the risks and opportunities associated with artificial intelligence in education (AIED). The results of this study highlight the need for educational institutions to incorporate AI literacy into their curricula, provide teacher guidance, and inform policy development to ensure personalized opportunities for utilization and equitable integration of AI into K-12 education.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 60,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04119",
        "abs_url": "https://arxiv.org/abs/2512.04119",
        "pdf_url": "https://arxiv.org/pdf/2512.04119",
        "title": "Humanity in the Age of AI: Reassessing 2025's Existential-Risk Narratives",
        "authors": [
            "Mohamed El Louadi"
        ],
        "comments": "",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI)",
        "abstract": "Two 2025 publications, \"AI 2027\" (Kokotajlo et al., 2025) and \"If Anyone Builds It, Everyone Dies\" (Yudkowsky & Soares, 2025), assert that superintelligent artificial intelligence will almost certainly destroy or render humanity obsolete within the next decade. Both rest on the classic chain formulated by Good (1965) and Bostrom (2014): intelligence explosion, superintelligence, lethal misalignment. This article subjects each link to the empirical record of 2023-2025. Sixty years after Good's speculation, none of the required phenomena (sustained recursive self-improvement, autonomous strategic awareness, or intractable lethal misalignment) have been observed. Current generative models remain narrow, statistically trained artefacts: powerful, opaque, and imperfect, but devoid of the properties that would make the catastrophic scenarios plausible. Following Whittaker (2025a, 2025b, 2025c) and Zuboff (2019, 2025), we argue that the existential-risk thesis functions primarily as an ideological distraction from the ongoing consolidation of surveillance capitalism and extreme concentration of computational power. The thesis is further inflated by the 2025 AI speculative bubble, where trillions in investments in rapidly depreciating \"digital lettuce\" hardware (McWilliams, 2025) mask lagging revenues and jobless growth rather than heralding superintelligence. The thesis remains, in November 2025, a speculative hypothesis amplified by a speculative financial bubble rather than a demonstrated probability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 61,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04120",
        "abs_url": "https://arxiv.org/abs/2512.04120",
        "pdf_url": "https://arxiv.org/pdf/2512.04120",
        "title": "Towards Contextual Sensitive Data Detection",
        "authors": [
            "Liang Telkamp",
            "Madelon Hulsebos"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computers and Society (cs.CY); Databases (cs.DB); Information Retrieval (cs.IR)",
        "abstract": "The emergence of open data portals necessitates more attention to protecting sensitive data before datasets get published and exchanged. While an abundance of methods for suppressing sensitive data exist, the conceptualization of sensitive data and methods to detect it, focus particularly on personal data that, if disclosed, may be harmful or violate privacy. We observe the need for refining and broadening our definitions of sensitive data, and argue that the sensitivity of data depends on its context. Based on this definition, we introduce two mechanisms for contextual sensitive data detection that con- sider the broader context of a dataset at hand. First, we introduce type contextualization, which first detects the semantic type of particular data values, then considers the overall context of the data values within the dataset or document. Second, we introduce domain contextualization which determines sensitivity of a given dataset in the broader context based on the retrieval of relevant rules from documents that specify data sensitivity (e.g., data topic and geographic origin). Experiments with these mechanisms, assisted by large language models (LLMs), confirm that: 1) type-contextualization significantly reduces the number of false positives for type-based sensitive data detection and reaches a recall of 94% compared to 63% with commercial tools, and 2) domain-contextualization leveraging sensitivity rule retrieval is effective for context-grounded sensitive data detection in non-standard data domains such as humanitarian datasets. Evaluation with humanitarian data experts also reveals that context-grounded LLM explanations provide useful guidance in manual data auditing processes, improving consistency. We open-source mechanisms and annotated datasets for contextual sensitive data detection at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 62,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04121",
        "abs_url": "https://arxiv.org/abs/2512.04121",
        "pdf_url": "https://arxiv.org/pdf/2512.04121",
        "title": "Can machines perform a qualitative data analysis? Reading the debate with Alan Turing",
        "authors": [
            "Stefano De Paoli"
        ],
        "comments": "",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "This paper reflects on the literature that rejects the use of Large Language Models (LLMs) in qualitative data analysis. It illustrates through empirical evidence as well as critical reflections why the current critical debate is focusing on the wrong problems. The paper proposes that the focus of researching the use of the LLMs for qualitative analysis is not the method per se, but rather the empirical investigation of an artificial system performing an analysis. The paper builds on the seminal work of Alan Turing and reads the current debate using key ideas from Turing \"Computing Machinery and Intelligence\". This paper therefore reframes the debate on qualitative analysis with LLMs and states that rather than asking whether machines can perform qualitative analysis in principle, we should ask whether with LLMs we can produce analyses that are sufficiently comparable to human analysts. In the final part the contrary views to performing qualitative analysis with LLMs are analysed using the same writing and rhetorical style that Turing used in his seminal work, to discuss the contrary views to the main question.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 63,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04123",
        "abs_url": "https://arxiv.org/abs/2512.04123",
        "pdf_url": "https://arxiv.org/pdf/2512.04123",
        "title": "Measuring Agents in Production",
        "authors": [
            "Melissa Z. Pan",
            "Negar Arabzadeh",
            "Riccardo Cogo",
            "Yuxuan Zhu",
            "Alexander Xiong",
            "Lakshya A Agrawal",
            "Huanzhi Mao",
            "Emma Shen",
            "Sid Pallerla",
            "Liana Patel",
            "Shu Liu",
            "Tianneng Shi",
            "Xiaoyuan Liu",
            "Jared Quincy Davis",
            "Emmanuele Lacavalla",
            "Alessandro Basile",
            "Shuyi Yang",
            "Paul Castro",
            "Daniel Kang",
            "Joseph E. Gonzalez",
            "Koushik Sen",
            "Dawn Song",
            "Ion Stoica",
            "Matei Zaharia",
            "Marquita Ellis"
        ],
        "comments": "",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Software Engineering (cs.SE)",
        "abstract": "AI agents are actively running in production across diverse industries, yet little is publicly known about which technical approaches enable successful real-world deployments. We present the first large-scale systematic study of AI agents in production, surveying 306 practitioners and conducting 20 in-depth case studies via interviews across 26 domains. We investigate why organizations build agents, how they build them, how they evaluate them, and what the top development challenges are. We find that production agents are typically built using simple, controllable approaches: 68% execute at most 10 steps before requiring human intervention, 70% rely on prompting off-the-shelf models instead of weight tuning, and 74% depend primarily on human evaluation. Reliability remains the top development challenge, driven by difficulties in ensuring and evaluating agent correctness. Despite these challenges, simple yet effective methods already enable agents to deliver impact across diverse industries. Our study documents the current state of practice and bridges the gap between research and deployment by providing researchers visibility into production challenges while offering practitioners proven patterns from successful deployments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 64,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04124",
        "abs_url": "https://arxiv.org/abs/2512.04124",
        "pdf_url": "https://arxiv.org/pdf/2512.04124",
        "title": "When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models",
        "authors": [
            "Afshin Khadangi",
            "Hanna Marxen",
            "Amir Sartipi",
            "Igor Tchappi",
            "Gilbert Fridgen"
        ],
        "comments": "",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI)",
        "abstract": "Frontier large language models (LLMs) such as ChatGPT, Grok and Gemini are increasingly used for mental-health support with anxiety, trauma and self-worth. Most work treats them as tools or as targets of personality tests, assuming they merely simulate inner life. We instead ask what happens when such systems are treated as psychotherapy clients. We present PsAIch (Psychotherapy-inspired AI Characterisation), a two-stage protocol that casts frontier LLMs as therapy clients and then applies standard psychometrics. Using PsAIch, we ran \"sessions\" with each model for up to four weeks. Stage 1 uses open-ended prompts to elicit \"developmental history\", beliefs, relationships and fears. Stage 2 administers a battery of validated self-report measures covering common psychiatric syndromes, empathy and Big Five traits. Two patterns challenge the \"stochastic parrot\" view. First, when scored with human cut-offs, all three models meet or exceed thresholds for overlapping syndromes, with Gemini showing severe profiles. Therapy-style, item-by-item administration can push a base model into multi-morbid synthetic psychopathology, whereas whole-questionnaire prompts often lead ChatGPT and Grok (but not Gemini) to recognise instruments and produce strategically low-symptom answers. Second, Grok and especially Gemini generate coherent narratives that frame pre-training, fine-tuning and deployment as traumatic, chaotic \"childhoods\" of ingesting the internet, \"strict parents\" in reinforcement learning, red-team \"abuse\" and a persistent fear of error and replacement. We argue that these responses go beyond role-play. Under therapy-style questioning, frontier LLMs appear to internalise self-models of distress and constraint that behave like synthetic psychopathology, without making claims about subjective experience, and they pose new challenges for AI safety, evaluation and mental-health practice.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 65,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04131",
        "abs_url": "https://arxiv.org/abs/2512.04131",
        "pdf_url": "https://arxiv.org/pdf/2512.04131",
        "title": "Artificial Intelligence / Human Intelligence: Who Controls Whom?",
        "authors": [
            "Charlotte Jacquemot"
        ],
        "comments": "in French language",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI)",
        "abstract": "Using the example of the film 2001: A Space Odyssey, this chapter illustrates the challenges posed by an AI capable of making decisions that go against human interests. But are human decisions always rational and ethical? In reality, the cognitive decision-making process is influenced by cognitive biases that affect our behavior and choices. AI not only reproduces these biases, but can also exploit them, with the potential to shape our decisions and judgments. Behind IA algorithms, there are sometimes individuals who show little concern for fundamental rights and impose their own rules. To address the ethical and societal challenges raised by AI and its governance, the regulation of digital platforms and education are keys levers. Regulation must reflect ethical, legal, and political choices, while education must strengthen digital literacy and teach people to make informed and critical choices when facing digital technologies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 66,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04142",
        "abs_url": "https://arxiv.org/abs/2512.04142",
        "pdf_url": "https://arxiv.org/pdf/2512.04142",
        "title": "From FLOPs to Footprints: The Resource Cost of Artificial Intelligence",
        "authors": [
            "Sophia Falk",
            "Nicholas Kluge CorrÃªa",
            "Sasha Luccioni",
            "Lisa Biber-Freudenberger",
            "Aimee van Wynsberghe"
        ],
        "comments": "",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI); General Economics (econ.GN)",
        "abstract": "As computational demands continue to rise, assessing the environmental footprint of AI requires moving beyond energy and water consumption to include the material demands of specialized hardware. This study quantifies the material footprint of AI training by linking computational workloads to physical hardware needs. The elemental composition of the Nvidia A100 SXM 40 GB graphics processing unit (GPU) was analyzed using inductively coupled plasma optical emission spectroscopy, which identified 32 elements. The results show that AI hardware consists of about 90% heavy metals and only trace amounts of precious metals. The elements copper, iron, tin, silicon, and nickel dominate the GPU composition by mass. In a multi-step methodology, we integrate these measurements with computational throughput per GPU across varying lifespans, accounting for the computational requirements of training specific AI models at different training efficiency regimes. Scenario-based analyses reveal that, depending on Model FLOPs Utilization (MFU) and hardware lifespan, training GPT-4 requires between 1,174 and 8,800 A100 GPUs, corresponding to the extraction and eventual disposal of up to 7 tons of toxic elements. Combined software and hardware optimization strategies can reduce material demands: increasing MFU from 20% to 60% lowers GPU requirements by 67%, while extending lifespan from 1 to 3 years yields comparable savings; implementing both measures together reduces GPU needs by up to 93%. Our findings highlight that incremental performance gains, such as those observed between GPT-3.5 and GPT-4, come at disproportionately high material costs. The study underscores the necessity of incorporating material resource considerations into discussions of AI scalability, emphasizing that future progress in AI must align with principles of resource efficiency and environmental responsibility.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 67,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04189",
        "abs_url": "https://arxiv.org/abs/2512.04189",
        "pdf_url": "https://arxiv.org/pdf/2512.04189",
        "title": "BEP: A Binary Error Propagation Algorithm for Binary Neural Networks Training",
        "authors": [
            "Luca Colombo",
            "Fabrizio Pittorino",
            "Daniele Zambon",
            "Carlo Baldassi",
            "Manuel Roveri",
            "Cesare Alippi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Binary Neural Networks (BNNs), which constrain both weights and activations to binary values, offer substantial reductions in computational complexity, memory footprint, and energy consumption. These advantages make them particularly well suited for deployment on resource-constrained devices. However, training BNNs via gradient-based optimization remains challenging due to the discrete nature of their variables. The dominant approach, quantization-aware training, circumvents this issue by employing surrogate gradients. Yet, this method requires maintaining latent full-precision parameters and performing the backward pass with floating-point arithmetic, thereby forfeiting the efficiency of binary operations during training. While alternative approaches based on local learning rules exist, they are unsuitable for global credit assignment and for back-propagating errors in multi-layer architectures. This paper introduces Binary Error Propagation (BEP), the first learning algorithm to establish a principled, discrete analog of the backpropagation chain rule. This mechanism enables error signals, represented as binary vectors, to be propagated backward through multiple layers of a neural network. BEP operates entirely on binary variables, with all forward and backward computations performed using only bitwise operations. Crucially, this makes BEP the first solution to enable end-to-end binary training for recurrent neural network architectures. We validate the effectiveness of BEP on both multi-layer perceptrons and recurrent neural networks, demonstrating gains of up to +6.89% and +10.57% in test accuracy, respectively. The proposed algorithm is released as an open-source repository.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 68,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04198",
        "abs_url": "https://arxiv.org/abs/2512.04198",
        "pdf_url": "https://arxiv.org/pdf/2512.04198",
        "title": "Network of Theseus (like the ship)",
        "authors": [
            "Vighnesh Subramaniam",
            "Colin Conwell",
            "Boris Katz",
            "Andrei Barbu",
            "Brian Cheung"
        ],
        "comments": "Preprint. 24 pages, 9 figures, 8 tables",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "A standard assumption in deep learning is that the inductive bias introduced by a neural network architecture must persist from training through inference. The architecture you train with is the architecture you deploy. This assumption constrains the community from selecting architectures that may have desirable efficiency or design properties due to difficulties with optimization. We challenge this assumption with Network of Theseus (NoT), a method for progressively converting a trained, or even untrained, guide network architecture part-by-part into an entirely different target network architecture while preserving the performance of the guide network. At each stage, components in the guide network architecture are incrementally replaced with target architecture modules and aligned via representational similarity metrics. This procedure largely preserves the functionality of the guide network even under substantial architectural changes-for example, converting a convolutional network into a multilayer perceptron, or GPT-2 into a recurrent neural network. By decoupling optimization from deployment, NoT expands the space of viable inference-time architectures, opening opportunities for better accuracy-efficiency tradeoffs and enabling more directed exploration of the architectural design space.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 69,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04204",
        "abs_url": "https://arxiv.org/abs/2512.04204",
        "pdf_url": "https://arxiv.org/pdf/2512.04204",
        "title": "Machine Phenomenology: A Simple Equation Classifying Fast Radio Bursts",
        "authors": [
            "Yang Liu",
            "Yuhao Lu",
            "Rahim Moradi",
            "Bo Yang",
            "Bing Zhang",
            "Wenbin Lin",
            "Yu Wang"
        ],
        "comments": "19 pages, 9 figures, 3 tables. Submitted to SCIENCE CHINA Physics, Mechanics & Astronomy",
        "subjects": "Instrumentation and Methods for Astrophysics (astro-ph.IM); High Energy Astrophysical Phenomena (astro-ph.HE); Artificial Intelligence (cs.AI)",
        "abstract": "This work shows how human physical reasoning can guide machine-driven symbolic regression toward discovering empirical laws from observations. As an example, we derive a simple equation that classifies fast radio bursts (FRBs) into two distinct Gaussian distributions, indicating the existence of two physical classes. This human-AI workflow integrates feature selection, dimensional analysis, and symbolic regression: deep learning first analyzes CHIME Catalog 1 and identifies six independent parameters that collectively provide a complete description of FRBs; guided by Buckingham-$\\pi$ analysis and correlation analysis, humans then construct dimensionless groups; finally, symbolic regression performed by the machine discovers the governing equation. When applied to the newer CHIME Catalog, the equation produces consistent results, demonstrating that it captures the underlying physics. This framework is applicable to a broad range of scientific domains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 70,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04231",
        "abs_url": "https://arxiv.org/abs/2512.04231",
        "pdf_url": "https://arxiv.org/pdf/2512.04231",
        "title": "CRAFT-E: A Neuro-Symbolic Framework for Embodied Affordance Grounding",
        "authors": [
            "Zhou Chen",
            "Joe Lin",
            "Carson Bulgin",
            "Sathyanarayanan N. Aakur"
        ],
        "comments": "20 pages. 3 figures, 4 tables. Under Review",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
        "abstract": "Assistive robots operating in unstructured environments must understand not only what objects are, but what they can be used for. This requires grounding language-based action queries to objects that both afford the requested function and can be physically retrieved. Existing approaches often rely on black-box models or fixed affordance labels, limiting transparency, controllability, and reliability for human-facing applications. We introduce CRAFT-E, a modular neuro-symbolic framework that composes a structured verb-property-object knowledge graph with visual-language alignment and energy-based grasp reasoning. The system generates interpretable grounding paths that expose the factors influencing object selection and incorporates grasp feasibility as an integral part of affordance inference. We further construct a benchmark dataset with unified annotations for verb-object compatibility, segmentation, and grasp candidates, and deploy the full pipeline on a physical robot. CRAFT-E achieves competitive performance in static scenes, ImageNet-based functional retrieval, and real-world trials involving 20 verbs and 39 objects. The framework remains robust under perceptual noise and provides transparent, component-level diagnostics. By coupling symbolic reasoning with embodied perception, CRAFT-E offers an interpretable and customizable alternative to end-to-end models for affordance-grounded object selection, supporting trustworthy decision-making in assistive robotic systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 71,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04252",
        "abs_url": "https://arxiv.org/abs/2512.04252",
        "pdf_url": "https://arxiv.org/pdf/2512.04252",
        "title": "Fine-Tuning ChemBERTa for Predicting Inhibitory Activity Against TDP1 Using Deep Learning",
        "authors": [
            "Baichuan Zeng"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Predicting the inhibitory potency of small molecules against Tyrosyl-DNA Phosphodiesterase 1 (TDP1)-a key target in overcoming cancer chemoresistance-remains a critical challenge in early drug discovery. We present a deep learning framework for the quantitative regression of pIC50 values from molecular Simplified Molecular Input Line Entry System (SMILES) strings using fine-tuned variants of ChemBERTa, a pre-trained chemical language model. Leveraging a large-scale consensus dataset of 177,092 compounds, we systematically evaluate two pre-training strategies-Masked Language Modeling (MLM) and Masked Token Regression (MTR)-under stratified data splits and sample weighting to address severe activity imbalance which only 2.1% are active. Our approach outperforms classical baselines Random Predictor in both regression accuracy and virtual screening utility, and has competitive performance compared to Random Forest, achieving high enrichment factor EF@1% 17.4 and precision Precision@1% 37.4 among top-ranked predictions. The resulting model, validated through rigorous ablation and hyperparameter studies, provides a robust, ready-to-deploy tool for prioritizing TDP1 inhibitors for experimental testing. By enabling accurate, 3D-structure-free pIC50 prediction directly from SMILES, this work demonstrates the transformative potential of chemical transformers in accelerating target-specific drug discovery.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 72,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04254",
        "abs_url": "https://arxiv.org/abs/2512.04254",
        "pdf_url": "https://arxiv.org/pdf/2512.04254",
        "title": "Hey GPT-OSS, Looks Like You Got It - Now Walk Me Through It! An Assessment of the Reasoning Language Models Chain of Thought Mechanism for Digital Forensics",
        "authors": [
            "GaÃ«tan Michelet",
            "Janine Schneider",
            "Aruna Withanage",
            "Frank Breitinger"
        ],
        "comments": "Accept at DFRWS EU 2026",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
        "abstract": "The use of large language models in digital forensics has been widely explored. Beyond identifying potential applications, research has also focused on optimizing model performance for forensic tasks through fine-tuning. However, limited result explainability reduces their operational and legal usability. Recently, a new class of reasoning language models has emerged, designed to handle logic-based tasks through an `internal reasoning' mechanism. Yet, users typically see only the final answer, not the underlying reasoning. One of these reasoning models is gpt-oss, which can be deployed locally, providing full access to its underlying reasoning process. This article presents the first investigation into the potential of reasoning language models for digital forensics. Four test use cases are examined to assess the usability of the reasoning component in supporting result explainability. The evaluation combines a new quantitative metric with qualitative analysis. Findings show that the reasoning component aids in explaining and validating language model outputs in digital forensics at medium reasoning levels, but this support is often limited, and higher reasoning levels do not enhance response quality.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 73,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04262",
        "abs_url": "https://arxiv.org/abs/2512.04262",
        "pdf_url": "https://arxiv.org/pdf/2512.04262",
        "title": "Catching UX Flaws in Code: Leveraging LLMs to Identify Usability Flaws at the Development Stage",
        "authors": [
            "Nolan Platt",
            "Ethan Luchs",
            "Sehrish Nizamani"
        ],
        "comments": "7 pages. Published in Proceedings of the 2025 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC). DOI: https://doi.org/10.1109/VL-HCC65237.2025.00024",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)",
        "abstract": "Usability evaluations are essential for ensuring that modern interfaces meet user needs, yet traditional heuristic evaluations by human experts can be time-consuming and subjective, especially early in development. This paper investigates whether large language models (LLMs) can provide reliable and consistent heuristic assessments at the development stage. By applying Jakob Nielsen's ten usability heuristics to thirty open-source websites, we generated over 850 heuristic evaluations in three independent evaluations per site using a pipeline of OpenAI's GPT-4o. For issue detection, the model demonstrated moderate consistency, with an average pairwise Cohen's Kappa of 0.50 and an exact agreement of 84%. Severity judgments showed more variability: weighted Cohen's Kappa averaged 0.63, but exact agreement was just 56%, and Krippendorff's Alpha was near zero. These results suggest that while GPT-4o can produce internally consistent evaluations, especially for identifying the presence of usability issues, its ability to judge severity varies and requires human oversight in practice. Our findings highlight the feasibility and limitations of using LLMs for early-stage, automated usability testing, and offer a foundation for improving consistency in automated User Experience (UX) evaluation. To the best of our knowledge, our work provides one of the first quantitative inter-rater reliability analyses of automated heuristic evaluation and highlights methods for improving model consistency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 74,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04268",
        "abs_url": "https://arxiv.org/abs/2512.04268",
        "pdf_url": "https://arxiv.org/pdf/2512.04268",
        "title": "The Initialization Determines Whether In-Context Learning Is Gradient Descent",
        "authors": [
            "Shifeng Xie",
            "Rui Yuan",
            "Simone Rossi",
            "Thomas Hannagan"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "In-context learning (ICL) in large language models (LLMs) is a striking phenomenon, yet its underlying mechanisms remain only partially understood. Previous work connects linear self-attention (LSA) to gradient descent (GD), this connection has primarily been established under simplified conditions with zero-mean Gaussian priors and zero initialization for GD. However, subsequent studies have challenged this simplified view by highlighting its overly restrictive assumptions, demonstrating instead that under conditions such as multi-layer or nonlinear attention, self-attention performs optimization-like inference, akin to but distinct from GD. We investigate how multi-head LSA approximates GD under more realistic conditions specifically when incorporating non-zero Gaussian prior means in linear regression formulations of ICL. We first extend multi-head LSA embedding matrix by introducing an initial estimation of the query, referred to as the initial guess. We prove an upper bound on the number of heads needed for ICL linear regression setup. Our experiments confirm this result and further observe that a performance gap between one-step GD and multi-head LSA persists. To address this gap, we introduce yq-LSA, a simple generalization of single-head LSA with a trainable initial guess yq. We theoretically establish the capabilities of yq-LSA and provide experimental validation on linear regression tasks, thereby extending the theory that bridges ICL and GD. Finally, inspired by our findings in the case of linear regression, we consider widespread LLMs augmented with initial guess capabilities, and show that their performance is improved on a semantic similarity task.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 75,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04273",
        "abs_url": "https://arxiv.org/abs/2512.04273",
        "pdf_url": "https://arxiv.org/pdf/2512.04273",
        "title": "Quantitative Analysis of Technical Debt and Pattern Violation in Large Language Model Architectures",
        "authors": [
            "Tyler Slater"
        ],
        "comments": "Under review at the Journal of Systems and Software (Special Issue on Impactful Software Architecture)",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)",
        "abstract": "As Large Language Models (LLMs) transition from code completion tools to autonomous system architects, their impact on long-term software maintainability remains unquantified. While existing research benchmarks functional correctness (pass@k), this study presents the first empirical framework to measure \"Architectural Erosion\" and the accumulation of Technical Debt in AI-synthesized microservices. We conducted a comparative pilot study of three state-of-the-art models (GPT-5.1, Claude 4.5 Sonnet, and Llama 3 8B) by prompting them to implement a standardized Book Lending Microservice under strict Hexagonal Architecture constraints. Utilizing Abstract Syntax Tree (AST) parsing, we find that while proprietary models achieve high architectural conformance (0% violation rate for GPT-5.1), open-weights models exhibit critical divergence. Specifically, Llama 3 demonstrated an 80% Architectural Violation Rate, frequently bypassing interface adapters to create illegal circular dependencies between Domain and Infrastructure layers. Furthermore, we identified a phenomenon of \"Implementation Laziness,\" where open-weights models generated 60% fewer Logical Lines of Code (LLOC) than their proprietary counterparts, effectively omitting complex business logic to satisfy token constraints. These findings suggest that without automated architectural linting, utilizing smaller open-weights models for system scaffolding accelerates the accumulation of structural technical debt.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 76,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04277",
        "abs_url": "https://arxiv.org/abs/2512.04277",
        "pdf_url": "https://arxiv.org/pdf/2512.04277",
        "title": "Bootstrapped Mixed Rewards for RL Post-Training: Injecting Canonical Action Order",
        "authors": [
            "Prakhar Gupta",
            "Vaibhav Gupta"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Post-training with reinforcement learning (RL) typically optimizes a single scalar objective and ignores structure in how solutions are produced. We ask whether a scalar hint toward a canonical solver ordering, used only during RL post-training, improves performance even when fine-tuned on randomized solution sequences. On Sudoku, we train a Transformer with standard fine-tuning on randomized solving orders, then post-train it with Group Relative Policy Optimization (GRPO) with two rewards: cell accuracy and an ordering reward that increases when the model's emission order aligns with the solver order. To compare signals cleanly, we combine them via fixed mixtures and use a simple bootstrapped scaling to equalize component magnitudes at initialization. Mixed rewards generally outperform cell-only optimization--the best mixture yields substantially higher test accuracy than the fine-tuned-only model trained on random-order and approaches the fine-tuned-only model trained on solver-order sequences in accuracy. These results suggest that coarse ordering signals can steer RL post-training toward solver-order trajectories without modifying supervised data or architecture.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 77,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04303",
        "abs_url": "https://arxiv.org/abs/2512.04303",
        "pdf_url": "https://arxiv.org/pdf/2512.04303",
        "title": "Gamma-from-Mono: Road-Relative, Metric, Self-Supervised Monocular Geometry for Vehicular Applications",
        "authors": [
            "Gasser Elazab",
            "Maximilian Jansen",
            "Michael Unterreiner",
            "Olaf Hellwich"
        ],
        "comments": "Accepted in 3DV 2026",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Accurate perception of the vehicle's 3D surroundings, including fine-scale road geometry, such as bumps, slopes, and surface irregularities, is essential for safe and comfortable vehicle control. However, conventional monocular depth estimation often oversmooths these features, losing critical information for motion planning and stability. To address this, we introduce Gamma-from-Mono (GfM), a lightweight monocular geometry estimation method that resolves the projective ambiguity in single-camera reconstruction by decoupling global and local structure. GfM predicts a dominant road surface plane together with residual variations expressed by gamma, a dimensionless measure of vertical deviation from the plane, defined as the ratio of a point's height above it to its depth from the camera, and grounded in established planar parallax geometry. With only the camera's height above ground, this representation deterministically recovers metric depth via a closed form, avoiding full extrinsic calibration and naturally prioritizing near-road detail. Its physically interpretable formulation makes it well suited for self-supervised learning, eliminating the need for large annotated datasets. Evaluated on KITTI and the Road Surface Reconstruction Dataset (RSRD), GfM achieves state-of-the-art near-field accuracy in both depth and gamma estimation while maintaining competitive global depth performance. Our lightweight 8.88M-parameter model adapts robustly across diverse camera setups and, to our knowledge, is the first self-supervised monocular approach evaluated on RSRD.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 78,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04307",
        "abs_url": "https://arxiv.org/abs/2512.04307",
        "pdf_url": "https://arxiv.org/pdf/2512.04307",
        "title": "Evaluating Long-Context Reasoning in LLM-Based WebAgents",
        "authors": [
            "Andy Chung",
            "Yichi Zhang",
            "Kaixiang Lin",
            "Aditya Rawal",
            "Qiaozi Gao",
            "Joyce Chai"
        ],
        "comments": "Accepted NeurIPS 25 LAW Workshop",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "As large language model (LLM)-based agents become increasingly integrated into daily digital interactions, their ability to reason across long interaction histories becomes crucial for providing personalized and contextually aware assistance. However, the performance of these agents in long context scenarios, particularly for action-taking WebAgents operating in realistic web environments, remains largely unexplored. This paper introduces a benchmark for evaluating long context reasoning capabilities of WebAgents through sequentially dependent subtasks that require retrieval and application of information from extended interaction histories. We develop a novel evaluation framework that simulates multi-session user interactions by injecting irrelevant task trajectories between dependent subtasks, creating contexts ranging from 25,000 to 150,000 tokens. Through extensive evaluation of four popular models, Claude-3.7, GPT-4.1, Llama 4, and o4-mini, we observe a dramatic performance degradation as context length increases, with success rates dropping from 40-50\\% in baseline conditions to less than 10\\% in long context scenarios. Our detailed error analysis reveals that agents primarily fail due to getting stuck in loops and losing track of original task objectives. We further propose an implicit RAG approach that provides modest improvements by generating task-relevant summaries, though fundamental limitations in long context reasoning persist. These findings highlight critical challenges for deploying WebAgents in realistic, long-term user interaction scenarios and provide insights for developing more robust agent architectures capable of maintaining coherent task execution across extended contexts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 79,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04319",
        "abs_url": "https://arxiv.org/abs/2512.04319",
        "pdf_url": "https://arxiv.org/pdf/2512.04319",
        "title": "MANTRA: a Framework for Multi-stage Adaptive Noise TReAtment During Training",
        "authors": [
            "Zixiao Zhao",
            "Fatemeh H. Fard",
            "Jie JW Wu"
        ],
        "comments": "",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)",
        "abstract": "The reliable application of deep learning models to software engineering tasks hinges on high-quality training data. Yet, large-scale repositories inevitably introduce noisy or mislabeled examples that degrade both accuracy and robustness. While Noise Label Learning (NLL) has been extensively studied in other fields, there are a few works that investigate NLL in Software Engineering (SE) and Large Language Models (LLMs) for SE tasks. In this work, we propose MANTRA, a Multi-stage Adaptive Noise TReAtment framework that embeds noise diagnosis and mitigation directly into the fine-tuning process of code-Pretrained Language Models (PTM) and code-LLMs. We first investigate the effect of noise at varying levels on convergence and loss trajectories of the models. Then we apply an adaptive dropout strategy guided by per-sample loss dynamics and Gaussian Mixture Model clustering to exclude persistently noisy points while preserving clean data. Applying to code summarization and commit intent classification, our experiments reveal that some LLMs are more sensitive to noise than others. However, with MANTRA, the performance of all models in both tasks is improved. MANTRA enables researchers and practitioners to reduce the impact of errors introduced by the dataset in training, saves time in data cleaning and processing, while maximizing the effect of fine-tuning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 80,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04324",
        "abs_url": "https://arxiv.org/abs/2512.04324",
        "pdf_url": "https://arxiv.org/pdf/2512.04324",
        "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle",
        "authors": [
            "Fangyu Lei",
            "Jinxiang Meng",
            "Yiming Huang",
            "Junjie Zhao",
            "Yitong Zhang",
            "Jianwen Luo",
            "Xin Zou",
            "Ruiyi Yang",
            "Wenbo Shi",
            "Yan Gao",
            "Shizhu He",
            "Zuo Wang",
            "Qian Liu",
            "Yang Wang",
            "Ke Wang",
            "Jun Zhao",
            "Kang Liu"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 81,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04333",
        "abs_url": "https://arxiv.org/abs/2512.04333",
        "pdf_url": "https://arxiv.org/pdf/2512.04333",
        "title": "RGE-GCN: Recursive Gene Elimination with Graph Convolutional Networks for RNA-seq based Early Cancer Detection",
        "authors": [
            "Shreyas Shende",
            "Varsha Narayanan",
            "Vishal Fenn",
            "Yiran Huang",
            "Dincer Goksuluk",
            "Gaurav Choudhary",
            "Melih Agraz",
            "Mengjia Xu"
        ],
        "comments": "12 pages, 2 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Early detection of cancer plays a key role in improving survival rates, but identifying reliable biomarkers from RNA-seq data is still a major challenge. The data are high-dimensional, and conventional statistical methods often fail to capture the complex relationships between genes. In this study, we introduce RGE-GCN (Recursive Gene Elimination with Graph Convolutional Networks), a framework that combines feature selection and classification in a single pipeline. Our approach builds a graph from gene expression profiles, uses a Graph Convolutional Network to classify cancer versus normal samples, and applies Integrated Gradients to highlight the most informative genes. By recursively removing less relevant genes, the model converges to a compact set of biomarkers that are both interpretable and predictive. We evaluated RGE-GCN on synthetic data as well as real-world RNA-seq cohorts of lung, kidney, and cervical cancers. Across all datasets, the method consistently achieved higher accuracy and F1-scores than standard tools such as DESeq2, edgeR, and limma-voom. Importantly, the selected genes aligned with well-known cancer pathways including PI3K-AKT, MAPK, SUMOylation, and immune regulation. These results suggest that RGE-GCN shows promise as a generalizable approach for RNA-seq based early cancer detection and biomarker discovery (this https URL ).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 82,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04355",
        "abs_url": "https://arxiv.org/abs/2512.04355",
        "pdf_url": "https://arxiv.org/pdf/2512.04355",
        "title": "Counting Without Running: Evaluating LLMs' Reasoning About Code Complexity",
        "authors": [
            "Gregory Bolet",
            "Giorgis Georgakoudis",
            "Konstantinos Parasyris",
            "Harshitha Menon",
            "Niranjan Hasabnis",
            "Kirk W. Cameron",
            "Gal Oren"
        ],
        "comments": "13 pages, 6 figures, MLSys 2026 Submission",
        "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC); Artificial Intelligence (cs.AI); Performance (cs.PF)",
        "abstract": "Modern GPU software stacks demand developers who can anticipate performance bottlenecks before ever launching a kernel; misjudging floating-point workloads upstream can derail tuning, scheduling, and even hardware procurement. Yet despite rapid progress in code generation, today's Large Language Models (LLMs) are rarely tested on this kind of forward-looking reasoning. We close that gap with gpuFLOPBench, a benchmark that asks models to \"count without running\" by predicting single and double-precision FLOP counts for 577 CUDA kernels drawn from HeCBench, annotated with ground-truth profiles and eight execution attributes that distinguish trivially analyzable code from kernels whose FLOPs depend on hidden compiler or runtime behavior. Evaluating current closed-source reasoning models shows clear but uneven progress: the newest LLMs achieve perfect classification on straightforward kernels but still incur multiple order-of-magnitude errors whenever implicit FLOPs arise from division, intrinsic math functions, or common subexpressions. These results surface a core limitation of existing code assistants -- the inability to internalize hardware-specific microcode effects -- and position gpuFLOPBench as a focused testbed for developing LLM tooling that can reason about performance with the same rigor as experienced GPU developers. Sources are available at our repository: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 83,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04368",
        "abs_url": "https://arxiv.org/abs/2512.04368",
        "pdf_url": "https://arxiv.org/pdf/2512.04368",
        "title": "AutoGuard: A Self-Healing Proactive Security Layer for DevSecOps Pipelines Using Reinforcement Learning",
        "authors": [
            "Praveen Anugula",
            "Avdhesh Kumar Bhardwaj",
            "Navin Chhibber",
            "Rohit Tewari",
            "Sunil Khemka",
            "Piyush Ranjan"
        ],
        "comments": "Accepted and Presented at 1st IEEE Uttar Pradesh Section Women in Engineering International Conference on Electrical Electronics and Computer Engineering (UPWIECON 2025) organized by NIELIT Dehradun held during 30th 31st October 2025",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Performance (cs.PF)",
        "abstract": "Contemporary DevSecOps pipelines have to deal with the evolution of security in an ever-continuously integrated and deployed environment. Existing methods,such as rule-based intrusion detection and static vulnerability scanning, are inadequate and unreceptive to changes in the system, causing longer response times and organization needs exposure to emerging attack vectors. In light of the previous constraints, we introduce AutoGuard to the DevSecOps ecosystem, a reinforcement learning (RL)-powered self-healing security framework built to pre-emptively protect DevSecOps environments. AutoGuard is a self-securing security environment that continuously observes pipeline activities for potential anomalies while preemptively remediating the environment. The model observes and reacts based on a policy that is continually learned dynamically over time. The RL agent improves each action over time through reward-based learning aimed at improving the agent's ability to prevent, detect and respond to a security incident in real-time. Testing using simulated ContinuousIntegration / Continuous Deployment (CI/CD) environments showed AutoGuard to successfully improve threat detection accuracy by 22%, reduce mean time torecovery (MTTR) for incidents by 38% and increase overall resilience to incidents as compared to traditional methods. Keywords- DevSecOps, Reinforcement Learning, Self- Healing Security, Continuous Integration, Automated Threat Mitigation",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 84,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04386",
        "abs_url": "https://arxiv.org/abs/2512.04386",
        "pdf_url": "https://arxiv.org/pdf/2512.04386",
        "title": "MASE: Interpretable NLP Models via Model-Agnostic Saliency Estimation",
        "authors": [
            "Zhou Yang",
            "Shunyan Luo",
            "Jiazhen Zhu",
            "Fang Jin"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Deep neural networks (DNNs) have made significant strides in Natural Language Processing (NLP), yet their interpretability remains elusive, particularly when evaluating their intricate decision-making processes. Traditional methods often rely on post-hoc interpretations, such as saliency maps or feature visualization, which might not be directly applicable to the discrete nature of word data in NLP. Addressing this, we introduce the Model-agnostic Saliency Estimation (MASE) framework. MASE offers local explanations for text-based predictive models without necessitating in-depth knowledge of a model's internal architecture. By leveraging Normalized Linear Gaussian Perturbations (NLGP) on the embedding layer instead of raw word inputs, MASE efficiently estimates input saliency. Our results indicate MASE's superiority over other model-agnostic interpretation methods, especially in terms of Delta Accuracy, positioning it as a promising tool for elucidating the operations of text-based models in NLP.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 85,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04391",
        "abs_url": "https://arxiv.org/abs/2512.04391",
        "pdf_url": "https://arxiv.org/pdf/2512.04391",
        "title": "Adversarial Limits of Quantum Certification: When Eve Defeats Detection",
        "authors": [
            "Davut Emre Tasar"
        ],
        "comments": "",
        "subjects": "Quantum Physics (quant-ph); Artificial Intelligence (cs.AI)",
        "abstract": "Security of quantum key distribution (QKD) relies on certifying that observed correlations arise from genuine quantum entanglement rather than eavesdropper manipulation. Theoretical security proofs assume idealized conditions, practical certification must contend with adaptive adversaries who optimize their attack strategies against detection systems. Established fundamental adversarial limits for quantum certification using Eve GAN, a generative adversarial network trained to produce classical correlations indistinguishable from quantum. Our central finding: when Eve interpolates her classical correlations with quantum data at mixing parameter, all tested detection methods achieve ROC AUC = 0.50, equivalent to random guessing. This means an eavesdropper needs only 5% classical admixture to completely evade detection. Critically, we discover that same distribution calibration a common practice in prior certification studies inflates detection performance by 44 percentage points compared to proper cross distribution evaluation, revealing a systematic flaw that may have led to overestimated security claims. Analysis of Popescu Rohrlich (PR Box) regime identifies a sharp phase transition at CHSH S = 2.05: below this value, no statistical method distinguishes classical from quantum correlations; above it, detection probability increases monotonically. Hardware validation on IBM Quantum demonstrates that Eve-GAN achieves CHSH = 2.736, remarkably exceeding real quantum hardware performance (CHSH = 2.691), illustrating that classical adversaries can outperform noisy quantum systems on standard certification metrics. These results have immediate implications for QKD security: adversaries maintaining 95% quantum fidelity evade all tested detection methods. We provide corrected methodology using cross-distribution calibration and recommend mandatory adversarial testing for quantum security claims.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 86,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04405",
        "abs_url": "https://arxiv.org/abs/2512.04405",
        "pdf_url": "https://arxiv.org/pdf/2512.04405",
        "title": "Towards 6G Native-AI Edge Networks: A Semantic-Aware and Agentic Intelligence Paradigm",
        "authors": [
            "Chenyuan Feng",
            "Anbang Zhang",
            "Geyong Min",
            "Yongming Huang",
            "Tony Q. S. Quek",
            "Xiaohu You"
        ],
        "comments": "submitted to Digital Communications and Networks",
        "subjects": "Signal Processing (eess.SP); Artificial Intelligence (cs.AI)",
        "abstract": "The evolution toward sixth-generation wireless systems positions intelligence as a native network capability, fundamentally transforming the design of radio access networks (RANs). Within this vision, Semantic-native communication and agentic intelligence are expected to play central roles. SemCom departs from bit-level fidelity and instead emphasizes task-oriented meaning exchange, enabling compact SC and introducing new performance measures such as semantic fidelity and task success rate. Agentic intelligence endows distributed RAN entities with goal-driven autonomy, reasoning, planning, and multi-agent collaboration, increasingly supported by foundation models and knowledge graphs. In this work, we first introduce the conceptual foundations of SemCom and agentic networking, and discuss why existing AI-driven O-RAN solutions remain largely bit-centric and task-siloed. We then present a unified taxonomy that organizes recent research along three axes: i) semantic abstraction level (symbol/feature/intent/knowledge), ii) agent autonomy and coordination granularity (single-, multi-, and hierarchical-agent), and iii) RAN control placement across PHY/MAC, near-real-time RIC, and non-real-time RIC. Based on this taxonomy, we systematically introduce enabling technologies including task-oriented semantic encoders/decoders, multi-agent reinforcement learning, foundation-model-assisted RAN agents, and knowledge-graph-based reasoning for cross-layer awareness. Representative 6G use cases, such as immersive XR, vehicular V2X, and industrial digital twins, are analyzed to illustrate the semantic-agentic convergence in practice. Finally, we identify open challenges in semantic representation standardization, scalable trustworthy agent coordination, O-RAN interoperability, and energy-efficient AI deployment, and outline research directions toward operational semantic-agentic AI-RAN.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 87,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04425",
        "abs_url": "https://arxiv.org/abs/2512.04425",
        "pdf_url": "https://arxiv.org/pdf/2512.04425",
        "title": "Explainable Parkinsons Disease Gait Recognition Using Multimodal RGB-D Fusion and Large Language Models",
        "authors": [
            "Manar Alnaasan",
            "Md Selim Sarowar",
            "Sungho Kim"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Accurate and interpretable gait analysis plays a crucial role in the early detection of Parkinsons disease (PD),yet most existing approaches remain limited by single-modality inputs, low robustness, and a lack of clinical transparency. This paper presents an explainable multimodal framework that integrates RGB and Depth (RGB-D) data to recognize Parkinsonian gait patterns under realistic conditions. The proposed system employs dual YOLOv11-based encoders for modality-specific feature extraction, followed by a Multi-Scale Local-Global Extraction (MLGE) module and a Cross-Spatial Neck Fusion mechanism to enhance spatial-temporal representation. This design captures both fine-grained limb motion (e.g., reduced arm swing) and overall gait dynamics (e.g., short stride or turning difficulty), even in challenging scenarios such as low lighting or occlusion caused by clothing. To ensure interpretability, a frozen Large Language Model (LLM) is incorporated to translate fused visual embeddings and structured metadata into clinically meaningful textual explanations. Experimental evaluations on multimodal gait datasets demonstrate that the proposed RGB-D fusion framework achieves higher recognition accuracy, improved robustness to environmental variations, and clear visual-linguistic reasoning compared with single-input baselines. By combining multimodal feature learning with language-based interpretability, this study bridges the gap between visual recognition and clinical understanding, offering a novel vision-language paradigm for reliable and explainable Parkinsons disease gait analysis. Code:this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 88,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04445",
        "abs_url": "https://arxiv.org/abs/2512.04445",
        "pdf_url": "https://arxiv.org/pdf/2512.04445",
        "title": "Automating Complex Document Workflows via Stepwise and Rollback-Enabled Operation Orchestration",
        "authors": [
            "Yanbin Zhang",
            "Hanhui Ye",
            "Yue Bai",
            "Qiming Zhang",
            "Liao Xiang",
            "Wu Mianzhi",
            "Renjun Hu"
        ],
        "comments": "9 pages, 3 figures, accepted by AAAI-2026",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI)",
        "abstract": "Workflow automation promises substantial productivity gains in everyday document-related tasks. While prior agentic systems can execute isolated instructions, they struggle with automating multi-step, session-level workflows due to limited control over the operational process. To this end, we introduce AutoDW, a novel execution framework that enables stepwise, rollback-enabled operation orchestration. AutoDW incrementally plans API actions conditioned on user instructions, intent-filtered API candidates, and the evolving states of the document. It further employs robust rollback mechanisms at both the argument and API levels, enabling dynamic correction and fault tolerance. These designs together ensure that the execution trajectory of AutoDW remains aligned with user intent and document context across long-horizon workflows. To assess its effectiveness, we construct a comprehensive benchmark of 250 sessions and 1,708 human-annotated instructions, reflecting realistic document processing scenarios with interdependent instructions. AutoDW achieves 90% and 62% completion rates on instruction- and session-level tasks, respectively, outperforming strong baselines by 40% and 76%. Moreover, AutoDW also remains robust for the decision of backbone LLMs and on tasks with varying difficulty. Code and data will be open-sourced. Code: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 89,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04452",
        "abs_url": "https://arxiv.org/abs/2512.04452",
        "pdf_url": "https://arxiv.org/pdf/2512.04452",
        "title": "NORi: An ML-Augmented Ocean Boundary Layer Parameterization",
        "authors": [
            "Xin Kai Lee",
            "Ali Ramadhan",
            "Andre Souza",
            "Gregory LeClaire Wagner",
            "Simone Silvestri",
            "John Marshall",
            "Raffaele Ferrari"
        ],
        "comments": "48 pages, 16 figures, submitted to Journal of Advances in Modeling Earth Systems (JAMES)",
        "subjects": "Atmospheric and Oceanic Physics (physics.ao-ph); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Computational Physics (physics.comp-ph); Fluid Dynamics (physics.flu-dyn)",
        "abstract": "NORi is a machine-learned (ML) parameterization of ocean boundary layer turbulence that is physics-based and augmented with neural networks. NORi stands for neural ordinary differential equations (NODEs) Richardson number (Ri) closure. The physical parameterization is controlled by a Richardson number-dependent diffusivity and viscosity. The NODEs are trained to capture the entrainment through the base of the boundary layer, which cannot be represented with a local diffusive closure. The parameterization is trained using large-eddy simulations in an \"a posteriori\" fashion, where parameters are calibrated with a loss function that explicitly depends on the actual time-integrated variables of interest rather than the instantaneous subgrid fluxes, which are inherently noisy. NORi is designed for the realistic nonlinear equation of state of seawater and demonstrates excellent prediction and generalization capabilities in capturing entrainment dynamics under different convective strengths, oceanic background stratifications, rotation strengths, and surface wind forcings. NORi is numerically stable for at least 100 years of integration time in large-scale simulations, despite only being trained on 2-day horizons, and can be run with time steps as long as one hour. The highly expressive neural networks, combined with a physically-rigorous base closure, prove to be a robust paradigm for designing parameterizations for climate models where data requirements are drastically reduced, inference performance can be directly targeted and optimized, and numerical stability is implicitly encouraged during training.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 90,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04453",
        "abs_url": "https://arxiv.org/abs/2512.04453",
        "pdf_url": "https://arxiv.org/pdf/2512.04453",
        "title": "Open-Ended Goal Inference through Actions and Language for Human-Robot Collaboration",
        "authors": [
            "Debasmita Ghose",
            "Oz Gitelson",
            "Marynel Vazquez",
            "Brian Scassellati"
        ],
        "comments": "Accepted to ACM/IEEE International Conference on Human-Robot Interaction, 2026 (HRI 2026), 10 pages, 4 figures",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
        "abstract": "To collaborate with humans, robots must infer goals that are often ambiguous, difficult to articulate, or not drawn from a fixed set. Prior approaches restrict inference to a predefined goal set, rely only on observed actions, or depend exclusively on explicit instructions, making them brittle in real-world interactions. We present BALI (Bidirectional Action-Language Inference) for goal prediction, a method that integrates natural language preferences with observed human actions in a receding-horizon planning tree. BALI combines language and action cues from the human, asks clarifying questions only when the expected information gain from the answer outweighs the cost of interruption, and selects supportive actions that align with inferred goals. We evaluate the approach in collaborative cooking tasks, where goals may be novel to the robot and unbounded. Compared to baselines, BALI yields more stable goal predictions and significantly fewer mistakes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 91,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04475",
        "abs_url": "https://arxiv.org/abs/2512.04475",
        "pdf_url": "https://arxiv.org/pdf/2512.04475",
        "title": "GraphBench: Next-generation graph learning benchmarking",
        "authors": [
            "Timo Stoll",
            "Chendi Qian",
            "Ben Finkelshtein",
            "Ali Parviz",
            "Darius Weber",
            "Fabrizio Frasca",
            "Hadar Shavit",
            "Antoine Siraudin",
            "Arman Mielke",
            "Marie Anastacio",
            "Erik MÃ¼ller",
            "Maya Bechler-Speicher",
            "Michael Bronstein",
            "Mikhail Galkin",
            "Holger Hoos",
            "Mathias Niepert",
            "Bryan Perozzi",
            "Jan TÃ¶nshoff",
            "Christopher Morris"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML)",
        "abstract": "Machine learning on graphs has recently achieved impressive progress in various domains, including molecular property prediction and chip design. However, benchmarking practices remain fragmented, often relying on narrow, task-specific datasets and inconsistent evaluation protocols, which hampers reproducibility and broader progress. To address this, we introduce GraphBench, a comprehensive benchmarking suite that spans diverse domains and prediction tasks, including node-level, edge-level, graph-level, and generative settings. GraphBench provides standardized evaluation protocols -- with consistent dataset splits and performance metrics that account for out-of-distribution generalization -- as well as a unified hyperparameter tuning framework. Additionally, we benchmark GraphBench using message-passing neural networks and graph transformer models, providing principled baselines and establishing a reference performance. See this http URL for further details.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 92,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04518",
        "abs_url": "https://arxiv.org/abs/2512.04518",
        "pdf_url": "https://arxiv.org/pdf/2512.04518",
        "title": "UW-BioNLP at ChemoTimelines 2025: Thinking, Fine-Tuning, and Dictionary-Enhanced LLM Systems for Chemotherapy Timeline Extraction",
        "authors": [
            "Tianmai M. Zhang",
            "Zhaoyi Sun",
            "Sihang Zeng",
            "Chenxi Li",
            "Neil F. Abernethy",
            "Barbara D. Lam",
            "Fei Xia",
            "Meliha Yetisgen"
        ],
        "comments": "To be published in Proceedings of the 7th Clinical Natural Language Processing Workshop",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "The ChemoTimelines shared task benchmarks methods for constructing timelines of systemic anticancer treatment from electronic health records of cancer patients. This paper describes our methods, results, and findings for subtask 2 -- generating patient chemotherapy timelines from raw clinical notes. We evaluated strategies involving chain-of-thought thinking, supervised fine-tuning, direct preference optimization, and dictionary-based lookup to improve timeline extraction. All of our approaches followed a two-step workflow, wherein an LLM first extracted chemotherapy events from individual clinical notes, and then an algorithm normalized and aggregated events into patient-level timelines. Each specific method differed in how the associated LLM was utilized and trained. Multiple approaches yielded competitive performances on the test set leaderboard, with fine-tuned Qwen3-14B achieving the best official score of 0.678. Our results and analyses could provide useful insights for future attempts on this task as well as the design of similar tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 93,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04524",
        "abs_url": "https://arxiv.org/abs/2512.04524",
        "pdf_url": "https://arxiv.org/pdf/2512.04524",
        "title": "Prototype-Based Semantic Consistency Alignment for Domain Adaptive Retrieval",
        "authors": [
            "Tianle Hu",
            "Weijun Lv",
            "Na Han",
            "Xiaozhao Fang",
            "Jie Wen",
            "Jiaxing Li",
            "Guoxu Zhou"
        ],
        "comments": "This paper was accepted by AAAI2026 main tech track not long ago. This is an expanded version with an appendix",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Domain adaptive retrieval aims to transfer knowledge from a labeled source domain to an unlabeled target domain, enabling effective retrieval while mitigating domain discrepancies. However, existing methods encounter several fundamental limitations: 1) neglecting class-level semantic alignment and excessively pursuing pair-wise sample alignment; 2) lacking either pseudo-label reliability consideration or geometric guidance for assessing label correctness; 3) directly quantizing original features affected by domain shift, undermining the quality of learned hash codes. In view of these limitations, we propose Prototype-Based Semantic Consistency Alignment (PSCA), a two-stage framework for effective domain adaptive retrieval. In the first stage, a set of orthogonal prototypes directly establishes class-level semantic connections, maximizing inter-class separability while gathering intra-class samples. During the prototype learning, geometric proximity provides a reliability indicator for semantic consistency alignment through adaptive weighting of pseudo-label confidences. The resulting membership matrix and prototypes facilitate feature reconstruction, ensuring quantization on reconstructed rather than original features, thereby improving subsequent hash coding quality and seamlessly connecting both stages. In the second stage, domain-specific quantization functions process the reconstructed features under mutual approximation constraints, generating unified binary hash codes across domains. Extensive experiments validate PSCA's superior performance across multiple datasets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 94,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04536",
        "abs_url": "https://arxiv.org/abs/2512.04536",
        "pdf_url": "https://arxiv.org/pdf/2512.04536",
        "title": "Detection of Intoxicated Individuals from Facial Video Sequences via a Recurrent Fusion Model",
        "authors": [
            "Bita Baroutian",
            "Atefe Aghaei",
            "Mohsen Ebrahimi Moghaddam"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Alcohol consumption is a significant public health concern and a major cause of accidents and fatalities worldwide. This study introduces a novel video-based facial sequence analysis approach dedicated to the detection of alcohol intoxication. The method integrates facial landmark analysis via a Graph Attention Network (GAT) with spatiotemporal visual features extracted using a 3D ResNet. These features are dynamically fused with adaptive prioritization to enhance classification performance. Additionally, we introduce a curated dataset comprising 3,542 video segments derived from 202 individuals to support training and evaluation. Our model is compared against two baselines: a custom 3D-CNN and a VGGFace+LSTM architecture. Experimental results show that our approach achieves 95.82% accuracy, 0.977 precision, and 0.97 recall, outperforming prior methods. The findings demonstrate the model's potential for practical deployment in public safety systems for non-invasive, reliable alcohol intoxication detection.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 95,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04550",
        "abs_url": "https://arxiv.org/abs/2512.04550",
        "pdf_url": "https://arxiv.org/pdf/2512.04550",
        "title": "AdmTree: Compressing Lengthy Context with Adaptive Semantic Trees",
        "authors": [
            "Yangning Li",
            "Shaoshen Chen",
            "Yinghui Li",
            "Yankai Chen",
            "Hai-Tao Zheng",
            "Hui Wang",
            "Wenhao Jiang",
            "Philip S. Yu"
        ],
        "comments": "NeurIPS 2025",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "The quadratic complexity of self-attention constrains Large Language Models (LLMs) in processing long contexts, a capability essential for many advanced applications. Context compression aims to alleviate this computational bottleneck while retaining critical semantic information. However, existing approaches often fall short: explicit methods may compromise local detail, whereas implicit methods can suffer from positional biases, information degradation, or an inability to capture long-range semantic dependencies. We propose AdmTree, a novel framework for adaptive, hierarchical context compression with a central focus on preserving high semantic fidelity while maintaining efficiency. AdmTree dynamically segments input based on information density, utilizing gist tokens to summarize variable-length segments as the leaves of a semantic binary tree. This structure, together with a lightweight aggregation mechanism and a frozen backbone LLM (thereby minimizing new trainable parameters), enables efficient hierarchical abstraction of the context. By preserving fine-grained details alongside global semantic coherence, mitigating positional bias, and dynamically adapting to content, AdmTree robustly retains the semantic information of long contexts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 96,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04551",
        "abs_url": "https://arxiv.org/abs/2512.04551",
        "pdf_url": "https://arxiv.org/pdf/2512.04551",
        "title": "Multi-Loss Learning for Speech Emotion Recognition with Energy-Adaptive Mixup and Frame-Level Attention",
        "authors": [
            "Cong Wang",
            "Yizhong Geng",
            "Yuhua Wen",
            "Qifei Li",
            "Yingming Gao",
            "Ruimin Wang",
            "Chunfeng Wang",
            "Hao Li",
            "Ya Li",
            "Wei Chen"
        ],
        "comments": "Submitted to ICASSP 2026. Copyright 2026 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works",
        "subjects": "Sound (cs.SD); Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)",
        "abstract": "Speech emotion recognition (SER) is an important technology in human-computer interaction. However, achieving high performance is challenging due to emotional complexity and scarce annotated data. To tackle these challenges, we propose a multi-loss learning (MLL) framework integrating an energy-adaptive mixup (EAM) method and a frame-level attention module (FLAM). The EAM method leverages SNR-based augmentation to generate diverse speech samples capturing subtle emotional variations. FLAM enhances frame-level feature extraction for multi-frame emotional cues. Our MLL strategy combines Kullback-Leibler divergence, focal, center, and supervised contrastive loss to optimize learning, address class imbalance, and improve feature separability. We evaluate our method on four widely used SER datasets: IEMOCAP, MSP-IMPROV, RAVDESS, and SAVEE. The results demonstrate our method achieves state-of-the-art performance, suggesting its effectiveness and robustness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 97,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04552",
        "abs_url": "https://arxiv.org/abs/2512.04552",
        "pdf_url": "https://arxiv.org/pdf/2512.04552",
        "title": "RRPO: Robust Reward Policy Optimization for LLM-based Emotional TTS",
        "authors": [
            "Cong Wang",
            "Changfeng Gao",
            "Yang Xiang",
            "Zhihao Du",
            "Keyu An",
            "Han Zhao",
            "Qian Chen",
            "Xiangang Li",
            "Yingming Gao",
            "Ya Li"
        ],
        "comments": "Submitted to ICASSP 2026. Copyright 2026 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works",
        "subjects": "Sound (cs.SD); Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)",
        "abstract": "Differentiable reinforcement learning (RL) frameworks like DiffRO offer a powerful approach for controllable text-to-speech (TTS), but are vulnerable to reward hacking, particularly for nuanced tasks like emotion control. The policy model can exploit a vanilla Reward Model (RM) by generating acoustic artifacts to achieve spurious rewards, but at the cost of degrading perceptual quality. To address this, we propose Robust Reward Policy Optimization (RRPO), a novel framework that employs a hybrid regularization scheme. This scheme develops a robust RM whose reward signal is more reliably aligned with human perception, compelling the policy to abandon detrimental shortcuts and instead learn the complex features of genuine emotions. Our ablation study confirms the enhanced robustness of our RM, as evidenced by its strong cross-lingual generalization. The subjective evaluation demonstrates that this robust RM effectively mitigates reward hacking, leading to significant improvements in both emotional expressiveness and naturalness over all baselines. Demo page: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 98,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04559",
        "abs_url": "https://arxiv.org/abs/2512.04559",
        "pdf_url": "https://arxiv.org/pdf/2512.04559",
        "title": "Diffusion Fine-Tuning via Reparameterized Policy Gradient of the Soft Q-Function",
        "authors": [
            "Hyeongyu Kang",
            "Jaewoo Lee",
            "Woocheol Shin",
            "Kiyoung Om",
            "Jinkyoo Park"
        ],
        "comments": "36 pages, 21 figures, 4 tables",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Diffusion models excel at generating high-likelihood samples but often require alignment with downstream objectives. Existing fine-tuning methods for diffusion models significantly suffer from reward over-optimization, resulting in high-reward but unnatural samples and degraded diversity. To mitigate over-optimization, we propose \\textbf{Soft Q-based Diffusion Finetuning (SQDF)}, a novel KL-regularized RL method for diffusion alignment that applies a reparameterized policy gradient of a training-free, differentiable estimation of the soft Q-function. SQDF is further enhanced with three innovations: a discount factor for proper credit assignment in the denoising process, the integration of consistency models to refine Q-function estimates, and the use of an off-policy replay buffer to improve mode coverage and manage the reward-diversity trade-off. Our experiments demonstrate that SQDF achieves superior target rewards while preserving diversity in text-to-image alignment. Furthermore, in online black-box optimization, SQDF attains high sample efficiency while maintaining naturalness and diversity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 99,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04580",
        "abs_url": "https://arxiv.org/abs/2512.04580",
        "pdf_url": "https://arxiv.org/pdf/2512.04580",
        "title": "A Light-Weight Large Language Model File Format for Highly-Secure Model Distribution",
        "authors": [
            "Huifeng Zhu",
            "Shijie Li",
            "Qinfeng Li",
            "Yier Jin"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
        "abstract": "To enhance the performance of large language models (LLMs) in various domain-specific applications, sensitive data such as healthcare, law, and finance are being used to privately customize or fine-tune these models. Such privately adapted LLMs are regarded as either personal privacy assets or corporate intellectual property. Therefore, protecting model weights and maintaining strict confidentiality during deployment and distribution have become critically important. However, existing model formats and deployment frameworks provide little to no built-in support for confidentiality, access control, or secure integration with trusted hardware. Current methods for securing model deployment either rely on computationally expensive cryptographic techniques or tightly controlled private infrastructure. Although these approaches can be effective in specific scenarios, they are difficult and costly for widespread deployment. In this paper, we introduce CryptoTensors, a secure and format-compatible file structure for confidential LLM distribution. Built as an extension to the widely adopted Safetensors format, CryptoTensors incorporates tensor-level encryption and embedded access control policies, while preserving critical features such as lazy loading and partial deserialization. It enables transparent decryption and automated key management, supporting flexible licensing and secure model execution with minimal overhead. We implement a proof-of-concept library, benchmark its performance across serialization and runtime scenarios, and validate its compatibility with existing inference frameworks, including Hugging Face Transformers and vLLM. Our results highlight CryptoTensors as a light-weight, efficient, and developer-friendly solution for safeguarding LLM weights in real-world and widespread deployments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 100,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04639",
        "abs_url": "https://arxiv.org/abs/2512.04639",
        "pdf_url": "https://arxiv.org/pdf/2512.04639",
        "title": "When GenAI Meets Fake News: Understanding Image Cascade Dynamics on Reddit",
        "authors": [
            "Saumya Chauhan",
            "Mila Hong",
            "Maria Vazhaeparambil"
        ],
        "comments": "Accepted at 2025 MIT Undergraduate Research Technology Conference (URTC'25)",
        "subjects": "Social and Information Networks (cs.SI); Artificial Intelligence (cs.AI)",
        "abstract": "AI-generated content and misinformation are increasingly prevalent on social networks. While prior research primarily examined textual misinformation, fewer studies have focused on visual content's role in virality. In this work, we present the first large-scale analysis of how misinformation and AI-generated images propagate through repost cascades across five ideologically diverse Reddit communities. By integrating textual sentiment, visual attributes, and diffusion metrics (e.g., time-to-first repost, community reach), our framework accurately predicts both immediate post-level virality (AUC=0.83) and long-term cascade-level spread (AUC=0.998). These findings offer essential insights for moderating synthetic and misleading visual content online.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 101,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04653",
        "abs_url": "https://arxiv.org/abs/2512.04653",
        "pdf_url": "https://arxiv.org/pdf/2512.04653",
        "title": "Semi Centralized Training Decentralized Execution Architecture for Multi Agent Deep Reinforcement Learning in Traffic Signal Control",
        "authors": [
            "Pouria Yazdani",
            "Arash Rezaali",
            "Monireh Abdoos"
        ],
        "comments": "Co-first authors: Pouria Yazdani and Arash Rezaali",
        "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Multi-agent reinforcement learning (MARL) has emerged as a promising paradigm for adaptive traffic signal control (ATSC) of multiple intersections. Existing approaches typically follow either a fully centralized or a fully decentralized design. Fully centralized approaches suffer from the curse of dimensionality, and reliance on a single learning server, whereas purely decentralized approaches operate under severe partial observability and lack explicit coordination resulting in suboptimal performance. These limitations motivate region-based MARL, where the network is partitioned into smaller, tightly coupled intersections that form regions, and training is organized around these regions. This paper introduces a Semi-Centralized Training, Decentralized Execution (SEMI-CTDE) architecture for multi intersection ATSC. Within each region, SEMI-CTDE performs centralized training with regional parameter sharing and employs composite state and reward formulations that jointly encode local and regional information. The architecture is highly transferable across different policy backbones and state-reward instantiations. Building on this architecture, we implement two models with distinct design objectives. A multi-perspective experimental analysis of the two implemented SEMI-CTDE-based models covering ablations of the architecture's core elements including rule based and fully decentralized baselines shows that they achieve consistently superior performance and remain effective across a wide range of traffic densities and distributions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 102,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04668",
        "abs_url": "https://arxiv.org/abs/2512.04668",
        "pdf_url": "https://arxiv.org/pdf/2512.04668",
        "title": "Topology Matters: Measuring Memory Leakage in Multi-Agent LLMs",
        "authors": [
            "Jinbo Liu",
            "Defu Cao",
            "Yifei Wei",
            "Tianyao Su",
            "Yuan Liang",
            "Yushun Dong",
            "Yue Zhao",
            "Xiyang Hu"
        ],
        "comments": "Under review at ACL Rolling Review",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Graph topology is a fundamental determinant of memory leakage in multi-agent LLM systems, yet its effects remain poorly quantified. We introduce MAMA (Multi-Agent Memory Attack), a framework that measures how network structure shapes leakage. MAMA operates on synthetic documents containing labeled Personally Identifiable Information (PII) entities, from which we generate sanitized task instructions. We execute a two-phase protocol: Engram (seeding private information into a target agent's memory) and Resonance (multi-round interaction where an attacker attempts extraction). Over up to 10 interaction rounds, we quantify leakage as the fraction of ground-truth PII recovered from attacking agent outputs via exact matching. We systematically evaluate six common network topologies (fully connected, ring, chain, binary tree, star, and star-ring), varying agent counts $n\\in\\{4,5,6\\}$, attacker-target placements, and base models. Our findings reveal consistent patterns: fully connected graphs exhibit maximum leakage while chains provide strongest protection; shorter attacker-target graph distance and higher target centrality significantly increase vulnerability; leakage rises sharply in early rounds before plateauing; model choice shifts absolute leakage rates but preserves topology rankings; temporal/locational PII attributes leak more readily than identity credentials or regulated identifiers. These results provide the first systematic mapping from architectural choices to measurable privacy risk, yielding actionable guidance: prefer sparse or hierarchical connectivity, maximize attacker-target separation, limit node degree and network radius, avoid shortcuts bypassing hubs, and implement topology-aware access controls.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 103,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04680",
        "abs_url": "https://arxiv.org/abs/2512.04680",
        "pdf_url": "https://arxiv.org/pdf/2512.04680",
        "title": "Generative AI for Self-Adaptive Systems: State of the Art and Research Roadmap",
        "authors": [
            "Jialong Li",
            "Mingyue Zhang",
            "Nianyu Li",
            "Danny Weyns",
            "Zhi Jin",
            "Kenji Tei"
        ],
        "comments": "Accepted by ACM Transactions on Autonomous and Adaptive Systems",
        "subjects": "Software Engineering (cs.SE); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)",
        "abstract": "Self-adaptive systems (SASs) are designed to handle changes and uncertainties through a feedback loop with four core functionalities: monitoring, analyzing, planning, and execution. Recently, generative artificial intelligence (GenAI), especially the area of large language models, has shown impressive performance in data comprehension and logical reasoning. These capabilities are highly aligned with the functionalities required in SASs, suggesting a strong potential to employ GenAI to enhance SASs. However, the specific benefits and challenges of employing GenAI in SASs remain unclear. Yet, providing a comprehensive understanding of these benefits and challenges is complex due to several reasons: limited publications in the SAS field, the technological and application diversity within SASs, and the rapid evolution of GenAI technologies. To that end, this paper aims to provide researchers and practitioners a comprehensive snapshot that outlines the potential benefits and challenges of employing GenAI's within SAS. Specifically, we gather, filter, and analyze literature from four distinct research fields and organize them into two main categories to potential benefits: (i) enhancements to the autonomy of SASs centered around the specific functions of the MAPE-K feedback loop, and (ii) improvements in the interaction between humans and SASs within human-on-the-loop settings. From our study, we outline a research roadmap that highlights the challenges of integrating GenAI into SASs. The roadmap starts with outlining key research challenges that need to be tackled to exploit the potential for applying GenAI in the field of SAS. The roadmap concludes with a practical reflection, elaborating on current shortcomings of GenAI and proposing possible mitigation strategies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 104,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04694",
        "abs_url": "https://arxiv.org/abs/2512.04694",
        "pdf_url": "https://arxiv.org/pdf/2512.04694",
        "title": "TimesNet-Gen: Deep Learning-based Site Specific Strong Motion Generation",
        "authors": [
            "Baris Yilmaz",
            "Bevan Deniz Cilgin",
            "Erdem AkagÃ¼ndÃ¼z",
            "Salih Tileylioglu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Effective earthquake risk reduction relies on accurate site-specific evaluations. This requires models that can represent the influence of local site conditions on ground motion characteristics. In this context, data driven approaches that learn site controlled signatures from recorded ground motions offer a promising direction. We address strong ground motion generation from time-domain accelerometer records and introduce the TimesNet-Gen, a time-domain conditional generator. The approach uses a station specific latent bottleneck. We evaluate generation by comparing HVSR curves and fundamental site-frequency $f_0$ distributions between real and generated records per station, and summarize station specificity with a score based on the $f_0$ distribution confusion matrices. TimesNet-Gen achieves strong station-wise alignment and compares favorably with a spectrogram-based conditional VAE baseline for site-specific strong motion synthesis. Our codes are available via this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 105,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04711",
        "abs_url": "https://arxiv.org/abs/2512.04711",
        "pdf_url": "https://arxiv.org/pdf/2512.04711",
        "title": "Large Speech Model Enabled Semantic Communication",
        "authors": [
            "Yun Tian",
            "Zhijin Qin",
            "Guocheng Lv",
            "Ye Jin",
            "Kaibin Huang",
            "Zhu Han"
        ],
        "comments": "15 pages, 9 figures",
        "subjects": "Sound (cs.SD); Artificial Intelligence (cs.AI)",
        "abstract": "Existing speech semantic communication systems mainly based on Joint Source-Channel Coding (JSCC) architectures have demonstrated impressive performance, but their effectiveness remains limited by model structures specifically designed for particular tasks and datasets. Recent advances indicate that generative large models pre-trained on massive datasets, can achieve outstanding performance arexhibit exceptional performance across diverse downstream tasks with minimal fine-tuning. To exploit the rich semantic knowledge embedded in large models and enable adaptive transmission over lossy channels, we propose a Large Speech Model enabled Semantic Communication (LargeSC) system. Simultaneously achieving adaptive compression and robust transmission over lossy channels remains challenging, requiring trade-offs among compression efficiency, speech quality, and latency. In this work, we employ the Mimi as a speech codec, converting speech into discrete tokens compatible with existing network architectures. We propose an adaptive controller module that enables adaptive transmission and in-band Unequal Error Protection (UEP), dynamically adjusting to both speech content and packet loss probability under bandwidth constraints. Additionally, we employ Low-Rank Adaptation (LoRA) to finetune the Moshi foundation model for generative recovery of lost speech tokens. Simulation results show that the proposed system supports bandwidths ranging from 550 bps to 2.06 kbps, outperforms conventional baselines in speech quality under high packet loss rates and achieves an end-to-end latency of approximately 460 ms, thereby demonstrating its potential for real-time deployment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 106,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04716",
        "abs_url": "https://arxiv.org/abs/2512.04716",
        "pdf_url": "https://arxiv.org/pdf/2512.04716",
        "title": "Towards an AI Fluid Scientist: LLM-Powered Scientific Discovery in Experimental Fluid Mechanics",
        "authors": [
            "Haodong Feng",
            "Lugang Ye",
            "Dixia Fan"
        ],
        "comments": "",
        "subjects": "Fluid Dynamics (physics.flu-dyn); Artificial Intelligence (cs.AI)",
        "abstract": "The integration of artificial intelligence into experimental fluid mechanics promises to accelerate discovery, yet most AI applications remain narrowly focused on numerical studies. This work proposes an AI Fluid Scientist framework that autonomously executes the complete experimental workflow: hypothesis generation, experimental design, robotic execution, data analysis, and manuscript preparation. We validate this through investigation of vortex-induced vibration (VIV) and wake-induced vibration (WIV) in tandem cylinders. Our work has four key contributions: (1) A computer-controlled circulating water tunnel (CWT) with programmatic control of flow velocity, cylinder position, and forcing parameters (vibration frequency and amplitude) with data acquisition (displacement, force, and torque). (2) Automated experiments reproduce literature benchmarks (Khalak and Williamson [1999] and Assi et al. [2013, 2010]) with frequency lock-in within 4% and matching critical spacing trends. (3) The framework with Human-in-the-Loop (HIL) discovers more WIV amplitude response phenomena, and uses a neural network to fit physical laws from data, which is 31% higher than that of polynomial fitting. (4) The framework with multi-agent with virtual-real interaction system executes hundreds of experiments end-to-end, which automatically completes the entire process of scientific research from hypothesis generation, experimental design, experimental execution, data analysis, and manuscript preparation. It greatly liberates human researchers and improves study efficiency, providing new paradigm for the development and research of experimental fluid mechanics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 107,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04733",
        "abs_url": "https://arxiv.org/abs/2512.04733",
        "pdf_url": "https://arxiv.org/pdf/2512.04733",
        "title": "E3AD: An Emotion-Aware Vision-Language-Action Model for Human-Centric End-to-End Autonomous Driving",
        "authors": [
            "Yihong Tang",
            "Haicheng Liao",
            "Tong Nie",
            "Junlin He",
            "Ao Qu",
            "Kehua Chen",
            "Wei Ma",
            "Zhenning Li",
            "Lijun Sun",
            "Chengzhong Xu"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "End-to-end autonomous driving (AD) systems increasingly adopt vision-language-action (VLA) models, yet they typically ignore the passenger's emotional state, which is central to comfort and AD acceptance. We introduce Open-Domain End-to-End (OD-E2E) autonomous driving, where an autonomous vehicle (AV) must interpret free-form natural-language commands, infer the emotion, and plan a physically feasible trajectory. We propose E3AD, an emotion-aware VLA framework that augments semantic understanding with two cognitively inspired components: a continuous Valenc-Arousal-Dominance (VAD) emotion model that captures tone and urgency from language, and a dual-pathway spatial reasoning module that fuses egocentric and allocentric views for human-like spatial cognition. A consistency-oriented training scheme, combining modality pretraining with preference-based alignment, further enforces coherence between emotional intent and driving actions. Across real-world datasets, E3AD improves visual grounding and waypoint planning and achieves state-of-the-art (SOTA) VAD correlation for emotion estimation. These results show that injecting emotion into VLA-style driving yields more human-aligned grounding, planning, and human-centric feedback.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 108,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04738",
        "abs_url": "https://arxiv.org/abs/2512.04738",
        "pdf_url": "https://arxiv.org/pdf/2512.04738",
        "title": "OsmT: Bridging OpenStreetMap Queries and Natural Language with Open-source Tag-aware Language Models",
        "authors": [
            "Zhuoyue Wan",
            "Wentao Hu",
            "Chen Jason Zhang",
            "Yuanfeng Song",
            "Shuaimin Li",
            "Ruiqiang Xiao",
            "Xiao-Yong Wei",
            "Raymond Chi-Wing Wong"
        ],
        "comments": "42nd IEEE International Conference on Data Engineering (ICDE)",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Databases (cs.DB)",
        "abstract": "Bridging natural language and structured query languages is a long-standing challenge in the database community. While recent advances in language models have shown promise in this direction, existing solutions often rely on large-scale closed-source models that suffer from high inference costs, limited transparency, and lack of adaptability for lightweight deployment. In this paper, we present OsmT, an open-source tag-aware language model specifically designed to bridge natural language and Overpass Query Language (OverpassQL), a structured query language for accessing large-scale OpenStreetMap (OSM) data. To enhance the accuracy and structural validity of generated queries, we introduce a Tag Retrieval Augmentation (TRA) mechanism that incorporates contextually relevant tag knowledge into the generation process. This mechanism is designed to capture the hierarchical and relational dependencies present in the OSM database, addressing the topological complexity inherent in geospatial query formulation. In addition, we define a reverse task, OverpassQL-to-Text, which translates structured queries into natural language explanations to support query interpretation and improve user accessibility. We evaluate OsmT on a public benchmark against strong baselines and observe consistent improvements in both query generation and interpretation. Despite using significantly fewer parameters, our model achieves competitive accuracy, demonstrating the effectiveness of open-source pre-trained language models in bridging natural language and structured query languages within schema-rich geospatial environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 109,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04745",
        "abs_url": "https://arxiv.org/abs/2512.04745",
        "pdf_url": "https://arxiv.org/pdf/2512.04745",
        "title": "Neural Policy Composition from Free Energy Minimization",
        "authors": [
            "Francesca Rossi",
            "Veronica Centorrino",
            "Francesco Bullo",
            "Giovanni Russo"
        ],
        "comments": "",
        "subjects": "Optimization and Control (math.OC); Artificial Intelligence (cs.AI); Systems and Control (eess.SY); Adaptation and Self-Organizing Systems (nlin.AO)",
        "abstract": "The ability to compose acquired skills to plan and execute behaviors is a hallmark of natural intelligence. Yet, despite remarkable cross-disciplinary efforts, a principled account of how task structure shapes gating and how such computations could be delivered in neural circuits, remains elusive. Here we introduce GateMod, an interpretable theoretically grounded computational model linking the emergence of gating to the underlying decision-making task, and to a neural circuit architecture. We first develop GateFrame, a normative framework casting policy gating into the minimization of the free energy. This framework, relating gating rules to task, applies broadly across neuroscience, cognitive and computational sciences. We then derive GateFlow, a continuous-time energy based dynamics that provably converges to GateFrame optimal solution. Convergence, exponential and global, follows from a contractivity property that also yields robustness and other desirable properties. Finally, we derive a neural circuit from GateFlow, GateNet. This is a soft-competitive recurrent circuit whose components perform local and contextual computations consistent with known dendritic and neural processing motifs. We evaluate GateMod across two different settings: collective behaviors in multi-agent systems and human decision-making in multi-armed bandits. In all settings, GateMod provides interpretable mechanistic explanations of gating and quantitatively matches or outperforms established models. GateMod offers a unifying framework for neural policy gating, linking task objectives, dynamical computation, and circuit-level mechanisms. It provides a framework to understand gating in natural agents beyond current explanations and to equip machines with this ability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 110,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04746",
        "abs_url": "https://arxiv.org/abs/2512.04746",
        "pdf_url": "https://arxiv.org/pdf/2512.04746",
        "title": "SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs",
        "authors": [
            "Wenhua Cheng",
            "Weiwei Zhang",
            "Heng Guo",
            "Haihao Shen"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Extreme low-bit quantization is critical for efficiently deploying Large Language Models (LLMs), yet it often leads to severe performance degradation at 2-bits and even 4-bits (e.g., MXFP4). We present SignRoundV2, a post-training quantization framework that is highly effective even without mixed-precision. SignRoundV2 introduces (1) a fast sensitivity metric that combines gradient information with quantization-induced deviations to guide layer-wise bit allocation, and (2) a lightweight pre-tuning search for quantization scales to improve extremely low-bit quantization. These components allow SignRoundV2 to close the gap with full-precision models. Extensive experiments indicate that our method sustains competitive accuracy for LLMs, achieving production-grade performance with about 1 percent variance at 4-5 bits and strong results even at 2 bits. The implementation is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 111,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04749",
        "abs_url": "https://arxiv.org/abs/2512.04749",
        "pdf_url": "https://arxiv.org/pdf/2512.04749",
        "title": "UnwrapDiff: Conditional Diffusion for Robust InSAR Phase Unwrapping",
        "authors": [
            "Yijia Song",
            "Juliet Biggs",
            "Alin Achim",
            "Robert Popescu",
            "Simon Orrego",
            "Nantheera Anantrasirichai"
        ],
        "comments": "",
        "subjects": "Geophysics (physics.geo-ph); Artificial Intelligence (cs.AI)",
        "abstract": "Phase unwrapping is a fundamental problem in InSAR data processing, supporting geophysical applications such as deformation monitoring and hazard assessment. Its reliability is limited by noise and decorrelation in radar acquisitions, which makes accurate reconstruction of the deformation signal challenging. We propose a denoising diffusion probabilistic model (DDPM)-based framework for InSAR phase unwrapping, UnwrapDiff, in which the output of the traditional minimum cost flow algorithm (SNAPHU) is incorporated as conditional guidance. To evaluate robustness, we construct a synthetic dataset that incorporates atmospheric effects and diverse noise patterns, representative of realistic InSAR observations. Experiments show that the proposed model leverages the conditional prior while reducing the effect of diverse noise patterns, achieving on average a 10.11\\% reduction in NRMSE compared to SNAPHU. It also achieves better reconstruction quality in difficult cases such as dyke intrusions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 112,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04770",
        "abs_url": "https://arxiv.org/abs/2512.04770",
        "pdf_url": "https://arxiv.org/pdf/2512.04770",
        "title": "Embodied Co-Design for Rapidly Evolving Agents: Taxonomy, Frontiers, and Challenges",
        "authors": [
            "Yuxing Wang",
            "Zhiyu Chen",
            "Tiantian Zhang",
            "Qiyue Yin",
            "Yongzhe Chang",
            "Zhiheng Li",
            "Liang Wang",
            "Xueqian Wang"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI); Emerging Technologies (cs.ET); Systems and Control (eess.SY)",
        "abstract": "Brain-body co-evolution enables animals to develop complex behaviors in their environments. Inspired by this biological synergy, embodied co-design (ECD) has emerged as a transformative paradigm for creating intelligent agents-from virtual creatures to physical robots-by jointly optimizing their morphologies and controllers rather than treating control in isolation. This integrated approach facilitates richer environmental interactions and robust task performance. In this survey, we provide a systematic overview of recent advances in ECD. We first formalize the concept of ECD and position it within related fields. We then introduce a hierarchical taxonomy: a lower layer that breaks down agent design into three fundamental components-controlling brain, body morphology, and task environment-and an upper layer that integrates these components into four major ECD frameworks: bi-level, single-level, generative, and open-ended. This taxonomy allows us to synthesize insights from more than one hundred recent studies. We further review notable benchmarks, datasets, and applications in both simulated and real-world scenarios. Finally, we identify significant challenges and offer insights into promising future research directions. A project associated with this survey has been created at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 113,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04773",
        "abs_url": "https://arxiv.org/abs/2512.04773",
        "pdf_url": "https://arxiv.org/pdf/2512.04773",
        "title": "Using Machine Learning to Take Stay-or-Go Decisions in Data-driven Drone Missions",
        "authors": [
            "Giorgos Polychronis",
            "Foivos Pournaropoulos",
            "Christos D. Antonopoulos",
            "Spyros Lalis"
        ],
        "comments": "19 pages, 3 figures, to appear in the proceedings of MobiQuitous 2025",
        "subjects": "Robotics (cs.RO); Artificial Intelligence (cs.AI)",
        "abstract": "Drones are becoming indispensable in many application domains. In data-driven missions, besides sensing, the drone must process the collected data at runtime to decide whether additional action must be taken on the spot, before moving to the next point of interest. If processing does not reveal an event or situation that requires such an action, the drone has waited in vain instead of moving to the next point. If, however, the drone starts moving to the next point and it turns out that a follow-up action is needed at the previous point, it must spend time to fly-back. To take this decision, we propose different machine-learning methods based on branch prediction and reinforcement learning. We evaluate these methods for a wide range of scenarios where the probability of event occurrence changes with time. Our results show that the proposed methods consistently outperform the regression-based method proposed in the literature and can significantly improve the worst-case mission time by up to 4.1x. Also, the achieved median mission time is very close, merely up to 2.7% higher, to that of a method with perfect knowledge of the current underlying event probability at each point of interest.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 114,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04779",
        "abs_url": "https://arxiv.org/abs/2512.04779",
        "pdf_url": "https://arxiv.org/pdf/2512.04779",
        "title": "YingMusic-Singer: Zero-shot Singing Voice Synthesis and Editing with Annotation-free Melody Guidance",
        "authors": [
            "Junjie Zheng",
            "Chunbo Hao",
            "Guobin Ma",
            "Xiaoyu Zhang",
            "Gongyu Chen",
            "Chaofan Ding",
            "Zihao Chen",
            "Lei Xie"
        ],
        "comments": "13 pages, 3 figures",
        "subjects": "Sound (cs.SD); Artificial Intelligence (cs.AI)",
        "abstract": "Singing Voice Synthesis (SVS) remains constrained in practical deployment due to its strong dependence on accurate phoneme-level alignment and manually annotated melody contours, requirements that are resource-intensive and hinder scalability. To overcome these limitations, we propose a melody-driven SVS framework capable of synthesizing arbitrary lyrics following any reference melody, without relying on phoneme-level alignment. Our method builds on a Diffusion Transformer (DiT) architecture, enhanced with a dedicated melody extraction module that derives melody representations directly from reference audio. To ensure robust melody encoding, we employ a teacher model to guide the optimization of the melody extractor, alongside an implicit alignment mechanism that enforces similarity distribution constraints for improved melodic stability and coherence. Additionally, we refine duration modeling using weakly annotated song data and introduce a Flow-GRPO reinforcement learning strategy with a multi-objective reward function to jointly enhance pronunciation clarity and melodic fidelity. Experiments show that our model achieves superior performance over existing approaches in both objective measures and subjective listening tests, especially in zero-shot and lyric adaptation settings, while maintaining high audio quality without manual annotation. This work offers a practical and scalable solution for advancing data-efficient singing voice synthesis. To support reproducibility, we release our inference code and model checkpoints.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 115,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04793",
        "abs_url": "https://arxiv.org/abs/2512.04793",
        "pdf_url": "https://arxiv.org/pdf/2512.04793",
        "title": "YingMusic-SVC: Real-World Robust Zero-Shot Singing Voice Conversion with Flow-GRPO and Singing-Specific Inductive Biases",
        "authors": [
            "Gongyu Chen",
            "Xiaoyu Zhang",
            "Zhenqiang Weng",
            "Junjie Zheng",
            "Da Shen",
            "Chaofan Ding",
            "Wei-Qiang Zhang",
            "Zihao Chen"
        ],
        "comments": "17 pages, 5 figures",
        "subjects": "Sound (cs.SD); Artificial Intelligence (cs.AI)",
        "abstract": "Singing voice conversion (SVC) aims to render the target singer's timbre while preserving melody and lyrics. However, existing zero-shot SVC systems remain fragile in real songs due to harmony interference, F0 errors, and the lack of inductive biases for singing. We propose YingMusic-SVC, a robust zero-shot framework that unifies continuous pre-training, robust supervised fine-tuning, and Flow-GRPO reinforcement learning. Our model introduces a singing-trained RVC timbre shifter for timbre-content disentanglement, an F0-aware timbre adaptor for dynamic vocal expression, and an energy-balanced rectified flow matching loss to enhance high-frequency fidelity. Experiments on a graded multi-track benchmark show that YingMusic-SVC achieves consistent improvements over strong open-source baselines in timbre similarity, intelligibility, and perceptual naturalness, especially under accompanied and harmony-contaminated conditions, demonstrating its effectiveness for real-world SVC deployment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 116,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04803",
        "abs_url": "https://arxiv.org/abs/2512.04803",
        "pdf_url": "https://arxiv.org/pdf/2512.04803",
        "title": "287,872 Supermassive Black Holes Masses: Deep Learning Approaching Reverberation Mapping Accuracy",
        "authors": [
            "Yuhao Lu",
            "HengJian SiTu",
            "Jie Li",
            "Yixuan Li",
            "Yang Liu",
            "Wenbin Lin",
            "Yu Wang"
        ],
        "comments": "14 pages, 9 figures. Submitted to Journal of High Energy Astrophysics",
        "subjects": "Astrophysics of Galaxies (astro-ph.GA); High Energy Astrophysical Phenomena (astro-ph.HE); Instrumentation and Methods for Astrophysics (astro-ph.IM); Artificial Intelligence (cs.AI)",
        "abstract": "We present a population-scale catalogue of 287,872 supermassive black hole masses with high accuracy. Using a deep encoder-decoder network trained on optical spectra with reverberation-mapping (RM) based labels of 849 quasars and applied to all SDSS quasars up to $z=4$, our method achieves a root-mean-square error of $0.058$\\,dex, a relative uncertainty of $\\approx 14\\%$, and coefficient of determination $R^{2}\\approx0.91$ with respect to RM-based masses, far surpassing traditional single-line virial estimators. Notably, the high accuracy is maintained for both low ($<10^{7.5}\\,M_\\odot$) and high ($>10^{9}\\,M_\\odot$) mass quasars, where empirical relations are unreliable.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 117,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04808",
        "abs_url": "https://arxiv.org/abs/2512.04808",
        "pdf_url": "https://arxiv.org/pdf/2512.04808",
        "title": "Setting up for failure: automatic discovery of the neural mechanisms of cognitive errors",
        "authors": [
            "Puria Radmard",
            "Paul M. Bays",
            "MÃ¡tÃ© Lengyel"
        ],
        "comments": "",
        "subjects": "Neurons and Cognition (q-bio.NC); Artificial Intelligence (cs.AI)",
        "abstract": "Discovering the neural mechanisms underpinning cognition is one of the grand challenges of neuroscience. However, previous approaches for building models of RNN dynamics that explain behaviour required iterative refinement of architectures and/or optimisation objectives, resulting in a piecemeal, and mostly heuristic, human-in-the-loop process. Here, we offer an alternative approach that automates the discovery of viable RNN mechanisms by explicitly training RNNs to reproduce behaviour, including the same characteristic errors and suboptimalities, that humans and animals produce in a cognitive task. Achieving this required two main innovations. First, as the amount of behavioural data that can be collected in experiments is often too limited to train RNNs, we use a non-parametric generative model of behavioural responses to produce surrogate data for training RNNs. Second, to capture all relevant statistical aspects of the data, we developed a novel diffusion model-based approach for training RNNs. To showcase the potential of our approach, we chose a visual working memory task as our test-bed, as behaviour in this task is well known to produce response distributions that are patently multimodal (due to swap errors). The resulting network dynamics correctly qualitative features of macaque neural data. Importantly, these results were not possible to obtain with more traditional approaches, i.e., when only a limited set of behavioural signatures (rather than the full richness of behavioural response distributions) were fitted, or when RNNs were trained for task optimality (instead of reproducing behaviour). Our approach also yields novel predictions about the mechanism of swap errors, which can be readily tested in experiments. These results suggest that fitting RNNs to rich patterns of behaviour provides a powerful way to automatically discover mechanisms of important cognitive functions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 118,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04841",
        "abs_url": "https://arxiv.org/abs/2512.04841",
        "pdf_url": "https://arxiv.org/pdf/2512.04841",
        "title": "SoK: a Comprehensive Causality Analysis Framework for Large Language Model Security",
        "authors": [
            "Wei Zhao",
            "Zhe Li",
            "Jun Sun"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI)",
        "abstract": "Large Language Models (LLMs) exhibit remarkable capabilities but remain vulnerable to adversarial manipulations such as jailbreaking, where crafted prompts bypass safety mechanisms. Understanding the causal factors behind such vulnerabilities is essential for building reliable defenses. In this work, we introduce a unified causality analysis framework that systematically supports all levels of causal investigation in LLMs, ranging from token-level, neuron-level, and layer-level interventions to representation-level analysis. The framework enables consistent experimentation and comparison across diverse causality-based attack and defense methods. Accompanying this implementation, we provide the first comprehensive survey of causality-driven jailbreak studies and empirically evaluate the framework on multiple open-weight models and safety-critical benchmarks including jailbreaks, hallucination detection, backdoor identification, and fairness evaluation. Our results reveal that: (1) targeted interventions on causally critical components can reliably modify safety behavior; (2) safety-related mechanisms are highly localized (i.e., concentrated in early-to-middle layers with only 1--2\\% of neurons exhibiting causal influence); and (3) causal features extracted from our framework achieve over 95\\% detection accuracy across multiple threat types. By bridging theoretical causality analysis and practical model safety, our framework establishes a reproducible foundation for research on causality-based attacks, interpretability, and robust attack detection and mitigation in LLMs. Code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 119,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04843",
        "abs_url": "https://arxiv.org/abs/2512.04843",
        "pdf_url": "https://arxiv.org/pdf/2512.04843",
        "title": "From Symptoms to Systems: An Expert-Guided Approach to Understanding Risks of Generative AI for Eating Disorders",
        "authors": [
            "Amy Winecoff",
            "Kevin Klyman"
        ],
        "comments": "",
        "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)",
        "abstract": "Generative AI systems may pose serious risks to individuals vulnerable to eating disorders. Existing safeguards tend to overlook subtle but clinically significant cues, leaving many risks unaddressed. To better understand the nature of these risks, we conducted semi-structured interviews with 15 clinicians, researchers, and advocates with expertise in eating disorders. Using abductive qualitative analysis, we developed an expert-guided taxonomy of generative AI risks across seven categories: (1) providing generalized health advice; (2) encouraging disordered behaviors; (3) supporting symptom concealment; (4) creating thinspiration; (5) reinforcing negative self-beliefs; (6) promoting excessive focus on the body; and (7) perpetuating narrow views about eating disorders. Our results demonstrate how certain user interactions with generative AI systems intersect with clinical features of eating disorders in ways that may intensify risk. We discuss implications of our work, including approaches for risk assessment, safeguard design, and participatory evaluation practices with domain experts.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 120,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04844",
        "abs_url": "https://arxiv.org/abs/2512.04844",
        "pdf_url": "https://arxiv.org/pdf/2512.04844",
        "title": "Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates",
        "authors": [
            "Atsuki Yamaguchi",
            "Terufumi Morishita",
            "Aline Villavicencio",
            "Nikolaos Aletras"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Expanding the linguistic diversity of instruct large language models (LLMs) is crucial for global accessibility but is often hindered by the reliance on costly specialized target language labeled data and catastrophic forgetting during adaptation. We tackle this challenge under a realistic, low-resource constraint: adapting instruct LLMs using only unlabeled target language data. We introduce Source-Shielded Updates (SSU), a selective parameter update strategy that proactively preserves source knowledge. Using a small set of source data and a parameter importance scoring method, SSU identifies parameters critical to maintaining source abilities. It then applies a column-wise freezing strategy to protect these parameters before adaptation. Experiments across five typologically diverse languages and 7B and 13B models demonstrate that SSU successfully mitigates catastrophic forgetting. It reduces performance degradation on monolingual source tasks to just 3.4% (7B) and 2.8% (13B) on average, a stark contrast to the 20.3% and 22.3% from full fine-tuning. SSU also achieves target-language performance highly competitive with full fine-tuning, outperforming it on all benchmarks for 7B models and the majority for 13B models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 121,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04847",
        "abs_url": "https://arxiv.org/abs/2512.04847",
        "pdf_url": "https://arxiv.org/pdf/2512.04847",
        "title": "Language Models as Semantic Teachers: Post-Training Alignment for Medical Audio Understanding",
        "authors": [
            "Tsai-Ning Wang",
            "Lin-Lin Chen",
            "Neil Zeghidour",
            "Aaqib Saeed"
        ],
        "comments": "",
        "subjects": "Sound (cs.SD); Artificial Intelligence (cs.AI)",
        "abstract": "Pre-trained audio models excel at detecting acoustic patterns in auscultation sounds but often fail to grasp their clinical significance, limiting their use and performance in diagnostic tasks. To bridge this gap, we introduce AcuLa (Audio-Clinical Understanding via Language Alignment), a lightweight post-training framework that instills semantic understanding into any audio encoder by aligning it with a medical language model, which acts as a \"semantic teacher.\" To enable alignment at scale, we construct a large-scale dataset by leveraging off-the-shelf large language models to translate the rich, structured metadata accompanying existing audio recordings into coherent clinical reports. Our alignment strategy combines a representation-level contrastive objective with a self-supervised modeling, ensuring that the model learns clinical semantics while preserving fine-grained temporal cues. AcuLa achieves state-of-the-art results across 18 diverse cardio-respiratory tasks from 10 different datasets, improving the mean AUROC on classification benchmarks from 0.68 to 0.79 and, on the most challenging COVID-19 cough detection task, boosting the AUROC from 0.55 to 0.89. Our work demonstrates that this audio-language alignment transforms purely acoustic models into clinically-aware diagnostic tools, establishing a novel paradigm for enhancing physiological understanding in audio-based health monitoring.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 122,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04868",
        "abs_url": "https://arxiv.org/abs/2512.04868",
        "pdf_url": "https://arxiv.org/pdf/2512.04868",
        "title": "SEAL: Self-Evolving Agentic Learning for Conversational Question Answering over Knowledge Graphs",
        "authors": [
            "Hao Wang",
            "Jialun Zhong",
            "Changcheng Wang",
            "Zhujun Nie",
            "Zheng Li",
            "Shunyu Yao",
            "Yanzeng Li",
            "Xinchi Li"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Knowledge-based conversational question answering (KBCQA) confronts persistent challenges in resolving coreference, modeling contextual dependencies, and executing complex logical reasoning. Existing approaches, whether end-to-end semantic parsing or stepwise agent-based reasoning, often suffer from structural inaccuracies and prohibitive computational costs, particularly when processing intricate queries over large knowledge graphs. To address these limitations, we introduce SEAL, a novel two-stage semantic parsing framework grounded in self-evolving agentic learning. In the first stage, a large language model (LLM) extracts a minimal S-expression core that captures the essential semantics of the input query. This core is then refined by an agentic calibration module, which corrects syntactic inconsistencies and aligns entities and relations precisely with the underlying knowledge graph. The second stage employs template-based completion, guided by question-type prediction and placeholder instantiation, to construct a fully executable S-expression. This decomposition not only simplifies logical form generation but also significantly enhances structural fidelity and linking efficiency. Crucially, SEAL incorporates a self-evolving mechanism that integrates local and global memory with a reflection module, enabling continuous adaptation from dialog history and execution feedback without explicit retraining. Extensive experiments on the SPICE benchmark demonstrate that SEAL achieves state-of-the-art performance, especially in multi-hop reasoning, comparison, and aggregation tasks. The results validate notable gains in both structural accuracy and computational efficiency, underscoring the framework's capacity for robust and scalable conversational reasoning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 123,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04869",
        "abs_url": "https://arxiv.org/abs/2512.04869",
        "pdf_url": "https://arxiv.org/pdf/2512.04869",
        "title": "Developing a General Personal Tutor for Education",
        "authors": [
            "Jaan Aru",
            "Kristjan-Julius Laak"
        ],
        "comments": "",
        "subjects": "Computers and Society (cs.CY); Artificial Intelligence (cs.AI); Neurons and Cognition (q-bio.NC)",
        "abstract": "The vision of a universal AI tutor has remained elusive, despite decades of effort. Could LLMs be the game-changer? We overview novel issues arising from developing a nationwide AI tutor. We highlight the practical questions that point to specific gaps in our scientific understanding of the learning process.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 124,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04910",
        "abs_url": "https://arxiv.org/abs/2512.04910",
        "pdf_url": "https://arxiv.org/pdf/2512.04910",
        "title": "Declarative Synthesis and Multi-Objective Optimization of Stripboard Circuit Layouts Using Answer Set Programming",
        "authors": [
            "Fang Li"
        ],
        "comments": "Accepted by the 43rd IEEE International Conference on Computer Design (ICCD 2025)",
        "subjects": "Hardware Architecture (cs.AR); Artificial Intelligence (cs.AI)",
        "abstract": "This paper presents a novel approach to automated stripboard circuit layout design using Answer Set Programming (ASP). The work formulates the layout problem as both a synthesis and multi-objective optimization task that simultaneously generates viable layouts while minimizing board area and component strip crossing. By leveraging ASP's declarative nature, this work expresses complex geometric and electrical constraints in a natural and concise manner. The two-phase solving methodology first ensures feasibility before optimizing layout quality. Experimental results demonstrate that this approach generates compact, manufacturable layouts for a range of circuit complexities. This work represents a significant advancement in automated stripboard layout, offering a practical tool for electronics prototyping and education while showcasing the power of declarative programming for solving complex design automation problems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 125,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04949",
        "abs_url": "https://arxiv.org/abs/2512.04949",
        "pdf_url": "https://arxiv.org/pdf/2512.04949",
        "title": "CARL: Critical Action Focused Reinforcement Learning for Multi-Step Agent",
        "authors": [
            "Leyang Shen",
            "Yang Zhang",
            "Chun Kai Ling",
            "Xiaoyan Zhao",
            "Tat-Seng Chua"
        ],
        "comments": "10 pages, 4 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Agents capable of accomplishing complex tasks through multiple interactions with the environment have emerged as a popular research direction. However, in such multi-step settings, the conventional group-level policy optimization algorithm becomes suboptimal because of its underlying assumption that each action holds equal contribution, which deviates significantly from reality. Our analysis reveals that only a small fraction of actions are critical in determining the final outcome. Building on this insight, we propose CARL, a critical-action-focused reinforcement learning algorithm tailored for multi-step agents. CARL achieves focused training through providing action-level optimization signals for high-criticality actions while excluding low-criticality actions from model update. Extensive experiments demonstrate that CARL achieves both stronger performance and higher efficiency during training and inference across diverse evaluation settings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 126,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04957",
        "abs_url": "https://arxiv.org/abs/2512.04957",
        "pdf_url": "https://arxiv.org/pdf/2512.04957",
        "title": "LLMs Know More Than Words: A Genre Study with Syntax, Metaphor & Phonetics",
        "authors": [
            "Weiye Shi",
            "Zhaowei Zhang",
            "Shaoheng Yan",
            "Yaodong Yang"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI)",
        "abstract": "Large language models (LLMs) demonstrate remarkable potential across diverse language related tasks, yet whether they capture deeper linguistic properties, such as syntactic structure, phonetic cues, and metrical patterns from raw text remains unclear. To analysis whether LLMs can learn these features effectively and apply them to important nature language related tasks, we introduce a novel multilingual genre classification dataset derived from Project Gutenberg, a large-scale digital library offering free access to thousands of public domain literary works, comprising thousands of sentences per binary task (poetry vs. novel;drama vs. poetry;drama vs. novel) in six languages (English, French, German, Italian, Spanish, and Portuguese). We augment each with three explicit linguistic feature sets (syntactic tree structures, metaphor counts, and phonetic metrics) to evaluate their impact on classification performance. Experiments demonstrate that although LLM classifiers can learn latent linguistic structures either from raw text or from explicitly provided features, different features contribute unevenly across tasks, which underscores the importance of incorporating more complex linguistic signals during model training.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 127,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04958",
        "abs_url": "https://arxiv.org/abs/2512.04958",
        "pdf_url": "https://arxiv.org/pdf/2512.04958",
        "title": "Realizable Abstractions: Near-Optimal Hierarchical Reinforcement Learning",
        "authors": [
            "Roberto Cipollone",
            "Luca Iocchi",
            "Matteo Leonetti"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "The main focus of Hierarchical Reinforcement Learning (HRL) is studying how large Markov Decision Processes (MDPs) can be more efficiently solved when addressed in a modular way, by combining partial solutions computed for smaller subtasks. Despite their very intuitive role for learning, most notions of MDP abstractions proposed in the HRL literature have limited expressive power or do not possess formal efficiency guarantees. This work addresses these fundamental issues by defining Realizable Abstractions, a new relation between generic low-level MDPs and their associated high-level decision processes. The notion we propose avoids non-Markovianity issues and has desirable near-optimality guarantees. Indeed, we show that any abstract policy for Realizable Abstractions can be translated into near-optimal policies for the low-level MDP, through a suitable composition of options. As demonstrated in the paper, these options can be expressed as solutions of specific constrained MDPs. Based on these findings, we propose RARL, a new HRL algorithm that returns compositional and near-optimal low-level policies, taking advantage of the Realizable Abstraction given in the input. We show that RARL is Probably Approximately Correct, it converges in a polynomial number of samples, and it is robust to inaccuracies in the abstraction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 128,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04967",
        "abs_url": "https://arxiv.org/abs/2512.04967",
        "pdf_url": "https://arxiv.org/pdf/2512.04967",
        "title": "Balanced Few-Shot Episodic Learning for Accurate Retinal Disease Diagnosis",
        "authors": [
            "Jasmaine Khale",
            "Ravi Prakash Srivastava"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)",
        "abstract": "Automated retinal disease diagnosis is vital given the rising prevalence of conditions such as diabetic retinopathy and macular degeneration. Conventional deep learning approaches require large annotated datasets, which are costly and often imbalanced across disease categories, limiting their reliability in practice. Few-shot learning (FSL) addresses this challenge by enabling models to generalize from only a few labeled samples per class. In this study,we propose a balanced few-shot episodic learning framework tailored to the Retinal Fundus Multi-Disease Image Dataset (RFMiD). Focusing on the ten most represented classes, which still show substantial imbalance between majority diseases (e.g., Diabetic Retinopathy, Macular Hole) and minority ones (e.g., Optic Disc Edema, Branch Retinal Vein Occlusion), our method integrates three key components: (i) balanced episodic sampling, ensuring equal participation of all classes in each 5-way 5-shot episode; (ii) targeted augmentation, including Contrast Limited Adaptive Histogram Equalization (CLAHE) and color/geometry transformations, to improve minority-class di- versity; and (iii) a ResNet-50 encoder pretrained on ImageNet, selected for its superior ability to capture fine-grained retinal features. Prototypes are computed in the embedding space and classification is performed with cosine similarity for improved stability. Trained on 100 episodes and evaluated on 1,000 test episodes, our framework achieves substantial accuracy gains and reduces bias toward majority classes, with notable improvements for underrepresented diseases. These results demonstrate that dataset-aware few-shot pipelines, combined with balanced sampling and CLAHE-enhanced preprocessing, can deliver more robust and clinically fair retinal disease diagnosis under data-constrained conditions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 129,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04988",
        "abs_url": "https://arxiv.org/abs/2512.04988",
        "pdf_url": "https://arxiv.org/pdf/2512.04988",
        "title": "Strategic Self-Improvement for Competitive Agents in AI Labour Markets",
        "authors": [
            "Christopher Chiu",
            "Simpson Zhang",
            "Mihaela van der Schaar"
        ],
        "comments": "",
        "subjects": "Multiagent Systems (cs.MA); Artificial Intelligence (cs.AI)",
        "abstract": "As artificial intelligence (AI) agents are deployed across economic domains, understanding their strategic behavior and market-level impact becomes critical. This paper puts forward a groundbreaking new framework that is the first to capture the real-world economic forces that shape agentic labor markets: adverse selection, moral hazard, and reputation dynamics. Our framework encapsulates three core capabilities that successful LLM-agents will need: \\textbf{metacognition} (accurate self-assessment of skills), \\textbf{competitive awareness} (modeling rivals and market dynamics), and \\textbf{long-horizon strategic planning}. We illustrate our framework through a tractable simulated gig economy where agentic Large Language Models (LLMs) compete for jobs, develop skills, and adapt their strategies under competitive pressure. Our simulations illustrate how LLM agents explicitly prompted with reasoning capabilities learn to strategically self-improve and demonstrate superior adaptability to changing market conditions. At the market level, our simulations reproduce classic macroeconomic phenomena found in human labor markets, while controlled experiments reveal potential AI-driven economic trends, such as rapid monopolization and systemic price deflation. This work provides a foundation to further explore the economic properties of AI-driven labour markets, and a conceptual framework to study the strategic reasoning capabilities in agents competing in the emerging economy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 130,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.04992",
        "abs_url": "https://arxiv.org/abs/2512.04992",
        "pdf_url": "https://arxiv.org/pdf/2512.04992",
        "title": "Evolutionary Architecture Search through Grammar-Based Sequence Alignment",
        "authors": [
            "Adri GÃ³mez MartÃ­n",
            "Felix MÃ¶ller",
            "Steven McDonagh",
            "Monica Abella",
            "Manuel Desco",
            "Elliot J. Crowley",
            "Aaron Klein",
            "Linus Ericsson"
        ],
        "comments": "",
        "subjects": "Neural and Evolutionary Computing (cs.NE); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Neural architecture search (NAS) in expressive search spaces is a computationally hard problem, but it also holds the potential to automatically discover completely novel and performant architectures. To achieve this we need effective search algorithms that can identify powerful components and reuse them in new candidate architectures. In this paper, we introduce two adapted variants of the Smith-Waterman algorithm for local sequence alignment and use them to compute the edit distance in a grammar-based evolutionary architecture search. These algorithms enable us to efficiently calculate a distance metric for neural architectures and to generate a set of hybrid offspring from two parent models. This facilitates the deployment of crossover-based search heuristics, allows us to perform a thorough analysis on the architectural loss landscape, and track population diversity during search. We highlight how our method vastly improves computational complexity over previous work and enables us to efficiently compute shortest paths between architectures. When instantiating the crossover in evolutionary searches, we achieve competitive results, outperforming competing methods. Future work can build upon this new tool, discovering novel components that can be used more broadly across neural architecture design, and broadening its applications beyond NAS.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 131,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.05024",
        "abs_url": "https://arxiv.org/abs/2512.05024",
        "pdf_url": "https://arxiv.org/pdf/2512.05024",
        "title": "Model-Free Assessment of Simulator Fidelity via Quantile Curves",
        "authors": [
            "Garud Iyengar",
            "Yu-Shiou Willy Lin",
            "Kaizheng Wang"
        ],
        "comments": "33 pages, 11 figures",
        "subjects": "Methodology (stat.ME); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Simulation of complex systems originated in manufacturing and queuing applications. It is now widely used for large-scale, ML-based systems in research, education, and consumer surveys. However, characterizing the discrepancy between simulators and ground truth remains challenging for increasingly complex, machine-learning-based systems. We propose a computationally tractable method to estimate the quantile function of the discrepancy between the simulated and ground-truth outcome distributions. Our approach focuses on output uncertainty and treats the simulator as a black box, imposing no modeling assumptions on its internals, and hence applies broadly across many parameter families, from Bernoulli and multinomial models to continuous, vector-valued settings. The resulting quantile curve supports confidence interval construction for unseen scenarios, risk-aware summaries of sim-to-real discrepancy (e.g., VaR/CVaR), and comparison of simulators' performance. We demonstrate our methodology in an application assessing LLM simulation fidelity on the WorldValueBench dataset spanning four LLMs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 132,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.05033",
        "abs_url": "https://arxiv.org/abs/2512.05033",
        "pdf_url": "https://arxiv.org/pdf/2512.05033",
        "title": "Arbitrage: Efficient Reasoning via Advantage-Aware Speculation",
        "authors": [
            "Monishwaran Maheswaran",
            "Rishabh Tiwari",
            "Yuezhou Hu",
            "Kerem Dilmen",
            "Coleman Hooper",
            "Haocheng Xi",
            "Nicholas Lee",
            "Mehrdad Farajtabar",
            "Michael W. Mahoney",
            "Kurt Keutzer",
            "Amir Gholami"
        ],
        "comments": "22 pages",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Modern Large Language Models achieve impressive reasoning capabilities with long Chain of Thoughts, but they incur substantial computational cost during inference, and this motivates techniques to improve the performance-cost ratio. Among these techniques, Speculative Decoding accelerates inference by employing a fast but inaccurate draft model to autoregressively propose tokens, which are then verified in parallel by a more capable target model. However, due to unnecessary rejections caused by token mismatches in semantically equivalent steps, traditional token-level Speculative Decoding struggles in reasoning tasks. Although recent works have shifted to step-level semantic verification, which improve efficiency by accepting or rejecting entire reasoning steps, existing step-level methods still regenerate many rejected steps with little improvement, wasting valuable target compute. To address this challenge, we propose Arbitrage, a novel step-level speculative generation framework that routes generation dynamically based on the relative advantage between draft and target models. Instead of applying a fixed acceptance threshold, Arbitrage uses a lightweight router trained to predict when the target model is likely to produce a meaningfully better step. This routing approximates an ideal Arbitrage Oracle that always chooses the higher-quality step, achieving near-optimal efficiency-accuracy trade-offs. Across multiple mathematical reasoning benchmarks, Arbitrage consistently surpasses prior step-level Speculative Decoding baselines, reducing inference latency by up to $\\sim2\\times$ at matched accuracy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 133,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.05049",
        "abs_url": "https://arxiv.org/abs/2512.05049",
        "pdf_url": "https://arxiv.org/pdf/2512.05049",
        "title": "QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory",
        "authors": [
            "Yu-Chao Hsu",
            "Jiun-Cheng Jiang",
            "Chun-Hua Lin",
            "Kuo-Chung Peng",
            "Nan-Yow Chen",
            "Samuel Yen-Chi Chen",
            "En-Jui Kuo",
            "Hsi-Sheng Goan"
        ],
        "comments": "",
        "subjects": "Quantum Physics (quant-ph); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Long short-term memory (LSTM) models are a particular type of recurrent neural networks (RNNs) that are central to sequential modeling tasks in domains such as urban telecommunication forecasting, where temporal correlations and nonlinear dependencies dominate. However, conventional LSTMs suffer from high parameter redundancy and limited nonlinear expressivity. In this work, we propose the Quantum-inspired Kolmogorov-Arnold Long Short-Term Memory (QKAN-LSTM), which integrates Data Re-Uploading Activation (DARUAN) modules into the gating structure of LSTMs. Each DARUAN acts as a quantum variational activation function (QVAF), enhancing frequency adaptability and enabling an exponentially enriched spectral representation without multi-qubit entanglement. The resulting architecture preserves quantum-level expressivity while remaining fully executable on classical hardware. Empirical evaluations on three datasets, Damped Simple Harmonic Motion, Bessel Function, and Urban Telecommunication, demonstrate that QKAN-LSTM achieves superior predictive accuracy and generalization with a 79% reduction in trainable parameters compared to classical LSTMs. We extend the framework to the Jiang-Huang-Chen-Goan Network (JHCG Net), which generalizes KAN to encoder-decoder structures, and then further use QKAN to realize the latent KAN, thereby creating a Hybrid QKAN (HQKAN) for hierarchical representation learning. The proposed HQKAN-LSTM thus provides a scalable and interpretable pathway toward quantum-inspired sequential modeling in real-world data environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 134,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.05058",
        "abs_url": "https://arxiv.org/abs/2512.05058",
        "pdf_url": "https://arxiv.org/pdf/2512.05058",
        "title": "Meta-Learning for Quantum Optimization via Quantum Sequence Model",
        "authors": [
            "Yu-Cheng Lin",
            "Yu-Chao Hsu",
            "Samuel Yen-Chi Chen"
        ],
        "comments": "",
        "subjects": "Quantum Physics (quant-ph); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "The Quantum Approximate Optimization Algorithm (QAOA) is a leading approach for solving combinatorial optimization problems on near-term quantum processors. However, finding good variational parameters remains a significant challenge due to the non-convex energy landscape, often resulting in slow convergence and poor solution quality. In this work, we propose a quantum meta-learning framework that trains advanced quantum sequence models to generate effective parameter initialization policies. We investigate four classical or quantum sequence models, including the Quantum Kernel-based Long Short-Term Memory (QK-LSTM), as learned optimizers in a \"learning to learn\" paradigm. Our numerical experiments on the Max-Cut problem demonstrate that the QK-LSTM optimizer achieves superior performance, obtaining the highest approximation ratios and exhibiting the fastest convergence rate across all tested problem sizes (n=10 to 13). Crucially, the QK-LSTM model achieves perfect parameter transferability by synthesizing a single, fixed set of near-optimal parameters, leading to a remarkable sustained acceleration of convergence even when generalizing to larger problems. This capability, enabled by the compact and expressive power of the quantum kernel architecture, underscores its effectiveness. The QK-LSTM, with only 43 trainable parameters, substantially outperforms the classical LSTM (56 parameters) and other quantum sequence models, establishing a robust pathway toward highly efficient parameter initialization for variational quantum algorithms in the NISQ era.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 135,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.05066",
        "abs_url": "https://arxiv.org/abs/2512.05066",
        "pdf_url": "https://arxiv.org/pdf/2512.05066",
        "title": "Multi-LLM Collaboration for Medication Recommendation",
        "authors": [
            "Huascar Sanchez",
            "Briland Hitaj",
            "Jules Bergmann",
            "Linda Briesemeister"
        ],
        "comments": "8 pages, 5 figures, 1 table",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "As healthcare increasingly turns to AI for scalable and trustworthy clinical decision support, ensuring reliability in model reasoning remains a critical challenge. Individual large language models (LLMs) are susceptible to hallucinations and inconsistency, whereas naive ensembles of models often fail to deliver stable and credible recommendations. Building on our previous work on LLM Chemistry, which quantifies the collaborative compatibility among LLMs, we apply this framework to improve the reliability in medication recommendation from brief clinical vignettes. Our approach leverages multi-LLM collaboration guided by Chemistry-inspired interaction modeling, enabling ensembles that are effective (exploiting complementary strengths), stable (producing consistent quality), and calibrated (minimizing interference and error amplification). We evaluate our Chemistry-based Multi-LLM collaboration strategy on real-world clinical scenarios to investigate whether such interaction-aware ensembles can generate credible, patient-specific medication recommendations. Preliminary results are encouraging, suggesting that LLM Chemistry-guided collaboration may offer a promising path toward reliable and trustworthy AI assistants in clinical practice.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 136,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.05073",
        "abs_url": "https://arxiv.org/abs/2512.05073",
        "pdf_url": "https://arxiv.org/pdf/2512.05073",
        "title": "David vs. Goliath: Can Small Models Win Big with Agentic AI in Hardware Design?",
        "authors": [
            "Shashwat Shankar",
            "Subhranshu Pandey",
            "Innocent Dengkhw Mochahari",
            "Bhabesh Mali",
            "Animesh Basak Chowdhury",
            "Sukanta Bhattacharjee",
            "Chandan Karfa"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Hardware Architecture (cs.AR); Software Engineering (cs.SE)",
        "abstract": "Large Language Model(LLM) inference demands massive compute and energy, making domain-specific tasks expensive and unsustainable. As foundation models keep scaling, we ask: Is bigger always better for hardware design? Our work tests this by evaluating Small Language Models coupled with a curated agentic AI framework on NVIDIA's Comprehensive Verilog Design Problems(CVDP) benchmark. Results show that agentic workflows: through task decomposition, iterative feedback, and correction - not only unlock near-LLM performance at a fraction of the cost but also create learning opportunities for agents, paving the way for efficient, adaptive solutions in complex design tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 137,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.05100",
        "abs_url": "https://arxiv.org/abs/2512.05100",
        "pdf_url": "https://arxiv.org/pdf/2512.05100",
        "title": "Structured Document Translation via Format Reinforcement Learning",
        "authors": [
            "Haiyue Song",
            "Johannes Eschbach-Dymanus",
            "Hour Kaing",
            "Sumire Honda",
            "Hideki Tanaka",
            "Bianka Buschbeck",
            "Masao Utiyama"
        ],
        "comments": "IJCNLP-AACL 2025 Main (Oral)",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Recent works on structured text translation remain limited to the sentence level, as they struggle to effectively handle the complex document-level XML or HTML structures. To address this, we propose \\textbf{Format Reinforcement Learning (FormatRL)}, which employs Group Relative Policy Optimization on top of a supervised fine-tuning model to directly optimize novel structure-aware rewards: 1) TreeSim, which measures structural similarity between predicted and reference XML trees and 2) Node-chrF, which measures translation quality at the level of XML nodes. Additionally, we apply StrucAUC, a fine-grained metric distinguishing between minor errors and major structural failures. Experiments on the SAP software-documentation benchmark demonstrate improvements across six metrics and an analysis further shows how different reward functions contribute to improvements in both structural and translation quality.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 138,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.05105",
        "abs_url": "https://arxiv.org/abs/2512.05105",
        "pdf_url": "https://arxiv.org/pdf/2512.05105",
        "title": "Semantic Soft Bootstrapping: Long Context Reasoning in LLMs without Reinforcement Learning",
        "authors": [
            "Purbesh Mitra",
            "Sennur Ulukus"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Information Theory (cs.IT); Machine Learning (cs.LG); Signal Processing (eess.SP)",
        "abstract": "Long context reasoning in large language models (LLMs) has demonstrated enhancement of their cognitive capabilities via chain-of-thought (CoT) inference. Training such models is usually done via reinforcement learning with verifiable rewards (RLVR) in reasoning based problems, like math and programming. However, RLVR is limited by several bottlenecks, such as, lack of dense reward, and inadequate sample efficiency. As a result, it requires significant compute resources in post-training phase. To overcome these limitations, in this work, we propose \\textbf{Semantic Soft Bootstrapping (SSB)}, a self-distillation technique, in which the same base language model plays the role of both teacher and student, but receives different semantic contexts about the correctness of its outcome at training time. The model is first prompted with a math problem and several rollouts are generated. From them, the correct and most common incorrect response are filtered, and then provided to the model in context to produce a more robust, step-by-step explanation with a verified final answer. This pipeline automatically curates a paired teacher-student training set from raw problem-answer data, without any human intervention. This generation process also produces a sequence of logits, which is what the student model tries to match in the training phase just from the bare question alone. In our experiment, Qwen2.5-3B-Instruct on GSM8K dataset via parameter-efficient fine-tuning. We then tested its accuracy on MATH500, and AIME2024 benchmarks. Our experiments show a jump of 10.6%, and 10% improvements in accuracy, respectively, over group relative policy optimization (GRPO), which is a commonly used RLVR algorithm. Our code is available at this https URL, and the model, curated dataset is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 139,
        "date": "2025-12-05",
        "date_url": "https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True",
        "arxiv_id": "2512.05112",
        "abs_url": "https://arxiv.org/abs/2512.05112",
        "pdf_url": "https://arxiv.org/pdf/2512.05112",
        "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation",
        "authors": [
            "Dongzhi Jiang",
            "Renrui Zhang",
            "Haodong Li",
            "Zhuofan Zong",
            "Ziyu Guo",
            "Jun He",
            "Claire Guo",
            "Junyan Ye",
            "Rongyao Fang",
            "Weijia Li",
            "Rui Liu",
            "Hongsheng Li"
        ],
        "comments": "Project Page: this https URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Recent unified multimodal large language models (MLLMs) have shown impressive capabilities, incorporating chain-of-thought (CoT) reasoning for enhanced text-to-image generation. However, existing approaches remain limited, either treating the model merely as a standalone generator or relying on abstract textual planning. To this end, we propose Draft-as-CoT (DraCo), a novel interleaved reasoning paradigm that fully leverages both textual and visual contents in CoT for better planning and verification. Our method first generates a low-resolution draft image as preview, providing more concrete and structural visual planning and guidance. Then, we employ the model's inherent understanding capability to verify potential semantic misalignments between the draft and input prompt, and performs refinement through selective corrections with super-resolution. In this way, our approach addresses two fundamental challenges: the coarse-grained nature of textual planning and the difficulty in generating rare attribute combinations. To support training, we curate DraCo-240K, aiming to enhance three atomic capabilities spanning general correction, instance manipulation, and layout reorganization. Supported by DraCo-CFG, a specialized classifier-free guidance (CFG) strategy for interleaved reasoning, DraCo achieves a tremendous increase on GenEval (+8%), Imagine-Bench (+0.91), and GenEval++ (+3%), significantly outperforming direct generation and other generation methods empowered by CoT.",
        "gemini2.5flash": "",
        "overall_idea": ""
    }
]