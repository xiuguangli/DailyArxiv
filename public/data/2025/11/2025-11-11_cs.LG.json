[
    {
        "order": 1,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05552",
        "abs_url": "https://arxiv.org/abs/2511.05552",
        "pdf_url": "https://arxiv.org/pdf/2511.05552",
        "title": "Deep one-gate per layer networks with skip connections are universal classifiers",
        "authors": [
            "Raul Rojas"
        ],
        "comments": "5 pages, 6 figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "This paper shows how a multilayer perceptron with two hidden layers, which has been designed to classify two classes of data points, can easily be transformed into a deep neural network with one-gate layers and skip connections.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 2,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05556",
        "abs_url": "https://arxiv.org/abs/2511.05556",
        "pdf_url": "https://arxiv.org/pdf/2511.05556",
        "title": "Daily Forecasting for Annual Time Series Datasets Using Similarity-Based Machine Learning Methods: A Case Study in the Energy Market",
        "authors": [
            "Mahdi Goldani"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); General Economics (econ.GN)",
        "abstract": "The policy environment of countries changes rapidly, influencing macro-level indicators such as the Energy Security Index. However, this index is only reported annually, limiting its responsiveness to short-term fluctuations. To address this gap, the present study introduces a daily proxy for the Energy Security Index and applies it to forecast energy security at a daily this http URL study employs a two stage approach first, a suitable daily proxy for the annual Energy Security Index is identified by applying six time series similarity measures to key energy related variables. Second, the selected proxy is modeled using the XGBoost algorithm to generate 15 day ahead forecasts, enabling high frequency monitoring of energy security this http URL the result of proxy choosing, Volume Brent consistently emerged as the most suitable proxy across the majority of methods. The model demonstrated strong performance, with an R squared of 0.981 on the training set and 0.945 on the test set, and acceptable error metrics . The 15 day forecast of Brent volume indicates short term fluctuations, with a peak around day 4, a decline until day 8, a rise near day 10, and a downward trend toward day 15, accompanied by prediction this http URL integrating time series similarity measures with machine learning based forecasting, this study provides a novel framework for converting low frequency macroeconomic indicators into high frequency, actionable signals. The approach enables real time monitoring of the Energy Security Index, offering policymakers and analysts a scalable and practical tool to respond more rapidly to fast changing policy and market conditions, especially in data scarce environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 3,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05558",
        "abs_url": "https://arxiv.org/abs/2511.05558",
        "pdf_url": "https://arxiv.org/pdf/2511.05558",
        "title": "Diversified Flow Matching with Translation Identifiability",
        "authors": [
            "Sagar Shrestha",
            "Xiao Fu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Diversified distribution matching (DDM) finds a unified translation function mapping a diverse collection of conditional source distributions to their target counterparts. DDM was proposed to resolve content misalignment issues in unpaired domain translation, achieving translation identifiability. However, DDM has only been implemented using GANs due to its constraints on the translation function. GANs are often unstable to train and do not provide the transport trajectory information -- yet such trajectories are useful in applications such as single-cell evolution analysis and robot route planning. This work introduces diversified flow matching (DFM), an ODE-based framework for DDM. Adapting flow matching (FM) to enforce a unified translation function as in DDM is challenging, as FM learns the translation function's velocity rather than the translation function itself. A custom bilevel optimization-based training loss, a nonlinear interpolant, and a structural reformulation are proposed to address these challenges, offering a tangible implementation. To our knowledge, DFM is the first ODE-based approach guaranteeing translation identifiability. Experiments on synthetic and real-world datasets validate the proposed method.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 4,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05563",
        "abs_url": "https://arxiv.org/abs/2511.05563",
        "pdf_url": "https://arxiv.org/pdf/2511.05563",
        "title": "Lookahead Unmasking Elicits Accurate Decoding in Diffusion Language Models",
        "authors": [
            "Sanghyun Lee",
            "Seungryong Kim",
            "Jongho Park",
            "Dongmin Park"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Masked Diffusion Models (MDMs) as language models generate by iteratively unmasking tokens, yet their performance crucially depends on the inference time order of unmasking. Prevailing heuristics, such as confidence based sampling, are myopic: they optimize locally, fail to leverage extra test-time compute, and let early decoding mistakes cascade. We propose Lookahead Unmasking (LookUM), which addresses these concerns by reformulating sampling as path selection over all possible unmasking orders without the need for an external reward model. Our framework couples (i) a path generator that proposes paths by sampling from pools of unmasking sets with (ii) a verifier that computes the uncertainty of the proposed paths and performs importance sampling to subsequently select the final paths. Empirically, erroneous unmasking measurably inflates sequence level uncertainty, and our method exploits this to avoid error-prone trajectories. We validate our framework across six benchmarks, such as mathematics, planning, and coding, and demonstrate consistent performance improvements. LookUM requires only two to three paths to achieve peak performance, demonstrating remarkably efficient path selection. The consistent improvements on both LLaDA and post-trained LLaDA 1.5 are particularly striking: base LLaDA with LookUM rivals the performance of RL-tuned LLaDA 1.5, while LookUM further enhances LLaDA 1.5 itself showing that uncertainty based verification provides orthogonal benefits to reinforcement learning and underscoring the versatility of our framework. Code will be publicly released.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 5,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05569",
        "abs_url": "https://arxiv.org/abs/2511.05569",
        "pdf_url": "https://arxiv.org/pdf/2511.05569",
        "title": "Data-driven jet fuel demand forecasting: A case study of Copenhagen Airport",
        "authors": [
            "Alessandro Contini",
            "Davide Cacciarelli",
            "Murat Kulahci"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Accurate forecasting of jet fuel demand is crucial for optimizing supply chain operations in the aviation market. Fuel distributors specifically require precise estimates to avoid inventory shortages or excesses. However, there is a lack of studies that analyze the jet fuel demand forecasting problem using machine learning models. Instead, many industry practitioners rely on deterministic or expertise-based models. In this research, we evaluate the performance of data-driven approaches using a substantial amount of data obtained from a major aviation fuel distributor in the Danish market. Our analysis compares the predictive capabilities of traditional time series models, Prophet, LSTM sequence-to-sequence neural networks, and hybrid models. A key challenge in developing these models is the required forecasting horizon, as fuel demand needs to be predicted for the next 30 days to optimize sourcing strategies. To ensure the reliability of the data-driven approaches and provide valuable insights to practitioners, we analyze three different datasets. The primary objective of this study is to present a comprehensive case study on jet fuel demand forecasting, demonstrating the advantages of employing data-driven models and highlighting the impact of incorporating additional variables in the predictive models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 6,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05582",
        "abs_url": "https://arxiv.org/abs/2511.05582",
        "pdf_url": "https://arxiv.org/pdf/2511.05582",
        "title": "Distillation-Accelerated Uncertainty Modeling for Multi-Objective RTA Interception",
        "authors": [
            "Gaoxiang Zhao",
            "Ruina Qiu",
            "Pengpeng Zhao",
            "Rongjin Wang",
            "Zhangang Lin",
            "Xiaoqiang Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Science and Game Theory (cs.GT)",
        "abstract": "Real-Time Auction (RTA) Interception aims to filter out invalid or irrelevant traffic to enhance the integrity and reliability of downstream data. However, two key challenges remain: (i) the need for accurate estimation of traffic quality together with sufficiently high confidence in the model's predictions, typically addressed through uncertainty modeling, and (ii) the efficiency bottlenecks that such uncertainty modeling introduces in real-time applications due to repeated inference. To address these challenges, we propose DAUM, a joint modeling framework that integrates multi-objective learning with uncertainty modeling, yielding both traffic quality predictions and reliable confidence estimates. Building on DAUM, we further apply knowledge distillation to reduce the computational overhead of uncertainty modeling, while largely preserving predictive accuracy and retaining the benefits of uncertainty estimation. Experiments on the JD advertisement dataset demonstrate that DAUM consistently improves predictive performance, with the distilled model delivering a tenfold increase in inference speed.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 7,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05585",
        "abs_url": "https://arxiv.org/abs/2511.05585",
        "pdf_url": "https://arxiv.org/pdf/2511.05585",
        "title": "Depth-induced NTK: Bridging Over-parameterized Neural Networks and Deep Neural Kernels",
        "authors": [
            "Yong-Ming Tian",
            "Shuang Liang",
            "Shao-Qun Zhang",
            "Feng-Lei Fan"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "While deep learning has achieved remarkable success across a wide range of applications, its theoretical understanding of representation learning remains limited. Deep neural kernels provide a principled framework to interpret over-parameterized neural networks by mapping hierarchical feature transformations into kernel spaces, thereby combining the expressive power of deep architectures with the analytical tractability of kernel methods. Recent advances, particularly neural tangent kernels (NTKs) derived by gradient inner products, have established connections between infinitely wide neural networks and nonparametric Bayesian inference. However, the existing NTK paradigm has been predominantly confined to the infinite-width regime, while overlooking the representational role of network depth. To address this gap, we propose a depth-induced NTK kernel based on a shortcut-related architecture, which converges to a Gaussian process as the network depth approaches infinity. We theoretically analyze the training invariance and spectrum properties of the proposed kernel, which stabilizes the kernel dynamics and mitigates degeneration. Experimental results further underscore the effectiveness of our proposed method. Our findings significantly extend the existing landscape of the neural kernel theory and provide an in-depth understanding of deep learning and the scaling law.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 8,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05586",
        "abs_url": "https://arxiv.org/abs/2511.05586",
        "pdf_url": "https://arxiv.org/pdf/2511.05586",
        "title": "Prompting Neural-Guided Equation Discovery Based on Residuals",
        "authors": [
            "Jannis Brugger",
            "Viktor Pfanschilling",
            "David Richter",
            "Mira Mezini",
            "Stefan Kramer"
        ],
        "comments": "16 pages, 7 figures, Discovery Science 2025",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Neural-guided equation discovery systems use a data set as prompt and predict an equation that describes the data set without extensive search. However, if the equation does not meet the user's expectations, there are few options for getting other equation suggestions without intensive work with the system. To fill this gap, we propose Residuals for Equation Discovery (RED), a post-processing method that improves a given equation in a targeted manner, based on its residuals. By parsing the initial equation to a syntax tree, we can use node-based calculation rules to compute the residual for each subequation of the initial equation. It is then possible to use this residual as new target variable in the original data set and generate a new prompt. If, with the new prompt, the equation discovery system suggests a subequation better than the old subequation on a validation set, we replace the latter by the former. RED is usable with any equation discovery system, is fast to calculate, and is easy to extend for new mathematical operations. In experiments on 53 equations from the Feynman benchmark, we show that it not only helps to improve all tested neural-guided systems, but also all tested classical genetic programming systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 9,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05591",
        "abs_url": "https://arxiv.org/abs/2511.05591",
        "pdf_url": "https://arxiv.org/pdf/2511.05591",
        "title": "FedSparQ: Adaptive Sparse Quantization with Error Feedback for Robust & Efficient Federated Learning",
        "authors": [
            "Chaimaa Medjadji",
            "Sadi Alawadi",
            "Feras M. Awaysheh",
            "Guilain Leduc",
            "Sylvain Kubler",
            "Yves Le Traon"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Federated Learning (FL) enables collaborative model training across decentralized clients while preserving data privacy by keeping raw data local. However, FL suffers from significant communication overhead due to the frequent exchange of high-dimensional model updates over constrained networks. In this paper, we present FedSparQ, a lightweight compression framework that dynamically sparsifies the gradient of each client through an adaptive threshold, applies half-precision quanti- zation to retained entries and integrates residuals from error feedback to prevent loss of information. FedSparQ requires no manual tuning of sparsity rates or quantization schedules, adapts seamlessly to both homogeneous and heterogeneous data distributions, and is agnostic to model architecture. Through extensive empirical evaluation on vision benchmarks under independent and identically distributed (IID) and non-IID data, we show that FedSparQ substantially reduces communication overhead (reducing by 90% of bytes sent compared to FedAvg) while preserving or improving model accuracy (improving by 6% compared to FedAvg non-compressed solution or to state-of-the- art compression models) and enhancing convergence robustness (by 50%, compared to the other baselines). Our approach provides a practical, easy-to-deploy solution for bandwidth- constrained federated deployments and lays the groundwork for future extensions in adaptive precision and privacy-preserving protocols.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 10,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05592",
        "abs_url": "https://arxiv.org/abs/2511.05592",
        "pdf_url": "https://arxiv.org/pdf/2511.05592",
        "title": "GRAVER: Generative Graph Vocabularies for Robust Graph Foundation Models Fine-tuning",
        "authors": [
            "Haonan Yuan",
            "Qingyun Sun",
            "Junhua Shi",
            "Xingcheng Fu",
            "Bryan Hooi",
            "Jianxin Li",
            "Philip S. Yu"
        ],
        "comments": "Accepted by the NeurIPS 2025",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Inspired by the remarkable success of foundation models in language and vision, Graph Foundation Models (GFMs) hold significant promise for broad applicability across diverse graph tasks and domains. However, existing GFMs struggle with unstable few-shot fine-tuning, where both performance and adaptation efficiency exhibit significant fluctuations caused by the randomness in the support sample selection and structural discrepancies between the pre-trained and target graphs. How to fine-tune GFMs robustly and efficiently to enable trustworthy knowledge transfer across domains and tasks is the major challenge. In this paper, we propose GRAVER, a novel Generative gRAph VocabulariEs for Robust GFM fine-tuning framework that tackles the aforementioned instability via generative augmentations. Specifically, to identify transferable units, we analyze and extract key class-specific subgraph patterns by ego-graph disentanglement and validate their transferability both theoretically and empirically. To enable effective pre-training across diverse domains, we leverage a universal task template based on ego-graph similarity and construct graph vocabularies via graphon-based generative experts. To facilitate robust and efficient prompt fine-tuning, we grave the support samples with in-context vocabularies, where the lightweight MoE-CoE network attentively routes knowledge from source domains. Extensive experiments demonstrate the superiority of GRAVER over effectiveness, robustness, and efficiency on downstream few-shot node and graph classification tasks compared with 15 state-of-the-art baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 11,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05593",
        "abs_url": "https://arxiv.org/abs/2511.05593",
        "pdf_url": "https://arxiv.org/pdf/2511.05593",
        "title": "Gradient Projection onto Historical Descent Directions for Communication-Efficient Federated Learning",
        "authors": [
            "Arnaud Descours",
            "LÃ©onard Deroose",
            "Jan Ramon"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Optimization and Control (math.OC); Statistics Theory (math.ST)",
        "abstract": "Federated Learning (FL) enables decentralized model training across multiple clients while optionally preserving data privacy. However, communication efficiency remains a critical bottleneck, particularly for large-scale models. In this work, we introduce two complementary algorithms: ProjFL, designed for unbiased compressors, and ProjFL+EF, tailored for biased compressors through an Error Feedback mechanism. Both methods rely on projecting local gradients onto a shared client-server subspace spanned by historical descent directions, enabling efficient information exchange with minimal communication overhead. We establish convergence guarantees for both algorithms under strongly convex, convex, and non-convex settings. Empirical evaluations on standard FL classification benchmarks with deep neural networks show that ProjFL and ProjFL+EF achieve accuracy comparable to existing baselines while substantially reducing communication costs.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 12,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05594",
        "abs_url": "https://arxiv.org/abs/2511.05594",
        "pdf_url": "https://arxiv.org/pdf/2511.05594",
        "title": "Optimizing Predictive Maintenance in Intelligent Manufacturing: An Integrated FNO-DAE-GNN-PPO MDP Framework",
        "authors": [
            "Shiqing Qiu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computational Engineering, Finance, and Science (cs.CE)",
        "abstract": "In the era of smart manufacturing, predictive maintenance (PdM) plays a pivotal role in improving equipment reliability and reducing operating costs. In this paper, we propose a novel Markov Decision Process (MDP) framework that integrates advanced soft computing techniques - Fourier Neural Operator (FNO), Denoising Autoencoder (DAE), Graph Neural Network (GNN), and Proximal Policy Optimisation (PPO) - to address the multidimensional challenges of predictive maintenance in complex manufacturing systems. Specifically, the proposed framework innovatively combines the powerful frequency-domain representation capability of FNOs to capture high-dimensional temporal patterns; DAEs to achieve robust, noise-resistant latent state embedding from complex non-Gaussian sensor data; and GNNs to accurately represent inter-device dependencies for coordinated system-wide maintenance decisions. Furthermore, by exploiting PPO, the framework ensures stable and efficient optimisation of long-term maintenance strategies to effectively handle uncertainty and non-stationary dynamics. Experimental validation demonstrates that the approach significantly outperforms multiple deep learning baseline models with up to 13% cost reduction, as well as strong convergence and inter-module synergy. The framework has considerable industrial potential to effectively reduce downtime and operating expenses through data-driven strategies.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 13,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05596",
        "abs_url": "https://arxiv.org/abs/2511.05596",
        "pdf_url": "https://arxiv.org/pdf/2511.05596",
        "title": "AutoHood3D: A Multi-Modal Benchmark for Automotive Hood Design and Fluid-Structure Interaction",
        "authors": [
            "Vansh Sharma",
            "Harish Jai Ganesh",
            "Maryam Akram",
            "Wanjiao Liu",
            "Venkat Raman"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computational Physics (physics.comp-ph); Fluid Dynamics (physics.flu-dyn)",
        "abstract": "This study presents a new high-fidelity multi-modal dataset containing 16000+ geometric variants of automotive hoods useful for machine learning (ML) applications such as engineering component design and process optimization, and multiphysics system surrogates. The dataset is centered on a practical multiphysics problem-hood deformation from fluid entrapment and inertial loading during rotary-dip painting. Each hood is numerically modeled with a coupled Large-Eddy Simulation (LES)-Finite Element Analysis (FEA), using 1.2M cells in total to ensure spatial and temporal accuracy. The dataset provides time-resolved physical fields, along with STL meshes and structured natural language prompts for text-to-geometry synthesis. Existing datasets are either confined to 2D cases, exhibit limited geometric variations, or lack the multi-modal annotations and data structures - shortcomings we address with AutoHood3D. We validate our numerical methodology, establish quantitative baselines across five neural architectures, and demonstrate systematic surrogate errors in displacement and force predictions. These findings motivate the design of novel approaches and multiphysics loss functions that enforce fluid-solid coupling during model training. By providing fully reproducible workflows, AutoHood3D enables physics-aware ML development, accelerates generative-design iteration, and facilitates the creation of new FSI benchmarks. Dataset and code URLs in Appendix.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 14,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05605",
        "abs_url": "https://arxiv.org/abs/2511.05605",
        "pdf_url": "https://arxiv.org/pdf/2511.05605",
        "title": "FiCABU: A Fisher-Based, Context-Adaptive Machine Unlearning Processor for Edge AI",
        "authors": [
            "Eun-Su Cho",
            "Jongin Choi",
            "Jeongmin Jin",
            "Jae-Jin Lee",
            "Woojoo Lee"
        ],
        "comments": "8 pages, 6 figures, 4 tables, DATE 2026 accepted paper",
        "subjects": "Machine Learning (cs.LG); Hardware Architecture (cs.AR)",
        "abstract": "Machine unlearning, driven by privacy regulations and the \"right to be forgotten\", is increasingly needed at the edge, yet server-centric or retraining-heavy methods are impractical under tight computation and energy budgets. We present FiCABU (Fisher-based Context-Adaptive Balanced Unlearning), a software-hardware co-design that brings unlearning to edge AI processors. FiCABU combines (i) Context-Adaptive Unlearning, which begins edits from back-end layers and halts once the target forgetting is reached, with (ii) Balanced Dampening, which scales dampening strength by depth to preserve retain accuracy. These methods are realized in a full RTL design of a RISC-V edge AI processor that integrates two lightweight IPs for Fisher estimation and dampening into a GEMM-centric streaming pipeline, validated on an FPGA prototype and synthesized in 45 nm for power analysis. Across CIFAR-20 and PinsFaceRecognition with ResNet-18 and ViT, FiCABU achieves random-guess forget accuracy while matching the retraining-free Selective Synaptic Dampening (SSD) baseline on retain accuracy, reducing computation by up to 87.52 percent (ResNet-18) and 71.03 percent (ViT). On the INT8 hardware prototype, FiCABU further improves retain preservation and reduces energy to 6.48 percent (CIFAR-20) and 0.13 percent (PinsFaceRecognition) of the SSD baseline. In sum, FiCABU demonstrates that back-end-first, depth-aware unlearning can be made both practical and efficient for resource-constrained edge AI devices.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 15,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05614",
        "abs_url": "https://arxiv.org/abs/2511.05614",
        "pdf_url": "https://arxiv.org/pdf/2511.05614",
        "title": "An MLCommons Scientific Benchmarks Ontology",
        "authors": [
            "Ben Hawks",
            "Gregor von Laszewski",
            "Matthew D. Sinclair",
            "Marco Colombo",
            "Shivaram Venkataraman",
            "Rutwik Jain",
            "Yiwei Jiang",
            "Nhan Tran",
            "Geoffrey Fox"
        ],
        "comments": "16 Pages, 3 Figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Performance (cs.PF); Computational Physics (physics.comp-ph)",
        "abstract": "Scientific machine learning research spans diverse domains and data modalities, yet existing benchmark efforts remain siloed and lack standardization. This makes novel and transformative applications of machine learning to critical scientific use-cases more fragmented and less clear in pathways to impact. This paper introduces an ontology for scientific benchmarking developed through a unified, community-driven effort that extends the MLCommons ecosystem to cover physics, chemistry, materials science, biology, climate science, and more. Building on prior initiatives such as XAI-BENCH, FastML Science Benchmarks, PDEBench, and the SciMLBench framework, our effort consolidates a large set of disparate benchmarks and frameworks into a single taxonomy of scientific, application, and system-level benchmarks. New benchmarks can be added through an open submission workflow coordinated by the MLCommons Science Working Group and evaluated against a six-category rating rubric that promotes and identifies high-quality benchmarks, enabling stakeholders to select benchmarks that meet their specific needs. The architecture is extensible, supporting future scientific and AI/ML motifs, and we discuss methods for identifying emerging computing patterns for unique scientific workloads. The MLCommons Science Benchmarks Ontology provides a standardized, scalable foundation for reproducible, cross-domain benchmarking in scientific machine learning. A companion webpage for this work has also been developed as the effort evolves: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 16,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05619",
        "abs_url": "https://arxiv.org/abs/2511.05619",
        "pdf_url": "https://arxiv.org/pdf/2511.05619",
        "title": "Frequency Matters: When Time Series Foundation Models Fail Under Spectral Shift",
        "authors": [
            "Tianze Wang",
            "Sofiane Ennadir",
            "John Pertoft",
            "Gabriela Zarzar Gandler",
            "Lele Cao",
            "Zineb Senane",
            "Styliani Katsarou",
            "Sahar Asadi",
            "Axel Karlsson",
            "Oleg Smirnov"
        ],
        "comments": "Accepted and presented at NeurIPS 2025 Workshop on Recent Advances in Time Series Foundation Models (BERT2S)",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Time series foundation models (TSFMs) have shown strong results on public benchmarks, prompting comparisons to a \"BERT moment\" for time series. Their effectiveness in industrial settings, however, remains uncertain. We examine why TSFMs often struggle to generalize and highlight spectral shift (a mismatch between the dominant frequency components in downstream tasks and those represented during pretraining) as a key factor. We present evidence from an industrial-scale player engagement prediction task in mobile gaming, where TSFMs underperform domain-adapted baselines. To isolate the mechanism, we design controlled synthetic experiments contrasting signals with seen versus unseen frequency bands, observing systematic degradation under spectral mismatch. These findings position frequency awareness as critical for robust TSFM deployment and motivate new pretraining and evaluation protocols that explicitly account for spectral diversity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 17,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05620",
        "abs_url": "https://arxiv.org/abs/2511.05620",
        "pdf_url": "https://arxiv.org/pdf/2511.05620",
        "title": "Fooling Algorithms in Non-Stationary Bandits using Belief Inertia",
        "authors": [
            "Gal Mendelson",
            "Eyal Tadmor"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Probability (math.PR); Machine Learning (stat.ML)",
        "abstract": "We study the problem of worst case regret in piecewise stationary multi armed bandits. While the minimax theory for stationary bandits is well established, understanding analogous limits in time-varying settings is challenging. Existing lower bounds rely on what we refer to as infrequent sampling arguments, where long intervals without exploration allow adversarial reward changes that induce large regret. In this paper, we introduce a fundamentally different approach based on a belief inertia argument. Our analysis captures how an algorithm's empirical beliefs, encoded through historical reward averages, create momentum that resists new evidence after a change. We show how this inertia can be exploited to construct adversarial instances that mislead classical algorithms such as Explore Then Commit, epsilon greedy, and UCB, causing them to suffer regret that grows linearly with T and with a substantial constant factor, regardless of how their parameters are tuned, even with a single change point. We extend the analysis to algorithms that periodically restart to handle non stationarity and prove that, even then, the worst case regret remains linear in T. Our results indicate that utilizing belief inertia can be a powerful method for deriving sharp lower bounds in non stationary bandits.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 18,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05629",
        "abs_url": "https://arxiv.org/abs/2511.05629",
        "pdf_url": "https://arxiv.org/pdf/2511.05629",
        "title": "SSTODE: Ocean-Atmosphere Physics-Informed Neural ODEs for Sea Surface Temperature Prediction",
        "authors": [
            "Zheng Jiang",
            "Wei Wang",
            "Gaowei Zhang",
            "Yi Wang"
        ],
        "comments": "To be published in the Proceedings of AAAI-AISI 2026",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Atmospheric and Oceanic Physics (physics.ao-ph)",
        "abstract": "Sea Surface Temperature (SST) is crucial for understanding upper-ocean thermal dynamics and ocean-atmosphere interactions, which have profound economic and social impacts. While data-driven models show promise in SST prediction, their black-box nature often limits interpretability and overlooks key physical processes. Recently, physics-informed neural networks have been gaining momentum but struggle with complex ocean-atmosphere dynamics due to 1) inadequate characterization of seawater movement (e.g., coastal upwelling) and 2) insufficient integration of external SST drivers (e.g., turbulent heat fluxes). To address these challenges, we propose SSTODE, a physics-informed Neural Ordinary Differential Equations (Neural ODEs) framework for SST prediction. First, we derive ODEs from fluid transport principles, incorporating both advection and diffusion to model ocean spatiotemporal dynamics. Through variational optimization, we recover a latent velocity field that explicitly governs the temporal dynamics of SST. Building upon ODE, we introduce an Energy Exchanges Integrator (EEI)-inspired by ocean heat budget equations-to account for external forcing factors. Thus, the variations in the components of these factors provide deeper insights into SST dynamics. Extensive experiments demonstrate that SSTODE achieves state-of-the-art performances in global and regional SST forecasting benchmarks. Furthermore, SSTODE visually reveals the impact of advection dynamics, thermal diffusion patterns, and diurnal heating-cooling cycles on SST evolution. These findings demonstrate the model's interpretability and physical consistency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 19,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05633",
        "abs_url": "https://arxiv.org/abs/2511.05633",
        "pdf_url": "https://arxiv.org/pdf/2511.05633",
        "title": "Physics-Guided Machine Learning for Uncertainty Quantification in Turbulence Models",
        "authors": [
            "Minghan Chu",
            "Weicheng Qian"
        ],
        "comments": "Accepted to NeurIPS 2025 Workshop on Machine Learning and the Physical Sciences (ML4PS), non-archival",
        "subjects": "Machine Learning (cs.LG); Fluid Dynamics (physics.flu-dyn)",
        "abstract": "Predicting the evolution of turbulent flows is central across science and engineering. Most studies rely on simulations with turbulence models, whose empirical simplifications introduce epistemic uncertainty. The Eigenspace Perturbation Method (EPM) is a widely used physics-based approach to quantify model-form uncertainty, but being purely physics-based it can overpredict uncertainty bounds. We propose a convolutional neural network (CNN)-based modulation of EPM perturbation magnitudes to improve calibration while preserving physical consistency. Across canonical cases, the hybrid ML-EPM framework yields substantially tighter, better-calibrated uncertainty estimates than baseline EPM alone.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 20,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05640",
        "abs_url": "https://arxiv.org/abs/2511.05640",
        "pdf_url": "https://arxiv.org/pdf/2511.05640",
        "title": "Blind Inverse Game Theory: Jointly Decoding Rewards and Rationality in Entropy-Regularized Competitive Games",
        "authors": [
            "Hamza Virk",
            "Sandro Amaglobeli",
            "Zuhayr Syed"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Science and Game Theory (cs.GT); Machine Learning (stat.ML)",
        "abstract": "Inverse Game Theory (IGT) methods based on the entropy-regularized Quantal Response Equilibrium (QRE) offer a tractable approach for competitive settings, but critically assume the agents' rationality parameter (temperature $\\tau$) is known a priori. When $\\tau$ is unknown, a fundamental scale ambiguity emerges that couples $\\tau$ with the reward parameters ($\\theta$), making them statistically unidentifiable. We introduce Blind-IGT, the first statistical framework to jointly recover both $\\theta$ and $\\tau$ from observed behavior. We analyze this bilinear inverse problem and establish necessary and sufficient conditions for unique identification by introducing a normalization constraint that resolves the scale ambiguity. We propose an efficient Normalized Least Squares (NLS) estimator and prove it achieves the optimal $\\mathcal{O}(N^{-1/2})$ convergence rate for joint parameter recovery. When strong identifiability conditions fail, we provide partial identification guarantees through confidence set construction. We extend our framework to Markov games and demonstrate optimal convergence rates with strong empirical performance even when transition dynamics are unknown.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 21,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05664",
        "abs_url": "https://arxiv.org/abs/2511.05664",
        "pdf_url": "https://arxiv.org/pdf/2511.05664",
        "title": "KLASS: KL-Guided Fast Inference in Masked Diffusion Models",
        "authors": [
            "Seo Hyun Kim",
            "Sunwoo Hong",
            "Hojung Jung",
            "Youngrok Park",
            "Se-Young Yun"
        ],
        "comments": "NeurIPS 2025 Spotlight. Code: this https URL",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Masked diffusion models have demonstrated competitive results on various tasks including language generation. However, due to its iterative refinement process, the inference is often bottlenecked by slow and static sampling speed. To overcome this problem, we introduce `KL-Adaptive Stability Sampling' (KLASS), a fast yet effective sampling method that exploits token-level KL divergence to identify stable, high-confidence predictions. By unmasking multiple tokens in each iteration without any additional model training, our approach speeds up generation significantly while maintaining sample quality. On reasoning benchmarks, KLASS achieves up to $2.78\\times$ wall-clock speedups while improving performance over standard greedy decoding, attaining state-of-the-art results among diffusion-based samplers. We further validate KLASS across diverse domains, including text, image, and molecular generation, showing its effectiveness as a broadly applicable sampler across different models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 22,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05694",
        "abs_url": "https://arxiv.org/abs/2511.05694",
        "pdf_url": "https://arxiv.org/pdf/2511.05694",
        "title": "Distributionally Robust Self Paced Curriculum Reinforcement Learning",
        "authors": [
            "Anirudh Satheesh",
            "Keenan Powell",
            "Vaneet Aggarwal"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "A central challenge in reinforcement learning is that policies trained in controlled environments often fail under distribution shifts at deployment into real-world environments. Distributionally Robust Reinforcement Learning (DRRL) addresses this by optimizing for worst-case performance within an uncertainty set defined by a robustness budget $\\epsilon$. However, fixing $\\epsilon$ results in a tradeoff between performance and robustness: small values yield high nominal performance but weak robustness, while large values can result in instability and overly conservative policies. We propose Distributionally Robust Self-Paced Curriculum Reinforcement Learning (DR-SPCRL), a method that overcomes this limitation by treating $\\epsilon$ as a continuous curriculum. DR-SPCRL adaptively schedules the robustness budget according to the agent's progress, enabling a balance between nominal and robust performance. Empirical results across multiple environments demonstrate that DR-SPCRL not only stabilizes training but also achieves a superior robustness-performance trade-off, yielding an average 11.8\\% increase in episodic return under varying perturbations compared to fixed or heuristic scheduling strategies, and achieving approximately 1.9$\\times$ the performance of the corresponding nominal RL algorithms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 23,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05696",
        "abs_url": "https://arxiv.org/abs/2511.05696",
        "pdf_url": "https://arxiv.org/pdf/2511.05696",
        "title": "AI-assisted workflow enables rapid, high-fidelity breast cancer clinical trial eligibility prescreening",
        "authors": [
            "Jacob T. Rosenthal",
            "Emma Hahesy",
            "Sulov Chalise",
            "Menglei Zhu",
            "Mert R. Sabuncu",
            "Lior Z. Braunstein",
            "Anyi Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Clinical trials play an important role in cancer care and research, yet participation rates remain low. We developed MSK-MATCH (Memorial Sloan Kettering Multi-Agent Trial Coordination Hub), an AI system for automated eligibility screening from clinical text. MSK-MATCH integrates a large language model with a curated oncology trial knowledge base and retrieval-augmented architecture providing explanations for all AI predictions grounded in source text. In a retrospective dataset of 88,518 clinical documents from 731 patients across six breast cancer trials, MSK-MATCH automatically resolved 61.9% of cases and triaged 38.1% for human review. This AI-assisted workflow achieved 98.6% accuracy, 98.4% sensitivity, and 98.7% specificity for patient-level eligibility classification, matching or exceeding performance of the human-only and AI-only comparisons. For the triaged cases requiring manual review, prepopulating eligibility screens with AI-generated explanations reduced screening time from 20 minutes to 43 seconds at an average cost of $0.96 per patient-trial pair.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 24,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05716",
        "abs_url": "https://arxiv.org/abs/2511.05716",
        "pdf_url": "https://arxiv.org/pdf/2511.05716",
        "title": "Distributionally Robust Multimodal Machine Learning",
        "authors": [
            "Peilin Yang",
            "Yu Ma"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We consider the problem of distributionally robust multimodal machine learning. Existing approaches often rely on merging modalities on the feature level (early fusion) or heuristic uncertainty modeling, which downplays modality-aware ef- fects and provide limited insights. We propose a novel distributionally robust optimization (DRO) framework that aims to study both the theoretical and practical insights of multimodal machine learning. We first justify this setup and show the significance of this problem through complexity analysis. We then establish both generalization upper bounds and minimax lower bounds which provide perfor- mance guarantees. These results are further extended in settings where we consider encoder-specific error propogations. Empirically, we demonstrate that our approach improves robustness in both simulation settings and real-world datasets. Together, these findings provide a principled foundation for employing multimodal machine learning models in high-stakes applications where uncertainty is unavoidable.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 25,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05726",
        "abs_url": "https://arxiv.org/abs/2511.05726",
        "pdf_url": "https://arxiv.org/pdf/2511.05726",
        "title": "GastroDL-Fusion: A Dual-Modal Deep Learning Framework Integrating Protein-Ligand Complexes and Gene Sequences for Gastrointestinal Disease Drug Discovery",
        "authors": [
            "Ziyang Gao",
            "Annie Cheung",
            "Yihao Ou"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)",
        "abstract": "Accurate prediction of protein-ligand binding affinity plays a pivotal role in accelerating the discovery of novel drugs and vaccines, particularly for gastrointestinal (GI) diseases such as gastric ulcers, Crohn's disease, and ulcerative colitis. Traditional computational models often rely on structural information alone and thus fail to capture the genetic determinants that influence disease mechanisms and therapeutic responses. To address this gap, we propose GastroDL-Fusion, a dual-modal deep learning framework that integrates protein-ligand complex data with disease-associated gene sequence information for drug and vaccine development. In our approach, protein-ligand complexes are represented as molecular graphs and modeled using a Graph Isomorphism Network (GIN), while gene sequences are encoded into biologically meaningful embeddings via a pre-trained Transformer (ProtBERT/ESM). These complementary modalities are fused through a multi-layer perceptron to enable robust cross-modal interaction learning. We evaluate the model on benchmark datasets of GI disease-related targets, demonstrating that GastroDL-Fusion significantly improves predictive performance over conventional methods. Specifically, the model achieves a mean absolute error (MAE) of 1.12 and a root mean square error (RMSE) of 1.75, outperforming CNN, BiLSTM, GIN, and Transformer-only baselines. These results confirm that incorporating both structural and genetic features yields more accurate predictions of binding affinities, providing a reliable computational tool for accelerating the design of targeted therapies and vaccines in the context of gastrointestinal diseases.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 26,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05730",
        "abs_url": "https://arxiv.org/abs/2511.05730",
        "pdf_url": "https://arxiv.org/pdf/2511.05730",
        "title": "QiVC-Net: Quantum-Inspired Variational Convolutional Network, with Application to Biosignal Classification",
        "authors": [
            "Amin Golnari",
            "Jamileh Yousefi",
            "Reza Moheimani",
            "Saeid Sanei"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Signal Processing (eess.SP)",
        "abstract": "This work introduces the quantum-inspired variational convolution (QiVC) framework, a novel learning paradigm that integrates principles of probabilistic inference, variational optimization, and quantum-inspired transformations within convolutional architectures. The central innovation of QiVC lies in its quantum-inspired rotated ensemble (QiRE) mechanism. QiRE performs differentiable low-dimensional subspace rotations of convolutional weights, analogously to quantum state evolution. This approach enables structured uncertainty modeling while preserving the intrinsic geometry of the parameter space, resulting in more expressive, stable, and uncertainty-aware representations. To demonstrate its practical potential, the concept is instantiated in a QiVC-based convolutional network (QiVC-Net) and evaluated in the context of biosignal classification, focusing on phonocardiogram (PCG) recordings, a challenging domain characterized by high noise, inter-subject variability, and often imbalanced data. The proposed QiVC-Net integrates an architecture in which the QiVC layer does not introduce additional parameters, instead performing an ensemble rotation of the convolutional weights through a structured mechanism ensuring robustness without added highly computational burden. Experiments on two benchmark datasets, PhysioNet CinC 2016 and PhysioNet CirCor DigiScope 2022, show that QiVC-Net achieves state-of-the-art performance, reaching accuracies of 97.84% and 97.89%, respectively. These findings highlight the versatility of the QiVC framework and its promise for advancing uncertainty-aware modeling in real-world biomedical signal analysis. The implementation of the QiVConv layer is openly available in GitHub.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 27,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05736",
        "abs_url": "https://arxiv.org/abs/2511.05736",
        "pdf_url": "https://arxiv.org/pdf/2511.05736",
        "title": "Near-Exponential Savings for Mean Estimation with Active Learning",
        "authors": [
            "Julian M. Morimoto",
            "Jacob Goldin",
            "Daniel E. Ho"
        ],
        "comments": "Accepted to the 39th Conference on Neural Information Processing Systems (NeurIPS 2025)",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We study the problem of efficiently estimating the mean of a $k$-class random variable, $Y$, using a limited number of labels, $N$, in settings where the analyst has access to auxiliary information (i.e.: covariates) $X$ that may be informative about $Y$. We propose an active learning algorithm (\"PartiBandits\") to estimate $\\mathbb{E}[Y]$. The algorithm yields an estimate, $\\widehat{\\mu}_{\\text{PB}}$, such that $\\left( \\widehat{\\mu}_{\\text{PB}} - \\mathbb{E}[Y]\\right)^2$ is $\\tilde{\\mathcal{O}}\\left( \\frac{\\nu + \\exp(c \\cdot (-N/\\log(N))) }{N} \\right)$, where $c > 0$ is a constant and $\\nu$ is the risk of the Bayes-optimal classifier. PartiBandits is essentially a two-stage algorithm. In the first stage, it learns a partition of the unlabeled data that shrinks the average conditional variance of $Y$. In the second stage it uses a UCB-style subroutine (\"WarmStart-UCB\") to request labels from each stratum round-by-round. Both the main algorithm's and the subroutine's convergence rates are minimax optimal in classical settings. PartiBandits bridges the UCB and disagreement-based approaches to active learning despite these two approaches being designed to tackle very different tasks. We illustrate our methods through simulation using nationwide electronic health records. Our methods can be implemented using the PartiBandits package in R.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 28,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05758",
        "abs_url": "https://arxiv.org/abs/2511.05758",
        "pdf_url": "https://arxiv.org/pdf/2511.05758",
        "title": "Primal-Only Actor Critic Algorithm for Robust Constrained Average Cost MDPs",
        "authors": [
            "Anirudh Satheesh",
            "Sooraj Sathish",
            "Swetha Ganesh",
            "Keenan Powell",
            "Vaneet Aggarwal"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In this work, we study the problem of finding robust and safe policies in Robust Constrained Average-Cost Markov Decision Processes (RCMDPs). A key challenge in this setting is the lack of strong duality, which prevents the direct use of standard primal-dual methods for constrained RL. Additional difficulties arise from the average-cost setting, where the Robust Bellman operator is not a contraction under any norm. To address these challenges, we propose an actor-critic algorithm for Average-Cost RCMDPs. We show that our method achieves both \\(\\epsilon\\)-feasibility and \\(\\epsilon\\)-optimality, and we establish a sample complexities of \\(\\tilde{O}\\left(\\epsilon^{-4}\\right)\\) and \\(\\tilde{O}\\left(\\epsilon^{-6}\\right)\\) with and without slackness assumption, which is comparable to the discounted setting.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 29,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05770",
        "abs_url": "https://arxiv.org/abs/2511.05770",
        "pdf_url": "https://arxiv.org/pdf/2511.05770",
        "title": "An Efficient Gradient-Aware Error-Bounded Lossy Compressor for Federated Learning",
        "authors": [
            "Zhijing Ye",
            "Sheng Di",
            "Jiamin Wang",
            "Zhiqing Zhong",
            "Zhaorui Zhang",
            "Xiaodong Yu"
        ],
        "comments": "Preprint version",
        "subjects": "Machine Learning (cs.LG); Distributed, Parallel, and Cluster Computing (cs.DC)",
        "abstract": "Federated learning (FL) enables collaborative model training without exposing clients' private data, but its deployment is often constrained by the communication cost of transmitting gradients between clients and the central server, especially under system heterogeneity where low-bandwidth clients bottleneck overall performance. Lossy compression of gradient data can mitigate this overhead, and error-bounded lossy compression (EBLC) is particularly appealing for its fine-grained utility-compression tradeoff. However, existing EBLC methods (e.g., SZ), originally designed for smooth scientific data with strong spatial locality, rely on generic predictors such as Lorenzo and interpolation for entropy reduction to improve compression ratio. Gradient tensors, in contrast, exhibit low smoothness and weak spatial correlation, rendering these predictors ineffective and leading to poor compression ratios. To address this limitation, we propose an EBLC framework tailored for FL gradient data to achieve high compression ratios while preserving model accuracy. The core of it is an innovative prediction mechanism that exploits temporal correlations across FL training rounds and structural regularities within convolutional kernels to reduce residual entropy. The predictor is compatible with standard quantizers and entropy coders and comprises (1) a cross-round magnitude predictor based on a normalized exponential moving average, and (2) a sign predictor that leverages gradient oscillation and kernel-level sign consistency. Experiments show that this new EBLC yields up to 1.53x higher compression ratios than SZ3 with lower accuracy loss. Integrated into a real-world FL framework, APPFL, it reduces end-to-end communication time by 76.1%-96.2% under various constrained-bandwidth scenarios, demonstrating strong scalability for real-world FL deployments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 30,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05790",
        "abs_url": "https://arxiv.org/abs/2511.05790",
        "pdf_url": "https://arxiv.org/pdf/2511.05790",
        "title": "SymLight: Exploring Interpretable and Deployable Symbolic Policies for Traffic Signal Control",
        "authors": [
            "Xiao-Cheng Liao",
            "Yi Mei",
            "Mengjie Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Deep Reinforcement Learning have achieved significant success in automatically devising effective traffic signal control (TSC) policies. Neural policies, however, tend to be over-parameterized and non-transparent, hindering their interpretability and deployability on resource-limited edge devices. This work presents SymLight, a priority function search framework based on Monte Carlo Tree Search (MCTS) for discovering inherently interpretable and deployable symbolic priority functions to serve as the TSC policies. The priority function, in particular, accepts traffic features as input and then outputs a priority for each traffic signal phase, which subsequently directs the phase transition. For effective search, we propose a concise yet expressive priority function representation. This helps mitigate the combinatorial explosion of the action space in MCTS. Additionally, a probabilistic structural rollout strategy is introduced to leverage structural patterns from previously discovered high-quality priority functions, guiding the rollout process. Our experiments on real-world datasets demonstrate SymLight's superior performance across a range of baselines. A key advantage is SymLight's ability to produce interpretable and deployable TSC policies while maintaining excellent performance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 31,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05804",
        "abs_url": "https://arxiv.org/abs/2511.05804",
        "pdf_url": "https://arxiv.org/pdf/2511.05804",
        "title": "Catching Contamination Before Generation: Spectral Kill Switches for Agents",
        "authors": [
            "Valentin NoÃ«l"
        ],
        "comments": "Preprint under review (2025). 9 pages, 2 figures. Code and scripts: to be released",
        "subjects": "Machine Learning (cs.LG); Signal Processing (eess.SP); Systems and Control (eess.SY); Machine Learning (stat.ML)",
        "abstract": "Agentic language models compose multi step reasoning chains, yet intermediate steps can be corrupted by inconsistent context, retrieval errors, or adversarial inputs, which makes post hoc evaluation too late because errors propagate before detection. We introduce a diagnostic that requires no additional training and uses only the forward pass to emit a binary accept or reject signal during agent execution. The method analyzes token graphs induced by attention and computes two spectral statistics in early layers, namely the high frequency energy ratio and spectral entropy. We formalize these signals, establish invariances, and provide finite sample estimators with uncertainty quantification. Under a two regime mixture assumption with a monotone likelihood ratio property, we show that a single threshold on the high frequency energy ratio is optimal in the Bayes sense for detecting context inconsistency. Empirically, the high frequency energy ratio exhibits robust bimodality during context verification across multiple model families, which enables gating decisions with overhead below one millisecond on our hardware and configurations. We demonstrate integration into retrieval augmented agent pipelines and discuss deployment as an inline safety monitor. The approach detects contamination while the model is still processing the text, before errors commit to the reasoning chain.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 32,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05811",
        "abs_url": "https://arxiv.org/abs/2511.05811",
        "pdf_url": "https://arxiv.org/pdf/2511.05811",
        "title": "MOSS: Efficient and Accurate FP8 LLM Training with Microscaling and Automatic Scaling",
        "authors": [
            "Yu Zhang",
            "Hui-Ling Zhen",
            "Mingxuan Yuan",
            "Bei Yu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Training large language models with FP8 formats offers significant efficiency gains. However, the reduced numerical precision of FP8 poses challenges for stable and accurate training. Current frameworks preserve training performance using mixed-granularity quantization, i.e., applying per-group quantization for activations and per-tensor/block quantization for weights. While effective, per-group quantization requires scaling along the inner dimension of matrix multiplication, introducing additional dequantization overhead. Moreover, these frameworks often rely on just-in-time scaling to dynamically adjust scaling factors based on the current data distribution. However, this online quantization is inefficient for FP8 training, as it involves multiple memory reads and writes that negate the performance benefits of FP8. To overcome these limitations, we propose MOSS, a novel FP8 training framework that ensures both efficiency and numerical stability. MOSS introduces two key innovations: (1) a two-level microscaling strategy for quantizing sensitive activations, which balances precision and dequantization cost by combining a high-precision global scale with compact, power-of-two local scales; and (2) automatic scaling for weights in linear layers, which eliminates the need for costly max-reduction operations by predicting and adjusting scaling factors during training. Leveraging these techniques, MOSS enables efficient FP8 training of a 7B parameter model, achieving performance comparable to the BF16 baseline while achieving up to 34% higher training throughput.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 33,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05823",
        "abs_url": "https://arxiv.org/abs/2511.05823",
        "pdf_url": "https://arxiv.org/pdf/2511.05823",
        "title": "AiEDA: An Open-Source AI-Aided Design Library for Design-to-Vector",
        "authors": [
            "Yihang Qiu",
            "Zengrong Huang",
            "Simin Tao",
            "Hongda Zhang",
            "Weiguo Li",
            "Xinhua Lai",
            "Rui Wang",
            "Weiqiang Wang",
            "Xingquan Li"
        ],
        "comments": "18 pages, 29 figures, accepted by TCAD 2025",
        "subjects": "Machine Learning (cs.LG); Hardware Architecture (cs.AR)",
        "abstract": "Recent research has demonstrated that artificial intelligence (AI) can assist electronic design automation (EDA) in improving both the quality and efficiency of chip design. But current AI for EDA (AI-EDA) infrastructures remain fragmented, lacking comprehensive solutions for the entire data pipeline from design execution to AI integration. Key challenges include fragmented flow engines that generate raw data, heterogeneous file formats for data exchange, non-standardized data extraction methods, and poorly organized data storage. This work introduces a unified open-source library for EDA (AiEDA) that addresses these issues. AiEDA integrates multiple design-to-vector data representation techniques that transform diverse chip design data into universal multi-level vector representations, establishing an AI-aided design (AAD) paradigm optimized for AI-EDA workflows. AiEDA provides complete physical design flows with programmatic data extraction and standardized Python interfaces bridging EDA datasets and AI frameworks. Leveraging the AiEDA library, we generate iDATA, a 600GB dataset of structured data derived from 50 real chip designs (28nm), and validate its effectiveness through seven representative AAD tasks spanning prediction, generation, optimization and analysis. The code is publicly available at this https URL, while the full iDATA dataset is being prepared for public release, providing a foundation for future AI-EDA research.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 34,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05826",
        "abs_url": "https://arxiv.org/abs/2511.05826",
        "pdf_url": "https://arxiv.org/pdf/2511.05826",
        "title": "CADM: Cluster-customized Adaptive Distance Metric for Categorical Data Clustering",
        "authors": [
            "Taixi Chen",
            "Yiu-ming Cheung",
            "Yiqun Zhang"
        ],
        "comments": "5 pages",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "An appropriate distance metric is crucial for categorical data clustering, as the distance between categorical data cannot be directly calculated. However, the distances between attribute values usually vary in different clusters induced by their different distributions, which has not been taken into account, thus leading to unreasonable distance measurement. Therefore, we propose a cluster-customized distance metric for categorical data clustering, which can competitively update distances based on different distributions of attributes in each cluster. In addition, we extend the proposed distance metric to the mixed data that contains both numerical and categorical attributes. Experiments demonstrate the efficacy of the proposed method, i.e., achieving an average ranking of around first in fourteen datasets. The source code is available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 35,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05863",
        "abs_url": "https://arxiv.org/abs/2511.05863",
        "pdf_url": "https://arxiv.org/pdf/2511.05863",
        "title": "EMOD: A Unified EEG Emotion Representation Framework Leveraging V-A Guided Contrastive Learning",
        "authors": [
            "Yuning Chen",
            "Sha Zhao",
            "Shijian Li",
            "Gang Pan"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Emotion recognition from EEG signals is essential for affective computing and has been widely explored using deep learning. While recent deep learning approaches have achieved strong performance on single EEG emotion datasets, their generalization across datasets remains limited due to the heterogeneity in annotation schemes and data formats. Existing models typically require dataset-specific architectures tailored to input structure and lack semantic alignment across diverse emotion labels. To address these challenges, we propose EMOD: A Unified EEG Emotion Representation Framework Leveraging Valence-Arousal (V-A) Guided Contrastive Learning. EMOD learns transferable and emotion-aware representations from heterogeneous datasets by bridging both semantic and structural gaps. Specifically, we project discrete and continuous emotion labels into a unified V-A space and formulate a soft-weighted supervised contrastive loss that encourages emotionally similar samples to cluster in the latent space. To accommodate variable EEG formats, EMOD employs a flexible backbone comprising a Triple-Domain Encoder followed by a Spatial-Temporal Transformer, enabling robust extraction and integration of temporal, spectral, and spatial features. We pretrain EMOD on eight public EEG datasets and evaluate its performance on three benchmark datasets. Experimental results show that EMOD achieves state-of-the-art performance, demonstrating strong adaptability and generalization across diverse EEG-based emotion recognition scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 36,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05872",
        "abs_url": "https://arxiv.org/abs/2511.05872",
        "pdf_url": "https://arxiv.org/pdf/2511.05872",
        "title": "Adaptation and Fine-tuning with TabPFN for Travelling Salesman Problem",
        "authors": [
            "Nguyen Gia Hien Vu",
            "Yifan Tang",
            "Rey Lim",
            "Yifan Yang",
            "Hang Ma",
            "Ke Wang",
            "G. Gary Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Combinatorics (math.CO)",
        "abstract": "Tabular Prior-Data Fitted Network (TabPFN) is a foundation model designed for small to medium-sized tabular data, which has attracted much attention recently. This paper investigates the application of TabPFN in Combinatorial Optimization (CO) problems. The aim is to lessen challenges in time and data-intensive training requirements often observed in using traditional methods including exact and heuristic algorithms, Machine Learning (ML)-based models, to solve CO problems. Proposing possibly the first ever application of TabPFN for such a purpose, we adapt and fine-tune the TabPFN model to solve the Travelling Salesman Problem (TSP), one of the most well-known CO problems. Specifically, we adopt the node-based approach and the node-predicting adaptation strategy to construct the entire TSP route. Our evaluation with varying instance sizes confirms that TabPFN requires minimal training, adapts to TSP using a single sample, performs better generalization across varying TSP instance sizes, and reduces performance degradation. Furthermore, the training process with adaptation and fine-tuning is completed within minutes. The methodology leads to strong solution quality even without post-processing and achieves performance comparable to other models with post-processing refinement. Our findings suggest that the TabPFN model is a promising approach to solve structured and CO problems efficiently under training resource constraints and rapid deployment requirements.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 37,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05878",
        "abs_url": "https://arxiv.org/abs/2511.05878",
        "pdf_url": "https://arxiv.org/pdf/2511.05878",
        "title": "FusionLog: Cross-System Log-based Anomaly Detection via Fusion of General and Proprietary Knowledge",
        "authors": [
            "Xinlong Zhao",
            "Tong Jia",
            "Minghua He",
            "Xixuan Yang",
            "Ying Li"
        ],
        "comments": "11 pages, 4 figures, and 2 tables",
        "subjects": "Machine Learning (cs.LG); Software Engineering (cs.SE)",
        "abstract": "Log-based anomaly detection is critical for ensuring the stability and reliability of web systems. One of the key problems in this task is the lack of sufficient labeled logs, which limits the rapid deployment in new systems. Existing works usually leverage large-scale labeled logs from a mature web system and a small amount of labeled logs from a new system, using transfer learning to extract and generalize general knowledge across both domains. However, these methods focus solely on the transfer of general knowledge and neglect the disparity and potential mismatch between such knowledge and the proprietary knowledge of target system, thus constraining performance. To address this limitation, we propose FusionLog, a novel zero-label cross-system log-based anomaly detection method that effectively achieves the fusion of general and proprietary knowledge, enabling cross-system generalization without any labeled target logs. Specifically, we first design a training-free router based on semantic similarity that dynamically partitions unlabeled target logs into 'general logs' and 'proprietary logs.' For general logs, FusionLog employs a small model based on system-agnostic representation meta-learning for direct training and inference, inheriting the general anomaly patterns shared between the source and target systems. For proprietary logs, we iteratively generate pseudo-labels and fine-tune the small model using multi-round collaborative knowledge distillation and fusion based on large language model (LLM) and small model (SM) to enhance its capability to recognize anomaly patterns specific to the target system. Experimental results on three public log datasets from different systems show that FusionLog achieves over 90% F1-score under a fully zero-label setting, significantly outperforming state-of-the-art cross-system log-based anomaly detection methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 38,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05924",
        "abs_url": "https://arxiv.org/abs/2511.05924",
        "pdf_url": "https://arxiv.org/pdf/2511.05924",
        "title": "From Kernels to Attention: A Transformer Framework for Density and Score Estimation",
        "authors": [
            "Vasily Ilin",
            "Peter Sushko"
        ],
        "comments": "14 pages, 14 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We introduce a unified attention-based framework for joint score and density estimation. Framing the problem as a sequence-to-sequence task, we develop a permutation- and affine-equivariant transformer that estimates both the probability density $f(x)$ and its score $\\nabla_x \\log f(x)$ directly from i.i.d. samples. Unlike traditional score-matching methods that require training a separate model for each distribution, our approach learns a single distribution-agnostic operator that generalizes across densities and sample sizes. The architecture employs cross-attention to connect observed samples with arbitrary query points, enabling generalization beyond the training data, while built-in symmetry constraints ensure equivariance to permutation and affine transformations. Analytically, we show that the attention weights can recover classical kernel density estimation (KDE), and verify it empirically, establishing a principled link between classical KDE and the transformer architecture. Empirically, the model achieves substantially lower error and better scaling than KDE and score-debiased KDE (SD-KDE), while exhibiting better runtime scaling. Together, these results establish transformers as general-purpose, data-adaptive operators for nonparametric density and score estimation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 39,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05960",
        "abs_url": "https://arxiv.org/abs/2511.05960",
        "pdf_url": "https://arxiv.org/pdf/2511.05960",
        "title": "Deep Survival Analysis of Longitudinal EHR Data for Joint Prediction of Hospitalization and Death in COPD Patients",
        "authors": [
            "Enrico Manzini",
            "Thomas Gonzalez Saito",
            "Joan Escudero",
            "Ana GÃ©nova",
            "Cristina Caso",
            "Tomas Perez-Porcuna",
            "Alexandre Perera-Lluna"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Patients with chronic obstructive pulmonary disease (COPD) have an increased risk of hospitalizations, strongly associated with decreased survival, yet predicting the timing of these events remains challenging and has received limited attention in the literature. In this study, we performed survival analysis to predict hospitalization and death in COPD patients using longitudinal electronic health records (EHRs), comparing statistical models, machine learning (ML), and deep learning (DL) approaches. We analyzed data from more than 150k patients from the SIDIAP database in Catalonia, Spain, from 2013 to 2017, modeling hospitalization as a first event and death as a semi-competing terminal event. Multiple models were evaluated, including Cox proportional hazards, SurvivalBoost, DeepPseudo, SurvTRACE, Dynamic Deep-Hit, and Deep Recurrent Survival Machine. Results showed that DL models utilizing recurrent architectures outperformed both ML and linear approaches in concordance and time-dependent AUC, especially for hospitalization, which proved to be the harder event to predict. This study is, to our knowledge, the first to apply deep survival analysis on longitudinal EHR data to jointly predict multiple time-to-event outcomes in COPD patients, highlighting the potential of DL approaches to capture temporal patterns and improve risk stratification.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 40,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05963",
        "abs_url": "https://arxiv.org/abs/2511.05963",
        "pdf_url": "https://arxiv.org/pdf/2511.05963",
        "title": "Next-Latent Prediction Transformers Learn Compact World Models",
        "authors": [
            "Jayden Teoh",
            "Manan Tomar",
            "Kwangjun Ahn",
            "Edward S. Hu",
            "Pratyusha Sharma",
            "Riashat Islam",
            "Alex Lamb",
            "John Langford"
        ],
        "comments": "Preprint by Microsoft Research",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Transformers replace recurrence with a memory that grows with sequence length and self-attention that enables ad-hoc look ups over past tokens. Consequently, they lack an inherent incentive to compress history into compact latent states with consistent transition rules. This often leads to learning solutions that generalize poorly. We introduce Next-Latent Prediction (NextLat), which extends standard next-token training with self-supervised predictions in the latent space. Specifically, NextLat trains a transformer to learn latent representations that are predictive of its next latent state given the next output token. Theoretically, we show that these latents provably converge to belief states, compressed information of the history necessary to predict the future. This simple auxiliary objective also injects a recurrent inductive bias into transformers, while leaving their architecture, parallel training, and inference unchanged. NextLat effectively encourages the transformer to form compact internal world models with its own belief states and transition dynamics -- a crucial property absent in standard next-token prediction transformers. Empirically, across benchmarks targeting core sequence modeling competencies -- world modeling, reasoning, planning, and language modeling -- NextLat demonstrates significant gains over standard next-token training in downstream accuracy, representation compression, and lookahead planning. NextLat stands as a simple and efficient paradigm for shaping transformer representations toward stronger generalization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 41,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05973",
        "abs_url": "https://arxiv.org/abs/2511.05973",
        "pdf_url": "https://arxiv.org/pdf/2511.05973",
        "title": "Explainable Deep Learning-based Classification of Wolff-Parkinson-White Electrocardiographic Signals",
        "authors": [
            "Alice Ragonesi",
            "Stefania Fresca",
            "Karli Gillette",
            "Stefan Kurath-Koller",
            "Gernot Plank",
            "Elena Zappon"
        ],
        "comments": "27 pages, 9 figures, 4 tables",
        "subjects": "Machine Learning (cs.LG); Numerical Analysis (math.NA); Tissues and Organs (q-bio.TO)",
        "abstract": "Wolff-Parkinson-White (WPW) syndrome is a cardiac electrophysiology (EP) disorder caused by the presence of an accessory pathway (AP) that bypasses the atrioventricular node, faster ventricular activation rate, and provides a substrate for atrio-ventricular reentrant tachycardia (AVRT). Accurate localization of the AP is critical for planning and guiding catheter ablation procedures. While traditional diagnostic tree (DT) methods and more recent machine learning (ML) approaches have been proposed to predict AP location from surface electrocardiogram (ECG), they are often constrained by limited anatomical localization resolution, poor interpretability, and the use of small clinical datasets. In this study, we present a Deep Learning (DL) model for the localization of single manifest APs across 24 cardiac regions, trained on a large, physiologically realistic database of synthetic ECGs generated using a personalized virtual heart model. We also integrate eXplainable Artificial Intelligence (XAI) methods, Guided Backpropagation, Grad-CAM, and Guided Grad-CAM, into the pipeline. This enables interpretation of DL decision-making and addresses one of the main barriers to clinical adoption: lack of transparency in ML predictions. Our model achieves localization accuracy above 95%, with a sensitivity of 94.32% and specificity of 99.78%. XAI outputs are physiologically validated against known depolarization patterns, and a novel index is introduced to identify the most informative ECG leads for AP localization. Results highlight lead V2 as the most critical, followed by aVF, V1, and aVL. This work demonstrates the potential of combining cardiac digital twins with explainable DL to enable accurate, transparent, and non-invasive AP localization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 42,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05980",
        "abs_url": "https://arxiv.org/abs/2511.05980",
        "pdf_url": "https://arxiv.org/pdf/2511.05980",
        "title": "Are Time-Indexed Foundation Models the Future of Time Series Imputation?",
        "authors": [
            "Etienne Le Naour",
            "Tahar Nabil",
            "Adrien Petralia",
            "Ghislain Agoua"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Foundation models for time series imputation remain largely unexplored. Recently, two such models, TabPFN-TS and MoTM, have emerged. These models share a common philosophy that places them within the family of time-indexed foundation models. This paper presents the first large-scale empirical study of these models for zero-shot imputation, which enables missing value recovery without retraining across a wide range of scenarios. We conduct extensive univariate experiments across 33 out-of-domain datasets (approximately 1.3M imputation windows) and evaluate their ability to integrate covariates at inference time to improve accuracy without fine-tuning. Our results demonstrate that time-indexed foundation models are a powerful and practical step toward achieving general-purpose, zero-shot imputation for real-world time series.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 43,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05985",
        "abs_url": "https://arxiv.org/abs/2511.05985",
        "pdf_url": "https://arxiv.org/pdf/2511.05985",
        "title": "Bespoke Co-processor for Energy-Efficient Health Monitoring on RISC-V-based Flexible Wearables",
        "authors": [
            "Theofanis Vergos",
            "Polykarpos Vergos",
            "Mehdi B. Tahoori",
            "Georgios Zervakis"
        ],
        "comments": "Accepted for publication at IEEE Design, Automation & Test in Europe (DATE 2026)",
        "subjects": "Machine Learning (cs.LG); Hardware Architecture (cs.AR)",
        "abstract": "Flexible electronics offer unique advantages for conformable, lightweight, and disposable healthcare wearables. However, their limited gate count, large feature sizes, and high static power consumption make on-body machine learning classification highly challenging. While existing bendable RISC-V systems provide compact solutions, they lack the energy efficiency required. We present a mechanically flexible RISC-V that integrates a bespoke multiply-accumulate co-processor with fixed coefficients to maximize energy efficiency and minimize latency. Our approach formulates a constrained programming problem to jointly determine co-processor constants and optimally map Multi-Layer Perceptron (MLP) inference operations, enabling compact, model-specific hardware by leveraging the low fabrication and non-recurring engineering costs of flexible technologies. Post-layout results demonstrate near-real-time performance across several healthcare datasets, with our circuits operating within the power budget of existing flexible batteries and occupying only 2.42 mm^2, offering a promising path toward accessible, sustainable, and conformable healthcare wearables. Our microprocessors achieve an average 2.35x speedup and 2.15x lower energy consumption compared to the state of the art.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 44,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06010",
        "abs_url": "https://arxiv.org/abs/2511.06010",
        "pdf_url": "https://arxiv.org/pdf/2511.06010",
        "title": "MoSKA: Mixture of Shared KV Attention for Efficient Long-Sequence LLM Inference",
        "authors": [
            "Myunghyun Rhee",
            "Sookyung Choi",
            "Euiseok Kim",
            "Joonseop Sim",
            "Youngpyo Joo",
            "Hoshik Kim"
        ],
        "comments": "4 pages, 5 figures, accepted for publication at IEEE Computer Architecture Letters (IEEE CAL), 2025",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)",
        "abstract": "The escalating context length in Large Language Models (LLMs) creates a severe performance bottleneck around the Key-Value (KV) cache, whose memory-bound nature leads to significant GPU under-utilization. This paper introduces Mixture of Shared KV Attention (MoSKA), an architecture that addresses this challenge by exploiting the heterogeneity of context data. It differentiates between per-request unique and massively reused shared sequences. The core of MoSKA is a novel Shared KV Attention mechanism that transforms the attention on shared data from a series of memory-bound GEMV operations into a single, compute-bound GEMM by batching concurrent requests. This is supported by an MoE-inspired sparse attention strategy that prunes the search space and a tailored Disaggregated Infrastructure that specializes hardware for unique and shared data. This comprehensive approach demonstrates a throughput increase of up to 538.7x over baselines in workloads with high context sharing, offering a clear architectural path toward scalable LLM inference.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 45,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06029",
        "abs_url": "https://arxiv.org/abs/2511.06029",
        "pdf_url": "https://arxiv.org/pdf/2511.06029",
        "title": "Lethe: Layer- and Time-Adaptive KV Cache Pruning for Reasoning-Intensive LLM Serving",
        "authors": [
            "Hui Zeng",
            "Daming Zhao",
            "Pengfei Yang",
            "Wenxuan Hou",
            "Tianyang Zheng",
            "Hui Li",
            "Weiye Ji",
            "Jidong Zhai"
        ],
        "comments": "aaai26 camera-ready version, 12 pages",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Generative reasoning with large language models (LLMs) often involves long decoding sequences, leading to substantial memory and latency overheads from accumulating key-value (KV) caches. While existing KV compression methods primarily focus on reducing prefill memory from long input sequences, they fall short in addressing the dynamic and layer-sensitive nature of long-form generation, which is central to reasoning tasks. We propose Lethe, a dynamic KV cache management framework that introduces adaptivity along both the spatial and temporal dimensions of decoding. Along the spatial dimension, Lethe performs layerwise sparsity-aware allocation, assigning token pruning budgets to each transformer layer based on estimated attention redundancy. Along the temporal dimension, Lethe conducts multi-round token pruning during generation, driven by a Recency-Aware Selective Retention} (RASR) mechanism. RASR extends traditional recency-based heuristics by also considering token relevance derived from evolving attention patterns, enabling informed decisions about which tokens to retain or evict. Empirical results demonstrate that Lethe achieves a favorable balance between efficiency and generation quality across diverse models and tasks, increases throughput by up to 2.56x.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 46,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06041",
        "abs_url": "https://arxiv.org/abs/2511.06041",
        "pdf_url": "https://arxiv.org/pdf/2511.06041",
        "title": "Advancing Ocean State Estimation with efficient and scalable AI",
        "authors": [
            "Yanfei Xiang",
            "Yuan Gao",
            "Hao Wu",
            "Quan Zhang",
            "Ruiqi Shu",
            "Xiao Zhou",
            "Xi Wu",
            "Xiaomeng Huang"
        ],
        "comments": "29 papes, 10 Figures",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Accurate and efficient global ocean state estimation remains a grand challenge for Earth system science, hindered by the dual bottlenecks of computational scalability and degraded data fidelity in traditional data assimilation (DA) and deep learning (DL) approaches. Here we present an AI-driven Data Assimilation Framework for Ocean (ADAF-Ocean) that directly assimilates multi-source and multi-scale observations, ranging from sparse in-situ measurements to 4 km satellite swaths, without any interpolation or data thinning. Inspired by Neural Processes, ADAF-Ocean learns a continuous mapping from heterogeneous inputs to ocean states, preserving native data fidelity. Through AI-driven super-resolution, it reconstructs 0.25$^\\circ$ mesoscale dynamics from coarse 1$^\\circ$ fields, which ensures both efficiency and scalability, with just 3.7\\% more parameters than the 1$^\\circ$ configuration. When coupled with a DL forecasting system, ADAF-Ocean extends global forecast skill by up to 20 days compared to baselines without assimilation. This framework establishes a computationally viable and scientifically rigorous pathway toward real-time, high-resolution Earth system monitoring.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 47,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06042",
        "abs_url": "https://arxiv.org/abs/2511.06042",
        "pdf_url": "https://arxiv.org/pdf/2511.06042",
        "title": "Physics-Informed Design of Input Convex Neural Networks for Consistency Optimal Transport Flow Matching",
        "authors": [
            "Fanghui Song",
            "Zhongjian Wang",
            "Jiebao Sun"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We propose a consistency model based on the optimal-transport flow. A physics-informed design of partially input-convex neural networks (PICNN) plays a central role in constructing the flow field that emulates the displacement interpolation. During the training stage, we couple the Hamilton-Jacobi (HJ) residual in the OT formulation with the original flow matching loss function. Our approach avoids inner optimization subproblems that are present in previous one-step OFM approaches. During the prediction stage, our approach supports both one-step (Brenier-map) and multi-step ODE sampling from the same learned potential, leveraging the straightness of the OT flow. We validate scalability and performance on standard OT benchmarks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 48,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06054",
        "abs_url": "https://arxiv.org/abs/2511.06054",
        "pdf_url": "https://arxiv.org/pdf/2511.06054",
        "title": "Function Based Isolation Forest (FuBIF): A Unifying Framework for Interpretable Isolation-Based Anomaly Detection",
        "authors": [
            "Alessio Arcudi",
            "Alessandro Ferreri",
            "Francesco Borsatti",
            "Gian Antonio Susto"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Anomaly Detection (AD) is evolving through algorithms capable of identifying outliers in complex datasets. The Isolation Forest (IF), a pivotal AD technique, exhibits adaptability limitations and biases. This paper introduces the Function-based Isolation Forest (FuBIF), a generalization of IF that enables the use of real-valued functions for dataset branching, significantly enhancing the flexibility of evaluation tree construction. Complementing this, the FuBIF Feature Importance (FuBIFFI) algorithm extends the interpretability in IF-based approaches by providing feature importance scores across possible FuBIF models. This paper details the operational framework of FuBIF, evaluates its performance against established methods, and explores its theoretical contributions. An open-source implementation is provided to encourage further research and ensure reproducibility.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 49,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06072",
        "abs_url": "https://arxiv.org/abs/2511.06072",
        "pdf_url": "https://arxiv.org/pdf/2511.06072",
        "title": "CatBack: Universal Backdoor Attacks on Tabular Data via Categorical Encoding",
        "authors": [
            "Behrad Tajalli",
            "Stefanos Koffas",
            "Stjepan Picek"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Cryptography and Security (cs.CR)",
        "abstract": "Backdoor attacks in machine learning have drawn significant attention for their potential to compromise models stealthily, yet most research has focused on homogeneous data such as images. In this work, we propose a novel backdoor attack on tabular data, which is particularly challenging due to the presence of both numerical and categorical features. Our key idea is a novel technique to convert categorical values into floating-point representations. This approach preserves enough information to maintain clean-model accuracy compared to traditional methods like one-hot or ordinal encoding. By doing this, we create a gradient-based universal perturbation that applies to all features, including categorical ones. We evaluate our method on five datasets and four popular models. Our results show up to a 100% attack success rate in both white-box and black-box settings (including real-world applications like Vertex AI), revealing a severe vulnerability for tabular data. Our method is shown to surpass the previous works like Tabdoor in terms of performance, while remaining stealthy against state-of-the-art defense mechanisms. We evaluate our attack against Spectral Signatures, Neural Cleanse, Beatrix, and Fine-Pruning, all of which fail to defend successfully against it. We also verify that our attack successfully bypasses popular outlier detection mechanisms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 50,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06077",
        "abs_url": "https://arxiv.org/abs/2511.06077",
        "pdf_url": "https://arxiv.org/pdf/2511.06077",
        "title": "Make It Long, Keep It Fast: End-to-End 10k-Sequence Modeling at Billion Scale on Douyin",
        "authors": [
            "Lin Guan",
            "Jia-Qi Yang",
            "Zhishan Zhao",
            "Beichuan Zhang",
            "Bo Sun",
            "Xuanyuan Luo",
            "Jinan Ni",
            "Xiaowen Li",
            "Yuhang Qi",
            "Zhifang Fan",
            "Hangyu Wang",
            "Qiwei Chen",
            "Yi Cheng",
            "Feng Zhang",
            "Xiao Yang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Information Retrieval (cs.IR)",
        "abstract": "Short-video recommenders such as Douyin must exploit extremely long user histories without breaking latency or cost budgets. We present an end-to-end system that scales long-sequence modeling to 10k-length histories in production. First, we introduce Stacked Target-to-History Cross Attention (STCA), which replaces history self-attention with stacked cross-attention from the target to the history, reducing complexity from quadratic to linear in sequence length and enabling efficient end-to-end training. Second, we propose Request Level Batching (RLB), a user-centric batching scheme that aggregates multiple targets for the same user/request to share the user-side encoding, substantially lowering sequence-related storage, communication, and compute without changing the learning objective. Third, we design a length-extrapolative training strategy -- train on shorter windows, infer on much longer ones -- so the model generalizes to 10k histories without additional training cost. Across offline and online experiments, we observe predictable, monotonic gains as we scale history length and model capacity, mirroring the scaling law behavior observed in large language models. Deployed at full traffic on Douyin, our system delivers significant improvements on key engagement metrics while meeting production latency, demonstrating a practical path to scaling end-to-end long-sequence recommendation to the 10k regime.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 51,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06083",
        "abs_url": "https://arxiv.org/abs/2511.06083",
        "pdf_url": "https://arxiv.org/pdf/2511.06083",
        "title": "Event-driven physics-informed operator learning for reliability analysis",
        "authors": [
            "Shailesh Garg",
            "Souvik Chakraborty"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Reliability analysis of engineering systems under uncertainty poses significant computational challenges, particularly for problems involving high-dimensional stochastic inputs, nonlinear system responses, and multiphysics couplings. Traditional surrogate modeling approaches often incur high energy consumption, which severely limits their scalability and deployability in resource-constrained environments. We introduce NeuroPOL, \\textit{the first neuroscience-inspired physics-informed operator learning framework} for reliability analysis. NeuroPOL incorporates Variable Spiking Neurons into a physics-informed operator architecture, replacing continuous activations with event-driven spiking dynamics. This innovation promotes sparse communication, significantly reduces computational load, and enables an energy-efficient surrogate model. The proposed framework lowers both computational and power demands, supporting real-time reliability assessment and deployment on edge devices and digital twins. By embedding governing physical laws into operator learning, NeuroPOL builds physics-consistent surrogates capable of accurate uncertainty propagation and efficient failure probability estimation, even for high-dimensional problems. We evaluate NeuroPOL on five canonical benchmarks, the Burgers equation, Nagumo equation, two-dimensional Poisson equation, two-dimensional Darcy equation, and incompressible Navier-Stokes equation with energy coupling. Results show that NeuroPOL achieves reliability measures comparable to standard physics-informed operators, while introducing significant communication sparsity, enabling scalable, distributed, and energy-efficient deployment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 52,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06094",
        "abs_url": "https://arxiv.org/abs/2511.06094",
        "pdf_url": "https://arxiv.org/pdf/2511.06094",
        "title": "Approximating Shapley Explanations in Reinforcement Learning",
        "authors": [
            "Daniel Beechey",
            "ÃzgÃ¼r ÅimÅek"
        ],
        "comments": "Camera-ready version. Published at the Conference on Neural Information Processing Systems (NeurIPS 2025)",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Reinforcement learning has achieved remarkable success in complex decision-making environments, yet its lack of transparency limits its deployment in practice, especially in safety-critical settings. Shapley values from cooperative game theory provide a principled framework for explaining reinforcement learning; however, the computational cost of Shapley explanations is an obstacle to their use. We introduce FastSVERL, a scalable method for explaining reinforcement learning by approximating Shapley values. FastSVERL is designed to handle the unique challenges of reinforcement learning, including temporal dependencies across multi-step trajectories, learning from off-policy data, and adapting to evolving agent behaviours in real time. FastSVERL introduces a practical, scalable approach for principled and rigorous interpretability in reinforcement learning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 53,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06111",
        "abs_url": "https://arxiv.org/abs/2511.06111",
        "pdf_url": "https://arxiv.org/pdf/2511.06111",
        "title": "Guardian-regularized Safe Offline Reinforcement Learning for Smart Weaning of Mechanical Circulatory Devices",
        "authors": [
            "Aysin Tumay",
            "Sophia Sun",
            "Sonia Fereidooni",
            "Aaron Dumas",
            "Elise Jortberg",
            "Rose Yu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We study the sequential decision-making problem for automated weaning of mechanical circulatory support (MCS) devices in cardiogenic shock patients. MCS devices are percutaneous micro-axial flow pumps that provide left ventricular unloading and forward blood flow, but current weaning strategies vary significantly across care teams and lack data-driven approaches. Offline reinforcement learning (RL) has proven to be successful in sequential decision-making tasks, but our setting presents challenges for training and evaluating traditional offline RL methods: prohibition of online patient interaction, highly uncertain circulatory dynamics due to concurrent treatments, and limited data availability. We developed an end-to-end machine learning framework with two key contributions (1) Clinically-aware OOD-regularized Model-based Policy Optimization (CORMPO), a density-regularized offline RL algorithm for out-of-distribution suppression that also incorporates clinically-informed reward shaping and (2) a Transformer-based probabilistic digital twin that models MCS circulatory dynamics for policy evaluation with rich physiological and clinical metrics. We prove that \\textsf{CORMPO} achieves theoretical performance guarantees under mild assumptions. CORMPO attains a higher reward than the offline RL baselines by 28% and higher scores in clinical metrics by 82.6% on real and synthetic datasets. Our approach offers a principled framework for safe offline policy learning in high-stakes medical applications where domain expertise and safety constraints are essential.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 54,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06132",
        "abs_url": "https://arxiv.org/abs/2511.06132",
        "pdf_url": "https://arxiv.org/pdf/2511.06132",
        "title": "On the Convergence and Stability of Distributed Sub-model Training",
        "authors": [
            "Yuyang Deng",
            "Fuli Qiao",
            "Mehrdad Mahdavi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "As learning models continue to grow in size, enabling on-device local training of these models has emerged as a critical challenge in federated learning. A popular solution is sub-model training, where the server only distributes randomly sampled sub-models to the edge clients, and clients only update these small models. However, those random sampling of sub-models may not give satisfying convergence performance. In this paper, observing the success of SGD with shuffling, we propose a distributed shuffled sub-model training, where the full model is partitioned into several sub-models in advance, and the server shuffles those sub-models, sends each of them to clients at each round, and by the end of local updating period, clients send back the updated sub-models, and server averages them. We establish the convergence rate of this algorithm. We also study the generalization of distributed sub-model training via stability analysis, and find that the sub-model training can improve the generalization via amplifying the stability of training process. The extensive experiments also validate our theoretical findings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 55,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06143",
        "abs_url": "https://arxiv.org/abs/2511.06143",
        "pdf_url": "https://arxiv.org/pdf/2511.06143",
        "title": "Enhancing Robustness of Graph Neural Networks through p-Laplacian",
        "authors": [
            "Anuj Kumar Sirohi",
            "Subhanu Halder",
            "Kabir Kumar",
            "Sandeep Kumar"
        ],
        "comments": "Accepted at 5th Workshop on Graphs and more Complex Structures For Learning and Reasoning (GCLR), The 40th AAAI Conference on Artificial Intelligence (AAAI-26)",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "With the increase of data in day-to-day life, businesses and different stakeholders need to analyze the data for better pre- dictions. Traditionally, relational data has been a source of various insights, but with the increase in computational power and the need to understand deeper relationships between en- tities, the need to design new techniques has arisen. For this graph data analysis has become an extraordinary tool for un- derstanding the data, which reveals more realistic and flexible modelling of complex relationships. Recently, Graph Neural Networks (GNNs) have shown great promise in various ap- plications, such as social network analysis, recommendation systems, drug discovery, and more. However, many adversar- ial attacks can happen over the data, whether during training (poisoning attack) or during testing (evasion attack), which can adversely manipulate the desired outcome from the GNN model. Therefore, it is crucial to make the GNNs robust to such attacks. The existing robustness methods are computa- tionally demanding and perform poorly when the intensity of attack increases. This paper presents a computationally ef- ficient framework, namely, pLAPGNN, based on weighted p-Laplacian for making GNNs robust. Empirical evaluation on real datasets establishes the efficacy and efficiency of the proposed method.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 56,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06161",
        "abs_url": "https://arxiv.org/abs/2511.06161",
        "pdf_url": "https://arxiv.org/pdf/2511.06161",
        "title": "LLM Attention Transplant for Transfer Learning of Tabular Data Across Disparate Domains",
        "authors": [
            "Ibna Kowsar",
            "Kazi F. Akhter",
            "Manar D. Samad"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Transfer learning of tabular data is non-trivial due to heterogeneity in the feature space across disparate domains. The limited success of traditional deep learning in tabular knowledge transfer can be advanced by leveraging large language models (LLMs). However, the efficacy of LLMs often stagnates for mixed data types structured in tables due to the limitations of text prompts and in-context learning. We propose a lightweight transfer learning framework that fine-tunes an LLM using source tabular data and transplants the LLM's selective $key$ and $value$ projection weights into a gated feature tokenized transformer (gFTT) built for tabular data. The gFTT model with cross-domain attention is fine-tuned using target tabular data for transfer learning, eliminating the need for shared features, LLM prompt engineering, and large-scale pretrained models. Our experiments using ten pairs of source-target data sets and 12 baselines demonstrate the superiority of the proposed LLM-attention transplant for transfer learning (LATTLE) method over traditional ML models, state-of-the-art deep tabular architectures, and transfer learning models trained on thousands to billions of tabular samples. The proposed attention transfer demonstrates an effective solution to learning relationships between data tables using an LLM in a low-resource learning environment. The source code for the proposed method is publicly available.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 57,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06164",
        "abs_url": "https://arxiv.org/abs/2511.06164",
        "pdf_url": "https://arxiv.org/pdf/2511.06164",
        "title": "Learning Gaussian DAG Models without Condition Number Bounds",
        "authors": [
            "Constantinos Daskalakis",
            "Vardis Kandiros",
            "Rui Yao"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We study the problem of learning the topology of a directed Gaussian Graphical Model under the equal-variance assumption, where the graph has $n$ nodes and maximum in-degree $d$. Prior work has established that $O(d \\log n)$ samples are sufficient for this task. However, an important factor that is often overlooked in these analyses is the dependence on the condition number of the covariance matrix of the model. Indeed, all algorithms from prior work require a number of samples that grows polynomially with this condition number. In many cases this is unsatisfactory, since the condition number could grow polynomially with $n$, rendering these prior approaches impractical in high-dimensional settings. In this work, we provide an algorithm that recovers the underlying graph and prove that the number of samples required is independent of the condition number. Furthermore, we establish lower bounds that nearly match the upper bound up to a $d$-factor, thus providing an almost tight characterization of the true sample complexity of the problem. Moreover, under a further assumption that all the variances of the variables are bounded, we design a polynomial-time algorithm that recovers the underlying graph, at the cost of an additional polynomial dependence of the sample complexity on $d$. We complement our theoretical findings with simulations on synthetic datasets that confirm our predictions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 58,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06169",
        "abs_url": "https://arxiv.org/abs/2511.06169",
        "pdf_url": "https://arxiv.org/pdf/2511.06169",
        "title": "Local K-Similarity Constraint for Federated Learning with Label Noise",
        "authors": [
            "Sanskar Amgain",
            "Prashant Shrestha",
            "Bidur Khanal",
            "Alina Devkota",
            "Yash Raj Shrestha",
            "Seungryul Baek",
            "Prashnna Gyawali",
            "Binod Bhattarai"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Federated learning on clients with noisy labels is a challenging problem, as such clients can infiltrate the global model, impacting the overall generalizability of the system. Existing methods proposed to handle noisy clients assume that a sufficient number of clients with clean labels are available, which can be leveraged to learn a robust global model while dampening the impact of noisy clients. This assumption fails when a high number of heterogeneous clients contain noisy labels, making the existing approaches ineffective. In such scenarios, it is important to locally regularize the clients before communication with the global model, to ensure the global model isn't corrupted by noisy clients. While pre-trained self-supervised models can be effective for local regularization, existing centralized approaches relying on pretrained initialization are impractical in a federated setting due to the potentially large size of these models, which increases communication costs. In that line, we propose a regularization objective for client models that decouples the pre-trained and classification models by enforcing similarity between close data points within the client. We leverage the representation space of a self-supervised pretrained model to evaluate the closeness among examples. This regularization, when applied with the standard objective function for the downstream task in standard noisy federated settings, significantly improves performance, outperforming existing state-of-the-art federated methods in multiple computer vision and medical image classification benchmarks. Unlike other techniques that rely on self-supervised pretrained initialization, our method does not require the pretrained model and classifier backbone to share the same architecture, making it architecture-agnostic.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 59,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06211",
        "abs_url": "https://arxiv.org/abs/2511.06211",
        "pdf_url": "https://arxiv.org/pdf/2511.06211",
        "title": "Sparse Linear Regression is Easy on Random Supports",
        "authors": [
            "Gautam Chandrasekaran",
            "Raghu Meka",
            "Konstantinos Stavropoulos"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Data Structures and Algorithms (cs.DS); Statistics Theory (math.ST); Machine Learning (stat.ML)",
        "abstract": "Sparse linear regression is one of the most basic questions in machine learning and statistics. Here, we are given as input a design matrix $X \\in \\mathbb{R}^{N \\times d}$ and measurements or labels ${y} \\in \\mathbb{R}^N$ where ${y} = {X} {w}^* + {\\xi}$, and ${\\xi}$ is the noise in the measurements. Importantly, we have the additional constraint that the unknown signal vector ${w}^*$ is sparse: it has $k$ non-zero entries where $k$ is much smaller than the ambient dimension. Our goal is to output a prediction vector $\\widehat{w}$ that has small prediction error: $\\frac{1}{N}\\cdot \\|{X} {w}^* - {X} \\widehat{w}\\|^2_2$. Information-theoretically, we know what is best possible in terms of measurements: under most natural noise distributions, we can get prediction error at most $\\epsilon$ with roughly $N = O(k \\log d/\\epsilon)$ samples. Computationally, this currently needs $d^{\\Omega(k)}$ run-time. Alternately, with $N = O(d)$, we can get polynomial-time. Thus, there is an exponential gap (in the dependence on $d$) between the two and we do not know if it is possible to get $d^{o(k)}$ run-time and $o(d)$ samples. We give the first generic positive result for worst-case design matrices ${X}$: For any ${X}$, we show that if the support of ${w}^*$ is chosen at random, we can get prediction error $\\epsilon$ with $N = \\text{poly}(k, \\log d, 1/\\epsilon)$ samples and run-time $\\text{poly}(d,N)$. This run-time holds for any design matrix ${X}$ with condition number up to $2^{\\text{poly}(d)}$. Previously, such results were known for worst-case ${w}^*$, but only for random design matrices from well-behaved families, matrices that have a very low condition number ($\\text{poly}(\\log d)$; e.g., as studied in compressed sensing), or those with special structural properties.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 60,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06216",
        "abs_url": "https://arxiv.org/abs/2511.06216",
        "pdf_url": "https://arxiv.org/pdf/2511.06216",
        "title": "Adaptive Multi-view Graph Contrastive Learning via Fractional-order Neural Diffusion Networks",
        "authors": [
            "Yanan Zhao",
            "Feng Ji",
            "Jingyang Dai",
            "Jiaze Ma",
            "Keyue Jiang",
            "Kai Zhao",
            "Wee Peng Tay"
        ],
        "comments": "Submitted to TPAMI",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Graph contrastive learning (GCL) learns node and graph representations by contrasting multiple views of the same graph. Existing methods typically rely on fixed, handcrafted views-usually a local and a global perspective, which limits their ability to capture multi-scale structural patterns. We present an augmentation-free, multi-view GCL framework grounded in fractional-order continuous dynamics. By varying the fractional derivative order $\\alpha \\in (0,1]$, our encoders produce a continuous spectrum of views: small $\\alpha$ yields localized features, while large $\\alpha$ induces broader, global aggregation. We treat $\\alpha$ as a learnable parameter so the model can adapt diffusion scales to the data and automatically discover informative views. This principled approach generates diverse, complementary representations without manual augmentations. Extensive experiments on standard benchmarks demonstrate that our method produces more robust and expressive embeddings and outperforms state-of-the-art GCL baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 61,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06229",
        "abs_url": "https://arxiv.org/abs/2511.06229",
        "pdf_url": "https://arxiv.org/pdf/2511.06229",
        "title": "Deep Reinforcement Learning for Dynamic Origin-Destination Matrix Estimation in Microscopic Traffic Simulations Considering Credit Assignment",
        "authors": [
            "Donggyu Min",
            "Seongjin Choi",
            "Dong-Kyu Kim"
        ],
        "comments": "11 pages, 10 figures, 3 tables",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "This paper focuses on dynamic origin-destination matrix estimation (DODE), a crucial calibration process necessary for the effective application of microscopic traffic simulations. The fundamental challenge of the DODE problem in microscopic simulations stems from the complex temporal dynamics and inherent uncertainty of individual vehicle dynamics. This makes it highly challenging to precisely determine which vehicle traverses which link at any given moment, resulting in intricate and often ambiguous relationships between origin-destination (OD) matrices and their contributions to resultant link flows. This phenomenon constitutes the credit assignment problem, a central challenge addressed in this study. We formulate the DODE problem as a Markov Decision Process (MDP) and propose a novel framework that applies model-free deep reinforcement learning (DRL). Within our proposed framework, the agent learns an optimal policy to sequentially generate OD matrices, refining its strategy through direct interaction with the simulation environment. The proposed method is validated on the Nguyen-Dupuis network using SUMO, where its performance is evaluated against ground-truth link flows aggregated at 5-minute intervals over a 30-minute horizon. Experimental results demonstrate that our approach achieves a 43.2% reduction in mean squared error (MSE) compared to the best-performing conventional baseline. By reframing DODE as a sequential decision-making problem, our approach addresses the credit assignment challenge through its learned policy, thereby overcoming the limitations of conventional methods and proposing a novel framework for calibration of microscopic traffic simulations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 62,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06231",
        "abs_url": "https://arxiv.org/abs/2511.06231",
        "pdf_url": "https://arxiv.org/pdf/2511.06231",
        "title": "Synheart Emotion: Privacy-Preserving On-Device Emotion Recognition from Biosignals",
        "authors": [
            "Henok Ademtew",
            "Israel Goytom"
        ],
        "comments": "Preprint submitted to the Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Human-computer interaction increasingly demands systems that recognize not only explicit user inputs but also implicit emotional states. While substantial progress has been made in affective computing, most emotion recognition systems rely on cloud-based inference, introducing privacy vulnerabilities and latency constraints unsuitable for real-time applications. This work presents a comprehensive evaluation of machine learning architectures for on-device emotion recognition from wrist-based photoplethysmography (PPG), systematically comparing different models spanning classical ensemble methods, deep neural networks, and transformers on the WESAD stress detection dataset. Results demonstrate that classical ensemble methods substantially outperform deep learning on small physiological datasets, with ExtraTrees achieving F1 = 0.826 on combined features and F1 = 0.623 on wrist-only features, compared to transformers achieving only F1 = 0.509-0.577. We deploy the wrist-only ExtraTrees model optimized via ONNX conversion, achieving a 4.08 MB footprint, 0.05 ms inference latency, and 152x speedup over the original implementation. Furthermore, ONNX optimization yields a 30.5% average storage reduction and 40.1x inference speedup, highlighting the feasibility of privacy-preserving on-device emotion recognition for real-world wearables.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 63,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06237",
        "abs_url": "https://arxiv.org/abs/2511.06237",
        "pdf_url": "https://arxiv.org/pdf/2511.06237",
        "title": "Mixtures of SubExperts for Large Language Continual Learning",
        "authors": [
            "Haeyong Kang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)",
        "abstract": "Adapting Large Language Models (LLMs) to a continuous stream of tasks is a critical yet challenging endeavor. While Parameter-Efficient Fine-Tuning (PEFT) methods have become a standard for this, they face a fundamental dilemma in continual learning. Reusing a single set of PEFT parameters for new tasks often leads to catastrophic forgetting of prior knowledge. Conversely, allocating distinct parameters for each task prevents forgetting but results in a linear growth of the model's size and fails to facilitate knowledge transfer between related tasks. To overcome these limitations, we propose a novel adaptive PEFT method referred to as \\textit{Mixtures of SubExperts (MoSEs)}, a novel continual learning framework designed for minimal forgetting and efficient scalability. MoSEs integrate a sparse Mixture of SubExperts into the transformer layers, governed by a task-specific routing mechanism. This architecture allows the model to isolate and protect knowledge within dedicated SubExperts, thereby minimizing parameter interference and catastrophic forgetting. Crucially, the router can adaptively select and combine previously learned sparse parameters for new tasks, enabling effective knowledge transfer while ensuring that the model's capacity grows sublinearly. We evaluate MoSEs on the comprehensive TRACE benchmark datasets. Our experiments demonstrate that MoSEs significantly outperform conventional continual learning approaches in both knowledge retention and scalability to new tasks, achieving state-of-the-art performance with substantial memory and computational savings.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 64,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06250",
        "abs_url": "https://arxiv.org/abs/2511.06250",
        "pdf_url": "https://arxiv.org/pdf/2511.06250",
        "title": "Test-Time Iterative Error Correction for Efficient Diffusion Models",
        "authors": [
            "Yunshan Zhong",
            "Yanwei Qi",
            "Yuxin Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "With the growing demand for high-quality image generation on resource-constrained devices, efficient diffusion models have received increasing attention. However, such models suffer from approximation errors introduced by efficiency techniques, which significantly degrade generation quality. Once deployed, these errors are difficult to correct, as modifying the model is typically infeasible in deployment environments. Through an analysis of error propagation across diffusion timesteps, we reveal that these approximation errors can accumulate exponentially, severely impairing output quality. Motivated by this insight, we propose Iterative Error Correction (IEC), a novel test-time method that mitigates inference-time errors by iteratively refining the model's output. IEC is theoretically proven to reduce error propagation from exponential to linear growth, without requiring any retraining or architectural changes. IEC can seamlessly integrate into the inference process of existing diffusion models, enabling a flexible trade-off between performance and efficiency. Extensive experiments show that IEC consistently improves generation quality across various datasets, efficiency techniques, and model architectures, establishing it as a practical and generalizable solution for test-time enhancement of efficient diffusion models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 65,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06259",
        "abs_url": "https://arxiv.org/abs/2511.06259",
        "pdf_url": "https://arxiv.org/pdf/2511.06259",
        "title": "Breaking the Modality Barrier: Generative Modeling for Accurate Molecule Retrieval from Mass Spectra",
        "authors": [
            "Yiwen Zhang",
            "Keyan Ding",
            "Yihang Wu",
            "Xiang Zhuang",
            "Yi Yang",
            "Qiang Zhang",
            "Huajun Chen"
        ],
        "comments": "Accepted by AAAI 2026",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Retrieving molecular structures from tandem mass spectra is a crucial step in rapid compound identification. Existing retrieval methods, such as traditional mass spectral library matching, suffer from limited spectral library coverage, while recent cross-modal representation learning frameworks often encounter modality misalignment, resulting in suboptimal retrieval accuracy and generalization. To address these limitations, we propose GLMR, a Generative Language Model-based Retrieval framework that mitigates the cross-modal misalignment through a two-stage process. In the pre-retrieval stage, a contrastive learning-based model identifies top candidate molecules as contextual priors for the input mass spectrum. In the generative retrieval stage, these candidate molecules are integrated with the input mass spectrum to guide a generative model in producing refined molecular structures, which are then used to re-rank the candidates based on molecular similarity. Experiments on both MassSpecGym and the proposed MassRET-20k dataset demonstrate that GLMR significantly outperforms existing methods, achieving over 40% improvement in top-1 accuracy and exhibiting strong generalizability.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 66,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06269",
        "abs_url": "https://arxiv.org/abs/2511.06269",
        "pdf_url": "https://arxiv.org/pdf/2511.06269",
        "title": "LLM$^3$-DTI: A Large Language Model and Multi-modal data co-powered framework for Drug-Target Interaction prediction",
        "authors": [
            "Yuhao Zhang",
            "Qinghong Guo",
            "Qixian Chen",
            "Liuwei Zhang",
            "Hongyan Cui",
            "Xiyi Chen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)",
        "abstract": "Drug-target interaction (DTI) prediction is of great significance for drug discovery and drug repurposing. With the accumulation of a large volume of valuable data, data-driven methods have been increasingly harnessed to predict DTIs, reducing costs across various dimensions. Therefore, this paper proposes a $\\textbf{L}$arge $\\textbf{L}$anguage $\\textbf{M}$odel and $\\textbf{M}$ulti-$\\textbf{M}$odel data co-powered $\\textbf{D}$rug $\\textbf{T}$arget $\\textbf{I}$nteraction prediction framework, named LLM$^3$-DTI. LLM$^3$-DTI constructs multi-modal data embedding to enhance DTI prediction performance. In this framework, the text semantic embeddings of drugs and targets are encoded by a domain-specific LLM. To effectively align and fuse multi-modal embedding. We propose the dual cross-attention mechanism and the TSFusion module. Finally, these multi-modal data are utilized for the DTI task through an output network. The experimental results indicate that LLM$^3$-DTI can proficiently identify validated DTIs, surpassing the performance of the models employed for comparison across diverse scenarios. Consequently, LLM$^3$-DTI is adept at fulfilling the task of DTI prediction with excellence. The data and code are available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 67,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06273",
        "abs_url": "https://arxiv.org/abs/2511.06273",
        "pdf_url": "https://arxiv.org/pdf/2511.06273",
        "title": "COTN: A Chaotic Oscillatory Transformer Network for Complex Volatile Systems under Extreme Conditions",
        "authors": [
            "Boyan Tang",
            "Yilong Zeng",
            "Xuanhao Ren",
            "Peng Xiao",
            "Yuhan Zhao",
            "Raymond Lee",
            "Jianghua Wu"
        ],
        "comments": "Submitted to IEEE Transactions on Neural Networks and Learning Systems",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Accurate prediction of financial and electricity markets, especially under extreme conditions, remains a significant challenge due to their intrinsic nonlinearity, rapid fluctuations, and chaotic patterns. To address these limitations, we propose the Chaotic Oscillatory Transformer Network (COTN). COTN innovatively combines a Transformer architecture with a novel Lee Oscillator activation function, processed through Max-over-Time pooling and a lambda-gating mechanism. This design is specifically tailored to effectively capture chaotic dynamics and improve responsiveness during periods of heightened volatility, where conventional activation functions (e.g., ReLU, GELU) tend to saturate. Furthermore, COTN incorporates an Autoencoder Self-Regressive (ASR) module to detect and isolate abnormal market patterns, such as sudden price spikes or crashes, thereby preventing corruption of the core prediction process and enhancing robustness. Extensive experiments across electricity spot markets and financial markets demonstrate the practical applicability and resilience of COTN. Our approach outperforms state-of-the-art deep learning models like Informer by up to 17% and traditional statistical methods like GARCH by as much as 40%. These results underscore COTN's effectiveness in navigating real-world market uncertainty and complexity, offering a powerful tool for forecasting highly volatile systems under duress.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 68,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06293",
        "abs_url": "https://arxiv.org/abs/2511.06293",
        "pdf_url": "https://arxiv.org/pdf/2511.06293",
        "title": "Achieving Fairness Without Harm via Selective Demographic Experts",
        "authors": [
            "Xuwei Tan",
            "Yuanlong Wang",
            "Thai-Hoang Pham",
            "Ping Zhang",
            "Xueru Zhang"
        ],
        "comments": "AAAI26; Extended version",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "As machine learning systems become increasingly integrated into human-centered domains such as healthcare, ensuring fairness while maintaining high predictive performance is critical. Existing bias mitigation techniques often impose a trade-off between fairness and accuracy, inadvertently degrading performance for certain demographic groups. In high-stakes domains like clinical diagnosis, such trade-offs are ethically and practically unacceptable. In this study, we propose a fairness-without-harm approach by learning distinct representations for different demographic groups and selectively applying demographic experts consisting of group-specific representations and personalized classifiers through a no-harm constrained selection. We evaluate our approach on three real-world medical datasets -- covering eye disease, skin cancer, and X-ray diagnosis -- as well as two face datasets. Extensive empirical results demonstrate the effectiveness of our approach in achieving fairness without harm.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 69,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06300",
        "abs_url": "https://arxiv.org/abs/2511.06300",
        "pdf_url": "https://arxiv.org/pdf/2511.06300",
        "title": "3dSAGER: Geospatial Entity Resolution over 3D Objects (Technical Report)",
        "authors": [
            "Bar Genossar",
            "Sagi Dalyot",
            "Roee Shraga",
            "Avigdor Gal"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Urban environments are continuously mapped and modeled by various data collection platforms, including satellites, unmanned aerial vehicles and street cameras. The growing availability of 3D geospatial data from multiple modalities has introduced new opportunities and challenges for integrating spatial knowledge at scale, particularly in high-impact domains such as urban planning and rapid disaster management. Geospatial entity resolution is the task of identifying matching spatial objects across different datasets, often collected independently under varying conditions. Existing approaches typically rely on spatial proximity, textual metadata, or external identifiers to determine correspondence. While useful, these signals are often unavailable, unreliable, or misaligned, especially in cross-source scenarios. To address these limitations, we shift the focus to the intrinsic geometry of 3D spatial objects and present 3dSAGER (3D Spatial-Aware Geospatial Entity Resolution), an end-to-end pipeline for geospatial entity resolution over 3D objects. 3dSAGER introduces a novel, spatial-reference-independent featurization mechanism that captures intricate geometric characteristics of matching pairs, enabling robust comparison even across datasets with incompatible coordinate systems where traditional spatial methods fail. As a key component of 3dSAGER, we also propose a new lightweight and interpretable blocking method, BKAFI, that leverages a trained model to efficiently generate high-recall candidate sets. We validate 3dSAGER through extensive experiments on real-world urban datasets, demonstrating significant gains in both accuracy and efficiency over strong baselines. Our empirical study further dissects the contributions of each component, providing insights into their impact and the overall design choices.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 70,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06307",
        "abs_url": "https://arxiv.org/abs/2511.06307",
        "pdf_url": "https://arxiv.org/pdf/2511.06307",
        "title": "DRIVE: Data Curation Best Practices for Reinforcement Learning with Verifiable Reward in Competitive Code Generation",
        "authors": [
            "Speed Zhu",
            "Jianwei Cai",
            "Guang Chen",
            "Lulu Wu",
            "Saiyong Yang",
            "Wiggin Zhou"
        ],
        "comments": "15 pages, 8 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Recent reasoning-first models (e.g., OpenAI o1, DeepSeek R1) have spurred a resurgence of interest in RLVR. Nevertheless, advances are dominated by mathematics (e.g., AIME), with competitive-programming code generation underexplored and data curation receiving less attention than RL algorithm design. We investigate how to construct RLVR datasets (i.e., RL prompts) and present practical training techniques that yield strong performance on competitive-programming code generation. Our pipeline begins with supervised fine-tuning (SFT) distilled from strong open-source models, augmented with general-purpose and reasoning-intensive data. RL then follows a two-stage process with executable, testcase-driven rewards: first, training on a large, uniformly distributed set of competitive-programming problems using Group Relative Policy Optimization (GRPO) with 8 rollouts per prompt and a relatively short response-generation window (e.g., 32k during SFT and 24k in this stage) to expand entropy and mitigate repetition and truncation; second, we perform \\textbf{Pre-GRPO}: updating on a small, high-quality set of challenging problems with a large rollout budget (64 rollouts per prompt) under a hard-focus curriculum that continuously retains the most difficult instances throughout training. We implement our method on Qwen2.5-32B and evaluate on LeetCode and Codeforces weekly contests to avoid data leakage. The resulting model achieves state-of-the-art performance among models of similar scale and is comparable to leading systems such as DeepSeek v3.1 and Doubao-1.5-Thinking. We also examine scaling trends and observe strong RL scaling on an internal large-scale MoE model. Our study distills concise best practices for data curation, entropy expansion, and curriculum design in RLVR for competitive-programming code generation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 71,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06341",
        "abs_url": "https://arxiv.org/abs/2511.06341",
        "pdf_url": "https://arxiv.org/pdf/2511.06341",
        "title": "Scalable Verification of Neural Control Barrier Functions Using Linear Bound Propagation",
        "authors": [
            "Nikolaus Vertovec",
            "Frederik Baymler Mathiesen",
            "Thom Badings",
            "Luca Laurenti",
            "Alessandro Abate"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Robotics (cs.RO); Systems and Control (eess.SY); Optimization and Control (math.OC)",
        "abstract": "Control barrier functions (CBFs) are a popular tool for safety certification of nonlinear dynamical control systems. Recently, CBFs represented as neural networks have shown great promise due to their expressiveness and applicability to a broad class of dynamics and safety constraints. However, verifying that a trained neural network is indeed a valid CBF is a computational bottleneck that limits the size of the networks that can be used. To overcome this limitation, we present a novel framework for verifying neural CBFs based on piecewise linear upper and lower bounds on the conditions required for a neural network to be a CBF. Our approach is rooted in linear bound propagation (LBP) for neural networks, which we extend to compute bounds on the gradients of the network. Combined with McCormick relaxation, we derive linear upper and lower bounds on the CBF conditions, thereby eliminating the need for computationally expensive verification procedures. Our approach applies to arbitrary control-affine systems and a broad range of nonlinear activation functions. To reduce conservatism, we develop a parallelizable refinement strategy that adaptively refines the regions over which these bounds are computed. Our approach scales to larger neural networks than state-of-the-art verification procedures for CBFs, as demonstrated by our numerical experiments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 72,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06356",
        "abs_url": "https://arxiv.org/abs/2511.06356",
        "pdf_url": "https://arxiv.org/pdf/2511.06356",
        "title": "Reaction Prediction via Interaction Modeling of Symmetric Difference Shingle Sets",
        "authors": [
            "Runhan Shi",
            "Letian Chen",
            "Gufeng Yu",
            "Yang Yang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Biomolecules (q-bio.BM)",
        "abstract": "Chemical reaction prediction remains a fundamental challenge in organic chemistry, where existing machine learning models face two critical limitations: sensitivity to input permutations (molecule/atom orderings) and inadequate modeling of substructural interactions governing reactivity. These shortcomings lead to inconsistent predictions and poor generalization to real-world scenarios. To address these challenges, we propose ReaDISH, a novel reaction prediction model that learns permutation-invariant representations while incorporating interaction-aware features. It introduces two innovations: (1) symmetric difference shingle encoding, which computes molecular shingle differences to capture reaction-specific structural changes while eliminating order sensitivity; and (2) geometry-structure interaction attention, a mechanism that models intra- and inter-molecular interactions at the shingle level. Extensive experiments demonstrate that ReaDISH improves reaction prediction performance across diverse benchmarks. It shows enhanced robustness with an average improvement of 8.76% on R$^2$ under permutation perturbations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 73,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06374",
        "abs_url": "https://arxiv.org/abs/2511.06374",
        "pdf_url": "https://arxiv.org/pdf/2511.06374",
        "title": "Adaptive Regularization for Large-Scale Sparse Feature Embedding Models",
        "authors": [
            "Mang Li",
            "Wei Lyu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "The one-epoch overfitting problem has drawn widespread attention, especially in CTR and CVR estimation models in search, advertising, and recommendation domains. These models which rely heavily on large-scale sparse categorical features, often suffer a significant decline in performance when trained for multiple epochs. Although recent studies have proposed heuristic solutions, they have not clearly identified the fundamental cause of this phenomenon. In this work, we provide a theoretical analysis that explains why overfitting occurs in models that use large-scale sparse categorical features. Based on this analysis, we propose an adaptive regularization method to address it. Our approach not only prevents the severe performance degradation observed during multi-epoch training, but also improves model performance within a single epoch. This method has already been deployed in online production systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 74,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06376",
        "abs_url": "https://arxiv.org/abs/2511.06376",
        "pdf_url": "https://arxiv.org/pdf/2511.06376",
        "title": "Vocabulary In-Context Learning in Transformers: Benefits of Positional Encoding",
        "authors": [
            "Qian Ma",
            "Ruoxiang Xu",
            "Yongqiang Cai"
        ],
        "comments": "Accepted as NIPS 2025 poster",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Numerous studies have demonstrated that the Transformer architecture possesses the capability for in-context learning (ICL). In scenarios involving function approximation, context can serve as a control parameter for the model, endowing it with the universal approximation property (UAP). In practice, context is represented by tokens from a finite set, referred to as a vocabulary, which is the case considered in this paper, \\emph{i.e.}, vocabulary in-context learning (VICL). We demonstrate that VICL in single-layer Transformers, without positional encoding, does not possess the UAP; however, it is possible to achieve the UAP when positional encoding is included. Several sufficient conditions for the positional encoding are provided. Our findings reveal the benefits of positional encoding from an approximation theory perspective in the context of ICL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 75,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06430",
        "abs_url": "https://arxiv.org/abs/2511.06430",
        "pdf_url": "https://arxiv.org/pdf/2511.06430",
        "title": "CG-TTRL: Context-Guided Test-Time Reinforcement Learning for On-Device Large Language Models",
        "authors": [
            "Peyman Hosseini",
            "Ondrej Bohdal",
            "Taha Ceritli",
            "Ignacio Castro",
            "Matthew Purver",
            "Mete Ozay",
            "Umberto Michieli"
        ],
        "comments": "12 pages, 7 Figures, 4 Tables",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
        "abstract": "Test-time Reinforcement Learning (TTRL) has shown promise in adapting foundation models for complex tasks at test-time, resulting in large performance improvements. TTRL leverages an elegant two-phase sampling strategy: first, multi-sampling derives a pseudo-label via majority voting, while subsequent downsampling and reward-based fine-tuning encourages the model to explore and learn diverse valid solutions, with the pseudo-label modulating the reward signal. Meanwhile, in-context learning has been widely explored at inference time and demonstrated the ability to enhance model performance without weight updates. However, TTRL's two-phase sampling strategy under-utilizes contextual guidance, which can potentially improve pseudo-label accuracy in the initial exploitation phase while regulating exploration in the second. To address this, we propose context-guided TTRL (CG-TTRL), integrating context dynamically into both sampling phases and propose a method for efficient context selection for on-device applications. Our evaluations on mathematical and scientific QA benchmarks show CG-TTRL outperforms TTRL (e.g. additional 7% relative accuracy improvement over TTRL), while boosting efficiency by obtaining strong performance after only a few steps of test-time training (e.g. 8% relative improvement rather than 1% over TTRL after 3 steps).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 76,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06443",
        "abs_url": "https://arxiv.org/abs/2511.06443",
        "pdf_url": "https://arxiv.org/pdf/2511.06443",
        "title": "How Wide and How Deep? Mitigating Over-Squashing of GNNs via Channel Capacity Constrained Estimation",
        "authors": [
            "Zinuo You",
            "Jin Zheng",
            "John Cartlidge"
        ],
        "comments": "29 pages, 11 figures. Author manuscript accepted for the 40th Annual AAAI Conference on Artificial Intelligence (AAAI-26), January 2026",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Existing graph neural networks typically rely on heuristic choices for hidden dimensions and propagation depths, which often lead to severe information loss during propagation, known as over-squashing. To address this issue, we propose Channel Capacity Constrained Estimation (C3E), a novel framework that formulates the selection of hidden dimensions and depth as a nonlinear programming problem grounded in information theory. Through modeling spectral graph neural networks as communication channels, our approach directly connects channel capacity to hidden dimensions, propagation depth, propagation mechanism, and graph structure. Extensive experiments on nine public datasets demonstrate that hidden dimensions and depths estimated by C3E can mitigate over-squashing and consistently improve representation learning. Experimental results show that over-squashing occurs due to the cumulative compression of information in representation matrices. Furthermore, our findings show that increasing hidden dimensions indeed mitigate information compression, while the role of propagation depth is more nuanced, uncovering a fundamental balance between information compression and representation complexity.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 77,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06451",
        "abs_url": "https://arxiv.org/abs/2511.06451",
        "pdf_url": "https://arxiv.org/pdf/2511.06451",
        "title": "A Risk-Neutral Neural Operator for Arbitrage-Free SPX-VIX Term Structures",
        "authors": [
            "Jian'an Zhang"
        ],
        "comments": "46 pages, 9 figures, includes appendices; v11 draft aligned with final outline",
        "subjects": "Machine Learning (cs.LG); Computational Finance (q-fin.CP)",
        "abstract": "We propose ARBITER, a risk-neutral neural operator for learning joint SPX-VIX term structures under no-arbitrage constraints. ARBITER maps market states to an operator that outputs implied volatility and variance curves while enforcing static arbitrage (calendar, vertical, butterfly), Lipschitz bounds, and monotonicity. The model couples operator learning with constrained decoders and is trained with extragradient-style updates plus projection. We introduce evaluation metrics for derivatives term structures (NAS, CNAS, NI, Dual-Gap, Stability Rate) and show gains over Fourier Neural Operator, DeepONet, and state-space sequence models on historical SPX and VIX data. Ablation studies indicate that tying the SPX and VIX legs reduces Dual-Gap and improves NI, Lipschitz projection stabilizes calibration, and selective state updates improve long-horizon generalization. We provide identifiability and approximation results and describe practical recipes for arbitrage-free interpolation and extrapolation across maturities and strikes.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 78,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06452",
        "abs_url": "https://arxiv.org/abs/2511.06452",
        "pdf_url": "https://arxiv.org/pdf/2511.06452",
        "title": "MULTIBENCH++: A Unified and Comprehensive Multimodal Fusion Benchmarking Across Specialized Domains",
        "authors": [
            "Leyan Xue",
            "Zongbo Han",
            "Kecheng Xue",
            "Xiaohong Liu",
            "Guangyu Wang",
            "Changqing Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Although multimodal fusion has made significant progress, its advancement is severely hindered by the lack of adequate evaluation benchmarks. Current fusion methods are typically evaluated on a small selection of public datasets, a limited scope that inadequately represents the complexity and diversity of real-world scenarios, potentially leading to biased evaluations. This issue presents a twofold challenge. On one hand, models may overfit to the biases of specific datasets, hindering their generalization to broader practical applications. On the other hand, the absence of a unified evaluation standard makes fair and objective comparisons between different fusion methods difficult. Consequently, a truly universal and high-performance fusion model has yet to emerge. To address these challenges, we have developed a large-scale, domain-adaptive benchmark for multimodal evaluation. This benchmark integrates over 30 datasets, encompassing 15 modalities and 20 predictive tasks across key application domains. To complement this, we have also developed an open-source, unified, and automated evaluation pipeline that includes standardized implementations of state-of-the-art models and diverse fusion paradigms. Leveraging this platform, we have conducted large-scale experiments, successfully establishing new performance baselines across multiple tasks. This work provides the academic community with a crucial platform for rigorous and reproducible assessment of multimodal models, aiming to propel the field of multimodal artificial intelligence to new heights.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 79,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06461",
        "abs_url": "https://arxiv.org/abs/2511.06461",
        "pdf_url": "https://arxiv.org/pdf/2511.06461",
        "title": "Reconstruction and Secrecy under Approximate Distance Queries",
        "authors": [
            "Shay Moran",
            "Elizaveta Nesterova"
        ],
        "comments": "39 pages. Conference version: NeurIPS 2025 (Spotlight). Extended appendix included",
        "subjects": "Machine Learning (cs.LG); Information Theory (cs.IT); Metric Geometry (math.MG)",
        "abstract": "Consider the task of locating an unknown target point using approximate distance queries: in each round, a reconstructor selects a query point and receives a noisy version of its distance to the target. This problem arises naturally in various contexts ranging from localization in GPS and sensor networks to privacy-aware data access, and spans a wide variety of metric spaces. It is relevant from the perspective of both the reconstructor (seeking accurate recovery) and the responder (aiming to limit information disclosure, e.g., for privacy or security reasons). We study this reconstruction game through a learning-theoretic lens, focusing on the rate and limits of the best possible reconstruction error. Our first result provides a tight geometric characterization of the optimal error in terms of the Chebyshev radius, a classical concept from geometry. This characterization applies to all compact metric spaces (in fact, even to all totally bounded spaces) and yields explicit formulas for natural metric spaces. Our second result addresses the asymptotic behavior of reconstruction, distinguishing between pseudo-finite spaces -- where the optimal error is attained after finitely many queries -- and spaces where the approximation curve exhibits nontrivial decay. We characterize pseudo-finiteness for convex Euclidean spaces.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 80,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06463",
        "abs_url": "https://arxiv.org/abs/2511.06463",
        "pdf_url": "https://arxiv.org/pdf/2511.06463",
        "title": "Error Estimate and Convergence Analysis for Data Valuation",
        "authors": [
            "Zhangyong Liang",
            "Huanhuan Gao",
            "Ji Zhang"
        ],
        "comments": "7 pages, 1 figure",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Data valuation quantifies data importance, but existing methods cannot ensure validity in a single training process. The neural dynamic data valuation (NDDV) method [3] addresses this limitation. Based on NDDV, we are the first to explore error estimation and convergence analysis in data valuation. Under Lipschitz and smoothness assumptions, we derive quadratic error bounds for loss differences that scale inversely with time steps and quadratically with control variations, ensuring stability. We also prove that the expected squared gradient norm for the training loss vanishes asymptotically, and that the meta loss converges sublinearly over iterations. In particular, NDDV achieves sublinear convergence.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 81,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06477",
        "abs_url": "https://arxiv.org/abs/2511.06477",
        "pdf_url": "https://arxiv.org/pdf/2511.06477",
        "title": "DyKAF: Dynamical Kronecker Approximation of the Fisher Information Matrix for Gradient Preconditioning",
        "authors": [
            "Nikolay Yudin",
            "Ekaterina Grishina",
            "Andrey Veprikov",
            "Alexandr Beznosikov",
            "Maxim Rakhuba"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Numerical Analysis (math.NA); Optimization and Control (math.OC)",
        "abstract": "Recently, optimizers that explicitly treat weights as matrices, rather than flattened vectors, have demonstrated their effectiveness. This perspective naturally leads to structured approximations of the Fisher matrix as preconditioners, where the matrix view induces a Kronecker-factorized form that enables memory-efficient representation. However, constructing such approximations both efficiently and accurately remains an open challenge, since obtaining the optimal factorization is resource-intensive and practical methods therefore rely on heuristic design choices. In this work, we introduce a novel approach that leverages projector-splitting integrators to construct effective preconditioners. Our optimizer, DyKAF (Dynamical Kronecker Approximation of the Fisher Matrix), consistently improves the Fisher matrix approximation quality. Experiments on large language model pre-training and fine-tuning demonstrate that DyKAF outperforms existing optimizers across a range of evaluation metrics.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 82,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06492",
        "abs_url": "https://arxiv.org/abs/2511.06492",
        "pdf_url": "https://arxiv.org/pdf/2511.06492",
        "title": "Explainable AI For Early Detection Of Sepsis",
        "authors": [
            "Atharva Thakur",
            "Shruti Dhumal"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Sepsis is a life-threatening condition that requires rapid detection and treatment to prevent progression to severe sepsis, septic shock, or multi-organ failure. Despite advances in medical technology, it remains a major challenge for clinicians. While recent machine learning models have shown promise in predicting sepsis onset, their black-box nature limits interpretability and clinical trust. In this study, we present an interpretable AI approach for sepsis analysis that integrates machine learning with clinical knowledge. Our method not only delivers accurate predictions of sepsis onset but also enables clinicians to understand, validate, and align model outputs with established medical expertise.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 83,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06493",
        "abs_url": "https://arxiv.org/abs/2511.06493",
        "pdf_url": "https://arxiv.org/pdf/2511.06493",
        "title": "Learning Time-Varying Graph Signals via Koopman",
        "authors": [
            "Sivaram Krishnan",
            "Jinho Choi",
            "Jihong Park"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Signal Processing (eess.SP)",
        "abstract": "A wide variety of real-world data, such as sea measurements, e.g., temperatures collected by distributed sensors and multiple unmanned aerial vehicles (UAV) trajectories, can be naturally represented as graphs, often exhibiting non-Euclidean structures. These graph representations may evolve over time, forming time-varying graphs. Effectively modeling and analyzing such dynamic graph data is critical for tasks like predicting graph evolution and reconstructing missing graph data. In this paper, we propose a framework based on the Koopman autoencoder (KAE) to handle time-varying graph data. Specifically, we assume the existence of a hidden non-linear dynamical system, where the state vector corresponds to the graph embedding of the time-varying graph signals. To capture the evolving graph structures, the graph data is first converted into a vector time series through graph embedding, representing the structural information in a finite-dimensional latent space. In this latent space, the KAE is applied to learn the underlying non-linear dynamics governing the temporal evolution of graph features, enabling both prediction and reconstruction tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 84,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06495",
        "abs_url": "https://arxiv.org/abs/2511.06495",
        "pdf_url": "https://arxiv.org/pdf/2511.06495",
        "title": "Probably Approximately Global Robustness Certification",
        "authors": [
            "Peter Blohm",
            "Patrick Indri",
            "Thomas GÃ¤rtner",
            "Sagar Malhotra"
        ],
        "comments": "ICML 2025",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "We propose and investigate probabilistic guarantees for the adversarial robustness of classification algorithms. While traditional formal verification approaches for robustness are intractable and sampling-based approaches do not provide formal guarantees, our approach is able to efficiently certify a probabilistic relaxation of robustness. The key idea is to sample an $\\epsilon$-net and invoke a local robustness oracle on the sample. Remarkably, the size of the sample needed to achieve probably approximately global robustness guarantees is independent of the input dimensionality, the number of classes, and the learning algorithm itself. Our approach can, therefore, be applied even to large neural networks that are beyond the scope of traditional formal verification. Experiments empirically confirm that it characterizes robustness better than state-of-the-art sampling-based approaches and scales better than formal methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 85,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06527",
        "abs_url": "https://arxiv.org/abs/2511.06527",
        "pdf_url": "https://arxiv.org/pdf/2511.06527",
        "title": "Efficient Approximation of Volterra Series for High-Dimensional Systems",
        "authors": [
            "Navin Khoshnan",
            "Claudia K Petritsch",
            "Bryce-Allen Bagley"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Systems and Control (eess.SY)",
        "abstract": "The identification of high-dimensional nonlinear dynamical systems via the Volterra series has significant potential, but has been severely hindered by the curse of dimensionality. Tensor Network (TN) methods such as the Modified Alternating Linear Scheme (MVMALS) have been a breakthrough for the field, offering a tractable approach by exploiting the low-rank structure in Volterra kernels. However, these techniques still encounter prohibitive computational and memory bottlenecks due to high-order polynomial scaling with respect to input dimension. To overcome this barrier, we introduce the Tensor Head Averaging (THA) algorithm, which significantly reduces complexity by constructing an ensemble of localized MVMALS models trained on small subsets of the input space. In this paper, we present a theoretical foundation for the THA algorithm. We establish observable, finite-sample bounds on the error between the THA ensemble and a full MVMALS model, and we derive an exact decomposition of the squared error. This decomposition is used to analyze the manner in which subset models implicitly compensate for omitted dynamics. We quantify this effect, and prove that correlation between the included and omitted dynamics creates an optimization incentive which drives THA's performance toward accuracy superior to a simple truncation of a full MVMALS model. THA thus offers a scalable and theoretically grounded approach for identifying previously intractable high-dimensional systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 86,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06538",
        "abs_url": "https://arxiv.org/abs/2511.06538",
        "pdf_url": "https://arxiv.org/pdf/2511.06538",
        "title": "Bayesian Uncertainty Quantification with Anchored Ensembles for Robust EV Power Consumption Prediction",
        "authors": [
            "Ghazal Farhani",
            "Taufiq Rahman",
            "Kieran Humphries"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Accurate EV power estimation underpins range prediction and energy management, yet practitioners need both point accuracy and trustworthy uncertainty. We propose an anchored-ensemble Long Short-Term Memory (LSTM) with a Student-t likelihood that jointly captures epistemic (model) and aleatoric (data) uncertainty. Anchoring imposes a Gaussian weight prior (MAP training), yielding posterior-like diversity without test-time sampling, while the t-head provides heavy-tailed robustness and closed-form prediction intervals. Using vehicle-kinematic time series (e.g., speed, motor RPM), our model attains strong accuracy: RMSE 3.36 +/- 1.10, MAE 2.21 +/- 0.89, R-squared = 0.93 +/- 0.02, explained variance 0.93 +/- 0.02, and delivers well-calibrated uncertainty bands with near-nominal coverage. Against competitive baselines (Student-t MC dropout; quantile regression with/without anchoring), our method matches or improves log-scores while producing sharper intervals at the same coverage. Crucially for real-time deployment, inference is a single deterministic pass per ensemble member (or a weight-averaged collapse), eliminating Monte Carlo latency. The result is a compact, theoretically grounded estimator that couples accuracy, calibration, and systems efficiency, enabling reliable range estimation and decision-making for production EV energy management.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 87,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06563",
        "abs_url": "https://arxiv.org/abs/2511.06563",
        "pdf_url": "https://arxiv.org/pdf/2511.06563",
        "title": "Practical Policy Distillation for Reinforcement Learning in Radio Access Networks",
        "authors": [
            "Sara Khosravi",
            "Burak Demirel",
            "Linghui Zhou",
            "Javier Rasines",
            "Pablo Soldati"
        ],
        "comments": "This paper is accepted for publication in IEEE International Symposium on Personal, Indoor and Mobile Radio Communications, 2025",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Adopting artificial intelligence (AI) in radio access networks (RANs) presents several challenges, including limited availability of link-level measurements (e.g., CQI reports), stringent real-time processing constraints (e.g., sub-1 ms per TTI), and network heterogeneity (different spectrum bands, cell types, and vendor equipment). A critical yet often overlooked barrier lies in the computational and memory limitations of RAN baseband hardware, particularly in legacy 4th Generation (4G) systems, which typically lack on-chip neural accelerators. As a result, only lightweight AI models (under 1 Mb and sub-100~\\mu s inference time) can be effectively deployed, limiting both their performance and applicability. However, achieving strong generalization across diverse network conditions often requires large-scale models with substantial resource demands. To address this trade-off, this paper investigates policy distillation in the context of a reinforcement learning-based link adaptation task. We explore two strategies: single-policy distillation, where a scenario-agnostic teacher model is compressed into one generalized student model; and multi-policy distillation, where multiple scenario-specific teachers are consolidated into a single generalist student. Experimental evaluations in a high-fidelity, 5th Generation (5G)-compliant simulator demonstrate that both strategies produce compact student models that preserve the teachers' generalization capabilities while complying with the computational and memory limitations of existing RAN hardware.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 88,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06597",
        "abs_url": "https://arxiv.org/abs/2511.06597",
        "pdf_url": "https://arxiv.org/pdf/2511.06597",
        "title": "Optimistic Online-to-Batch Conversions for Accelerated Convergence and Universality",
        "authors": [
            "Yu-Hu Yan",
            "Peng Zhao",
            "Zhi-Hua Zhou"
        ],
        "comments": "NeurIPS 2025",
        "subjects": "Machine Learning (cs.LG); Optimization and Control (math.OC)",
        "abstract": "In this work, we study offline convex optimization with smooth objectives, where the classical Nesterov's Accelerated Gradient (NAG) method achieves the optimal accelerated convergence. Extensive research has aimed to understand NAG from various perspectives, and a recent line of work approaches this from the viewpoint of online learning and online-to-batch conversion, emphasizing the role of optimistic online algorithms for acceleration. In this work, we contribute to this perspective by proposing novel optimistic online-to-batch conversions that incorporate optimism theoretically into the analysis, thereby significantly simplifying the online algorithm design while preserving the optimal convergence rates. Specifically, we demonstrate the effectiveness of our conversions through the following results: (i) when combined with simple online gradient descent, our optimistic conversion achieves the optimal accelerated convergence; (ii) our conversion also applies to strongly convex objectives, and by leveraging both optimistic online-to-batch conversion and optimistic online algorithms, we achieve the optimal accelerated convergence rate for strongly convex and smooth objectives, for the first time through the lens of online-to-batch conversion; (iii) our optimistic conversion can achieve universality to smoothness -- applicable to both smooth and non-smooth objectives without requiring knowledge of the smoothness coefficient -- and remains efficient as non-universal methods by using only one gradient query in each iteration. Finally, we highlight the effectiveness of our optimistic online-to-batch conversions by a precise correspondence with NAG.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 89,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06598",
        "abs_url": "https://arxiv.org/abs/2511.06598",
        "pdf_url": "https://arxiv.org/pdf/2511.06598",
        "title": "Adaptive Initial Residual Connections for GNNs with Theoretical Guarantees",
        "authors": [
            "Mohammad Shirzadi",
            "Ali Safarpoor Dehkordi",
            "Ahad N. Zehmakan"
        ],
        "comments": "This is the full version of the paper accepted to the 40th Annual AAAI Conference on Artificial Intelligence (AAAI-2026)",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Message passing is the core operation in graph neural networks, where each node updates its embeddings by aggregating information from its neighbors. However, in deep architectures, this process often leads to diminished expressiveness. A popular solution is to use residual connections, where the input from the current (or initial) layer is added to aggregated neighbor information to preserve embeddings across layers. Following a recent line of research, we investigate an adaptive residual scheme in which different nodes have varying residual strengths. We prove that this approach prevents oversmoothing; particularly, we show that the Dirichlet energy of the embeddings remains bounded away from zero. This is the first theoretical guarantee not only for the adaptive setting, but also for static residual connections (where residual strengths are shared across nodes) with activation functions. Furthermore, extensive experiments show that this adaptive approach outperforms standard and state-of-the-art message passing mechanisms, especially on heterophilic graphs. To improve the time complexity of our approach, we introduce a variant in which residual strengths are not learned but instead set heuristically, a choice that performs as well as the learnable version.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 90,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06607",
        "abs_url": "https://arxiv.org/abs/2511.06607",
        "pdf_url": "https://arxiv.org/pdf/2511.06607",
        "title": "Explainable Probabilistic Machine Learning for Predicting Drilling Fluid Loss of Circulation in Marun Oil Field",
        "authors": [
            "Seshu Kumar Damarla",
            "Xiuli Zhu"
        ],
        "comments": "5 pages, 3 tables, 4 figrues",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Lost circulation remains a major and costly challenge in drilling operations, often resulting in wellbore instability, stuck pipe, and extended non-productive time. Accurate prediction of fluid loss is therefore essential for improving drilling safety and efficiency. This study presents a probabilistic machine learning framework based on Gaussian Process Regression (GPR) for predicting drilling fluid loss in complex formations. The GPR model captures nonlinear dependencies among drilling parameters while quantifying predictive uncertainty, offering enhanced reliability for high-risk decision-making. Model hyperparameters are optimized using the Limited memory Broyden Fletcher Goldfarb Shanno (LBFGS) algorithm to ensure numerical stability and robust generalization. To improve interpretability, Local Interpretable Model agnostic Explanations (LIME) are employed to elucidate how individual features influence model predictions. The results highlight the potential of explainable probabilistic learning for proactive identification of lost-circulation risks, optimized design of lost circulation materials (LCM), and reduction of operational uncertainties in drilling applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 91,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06609",
        "abs_url": "https://arxiv.org/abs/2511.06609",
        "pdf_url": "https://arxiv.org/pdf/2511.06609",
        "title": "A Weak Penalty Neural ODE for Learning Chaotic Dynamics from Noisy Time Series",
        "authors": [
            "Xuyang Li",
            "John Harlim",
            "Romit Maulik"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Dynamical Systems (math.DS)",
        "abstract": "Accurate forecasting of complex high-dimensional dynamical systems from observational data is essential for several applications across science and engineering. A key challenge, however, is that real-world measurements are often corrupted by noise, which severely degrades the performance of data-driven models. Particularly, in chaotic dynamical systems, where small errors amplify rapidly, it is challenging to identify a data-driven model from noisy data that achieves short-term accuracy while preserving long-term invariant properties. In this paper, we propose the use of the weak formulation as a complementary approach to the classical strong formulation of data-driven time-series forecasting models. Specifically, we focus on the neural ordinary differential equation (NODE) architecture. Unlike the standard strong formulation, which relies on the discretization of the NODE followed by optimization, the weak formulation constrains the model using a set of integrated residuals over temporal subdomains. While such a formulation yields an effective NODE model, we discover that the performance of a NODE can be further enhanced by employing this weak formulation as a penalty alongside the classical strong formulation-based learning. Through numerical demonstrations, we illustrate that our proposed training strategy, which we coined as the Weak-Penalty NODE (WP-NODE), achieves state-of-the-art forecasting accuracy and exceptional robustness across benchmark chaotic dynamical systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 92,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06610",
        "abs_url": "https://arxiv.org/abs/2511.06610",
        "pdf_url": "https://arxiv.org/pdf/2511.06610",
        "title": "Non-Rival Data as Rival Products: An Encapsulation-Forging Approach for Data Synthesis",
        "authors": [
            "Kaidong Wang",
            "Jiale Li",
            "Shao-Bo Lin",
            "Yao Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "The non-rival nature of data creates a dilemma for firms: sharing data unlocks value but risks eroding competitive advantage. Existing data synthesis methods often exacerbate this problem by creating data with symmetric utility, allowing any party to extract its value. This paper introduces the Encapsulation-Forging (EnFo) framework, a novel approach to generate rival synthetic data with asymmetric utility. EnFo operates in two stages: it first encapsulates predictive knowledge from the original data into a designated ``key'' model, and then forges a synthetic dataset by optimizing the data to intentionally overfit this key model. This process transforms non-rival data into a rival product, ensuring its value is accessible only to the intended model, thereby preventing unauthorized use and preserving the data owner's competitive edge. Our framework demonstrates remarkable sample efficiency, matching the original data's performance with a fraction of its size, while providing robust privacy protection and resistance to misuse. EnFo offers a practical solution for firms to collaborate strategically without compromising their core analytical advantage.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 93,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06633",
        "abs_url": "https://arxiv.org/abs/2511.06633",
        "pdf_url": "https://arxiv.org/pdf/2511.06633",
        "title": "Dual-branch Spatial-Temporal Self-supervised Representation for Enhanced Road Network Learning",
        "authors": [
            "Qinghong Guo",
            "Yu Wang",
            "Ji Cao",
            "Tongya Zheng",
            "Junshu Dai",
            "Bingde Hu",
            "Shunyu Liu",
            "Canghong Jin"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Road network representation learning (RNRL) has attracted increasing attention from both researchers and practitioners as various spatiotemporal tasks are emerging. Recent advanced methods leverage Graph Neural Networks (GNNs) and contrastive learning to characterize the spatial structure of road segments in a self-supervised paradigm. However, spatial heterogeneity and temporal dynamics of road networks raise severe challenges to the neighborhood smoothing mechanism of self-supervised GNNs. To address these issues, we propose a $\\textbf{D}$ual-branch $\\textbf{S}$patial-$\\textbf{T}$emporal self-supervised representation framework for enhanced road representations, termed as DST. On one hand, DST designs a mix-hop transition matrix for graph convolution to incorporate dynamic relations of roads from trajectories. Besides, DST contrasts road representations of the vanilla road network against that of the hypergraph in a spatial self-supervised way. The hypergraph is newly built based on three types of hyperedges to capture long-range relations. On the other hand, DST performs next token prediction as the temporal self-supervised task on the sequences of traffic dynamics based on a causal Transformer, which is further regularized by differentiating traffic modes of weekdays from those of weekends. Extensive experiments against state-of-the-art methods verify the superiority of our proposed framework. Moreover, the comprehensive spatiotemporal modeling facilitates DST to excel in zero-shot learning scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 94,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06634",
        "abs_url": "https://arxiv.org/abs/2511.06634",
        "pdf_url": "https://arxiv.org/pdf/2511.06634",
        "title": "CaberNet: Causal Representation Learning for Cross-Domain HVAC Energy Prediction",
        "authors": [
            "Kaiyuan Zhai",
            "Jiacheng Cui",
            "Zhehao Zhang",
            "Junyu Xue",
            "Yang Deng",
            "Kui Wu",
            "Guoming Tang"
        ],
        "comments": "Accepted at ACM e-Energy 2026",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Cross-domain HVAC energy prediction is essential for scalable building energy management, particularly because collecting extensive labeled data for every new building is both costly and impractical. Yet, this task remains highly challenging due to the scarcity and heterogeneity of data across different buildings, climate zones, and seasonal patterns. In particular, buildings situated in distinct climatic regions introduce variability that often leads existing methods to overfit to spurious correlations, rely heavily on expert intervention, or compromise on data diversity. To address these limitations, we propose CaberNet, a causal and interpretable deep sequence model that learns invariant (Markov blanket) representations for robust cross-domain prediction. In a purely data-driven fashion and without requiring any prior knowledge, CaberNet integrates i) a global feature gate trained with a self-supervised Bernoulli regularization to distinguish superior causal features from inferior ones, and ii) a domain-wise training scheme that balances domain contributions, minimizes cross-domain loss variance, and promotes latent factor independence. We evaluate CaberNet on real-world datasets collected from three buildings located in three climatically diverse cities, and it consistently outperforms all baselines, achieving a 22.9\\% reduction in normalized mean squared error (NMSE) compared to the best benchmark. Our code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 95,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06641",
        "abs_url": "https://arxiv.org/abs/2511.06641",
        "pdf_url": "https://arxiv.org/pdf/2511.06641",
        "title": "Neyman-Pearson Classification under Both Null and Alternative Distributions Shift",
        "authors": [
            "Mohammadreza M. Kalan",
            "Yuyang Deng",
            "Eitan J. Neugut",
            "Samory Kpotufe"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "We consider the problem of transfer learning in Neyman-Pearson classification, where the objective is to minimize the error w.r.t. a distribution $\\mu_1$, subject to the constraint that the error w.r.t. a distribution $\\mu_0$ remains below a prescribed threshold. While transfer learning has been extensively studied in traditional classification, transfer learning in imbalanced classification such as Neyman-Pearson classification has received much less attention. This setting poses unique challenges, as both types of errors must be simultaneously controlled. Existing works address only the case of distribution shift in $\\mu_1$, whereas in many practical scenarios shifts may occur in both $\\mu_0$ and $\\mu_1$. We derive an adaptive procedure that not only guarantees improved Type-I and Type-II errors when the source is informative, but also automatically adapt to situations where the source is uninformative, thereby avoiding negative transfer. In addition to such statistical guarantees, the procedures is efficient, as shown via complementary computational guarantees.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 96,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06642",
        "abs_url": "https://arxiv.org/abs/2511.06642",
        "pdf_url": "https://arxiv.org/pdf/2511.06642",
        "title": "Improving Asset Allocation in a Fast Moving Consumer Goods B2B Company: An Interpretable Machine Learning Framework for Commercial Cooler Assignment Based on Multi-Tier Growth Targets",
        "authors": [
            "Renato Castro",
            "Rodrigo Paredes",
            "Douglas Kahn"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In the fast-moving consumer goods (FMCG) industry, deciding where to place physical assets, such as commercial beverage coolers, can directly impact revenue growth and execution efficiency. Although churn prediction and demand forecasting have been widely studied in B2B contexts, the use of machine learning to guide asset allocation remains relatively unexplored. This paper presents a framework focused on predicting which beverage clients are most likely to deliver strong returns in volume after receiving a cooler. Using a private dataset from a well-known Central American brewing and beverage company of 3,119 B2B traditional trade channel clients that received a cooler from 2022-01 to 2024-07, and tracking 12 months of sales transactions before and after cooler installation, three growth thresholds were defined: 10%, 30% and 50% growth in sales volume year over year. The analysis compares results of machine learning models such as XGBoost, LightGBM, and CatBoost combined with SHAP for interpretable feature analysis in order to have insights into improving business operations related to cooler allocation; the results show that the best model has AUC scores of 0.857, 0.877, and 0.898 across the thresholds on the validation set. Simulations suggest that this approach can improve ROI because it better selects potential clients to grow at the expected level and increases cost savings by not assigning clients that will not grow, compared to traditional volume-based approaches with substantial business management recommendations",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 97,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06662",
        "abs_url": "https://arxiv.org/abs/2511.06662",
        "pdf_url": "https://arxiv.org/pdf/2511.06662",
        "title": "Dual-Pathway Fusion of EHRs and Knowledge Graphs for Predicting Unseen Drug-Drug Interactions",
        "authors": [
            "Franklin Lee",
            "Tengfei Ma"
        ],
        "comments": "ML4H 2025 Findings",
        "subjects": "Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)",
        "abstract": "Drug-drug interactions (DDIs) remain a major source of preventable harm, and many clinically important mechanisms are still unknown. Existing models either rely on pharmacologic knowledge graphs (KGs), which fail on unseen drugs, or on electronic health records (EHRs), which are noisy, temporal, and site-dependent. We introduce, to our knowledge, the first system that conditions KG relation scoring on patient-level EHR context and distills that reasoning into an EHR-only model for zero-shot inference. A fusion \"Teacher\" learns mechanism-specific relations for drug pairs represented in both sources, while a distilled \"Student\" generalizes to new or rarely used drugs without KG access at inference. Both operate under a shared ontology (set) of pharmacologic mechanisms (drug relations) to produce interpretable, auditable alerts rather than opaque risk scores. Trained on a multi-institution EHR corpus paired with a curated DrugBank DDI graph, and evaluated using a clinically aligned, decision-focused protocol with leakage-safe negatives that avoid artificially easy pairs, the system maintains precision across multi-institutuion test data, produces mechanism-specific, clinically consistent predictions, reduces false alerts (higher precision) at comparable overall detection performance (F1), and misses fewer true interactions compared to prior methods. Case studies further show zero-shot identification of clinically recognized CYP-mediated and pharmacodynamic mechanisms for drugs absent from the KG, supporting real-world use in clinical decision support and pharmacovigilance.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 98,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06681",
        "abs_url": "https://arxiv.org/abs/2511.06681",
        "pdf_url": "https://arxiv.org/pdf/2511.06681",
        "title": "An Adaptive Machine Learning Triage Framework for Predicting Alzheimer's Disease Progression",
        "authors": [
            "Richard Hou",
            "Shengpu Tang",
            "Wei Jin"
        ],
        "comments": "Findings paper presented at Machine Learning for Health (ML4H) symposium 2025, December 1-2, 2025, San Diego, CA, USA, 9 pages. Shengpu Tang and Wei Jin contributed equally as senior authors",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Accurate predictions of conversion from mild cognitive impairment (MCI) to Alzheimer's disease (AD) can enable effective personalized therapy. While cognitive tests and clinical data are routinely collected, they lack the predictive power of PET scans and CSF biomarker analysis, which are prohibitively expensive to obtain for every patient. To address this cost-accuracy dilemma, we design a two-stage machine learning framework that selectively obtains advanced, costly features based on their predicted \"value of information\". We apply our framework to predict AD progression for MCI patients using data from the Alzheimer's Disease Neuroimaging Initiative (ADNI). Our framework reduces the need for advanced testing by 20% while achieving a test AUROC of 0.929, comparable to the model that uses both basic and advanced features (AUROC=0.915, p=0.1010). We also provide an example interpretability analysis showing how one may explain the triage decision. Our work presents an interpretable, data-driven framework that optimizes AD diagnostic pathways and balances accuracy with cost, representing a step towards making early, reliable AD prediction more accessible in real-world practice. Future work should consider multiple categories of advanced features and larger-scale validation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 99,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06686",
        "abs_url": "https://arxiv.org/abs/2511.06686",
        "pdf_url": "https://arxiv.org/pdf/2511.06686",
        "title": "Mitigating Modality Imbalance in Multi-modal Learning via Multi-objective Optimization",
        "authors": [
            "Heshan Fernando",
            "Parikshit Ram",
            "Yi Zhou",
            "Soham Dan",
            "Horst Samulowitz",
            "Nathalie Baracaldo",
            "Tianyi Chen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Multi-modal learning (MML) aims to integrate information from multiple modalities, which is expected to lead to superior performance over single-modality learning. However, recent studies have shown that MML can underperform, even compared to single-modality approaches, due to imbalanced learning across modalities. Methods have been proposed to alleviate this imbalance issue using different heuristics, which often lead to computationally intensive subroutines. In this paper, we reformulate the MML problem as a multi-objective optimization (MOO) problem that overcomes the imbalanced learning issue among modalities and propose a gradient-based algorithm to solve the modified MML problem. We provide convergence guarantees for the proposed method, and empirical evaluations on popular MML benchmarks showcasing the improved performance of the proposed method over existing balanced MML and MOO baselines, with up to ~20x reduction in subroutine computation time. Our code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 100,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06692",
        "abs_url": "https://arxiv.org/abs/2511.06692",
        "pdf_url": "https://arxiv.org/pdf/2511.06692",
        "title": "Peeling Context from Cause for Multimodal Molecular Property Prediction",
        "authors": [
            "Tao Li",
            "Kaiyuan Hou",
            "Tuan Vinh",
            "Carl Yang",
            "Monika Raj"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Deep models are used for molecular property prediction, yet they are often difficult to interpret and may rely on spurious context rather than causal structure, which reduces reliability under distribution shift and harms predictive performance. We introduce CLaP (Causal Layerwise Peeling), a framework that separates causal signal from context in a layerwise manner and integrates diverse graph representations of molecules. At each layer, a causal block performs a soft split into causal and non-causal branches, fuses causal evidence across modalities, and progressively removes batch-coupled context to focus on label-relevant structure, thereby limiting shortcut signals and stabilizing layerwise refinement. Across four molecular benchmarks, CLaP consistently improves MAE, MSE, and $R^2$ over competitive baselines. The model also produces atom-level causal saliency maps that highlight substructures responsible for predictions, providing actionable guidance for targeted molecular edits. Case studies confirm the accuracy of these maps and their alignment with chemical intuition. By peeling context from cause at every layer, the model yields predictors that are both accurate and interpretable for molecular design.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 101,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06696",
        "abs_url": "https://arxiv.org/abs/2511.06696",
        "pdf_url": "https://arxiv.org/pdf/2511.06696",
        "title": "Magnitude-Modulated Equivariant Adapter for Parameter-Efficient Fine-Tuning of Equivariant Graph Neural Networks",
        "authors": [
            "Dian Jin",
            "Yancheng Yuan",
            "Xiaoming Tao"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Pretrained equivariant graph neural networks based on spherical harmonics offer efficient and accurate alternatives to computationally expensive ab-initio methods, yet adapting them to new tasks and chemical environments still requires fine-tuning. Conventional parameter-efficient fine-tuning (PEFT) techniques, such as Adapters and LoRA, typically break symmetry, making them incompatible with those equivariant architectures. ELoRA, recently proposed, is the first equivariant PEFT method. It achieves improved parameter efficiency and performance on many benchmarks. However, the relatively high degrees of freedom it retains within each tensor order can still perturb pretrained feature distributions and ultimately degrade performance. To address this, we present Magnitude-Modulated Equivariant Adapter (MMEA), a novel equivariant fine-tuning method which employs lightweight scalar gating to modulate feature magnitudes on a per-order and per-multiplicity basis. We demonstrate that MMEA preserves strict equivariance and, across multiple benchmarks, consistently improves energy and force predictions to state-of-the-art levels while training fewer parameters than competing approaches. These results suggest that, in many practical scenarios, modulating channel magnitudes is sufficient to adapt equivariant models to new chemical environments without breaking symmetry, pointing toward a new paradigm for equivariant PEFT design.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 102,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06719",
        "abs_url": "https://arxiv.org/abs/2511.06719",
        "pdf_url": "https://arxiv.org/pdf/2511.06719",
        "title": "MobileLLM-Pro Technical Report",
        "authors": [
            "Patrick Huber",
            "Ernie Chang",
            "Wei Wen",
            "Igor Fedorov",
            "Tarek Elgamal",
            "Hanxian Huang",
            "Naveen Suda",
            "Chinnadhurai Sankar",
            "Vish Vogeti",
            "Yanghan Wang",
            "Alex Gladkov",
            "Kai Sheng Tai",
            "Abdelrahman Elogeel",
            "Tarek Hefny",
            "Vikas Chandra",
            "Ahmed Aly",
            "Anuj Kumar",
            "Raghuraman Krishnamoorthi",
            "Adithya Sagar"
        ],
        "comments": "17 pages",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Efficient on-device language models around 1 billion parameters are essential for powering low-latency AI applications on mobile and wearable devices. However, achieving strong performance in this model class, while supporting long context windows and practical deployment remains a significant challenge. We introduce MobileLLM-Pro, a 1-billion-parameter language model optimized for on-device deployment. MobileLLM-Pro achieves state-of-the-art results across 11 standard benchmarks, significantly outperforming both Gemma 3-1B and Llama 3.2-1B, while supporting context windows of up to 128,000 tokens and showing only minor performance regressions at 4-bit quantization. These improvements are enabled by four core innovations: (1) implicit positional distillation, a novel technique that effectively instills long-context capabilities through knowledge distillation; (2) a specialist model merging framework that fuses multiple domain experts into a compact model without parameter growth; (3) simulation-driven data mixing using utility estimation; and (4) 4-bit quantization-aware training with self-distillation. We release our model weights and code to support future research in efficient on-device language models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 103,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06723",
        "abs_url": "https://arxiv.org/abs/2511.06723",
        "pdf_url": "https://arxiv.org/pdf/2511.06723",
        "title": "Multi-Modal Continual Learning via Cross-Modality Adapters and Representation Alignment with Knowledge Preservation",
        "authors": [
            "Evelyn Chee",
            "Wynne Hsu",
            "Mong Li Lee"
        ],
        "comments": "Accepted to ECAI 2025",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Continual learning is essential for adapting models to new tasks while retaining previously acquired knowledge. While existing approaches predominantly focus on uni-modal data, multi-modal learning offers substantial benefits by utilizing diverse sensory inputs, akin to human perception. However, multi-modal continual learning presents additional challenges, as the model must effectively integrate new information from various modalities while preventing catastrophic forgetting. In this work, we propose a pre-trained model-based framework for multi-modal continual learning. Our framework includes a novel cross-modality adapter with a mixture-of-experts structure to facilitate effective integration of multi-modal information across tasks. We also introduce a representation alignment loss that fosters learning of robust multi-modal representations, and regularize relationships between learned representations to preserve knowledge from previous tasks. Experiments on several multi-modal datasets demonstrate that our approach consistently outperforms baselines in both class-incremental and domain-incremental learning, achieving higher accuracy and reduced forgetting.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 104,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06756",
        "abs_url": "https://arxiv.org/abs/2511.06756",
        "pdf_url": "https://arxiv.org/pdf/2511.06756",
        "title": "Dual Mamba for Node-Specific Representation Learning: Tackling Over-Smoothing with Selective State Space Modeling",
        "authors": [
            "Xin He",
            "Yili Wang",
            "Yiwei Dai",
            "Xin Wang"
        ],
        "comments": "11 pages, 4 figures",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Over-smoothing remains a fundamental challenge in deep Graph Neural Networks (GNNs), where repeated message passing causes node representations to become indistinguishable. While existing solutions, such as residual connections and skip layers, alleviate this issue to some extent, they fail to explicitly model how node representations evolve in a node-specific and progressive manner across layers. Moreover, these methods do not take global information into account, which is also crucial for mitigating the over-smoothing problem. To address the aforementioned issues, in this work, we propose a Dual Mamba-enhanced Graph Convolutional Network (DMbaGCN), which is a novel framework that integrates Mamba into GNNs to address over-smoothing from both local and global perspectives. DMbaGCN consists of two modules: the Local State-Evolution Mamba (LSEMba) for local neighborhood aggregation and utilizing Mamba's selective state space modeling to capture node-specific representation dynamics across layers, and the Global Context-Aware Mamba (GCAMba) that leverages Mamba's global attention capabilities to incorporate global context for each node. By combining these components, DMbaGCN enhances node discriminability in deep GNNs, thereby mitigating over-smoothing. Extensive experiments on multiple benchmarks demonstrate the effectiveness and efficiency of our method.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 105,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06757",
        "abs_url": "https://arxiv.org/abs/2511.06757",
        "pdf_url": "https://arxiv.org/pdf/2511.06757",
        "title": "Implicit Federated In-context Learning For Task-Specific LLM Fine-Tuning",
        "authors": [
            "Dongcheng Li",
            "Junhan Chen",
            "Aoxiang Zhou",
            "Chunpei Li",
            "Youquan Xian",
            "Peng Liu",
            "Xianxian Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "As large language models continue to develop and expand, the extensive public data they rely on faces the risk of depletion. Consequently, leveraging private data within organizations to enhance the performance of large models has emerged as a key challenge. The federated learning paradigm, combined with model fine-tuning techniques, effectively reduces the number of trainable parameters. However,the necessity to process high-dimensional feature spaces results in substantial overall computational overhead. To address this issue, we propose the Implicit Federated In-Context Learning (IFed-ICL) framework. IFed-ICL draws inspiration from federated learning to establish a novel distributed collaborative paradigm, by converting client local context examples into implicit vector representations, it enables distributed collaborative computation during the inference phase and injects model residual streams to enhance model performance. Experiments demonstrate that our proposed method achieves outstanding performance across multiple text classification tasks. Compared to traditional methods, IFed-ICL avoids the extensive parameter updates required by conventional fine-tuning methods while reducing data transmission and local computation at the client level in federated learning. This enables efficient distributed context learning using local private-domain data, significantly improving model performance on specific tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 106,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06776",
        "abs_url": "https://arxiv.org/abs/2511.06776",
        "pdf_url": "https://arxiv.org/pdf/2511.06776",
        "title": "Data Trajectory Alignment for LLM Domain Adaptation: A Two-Phase Synthesis Framework for Telecommunications Mathematics",
        "authors": [
            "Zhicheng Zhou",
            "Jing Li",
            "Suming Qiu",
            "Junjie Huang",
            "Linyuan Qiu",
            "Zhijie Sun"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "General-purpose large language models (LLMs) are increasingly deployed in verticals such as telecommunications, where adaptation is hindered by scarce, low-information-density corpora and tight mobile/edge constraints. We propose Data Trajectory Alignment (DTA), a two-phase, model-agnostic data curation framework that treats solution processes - not only final answers - as first-class supervision. Phase I (Initializing) synthesizes diverse, high-coverage candidates using an ensemble of strong teachers. Phase II (DTA) rewrites teacher solutions to align intermediate steps and presentation style with the target student's inductive biases and then performs signal-aware exemplar selection via agreement checks and reflection-based judging. Instantiated on telecommunications mathematics (e.g., link budgets, SNR/AMC selection, and power-control feasibility), DTA yields state-of-the-art (SOTA) accuracy on TELEMATH without enabling explicit \"thinking\" modes: 72.45% pass@1, surpassing distilled-only training by +17.65 points and outperforming a strong baseline (Qwen3-32B with thinking enabled) by +2.94 points. Token-shift analyses indicate that DTA concentrates gains on logical-structural discourse markers rather than merely amplifying domain nouns, indicating improved reasoning scaffolding. Under edge-like inference settings, DTA improves efficiency by reducing reliance on multi-sample voting and disabling expensive reasoning heuristics, cutting energy per output token by ~42% versus Qwen3-32B (thinking mode enabled) and end-to-end latency by ~60% versus Qwen3-32B (thinking mode disabled). These results demonstrate that aligning how solutions are produced enables compact, high-yield supervision that is effective for both accuracy and efficiency, offering a practical recipe for domain adaptation in low-resource verticals beyond telecom.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 107,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06785",
        "abs_url": "https://arxiv.org/abs/2511.06785",
        "pdf_url": "https://arxiv.org/pdf/2511.06785",
        "title": "Resource Efficient Sleep Staging via Multi-Level Masking and Prompt Learning",
        "authors": [
            "Lejun Ai",
            "Yulong Li",
            "Haodong Yi",
            "Jixuan Xie",
            "Yue Wang",
            "Jia Liu",
            "Min Chen",
            "Rui Wang"
        ],
        "comments": "16 pages, 4 figures, to be published in AAAI 2026",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Automatic sleep staging plays a vital role in assessing sleep quality and diagnosing sleep disorders. Most existing methods rely heavily on long and continuous EEG recordings, which poses significant challenges for data acquisition in resource-constrained systems, such as wearable or home-based monitoring systems. In this paper, we propose the task of resource-efficient sleep staging, which aims to reduce the amount of signal collected per sleep epoch while maintaining reliable classification performance. To solve this task, we adopt the masking and prompt learning strategy and propose a novel framework called Mask-Aware Sleep Staging (MASS). Specifically, we design a multi-level masking strategy to promote effective feature modeling under partial and irregular observations. To mitigate the loss of contextual information introduced by masking, we further propose a hierarchical prompt learning mechanism that aggregates unmasked data into a global prompt, serving as a semantic anchor for guiding both patch-level and epoch-level feature modeling. MASS is evaluated on four datasets, demonstrating state-of-the-art performance, especially when the amount of data is very limited. This result highlights its potential for efficient and scalable deployment in real-world low-resource sleep monitoring environments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 108,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06786",
        "abs_url": "https://arxiv.org/abs/2511.06786",
        "pdf_url": "https://arxiv.org/pdf/2511.06786",
        "title": "Rethinking Parameter Sharing as Graph Coloring for Structured Compression",
        "authors": [
            "Boyang Zhang",
            "Daning Cheng",
            "Yunquan Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Modern deep models have massive parameter sizes, leading to high inference-time memory usage that limits practical deployment. Parameter sharing, a form of structured compression, effectively reduces redundancy, but existing approaches remain heuristic-restricted to adjacent layers and lacking a systematic analysis for cross-layer sharing. However, extending sharing across multiple layers leads to an exponentially expanding configuration space, making exhaustive search computationally infeasible and forming a critical bottleneck for parameter sharing. We recast parameter sharing from a group-theoretic perspective as introducing structural symmetries in the model's parameter space. A sharing configuration can be described by a coloring function $\\alpha:L\\rightarrow C$ (L: layer indices and C: sharing classes), which determines inter-layer sharing groups while preserving structural symmetry. To determine the coloring function, we propose a second-order geometric criterion based on Taylor expansion and the Hessian spectrum. By projecting perturbations onto the Hessian's low-curvature eigensubspace, the criterion provides an analytic rule for selecting sharing groups that minimize performance impact, yielding a principled and scalable configuration procedure. Across diverse architectures and tasks, Geo-Sharing consistently outperforms state-of-the-art heuristic sharing strategies, achieving higher compression ratios with smaller accuracy degradation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 109,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06791",
        "abs_url": "https://arxiv.org/abs/2511.06791",
        "pdf_url": "https://arxiv.org/pdf/2511.06791",
        "title": "Coupling Agent-based Modeling and Life Cycle Assessment to Analyze Trade-offs in Resilient Energy Transitions",
        "authors": [
            "Beichen Zhang",
            "Mohammed T. Zaki",
            "Hanna Breunig",
            "Newsha K. Ajami"
        ],
        "comments": "4 pages (+4 pages in appendix), 3 figures (+ 2 figures in appendix), 8 tables in appendix, NeurIPS Workshop on Tackling Climate Change with Machine Learning, 2025",
        "subjects": "Machine Learning (cs.LG); Multiagent Systems (cs.MA)",
        "abstract": "Transitioning to sustainable and resilient energy systems requires navigating complex and interdependent trade-offs across environmental, social, and resource dimensions. Neglecting these trade-offs can lead to unintended consequences across sectors. However, existing assessments often evaluate emerging energy pathways and their impacts in silos, overlooking critical interactions such as regional resource competition and cumulative impacts. We present an integrated modeling framework that couples agent-based modeling and Life Cycle Assessment (LCA) to simulate how energy transition pathways interact with regional resource competition, ecological constraints, and community-level burdens. We apply the model to a case study in Southern California. The results demonstrate how integrated and multiscale decision making can shape energy pathway deployment and reveal spatially explicit trade-offs under scenario-driven constraints. This modeling framework can further support more adaptive and resilient energy transition planning on spatial and institutional scales.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 110,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06794",
        "abs_url": "https://arxiv.org/abs/2511.06794",
        "pdf_url": "https://arxiv.org/pdf/2511.06794",
        "title": "Beyond Uniform Deletion: A Data Value-Weighted Framework for Certified Machine Unlearning",
        "authors": [
            "Lisong He",
            "Yi Yang",
            "Xiangyu Chang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "As the right to be forgotten becomes legislated worldwide, machine unlearning mechanisms have emerged to efficiently update models for data deletion and enhance user privacy protection. However, existing machine unlearning algorithms frequently neglect the fact that different data points may contribute unequally to model performance (i.e., heterogeneous data values). Treat them equally in machine unlearning procedure can potentially degrading the performance of updated models. To address this limitation, we propose Data Value-Weighted Unlearning (DVWU), a general unlearning framework that accounts for data value heterogeneity into the unlearning process. Specifically, we design a weighting strategy based on data values, which are then integrated into the unlearning procedure to enable differentiated unlearning for data points with varying utility to the model. The DVWU framework can be broadly adapted to various existing machine unlearning methods. We use the one-step Newton update as an example for implementation, developing both output and objective perturbation algorithms to achieve certified unlearning. Experiments on both synthetic and real-world datasets demonstrate that our methods achieve superior predictive performance and robustness compared to conventional unlearning approaches. We further show the extensibility of our framework on gradient ascent method by incorporating the proposed weighting strategy into the gradient terms, highlighting the adaptability of DVWU for broader gradient-based deep unlearning methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 111,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06797",
        "abs_url": "https://arxiv.org/abs/2511.06797",
        "pdf_url": "https://arxiv.org/pdf/2511.06797",
        "title": "FedNET: Federated Learning for Proactive Traffic Management and Network Capacity Planning",
        "authors": [
            "Saroj Kumar Panda",
            "Basabdatta Palit",
            "Sadananda Behera"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We propose FedNET, a proactive and privacy-preserving framework for early identification of high-risk links in large-scale communication networks, that leverages a distributed multi-step traffic forecasting method. FedNET employs Federated Learning (FL) to model the temporal evolution of node-level traffic in a distributed manner, enabling accurate multi-step-ahead predictions (e.g., several hours to days) without exposing sensitive network data. Using these node-level forecasts and known routing information, FedNET estimates the future link-level utilization by aggregating traffic contributions across all source-destination pairs. The links are then ranked according to the predicted load intensity and temporal variability, providing an early warning signal for potential high-risk links. We compare the federated traffic prediction of FedNET against a centralized multi-step learning baseline and then systematically analyze the impact of history and prediction window sizes on forecast accuracy using the $R^2$ score. Results indicate that FL achieves accuracy close to centralized training, with shorter prediction horizons consistently yielding the highest accuracy ($R^2 >0.92$), while longer horizons providing meaningful forecasts ($R^2 \\approx 0.45\\text{--}0.55$). We further validate the efficacy of the FedNET framework in predicting network utilization on a realistic network topology and demonstrate that it consistently identifies high-risk links well in advance (i.e., three days ahead) of the critical stress states emerging, making it a practical tool for anticipatory traffic engineering and capacity planning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 112,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06802",
        "abs_url": "https://arxiv.org/abs/2511.06802",
        "pdf_url": "https://arxiv.org/pdf/2511.06802",
        "title": "Neural-Initialized Newton: Accelerating Nonlinear Finite Elements via Operator Learning",
        "authors": [
            "Kianoosh Taghikhani",
            "Yusuke Yamazaki",
            "Jerry Paul Varghese",
            "Markus Apel",
            "Reza Najian Asl",
            "Shahed Rezaei"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We propose a Newton-based scheme, initialized by neural operator predictions, to accelerate the parametric solution of nonlinear problems in computational solid mechanics. First, a physics informed conditional neural field is trained to approximate the nonlinear parametric solutionof the governing equations. This establishes a continuous mapping between the parameter and solution spaces, which can then be evaluated for a given parameter at any spatial resolution. Second, since the neural approximation may not be exact, it is subsequently refined using a Newton-based correction initialized by the neural output. To evaluate the effectiveness of this hybrid approach, we compare three solution strategies: (i) the standard Newton-Raphson solver used in NFEM, which is robust and accurate but computationally demanding; (ii) physics-informed neural operators, which provide rapid inference but may lose accuracy outside the training distribution and resolution; and (iii) the neural-initialized Newton (NiN) strategy, which combines the efficiency of neural operators with the robustness of NFEM. The results demonstrate that the proposed hybrid approach reduces computational cost while preserving accuracy, highlighting its potential to accelerate large-scale nonlinear simulations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 113,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06831",
        "abs_url": "https://arxiv.org/abs/2511.06831",
        "pdf_url": "https://arxiv.org/pdf/2511.06831",
        "title": "DeepRWCap: Neural-Guided Random-Walk Capacitance Solver for IC Design",
        "authors": [
            "Hector R. Rodriguez",
            "Jiechen Huang",
            "Wenjian Yu"
        ],
        "comments": "Accepted to AAAI-26",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Monte Carlo random walk methods are widely used in capacitance extraction for their mesh-free formulation and inherent parallelism. However, modern semiconductor technologies with densely packed structures present significant challenges in unbiasedly sampling transition domains in walk steps with multiple high-contrast dielectric materials. We present DeepRWCap, a machine learning-guided random walk solver that predicts the transition quantities required to guide each step of the walk. These include Poisson kernels, gradient kernels, signs and magnitudes of weights. DeepRWCap employs a two-stage neural architecture that decomposes structured outputs into face-wise distributions and spatial kernels on cube faces. It uses 3D convolutional networks to capture volumetric dielectric interactions and 2D depthwise separable convolutions to model localized kernel behavior. The design incorporates grid-based positional encodings and structural design choices informed by cube symmetries to reduce learning redundancy and improve generalization. Trained on 100,000 procedurally generated dielectric configurations, DeepRWCap achieves a mean relative error of $1.24\\pm0.53$\\% when benchmarked against the commercial Raphael solver on the self-capacitance estimation of 10 industrial designs spanning 12 to 55 nm nodes. Compared to the state-of-the-art stochastic difference method Microwalk, DeepRWCap achieves an average 23\\% speedup. On complex designs with runtimes over 10 s, it reaches an average 49\\% acceleration.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 114,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06837",
        "abs_url": "https://arxiv.org/abs/2511.06837",
        "pdf_url": "https://arxiv.org/pdf/2511.06837",
        "title": "Minimum Width of Deep Narrow Networks for Universal Approximation",
        "authors": [
            "Xiao-Song Yang",
            "Qi Zhou",
            "Xuan Zhou"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Determining the minimum width of fully connected neural networks has become a fundamental problem in recent theoretical studies of deep neural networks. In this paper, we study the lower bounds and upper bounds of the minimum width required for fully connected neural networks in order to have universal approximation capability, which is important in network design and training. We show that $w_{min}\\leq\\max(2d_x+1, d_y)$ for networks with ELU, SELU, and the upper bound of this inequality is attained when $d_y=2d_x$, where $d_x$, $d_y$ denote the input and output dimensions, respectively. Besides, we show that $d_x+1\\leq w_{min}\\leq d_x+d_y$ for networks with LeakyReLU, ELU, CELU, SELU, Softplus, by proving that ReLU can be approximated by these activation functions. In addition, in the case that the activation function is injective or can be uniformly approximated by a sequence of injective functions (e.g., ReLU), we present a new proof of the inequality $w_{min}\\ge d_y+\\mathbf{1}_{d_x<d_y\\leq2d_x}$ by constructing a more intuitive example via a new geometric approach based on Poincar$\\acute{\\text{e}}$-Miranda Theorem.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 115,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06842",
        "abs_url": "https://arxiv.org/abs/2511.06842",
        "pdf_url": "https://arxiv.org/pdf/2511.06842",
        "title": "MI-to-Mid Distilled Compression (M2M-DC): An Hybrid-Information-Guided-Block Pruning with Progressive Inner Slicing Approach to Model Compression",
        "authors": [
            "Lionel Levine",
            "Sajjad Ghiasvand",
            "Haniyeh Ehsani Oskouie",
            "Majid Sarrafzadeh"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We introduce MI-to-Mid Distilled Compression (M2M-DC), a two-scale, shape-safe compression framework that interleaves information-guided block pruning with progressive inner slicing and staged knowledge distillation (KD). First, M2M-DC ranks residual (or inverted-residual) blocks by a label-aware mutual information (MI) signal and removes the least informative units (structured prune-after-training). It then alternates short KD phases with stage-coherent, residual-safe channel slicing: (i) stage \"planes\" (co-slicing conv2 out-channels with the downsample path and next-stage inputs), and (ii) an optional mid-channel trim (conv1 out / bn1 / conv2 in). This targets complementary redundancy, whole computational motifs and within-stage width while preserving residual shape invariants. On CIFAR-100, M2M-DC yields a clean accuracy-compute frontier. For ResNet-18, we obtain 85.46% Top-1 with 3.09M parameters and 0.0139 GMacs (72% params, 63% GMacs vs. teacher; mean final 85.29% over three seeds). For ResNet-34, we reach 85.02% Top-1 with 5.46M params and 0.0195 GMacs (74% / 74% vs. teacher; mean final 84.62%). Extending to inverted-residuals, MobileNetV2 achieves a mean final 68.54% Top-1 at 1.71M params (27%) and 0.0186 conv GMacs (24%), improving over the teacher's 66.03% by +2.5 points across three seeds. Because M2M-DC exposes only a thin, architecture-aware interface (blocks, stages, and down sample/skip wiring), it generalizes across residual CNNs and extends to inverted-residual families with minor legalization rules. The result is a compact, practical recipe for deployment-ready models that match or surpass teacher accuracy at a fraction of the compute.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 116,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06854",
        "abs_url": "https://arxiv.org/abs/2511.06854",
        "pdf_url": "https://arxiv.org/pdf/2511.06854",
        "title": "Beyond Observations: Reconstruction Error-Guided Irregularly Sampled Time Series Representation Learning",
        "authors": [
            "Jiexi Liu",
            "Meng Cao",
            "Songcan Chen"
        ],
        "comments": "Accepted by AAAI 2026",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Irregularly sampled time series (ISTS), characterized by non-uniform time intervals with natural missingness, are prevalent in real-world applications. Existing approaches for ISTS modeling primarily rely on observed values to impute unobserved ones or infer latent dynamics. However, these methods overlook a critical source of learning signal: the reconstruction error inherently produced during model training. Such error implicitly reflects how well a model captures the underlying data structure and can serve as an informative proxy for unobserved values. To exploit this insight, we propose iTimER, a simple yet effective self-supervised pre-training framework for ISTS representation learning. iTimER models the distribution of reconstruction errors over observed values and generates pseudo-observations for unobserved timestamps through a mixup strategy between sampled errors and the last available observations. This transforms unobserved timestamps into noise-aware training targets, enabling meaningful reconstruction signals. A Wasserstein metric aligns reconstruction error distributions between observed and pseudo-observed regions, while a contrastive learning objective enhances the discriminability of learned representations. Extensive experiments on classification, interpolation, and forecasting tasks demonstrate that iTimER consistently outperforms state-of-the-art methods under the ISTS setting.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 117,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06856",
        "abs_url": "https://arxiv.org/abs/2511.06856",
        "pdf_url": "https://arxiv.org/pdf/2511.06856",
        "title": "Contact Wasserstein Geodesics for Non-Conservative Schrodinger Bridges",
        "authors": [
            "Andrea Testa",
            "Soren Hauberg",
            "Tamim Asfour",
            "Leonel Rozo"
        ],
        "comments": "38 pages, 18 figures",
        "subjects": "Machine Learning (cs.LG); Differential Geometry (math.DG)",
        "abstract": "The SchrÃ¶dinger Bridge provides a principled framework for modeling stochastic processes between distributions; however, existing methods are limited by energy-conservation assumptions, which constrains the bridge's shape preventing it from model varying-energy phenomena. To overcome this, we introduce the non-conservative generalized SchrÃ¶dinger bridge (NCGSB), a novel, energy-varying reformulation based on contact Hamiltonian mechanics. By allowing energy to change over time, the NCGSB provides a broader class of real-world stochastic processes, capturing richer and more faithful intermediate dynamics. By parameterizing the Wasserstein manifold, we lift the bridge problem to a tractable geodesic computation in a finite-dimensional space. Unlike computationally expensive iterative solutions, our contact Wasserstein geodesic (CWG) is naturally implemented via a ResNet architecture and relies on a non-iterative solver with near-linear complexity. Furthermore, CWG supports guided generation by modulating a task-specific distance metric. We validate our framework on tasks including manifold navigation, molecular dynamics predictions, and image generation, demonstrating its practical benefits and versatility.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 118,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06893",
        "abs_url": "https://arxiv.org/abs/2511.06893",
        "pdf_url": "https://arxiv.org/pdf/2511.06893",
        "title": "DeepBooTS: Dual-Stream Residual Boosting for Drift-Resilient Time-Series Forecasting",
        "authors": [
            "Daojun Liang",
            "Jing Chen",
            "Xiao Wang",
            "Yinglong Wang",
            "Suo Li"
        ],
        "comments": "28 pages,17 pages, Published in AAAI-26",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Time-Series (TS) exhibits pronounced non-stationarity. Consequently, most forecasting methods display compromised robustness to concept drift, despite the prevalent application of instance normalization. We tackle this challenge by first analysing concept drift through a bias-variance lens and proving that weighted ensemble reduces variance without increasing bias. These insights motivate DeepBooTS, a novel end-to-end dual-stream residual-decreasing boosting method that progressively reconstructs the intrinsic signal. In our design, each block of a deep model becomes an ensemble of learners with an auxiliary output branch forming a highway to the final prediction. The block-wise outputs correct the residuals of previous blocks, leading to a learning-driven decomposition of both inputs and targets. This method enhances versatility and interpretability while substantially improving robustness to concept drift. Extensive experiments, including those on large-scale datasets, show that the proposed method outperforms existing methods by a large margin, yielding an average performance improvement of 15.8% across various datasets, establishing a new benchmark for TS forecasting.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 119,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06895",
        "abs_url": "https://arxiv.org/abs/2511.06895",
        "pdf_url": "https://arxiv.org/pdf/2511.06895",
        "title": "On The Presence of Double-Descent in Deep Reinforcement Learning",
        "authors": [
            "Viktor VeselÃ½",
            "Aleksandar Todorov",
            "Matthia Sabatelli"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)",
        "abstract": "The double descent (DD) paradox, where over-parameterized models see generalization improve past the interpolation point, remains largely unexplored in the non-stationary domain of Deep Reinforcement Learning (DRL). We present preliminary evidence that DD exists in model-free DRL, investigating it systematically across varying model capacity using the Actor-Critic framework. We rely on an information-theoretic metric, Policy Entropy, to measure policy uncertainty throughout training. Preliminary results show a clear epoch-wise DD curve; the policy's entrance into the second descent region correlates with a sustained, significant reduction in Policy Entropy. This entropic decay suggests that over-parameterization acts as an implicit regularizer, guiding the policy towards robust, flatter minima in the loss landscape. These findings establish DD as a factor in DRL and provide an information-based mechanism for designing agents that are more general, transferable, and robust.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 120,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06902",
        "abs_url": "https://arxiv.org/abs/2511.06902",
        "pdf_url": "https://arxiv.org/pdf/2511.06902",
        "title": "A Closer Look at Knowledge Distillation in Spiking Neural Network Training",
        "authors": [
            "Xu Liu",
            "Na Xia",
            "Jinxing Zhou",
            "Jingyuan Xu",
            "Dan Guo"
        ],
        "comments": "Accepted by AAAI 2026",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Spiking Neural Networks (SNNs) become popular due to excellent energy efficiency, yet facing challenges for effective model training. Recent works improve this by introducing knowledge distillation (KD) techniques, with the pre-trained artificial neural networks (ANNs) used as teachers and the target SNNs as students. This is commonly accomplished through a straightforward element-wise alignment of intermediate features and prediction logits from ANNs and SNNs, often neglecting the intrinsic differences between their architectures. Specifically, ANN's outputs exhibit a continuous distribution, whereas SNN's outputs are characterized by sparsity and discreteness. To mitigate this issue, we introduce two innovative KD strategies. Firstly, we propose the Saliency-scaled Activation Map Distillation (SAMD), which aligns the spike activation map of the student SNN with the class-aware activation map of the teacher ANN. Rather than performing KD directly on the raw %and distinct features of ANN and SNN, our SAMD directs the student to learn from saliency activation maps that exhibit greater semantic and distribution consistency. Additionally, we propose a Noise-smoothed Logits Distillation (NLD), which utilizes Gaussian noise to smooth the sparse logits of student SNN, facilitating the alignment with continuous logits from teacher ANN. Extensive experiments on multiple datasets demonstrate the effectiveness of our methods. Code is available~\\footnote{this https URL}.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 121,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06913",
        "abs_url": "https://arxiv.org/abs/2511.06913",
        "pdf_url": "https://arxiv.org/pdf/2511.06913",
        "title": "Sampling and Loss Weights in Multi-Domain Training",
        "authors": [
            "Mahdi Salmani",
            "Pratik Worah",
            "Meisam Razaviyayn",
            "Vahab Mirrokni"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "In the training of large deep neural networks, there is a need for vast amounts of training data. To meet this need, data is collected from multiple domains, such as Wikipedia and GitHub. These domains are heterogeneous in both data quality and the diversity of information they provide. This raises the question of how much we should rely on each domain. Several methods have attempted to address this issue by assigning sampling weights to each data domain using heuristics or approximations. As a first step toward a deeper understanding of the role of data mixing, this work revisits the problem by studying two kinds of weights: sampling weights, which control how much each domain contributes in a batch, and loss weights, which scale the loss from each domain during training. Through a rigorous study of linear regression, we show that these two weights play complementary roles. First, they can reduce the variance of gradient estimates in iterative methods such as stochastic gradient descent (SGD). Second, they can improve generalization performance by reducing the generalization gap. We provide both theoretical and empirical support for these claims. We further study the joint dynamics of sampling weights and loss weights, examining how they can be combined to capture both contributions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 122,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06961",
        "abs_url": "https://arxiv.org/abs/2511.06961",
        "pdf_url": "https://arxiv.org/pdf/2511.06961",
        "title": "Hybrid Autoencoders for Tabular Data: Leveraging Model-Based Augmentation in Low-Label Settings",
        "authors": [
            "Erel Naor",
            "Ofir Lindenbaum"
        ],
        "comments": "accepted to neurips 2025, main text is 10 pages",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Deep neural networks often under-perform on tabular data due to their sensitivity to irrelevant features and a spectral bias toward smooth, low-frequency functions. These limitations hinder their ability to capture the sharp, high-frequency signals that often define tabular structure, especially under limited labeled samples. While self-supervised learning (SSL) offers promise in such settings, it remains challenging in tabular domains due to the lack of effective data augmentations. We propose a hybrid autoencoder that combines a neural encoder with an oblivious soft decision tree (OSDT) encoder, each guided by its own stochastic gating network that performs sample-specific feature selection. Together, these structurally different encoders and model-specific gating networks implement model-based augmentation, producing complementary input views tailored to each architecture. The two encoders, trained with a shared decoder and cross-reconstruction loss, learn distinct yet aligned representations that reflect their respective inductive biases. During training, the OSDT encoder (robust to noise and effective at modeling localized, high-frequency structure) guides the neural encoder toward representations more aligned with tabular data. At inference, only the neural encoder is used, preserving flexibility and SSL compatibility. Spectral analysis highlights the distinct inductive biases of each encoder. Our method achieves consistent gains in low-label classification and regression across diverse tabular datasets, outperforming deep and tree-based supervised baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 123,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06973",
        "abs_url": "https://arxiv.org/abs/2511.06973",
        "pdf_url": "https://arxiv.org/pdf/2511.06973",
        "title": "Oh That Looks Familiar: A Novel Similarity Measure for Spreadsheet Template Discovery",
        "authors": [
            "Ananad Krishnakumar",
            "Vengadesh Ravikumaran"
        ],
        "comments": "5 pages, 2 figures, Accepted for EuroIPS: AI for Tabular Data Workshop (2025)",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "Traditional methods for identifying structurally similar spreadsheets fail to capture the spatial layouts and type patterns defining templates. To quantify spreadsheet similarity, we introduce a hybrid distance metric that combines semantic embeddings, data type information, and spatial positioning. In order to calculate spreadsheet similarity, our method converts spreadsheets into cell-level embeddings and then uses aggregation techniques like Chamfer and Hausdorff distances. Experiments across template families demonstrate superior unsupervised clustering performance compared to the graph-based Mondrian baseline, achieving perfect template reconstruction (Adjusted Rand Index of 1.00 versus 0.90) on the FUSTE dataset. Our approach facilitates large-scale automated template discovery, which in turn enables downstream applications such as retrieval-augmented generation over tabular collections, model training, and bulk data cleaning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 124,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06976",
        "abs_url": "https://arxiv.org/abs/2511.06976",
        "pdf_url": "https://arxiv.org/pdf/2511.06976",
        "title": "Rethinking Crystal Symmetry Prediction: A Decoupled Perspective",
        "authors": [
            "Liheng Yu",
            "Zhe Zhao",
            "Xucong Wang",
            "Di Wu",
            "Pengkun Wang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Efficiently and accurately determining the symmetry is a crucial step in the structural analysis of crystalline materials. Existing methods usually mindlessly apply deep learning models while ignoring the underlying chemical rules. More importantly, experiments show that they face a serious sub-property confusion SPC problem. To address the above challenges, from a decoupled perspective, we introduce the XRDecoupler framework, a problem-solving arsenal specifically designed to tackle the SPC problem. Imitating the thinking process of chemists, we innovatively incorporate multidimensional crystal symmetry information as superclass guidance to ensure that the model's prediction process aligns with chemical intuition. We further design a hierarchical PXRD pattern learning model and a multi-objective optimization approach to achieve high-quality representation and balanced optimization. Comprehensive evaluations on three mainstream databases (e.g., CCDC, CoREMOF, and InorganicData) demonstrate that XRDecoupler excels in performance, interpretability, and generalization.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 125,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06978",
        "abs_url": "https://arxiv.org/abs/2511.06978",
        "pdf_url": "https://arxiv.org/pdf/2511.06978",
        "title": "Fast Bayesian Updates via Harmonic Representations",
        "authors": [
            "Di Zhang"
        ],
        "comments": "13 pages",
        "subjects": "Machine Learning (cs.LG); Information Theory (cs.IT); Numerical Analysis (math.NA); Statistics Theory (math.ST)",
        "abstract": "Bayesian inference, while foundational to probabilistic reasoning, is often hampered by the computational intractability of posterior distributions, particularly through the challenging evidence integral. Conventional approaches like Markov Chain Monte Carlo (MCMC) and Variational Inference (VI) face significant scalability and efficiency limitations. This paper introduces a novel, unifying framework for fast Bayesian updates by leveraging harmonic analysis. We demonstrate that representing the prior and likelihood in a suitable orthogonal basis transforms the Bayesian update rule into a spectral convolution. Specifically, the Fourier coefficients of the posterior are shown to be the normalized convolution of the prior and likelihood coefficients. To achieve computational feasibility, we introduce a spectral truncation scheme, which, for smooth functions, yields an exceptionally accurate finite-dimensional approximation and reduces the update to a circular convolution. This formulation allows us to exploit the Fast Fourier Transform (FFT), resulting in a deterministic algorithm with O(N log N) complexity -- a substantial improvement over the O(N^2) cost of naive methods. We establish rigorous mathematical criteria for the applicability of our method, linking its efficiency to the smoothness and spectral decay of the involved distributions. The presented work offers a paradigm shift, connecting Bayesian computation to signal processing and opening avenues for real-time, sequential inference in a wide class of problems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 126,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06979",
        "abs_url": "https://arxiv.org/abs/2511.06979",
        "pdf_url": "https://arxiv.org/pdf/2511.06979",
        "title": "Breaking the Gradient Barrier: Unveiling Large Language Models for Strategic Classification",
        "authors": [
            "Xinpeng Lv",
            "Yunxin Mao",
            "Haoxuan Li",
            "Ke Liang",
            "Jinxuan Yang",
            "Wanrong Huang",
            "Haoang Chi",
            "Huan Chen",
            "Long Lan",
            "Yuanlong Chen",
            "Wenjing Yang",
            "Haotian Wang"
        ],
        "comments": "Accepted by NeurIPS 2025",
        "subjects": "Machine Learning (cs.LG); Computer Science and Game Theory (cs.GT)",
        "abstract": "Strategic classification~(SC) explores how individuals or entities modify their features strategically to achieve favorable classification outcomes. However, existing SC methods, which are largely based on linear models or shallow neural networks, face significant limitations in terms of scalability and capacity when applied to real-world datasets with significantly increasing scale, especially in financial services and the internet sector. In this paper, we investigate how to leverage large language models to design a more scalable and efficient SC framework, especially in the case of growing individuals engaged with decision-making processes. Specifically, we introduce GLIM, a gradient-free SC method grounded in in-context learning. During the feed-forward process of self-attention, GLIM implicitly simulates the typical bi-level optimization process of SC, including both the feature manipulation and decision rule optimization. Without fine-tuning the LLMs, our proposed GLIM enjoys the advantage of cost-effective adaptation in dynamic strategic environments. Theoretically, we prove GLIM can support pre-trained LLMs to adapt to a broad range of strategic manipulations. We validate our approach through experiments with a collection of pre-trained LLMs on real-world and synthetic datasets in financial and internet domains, demonstrating that our GLIM exhibits both robustness and efficiency, and offering an effective solution for large-scale SC tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 127,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06988",
        "abs_url": "https://arxiv.org/abs/2511.06988",
        "pdf_url": "https://arxiv.org/pdf/2511.06988",
        "title": "HCFSLN: Adaptive Hyperbolic Few-Shot Learning for Multimodal Anxiety Detection",
        "authors": [
            "Aditya Sneh",
            "Nilesh Kumar Sahu",
            "Anushka Sanjay Shelke",
            "Arya Adyasha",
            "Haroon R. Lone"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Human-Computer Interaction (cs.HC)",
        "abstract": "Anxiety disorders impact millions globally, yet traditional diagnosis relies on clinical interviews, while machine learning models struggle with overfitting due to limited data. Large-scale data collection remains costly and time-consuming, restricting accessibility. To address this, we introduce the Hyperbolic Curvature Few-Shot Learning Network (HCFSLN), a novel Few-Shot Learning (FSL) framework for multimodal anxiety detection, integrating speech, physiological signals, and video data. HCFSLN enhances feature separability through hyperbolic embeddings, cross-modal attention, and an adaptive gating network, enabling robust classification with minimal data. We collected a multimodal anxiety dataset from 108 participants and benchmarked HCFSLN against six FSL baselines, achieving 88% accuracy, outperforming the best baseline by 14%. These results highlight the effectiveness of hyperbolic space for modeling anxiety-related speech patterns and demonstrate FSL's potential for anxiety classification.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 128,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06991",
        "abs_url": "https://arxiv.org/abs/2511.06991",
        "pdf_url": "https://arxiv.org/pdf/2511.06991",
        "title": "CoLM: Collaborative Large Models via A Client-Server Paradigm",
        "authors": [
            "Siqi Huang",
            "Sida Huang",
            "Hongyuan Zhang"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Large models have achieved remarkable performance across a range of reasoning and understanding tasks. Prior work often utilizes model ensembles or multi-agent systems to collaboratively generate responses, effectively operating in a server-to-server paradigm. However, such approaches do not align well with practical deployment settings, where a limited number of server-side models are shared by many clients under modern internet architectures. In this paper, we introduce \\textbf{CoLM} (\\textbf{Co}llaboration in \\textbf{L}arge-\\textbf{M}odels), a novel framework for collaborative reasoning that redefines cooperation among large models from a client-server perspective. Unlike traditional ensemble methods that rely on simultaneous inference from multiple models to produce a single output, CoLM allows the outputs of multiple models to be aggregated or shared, enabling each client model to independently refine and update its own generation based on these high-quality outputs. This design enables collaborative benefits by fully leveraging both client-side and shared server-side models. We further extend CoLM to vision-language models (VLMs), demonstrating its applicability beyond language tasks. Experimental results across multiple benchmarks show that CoLM consistently improves model performance on previously failed queries, highlighting the effectiveness of collaborative guidance in enhancing single-model capabilities.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 129,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07023",
        "abs_url": "https://arxiv.org/abs/2511.07023",
        "pdf_url": "https://arxiv.org/pdf/2511.07023",
        "title": "Correcting False Alarms from Unseen: Adapting Graph Anomaly Detectors at Test Time",
        "authors": [
            "Junjun Pan",
            "Yixin Liu",
            "Chuan Zhou",
            "Fei Xiong",
            "Alan Wee-Chung Liew",
            "Shirui Pan"
        ],
        "comments": "9 pages, 5 figures, accepted by AAAI 2026",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Graph anomaly detection (GAD), which aims to detect outliers in graph-structured data, has received increasing research attention recently. However, existing GAD methods assume identical training and testing distributions, which is rarely valid in practice. In real-world scenarios, unseen but normal samples may emerge during deployment, leading to a normality shift that degrades the performance of GAD models trained on the original data. Through empirical analysis, we reveal that the degradation arises from (1) semantic confusion, where unseen normal samples are misinterpreted as anomalies due to their novel patterns, and (2) aggregation contamination, where the representations of seen normal nodes are distorted by unseen normals through message aggregation. While retraining or fine-tuning GAD models could be a potential solution to the above challenges, the high cost of model retraining and the difficulty of obtaining labeled data often render this approach impractical in real-world applications. To bridge the gap, we proposed a lightweight and plug-and-play Test-time adaptation framework for correcting Unseen Normal pattErns (TUNE) in GAD. To address semantic confusion, a graph aligner is employed to align the shifted data to the original one at the graph attribute level. Moreover, we utilize the minimization of representation-level shift as a supervision signal to train the aligner, which leverages the estimated aggregation contamination as a key indicator of normality shift. Extensive experiments on 10 real-world datasets demonstrate that TUNE significantly enhances the generalizability of pre-trained GAD models to both synthetic and real unseen normal patterns.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 130,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07032",
        "abs_url": "https://arxiv.org/abs/2511.07032",
        "pdf_url": "https://arxiv.org/pdf/2511.07032",
        "title": "Fair Bayesian Data Selection via Generalized Discrepancy Measures",
        "authors": [
            "Yixuan Zhang",
            "Jiabin Luo",
            "Zhenggang Wang",
            "Feng Zhou",
            "Quyu Kong"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Fairness concerns are increasingly critical as machine learning models are deployed in high-stakes applications. While existing fairness-aware methods typically intervene at the model level, they often suffer from high computational costs, limited scalability, and poor generalization. To address these challenges, we propose a Bayesian data selection framework that ensures fairness by aligning group-specific posterior distributions of model parameters and sample weights with a shared central distribution. Our framework supports flexible alignment via various distributional discrepancy measures, including Wasserstein distance, maximum mean discrepancy, and $f$-divergence, allowing geometry-aware control without imposing explicit fairness constraints. This data-centric approach mitigates group-specific biases in training data and improves fairness in downstream tasks, with theoretical guarantees. Experiments on benchmark datasets show that our method consistently outperforms existing data selection and model-based fairness methods in both fairness and accuracy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 131,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07073",
        "abs_url": "https://arxiv.org/abs/2511.07073",
        "pdf_url": "https://arxiv.org/pdf/2511.07073",
        "title": "Breaking Privacy in Federated Clustering: Perfect Input Reconstruction via Temporal Correlations",
        "authors": [
            "Guang Yang",
            "Lixia Luo",
            "Qiongxiu Li"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Federated clustering allows multiple parties to discover patterns in distributed data without sharing raw samples. To reduce overhead, many protocols disclose intermediate centroids during training. While often treated as harmless for efficiency, whether such disclosure compromises privacy remains an open question. Prior analyses modeled the problem as a so-called Hidden Subset Sum Problem (HSSP) and argued that centroid release may be safe, since classical HSSP attacks fail to recover inputs. We revisit this question and uncover a new leakage mechanism: temporal regularities in $k$-means iterations create exploitable structure that enables perfect input reconstruction. Building on this insight, we propose Trajectory-Aware Reconstruction (TAR), an attack that combines temporal assignment information with algebraic analysis to recover exact original inputs. Our findings provide the first rigorous evidence, supported by a practical attack, that centroid disclosure in federated clustering significantly compromises privacy, exposing a fundamental tension between privacy and efficiency.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 132,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07087",
        "abs_url": "https://arxiv.org/abs/2511.07087",
        "pdf_url": "https://arxiv.org/pdf/2511.07087",
        "title": "Direct Molecular Polarizability Prediction with SO(3) Equivariant Local Frame GNNs",
        "authors": [
            "Jean Philip Filling",
            "Felix Post",
            "Michael Wand",
            "Denis Andrienko"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "We introduce a novel equivariant graph neural network (GNN) architecture designed to predict the tensorial response properties of molecules. Unlike traditional frameworks that focus on regressing scalar quantities and derive tensorial properties from their derivatives, our approach maintains $SO(3)$-equivariance through the use of local coordinate frames. Our GNN effectively captures geometric information by integrating scalar, vector, and tensor channels within a local message-passing framework. To assess the accuracy of our model, we apply it to predict the polarizabilities of molecules in the QM7-X dataset and show that tensorial message passing outperforms scalar message passing models. This work marks an advancement towards developing structured, geometry-aware neural models for molecular property prediction.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 133,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07127",
        "abs_url": "https://arxiv.org/abs/2511.07127",
        "pdf_url": "https://arxiv.org/pdf/2511.07127",
        "title": "REACT-LLM: A Benchmark for Evaluating LLM Integration with Causal Features in Clinical Prognostic Tasks",
        "authors": [
            "Linna Wang",
            "Zhixuan You",
            "Qihui Zhang",
            "Jiunan Wen",
            "Ji Shi",
            "Yimin Chen",
            "Yusen Wang",
            "Fanqi Ding",
            "Ziliang Feng",
            "Li Lu"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Large Language Models (LLMs) and causal learning each hold strong potential for clinical decision making (CDM). However, their synergy remains poorly understood, largely due to the lack of systematic benchmarks evaluating their integration in clinical risk prediction. In real-world healthcare, identifying features with causal influence on outcomes is crucial for actionable and trustworthy predictions. While recent work highlights LLMs' emerging causal reasoning abilities, there lacks comprehensive benchmarks to assess their causal learning and performance informed by causal features in clinical risk prediction. To address this, we introduce REACT-LLM, a benchmark designed to evaluate whether combining LLMs with causal features can enhance clinical prognostic performance and potentially outperform traditional machine learning (ML) methods. Unlike existing LLM-clinical benchmarks that often focus on a limited set of outcomes, REACT-LLM evaluates 7 clinical outcomes across 2 real-world datasets, comparing 15 prominent LLMs, 6 traditional ML models, and 3 causal discovery (CD) algorithms. Our findings indicate that while LLMs perform reasonably in clinical prognostics, they have not yet outperformed traditional ML models. Integrating causal features derived from CD algorithms into LLMs offers limited performance gains, primarily due to the strict assumptions of many CD methods, which are often violated in complex clinical data. While the direct integration yields limited improvement, our benchmark reveals a more promising synergy.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 134,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07158",
        "abs_url": "https://arxiv.org/abs/2511.07158",
        "pdf_url": "https://arxiv.org/pdf/2511.07158",
        "title": "Guiding Generative Models to Uncover Diverse and Novel Crystals via Reinforcement Learning",
        "authors": [
            "Hyunsoo Park",
            "Aron Walsh"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computational Physics (physics.comp-ph)",
        "abstract": "Discovering functional crystalline materials entails navigating an immense combinatorial design space. While recent advances in generative artificial intelligence have enabled the sampling of chemically plausible compositions and structures, a fundamental challenge remains: the objective misalignment between likelihood-based sampling in generative modelling and targeted focus on underexplored regions where novel compounds reside. Here, we introduce a reinforcement learning framework that guides latent denoising diffusion models toward diverse and novel, yet thermodynamically viable crystalline compounds. Our approach integrates group relative policy optimisation with verifiable, multi-objective rewards that jointly balance creativity, stability, and diversity. Beyond de novo generation, we demonstrate enhanced property-guided design that preserves chemical validity, while targeting desired functional properties. This approach establishes a modular foundation for controllable AI-driven inverse design that addresses the novelty-validity trade-off across scientific discovery applications of generative models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 135,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07161",
        "abs_url": "https://arxiv.org/abs/2511.07161",
        "pdf_url": "https://arxiv.org/pdf/2511.07161",
        "title": "LLMscape",
        "authors": [
            "Gottfried Haider",
            "Jie Zhang"
        ],
        "comments": "Accepted to NeurIPS 2025, Creative AI Track",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "LLMscape is an interactive installation that investigates how humans and AI construct meaning under shared conditions of uncertainty. Within a mutable, projection-mapped landscape, human participants reshape the world and engage with multiple AI agents, each developing incomplete and provisional accounts of their environment. Exhibited in Shanghai and continually evolving, the work positions AI not as deterministic tools but as embodied co-witnesses to an unstable world, examining the parallels between human and artificial meaning-making and inviting reflection on our shared epistemic limits.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 136,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07163",
        "abs_url": "https://arxiv.org/abs/2511.07163",
        "pdf_url": "https://arxiv.org/pdf/2511.07163",
        "title": "Combining digital data streams and epidemic networks for real time outbreak detection",
        "authors": [
            "Ruiqi Lyu",
            "Alistair Turcan",
            "Bryan Wilder"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Responding to disease outbreaks requires close surveillance of their trajectories, but outbreak detection is hindered by the high noise in epidemic time series. Aggregating information across data sources has shown great denoising ability in other fields, but remains underexplored in epidemiology. Here, we present LRTrend, an interpretable machine learning framework to identify outbreaks in real time. LRTrend effectively aggregates diverse health and behavioral data streams within one region and learns disease-specific epidemic networks to aggregate information across regions. We reveal diverse epidemic clusters and connections across the United States that are not well explained by commonly used human mobility networks and may be informative for future public health coordination. We apply LRTrend to 2 years of COVID-19 data in 305 hospital referral regions and frequently detect regional Delta and Omicron waves within 2 weeks of the outbreak's start, when case counts are a small fraction of the wave's resulting peak.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 137,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07170",
        "abs_url": "https://arxiv.org/abs/2511.07170",
        "pdf_url": "https://arxiv.org/pdf/2511.07170",
        "title": "On Stealing Graph Neural Network Models",
        "authors": [
            "Marcin Podhajski",
            "Jan DubiÅski",
            "Franziska Boenisch",
            "Adam Dziedzic",
            "Agnieszka PrÄgowska",
            "Tomasz P. Michalak"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Cryptography and Security (cs.CR)",
        "abstract": "Current graph neural network (GNN) model-stealing methods rely heavily on queries to the victim model, assuming no hard query limits. However, in reality, the number of allowed queries can be severely limited. In this paper, we demonstrate how an adversary can extract the GNN with very limited interactions with the model. Our approach first enables the adversary to obtain the model backbone without making direct queries to the victim model and then to strategically utilize a fixed query limit to extract the most informative data. The experiments on eight real-world datasets demonstrate the effectiveness of the attack, even under a very restricted query limit and under defense against model extraction in place. Our findings underscore the need for robust defenses against GNN model extraction threats.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 138,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07198",
        "abs_url": "https://arxiv.org/abs/2511.07198",
        "pdf_url": "https://arxiv.org/pdf/2511.07198",
        "title": "Synergy over Discrepancy: A Partition-Based Approach to Multi-Domain LLM Fine-Tuning",
        "authors": [
            "Hua Ye",
            "Siyuan Chen",
            "Haoliang Zhang",
            "Weihao Luo",
            "Yanbin Li",
            "Xuan Zhang"
        ],
        "comments": "20 pages, 5 figures, 21 tables. Accepted at NeurIPS 2025. Corresponding author: Xuan Zhang (xuanzhang2199@gmail.com)",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Large language models (LLMs) demonstrate impressive generalization abilities, yet adapting them effectively across multiple heterogeneous domains remains challenging due to inter-domain interference. To overcome this challenge, we propose a partition-based multi-stage fine-tuning framework designed to exploit inter-domain synergies while minimizing negative transfer. Our approach strategically partitions domains into subsets (stages) by balancing domain discrepancy, synergy, and model capacity constraints. We theoretically analyze the proposed framework and derive novel generalization bounds that justify our partitioning strategy. Extensive empirical evaluations on various language understanding tasks show that our method consistently outperforms state-of-the-art baselines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 139,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07213",
        "abs_url": "https://arxiv.org/abs/2511.07213",
        "pdf_url": "https://arxiv.org/pdf/2511.07213",
        "title": "DETECT: Data-Driven Evaluation of Treatments Enabled by Classification Transformers",
        "authors": [
            "Yuanheng Mao",
            "Lillian Yang",
            "Stephen Yang",
            "Ethan Shao",
            "Zihan Li"
        ],
        "comments": "5 pages, 4 figures, 2 tables, accepted for presentation by IEEE ICDM 2025 UGHS Symposium and publication with proceedings forthcoming",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Chronic pain is a global health challenge affecting millions of individuals, making it essential for physicians to have reliable and objective methods to measure the functional impact of clinical treatments. Traditionally used methods, like the numeric rating scale, while personalized and easy to use, are subjective due to their self-reported nature. Thus, this paper proposes DETECT (Data-Driven Evaluation of Treatments Enabled by Classification Transformers), a data-driven framework that assesses treatment success by comparing patient activities of daily life before and after treatment. We use DETECT on public benchmark datasets and simulated patient data from smartphone sensors. Our results demonstrate that DETECT is objective yet lightweight, making it a significant and novel contribution to clinical decision-making. By using DETECT, independently or together with other self-reported metrics, physicians can improve their understanding of their treatment impacts, ultimately leading to more personalized and responsive patient care.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 140,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07235",
        "abs_url": "https://arxiv.org/abs/2511.07235",
        "pdf_url": "https://arxiv.org/pdf/2511.07235",
        "title": "Deep Neural Operator Learning for Probabilistic Models",
        "authors": [
            "Erhan Bayraktar",
            "Qi Feng",
            "Zecheng Zhang",
            "Zhaoyu Zhang"
        ],
        "comments": "36 pages, 1 figure",
        "subjects": "Machine Learning (cs.LG); Computational Finance (q-fin.CP)",
        "abstract": "We propose a deep neural-operator framework for a general class of probability models. Under global Lipschitz conditions on the operator over the entire Euclidean space-and for a broad class of probabilistic models-we establish a universal approximation theorem with explicit network-size bounds for the proposed architecture. The underlying stochastic processes are required only to satisfy integrability and general tail-probability conditions. We verify these assumptions for both European and American option-pricing problems within the forward-backward SDE (FBSDE) framework, which in turn covers a broad class of operators arising from parabolic PDEs, with or without free boundaries. Finally, we present a numerical example for a basket of American options, demonstrating that the learned model produces optimal stopping boundaries for new strike prices without retraining.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 141,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07236",
        "abs_url": "https://arxiv.org/abs/2511.07236",
        "pdf_url": "https://arxiv.org/pdf/2511.07236",
        "title": "Does TabPFN Understand Causal Structures?",
        "authors": [
            "Omar Swelam",
            "Lennart Purucker",
            "Jake Robertson",
            "Hanne Raum",
            "Joschka Boedecker",
            "Frank Hutter"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Causal discovery is fundamental for multiple scientific domains, yet extracting causal information from real world data remains a significant challenge. Given the recent success on real data, we investigate whether TabPFN, a transformer-based tabular foundation model pre-trained on synthetic datasets generated from structural causal models, encodes causal information in its internal representations. We develop an adapter framework using a learnable decoder and causal tokens that extract causal signals from TabPFN's frozen embeddings and decode them into adjacency matrices for causal discovery. Our evaluations demonstrate that TabPFN's embeddings contain causal information, outperforming several traditional causal discovery algorithms, with such causal information being concentrated in mid-range layers. These findings establish a new direction for interpretable and adaptable foundation models and demonstrate the potential for leveraging pre-trained tabular models for causal discovery.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 142,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07237",
        "abs_url": "https://arxiv.org/abs/2511.07237",
        "pdf_url": "https://arxiv.org/pdf/2511.07237",
        "title": "The Few Govern the Many:Unveiling Few-Layer Dominance for Time Series Models",
        "authors": [
            "Xin Qiu",
            "Junlong Tong",
            "Yirong Sun",
            "Yunpu Ma",
            "Xiaoyu Shen"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computation and Language (cs.CL)",
        "abstract": "Large-scale models are at the forefront of time series (TS) forecasting, dominated by two paradigms: fine-tuning text-based Large Language Models (LLM4TS) and training Time Series Foundation Models (TSFMs) from scratch. Both approaches share a foundational assumption that scaling up model capacity and data volume leads to improved performance. However, we observe a \\textit{\\textbf{scaling paradox}} in TS models, revealing a puzzling phenomenon that larger models do \\emph{NOT} achieve better performance. Through extensive experiments on two model families across four scales (100M to 1.7B parameters) and diverse data (up to 6B observations), we rigorously confirm that the scaling paradox is a pervasive issue. We then diagnose its root cause by analyzing internal representations, identifying a phenomenon we call \\textit{few-layer dominance}: only a small subset of layers are functionally important, while the majority are redundant, under-utilized, and can even distract training. Based on this discovery, we propose a practical method to automatically identify and retain only these dominant layers. In our models, retaining only 21\\% of the parameters achieves up to a 12\\% accuracy improvement and a 2.7$\\times$ inference speedup. We validate the universality of our method on 8 prominent SOTA models (LLM4TS and TSFMs, 90M to 6B), showing that retaining less than 30\\% of layers achieves comparable or superior accuracy in over 95\\% of tasks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 143,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07272",
        "abs_url": "https://arxiv.org/abs/2511.07272",
        "pdf_url": "https://arxiv.org/pdf/2511.07272",
        "title": "Understanding the role of depth in the neural tangent kernel for overparameterized neural networks",
        "authors": [
            "William St-Arnaud",
            "Margarida Carvalho",
            "Golnoosh Farnadi"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Overparameterized fully-connected neural networks have been shown to behave like kernel models when trained with gradient descent, under mild conditions on the width, the learning rate, and the parameter initialization. In the limit of infinitely large widths and small learning rate, the kernel that is obtained allows to represent the output of the learned model with a closed-form solution. This closed-form solution hinges on the invertibility of the limiting kernel, a property that often holds on real-world datasets. In this work, we analyze the sensitivity of large ReLU networks to increasing depths by characterizing the corresponding limiting kernel. Our theoretical results demonstrate that the normalized limiting kernel approaches the matrix of ones. In contrast, they show the corresponding closed-form solution approaches a fixed limit on the sphere. We empirically evaluate the order of magnitude in network depth required to observe this convergent behavior, and we describe the essential properties that enable the generalization of our results to other kernels.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 144,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07274",
        "abs_url": "https://arxiv.org/abs/2511.07274",
        "pdf_url": "https://arxiv.org/pdf/2511.07274",
        "title": "Multi-modal Dynamic Proxy Learning for Personalized Multiple Clustering",
        "authors": [
            "Jinfeng Xu",
            "Zheyu Chen",
            "Shuo Yang",
            "Jinze Li",
            "Ziyue Peng",
            "Zewei Liu",
            "Hewei Wang",
            "Jiayi Zhang",
            "Edith C. H. Ngai"
        ],
        "comments": "Accepted by AAAI 2026",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Multiple clustering aims to discover diverse latent structures from different perspectives, yet existing methods generate exhaustive clusterings without discerning user interest, necessitating laborious manual screening. Current multi-modal solutions suffer from static semantic rigidity: predefined candidate words fail to adapt to dataset-specific concepts, and fixed fusion strategies ignore evolving feature interactions. To overcome these limitations, we propose Multi-DProxy, a novel multi-modal dynamic proxy learning framework that leverages cross-modal alignment through learnable textual proxies. Multi-DProxy introduces 1) gated cross-modal fusion that synthesizes discriminative joint representations by adaptively modeling feature interactions. 2) dual-constraint proxy optimization where user interest constraints enforce semantic consistency with domain concepts while concept constraints employ hard example mining to enhance cluster discrimination. 3) dynamic candidate management that refines textual proxies through iterative clustering feedback. Therefore, Multi-DProxy not only effectively captures a user's interest through proxies but also enables the identification of relevant clusterings with greater precision. Extensive experiments demonstrate state-of-the-art performance with significant improvements over existing methods across a broad set of multi-clustering benchmarks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 145,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07276",
        "abs_url": "https://arxiv.org/abs/2511.07276",
        "pdf_url": "https://arxiv.org/pdf/2511.07276",
        "title": "RobustA: Robust Anomaly Detection in Multimodal Data",
        "authors": [
            "Salem AlMarri",
            "Muhammad Irzam Liaqat",
            "Muhammad Zaigham Zaheer",
            "Shah Nawaz",
            "Karthik Nandakumar",
            "Markus Schedl"
        ],
        "comments": "Submitted to IEEE Transactions on Image Processing",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "In recent years, multimodal anomaly detection methods have demonstrated remarkable performance improvements over video-only models. However, real-world multimodal data is often corrupted due to unforeseen environmental distortions. In this paper, we present the first-of-its-kind work that comprehensively investigates the adverse effects of corrupted modalities on multimodal anomaly detection task. To streamline this work, we propose RobustA, a carefully curated evaluation dataset to systematically observe the impacts of audio and visual corruptions on the overall effectiveness of anomaly detection systems. Furthermore, we propose a multimodal anomaly detection method, which shows notable resilience against corrupted modalities. The proposed method learns a shared representation space for different modalities and employs a dynamic weighting scheme during inference based on the estimated level of corruption. Our work represents a significant step forward in enabling the real-world application of multimodal anomaly detection, addressing situations where the likely events of modality corruptions occur. The proposed evaluation dataset with corrupted modalities and respective extracted features will be made publicly available.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 146,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07282",
        "abs_url": "https://arxiv.org/abs/2511.07282",
        "pdf_url": "https://arxiv.org/pdf/2511.07282",
        "title": "MG-HGNN: A Heterogeneous GNN Framework for Indoor Wi-Fi Fingerprint-Based Localization",
        "authors": [
            "Yibu Wang",
            "Zhaoxin Zhang",
            "Ning Li",
            "Xinlong Zhao",
            "Dong Zhao",
            "Tianzi Zhao"
        ],
        "comments": "16 pages, 11 figures, 11 tables",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Received signal strength indicator (RSSI) is the primary representation of Wi-Fi fingerprints and serves as a crucial tool for indoor localization. However, existing RSSI-based positioning methods often suffer from reduced accuracy due to environmental complexity and challenges in processing multi-source information. To address these issues, we propose a novel multi-graph heterogeneous GNN framework (MG-HGNN) to enhance spatial awareness and improve positioning performance. In this framework, two graph construction branches perform node and edge embedding, respectively, to generate informative graphs. Subsequently, a heterogeneous graph neural network is employed for graph representation learning, enabling accurate positioning. The MG-HGNN framework introduces the following key innovations: 1) multi-type task-directed graph construction that combines label estimation and feature encoding for richer graph information; 2) a heterogeneous GNN structure that enhances the performance of conventional GNN models. Evaluations on the UJIIndoorLoc and UTSIndoorLoc public datasets demonstrate that MG-HGNN not only achieves superior performance compared to several state-of-the-art methods, but also provides a novel perspective for enhancing GNN-based localization methods. Ablation studies further confirm the rationality and effectiveness of the proposed framework.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 147,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07308",
        "abs_url": "https://arxiv.org/abs/2511.07308",
        "pdf_url": "https://arxiv.org/pdf/2511.07308",
        "title": "Can Training Dynamics of Scale-Invariant Neural Networks Be Explained by the Thermodynamics of an Ideal Gas?",
        "authors": [
            "Ildus Sadrtdinov",
            "Ekaterina Lobacheva",
            "Ivan Klimov",
            "Mikhail I. Katsnelson",
            "Dmitry Vetrov"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Understanding the training dynamics of deep neural networks remains a major open problem, with physics-inspired approaches offering promising insights. Building on this perspective, we develop a thermodynamic framework to describe the stationary distributions of stochastic gradient descent (SGD) with weight decay for scale-invariant neural networks, a setting that both reflects practical architectures with normalization layers and permits theoretical analysis. We establish analogies between training hyperparameters (e.g., learning rate, weight decay) and thermodynamic variables such as temperature, pressure, and volume. Starting with a simplified isotropic noise model, we uncover a close correspondence between SGD dynamics and ideal gas behavior, validated through theory and simulation. Extending to training of neural networks, we show that key predictions of the framework, including the behavior of stationary entropy, align closely with experimental observations. This framework provides a principled foundation for interpreting training dynamics and may guide future work on hyperparameter tuning and the design of learning rate schedulers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 148,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07328",
        "abs_url": "https://arxiv.org/abs/2511.07328",
        "pdf_url": "https://arxiv.org/pdf/2511.07328",
        "title": "Q-RAG: Long Context Multi-step Retrieval via Value-based Embedder Training",
        "authors": [
            "Artyom Sorokin",
            "Nazar Buzun",
            "Alexander Anokhin",
            "Oleg Inozemcev",
            "Egor Vedernikov",
            "Petr Anokhin",
            "Mikhail Burtsev",
            "Trushkov Alexey",
            "Yin Wenshuai",
            "Evgeny Burnaev"
        ],
        "comments": "16 pages, 3 figures, 2 tables",
        "subjects": "Machine Learning (cs.LG); Information Retrieval (cs.IR)",
        "abstract": "Retrieval-Augmented Generation (RAG) methods enhance LLM performance by efficiently filtering relevant context for LLMs, reducing hallucinations and inference cost. However, most existing RAG methods focus on single-step retrieval, which is often insufficient for answering complex questions that require multi-step search. Recently, multi-step retrieval approaches have emerged, typically involving the fine-tuning of small LLMs to perform multi-step retrieval. This type of fine-tuning is highly resource-intensive and does not enable the use of larger LLMs. In this work, we propose Q-RAG, a novel approach that fine-tunes the Embedder model for multi-step retrieval using reinforcement learning (RL). Q-RAG offers a competitive, resource-efficient alternative to existing multi-step retrieval methods for open-domain question answering and achieves state-of-the-art results on the popular long-context benchmarks Babilong and RULER for contexts up to 10M tokens.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 149,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07329",
        "abs_url": "https://arxiv.org/abs/2511.07329",
        "pdf_url": "https://arxiv.org/pdf/2511.07329",
        "title": "Preparation of Fractal-Inspired Computational Architectures for Advanced Large Language Model Analysis",
        "authors": [
            "Yash Mittal",
            "Dmitry Ignatov",
            "Radu Timofte"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)",
        "abstract": "It introduces FractalNet, a fractal-inspired computational architectures for advanced large language model analysis that mainly challenges model diversity on a large scale in an efficient manner. The new set-up involves a template-driven generator, runner, and evaluation framework that, through systematic permutations of convolutional, normalization, activation, and dropout layers, can create more than 1,200 variants of neural networks. Fractal templates allow for structural recursion and multi-column pathways, thus, models become deeper and wider in a balanced way. Training utilizes PyTorch, Automatic Mixed Precision (AMP), and gradient checkpointing and is carried out on the CIFAR-10 dataset for five epochs. The outcomes show that fractal-based architectures are capable of strong performance and are computationally efficient. The paper positions fractal design as a feasible and resource-efficient method of automated architecture exploration.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 150,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07343",
        "abs_url": "https://arxiv.org/abs/2511.07343",
        "pdf_url": "https://arxiv.org/pdf/2511.07343",
        "title": "TNT: Improving Chunkwise Training for Test-Time Memorization",
        "authors": [
            "Zeman Li",
            "Ali Behrouz",
            "Yuan Deng",
            "Peilin Zhong",
            "Praneeth Kacham",
            "Mahdi Karami",
            "Meisam Razaviyayn",
            "Vahab Mirrokni"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Recurrent neural networks (RNNs) with deep test-time memorization modules, such as Titans and TTT, represent a promising, linearly-scaling paradigm distinct from Transformers. While these expressive models do not yet match the peak performance of state-of-the-art Transformers, their potential has been largely untapped due to prohibitively slow training and low hardware utilization. Existing parallelization methods force a fundamental conflict governed by the chunksize hyperparameter: large chunks boost speed but degrade performance, necessitating a fixed, suboptimal compromise. To solve this challenge, we introduce TNT, a novel training paradigm that decouples training efficiency from inference performance through a two-stage process. Stage one is an efficiency-focused pre-training phase utilizing a hierarchical memory. A global module processes large, hardware-friendly chunks for long-range context, while multiple parallel local modules handle fine-grained details. Crucially, by periodically resetting local memory states, we break sequential dependencies to enable massive context parallelization. Stage two is a brief fine-tuning phase where only the local memory modules are adapted to a smaller, high-resolution chunksize, maximizing accuracy with minimal overhead. Evaluated on Titans and TTT models, TNT achieves a substantial acceleration in training speed-up to 17 times faster than the most accurate baseline configuration - while simultaneously improving model accuracy. This improvement removes a critical scalability barrier, establishing a practical foundation for developing expressive RNNs and facilitating future work to close the performance gap with Transformers.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 151,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07365",
        "abs_url": "https://arxiv.org/abs/2511.07365",
        "pdf_url": "https://arxiv.org/pdf/2511.07365",
        "title": "Private Sketches for Linear Regression",
        "authors": [
            "Shrutimoy Das",
            "Debanuj Nayak",
            "Anirban Dasgupta"
        ],
        "comments": "13 pages",
        "subjects": "Machine Learning (cs.LG); Machine Learning (stat.ML)",
        "abstract": "Linear regression is frequently applied in a variety of domains. In order to improve the efficiency of these methods, various methods have been developed that compute summaries or \\emph{sketches} of the datasets. Certain domains, however, contain sensitive data which necessitates that the application of these statistical methods does not reveal private information. Differentially private (DP) linear regression methods have been developed for mitigating this problem. These techniques typically involve estimating a noisy version of the parameter vector. Instead, we propose releasing private sketches of the datasets. We present differentially private sketches for the problems of least squares regression, as well as least absolute deviations regression. The availability of these private sketches facilitates the application of commonly available solvers for regression, without the risk of privacy leakage.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 152,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07372",
        "abs_url": "https://arxiv.org/abs/2511.07372",
        "pdf_url": "https://arxiv.org/pdf/2511.07372",
        "title": "Provable Benefit of Curriculum in Transformer Tree-Reasoning Post-Training",
        "authors": [
            "Dake Bu",
            "Wei Huang",
            "Andi Han",
            "Atsushi Nitanda",
            "Hau-San Wong",
            "Qingfu Zhang",
            "Taiji Suzuki"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Recent curriculum techniques in the post-training stage of LLMs have been widely observed to outperform non-curriculum approaches in enhancing reasoning performance, yet a principled understanding of why and to what extent they work remains elusive. To address this gap, we develop a theoretical framework grounded in the intuition that progressively learning through manageable steps is more efficient than directly tackling a hard reasoning task, provided each stage stays within the model's effective competence. Under mild complexity conditions linking consecutive curriculum stages, we show that curriculum post-training avoids the exponential complexity bottleneck. To substantiate this result, drawing insights from the Chain-of-Thoughts (CoTs) solving mathematical problems such as Countdown and parity, we model CoT generation as a states-conditioned autoregressive reasoning tree, define a uniform-branching base model to capture pretrained behavior, and formalize curriculum stages as either depth-increasing (longer reasoning chains) or hint-decreasing (shorter prefixes) subtasks. Our analysis shows that, under outcome-only reward signals, reinforcement learning finetuning achieves high accuracy with polynomial sample complexity, whereas direct learning suffers from an exponential bottleneck. We further establish analogous guarantees for test-time scaling, where curriculum-aware querying reduces both reward oracle calls and sampling cost from exponential to polynomial order.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 153,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07379",
        "abs_url": "https://arxiv.org/abs/2511.07379",
        "pdf_url": "https://arxiv.org/pdf/2511.07379",
        "title": "LoReTTA: A Low Resource Framework To Poison Continuous Time Dynamic Graphs",
        "authors": [
            "Himanshu Pal",
            "Venkata Sai Pranav Bachina",
            "Ankit Gangwal",
            "Charu Sharma"
        ],
        "comments": "Accepted at AAAI 2026",
        "subjects": "Machine Learning (cs.LG); Artificial Intelligence (cs.AI)",
        "abstract": "Temporal Graph Neural Networks (TGNNs) are increasingly used in high-stakes domains, such as financial forecasting, recommendation systems, and fraud detection. However, their susceptibility to poisoning attacks poses a critical security risk. We introduce LoReTTA (Low Resource Two-phase Temporal Attack), a novel adversarial framework on Continuous-Time Dynamic Graphs, which degrades TGNN performance by an average of 29.47% across 4 widely benchmark datasets and 4 State-of-the-Art (SotA) models. LoReTTA operates through a two-stage approach: (1) sparsify the graph by removing high-impact edges using any of the 16 tested temporal importance metrics, (2) strategically replace removed edges with adversarial negatives via LoReTTA's novel degree-preserving negative sampling algorithm. Our plug-and-play design eliminates the need for expensive surrogate models while adhering to realistic unnoticeability constraints. LoReTTA degrades performance by upto 42.0% on MOOC, 31.5% on Wikipedia, 28.8% on UCI, and 15.6% on Enron. LoReTTA outperforms 11 attack baselines, remains undetectable to 4 leading anomaly detection systems, and is robust to 4 SotA adversarial defense training methods, establishing its effectiveness, unnoticeability, and robustness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 154,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07390",
        "abs_url": "https://arxiv.org/abs/2511.07390",
        "pdf_url": "https://arxiv.org/pdf/2511.07390",
        "title": "A Diffusion Model to Shrink Proteins While Maintaining Their Function",
        "authors": [
            "Ethan Baron",
            "Alan N. Amin",
            "Ruben Weitzman",
            "Debora Marks",
            "Andrew Gordon Wilson"
        ],
        "comments": "Code available at this https URL",
        "subjects": "Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)",
        "abstract": "Many proteins useful in modern medicine or bioengineering are challenging to make in the lab, fuse with other proteins in cells, or deliver to tissues in the body, because their sequences are too long. Shortening these sequences typically involves costly, time-consuming experimental campaigns. Ideally, we could instead use modern models of massive databases of sequences from nature to learn how to propose shrunken proteins that resemble sequences found in nature. Unfortunately, these models struggle to efficiently search the combinatorial space of all deletions, and are not trained with inductive biases to learn how to delete. To address this gap, we propose SCISOR, a novel discrete diffusion model that deletes letters from sequences to generate protein samples that resemble those found in nature. To do so, SCISOR trains a de-noiser to reverse a forward noising process that adds random insertions to natural sequences. As a generative model, SCISOR fits evolutionary sequence data competitively with previous large models. In evaluation, SCISOR achieves state-of-the-art predictions of the functional effects of deletions on ProteinGym. Finally, we use the SCISOR de-noiser to shrink long protein sequences, and show that its suggested deletions result in significantly more realistic proteins and more often preserve functional motifs than previous models of evolutionary sequences.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 155,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07396",
        "abs_url": "https://arxiv.org/abs/2511.07396",
        "pdf_url": "https://arxiv.org/pdf/2511.07396",
        "title": "C3PO: Optimized Large Language Model Cascades with Probabilistic Cost Constraints for Reasoning",
        "authors": [
            "Antonios Valkanas",
            "Soumyasundar Pal",
            "Pavel Rumiantsev",
            "Yingxue Zhang",
            "Mark Coates"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Large language models (LLMs) have achieved impressive results on complex reasoning tasks, but their high inference cost remains a major barrier to real-world deployment. A promising solution is to use cascaded inference, where small, cheap models handle easy queries, and only the hardest examples are escalated to more powerful models. However, existing cascade methods typically rely on supervised training with labeled data, offer no theoretical generalization guarantees, and provide limited control over test-time computational cost. We introduce C3PO (Cost Controlled Cascaded Prediction Optimization), a self-supervised framework for optimizing LLM cascades under probabilistic cost constraints. By focusing on minimizing regret with respect to the most powerful model (MPM), C3PO avoids the need for labeled data by constructing a cascade using only unlabeled model outputs. It leverages conformal prediction to bound the probability that inference cost exceeds a user-specified budget. We provide theoretical guarantees on both cost control and generalization error, and show that our optimization procedure is effective even with small calibration sets. Empirically, C3PO achieves state-of-the-art performance across a diverse set of reasoning benchmarks including GSM8K, MATH-500, BigBench-Hard and AIME, outperforming strong LLM cascading baselines in both accuracy and cost-efficiency. Our results demonstrate that principled, label-free cascade optimization can enable scalable LLM deployment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 156,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07406",
        "abs_url": "https://arxiv.org/abs/2511.07406",
        "pdf_url": "https://arxiv.org/pdf/2511.07406",
        "title": "Entangled SchrÃ¶dinger Bridge Matching",
        "authors": [
            "Sophia Tang",
            "Yinuo Zhang",
            "Pranam Chatterjee"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG); Biomolecules (q-bio.BM)",
        "abstract": "Simulating trajectories of multi-particle systems on complex energy landscapes is a central task in molecular dynamics (MD) and drug discovery, but remains challenging at scale due to computationally expensive and long simulations. Previous approaches leverage techniques such as flow or SchrÃ¶dinger bridge matching to implicitly learn joint trajectories through data snapshots. However, many systems, including biomolecular systems and heterogeneous cell populations, undergo dynamic interactions that evolve over their trajectory and cannot be captured through static snapshots. To close this gap, we introduce Entangled SchrÃ¶dinger Bridge Matching (EntangledSBM), a framework that learns the first- and second-order stochastic dynamics of interacting, multi-particle systems where the direction and magnitude of each particle's path depend dynamically on the paths of the other particles. We define the Entangled SchrÃ¶dinger Bridge (EntangledSB) problem as solving a coupled system of bias forces that entangle particle velocities. We show that our framework accurately simulates heterogeneous cell populations under perturbations and rare transitions in high-dimensional biomolecular systems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 157,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07419",
        "abs_url": "https://arxiv.org/abs/2511.07419",
        "pdf_url": "https://arxiv.org/pdf/2511.07419",
        "title": "Routing Manifold Alignment Improves Generalization of Mixture-of-Experts LLMs",
        "authors": [
            "Zhongyang Li",
            "Ziyue Li",
            "Tianyi Zhou"
        ],
        "comments": "",
        "subjects": "Machine Learning (cs.LG)",
        "abstract": "Sparse Mixture-of-Experts (MoE) have been widely adopted in recent large language models since it can efficiently scale up the model capability without increasing the inference cost. However, evaluations on broad downstream tasks reveal a consistent suboptimality of the routers in existing MoE LLMs, which results in a severe performance gap (e.g., 10-20% in accuracy) to the optimal routing. In this paper, we show that aligning the manifold of routing weights with that of task embedding can effectively reduce the gap and improve MoE LLMs' generalization performance. Our method, \"Routing Manifold Alignment (RoMA)\", introduces an additional manifold regularization term in the post-training objective and only requires lightweight finetuning of routers (with other parameters frozen). Specifically, the regularization encourages the routing weights of each sample to be close to those of its successful neighbors (whose routing weights lead to correct answers) in a task embedding space. Consequently, samples targeting similar tasks will share similar expert choices across layers. Building such bindings between tasks and experts over different samples is essential to achieve better generalization. Moreover, RoMA demonstrates the advantage of unifying the task understanding (by embedding models) with solution generation (by MoE LLMs). In experiments, we finetune routers in OLMoE, DeepSeekMoE, and Qwen3-MoE using RoMA. Evaluations on diverse benchmarks and extensive comparisons with baselines show the substantial improvement brought by RoMA.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 158,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05497",
        "abs_url": "https://arxiv.org/abs/2511.05497",
        "pdf_url": "https://arxiv.org/pdf/2511.05497",
        "title": "Socially Aware Music Recommendation: A Multi-Modal Graph Neural Networks for Collaborative Music Consumption and Community-Based Engagement",
        "authors": [
            "Kajwan Ziaoddini"
        ],
        "comments": "",
        "subjects": "Information Retrieval (cs.IR); Machine Learning (cs.LG); Multimedia (cs.MM)",
        "abstract": "This study presents a novel Multi-Modal Graph Neural Network (MM-GNN) framework for socially aware music recommendation, designed to enhance personalization and foster community-based engagement. The proposed model introduces a fusion-free deep mutual learning strategy that aligns modality-specific representations from lyrics, audio, and visual data while maintaining robustness against missing modalities. A heterogeneous graph structure is constructed to capture both user-song interactions and user-user social relationships, enabling the integration of individual preferences with social influence. Furthermore, emotion-aware embeddings derived from acoustic and textual signals contribute to emotionally aligned recommendations. Experimental evaluations on benchmark datasets demonstrate that MM-GNN significantly outperforms existing state-of-the-art methods across various performance metrics. Ablation studies further validate the critical impact of each model component, confirming the effectiveness of the framework in delivering accurate and socially contextualized music recommendations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 159,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05503",
        "abs_url": "https://arxiv.org/abs/2511.05503",
        "pdf_url": "https://arxiv.org/pdf/2511.05503",
        "title": "iEEG Seizure Detection with a Sparse Hyperdimensional Computing Accelerator",
        "authors": [
            "Stef Cuyckens",
            "Ryan Antonio",
            "Chao Fang",
            "Marian Verhelst"
        ],
        "comments": "To appear at the 20th International Conference on PhD Research in Microelectronics and Electronics (PRIME 2025)",
        "subjects": "Hardware Architecture (cs.AR); Machine Learning (cs.LG)",
        "abstract": "Implantable devices for reliable intracranial electroencephalography (iEEG) require efficient, accurate, and real-time detection of seizures. Dense hyperdimensional computing (HDC) proves to be efficient over neural networks; however, it still consumes considerable switching power for an ultra-low energy application. Sparse HDC, on the other hand, has the potential of further reducing the energy consumption, yet at the expense of having to support more complex operations and introducing an extra hyperparameter, the maximum hypervector density. To improve the energy and area efficiency of the sparse HDC operations, this work introduces the compressed item memory (CompIM) and simplifies the spatial bundling. We also analyze how a proper hyperparameter choice improves the detection delay compared to dense HDC. Ultimately, our optimizations achieve a 1.73x more energy- and 2.20x more area-efficient hardware design than the naive sparse implementation. We are also 7.50x more energy- and 3.24x more area-efficient than the dense HDC implementation. This work highlights the hardware advantages of sparse HDC, demonstrating its potential to enable smaller brain implants with a substantially extended battery life compared to the current state-of-the-art.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 160,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05536",
        "abs_url": "https://arxiv.org/abs/2511.05536",
        "pdf_url": "https://arxiv.org/pdf/2511.05536",
        "title": "Gravity-Awareness: Deep Learning Models and LLM Simulation of Human Awareness in Altered Gravity",
        "authors": [
            "Bakytzhan Alibekov",
            "Alina Gutoreva",
            "Elisa Raffaella-Ferre"
        ],
        "comments": "64 pages, 8 figures, 2 datasets, 1 protocol",
        "subjects": "Neurons and Cognition (q-bio.NC); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Signal Processing (eess.SP)",
        "abstract": "Earth's gravity has fundamentally shaped human development by guiding the brain's integration of vestibular, visual, and proprioceptive inputs into an internal model of gravity: a dynamic neural representation enabling prediction and interpretation of gravitational forces. This work presents a dual computational framework to quantitatively model these adaptations. The first component is a lightweight Multi-Layer Perceptron (MLP) that predicts g-load-dependent changes in key electroencephalographic (EEG) frequency bands, representing the brain's cortical state. The second component utilizes a suite of independent Gaussian Processes (GPs) to model the body's broader physiological state, including Heart Rate Variability (HRV), Electrodermal Activity (EDA), and motor behavior. Both models were trained on data derived from a comprehensive review of parabolic flight literature, using published findings as anchor points to construct robust, continuous functions. To complement this quantitative analysis, we simulated subjective human experience under different gravitational loads, ranging from microgravity (0g) and partial gravity (Moon 0.17g, Mars 0.38g) to hypergravity associated with spacecraft launch and re-entry (1.8g), using a large language model (Claude 3.5 Sonnet). The model was prompted with physiological parameters to generate introspective narratives of alertness and self-awareness, which closely aligned with the quantitative findings from both the EEG and physiological models. This combined framework integrates quantitative physiological modeling with generative cognitive simulation, offering a novel approach to understanding and predicting human performance in altered gravity",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 161,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05537",
        "abs_url": "https://arxiv.org/abs/2511.05537",
        "pdf_url": "https://arxiv.org/pdf/2511.05537",
        "title": "Bridging Accuracy and Explainability in EEG-based Graph Attention Network for Depression Detection",
        "authors": [
            "Soujanya Hazra",
            "Sanjay Ghosh"
        ],
        "comments": "13 pages, 3 tables, and 7 fugures",
        "subjects": "Signal Processing (eess.SP); Machine Learning (cs.LG); Image and Video Processing (eess.IV); Neurons and Cognition (q-bio.NC)",
        "abstract": "Depression is a major cause of global mental illness and significantly influences suicide rates. Timely and accurate diagnosis is essential for effective intervention. Electroencephalography (EEG) provides a non-invasive and accessible method for examining cerebral activity and identifying disease-associated patterns. We propose a novel graph-based deep learning framework, named Edge-gated, axis-mixed Pooling Attention Network (ExPANet), for differentiating major depressive disorder (MDD) patients from healthy controls (HC). EEG recordings undergo preprocessing to eliminate artifacts and are segmented into short periods of activity. We extract 14 features from each segment, which include time, frequency, fractal, and complexity domains. Electrodes are represented as nodes, whereas edges are determined by the phase-locking value (PLV) to represent functional connectivity. The generated brain graphs are examined utilizing an adapted graph attention network. This architecture acquires both localized electrode characteristics and comprehensive functional connectivity patterns. The proposed framework attains superior performance relative to current EEG-based approaches across two different datasets. A fundamental advantage of our methodology is its explainability. We evaluated the significance of features, channels, and edges, in addition to intrinsic attention weights. These studies highlight features, cerebral areas, and connectivity associations that are especially relevant to MDD, many of which correspond with clinical data. Our findings demonstrate a reliable and transparent method for EEG-based screening of MDD, using deep learning with clinically relevant results.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 162,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05541",
        "abs_url": "https://arxiv.org/abs/2511.05541",
        "pdf_url": "https://arxiv.org/pdf/2511.05541",
        "title": "Temporal Sparse Autoencoders: Leveraging the Sequential Nature of Language for Interpretability",
        "authors": [
            "Usha Bhalla",
            "Alex Oesterling",
            "Claudio Mayrink Verdun",
            "Himabindu Lakkaraju",
            "Flavio P. Calmon"
        ],
        "comments": "23 Pages, 10 figures",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Translating the internal representations and computations of models into concepts that humans can understand is a key goal of interpretability. While recent dictionary learning methods such as Sparse Autoencoders (SAEs) provide a promising route to discover human-interpretable features, they suffer from a variety of problems, including a systematic failure to capture the rich conceptual information that drives linguistic understanding. Instead, they exhibit a bias towards shallow, token-specific, or noisy features, such as \"the phrase 'The' at the start of sentences\". In this work, we propose that this is due to a fundamental issue with how dictionary learning methods for LLMs are trained. Language itself has a rich, well-studied structure spanning syntax, semantics, and pragmatics; however, current unsupervised methods largely ignore this linguistic knowledge, leading to poor feature discovery that favors superficial patterns over meaningful concepts. We focus on a simple but important aspect of language: semantic content has long-range dependencies and tends to be smooth over a sequence, whereas syntactic information is much more local. Building on this insight, we introduce Temporal Sparse Autoencoders (T-SAEs), which incorporate a novel contrastive loss encouraging consistent activations of high-level features over adjacent tokens. This simple yet powerful modification enables SAEs to disentangle semantic from syntactic features in a self-supervised manner. Across multiple datasets and models, T-SAEs recover smoother, more coherent semantic concepts without sacrificing reconstruction quality. Strikingly, they exhibit clear semantic structure despite being trained without explicit semantic signal, offering a new pathway for unsupervised interpretability in language models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 163,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05550",
        "abs_url": "https://arxiv.org/abs/2511.05550",
        "pdf_url": "https://arxiv.org/pdf/2511.05550",
        "title": "Factual and Musical Evaluation Metrics for Music Language Models",
        "authors": [
            "Daniel Chenyu Lin",
            "Michael Freeman",
            "John Thickstun"
        ],
        "comments": "18 pages; first submission",
        "subjects": "Sound (cs.SD); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Music language models (Music LMs), like vision language models, leverage multimodal representations to answer natural language queries about musical audio recordings. Although Music LMs are reportedly improving, we find that current evaluations fail to capture whether their answers are correct. Specifically, for all Music LMs that we examine, widely-used evaluation metrics such as BLEU, METEOR, and BERTScore fail to measure anything beyond linguistic fluency of the model's responses. To measure the true performance of Music LMs, we propose (1) a better general-purpose evaluation metric for Music LMs adapted to the music domain and (2) a factual evaluation framework to quantify the correctness of a Music LM's responses. Our framework is agnostic to the modality of the question-answering model and could be generalized to quantify performance in other open-ended question-answering domains. We use open datasets in our experiments and will release all code on publication.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 164,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05567",
        "abs_url": "https://arxiv.org/abs/2511.05567",
        "pdf_url": "https://arxiv.org/pdf/2511.05567",
        "title": "Automatic Extraction of Road Networks by using Teacher-Student Adaptive Structural Deep Belief Network and Its Application to Landslide Disaster",
        "authors": [
            "Shin Kamada",
            "Takumi Ichimura"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "An adaptive structural learning method of Restricted Boltzmann Machine (RBM) and Deep Belief Network (DBN) has been developed as one of prominent deep learning models. The neuron generation-annihilation algorithm in RBM and layer generation algorithm in DBN make an optimal network structure for given input during the learning. In this paper, our model is applied to an automatic recognition method of road network system, called RoadTracer. RoadTracer can generate a road map on the ground surface from aerial photograph data. A novel method of RoadTracer using the Teacher-Student based ensemble learning model of Adaptive DBN is proposed, since the road maps contain many complicated features so that a model with high representation power to detect should be required. The experimental results showed the detection accuracy of the proposed model was improved from 40.0\\% to 89.0\\% on average in the seven major cities among the test dataset. In addition, we challenged to apply our method to the detection of available roads when landslide by natural disaster is occurred, in order to rapidly obtain a way of transportation. For fast inference, a small size of the trained model was implemented on a small embedded edge device as lightweight deep learning. We reported the detection results for the satellite image before and after the rainfall disaster in Japan.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 165,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05584",
        "abs_url": "https://arxiv.org/abs/2511.05584",
        "pdf_url": "https://arxiv.org/pdf/2511.05584",
        "title": "Beyond Resolution: Multi - Scale Weather and Climate Data for Alpine Renewable Energy in the Digital Twin Era - First Evaluations and Recommendations",
        "authors": [
            "Irene Schicker",
            "Marianne BÃ¼gelmayer-Blaschek",
            "Annemarie Lexer",
            "Katharina Baier",
            "Kristofer Hasel",
            "Paolo Gazzaneo"
        ],
        "comments": "perspectives paper submitted to iScience, 25 pages, 2 tables, 3 figures",
        "subjects": "Atmospheric and Oceanic Physics (physics.ao-ph); Machine Learning (cs.LG)",
        "abstract": "When Austrian hydropower production plummeted by 44% in early 2025 due to reduced snowpack, it exposed a critical vulnerability: standard meteorological and climatological datasets systematically fail in mountain regions that hold untapped renewable potential. This perspectives paper evaluates emerging solutions to the Alpine energy-climate data gap, analyzing datasets from global reanalyses (ERA5, 31 km) to kilometre-scale Digital Twins (Climate DT, Extremes DT, 4.4 km), regional reanalyses (ARA, 2.5 km), and next-generation AI weather prediction models (AIFS, 31 km). The multi-resolution assessment reveals that no single dataset excels universally: coarse reanalyses provide essential climatologies but miss valley-scale processes, while Digital Twins resolve Alpine dynamics yet remain computationally demanding. Effective energy planning therefore requires strategic dataset combinations validated against energy-relevant indices such as population-weighted extremes, wind-gust return periods, and Alpine-adjusted storm thresholds. A key frontier is sub-hourly (10-15 min) temporal resolution to match grid-operation needs. Six evidence-based recommendations outline pathways for bridging spatial and temporal scales. As renewable deployment expands globally into complex terrain, the Alpine region offers transferable perspectives for tackling identical forecasting and climate analysis challenges in mountainous regions worldwide.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 166,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05597",
        "abs_url": "https://arxiv.org/abs/2511.05597",
        "pdf_url": "https://arxiv.org/pdf/2511.05597",
        "title": "From Prompts to Power: Measuring the Energy Footprint of LLM Inference",
        "authors": [
            "Francisco Caravaca",
            "Ãngel Cuevas",
            "RubÃ©n Cuevas"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "The rapid expansion of Large Language Models (LLMs) has introduced unprecedented energy demands, extending beyond training to large-scale inference workloads that often dominate total lifecycle consumption. Deploying these models requires energy-intensive GPU infrastructure, and in some cases has even prompted plans to power data centers with nuclear energy. Despite this growing relevance, systematic analyses of inference energy consumption remain limited. In this work, we present a large-scale measurement-based study comprising over 32,500 measurements across 21 GPU configurations and 155 model architectures, from small open-source models to frontier systems. Using the vLLM inference engine, we quantify energy usage at the prompt level and identify how architectural and operational factors shape energy demand. Building on these insights, we develop a predictive model that accurately estimates inference energy consumption across unseen architectures and hardware, and implement it as a browser extension to raise awareness of the environmental impact of generative AI.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 167,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05622",
        "abs_url": "https://arxiv.org/abs/2511.05622",
        "pdf_url": "https://arxiv.org/pdf/2511.05622",
        "title": "Grounding Foundational Vision Models with 3D Human Poses for Robust Action Recognition",
        "authors": [
            "Nicholas Babey",
            "Tiffany Gu",
            "Yiheng Li",
            "Cristian Meo",
            "Kevin Zhu"
        ],
        "comments": "Accepted at NeurIPS 2025 SpaVLE, for code see this https URL , 9 pages, 1 figure",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)",
        "abstract": "For embodied agents to effectively understand and interact within the world around them, they require a nuanced comprehension of human actions grounded in physical space. Current action recognition models, often relying on RGB video, learn superficial correlations between patterns and action labels, so they struggle to capture underlying physical interaction dynamics and human poses in complex scenes. We propose a model architecture that grounds action recognition in physical space by fusing two powerful, complementary representations: V-JEPA 2's contextual, predictive world dynamics and CoMotion's explicit, occlusion-tolerant human pose data. Our model is validated on both the InHARD and UCF-19-Y-OCC benchmarks for general action recognition and high-occlusion action recognition, respectively. Our model outperforms three other baselines, especially within complex, occlusive scenes. Our findings emphasize a need for action recognition to be supported by spatial understanding instead of statistical pattern recognition.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 168,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05650",
        "abs_url": "https://arxiv.org/abs/2511.05650",
        "pdf_url": "https://arxiv.org/pdf/2511.05650",
        "title": "Optimizing Diversity and Quality through Base-Aligned Model Collaboration",
        "authors": [
            "Yichen Wang",
            "Chenghao Yang",
            "Tenghao Huang",
            "Muhao Chen",
            "Jonathan May",
            "Mina Lee"
        ],
        "comments": "52 pages, 16 figures",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Alignment has greatly improved large language models (LLMs)' output quality at the cost of diversity, yielding highly similar outputs across generations. We propose Base-Aligned Model Collaboration (BACo), an inference-time token-level model collaboration framework that dynamically combines a base LLM with its aligned counterpart to optimize diversity and quality. Inspired by prior work (Fei et al., 2025), BACo employs routing strategies that determine, at each token, from which model to decode based on next-token prediction uncertainty and predicted contents' semantic role. Prior diversity-promoting methods, such as retraining, prompt engineering, and multi-sampling methods, improve diversity but often degrade quality or require costly decoding or post-training. In contrast, BACo achieves both high diversity and quality post hoc within a single pass, while offering strong controllability. We explore a family of routing strategies, across three open-ended generation tasks and 13 metrics covering diversity and quality, BACo consistently surpasses state-of-the-art inference-time baselines. With our best router, BACo achieves a 21.3% joint improvement in diversity and quality. Human evaluations also mirror these improvements. The results suggest that collaboration between base and aligned models can optimize and control diversity and quality.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 169,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05757",
        "abs_url": "https://arxiv.org/abs/2511.05757",
        "pdf_url": "https://arxiv.org/pdf/2511.05757",
        "title": "Zero-Shot Function Encoder-Based Differentiable Predictive Control",
        "authors": [
            "Hassan Iqbal",
            "Xingjian Li",
            "Tyler Ingebrand",
            "Adam Thorpe",
            "Krishna Kumar",
            "Ufuk Topcu",
            "JÃ¡n DrgoÅa"
        ],
        "comments": "",
        "subjects": "Systems and Control (eess.SY); Machine Learning (cs.LG)",
        "abstract": "We introduce a differentiable framework for zero-shot adaptive control over parametric families of nonlinear dynamical systems. Our approach integrates a function encoder-based neural ODE (FE-NODE) for modeling system dynamics with a differentiable predictive control (DPC) for offline self-supervised learning of explicit control policies. The FE-NODE captures nonlinear behaviors in state transitions and enables zero-shot adaptation to new systems without retraining, while the DPC efficiently learns control policies across system parameterizations, thus eliminating costly online optimization common in classical model predictive control. We demonstrate the efficiency, accuracy, and online adaptability of the proposed method across a range of nonlinear systems with varying parametric scenarios, highlighting its potential as a general-purpose tool for fast zero-shot adaptive control.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 170,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05772",
        "abs_url": "https://arxiv.org/abs/2511.05772",
        "pdf_url": "https://arxiv.org/pdf/2511.05772",
        "title": "Sign language recognition from skeletal data using graph and recurrent neural networks",
        "authors": [
            "B. Mederos",
            "J. MejÃ­a",
            "A. Medina-Reyes",
            "Y. Espinosa-Almeyda",
            "J. D. DÃ­az-Roman",
            "I. RodrÃ­guez-Mederos",
            "M. MejÃ­a-Carreon",
            "F. Gonzalez-Lopez"
        ],
        "comments": "15 pages, 2 figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "This work presents an approach for recognizing isolated sign language gestures using skeleton-based pose data extracted from video sequences. A Graph-GRU temporal network is proposed to model both spatial and temporal dependencies between frames, enabling accurate classification. The model is trained and evaluated on the AUTSL (Ankara university Turkish sign language) dataset, achieving high accuracy. Experimental results demonstrate the effectiveness of integrating graph-based spatial representations with temporal modeling, providing a scalable framework for sign language recognition. The results of this approach highlight the potential of pose-driven methods for sign language understanding.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 171,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05784",
        "abs_url": "https://arxiv.org/abs/2511.05784",
        "pdf_url": "https://arxiv.org/pdf/2511.05784",
        "title": "DRAGON: Guard LLM Unlearning in Context via Negative Detection and Reasoning",
        "authors": [
            "Yaxuan Wang",
            "Chris Yuhao Liu",
            "Quan Liu",
            "Jinglong Pang",
            "Wei Wei",
            "Yujia Bao",
            "Yang Liu"
        ],
        "comments": "Please refer to the NeurIPS 2025 submission: this https URL The paper has been accepted to the ICML 2025 MUGen Workshop: this https URL",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Unlearning in Large Language Models (LLMs) is crucial for protecting private data and removing harmful knowledge. Most existing approaches rely on fine-tuning to balance unlearning efficiency with general language capabilities. However, these methods typically require training or access to retain data, which is often unavailable in real world scenarios. Although these methods can perform well when both forget and retain data are available, few works have demonstrated equivalent capability in more practical, data-limited scenarios. To overcome these limitations, we propose Detect-Reasoning Augmented GeneratiON (DRAGON), a systematic, reasoning-based framework that utilizes in-context chain-of-thought (CoT) instructions to guard deployed LLMs before inference. Instead of modifying the base model, DRAGON leverages the inherent instruction-following ability of LLMs and introduces a lightweight detection module to identify forget-worthy prompts without any retain data. These are then routed through a dedicated CoT guard model to enforce safe and accurate in-context intervention. To robustly evaluate unlearning performance, we introduce novel metrics for unlearning performance and the continual unlearning setting. Extensive experiments across three representative unlearning tasks validate the effectiveness of DRAGON, demonstrating its strong unlearning capability, scalability, and applicability in practical scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 172,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05810",
        "abs_url": "https://arxiv.org/abs/2511.05810",
        "pdf_url": "https://arxiv.org/pdf/2511.05810",
        "title": "DiagnoLLM: A Hybrid Bayesian Neural Language Framework for Interpretable Disease Diagnosis",
        "authors": [
            "Bowen Xu",
            "Xinyue Zeng",
            "Jiazhen Hu",
            "Tuo Wang",
            "Adithya Kulkarni"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Building trustworthy clinical AI systems requires not only accurate predictions but also transparent, biologically grounded explanations. We present \\texttt{DiagnoLLM}, a hybrid framework that integrates Bayesian deconvolution, eQTL-guided deep learning, and LLM-based narrative generation for interpretable disease diagnosis. DiagnoLLM begins with GP-unmix, a Gaussian Process-based hierarchical model that infers cell-type-specific gene expression profiles from bulk and single-cell RNA-seq data while modeling biological uncertainty. These features, combined with regulatory priors from eQTL analysis, power a neural classifier that achieves high predictive performance in Alzheimer's Disease (AD) detection (88.0\\% accuracy). To support human understanding and trust, we introduce an LLM-based reasoning module that translates model outputs into audience-specific diagnostic reports, grounded in clinical features, attribution signals, and domain knowledge. Human evaluations confirm that these reports are accurate, actionable, and appropriately tailored for both physicians and patients. Our findings show that LLMs, when deployed as post-hoc reasoners rather than end-to-end predictors, can serve as effective communicators within hybrid diagnostic pipelines.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 173,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05849",
        "abs_url": "https://arxiv.org/abs/2511.05849",
        "pdf_url": "https://arxiv.org/pdf/2511.05849",
        "title": "EGG-SR: Embedding Symbolic Equivalence into Symbolic Regression via Equality Graph",
        "authors": [
            "Nan Jiang",
            "Ziyi Wang",
            "Yexiang Xue"
        ],
        "comments": "",
        "subjects": "Symbolic Computation (cs.SC); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Symbolic regression seeks to uncover physical laws from experimental data by searching for closed-form expressions, which is an important task in AI-driven scientific discovery. Yet the exponential growth of the search space of expression renders the task computationally challenging. A promising yet underexplored direction for reducing the effective search space and accelerating training lies in symbolic equivalence: many expressions, although syntactically different, define the same function -- for example, $\\log(x_1^2x_2^3)$, $\\log(x_1^2)+\\log(x_2^3)$, and $2\\log(x_1)+3\\log(x_2)$. Existing algorithms treat such variants as distinct outputs, leading to redundant exploration and slow learning. We introduce EGG-SR, a unified framework that integrates equality graphs (e-graphs) into diverse symbolic regression algorithms, including Monte Carlo Tree Search (MCTS), deep reinforcement learning (DRL), and large language models (LLMs). EGG-SR compactly represents equivalent expressions through the proposed EGG module, enabling more efficient learning by: (1) pruning redundant subtree exploration in EGG-MCTS, (2) aggregating rewards across equivalence classes in EGG-DRL, and (3) enriching feedback prompts in EGG-LLM. Under mild assumptions, we show that embedding e-graphs tightens the regret bound of MCTS and reduces the variance of the DRL gradient estimator. Empirically, EGG-SR consistently enhances multiple baselines across challenging benchmarks, discovering equations with lower normalized mean squared error than state-of-the-art methods. Code implementation is available at: this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 174,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05955",
        "abs_url": "https://arxiv.org/abs/2511.05955",
        "pdf_url": "https://arxiv.org/pdf/2511.05955",
        "title": "CSGaze: Context-aware Social Gaze Prediction",
        "authors": [
            "Surbhi Madan",
            "Shreya Ghosh",
            "Ramanathan Subramanian",
            "Abhinav Dhall",
            "Tom Gedeon"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "A person's gaze offers valuable insights into their focus of attention, level of social engagement, and confidence. In this work, we investigate how contextual cues combined with visual scene and facial information can be effectively utilized to predict and interpret social gaze patterns during conversational interactions. We introduce CSGaze, a context aware multimodal approach that leverages facial, scene information as complementary inputs to enhance social gaze pattern prediction from multi-person images. The model also incorporates a fine-grained attention mechanism centered on the principal speaker, which helps in better modeling social gaze dynamics. Experimental results show that CSGaze performs competitively with state-of-the-art methods on GP-Static, UCO-LAEO and AVA-LAEO. Our findings highlight the role of contextual cues in improving social gaze prediction. Additionally, we provide initial explainability through generated attention scores, offering insights into the model's decision-making process. We also demonstrate our model's generalizability by testing our model on open set datasets that demonstrating its robustness across diverse scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 175,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05969",
        "abs_url": "https://arxiv.org/abs/2511.05969",
        "pdf_url": "https://arxiv.org/pdf/2511.05969",
        "title": "Interpretable Recognition of Cognitive Distortions in Natural Language Texts",
        "authors": [
            "Anton Kolonin",
            "Anna Arinicheva"
        ],
        "comments": "9 pages, 4 figures",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG)",
        "abstract": "We propose a new approach to multi-factor classification of natural language texts based on weighted structured patterns such as N-grams, taking into account the heterarchical relationships between them, applied to solve such a socially impactful problem as the automation of detection of specific cognitive distortions in psychological care, relying on an interpretable, robust and transparent artificial intelligence model. The proposed recognition and learning algorithms improve the current state of the art in this field. The improvement is tested on two publicly available datasets, with significant improvements over literature-known F1 scores for the task, with optimal hyper-parameters determined, having code and models available for future use by the community.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 176,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05983",
        "abs_url": "https://arxiv.org/abs/2511.05983",
        "pdf_url": "https://arxiv.org/pdf/2511.05983",
        "title": "Benchmarking of Clustering Validity Measures Revisited",
        "authors": [
            "Connor Simpson",
            "Ricardo J. G. B. Campello",
            "Elizabeth Stojanovski"
        ],
        "comments": "48 pages, 17 tables, 17 figures",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Validation plays a crucial role in the clustering process. Many different internal validity indexes exist for the purpose of determining the best clustering solution(s) from a given collection of candidates, e.g., as produced by different algorithms or different algorithm hyper-parameters. In this study, we present a comprehensive benchmark study of 26 internal validity indexes, which includes highly popular classic indexes as well as more recently developed ones. We adopted an enhanced revision of the methodology presented in Vendramin et al. (2010), developed here to address several shortcomings of this previous work. This overall new approach consists of three complementary custom-tailored evaluation sub-methodologies, each of which has been designed to assess specific aspects of an index's behaviour while preventing potential biases of the other sub-methodologies. Each sub-methodology features two complementary measures of performance, alongside mechanisms that allow for an in-depth investigation of more complex behaviours of the internal validity indexes under study. Additionally, a new collection of 16177 datasets has been produced, paired with eight widely-used clustering algorithms, for a wider applicability scope and representation of more diverse clustering scenarios.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 177,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.05990",
        "abs_url": "https://arxiv.org/abs/2511.05990",
        "pdf_url": "https://arxiv.org/pdf/2511.05990",
        "title": "Learning solutions of parameterized stiff ODEs using Gaussian processes",
        "authors": [
            "Idoia Cortes Garcia",
            "P. FÃ¶rster",
            "W. Schilders",
            "S. SchÃ¶ps"
        ],
        "comments": "21 pages, 10 figures",
        "subjects": "Numerical Analysis (math.NA); Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)",
        "abstract": "Stiff ordinary differential equations (ODEs) play an important role in many scientific and engineering applications. Often, the dependence of the solution of the ODE on additional parameters is of interest, e.g.\\ when dealing with uncertainty quantification or design optimization. Directly studying this dependence can quickly become too computationally expensive, such that cheaper surrogate models approximating the solution are of interest. One popular class of surrogate models are Gaussian processes (GPs). They perform well when approximating stationary functions, functions which have a similar level of variation along any given parameter direction, however solutions to stiff ODEs are often characterized by a mixture of regions of rapid and slow variation along the time axis and when dealing with such nonstationary functions, GP performance frequently degrades drastically. We therefore aim to reparameterize stiff ODE solutions based on the available data, to make them appear more stationary and hence recover good GP performance. This approach comes with minimal computational overhead and requires no internal changes to the GP implementation, as it can be seen as a separate preprocessing step. We illustrate the achieved benefits using multiple examples.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 178,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06040",
        "abs_url": "https://arxiv.org/abs/2511.06040",
        "pdf_url": "https://arxiv.org/pdf/2511.06040",
        "title": "The Algorithmic Phase Transition in Symmetric Correlated Spiked Wigner Model",
        "authors": [
            "Zhangsong Li"
        ],
        "comments": "47 pages",
        "subjects": "Statistics Theory (math.ST); Machine Learning (cs.LG); Probability (math.PR); Machine Learning (stat.ML)",
        "abstract": "We study the computational task of detecting and estimating correlated signals in a pair of spiked Wigner matrices. Our model consists of observations $$ X = \\tfrac{\\lambda}{\\sqrt{n}} xx^{\\top} + W \\,, \\quad Y = \\tfrac{\\mu}{\\sqrt{n}} yy^{\\top} + Z \\,. $$ where $x,y \\in \\mathbb R^n$ are signal vectors with norm $\\|x\\|,\\|y\\| \\approx\\sqrt{n}$ and correlation $\\langle x,y \\rangle \\approx \\rho\\|x\\|\\|y\\|$, while $W,Z$ are independent Gaussian noise matrices. We propose an efficient algorithm that succeeds whenever $F(\\lambda,\\mu,\\rho)>1$, where $$ F(\\lambda,\\mu,\\rho)=\\max\\Big\\{ \\lambda,\\mu, \\frac{ \\lambda^2 \\rho^2 }{ 1-\\lambda^2+\\lambda^2 \\rho^2 } + \\frac{ \\mu^2 \\rho^2 }{ 1-\\mu^2+\\mu^2 \\rho^2 } \\Big\\} \\,. $$ Our result shows that an algorithm can leverage the correlation between the spikes to detect and estimate the signals even in regimes where efficiently recovering either $x$ from $X$ alone or $y$ from $Y$ alone is believed to be computationally infeasible. We complement our algorithmic result with evidence for a matching computational lower bound. In particular, we prove that when $F(\\lambda,\\mu,\\rho)<1$, all algorithms based on {\\em low-degree polynomials} fails to distinguish $(X,Y)$ with two independent Wigner matrices. This low-degree analysis strongly suggests that $F(\\lambda,\\mu,\\rho)=1$ is the precise computation threshold for this problem.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 179,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06048",
        "abs_url": "https://arxiv.org/abs/2511.06048",
        "pdf_url": "https://arxiv.org/pdf/2511.06048",
        "title": "Visual Exploration of Feature Relationships in Sparse Autoencoders with Curated Concepts",
        "authors": [
            "Xinyuan Yan",
            "Shusen Liu",
            "Kowshik Thopalli",
            "Bei Wang"
        ],
        "comments": "8 pages (5 main paper+3 refernce), 2 figures, pulished at Mechanistic Interpretability Workshop at NeurIPS 2025",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Sparse autoencoders (SAEs) have emerged as a powerful tool for uncovering interpretable features in large language models (LLMs) through the sparse directions they learn. However, the sheer number of extracted directions makes comprehensive exploration intractable. While conventional embedding techniques such as UMAP can reveal global structure, they suffer from limitations including high-dimensional compression artifacts, overplotting, and misleading neighborhood distortions. In this work, we propose a focused exploration framework that prioritizes curated concepts and their corresponding SAE features over attempts to visualize all available features simultaneously. We present an interactive visualization system that combines topology-based visual encoding with dimensionality reduction to faithfully represent both local and global relationships among selected features. This hybrid approach enables users to investigate SAE behavior through targeted, interpretable subsets, facilitating deeper and more nuanced analysis of concept representation in latent space.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 180,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06086",
        "abs_url": "https://arxiv.org/abs/2511.06086",
        "pdf_url": "https://arxiv.org/pdf/2511.06086",
        "title": "MuonAll: Muon Variant for Efficient Finetuning of Large Language Models",
        "authors": [
            "Saurabh Page",
            "Advait Joshi",
            "S. S. Sonawane"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Muon optimizer has demonstrated robust results in pretraining of language models but its performance in finetuning of existing public pretrained models is not yet explored. Currently, Muon is used along with AdamW introducing a scope of improvement for adopting all parameters inside Muon. We introduce MuonAll, which incorporates all the parameters inside Muon by transforming into 2D matrices. We conduct extensive finetuning experiments across publicly available language models with model sizes upto half billion parameters. Muon and MuonAll perform at par with AdamW across major benchmarks, highlighting their effectiveness as alternative optimizers. We open-source the distributed implementations of Muon and MuonAll, available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 181,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06105",
        "abs_url": "https://arxiv.org/abs/2511.06105",
        "pdf_url": "https://arxiv.org/pdf/2511.06105",
        "title": "Forecasting Thermospheric Density with Transformers for Multi-Satellite Orbit Management",
        "authors": [
            "Cedric BÃ¶s",
            "Alessandro Bortotto",
            "Mohamed Khalil Ben-Larbi"
        ],
        "comments": "6 pages, 3 figures, conference",
        "subjects": "Space Physics (physics.space-ph); Machine Learning (cs.LG)",
        "abstract": "Accurate thermospheric density prediction is crucial for reliable satellite operations in Low Earth Orbits, especially at high solar and geomagnetic activity. Physics-based models such as TIE-GCM offer high fidelity but are computationally expensive, while empirical models like NRLMSIS are efficient yet lack predictive power. This work presents a transformer-based model that forecasts densities up to three days ahead and is intended as a drop-in replacement for an empirical baseline. Unlike recent approaches, it avoids spatial reduction and complex input pipelines, operating directly on a compact input set. Validated on real-world data, the model improves key prediction metrics and shows potential to support mission planning.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 182,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06120",
        "abs_url": "https://arxiv.org/abs/2511.06120",
        "pdf_url": "https://arxiv.org/pdf/2511.06120",
        "title": "A Deep Learning Model for Predicting Transformation Legality",
        "authors": [
            "Avani Tiwari",
            "Yacine Hakimi",
            "Riyadh Baghdadi"
        ],
        "comments": "",
        "subjects": "Programming Languages (cs.PL); Machine Learning (cs.LG)",
        "abstract": "Compilers must check the legality of code transformations to guarantee the correctness of applying a sequence of code transformations to a given code. While such a legality check needs to be precisely computed in general, we can use an approximate legality prediction model in certain cases, such as training a reinforcement learning (RL) agent for schedule prediction. In this paper, we propose an approximate method for legality checks. We propose a novel DL model for predicting the legality of transformations. The model takes the code representation and a list of transformations as input and predicts whether applying those transformations to the code is legal. We implement and evaluate the proposed model, demonstrating its effectiveness. Our evaluation shows an F1 score of 0.91 on a test set of randomly generated programs. To further evaluate the model in a practical scenario, we used the model to replace the legality check used during the training of an RL agent designed for automatic code optimization. We demonstrate that such a replacement enables the agent to train on twice as many steps, resulting in faster training and reducing resource usage by approximately 80\\% for CPU and 35\\% for RAM. The agent trained using this approach maintains comparable performance, with only a 4\\% reduction on benchmarks from the Polybench suite compared to the traditional method.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 183,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06197",
        "abs_url": "https://arxiv.org/abs/2511.06197",
        "pdf_url": "https://arxiv.org/pdf/2511.06197",
        "title": "Enhancing Adversarial Robustness of IoT Intrusion Detection via SHAP-Based Attribution Fingerprinting",
        "authors": [
            "Dilli Prasad Sharma",
            "Liang Xue",
            "Xiaowei Sun",
            "Xiaodong Lin",
            "Pulei Xiong"
        ],
        "comments": "",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)",
        "abstract": "The rapid proliferation of Internet of Things (IoT) devices has transformed numerous industries by enabling seamless connectivity and data-driven automation. However, this expansion has also exposed IoT networks to increasingly sophisticated security threats, including adversarial attacks targeting artificial intelligence (AI) and machine learning (ML)-based intrusion detection systems (IDS) to deliberately evade detection, induce misclassification, and systematically undermine the reliability and integrity of security defenses. To address these challenges, we propose a novel adversarial detection model that enhances the robustness of IoT IDS against adversarial attacks through SHapley Additive exPlanations (SHAP)-based fingerprinting. Using SHAP's DeepExplainer, we extract attribution fingerprints from network traffic features, enabling the IDS to reliably distinguish between clean and adversarially perturbed inputs. By capturing subtle attribution patterns, the model becomes more resilient to evasion attempts and adversarial manipulations. We evaluated the model on a standard IoT benchmark dataset, where it significantly outperformed a state-of-the-art method in detecting adversarial attacks. In addition to enhanced robustness, this approach improves model transparency and interpretability, thereby increasing trust in the IDS through explainable AI.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 184,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06213",
        "abs_url": "https://arxiv.org/abs/2511.06213",
        "pdf_url": "https://arxiv.org/pdf/2511.06213",
        "title": "Time Matters: A Novel Real-Time Long- and Short-term User Interest Model for Click-Through Rate Prediction",
        "authors": [
            "Xian-Jin Gui"
        ],
        "comments": "This work was doned when the first author interned at Alibaba Group",
        "subjects": "Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "Click-Through Rate (CTR) prediction is a core task in online personalization platform. A key step for CTR prediction is to learn accurate user representation to capture their interests. Generally, the interest expressed by a user is time-variant, i.e., a user activates different interests at different time. However, most previous CTR prediction methods overlook the correlation between the activated interest and the occurrence time, resulting in what they actually learn is the mixture of the interests expressed by the user at all time, rather than the real-time interest at the certain prediction time. To capture the correlation between the activated interest and the occurrence time, in this paper we investigate users' interest evolution from the perspective of the whole time line and develop two regular patterns: periodic pattern and time-point pattern. Based on the two patterns, we propose a novel time-aware long- and short-term user interest modeling method to model users' dynamic interests at different time. Extensive experiments on public datasets as well as an industrial dataset verify the effectiveness of exploiting the two patterns and demonstrate the superiority of our proposed method compared with other state-of-the-art ones.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 185,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06235",
        "abs_url": "https://arxiv.org/abs/2511.06235",
        "pdf_url": "https://arxiv.org/pdf/2511.06235",
        "title": "Sparsity via Hyperpriors: A Theoretical and Algorithmic Study under Empirical Bayes Framework",
        "authors": [
            "Zhitao Li",
            "Yiqiu Dong",
            "Xueying Zeng"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Numerical Analysis (math.NA)",
        "abstract": "This paper presents a comprehensive analysis of hyperparameter estimation within the empirical Bayes framework (EBF) for sparse learning. By studying the influence of hyperpriors on the solution of EBF, we establish a theoretical connection between the choice of the hyperprior and the sparsity as well as the local optimality of the resulting solutions. We show that some strictly increasing hyperpriors, such as half-Laplace and half-generalized Gaussian with the power in $(0,1)$, effectively promote sparsity and improve solution stability with respect to measurement noise. Based on this analysis, we adopt a proximal alternating linearized minimization (PALM) algorithm with convergence guaranties for both convex and concave hyperpriors. Extensive numerical tests on two-dimensional image deblurring problems demonstrate that introducing appropriate hyperpriors significantly promotes the sparsity of the solution and enhances restoration accuracy. Furthermore, we illustrate the influence of the noise level and the ill-posedness of inverse problems to EBF solutions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 186,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06239",
        "abs_url": "https://arxiv.org/abs/2511.06239",
        "pdf_url": "https://arxiv.org/pdf/2511.06239",
        "title": "Functional Adjoint Sampler: Scalable Sampling on Infinite Dimensional Spaces",
        "authors": [
            "Byoungwoo Park",
            "Juho Lee",
            "Guan-Horng Liu"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Learning-based methods for sampling from the Gibbs distribution in finite-dimensional spaces have progressed quickly, yet theory and algorithmic design for infinite-dimensional function spaces remain limited. This gap persists despite their strong potential for sampling the paths of conditional diffusion processes, enabling efficient simulation of trajectories of diffusion processes that respect rare events or boundary constraints. In this work, we present the adjoint sampler for infinite-dimensional function spaces, a stochastic optimal control-based diffusion sampler that operates in function space and targets Gibbs-type distributions on infinite-dimensional Hilbert spaces. Our Functional Adjoint Sampler (FAS) generalizes Adjoint Sampling (Havens et al., 2025) to Hilbert spaces based on a SOC theory called stochastic maximum principle, yielding a simple and scalable matching-type objective for a functional representation. We show that FAS achieves superior transition path sampling performance across synthetic potential and real molecular systems, including Alanine Dipeptide and Chignolin.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 187,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06305",
        "abs_url": "https://arxiv.org/abs/2511.06305",
        "pdf_url": "https://arxiv.org/pdf/2511.06305",
        "title": "Setting $\\varepsilon$ is not the Issue in Differential Privacy",
        "authors": [
            "Edwige Cyffers"
        ],
        "comments": "Accepted to NeurIPS Position Paper track",
        "subjects": "Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
        "abstract": "This position paper argues that setting the privacy budget in differential privacy should not be viewed as an important limitation of differential privacy compared to alternative methods for privacy-preserving machine learning. The so-called problem of interpreting the privacy budget is often presented as a major hindrance to the wider adoption of differential privacy in real-world deployments and is sometimes used to promote alternative mitigation techniques for data protection. We believe this misleads decision-makers into choosing unsafe methods. We argue that the difficulty in interpreting privacy budgets does not stem from the definition of differential privacy itself, but from the intrinsic difficulty of estimating privacy risks in context, a challenge that any rigorous method for privacy risk assessment face. Moreover, we claim that any sound method for estimating privacy risks should, given the current state of research, be expressible within the differential privacy framework or justify why it cannot.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 188,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06380",
        "abs_url": "https://arxiv.org/abs/2511.06380",
        "pdf_url": "https://arxiv.org/pdf/2511.06380",
        "title": "What Makes Reasoning Invalid: Echo Reflection Mitigation for Large Language Models",
        "authors": [
            "Chen He",
            "Xun Jiang",
            "Lei Wang",
            "Hao Yang",
            "Chong Peng",
            "Peng Yan",
            "Fumin Shen",
            "Xing Xu"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of reasoning tasks. Recent methods have further improved LLM performance in complex mathematical reasoning. However, when extending these methods beyond the domain of mathematical reasoning to tasks involving complex domain-specific knowledge, we observe a consistent failure of LLMs to generate novel insights during the reflection stage. Instead of conducting genuine cognitive refinement, the model tends to mechanically reiterate earlier reasoning steps without introducing new information or perspectives, a phenomenon referred to as \"Echo Reflection\". We attribute this behavior to two key defects: (1) Uncontrollable information flow during response generation, which allows premature intermediate thoughts to propagate unchecked and distort final decisions; (2) Insufficient exploration of internal knowledge during reflection, leading to repeating earlier findings rather than generating new cognitive insights. Building on these findings, we proposed a novel reinforcement learning method termed Adaptive Entropy Policy Optimization (AEPO). Specifically, the AEPO framework consists of two major components: (1) Reflection-aware Information Filtration, which quantifies the cognitive information flow and prevents the final answer from being affected by earlier bad cognitive information; (2) Adaptive-Entropy Optimization, which dynamically balances exploration and exploitation across different reasoning stages, promoting both reflective diversity and answer correctness. Extensive experiments demonstrate that AEPO consistently achieves state-of-the-art performance over mainstream reinforcement learning baselines across diverse benchmarks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 189,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06387",
        "abs_url": "https://arxiv.org/abs/2511.06387",
        "pdf_url": "https://arxiv.org/pdf/2511.06387",
        "title": "Learning the Inverse Ryu--Takayanagi Formula with Transformers",
        "authors": [
            "Sejin Kim"
        ],
        "comments": "15 pages, 6 figures",
        "subjects": "High Energy Physics - Theory (hep-th); Machine Learning (cs.LG)",
        "abstract": "We study the inverse problem of holographic entanglement entropy in AdS$_3$ using a data-driven generative model. Training data consist of randomly generated geometries and their holographic entanglement entropies using the Ryu--Takayanagi formula. After training, the Transformer reconstructs the blackening function within our metric ansatz from previously unseen inputs. The Transformer achieves accurate reconstructions on smooth black hole geometries and extrapolates to horizonless backgrounds. We describe the architecture and data generation process, and we quantify accuracy on both $f(z)$ and the reconstructed $S(\\ell)$. Code and evaluation scripts are available at the provided repository.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 190,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06407",
        "abs_url": "https://arxiv.org/abs/2511.06407",
        "pdf_url": "https://arxiv.org/pdf/2511.06407",
        "title": "Fast Riemannian-manifold Hamiltonian Monte Carlo for hierarchical Gaussian-process models",
        "authors": [
            "Takashi Hayakawa",
            "Satoshi Asai"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Computation (stat.CO)",
        "abstract": "Hierarchical Bayesian models based on Gaussian processes are considered useful for describing complex nonlinear statistical dependencies among variables in real-world data. However, effective Monte Carlo algorithms for inference with these models have not yet been established, except for several simple cases. In this study, we show that, compared with the slow inference achieved with existing program libraries, the performance of Riemannian-manifold Hamiltonian Monte Carlo (RMHMC) can be drastically improved by optimising the computation order according to the model structure and dynamically programming the eigendecomposition. This improvement cannot be achieved when using an existing library based on a naive automatic differentiator. We numerically demonstrate that RMHMC effectively samples from the posterior, allowing the calculation of model evidence, in a Bayesian logistic regression on simulated data and in the estimation of propensity functions for the American national medical expenditure data using several Bayesian multiple-kernel models. These results lay a foundation for implementing effective Monte Carlo algorithms for analysing real-world data with Gaussian processes, and highlight the need to develop a customisable library set that allows users to incorporate dynamically programmed objects and finely optimises the mode of automatic differentiation depending on the model structure.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 191,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06425",
        "abs_url": "https://arxiv.org/abs/2511.06425",
        "pdf_url": "https://arxiv.org/pdf/2511.06425",
        "title": "Non-Negative Stiefel Approximating Flow: Orthogonalish Matrix Optimization for Interpretable Embeddings",
        "authors": [
            "Brian B. Avants",
            "Nicholas J. Tustison",
            "James R Stone"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Methodology (stat.ME)",
        "abstract": "Interpretable representation learning is a central challenge in modern machine learning, particularly in high-dimensional settings such as neuroimaging, genomics, and text analysis. Current methods often struggle to balance the competing demands of interpretability and model flexibility, limiting their effectiveness in extracting meaningful insights from complex data. We introduce Non-negative Stiefel Approximating Flow (NSA-Flow), a general-purpose matrix estimation framework that unifies ideas from sparse matrix factorization, orthogonalization, and constrained manifold learning. NSA-Flow enforces structured sparsity through a continuous balance between reconstruction fidelity and column-wise decorrelation, parameterized by a single tunable weight. The method operates as a smooth flow near the Stiefel manifold with proximal updates for non-negativity and adaptive gradient control, yielding representations that are simultaneously sparse, stable, and interpretable. Unlike classical regularization schemes, NSA-Flow provides an intuitive geometric mechanism for manipulating sparsity at the level of global structure while simplifying latent features. We demonstrate that the NSA-Flow objective can be optimized smoothly and integrates seamlessly with existing pipelines for dimensionality reduction while improving interpretability and generalization in both simulated and real biomedical data. Empirical validation on the Golub leukemia dataset and in Alzheimer's disease demonstrate that the NSA-Flow constraints can maintain or improve performance over related methods with little additional methodological effort. NSA-Flow offers a scalable, general-purpose tool for interpretable ML, applicable across data science domains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 192,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06441",
        "abs_url": "https://arxiv.org/abs/2511.06441",
        "pdf_url": "https://arxiv.org/pdf/2511.06441",
        "title": "Towards Resource-Efficient Multimodal Intelligence: Learned Routing among Specialized Expert Models",
        "authors": [
            "Mayank Saini",
            "Arit Kumar Bishwas"
        ],
        "comments": "15 pages, 4 figures",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "As AI moves beyond text, large language models (LLMs) increasingly power vision, audio, and document understanding; however, their high inference costs hinder real-time, scalable deployment. Conversely, smaller open-source models offer cost advantages but struggle with complex or multimodal queries. We introduce a unified, modular framework that intelligently routes each query - textual, multimodal, or complex - to the most fitting expert model, using a learned routing network that balances cost and quality. For vision tasks, we employ a two-stage open-source pipeline optimized for efficiency and reviving efficient classical vision components where they remain SOTA for sub-tasks. On benchmarks such as Massive Multitask Language Understanding (MMLU) and Visual Question Answering (VQA), we match or exceed the performance of always-premium LLM (monolithic systems with one model serving all query types) performance, yet reduce the reliance on costly models by over 67%. With its extensible, multi-agent orchestration, we deliver high-quality, resource-efficient AI at scale.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 193,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06458",
        "abs_url": "https://arxiv.org/abs/2511.06458",
        "pdf_url": "https://arxiv.org/pdf/2511.06458",
        "title": "EchoMark: Perceptual Acoustic Environment Transfer with Watermark-Embedded Room Impulse Response",
        "authors": [
            "Chenpei Huang",
            "Lingfeng Yao",
            "Kyu In Lee",
            "Lan Emily Zhang",
            "Xun Chen",
            "Miao Pan"
        ],
        "comments": "",
        "subjects": "Sound (cs.SD); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)",
        "abstract": "Acoustic Environment Matching (AEM) is the task of transferring clean audio into a target acoustic environment, enabling engaging applications such as audio dubbing and auditory immersive virtual reality (VR). Recovering similar room impulse response (RIR) directly from reverberant speech offers more accessible and flexible AEM solution. However, this capability also introduces vulnerabilities of arbitrary ``relocation\" if misused by malicious user, such as facilitating advanced voice spoofing attacks or undermining the authenticity of recorded evidence. To address this issue, we propose EchoMark, the first deep learning-based AEM framework that generates perceptually similar RIRs with embedded watermark. Our design tackle the challenges posed by variable RIR characteristics, such as different durations and energy decays, by operating in the latent domain. By jointly optimizing the model with a perceptual loss for RIR reconstruction and a loss for watermark detection, EchoMark achieves both high-quality environment transfer and reliable watermark recovery. Experiments on diverse datasets validate that EchoMark achieves room acoustic parameter matching performance comparable to FiNS, the state-of-the-art RIR estimator. Furthermore, a high Mean Opinion Score (MOS) of 4.22 out of 5, watermark detection accuracy exceeding 99\\%, and bit error rates (BER) below 0.3\\% collectively demonstrate the effectiveness of EchoMark in preserving perceptual quality while ensuring reliable watermark embedding.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 194,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06479",
        "abs_url": "https://arxiv.org/abs/2511.06479",
        "pdf_url": "https://arxiv.org/pdf/2511.06479",
        "title": "Bridging Theory and Practice: A Stochastic Learning-Optimization Model for Resilient Automotive Supply Chains",
        "authors": [
            "Muhammad Shahnawaz",
            "Adeel Safder"
        ],
        "comments": "14 pages, 4 figures",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG); Optimization and Control (math.OC)",
        "abstract": "Supply chain disruptions and volatile demand pose significant challenges to the UK automotive industry, which relies heavily on Just-In-Time (JIT) manufacturing. While qualitative studies highlight the potential of integrating Artificial Intelligence (AI) with traditional optimization, a formal, quantitative demonstration of this synergy is lacking. This paper introduces a novel stochastic learning-optimization framework that integrates Bayesian inference with inventory optimization for supply chain management (SCM). We model a two-echelon inventory system subject to stochastic demand and supply disruptions, comparing a traditional static optimization policy against an adaptive policy where Bayesian learning continuously updates parameter estimates to inform stochastic optimization. Our simulations over 365 periods across three operational scenarios demonstrate that the integrated approach achieves 7.4\\% cost reduction in stable environments and 5.7\\% improvement during supply disruptions, while revealing important limitations during sudden demand shocks due to the inherent conservatism of Bayesian updating. This work provides mathematical validation for practitioner observations and establishes a formal framework for understanding AI-driven supply chain resilience, while identifying critical boundary conditions for successful implementation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 195,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06512",
        "abs_url": "https://arxiv.org/abs/2511.06512",
        "pdf_url": "https://arxiv.org/pdf/2511.06512",
        "title": "EASE: Practical and Efficient Safety Alignment for Small Language Models",
        "authors": [
            "Haonan Shi",
            "Guoli Wang",
            "Tu Ouyang",
            "An Wang"
        ],
        "comments": "Accepted to AAAI 2026",
        "subjects": "Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
        "abstract": "Small language models (SLMs) are increasingly deployed on edge devices, making their safety alignment crucial yet challenging. Current shallow alignment methods that rely on direct refusal of malicious queries fail to provide robust protection, particularly against adversarial jailbreaks. While deliberative safety reasoning alignment offers deeper alignment for defending against sophisticated attacks, effectively implanting such reasoning capability in SLMs with limited capabilities remains an open challenge. Moreover, safety reasoning incurs significant computational overhead as models apply reasoning to nearly all queries, making it impractical for resource-constrained edge deployment scenarios that demand rapid responses. We propose EASE, a novel framework that enables practical and Efficient safety Alignment for Small languagE models. Our approach first identifies the optimal safety reasoning teacher that can effectively distill safety reasoning capabilities to SLMs. We then align models to selectively activate safety reasoning for dangerous adversarial jailbreak queries while providing direct responses to straightforward malicious queries and general helpful tasks. This selective mechanism enables small models to maintain robust safety guarantees against sophisticated attacks while preserving computational efficiency for benign interactions. Experimental results demonstrate that EASE reduces jailbreak attack success rates by up to 17% compared to shallow alignment methods while reducing inference overhead by up to 90% compared to deliberative safety reasoning alignment, making it practical for SLMs real-world edge deployments.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 196,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06571",
        "abs_url": "https://arxiv.org/abs/2511.06571",
        "pdf_url": "https://arxiv.org/pdf/2511.06571",
        "title": "Rep2Text: Decoding Full Text from a Single LLM Token Representation",
        "authors": [
            "Haiyan Zhao",
            "Zirui He",
            "Fan Yang",
            "Ali Payani",
            "Mengnan Du"
        ],
        "comments": "15 pages, 7 figures, 4 tables",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Large language models (LLMs) have achieved remarkable progress across diverse tasks, yet their internal mechanisms remain largely opaque. In this work, we address a fundamental question: to what extent can the original input text be recovered from a single last-token representation within an LLM? We propose Rep2Text, a novel framework for decoding full text from last-token representations. Rep2Text employs a trainable adapter that projects a target model's internal representations into the embedding space of a decoding language model, which then autoregressively reconstructs the input text. Experiments on various model combinations (Llama-3.1-8B, Gemma-7B, Mistral-7B-v0.1, Llama-3.2-3B) demonstrate that, on average, over half of the information in 16-token sequences can be recovered from this compressed representation while maintaining strong semantic integrity and coherence. Furthermore, our analysis reveals an information bottleneck effect: longer sequences exhibit decreased token-level recovery while preserving strong semantic integrity. Besides, our framework also demonstrates robust generalization to out-of-distribution medical data.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 197,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06582",
        "abs_url": "https://arxiv.org/abs/2511.06582",
        "pdf_url": "https://arxiv.org/pdf/2511.06582",
        "title": "TabRAG: Tabular Document Retrieval via Structured Language Representations",
        "authors": [
            "Jacob Si",
            "Mike Qu",
            "Michelle Lee",
            "Yingzhen Li"
        ],
        "comments": "NeurIPS 2025 AI4Tab",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "Ingesting data for Retrieval-Augmented Generation (RAG) involves either fine-tuning the embedding model directly on the target corpus or parsing documents for embedding model encoding. The former, while accurate, incurs high computational hardware requirements, while the latter suffers from suboptimal performance when extracting tabular data. In this work, we address the latter by presenting TabRAG, a parsing-based RAG pipeline designed to tackle table-heavy documents via structured language representations. TabRAG outperforms existing popular parsing-based methods for generation and retrieval. Code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 198,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06585",
        "abs_url": "https://arxiv.org/abs/2511.06585",
        "pdf_url": "https://arxiv.org/pdf/2511.06585",
        "title": "Learning Biomolecular Motion: The Physics-Informed Machine Learning Paradigm",
        "authors": [
            "Aaryesh Deshpande"
        ],
        "comments": "31 pages, 4 figures, 3 tables. Review article",
        "subjects": "Biomolecules (q-bio.BM); Machine Learning (cs.LG); Computational Physics (physics.comp-ph); Machine Learning (stat.ML)",
        "abstract": "The convergence of statistical learning and molecular physics is transforming our approach to modeling biomolecular systems. Physics-informed machine learning (PIML) offers a systematic framework that integrates data-driven inference with physical constraints, resulting in models that are accurate, mechanistic, generalizable, and able to extrapolate beyond observed domains. This review surveys recent advances in physics-informed neural networks and operator learning, differentiable molecular simulation, and hybrid physics-ML potentials, with emphasis on long-timescale kinetics, rare events, and free-energy estimation. We frame these approaches as solutions to the \"biomolecular closure problem\", recovering unresolved interactions beyond classical force fields while preserving thermodynamic consistency and mechanistic interpretability. We examine theoretical foundations, tools and frameworks, computational trade-offs, and unresolved issues, including model expressiveness and stability. We outline prospective research avenues at the intersection of machine learning, statistical physics, and computational chemistry, contending that future advancements will depend on mechanistic inductive biases, and integrated differentiable physical learning frameworks for biomolecular simulation and discovery.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 199,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06625",
        "abs_url": "https://arxiv.org/abs/2511.06625",
        "pdf_url": "https://arxiv.org/pdf/2511.06625",
        "title": "Explainable Cross-Disease Reasoning for Cardiovascular Risk Assessment from LDCT",
        "authors": [
            "Yifei Zhang",
            "Jiashuo Zhang",
            "Xiaofeng Yang",
            "Liang Zhao"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Low-dose chest computed tomography (LDCT) inherently captures both pulmonary and cardiac structures, offering a unique opportunity for joint assessment of lung and cardiovascular health. However, most existing approaches treat these domains as independent tasks, overlooking their physiological interplay and shared imaging biomarkers. We propose an Explainable Cross-Disease Reasoning Framework that enables interpretable cardiopulmonary risk assessment from a single LDCT scan. The framework introduces an agentic reasoning process that emulates clinical diagnostic thinking-first perceiving pulmonary findings, then reasoning through established medical knowledge, and finally deriving a cardiovascular judgment with explanatory rationale. It integrates three synergistic components: a pulmonary perception module that summarizes lung abnormalities, a knowledge-guided reasoning module that infers their cardiovascular implications, and a cardiac representation module that encodes structural biomarkers. Their outputs are fused to produce a holistic cardiovascular risk prediction that is both accurate and physiologically grounded. Experiments on the NLST cohort demonstrate that the proposed framework achieves state-of-the-art performance for CVD screening and mortality prediction, outperforming single-disease and purely image-based baselines. Beyond quantitative gains, the framework provides human-verifiable reasoning that aligns with cardiological understanding, revealing coherent links between pulmonary abnormalities and cardiac stress mechanisms. Overall, this work establishes a unified and explainable paradigm for cardiovascular analysis from LDCT, bridging the gap between image-based prediction and mechanism-based medical interpretation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 200,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06645",
        "abs_url": "https://arxiv.org/abs/2511.06645",
        "pdf_url": "https://arxiv.org/pdf/2511.06645",
        "title": "Adaptive Testing for Segmenting Watermarked Texts From Language Models",
        "authors": [
            "Xingchi Li",
            "Xiaochi Liu",
            "Guanxun Li"
        ],
        "comments": "13 pages, 3 figures, accepted for publication in STAT, October 28, 2025",
        "subjects": "Machine Learning (stat.ML); Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "The rapid adoption of large language models (LLMs), such as GPT-4 and Claude 3.5, underscores the need to distinguish LLM-generated text from human-written content to mitigate the spread of misinformation and misuse in education. One promising approach to address this issue is the watermark technique, which embeds subtle statistical signals into LLM-generated text to enable reliable identification. In this paper, we first generalize the likelihood-based LLM detection method of a previous study by introducing a flexible weighted formulation, and further adapt this approach to the inverse transform sampling method. Moving beyond watermark detection, we extend this adaptive detection strategy to tackle the more challenging problem of segmenting a given text into watermarked and non-watermarked substrings. In contrast to the approach in a previous study, which relies on accurate estimation of next-token probabilities that are highly sensitive to prompt estimation, our proposed framework removes the need for precise prompt estimation. Extensive numerical experiments demonstrate that the proposed methodology is both effective and robust in accurately segmenting texts containing a mixture of watermarked and non-watermarked content.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 201,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06663",
        "abs_url": "https://arxiv.org/abs/2511.06663",
        "pdf_url": "https://arxiv.org/pdf/2511.06663",
        "title": "GNN-Enabled Robust Hybrid Beamforming with Score-Based CSI Generation and Denoising",
        "authors": [
            "Yuhang Li",
            "Yang Lu",
            "Bo Ai",
            "Zhiguo Ding",
            "Dusit Niyato",
            "Arumugam Nallanathan"
        ],
        "comments": "",
        "subjects": "Systems and Control (eess.SY); Machine Learning (cs.LG)",
        "abstract": "Accurate Channel State Information (CSI) is critical for Hybrid Beamforming (HBF) tasks. However, obtaining high-resolution CSI remains challenging in practical wireless communication systems. To address this issue, we propose to utilize Graph Neural Networks (GNNs) and score-based generative models to enable robust HBF under imperfect CSI conditions. Firstly, we develop the Hybrid Message Graph Attention Network (HMGAT) which updates both node and edge features through node-level and edge-level message passing. Secondly, we design a Bidirectional Encoder Representations from Transformers (BERT)-based Noise Conditional Score Network (NCSN) to learn the distribution of high-resolution CSI, facilitating CSI generation and data augmentation to further improve HMGAT's performance. Finally, we present a Denoising Score Network (DSN) framework and its instantiation, termed DeBERT, which can denoise imperfect CSI under arbitrary channel error levels, thereby facilitating robust HBF. Experiments on DeepMIMO urban datasets demonstrate the proposed models' superior generalization, scalability, and robustness across various HBF tasks with perfect and imperfect CSI.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 202,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06668",
        "abs_url": "https://arxiv.org/abs/2511.06668",
        "pdf_url": "https://arxiv.org/pdf/2511.06668",
        "title": "When Evidence Contradicts: Toward Safer Retrieval-Augmented Generation in Healthcare",
        "authors": [
            "Saeedeh Javadi",
            "Sara Mirabi",
            "Manan Gangar",
            "Bahadorreza Ofoghi"
        ],
        "comments": "",
        "subjects": "Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "In high-stakes information domains such as healthcare, where large language models (LLMs) can produce hallucinations or misinformation, retrieval-augmented generation (RAG) has been proposed as a mitigation strategy, grounding model outputs in external, domain-specific documents. Yet, this approach can introduce errors when source documents contain outdated or contradictory information. This work investigates the performance of five LLMs in generating RAG-based responses to medicine-related queries. Our contributions are three-fold: i) the creation of a benchmark dataset using consumer medicine information documents from the Australian Therapeutic Goods Administration (TGA), where headings are repurposed as natural language questions, ii) the retrieval of PubMed abstracts using TGA headings, stratified across multiple publication years, to enable controlled temporal evaluation of outdated evidence, and iii) a comparative analysis of the frequency and impact of outdated or contradictory content on model-generated responses, assessing how LLMs integrate and reconcile temporally inconsistent information. Our findings show that contradictions between highly similar abstracts do, in fact, degrade performance, leading to inconsistencies and reduced factual accuracy in model answers. These results highlight that retrieval similarity alone is insufficient for reliable medical RAG and underscore the need for contradiction-aware filtering strategies to ensure trustworthy responses in high-stakes domains.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 203,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06675",
        "abs_url": "https://arxiv.org/abs/2511.06675",
        "pdf_url": "https://arxiv.org/pdf/2511.06675",
        "title": "Adam symmetry theorem: characterization of the convergence of the stochastic Adam optimizer",
        "authors": [
            "Steffen Dereich",
            "Thang Do",
            "Arnulf Jentzen",
            "Philippe von Wurstemberger"
        ],
        "comments": "66 pages",
        "subjects": "Optimization and Control (math.OC); Machine Learning (cs.LG)",
        "abstract": "Beside the standard stochastic gradient descent (SGD) method, the Adam optimizer due to Kingma & Ba (2014) is currently probably the best-known optimization method for the training of deep neural networks in artificial intelligence (AI) systems. Despite the popularity and the success of Adam it remains an \\emph{open research problem} to provide a rigorous convergence analysis for Adam even for the class of strongly convex SOPs. In one of the main results of this work we establish convergence rates for Adam in terms of the number of gradient steps (convergence rate \\nicefrac{1}{2} w.r.t. the size of the learning rate), the size of the mini-batches (convergence rate 1 w.r.t. the size of the mini-batches), and the size of the second moment parameter of Adam (convergence rate 1 w.r.t. the distance of the second moment parameter to 1) for the class of strongly convex SOPs. In a further main result of this work, which we refer to as \\emph{Adam symmetry theorem}, we illustrate the optimality of the established convergence rates by proving for a special class of simple quadratic strongly convex SOPs that Adam converges as the number of gradient steps increases to infinity to the solution of the SOP (the unique minimizer of the strongly convex objective function) if and \\emph{only} if the random variables in the SOP (the data in the SOP) are \\emph{symmetrically distributed}. In particular, in the standard case where the random variables in the SOP are not symmetrically distributed we \\emph{disprove} that Adam converges to the minimizer of the SOP as the number of Adam steps increases to infinity. We also complement the conclusions of our convergence analysis and the Adam symmetry theorem by several numerical simulations that indicate the sharpness of the established convergence rates and that illustrate the practical appearance of the phenomena revealed in the \\emph{Adam symmetry theorem}.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 204,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06698",
        "abs_url": "https://arxiv.org/abs/2511.06698",
        "pdf_url": "https://arxiv.org/pdf/2511.06698",
        "title": "Lassoed Forests: Random Forests with Adaptive Lasso Post-selection",
        "authors": [
            "Jing Shang",
            "James Bannon",
            "Benjamin Haibe-Kains",
            "Robert Tibshirani"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "Random forests are a statistical learning technique that use bootstrap aggregation to average high-variance and low-bias trees. Improvements to random forests, such as applying Lasso regression to the tree predictions, have been proposed in order to reduce model bias. However, these changes can sometimes degrade performance (e.g., an increase in mean squared error). In this paper, we show in theory that the relative performance of these two methods, standard and Lasso-weighted random forests, depends on the signal-to-noise ratio. We further propose a unified framework to combine random forests and Lasso selection by applying adaptive weighting and show mathematically that it can strictly outperform the other two methods. We compare the three methods through simulation, including bias-variance decomposition, error estimates evaluation, and variable importance analysis. We also show the versatility of our method by applications to a variety of real-world datasets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 205,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06714",
        "abs_url": "https://arxiv.org/abs/2511.06714",
        "pdf_url": "https://arxiv.org/pdf/2511.06714",
        "title": "The Wisdom of the Crowd: High-Fidelity Classification of Cyber-Attacks and Faults in Power Systems Using Ensemble and Machine Learning",
        "authors": [
            "Emad Abukhousa",
            "Syed Sohail Feroz Syed Afroz",
            "Fahad Alsaeed",
            "Abdulaziz Qwbaiban",
            "Saman Zonouz",
            "A.P. Sakis Meliopoulos"
        ],
        "comments": "",
        "subjects": "Systems and Control (eess.SY); Machine Learning (cs.LG)",
        "abstract": "This paper presents a high-fidelity evaluation framework for machine learning (ML)-based classification of cyber-attacks and physical faults using electromagnetic transient simulations with digital substation emulation at 4.8 kHz. Twelve ML models, including ensemble algorithms and a multi-layer perceptron (MLP), were trained on labeled time-domain measurements and evaluated in a real-time streaming environment designed for sub-cycle responsiveness. The architecture incorporates a cycle-length smoothing filter and confidence threshold to stabilize decisions. Results show that while several models achieved near-perfect offline accuracies (up to 99.9%), only the MLP sustained robust coverage (98-99%) under streaming, whereas ensembles preserved perfect anomaly precision but abstained frequently (10-49% coverage). These findings demonstrate that offline accuracy alone is an unreliable indicator of field readiness and underscore the need for realistic testing and inference pipelines to ensure dependable classification in inverter-based resources (IBR)-rich networks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 206,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06774",
        "abs_url": "https://arxiv.org/abs/2511.06774",
        "pdf_url": "https://arxiv.org/pdf/2511.06774",
        "title": "Bilevel Learning via Inexact Stochastic Gradient Descent",
        "authors": [
            "Mohammad Sadegh Salehi",
            "Subhadip Mukherjee",
            "Lindon Roberts",
            "Matthias J. Ehrhardt"
        ],
        "comments": "",
        "subjects": "Optimization and Control (math.OC); Machine Learning (cs.LG)",
        "abstract": "Bilevel optimization is a central tool in machine learning for high-dimensional hyperparameter tuning. Its applications are vast; for instance, in imaging it can be used for learning data-adaptive regularizers and optimizing forward operators in variational regularization. These problems are large in many ways: a lot of data is usually available to train a large number of parameters, calling for stochastic gradient-based algorithms. However, exact gradients with respect to parameters (so-called hypergradients) are not available, and their precision is usually linearly related to computational cost. Hence, algorithms must solve the problem efficiently without unnecessary precision. The design of such methods is still not fully understood, especially regarding how accuracy requirements and step size schedules affect theoretical guarantees and practical performance. Existing approaches introduce stochasticity at both the upper level (e.g., in sampling or mini-batch estimates) and the lower level (e.g., in solving the inner problem) to improve generalization, but they typically fix the number of lower-level iterations, which conflicts with asymptotic convergence assumptions. In this work, we advance the theory of inexact stochastic bilevel optimization. We prove convergence and establish rates under decaying accuracy and step size schedules, showing that with optimal configurations convergence occurs at an $\\mathcal{O}(k^{-1/4})$ rate in expectation. Experiments on image denoising and inpainting with convex ridge regularizers and input-convex networks confirm our analysis: decreasing step sizes improve stability, accuracy scheduling is more critical than step size strategy, and adaptive preconditioning (e.g., Adam) further boosts performance. These results bridge theory and practice, providing convergence guarantees and practical guidance for large-scale imaging problems.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 207,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06782",
        "abs_url": "https://arxiv.org/abs/2511.06782",
        "pdf_url": "https://arxiv.org/pdf/2511.06782",
        "title": "HEDN: A Hard-Easy Dual Network with Task Difficulty Assessment for EEG Emotion Recognition",
        "authors": [
            "Qiang Wang",
            "Liying Yang"
        ],
        "comments": "",
        "subjects": "Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)",
        "abstract": "Multi-source domain adaptation represents an effective approach to addressing individual differences in cross-subject EEG emotion recognition. However, existing methods treat all source domains equally, neglecting the varying transfer difficulties between different source domains and the target domain. This oversight can lead to suboptimal adaptation. To address this challenge, we propose a novel Hard-Easy Dual Network (HEDN), which dynamically identifies \"Hard Source\" and \"Easy Source\" through a Task Difficulty Assessment (TDA) mechanism and establishes two specialized knowledge adaptation branches. Specifically, the Hard Network is dedicated to handling \"Hard Source\" with higher transfer difficulty by aligning marginal distribution differences between source and target domains. Conversely, the Easy Network focuses on \"Easy Source\" with low transfer difficulty, utilizing a prototype classifier to model intra-class clustering structures while generating reliable pseudo-labels for the target domain through a prototype-guided label propagation algorithm. Extensive experiments on two benchmark datasets, SEED and SEED-IV, demonstrate that HEDN achieves state-of-the-art performance in cross-subject EEG emotion recognition, with average accuracies of 93.58\\% on SEED and 79.82\\% on SEED-IV, respectively. These results confirm the effectiveness and generalizability of HEDN in cross-subject EEG emotion recognition.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 208,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06805",
        "abs_url": "https://arxiv.org/abs/2511.06805",
        "pdf_url": "https://arxiv.org/pdf/2511.06805",
        "title": "MathSE: Improving Multimodal Mathematical Reasoning via Self-Evolving Iterative Reflection and Reward-Guided Fine-Tuning",
        "authors": [
            "Jinhao Chen",
            "Zhen Yang",
            "Jianxin Shi",
            "Tianyu Wo",
            "Jie Tang"
        ],
        "comments": "19 pages, 11 figures",
        "subjects": "Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Multimodal large language models (MLLMs) have demonstrated remarkable capabilities in vision-language answering tasks. Despite their strengths, these models often encounter challenges in achieving complex reasoning tasks such as mathematical problem-solving. Previous works have focused on fine-tuning on specialized mathematical datasets. However, these datasets are typically distilled directly from teacher models, which capture only static reasoning patterns and leaving substantial gaps compared to student models. This reliance on fixed teacher-derived datasets not only restricts the model's ability to adapt to novel or more intricate questions that extend beyond the confines of the training data, but also lacks the iterative depth needed for robust generalization. To overcome these limitations, we propose \\textbf{\\method}, a \\textbf{Math}ematical \\textbf{S}elf-\\textbf{E}volving framework for MLLMs. In contrast to traditional one-shot fine-tuning paradigms, \\method iteratively refines the model through cycles of inference, reflection, and reward-based feedback. Specifically, we leverage iterative fine-tuning by incorporating correct reasoning paths derived from previous-stage inference and integrating reflections from a specialized Outcome Reward Model (ORM). To verify the effectiveness of \\method, we evaluate it on a suite of challenging benchmarks, demonstrating significant performance gains over backbone models. Notably, our experimental results on MathVL-test surpass the leading open-source multimodal mathematical reasoning model QVQ. Our code and models are available at \\texttt{https://zheny2751\\this http URL\\allowbreak this http URL}.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 209,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06812",
        "abs_url": "https://arxiv.org/abs/2511.06812",
        "pdf_url": "https://arxiv.org/pdf/2511.06812",
        "title": "Convergence of Actor-Critic Learning for Mean Field Games and Mean Field Control in Continuous Spaces",
        "authors": [
            "Jean-Pierre Fouque",
            "Mathieu LauriÃ¨re",
            "Mengrui Zhang"
        ],
        "comments": "",
        "subjects": "Optimization and Control (math.OC); Machine Learning (cs.LG); Probability (math.PR)",
        "abstract": "We establish the convergence of the deep actor-critic reinforcement learning algorithm presented in [Angiuli et al., 2023a] in the setting of continuous state and action spaces with an infinite discrete-time horizon. This algorithm provides solutions to Mean Field Game (MFG) or Mean Field Control (MFC) problems depending on the ratio between two learning rates: one for the value function and the other for the mean field term. In the MFC case, to rigorously identify the limit, we introduce a discretization of the state and action spaces, following the approach used in the finite-space case in [Angiuli et al., 2023b]. The convergence proofs rely on a generalization of the two-timescale framework introduced in [Borkar, 1997]. We further extend our convergence results to Mean Field Control Games, which involve locally cooperative and globally competitive populations. Finally, we present numerical experiments for linear-quadratic problems in one and two dimensions, for which explicit solutions are available.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 210,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06818",
        "abs_url": "https://arxiv.org/abs/2511.06818",
        "pdf_url": "https://arxiv.org/pdf/2511.06818",
        "title": "Learning to Focus: Focal Attention for Selective and Scalable Transformers",
        "authors": [
            "Dhananjay Ram",
            "Wei Xia",
            "Stefano Soatto"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Attention is a core component of transformer architecture, whether encoder-only, decoder-only, or encoder-decoder model. However, the standard softmax attention often produces noisy probability distribution, which can impair effective feature selection at every layer of these models, particularly for long contexts. We propose Focal Attention, a simple yet effective modification that sharpens the attention distribution by controlling the softmax temperature, either as a fixed hyperparameter or as a learnable parameter during training. This sharpening enables the model to concentrate on the most relevant tokens while suppressing irrelevant ones. Empirically, Focal Attention scales more favorably than standard transformer with respect to model size, training data, and context length. Across diverse benchmarks, it achieves the same accuracy with up to 42% fewer parameters or 33% less training data. On long-context tasks, it delivers substantial relative improvements ranging from 17% to 82%, demonstrating its effectiveness in real world applications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 211,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06821",
        "abs_url": "https://arxiv.org/abs/2511.06821",
        "pdf_url": "https://arxiv.org/pdf/2511.06821",
        "title": "Dimensionality reduction and width of deep neural networks based on topological degree theory",
        "authors": [
            "Xiao-Song Yang"
        ],
        "comments": "",
        "subjects": "General Topology (math.GN); Machine Learning (cs.LG)",
        "abstract": "In this paper we present a mathematical framework on linking of embeddings of compact topological spaces into Euclidean spaces and separability of linked embeddings under a specific class of dimension reduction maps. As applications of the established theory, we provide some fascinating insights into classification and approximation problems in deep learning theory in the setting of deep neural networks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 212,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06838",
        "abs_url": "https://arxiv.org/abs/2511.06838",
        "pdf_url": "https://arxiv.org/pdf/2511.06838",
        "title": "P3-LLM: An Integrated NPU-PIM Accelerator for LLM Inference Using Hybrid Numerical Formats",
        "authors": [
            "Yuzong Chen",
            "Chao Fang",
            "Xilai Dai",
            "Yuheng Wu",
            "Thierry Tambe",
            "Marian Verhelst",
            "Mohamed S. Abdelfattah"
        ],
        "comments": "Preprint. Under review",
        "subjects": "Hardware Architecture (cs.AR); Machine Learning (cs.LG)",
        "abstract": "The substantial memory bandwidth and computational demand of large language models (LLMs) present critical challenges for efficient inference. To tackle this, the literature has explored heterogeneous systems that combine neural processing units (NPUs) with DRAM-based processing-in-memory (PIM) for LLM acceleration. However, existing high-precision (e.g., FP16) PIM compute units incur significant area and power overhead in DRAM technology, limiting the effective computation throughput. In this paper, we introduce P3-LLM, a novel NPU-PIM integrated accelerator for LLM inference using hybrid numerical formats. Our approach is threefold: First, we propose a flexible mixed-precision quantization scheme, which leverages hybrid numerical formats to quantize different LLM operands with high compression efficiency and minimal accuracy loss. Second, we architect an efficient PIM accelerator co-design for P3-LLM, featuring lightweight compute units to support our hybrid numerical formats. The enhanced PIM compute units significantly boost the computation throughput under iso-area constraints. Third, we optimize the low-precision dataflow of different LLM modules by applying operator fusion to minimize the overhead of runtime dequantization. Our evaluation on a diverse set of representative LLMs and tasks demonstrates that P3-LLM achieves state-of-the-art quantization accuracy in terms of both KV-cache-only quantization and weight-activation quantization. Combining the proposed quantization scheme with PIM architecture co-design, P3-LLM yields an average of $4.9\\times$, $2.0\\times$, and $3.4\\times$ speedups over the state-of-the-art LLM accelerators HBM-PIM, Ecco, and Pimba, respectively. Our quantization code is available at this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 213,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06852",
        "abs_url": "https://arxiv.org/abs/2511.06852",
        "pdf_url": "https://arxiv.org/pdf/2511.06852",
        "title": "Differentiated Directional Intervention A Framework for Evading LLM Safety Alignment",
        "authors": [
            "Peng Zhang",
            "peijie sun"
        ],
        "comments": "AAAI-26-AIA",
        "subjects": "Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Software Engineering (cs.SE)",
        "abstract": "Safety alignment instills in Large Language Models (LLMs) a critical capacity to refuse malicious requests. Prior works have modeled this refusal mechanism as a single linear direction in the activation space. We posit that this is an oversimplification that conflates two functionally distinct neural processes: the detection of harm and the execution of a refusal. In this work, we deconstruct this single representation into a Harm Detection Direction and a Refusal Execution Direction. Leveraging this fine-grained model, we introduce Differentiated Bi-Directional Intervention (DBDI), a new white-box framework that precisely neutralizes the safety alignment at critical layer. DBDI applies adaptive projection nullification to the refusal execution direction while suppressing the harm detection direction via direct steering. Extensive experiments demonstrate that DBDI outperforms prominent jailbreaking methods, achieving up to a 97.88\\% attack success rate on models such as Llama-2. By providing a more granular and mechanistic framework, our work offers a new direction for the in-depth understanding of LLM safety alignment.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 214,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.06886",
        "abs_url": "https://arxiv.org/abs/2511.06886",
        "pdf_url": "https://arxiv.org/pdf/2511.06886",
        "title": "Inclusion of Role into Named Entity Recognition and Ranking",
        "authors": [
            "Neelesh Kumar Shukla",
            "Sanasam Ranbir Singh"
        ],
        "comments": "MTP Paper",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Most of the Natural Language Processing sys- tems are involved in entity-based processing for several tasks like Information Extraction, Question-Answering, Text-Summarization and so on. A new challenge comes when entities play roles according to their act or attributes in certain context. Entity Role Detection is the task of assigning such roles to the entities. Usu- ally real-world entities are of types: person, lo- cation and organization etc. Roles could be con- sidered as domain-dependent subtypes of these types. In the cases, where retrieving a subset of entities based on their roles is needed, poses the problem of defining the role and entities having those roles. This paper presents the study of study of solving Entity Role Detection prob- lem by modeling it as Named Entity Recogni- tion (NER) and Entity Retrieval/Ranking task. In NER, these roles could be considered as mutually exclusive classes and standard NER methods like sequence tagging could be used. For Entity Retrieval, Roles could be formulated as Query and entities as Collection on which the query needs to be executed. The aspect of Entity Retrieval task, which is different than document retrieval task is that the entities and roles against which they need to be retrieved are indirectly described. We have formulated au- tomated ways of learning representative words and phrases and building representations of roles and entities using them. We have also explored different contexts like sentence and document. Since the roles depend upon con- text, so it is not always possible to have large domain-specific dataset or knowledge bases for learning purposes, so we have tried to exploit the information from small dataset in domain- agnostic way.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 215,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07007",
        "abs_url": "https://arxiv.org/abs/2511.07007",
        "pdf_url": "https://arxiv.org/pdf/2511.07007",
        "title": "TrueCity: Real and Simulated Urban Data for Cross-Domain 3D Scene Understanding",
        "authors": [
            "Duc Nguyen",
            "Yan-Ling Lai",
            "Qilin Zhang",
            "Prabin Gyawali",
            "Benedikt Schwab",
            "Olaf Wysocki",
            "Thomas H. Kolbe"
        ],
        "comments": "The paper accepted for 3DV 2026 (International Conference on 3D Vision 2026)",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "3D semantic scene understanding remains a long-standing challenge in the 3D computer vision community. One of the key issues pertains to limited real-world annotated data to facilitate generalizable models. The common practice to tackle this issue is to simulate new data. Although synthetic datasets offer scalability and perfect labels, their designer-crafted scenes fail to capture real-world complexity and sensor noise, resulting in a synthetic-to-real domain gap. Moreover, no benchmark provides synchronized real and simulated point clouds for segmentation-oriented domain shift analysis. We introduce TrueCity, the first urban semantic segmentation benchmark with cm-accurate annotated real-world point clouds, semantic 3D city models, and annotated simulated point clouds representing the same city. TrueCity proposes segmentation classes aligned with international 3D city modeling standards, enabling consistent evaluation of synthetic-to-real gap. Our extensive experiments on common baselines quantify domain shift and highlight strategies for exploiting synthetic data to enhance real-world 3D scene understanding. We are convinced that the TrueCity dataset will foster further development of sim-to-real gap quantification and enable generalizable data-driven models. The data, code, and 3D models are available online: this https URL",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 216,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07011",
        "abs_url": "https://arxiv.org/abs/2511.07011",
        "pdf_url": "https://arxiv.org/pdf/2511.07011",
        "title": "Multilingual Lexical Feature Analysis of Spoken Language for Predicting Major Depression Symptom Severity",
        "authors": [
            "Anastasiia Tokareva",
            "Judith Dineley",
            "Zoe Firth",
            "Pauline Conde",
            "Faith Matcham",
            "Sara Siddi",
            "Femke Lamers",
            "Ewan Carr",
            "Carolin Oetzmann",
            "Daniel Leightley",
            "Yuezhou Zhang",
            "Amos A. Folarin",
            "Josep Maria Haro",
            "Brenda W.J.H. Penninx",
            "Raquel Bailon",
            "Srinivasan Vairavan",
            "Til Wykes",
            "Richard J.B. Dobson",
            "Vaibhav A. Narayan",
            "Matthew Hotopf",
            "Nicholas Cummins",
            "RADAR-CNS Consortium"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "Background: Captured between clinical appointments using mobile devices, spoken language has potential for objective, more regular assessment of symptom severity and earlier detection of relapse in major depressive disorder. However, research to date has largely been in non-clinical cross-sectional samples of written language using complex machine learning (ML) approaches with limited interpretability. Methods: We describe an initial exploratory analysis of longitudinal speech data and PHQ-8 assessments from 5,836 recordings of 586 participants in the UK, Netherlands, and Spain, collected in the RADAR-MDD study. We sought to identify interpretable lexical features associated with MDD symptom severity with linear mixed-effects modelling. Interpretable features and high-dimensional vector embeddings were also used to test the prediction performance of four regressor ML models. Results: In English data, MDD symptom severity was associated with 7 features including lexical diversity measures and absolutist language. In Dutch, associations were observed with words per sentence and positive word frequency; no associations were observed in recordings collected in Spain. The predictive power of lexical features and vector embeddings was near chance level across all languages. Limitations: Smaller samples in non-English speech and methodological choices, such as the elicitation prompt, may have also limited the effect sizes observable. A lack of NLP tools in languages other than English restricted our feature choice. Conclusion: To understand the value of lexical markers in clinical research and practice, further research is needed in larger samples across several languages using improved protocols, and ML models that account for within- and between-individual variations in language.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 217,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07047",
        "abs_url": "https://arxiv.org/abs/2511.07047",
        "pdf_url": "https://arxiv.org/pdf/2511.07047",
        "title": "Anatomy-Aware Lymphoma Lesion Detection in Whole-Body PET/CT",
        "authors": [
            "Simone Bendazzoli",
            "Antonios Tzortzakakis",
            "Andreas Abrahamsson",
            "BjÃ¶rn Engelbrekt Wahlin",
            "Ãrjan Smedby",
            "Maria Holstensson",
            "Rodrigo Moreno"
        ],
        "comments": "",
        "subjects": "Image and Video Processing (eess.IV); Machine Learning (cs.LG)",
        "abstract": "Early cancer detection is crucial for improving patient outcomes, and 18F FDG PET/CT imaging plays a vital role by combining metabolic and anatomical information. Accurate lesion detection remains challenging due to the need to identify multiple lesions of varying sizes. In this study, we investigate the effect of adding anatomy prior information to deep learning-based lesion detection models. In particular, we add organ segmentation masks from the TotalSegmentator tool as auxiliary inputs to provide anatomical context to nnDetection, which is the state-of-the-art for lesion detection, and Swin Transformer. The latter is trained in two stages that combine self-supervised pre-training and supervised fine-tuning. The method is tested in the AutoPET and Karolinska lymphoma datasets. The results indicate that the inclusion of anatomical priors substantially improves the detection performance within the nnDetection framework, while it has almost no impact on the performance of the vision transformer. Moreover, we observe that Swin Transformer does not offer clear advantages over conventional convolutional neural network (CNN) encoders used in nnDetection. These findings highlight the critical role of the anatomical context in cancer lesion detection, especially in CNN-based models.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 218,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07055",
        "abs_url": "https://arxiv.org/abs/2511.07055",
        "pdf_url": "https://arxiv.org/pdf/2511.07055",
        "title": "When Sufficient is not Enough: Utilizing the Rashomon Effect for Complete Evidence Extraction",
        "authors": [
            "Katharina Beckh",
            "Stefan RÃ¼ping"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "Feature attribution methods typically provide minimal sufficient evidence justifying a model decision. However, in many applications this is inadequate. For compliance and cataloging, the full set of contributing features must be identified - complete evidence. We perform a case study on a medical dataset which contains human-annotated complete evidence. We show that individual models typically recover only subsets of complete evidence and that aggregating evidence from several models improves evidence recall from $\\sim$0.60 (single best model) to $\\sim$0.86 (ensemble). We analyze the recall-precision trade-off, the role of training with evidence, dynamic ensembles with certainty thresholds, and discuss implications.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 219,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07065",
        "abs_url": "https://arxiv.org/abs/2511.07065",
        "pdf_url": "https://arxiv.org/pdf/2511.07065",
        "title": "Aligning Attention with Human Rationales for Self-Explaining Hate Speech Detection",
        "authors": [
            "Brage Eilertsen",
            "RÃ¸skva BjÃ¸rgfinsdÃ³ttir",
            "Francielle Vargas",
            "Ali Ramezani-Kebrya"
        ],
        "comments": "Accepted at the Annual AAAI Conference on Artificial Intelligence (AAAI26)",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "The opaque nature of deep learning models presents significant challenges for the ethical deployment of hate speech detection systems. To address this limitation, we introduce Supervised Rational Attention (SRA), a framework that explicitly aligns model attention with human rationales, improving both interpretability and fairness in hate speech classification. SRA integrates a supervised attention mechanism into transformer-based classifiers, optimizing a joint objective that combines standard classification loss with an alignment loss term that minimizes the discrepancy between attention weights and human-annotated rationales. We evaluated SRA on hate speech benchmarks in English (HateXplain) and Portuguese (HateBRXplain) with rationale annotations. Empirically, SRA achieves 2.4x better explainability compared to current baselines, and produces token-level explanations that are more faithful and human-aligned. In terms of fairness, SRA achieves competitive fairness across all measures, with second-best performance in detecting toxic posts targeting identity groups, while maintaining comparable results on other metrics. These findings demonstrate that incorporating human rationales into attention mechanisms can enhance interpretability and faithfulness without compromising fairness.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 220,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07068",
        "abs_url": "https://arxiv.org/abs/2511.07068",
        "pdf_url": "https://arxiv.org/pdf/2511.07068",
        "title": "ClusterMine: Robust Label-Free Visual Out-Of-Distribution Detection via Concept Mining from Text Corpora",
        "authors": [
            "Nikolas Adaloglou",
            "Diana Petrusheva",
            "Mohamed Asker",
            "Felix Michels",
            "Markus Kollmann"
        ],
        "comments": "Accepted in WACV 2026. Code in this https URL 9 Tables, 11 Figures",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Large-scale visual out-of-distribution (OOD) detection has witnessed remarkable progress by leveraging vision-language models such as CLIP. However, a significant limitation of current methods is their reliance on a pre-defined set of in-distribution (ID) ground-truth label names (positives). These fixed label names can be unavailable, unreliable at scale, or become less relevant due to in-distribution shifts after deployment. Towards truly unsupervised OOD detection, we utilize widely available text corpora for positive label mining, bypassing the need for positives. In this paper, we utilize widely available text corpora for positive label mining under a general concept mining paradigm. Within this framework, we propose ClusterMine, a novel positive label mining method. ClusterMine is the first method to achieve state-of-the-art OOD detection performance without access to positive labels. It extracts positive concepts from a large text corpus by combining visual-only sample consistency (via clustering) and zero-shot image-text consistency. Our experimental study reveals that ClusterMine is scalable across a plethora of CLIP models and achieves state-of-the-art robustness to covariate in-distribution shifts. The code is available at this https URL.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 221,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07092",
        "abs_url": "https://arxiv.org/abs/2511.07092",
        "pdf_url": "https://arxiv.org/pdf/2511.07092",
        "title": "Sample-efficient quantum error mitigation via classical learning surrogates",
        "authors": [
            "Wei-You Liao",
            "Ge Yan",
            "Yujin Song",
            "Tian-Ci Tian",
            "Wei-Ming Zhu",
            "De-Tao Jiang",
            "Yuxuan Du",
            "He-Liang Huang"
        ],
        "comments": "26 pages, 8 figures",
        "subjects": "Quantum Physics (quant-ph); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "The pursuit of practical quantum utility on near-term quantum processors is critically challenged by their inherent noise. Quantum error mitigation (QEM) techniques are leading solutions to improve computation fidelity with relatively low qubit-overhead, while full-scale quantum error correction remains a distant goal. However, QEM techniques incur substantial measurement overheads, especially when applied to families of quantum circuits parameterized by classical inputs. Focusing on zero-noise extrapolation (ZNE), a widely adopted QEM technique, here we devise the surrogate-enabled ZNE (S-ZNE), which leverages classical learning surrogates to perform ZNE entirely on the classical side. Unlike conventional ZNE, whose measurement cost scales linearly with the number of circuits, S-ZNE requires only constant measurement overhead for an entire family of quantum circuits, offering superior scalability. Theoretical analysis indicates that S-ZNE achieves accuracy comparable to conventional ZNE in many practical scenarios, and numerical experiments on up to 100-qubit ground-state energy and quantum metrology tasks confirm its effectiveness. Our approach provides a template that can be effectively extended to other quantum error mitigation protocols, opening a promising path toward scalable error mitigation.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 222,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07109",
        "abs_url": "https://arxiv.org/abs/2511.07109",
        "pdf_url": "https://arxiv.org/pdf/2511.07109",
        "title": "A Provably-Correct and Robust Convex Model for Smooth Separable NMF",
        "authors": [
            "Junjun Pan",
            "Valentin Leplat",
            "Michael Ng",
            "Nicolas Gillis"
        ],
        "comments": "30 pages, 10 figures, code available from this https URL",
        "subjects": "Numerical Analysis (math.NA); Machine Learning (cs.LG); Signal Processing (eess.SP); Optimization and Control (math.OC); Machine Learning (stat.ML)",
        "abstract": "Nonnegative matrix factorization (NMF) is a linear dimensionality reduction technique for nonnegative data, with applications such as hyperspectral unmixing and topic modeling. NMF is a difficult problem in general (NP-hard), and its solutions are typically not unique. To address these two issues, additional constraints or assumptions are often used. In particular, separability assumes that the basis vectors in the NMF are equal to some columns of the input matrix. In that case, the problem is referred to as separable NMF (SNMF) and can be solved in polynomial-time with robustness guarantees, while identifying a unique solution. However, in real-world scenarios, due to noise or variability, multiple data points may lie near the basis vectors, which SNMF does not leverage. In this work, we rely on the smooth separability assumption, which assumes that each basis vector is close to multiple data points. We explore the properties of the corresponding problem, referred to as smooth SNMF (SSNMF), and examine how it relates to SNMF and orthogonal NMF. We then propose a convex model for SSNMF and show that it provably recovers the sought-after factors, even in the presence of noise. We finally adapt an existing fast gradient method to solve this convex model for SSNMF, and show that it compares favorably with state-of-the-art methods on both synthetic and hyperspectral datasets.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 223,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07129",
        "abs_url": "https://arxiv.org/abs/2511.07129",
        "pdf_url": "https://arxiv.org/pdf/2511.07129",
        "title": "LoRA on the Go: Instance-level Dynamic LoRA Selection and Merging",
        "authors": [
            "Seungeon Lee",
            "Soumi Das",
            "Manish Gupta",
            "Krishna P. Gummadi"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)",
        "abstract": "Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient approach for fine-tuning large language this http URL, conventional LoRA adapters are typically trained for a single task, limiting their applicability in real-world settings where inputs may span diverse and unpredictable domains. At inference time, existing approaches combine multiple LoRAs for improving performance on diverse tasks, while usually requiring labeled data or additional task-specific training, which is expensive at scale. In this work, we introduce LoRA on the Go (LoGo), a training-free framework that dynamically selects and merges adapters at the instance level without any additional requirements. LoGo leverages signals extracted from a single forward pass through LoRA adapters, to identify the most relevant adapters and determine their contributions on-the-fly. Across 5 NLP benchmarks, 27 datasets, and 3 model families, LoGo outperforms training-based baselines on some tasks upto a margin of 3.6% while remaining competitive on other tasks and maintaining inference throughput, highlighting its effectiveness and practicality.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 224,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07139",
        "abs_url": "https://arxiv.org/abs/2511.07139",
        "pdf_url": "https://arxiv.org/pdf/2511.07139",
        "title": "Trading Vector Data in Vector Databases",
        "authors": [
            "Jin Cheng",
            "Xiangxiang Dai",
            "Ningning Ding",
            "John C.S. Lui",
            "Jianwei Huang"
        ],
        "comments": "Accepted by ICDE 2026",
        "subjects": "Databases (cs.DB); Machine Learning (cs.LG)",
        "abstract": "Vector data trading is essential for cross-domain learning with vector databases, yet it remains largely unexplored. We study this problem under online learning, where sellers face uncertain retrieval costs and buyers provide stochastic feedback to posted prices. Three main challenges arise: (1) heterogeneous and partial feedback in configuration learning, (2) variable and complex feedback in pricing learning, and (3) inherent coupling between configuration and pricing decisions. We propose a hierarchical bandit framework that jointly optimizes retrieval configurations and pricing. Stage I employs contextual clustering with confidence-based exploration to learn effective configurations with logarithmic regret. Stage II adopts interval-based price selection with local Taylor approximation to estimate buyer responses and achieve sublinear regret. We establish theoretical guarantees with polynomial time complexity and validate the framework on four real-world datasets, demonstrating consistent improvements in cumulative reward and regret reduction compared with existing methods.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 225,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07155",
        "abs_url": "https://arxiv.org/abs/2511.07155",
        "pdf_url": "https://arxiv.org/pdf/2511.07155",
        "title": "Dynamics-Decoupled Trajectory Alignment for Sim-to-Real Transfer in Reinforcement Learning for Autonomous Driving",
        "authors": [
            "Thomas Steinecker",
            "Alexander Bienemann",
            "Denis Trescher",
            "Thorsten Luettel",
            "Mirko Maehlisch"
        ],
        "comments": "",
        "subjects": "Robotics (cs.RO); Machine Learning (cs.LG)",
        "abstract": "Reinforcement learning (RL) has shown promise in robotics, but deploying RL on real vehicles remains challenging due to the complexity of vehicle dynamics and the mismatch between simulation and reality. Factors such as tire characteristics, road surface conditions, aerodynamic disturbances, and vehicle load make it infeasible to model real-world dynamics accurately, which hinders direct transfer of RL agents trained in simulation. In this paper, we present a framework that decouples motion planning from vehicle control through a spatial and temporal alignment strategy between a virtual vehicle and the real system. An RL agent is first trained in simulation using a kinematic bicycle model to output continuous control actions. Its behavior is then distilled into a trajectory-predicting agent that generates finite-horizon ego-vehicle trajectories, enabling synchronization between virtual and real vehicles. At deployment, a Stanley controller governs lateral dynamics, while longitudinal alignment is maintained through adaptive update mechanisms that compensate for deviations between virtual and real trajectories. We validate our approach on a real vehicle and demonstrate that the proposed alignment strategy enables robust zero-shot transfer of RL-based motion planning from simulation to reality, successfully decoupling high-level trajectory generation from low-level vehicle control.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 226,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07197",
        "abs_url": "https://arxiv.org/abs/2511.07197",
        "pdf_url": "https://arxiv.org/pdf/2511.07197",
        "title": "Simulation-based Methods for Optimal Sampling Design in Systems Biology",
        "authors": [
            "Tuan Minh Ha",
            "Binh Thanh Nguyen",
            "Lam Si Tung Ho"
        ],
        "comments": "",
        "subjects": "Machine Learning (stat.ML); Machine Learning (cs.LG)",
        "abstract": "In many areas of systems biology, including virology, pharmacokinetics, and population biology, dynamical systems are commonly used to describe biological processes. These systems can be characterized by estimating their parameters from sampled data. The key problem is how to optimally select sampling points to achieve accurate parameter estimation. Classical approaches often rely on Fisher information matrix-based criteria such as A-, D-, and E-optimality, which require an initial parameter estimate and may yield suboptimal results when the estimate is inaccurate. This study proposes two simulation-based methods for optimal sampling design that do not depend on initial parameter estimates. The first method, E-optimal-ranking (EOR), employs the E-optimal criterion, while the second utilizes a Long Short-Term Memory (LSTM) neural network. Simulation studies based on the Lotka-Volterra and three-compartment models demonstrate that the proposed methods outperform both random selection and classical E-optimal design.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 227,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07210",
        "abs_url": "https://arxiv.org/abs/2511.07210",
        "pdf_url": "https://arxiv.org/pdf/2511.07210",
        "title": "Breaking the Stealth-Potency Trade-off in Clean-Image Backdoors with Generative Trigger Optimization",
        "authors": [
            "Binyan Xu",
            "Fan Yang",
            "Di Tang",
            "Xilin Dai",
            "Kehuan Zhang"
        ],
        "comments": "19 pages, 22 figures, 15 tables. To appear in AAAI '26 (Oral). This paper extends the AAAI-2026 version by including the Appendix",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Cryptography and Security (cs.CR); Machine Learning (cs.LG)",
        "abstract": "Clean-image backdoor attacks, which use only label manipulation in training datasets to compromise deep neural networks, pose a significant threat to security-critical applications. A critical flaw in existing methods is that the poison rate required for a successful attack induces a proportional, and thus noticeable, drop in Clean Accuracy (CA), undermining their stealthiness. This paper presents a new paradigm for clean-image attacks that minimizes this accuracy degradation by optimizing the trigger itself. We introduce Generative Clean-Image Backdoors (GCB), a framework that uses a conditional InfoGAN to identify naturally occurring image features that can serve as potent and stealthy triggers. By ensuring these triggers are easily separable from benign task-related features, GCB enables a victim model to learn the backdoor from an extremely small set of poisoned examples, resulting in a CA drop of less than 1%. Our experiments demonstrate GCB's remarkable versatility, successfully adapting to six datasets, five architectures, and four tasks, including the first demonstration of clean-image backdoors in regression and segmentation. GCB also exhibits resilience against most of the existing backdoor defenses.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 228,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07233",
        "abs_url": "https://arxiv.org/abs/2511.07233",
        "pdf_url": "https://arxiv.org/pdf/2511.07233",
        "title": "Noise & pattern: identity-anchored Tikhonov regularization for robust structural anomaly detection",
        "authors": [
            "Alexander Bauer",
            "Klaus-Robert MÃ¼ller"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Anomaly detection plays a pivotal role in automated industrial inspection, aiming to identify subtle or rare defects in otherwise uniform visual patterns. As collecting representative examples of all possible anomalies is infeasible, we tackle structural anomaly detection using a self-supervised autoencoder that learns to repair corrupted inputs. To this end, we introduce a corruption model that injects artificial disruptions into training images to mimic structural defects. While reminiscent of denoising autoencoders, our approach differs in two key aspects. First, instead of unstructured i.i.d.\\ noise, we apply structured, spatially coherent perturbations that make the task a hybrid of segmentation and inpainting. Second, and counterintuitively, we add and preserve Gaussian noise on top of the occlusions, which acts as a Tikhonov regularizer anchoring the Jacobian of the reconstruction function toward identity. This identity-anchored regularization stabilizes reconstruction and further improves both detection and segmentation accuracy. On the MVTec AD benchmark, our method achieves state-of-the-art results (I/P-AUROC: 99.9/99.4), supporting our theoretical framework and demonstrating its practical relevance for automatic inspection.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 229,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07244",
        "abs_url": "https://arxiv.org/abs/2511.07244",
        "pdf_url": "https://arxiv.org/pdf/2511.07244",
        "title": "A Fully Polynomial-Time Algorithm for Robustly Learning Halfspaces over the Hypercube",
        "authors": [
            "Gautam Chandrasekaran",
            "Adam R. Klivans",
            "Konstantinos Stavropoulos",
            "Arsen Vasilyan"
        ],
        "comments": "52 pages, 1 figure",
        "subjects": "Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG)",
        "abstract": "We give the first fully polynomial-time algorithm for learning halfspaces with respect to the uniform distribution on the hypercube in the presence of contamination, where an adversary may corrupt some fraction of examples and labels arbitrarily. We achieve an error guarantee of $\\eta^{O(1)}+\\epsilon$ where $\\eta$ is the noise rate. Such a result was not known even in the agnostic setting, where only labels can be adversarially corrupted. All prior work over the last two decades has a superpolynomial dependence in $1/\\epsilon$ or succeeds only with respect to continuous marginals (such as log-concave densities). Previous analyses rely heavily on various structural properties of continuous distributions such as anti-concentration. Our approach avoids these requirements and makes use of a new algorithm for learning Generalized Linear Models (GLMs) with only a polylogarithmic dependence on the activation function's Lipschitz constant. More generally, our framework shows that supervised learning with respect to discrete distributions is not as difficult as previously thought.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 230,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07262",
        "abs_url": "https://arxiv.org/abs/2511.07262",
        "pdf_url": "https://arxiv.org/pdf/2511.07262",
        "title": "AgenticSciML: Collaborative Multi-Agent Systems for Emergent Discovery in Scientific Machine Learning",
        "authors": [
            "Qile Jiang",
            "George Karniadakis"
        ],
        "comments": "",
        "subjects": "Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)",
        "abstract": "Scientific Machine Learning (SciML) integrates data-driven inference with physical modeling to solve complex problems in science and engineering. However, the design of SciML architectures, loss formulations, and training strategies remains an expert-driven research process, requiring extensive experimentation and problem-specific insights. Here we introduce AgenticSciML, a collaborative multi-agent system in which over 10 specialized AI agents collaborate to propose, critique, and refine SciML solutions through structured reasoning and iterative evolution. The framework integrates structured debate, retrieval-augmented method memory, and ensemble-guided evolutionary search, enabling the agents to generate and assess new hypotheses about architectures and optimization procedures. Across physics-informed learning and operator learning tasks, the framework discovers solution methods that outperform single-agent and human-designed baselines by up to four orders of magnitude in error reduction. The agents produce novel strategies -- including adaptive mixture-of-expert architectures, decomposition-based PINNs, and physics-informed operator learning models -- that do not appear explicitly in the curated knowledge base. These results show that collaborative reasoning among AI agents can yield emergent methodological innovation, suggesting a path toward scalable, transparent, and autonomous discovery in scientific computing.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 231,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07270",
        "abs_url": "https://arxiv.org/abs/2511.07270",
        "pdf_url": "https://arxiv.org/pdf/2511.07270",
        "title": "High-Dimensional Asymptotics of Differentially Private PCA",
        "authors": [
            "Youngjoo Yun",
            "Rishabh Dudeja"
        ],
        "comments": "",
        "subjects": "Statistics Theory (math.ST); Information Theory (cs.IT); Machine Learning (cs.LG); Probability (math.PR); Machine Learning (stat.ML)",
        "abstract": "In differential privacy, statistics of a sensitive dataset are privatized by introducing random noise. Most privacy analyses provide privacy bounds specifying a noise level sufficient to achieve a target privacy guarantee. Sometimes, these bounds are pessimistic and suggest adding excessive noise, which overwhelms the meaningful signal. It remains unclear if such high noise levels are truly necessary or a limitation of the proof techniques. This paper explores whether we can obtain sharp privacy characterizations that identify the smallest noise level required to achieve a target privacy level for a given mechanism. We study this problem in the context of differentially private principal component analysis, where the goal is to privatize the leading principal components (PCs) of a dataset with n samples and p features. We analyze the exponential mechanism for this problem in a model-free setting and provide sharp utility and privacy characterizations in the high-dimensional limit ($p\\rightarrow\\infty$). Our privacy result shows that, in high dimensions, detecting the presence of a target individual in the dataset using the privatized PCs is exactly as hard as distinguishing two Gaussians with slightly different means, where the mean difference depends on certain spectral properties of the dataset. Our privacy analysis combines the hypothesis-testing formulation of privacy guarantees proposed by Dong, Roth, and Su (2022) with classical contiguity arguments due to Le Cam to obtain sharp high-dimensional privacy characterizations.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 232,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07280",
        "abs_url": "https://arxiv.org/abs/2511.07280",
        "pdf_url": "https://arxiv.org/pdf/2511.07280",
        "title": "The Value of Personalized Recommendations: Evidence from Netflix",
        "authors": [
            "Kevin Zielnicki",
            "Guy Aridor",
            "AurÃ©lien Bibaut",
            "Allen Tran",
            "Winston Chou",
            "Nathan Kallus"
        ],
        "comments": "",
        "subjects": "General Economics (econ.GN); Information Retrieval (cs.IR); Machine Learning (cs.LG)",
        "abstract": "Personalized recommendation systems shape much of user choice online, yet their targeted nature makes separating out the value of recommendation and the underlying goods challenging. We build a discrete choice model that embeds recommendation-induced utility, low-rank heterogeneity, and flexible state dependence and apply the model to viewership data at Netflix. We exploit idiosyncratic variation introduced by the recommendation algorithm to identify and separately value these components as well as to recover model-free diversion ratios that we can use to validate our structural model. We use the model to evaluate counterfactuals that quantify the incremental engagement generated by personalized recommendations. First, we show that replacing the current recommender system with a matrix factorization or popularity-based algorithm would lead to 4% and 12% reduction in engagement, respectively, and decreased consumption diversity. Second, most of the consumption increase from recommendations comes from effective targeting, not mechanical exposure, with the largest gains for mid-popularity goods (as opposed to broadly appealing or very niche goods).",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 233,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07313",
        "abs_url": "https://arxiv.org/abs/2511.07313",
        "pdf_url": "https://arxiv.org/pdf/2511.07313",
        "title": "De-Individualizing fMRI Signals via Mahalanobis Whitening and Bures Geometry",
        "authors": [
            "Aaron Jacobson",
            "Tingting Dan",
            "Martin Styner",
            "Guorong Wu",
            "Shahar Kovalsky",
            "Caroline Moosmueller"
        ],
        "comments": "34 pages, 7 figures",
        "subjects": "Neurons and Cognition (q-bio.NC); Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)",
        "abstract": "Functional connectivity has been widely investigated to understand brain disease in clinical studies and imaging-based neuroscience, and analyzing changes in functional connectivity has proven to be valuable for understanding and computationally evaluating the effects on brain function caused by diseases or experimental stimuli. By using Mahalanobis data whitening prior to the use of dimensionality reduction algorithms, we are able to distill meaningful information from fMRI signals about subjects and the experimental stimuli used to prompt them. Furthermore, we offer an interpretation of Mahalanobis whitening as a two-stage de-individualization of data which is motivated by similarity as captured by the Bures distance, which is connected to quantum mechanics. These methods have potential to aid discoveries about the mechanisms that link brain function with cognition and behavior and may improve the accuracy and consistency of Alzheimer's diagnosis, especially in the preclinical stage of disease progression.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 234,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07317",
        "abs_url": "https://arxiv.org/abs/2511.07317",
        "pdf_url": "https://arxiv.org/pdf/2511.07317",
        "title": "RLVE: Scaling Up Reinforcement Learning for Language Models with Adaptive Verifiable Environments",
        "authors": [
            "Zhiyuan Zeng",
            "Hamish Ivison",
            "Yiping Wang",
            "Lifan Yuan",
            "Shuyue Stella Li",
            "Zhuorui Ye",
            "Siting Li",
            "Jacqueline He",
            "Runlong Zhou",
            "Tong Chen",
            "Chenyang Zhao",
            "Yulia Tsvetkov",
            "Simon Shaolei Du",
            "Natasha Jaques",
            "Hao Peng",
            "Pang Wei Koh",
            "Hannaneh Hajishirzi"
        ],
        "comments": "",
        "subjects": "Computation and Language (cs.CL); Machine Learning (cs.LG)",
        "abstract": "We introduce Reinforcement Learning (RL) with Adaptive Verifiable Environments (RLVE), an approach using verifiable environments that procedurally generate problems and provide algorithmically verifiable rewards, to scale up RL for language models (LMs). RLVE enables each verifiable environment to dynamically adapt its problem difficulty distribution to the policy model's capabilities as training progresses. In contrast, static data distributions often lead to vanishing learning signals when problems are either too easy or too hard for the policy. To implement RLVE, we create RLVE-Gym, a large-scale suite of 400 verifiable environments carefully developed through manual environment engineering. Using RLVE-Gym, we show that environment scaling, i.e., expanding the collection of training environments, consistently improves generalizable reasoning capabilities. RLVE with joint training across all 400 environments in RLVE-Gym yields a 3.37% absolute average improvement across six reasoning benchmarks, starting from one of the strongest 1.5B reasoning LMs. By comparison, continuing this LM's original RL training yields only a 0.49% average absolute gain despite using over 3x more compute. We release our code publicly.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 235,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07325",
        "abs_url": "https://arxiv.org/abs/2511.07325",
        "pdf_url": "https://arxiv.org/pdf/2511.07325",
        "title": "Garbage Vulnerable Point Monitoring using IoT and Computer Vision",
        "authors": [
            "R. Kumar",
            "A. Lall",
            "S. Chaudhari",
            "M. Kale",
            "A. Vattem"
        ],
        "comments": "",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "This paper proposes a smart way to manage municipal solid waste by using the Internet of Things (IoT) and computer vision (CV) to monitor illegal waste dumping at garbage vulnerable points (GVPs) in urban areas. The system can quickly detect and monitor dumped waste using a street-level camera and object detection algorithm. Data was collected from the Sangareddy district in Telangana, India. A series of comprehensive experiments was carried out using the proposed dataset to assess the accuracy and overall performance of various object detection models. Specifically, we performed an in-depth evaluation of YOLOv8, YOLOv10, YOLO11m, and RT-DETR on our dataset. Among these models, YOLO11m achieved the highest accuracy of 92.39\\% in waste detection, demonstrating its effectiveness in detecting waste. Additionally, it attains an mAP@50 of 0.91, highlighting its high precision. These findings confirm that the object detection model is well-suited for monitoring and tracking waste dumping events at GVP locations. Furthermore, the system effectively captures waste disposal patterns, including hourly, daily, and weekly dumping trends, ensuring comprehensive daily and nightly monitoring.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 236,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07347",
        "abs_url": "https://arxiv.org/abs/2511.07347",
        "pdf_url": "https://arxiv.org/pdf/2511.07347",
        "title": "Walsh-Hadamard Neural Operators for Solving PDEs with Discontinuous Coefficients",
        "authors": [
            "Giorrgio M. Cavallazzi",
            "Miguel Perex Cuadrado",
            "Alfredo Pinelli"
        ],
        "comments": "",
        "subjects": "Computational Physics (physics.comp-ph); Machine Learning (cs.LG)",
        "abstract": "Neural operators have emerged as powerful tools for learning solution operators of partial differential equations (PDEs). However, standard spectral methods based on Fourier transforms struggle with problems involving discontinuous coefficients due to the Gibbs phenomenon and poor representation of sharp interfaces. We introduce the Walsh-Hadamard Neural Operator (WHNO), which leverages Walsh-Hadamard transforms-a spectral basis of rectangular wave functions naturally suited for piecewise constant fields-combined with learnable spectral weights that transform low-sequency Walsh coefficients to capture global dependencies efficiently. We validate WHNO on three problems: steady-state Darcy flow (preliminary validation), heat conduction with discontinuous thermal conductivity, and the 2D Burgers equation with discontinuous initial conditions. In controlled comparisons with Fourier Neural Operators (FNO) under identical conditions, WHNO demonstrates superior accuracy with better preservation of sharp solution features at material interfaces. Critically, we discover that weighted ensemble combinations of WHNO and FNO achieve substantial improvements over either model alone: for both heat conduction and Burgers equation, optimal ensembles reduce mean squared error by 35-40 percent and maximum error by up to 25 percent compared to individual models. This demonstrates that Walsh-Hadamard and Fourier representations capture complementary aspects of discontinuous PDE solutions, with WHNO excelling at sharp interfaces while FNO captures smooth features effectively.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 237,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07366",
        "abs_url": "https://arxiv.org/abs/2511.07366",
        "pdf_url": "https://arxiv.org/pdf/2511.07366",
        "title": "UAV-Assisted Resilience in 6G and Beyond Network Energy Saving: A Multi-Agent DRL Approach",
        "authors": [
            "Dao Lan Vy Dinh",
            "Anh Nguyen Thi Mai",
            "Hung Tran",
            "Giang Quynh Le Vu",
            "Tu Dac Ho",
            "Zhenni Pan",
            "Vo Nhan Van",
            "Symeon Chatzinotas",
            "Dinh-Hieu Tran"
        ],
        "comments": "6 pages, 5 figures, 1 table",
        "subjects": "Networking and Internet Architecture (cs.NI); Machine Learning (cs.LG)",
        "abstract": "This paper investigates the unmanned aerial vehicle (UAV)-assisted resilience perspective in the 6G network energy saving (NES) scenario. More specifically, we consider multiple ground base stations (GBSs) and each GBS has three different sectors/cells in the terrestrial networks, and multiple cells are turned off due to NES or incidents, e.g., disasters, hardware failures, or outages. To address this, we propose a Multi-Agent Deep Deterministic Policy Gradient (MADDPG) framework to enable UAV-assisted communication by jointly optimizing UAV trajectories, transmission power, and user-UAV association under a sleeping ground base station (GBS) strategy. This framework aims to ensure the resilience of active users in the network and the long-term operability of UAVs. Specifically, it maximizes service coverage for users during power outages or NES zones, while minimizing the energy consumption of UAVs. Simulation results demonstrate that the proposed MADDPG policy consistently achieves high coverage ratio across different testing episodes, outperforming other baselines. Moreover, the MADDPG framework attains the lowest total energy consumption, with a reduction of approximately 24\\% compared to the conventional all GBS ON configuration, while maintaining a comparable user service rate. These results confirm the effectiveness of the proposed approach in achieving a superior trade-off between energy efficiency and service performance, supporting the development of sustainable and resilient UAV-assisted cellular networks.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 238,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07398",
        "abs_url": "https://arxiv.org/abs/2511.07398",
        "pdf_url": "https://arxiv.org/pdf/2511.07398",
        "title": "Solving bilevel optimization via sequential minimax optimization",
        "authors": [
            "Zhaosong Lu",
            "Sanyou Mei"
        ],
        "comments": "Accepted by Mathematics of Operations Research",
        "subjects": "Optimization and Control (math.OC); Machine Learning (cs.LG); Numerical Analysis (math.NA); Machine Learning (stat.ML)",
        "abstract": "In this paper we propose a sequential minimax optimization (SMO) method for solving a class of constrained bilevel optimization problems in which the lower-level part is a possibly nonsmooth convex optimization problem, while the upper-level part is a possibly nonconvex optimization problem. Specifically, SMO applies a first-order method to solve a sequence of minimax subproblems, which are obtained by employing a hybrid of modified augmented Lagrangian and penalty schemes on the bilevel optimization problems. Under suitable assumptions, we establish an operation complexity of $O(\\varepsilon^{-7}\\log\\varepsilon^{-1})$ and $O(\\varepsilon^{-6}\\log\\varepsilon^{-1})$, measured in terms of fundamental operations, for SMO in finding an $\\varepsilon$-KKT solution of the bilevel optimization problems with merely convex and strongly convex lower-level objective functions, respectively. The latter result improves the previous best-known operation complexity by a factor of $\\varepsilon^{-1}$. Preliminary numerical results demonstrate significantly superior computational performance compared to the recently developed first-order penalty method.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 239,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07399",
        "abs_url": "https://arxiv.org/abs/2511.07399",
        "pdf_url": "https://arxiv.org/pdf/2511.07399",
        "title": "StreamDiffusionV2: A Streaming System for Dynamic and Interactive Video Generation",
        "authors": [
            "Tianrui Feng",
            "Zhi Li",
            "Shuo Yang",
            "Haocheng Xi",
            "Muyang Li",
            "Xiuyu Li",
            "Lvmin Zhang",
            "Keting Yang",
            "Kelly Peng",
            "Song Han",
            "Maneesh Agrawala",
            "Kurt Keutzer",
            "Akio Kodaira",
            "Chenfeng Xu"
        ],
        "comments": "Project Page: this http URL",
        "subjects": "Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)",
        "abstract": "Generative models are reshaping the live-streaming industry by redefining how content is created, styled, and delivered. Previous image-based streaming diffusion models have powered efficient and creative live streaming products but have hit limits on temporal consistency due to the foundation of image-based designs. Recent advances in video diffusion have markedly improved temporal consistency and sampling efficiency for offline generation. However, offline generation systems primarily optimize throughput by batching large workloads. In contrast, live online streaming operates under strict service-level objectives (SLOs): time-to-first-frame must be minimal, and every frame must meet a per-frame deadline with low jitter. Besides, scalable multi-GPU serving for real-time streams remains largely unresolved so far. To address this, we present StreamDiffusionV2, a training-free pipeline for interactive live streaming with video diffusion models. StreamDiffusionV2 integrates an SLO-aware batching scheduler and a block scheduler, together with a sink-token--guided rolling KV cache, a motion-aware noise controller, and other system-level optimizations. Moreover, we introduce a scalable pipeline orchestration that parallelizes the diffusion process across denoising steps and network layers, achieving near-linear FPS scaling without violating latency guarantees. The system scales seamlessly across heterogeneous GPU environments and supports flexible denoising steps (e.g., 1--4), enabling both ultra-low-latency and higher-quality modes. Without TensorRT or quantization, StreamDiffusionV2 renders the first frame within 0.5s and attains 58.28 FPS with a 14B-parameter model and 64.52 FPS with a 1.3B-parameter model on four H100 GPUs, making state-of-the-art generative live streaming practical and accessible--from individual creators to enterprise-scale platforms.",
        "gemini2.5flash": "",
        "overall_idea": ""
    },
    {
        "order": 240,
        "date": "2025-11-11",
        "date_url": "https://arxiv.org/catchup/cs.LG/2025-11-11?abs=True",
        "arxiv_id": "2511.07413",
        "abs_url": "https://arxiv.org/abs/2511.07413",
        "pdf_url": "https://arxiv.org/pdf/2511.07413",
        "title": "DigiData: Training and Evaluating General-Purpose Mobile Control Agents",
        "authors": [
            "Yuxuan Sun",
            "Manchen Wang",
            "Shengyi Qian",
            "William R. Wong",
            "Eric Gan",
            "Pierluca D'Oro",
            "Alejandro Castillejo Munoz",
            "Sneha Silwal",
            "Pedro Matias",
            "Nitin Kamra",
            "Satwik Kottur",
            "Nick Raines",
            "Xuanyi Zhao",
            "Joy Chen",
            "Joseph Greer",
            "Andrea Madotto",
            "Allen Bolourchi",
            "James Valori",
            "Kevin Carlberg",
            "Karl Ridgeway",
            "Joseph Tighe"
        ],
        "comments": "Website: this https URL",
        "subjects": "Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)",
        "abstract": "AI agents capable of controlling user interfaces have the potential to transform human interaction with digital devices. To accelerate this transformation, two fundamental building blocks are essential: high-quality datasets that enable agents to achieve complex and human-relevant goals, and robust evaluation methods that allow researchers and practitioners to rapidly enhance agent performance. In this paper, we introduce DigiData, a large-scale, high-quality, diverse, multi-modal dataset designed for training mobile control agents. Unlike existing datasets, which derive goals from unstructured interactions, DigiData is meticulously constructed through comprehensive exploration of app features, resulting in greater diversity and higher goal complexity. Additionally, we present DigiData-Bench, a benchmark for evaluating mobile control agents on real-world complex tasks. We demonstrate that the commonly used step-accuracy metric falls short in reliably assessing mobile control agents and, to address this, we propose dynamic evaluation protocols and AI-powered evaluations as rigorous alternatives for agent assessment. Our contributions aim to significantly advance the development of mobile control agents, paving the way for more intuitive and effective human-device interactions.",
        "gemini2.5flash": "",
        "overall_idea": ""
    }
]